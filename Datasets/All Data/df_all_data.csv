,Text
0,"This evaluation had two limitations. First, although the method is not language-dependent, it was evaluated on a single language, Japanese. It would be worthwhile to evaluate the method on other languages to examine the approach’s versatility. Second, the method uses dictionaries to obtain patterns. Although Japanese morphological analysis commonly uses dictionaries to perform lemmatization, it would be worthwhile to evaluate the method with only training data or dictionaries derived from text.
Below, I discuss the current limitations for word segmentation, POS tagging, and lemmatization in detail.
Word segmentation The proposed method’s accuracy of word segmentation will depend on the target language’s typological factors (Shao et al., 2018), such as the character set size, lexicon size, and average word length. Among those factors, the character set size will especially matter because the current patterns mostly comprise surface strings and are likely to suffer from data sparseness. It will thus be valuable to evaluate the method on Chinese, which has a larger character set than Japanese. It will also be important to evaluate the method on languages with different typological factors from Japanese, such as Hebrew and Finnish. The training data size will not matter if the method is used to approximate some existing resource-efficient method via structure compilation (Liang et al., 2008).
POS tagging Compared to word segmentation, POS tagging requires more complex and abstract feature sets that are tailored for the target language and POS tag set (Spoustová et al., 2009), which poses a challenge for the proposed method. The current pattern template is tailored for Japanese and the JUMAN POS tag set; hence, for other languages and POS tag sets, a pattern template will need to be designed by referring to the feature templates of existing learning-based methods for the target language and POS tag set. Because the method jointly solves word segmentation and POS tagging in a left-to-right manner, patterns cannot leverage certain abstract features from posterior contexts of the target word (e.g., the next word’s suffix). For application to other languages, it would be worthwhile to explore not only left-to-right processing but also right-to-left processing and a cascaded pipeline approach.
Lemmatization The approach here currently requires a morphological dictionary with lemmas or a fine-grained POS tag set that includes conjugation types and forms to perform lemmatization. Because lemma generation rules for other languages can be induced from lemma-annotated datasets (Straka, 2018), the method could be applied to other languages by using such lemma generation rules as the target labels for classification. Challenging target languages include morphologically rich languages such as Arabic and Czech."
1,"This paper argues that the proposed task of ellipsisdependent reasoning is a difficult challenge for GPT-3 models, which are among the most powerful current language models. The data constructed here is restricted to English, and furthermore is restricted to a single form of ellipsis, namely verb phrase ellipsis. It may well be that other forms of ellipsis may give rise to different effects, and it is also important to test the claims made here on other languages."
2,"While we have improved the asymptotic running time of a classic algorithm with regard to grammar size, the time complexity of our algorithm is still cubic in the length of the input. Our result follows the tradition of dynamic programming algorithms that trade time for space by memoizing and reusing pre-computed intermediate results. The usefulness of this trade-off in practice depends on the specifics of the grammar, and while the complexity is strictly better in terms of non-terminals, it will be most noticeable for denser grammars with many nonterminals."
3,"Limitations (unnumbered)
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
4,"One of the main limitations is that we used the standard LIWC-based analysis approach, which is purely lexical and does not take into account the context in which a word appears. Consequently, many words that have very specific senses in the context of the IETF get miscounted as occurrences of LIWC categories. This could be addressed by a more advanced method of mapping to LIWC categories that would account for context. Another limitation is that we manually generated a filtering list containing words specific to the IETF. This list might not be exhaustive enough. Also, we were limited by not conducting an exhaustive hyper-parameter search on our models. We also understand that many emails are longer than 512 tokens (the input limit of the BERT model we used) and might have not been captured completely by our BERT model. However, most of the emails do fit into this BERT sequence length limit. We did not fine tune BERT on the IETF data; this might have given better performance, although it is not clear if it would have given more insight: our main goal is not performance but analyzing/comparing characteristics of existing models. It is also worth highlighting that the data used in this work is strictly in English, and the psycholinguistic categories in LIWC are also based on English language. Hence, this study may be biased and not fully capture variations in linguistic traits that are culturally agnostic.
Ethical considerations — Participation in the IETF is bound by agreements and policies explicitly stating that mailing list discussions and Datatracker metadata will be made publicly available.7 We use only this publicly available data in our analysis. We have discussed our work with the IETF leadership to confirm that it fits their acceptable use policies. We have also made provisions to manage the data securely, and retain it only as necessary for our work.
7See both https://www.ietf.org/about/note-well/ and the IETF privacy policy available at https://www.ietf. org/privacy-statement/."
5,Section 6
6,"This paper mainly focuses on modelling basic meaning to identify metaphors, typically learning basic meanings from literal annotations of the VUA dataset. However, our analysis reveals that the literal annotations of the VUA dataset are incomplete, which means that some words in VUA have no literal instances annotated. Although we propose using contextual word embeddings as a backup in this paper, another promising solution for this issue might be using external resources such as dictionaries. Leveraging dictionaries is commonly used to assist manual metaphor detection, so it could also help our BasicMIP mechanism to generalise. We leave this for future work."
7,"We highlight three limitations of our work. The first is that xsim++ is automatically constructed. There could be noisy sentences leading to errors that are irrelevant to the quality of encoders. The second is that xsim++ applies transformations solely to English sentences. Generalizing it to non-English language pairs requires additional research. Finally, we have experimented with the two most popular multilingual encoders: LASER and LaBSE. There are other available approaches which would be interesting to also validate xsim++ against."
8,"There are some limitations in the use of GPDA.
• The label propagation procedure requires anchor matching in the light of annotation precision, which limits the unlabeled data source. However, Wikipedia is a open-domain easyto-fetch corpus with anchor links, which can somehow mitigate the issue.
• Augmented Data generated by GPDA provide more diversity. But for some datasets, simple modifications (NERDA) on the original words performs better. We are investigating a hybrid approach to apply GPDA and NERDA in the same framework."
9,Left blank.
10,"As mentioned in the main paper, one of the limitations of our Centrum model is that it tends to produce longer outputs in comparison to PRIMERA. This necessitates controlling the length of the summary by truncating to a desired length. Moreover, due to our requirement of at least three documents in a cluster for centroid computation, we are unable to utilize clusters of only two documents present in Gu et al. (2020). This constraint significantly reduces the utilizable corpus size, leading us to work with roughly 45% of the corpus size used by PRIMERA. Future research could explore the possibility of initializing Centrum with the gap sentence generation-based Pegasus (Zhang et al., 2020) single document summarization objective, potentially allowing for full utilization of the corpus size of Gu et al. (2020)."
11,Section 6
12,"Section ""Limitations"" (unnumbered, page 5)
7 A2. Did you discuss any potential risks of your work? There are no reasonable risks in our work."
13,"Limitations
7 A2. Did you discuss any potential risks of your work? We don’t see the potential of how our two techniques can be misused.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 4
3 B1. Did you cite the creators of artifacts you used? 2, 4"
14,"Limitations The model we proposed is specifically for classification, while it is possible to be extended to other NLP tasks by changing the highlevel task-specific layer. Besides, in the evaluation, we focused on English corpora. We plan to test on other languages in the future.
Potential Risks We make our code publicly available so that everyone can access our code. As the model is a classification model, it does not generate risky content. Users should also notice that the classification predictions may not be perfectly correct."
15,
16,"Unnumbered ""Limitations"" section at the end."
17,"While BOLT shows an impressive performance in imposing soft constraints and some hard constraints, it still lacks when it comes to imposing harder constraints, for e.g., keyword control with more than three keywords. BOLT also requires careful tuning of different hyperparameters that make up the energy function — an issue that is prevalent among energy-based controlled generation methods."
18,"Although multilingual, the constructed open KB is limited to the sampling of the chosen six languages. We do not know how well the system will generalize to various language families that have not been considered here. Further, even among the languages considered, the performance of even the best-performing systems, as measured through H@1 is still in the low 20’s. Therefore the models are not yet ready to be deployed for real-world applications."
19,Limitations section (after Section 6: Conclusions)
20,"Broader Impacts
Despite the promising performance of MARCO at detoxifying text, there are several limitations, ethical considerations, and broader impacts of our approach, which we list below.
First, in this work, we seek to detoxify sentences. However, toxicity itself is a subjective and sensitive concept with large potential downstream impacts caused by annotator and subsequent model biases (Sap et al., 2022). We somewhat mitigate this variation by selecting human evaluators that scored highly on a toxicity qualification task (see Appendix D), in line with a prescriptive paradigm of toxicity annotation (Rottger et al., 2022). Future work could investigate the effect of demographics on preference for different rewriting algorithms, e.g., in a more descriptive paradigm.
In addition, achieving meaningful semantic preservation in detoxification is challenging. Specifically, it is difficult to disentangle the toxic and non-toxic meanings from the input, making it challenging to generate detoxified rewrites with high preservation of only the non-toxic content; this may risk minimizing marginalized groups’ speech (Xu et al., 2021). Partially, this could be due to a lack of context incorporation (social, conversational, preceding sentences; Yerukola et al., 2023); future work should consider adapting detoxification methods in context (Cheng et al., 2020; Roy et al., 2023).
MARCO also requires finetuning two pretrained LMs, which is not computationally insignificant (Strubell et al., 2019; Schwartz et al., 2020). Future work could explore using smaller LMs to control a larger model (Liu et al., 2021), or even more lightweight approaches.
Additionally, we acknowledge that in the evaluation, we expose Turkers to toxic content, which might harm individuals, especially those with identities that the offensive content applies to (Roberts, 2017; Steiger et al., 2021). However, we pay a fair wage (US$8/h) and our work is approved by our institution’s ethics review board (IRB). See Appendix D for further details.
Another major ethical implication of our work is that, following previous work, we use the Perspective API to automatically assess toxicity, a classi-
fier which contains documented biases (e.g., demographic biases and racial biases; Dixon et al., 2018; Sap et al., 2019). Future research could consider different, more holistic views of toxicity and biases (e.g., Sap et al., 2020).
Finally, although our application in this paper is detoxification, we acknowledge that MARCO could be applied for the opposite purpose, ie., generation of toxic text from non-toxic text; this is a malicious application which we condemn. Although this issue is more prevalent for controlled generation methods (McGuffie and Newhouse, 2020), this is still a risk MARCO faces. In a similar vein, we do not endorse using the toxicity or microaggression datasets to develop models to generate more toxicity or microaggressions, as this may incur harm, especially to marginalized/vulnerable populations."
21,"Perhaps the main limitation of this work is that we only explore the approach within the context of machine translation benchmarks, although we conduct extensive experiments within this task that cover different training data scales and diverse pairs of languages, including low-resource ones. Nevertheless, we remark that the proposed approach is entirely general-purpose, and can be applied to any other language generation or even any neural classification tasks. We leave it to future work to investigate whether the same gains would apply in those settings. Furthermore, we have not yet explored how this technique would interact with other modelling choices, such as different optimizers, training objectives, or subword tokenisation algorithms. Lastly, our unigram initialisation of the bias term is currently done at the level of subword units, which do not always correspond to lexically or morphologically meaningful linguistic units. We leave the extension of this approach to more meaningful linguistic units, such as words or morphemes, to future work."
22,"Our approaches that are developed in the parameterefficient weight ensembling framework, and experiments have the following limitations. First of all, our framework cannot efficiently extract information from the parameters of the trained lightweight objects, resulting in relatively unsatisfactory performance of the approach resorting to the information from the weights, i.e., GraNd. Furthermore, the modules that we focus on in our analysis of module importance are only blocks and sub-layers of the blocks. We have not probed finer modules, in which we speculate more precise information about transferring lightweight objects across tasks is concealed. Last, all tasks in our experiments are formulated into the text-to-text format, and we have not conducted analysis on tasks in other formats."
23,"Page 5, after the Conclusion, and before the References.
7 A2. Did you discuss any potential risks of your work? Our work is only about algorithms, it is almost impossible to exist potential risks, so we didn’t discuss them in our paper."
24,"In this paper, we suggest incorporating textual and visual data from search engines for multimodal relation extraction. Despite the fact that the proposed model yields competitive results on the benchmark, it still has several limitations. Firstly, using a search engine is a feasible way to obtain related knowledge, but it also brings the issue of noisy evidence. Unrelated visual and textual evidence returned by the search engine may lead to incorrect predictions from the model. Additionally, not all the retrieved evidence is equally reliable, and sometimes sources may contradict each other. On the other hand, retrieval-augmented methods are slower than content-based counterparts, since retrieving evidence from the Internet requires extra time. Therefore, it may not satisfy some of the time-sensitive scenarios. Lastly, evidence may be presented in different forms other than texts and images. For instance, structural information such as tables, info lists, and knowledge graphs also provide important contexts for identifying semantic relations. Humans are able to extract relevant information from these heterogeneous sources for inference, while our relation extraction system can only model and reason over textual and visual evidence."
25,"In our work we faced numerous types of limitations that fall under different categories.
Data Our relatively small dataset size limits our analysis, especially with the use of language models. Furthermore, the label distribution is skewed across the different specialties (domains), which affects model performance, robustness and generalizability. The differences in distribution might be the result of how the data was collected, which was not in light of the anchor words, or due to the domain’s nature and/or the medical providers’ language of that specialty. Furthermore, the time frame that the data was sampled from might manifest certain biases that are different from other time frames. Finally, our datasets are only representative of a small number of specialties from two medical institutions. Patient populations and providers may vary greatly across medical fields and additional institutions.
Task The formulation of the labels for our task imposes limitations and challenges. Stigmatizing language is subjective and can vary between the perspective of the patient and the medical provider. As a result, we are aware that our medical experts’ annotations might impose a bias. Additionally, the negative connotations of language might be ambiguous and can change depending on a medical expert’s identity, background and specialty, which creates a bias that is hard to mitigate.
Computational Resources We only used IRBapproved servers to access the dataset and perform the experiments. Because these platforms had limited computational capacity and lacked the specifications required to build more complex neural models, we were not able to include more recent language models in our experiments that might
have yielded better performance. In the future, we hope to have access to machines that support more recent and state-of-the-art models."
26,"One potential way to improve the extractive performance of a generative system is to explicitly model the likelihood of extracts during training. Driven by this intuition, we investigate creating a mixture of extractive and abstractive candidates for contrastive learning in BRIO. Specifically, we obtain extractive candidates with beam labeling proposed in Xu and Lapata (2022b), while the abstractive ones are from the original BRIO training data. Nevertheless, as we can see, this mixing method hurts both BRIO’s extractive and abstractive performance. However, it is noteworthy that extractive summary is important in a wider context, as shown in Section 4: reference summaries
in CNN/DM are highly extractive and optimizing a model on these summaries therefore may have provided it with the task instruction needed for extractive summarization, albeit implicitly. We leave the study of a more effective extract-aware learning strategy for future study.
Furthermore, we emphasize that the conclusions drawn in this paper are based on results produced on English datasets from the news domain. Even though these datasets are established benchmark datasets for summarization it is imaginable that other domains and languages may have produced different evidence. Despite this, the results remain insightful as the results show that extractive summarization is in fact feasible with modern abstractive systems. In future research, we look forward to shedding light on the possibilities and limitations of the proposed methods in a broader context."
27,"Section 9.
7 A2. Did you discuss any potential risks of your work? Section 9."
28,"Our proposed method has the following main limitations which we believe are important directions for future work to address:
1. Gender dependency: Our approach does not account for sentences that only make sense for a single gender. For example, sentences like ""She needs to see a gynecologist"" would not be captured by our method. This is a common problem encountered by most debiasing algorithms as it is difficult to distinguish these.
2. Finite wordlist: The wordlist does not contain all gender-based words as the language con-
tinues to evolve. We believe that future works could employ better approaches that can automatically mine gender words relevant to a dataset.
3. Blunt substitution: The phrase substitution method is an improvement over direct word substitution, but there are still plenty of instances where the new sentence might be semantically incorrect. This does not have any major implication on inference as we are only doing few-shot learning, but it should not be extended to the entire dataset.
4. Binary gender: The method only focuses on the male and female gender. It does not consider non-binary or gender-neutral pronouns such as ""ze/hir."" This can be solved by using an updated wordlist, but the authors could not come across one at the time of writing.
5. Downstream analyses: While our work proposes methods that show reduced gender bias as per a set of metrics, the work in no way claims to reduce gender bias in general, especially on downstream tasks. However, we strongly believe that this technique holds potential to reduce gender bias on downstream tasks as well since we adopt a regular finetuning approach and focus mainly on better data interventions. Moreover, recent research has shown that fine-tuning-based debiasing approaches do not damage a model’s internal representations to a critical extent (Meade et al., 2022).
Overall, these limitations suggest that our approach may not be suitable for use in contexts where gender-specific or non-binary language is prevalent, and the underlying wordlist should be frequently updated."
29,"In the section ""Limitations"""
30,"The general performance of CLiCoTEA could be improved with a better MPLM than mBERT, such as XLM-R which has a larger token vocabulary and has been pre-trained on a much larger dataset. Our approach is currently not applicable to generation tasks where a multilingual text decoder is needed to generate text in unseen languages. We leave this
adaptation for future work. Unlike the statement made in Zeng et al. (2022), current multilingual VL models still do not surpass the Translate-Test baseline of the tasks from IGLUE benchmark. The performance of CLiCoTEA is promising but the best scores are still obtained when translating everything to English and using the (English-only) ALBEF model. The smallest difference in accuracy on MaRVL dataset between CLiCoTEA and ALBEF with Translate-Test is obtained in Swahili (-2%), while the gap is much larger (around -6%) for the other languages. Outperforming the Translate-Test achieved by ALBEF still remains an open challenge, especially for high-resource languages."
31,"The method to select negative examples could be improved, as randomly selecting negative examples for training might lead to identifying most of examples in the evaluation datasets as reasonable. Secondly, we did not explore using other number of candidates in the training set, we always use 2 candidate answers for each question."
32,"Section 7
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
33,"Limitations Section
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
34,"Our study has three important limitations. First, our study is small in scope. By their nature, experts are difficult to recruit and consequently the domains we can cover are limited. The small sample also suggests that the quantitative measures may not be stable in a larger or more representative sample.
Second, our observation process was somewhat artificial. We generated replies for our experts and did not to do any prompt tuning. This reflects the way the expert chose to ask the question, but does not capture the ceiling of performance that would be possible in a conversation. As the Family Medicine expert noted about our question comparing Wikipedia to ChatGPT, “for more detail one could spend more time with Wikipedia and to the organization themselves, but chat provides an immediate general summary and the opportunity to drill down further with ongoing questions and conversation.I have used chat GTP to do medical and biological research In a matter of minutes which would have taken me hours previously”. A more extensive study on information seeking behaviors would be of interest and Liu et al. (2023) is a useful step in that direction.
Third, the responses across experts are not necessarily comparable. We allowed experts to choose their own questions and provide their own interpretations of the key measures like coherence or conciseness. Comparability of scales across contexts is a long-standing problem in survey research (King and Wand, 2007) and we highlight some of the concerns around the accuracy question above. Nevertheless, we felt that asking a set of closedended questions would help to provide some aggregate judgment, adding some systematic data to the anecdotes shared in public forums. While we caution about drawing any binding conclusions from our preliminary work, we felt that given the fast-evolving nature of the technology, a quick assessment was merited. Our findings are broadly supported using different questions and methodology in Liu et al. (2023).
One important aspect that is out of scope in our analysis is differential accuracy by question asker. Latanya Sweeney’s classic study of racial discrimination in online ads (Sweeney, 2013) points to the possibility that how a question is asked or where it is asked from could result in inaccurate or harmful answers for marginalized communities (see also Noble, 2018; Benjamin, 2019). We have also focused exclusively on English language questions and answers, but differences in easily-available training data across languages can produce substantial differences in the information offered. For example, Yang and Roberts (2021) shows that embeddings trained on Baidu Baike—an online Chinese encyclopedia—encode substantially different
associations with sensitive historical events and people than Chinese Language Wikipedia (which is regularly blocked in China). There is much more to understand about the degree to which large language models can mimic expertise."
35,"HyDE has yet to be tested in a large-scale and multisite setting, which may offer more generalization challenges. Furthermore, an evaluation of notelevel classification performance was not conducted. Although we expect that HyDE would perform well under such an evaluation, this would require heuristics to aggregate multiple MCMs per note."
36,"Section 5
7 A2. Did you discuss any potential risks of your work? Section 6"
37,It’s the last section and it is unnumbered.
38,"The underlying assumption of our method is that abstract reflects the entire article, creating an unbiased summary of the paper. However, abstract does not guarantee an objective representation of the paper, can often emphasize the main findings while discarding details that the authors deem insignificant. This can lead to potential inaccuracies in paper representations, affecting the results of paper retrieval and recommendation.
Also, in this work we did not exhaust all possible training settings and evaluation strategies due to limited resources. We perform evaluation using three different standards. While we selected the most relevant evaluation tasks, it would be interesting to assess the quality of representations in other ways, such as citation graph reconstruction, predicting reader activity and other clustering-based evaluations. Additionally, with the emergence of large-scale language models, another interesting direction for future research is to investigate the relationship between model size and final performance."
39,"Our eKnowIA attack contains logical rules designed specifically for the English language. While these rules may apply or be adapted to other languages with simple morphology, there could be languages in which completely new rules may be needed. Both our attack and the KNOW method rely on knowledge bases, which may sometimes be noisy. We employed manual efforts to eliminate (a small number of) noisy triples from ConceptNet. Our attack also relies on a manual annotation to ensure that the adversarial inputs are natural (estimated to be the case 81.5% of the time). Finally, we were not able to test our methods on instances with long text, as we are not aware of datasets with NLEs for long text inputs or long NLEs."
40,"The main limitation is that the in-domain noise is hard to recognize in noisy multi-party conversations. Though our proposed RARM achieves the
best performance compared to all baselines, we find that if the content of the noise is close to the multi-party conversation’s content, the average accuracy of all methods is not high, how to improve the performance on these hard samples is worthy of further study."
41,"Whilst the system presented within this paper is capable of allowing human-in-the-loop contributions (via selecting the input keywords on which to condition the output), it is not able to produce tongue-twisters that take advantage of particular features of speech sounds such as place and manner of articulation, in order to create more advanced outputs that exploit phonetic relatedness (rather than exact matches). The same can be said of our proposed metrics, PO and Init-PO, which do not account for phonetic similarity across sounds that share manner/place of articulation (e.g. ""she sells sea shells""). Additionally, whilst commonly known tongue twisters may follow a particular format (e.g. rhyme schemes), such schemes and templates have not been enforced here. We also do not demonstrate the capabilities of these systems if they were trained on phonetic transcriptions explicitly, as we only aim to assess their performance when training on graphemes in standard orthography."
42,"In this section, we discuss some limitations of our method and future work based on the limitations.
First, the enhancement of the word-level module is not as strong as the remedy of the sentence-level module. Our word-level module solely achieves improvement compared with XBERTScore but doesn’t improve as much as the sentence-level module. The main reason is that the XBERTScore framework lacks sentence-level semantic knowledge. Besides, our word-level self-guided contrastive method doesn’t resort to external information and only consolidates the alignment already existing in the pre-trained language model. Second, ReFreeEval performs comparably with baseline models on language pairs involving German. We guess it is due to the evaluation of QE. Ma et al. (2019) mention that the evaluation results across all language pairs are unstable in “QE as a Metric” track and can’t explain yet.
In the future, we’ll further explore valuable external information on word level. And we’ll try to explore discrepancies among language pairs to optimize the results. In addition, our simple but effective data augmentation method - clause per-
mutation doesn’t rely on rules or toolkits, which is an initial attempt at modeling fluency. It could benefit from further refinement such as languagespecific knowledge, syntactic and semantic parsing to recognize clauses. We’ll conduct an in-depth investigation into further work."
43,"Section: Limitations
7 A2. Did you discuss any potential risks of your work? No potential risks"
44,Limitations
45,"The experimental environment we used for testing our agents gives artificially generated natural language text, whose distribution of vocabulary, syntax, and semantic frames is controlled and limited to what the natural language text generators can provide. While we tried to include out of vocabulary for entities in our experiments, applying the proposed approach to natural language text in the wild, such as chatbots working with human, must be faced with issues such as out-of-vocabulary entities, relations, etc. We believe, however, approaching from controlled “wildness” is an important direction of the work for interactive-text agents.
The experiments and embodiment of the method presented here also makes some assumptions on the underlying model (MDP) of the environment. These are discussed in the problem definition and methods (section 2 and 3). Perhaps the most important is the assumption that the environment can be sufficiently approximated with logical states. We also used a deterministic planner so highly stochastic environments are currently out-of-scope."
46,"One limitation of our work is that we focus on robustness of pre-trained transformer language models against word-level adversarial attacks, which is the most common setting in this area. Future work could extend this empirical study to other types of attacks (for example, character-level and sentence-level attacks) and for diverse types of architectures. Further, it will be very interesting to theoretically understand how label smoothing provides (1) the implicit robustness to text adversarial attacks and (2) mitigates over-confident predictions on the adversarially attacked examples."
47,Section 5.
48,"We find that 8 out of 14 reviewed works do not include an adequate discussion of the limitations of gloss approaches, inadvertently overstating the potential usefulness of their experiments.
In the context of sign languages, glosses are unique identifiers for individual signs. However, a linear sequence of glosses is not an adequate representation of a signed utterance, where different channels (manual and non-manual) are engaged simultaneously. Linguistically relevant cues such as non-manual movement or use of three-dimensional space may be missing (Yin et al., 2021).
The gloss transcription conventions of different corpora vary greatly, as does the level of detail (see Kopf et al. (2022) for an overview of differences and commonalities between corpora). Therefore, glosses in different corpora or across languages are not comparable. Gloss transcription is an enormously laborious process done by expert linguists.
Besides, glosses are a linguistic tool, not a writing system established in Deaf communities. Sign language users generally do not read or write glosses in their everyday lives.
Taken together, this means that gloss translation suffers from an inherent and irrecoverable information loss, that creating an abundance of translations transcribed as glosses is unrealistic, and that gloss translation systems are not immediately useful to end users."
49,"Section without number, after conclusion"
50,"We identify three limitations to our approach. First, parsing research claims and automatically classifying a sentence’s purpose (its meta-discourse) are not solved problems. It is more prudent to surface novel claims supported by original research than an author’s allusion to other research as background context. Second, the domain of biomedical scientific text is complicated by wordy prose, hedging, and long-distance anaphora. These aspects make natural language understanding challenging and present implementational challenges for tokenization, including truncating long sentences and extracting meaning from out-of-vocabulary tokens. Third, commonsense reasoning for detecting contradictions in biomedical text requires expert background knowledge and a working definition of when contexts are sufficiently aligned such that two claims are called contradictory, which may differ depending on the use case. We believe that context sensitivity and interpretability analysis of LLMs for NLI in challenging domains like this using attention mechanisms or frameworks such as
maieutic prompting (Jung et al., 2022) are particularly fruitful research directions."
51,Final required Limitations section
52,"We acknowledge the following limitations of our work:
• While the oracle selects a sentence according to the benefits it provides when performing NER, it does not consider the interactions between selected sentences. This may lead to lowered performances when the several sentences are retrieved at once.
• The retrieval heuristics considered are naive on purpose, as the focus of this work is not performance. Stronger retrieval heuristics may achieve better results than presented in this article.
• The studied documents only consist in the first chapter of a set of novels. Using complete novel would increase the number of possible information to retrieve for the presented global heuristics."
53,"Our analysis of the behavior of SPRL focused on intrinsic task scores. Higher SPRL scores suggest a better system. In practice, we do not yet understand how these scores affect downstream uses of SPRL labels. Furthermore, SPRL datasets are relatively small and are English only. As we are limited to the labels in the existing datasets, we are uncertain about how our results would generalize to larger datasets, new domains, and other languages."
54,"Limitations (unnumbered, page 5)"
55,"While the initial results are promising, the accuracy of our method remains lower than human VQA accuracy and models finetuned on the VQA datasets, which suggests that there may still be substantial progress that must be made before few-shot VQA methods with code synthesis are useful for practical real world applications. Also, further work is needed on extending the framework to additional primitives, as the results in Appendix F show that doing so does not always lead to improvements over the baseline method. Another limitation of our approach is that it relies on large capable LMs, which may be restricted in use due to compute requirements or cost (e.g. via available APIs). We also focus in this work on benchmarking VQA capabilities with English as the primary language – future work may extend this to other languages via multilingual LMs."
56,Section 6
57,"The first clear limitation of our approach is its textbased nature. This prevents important audio information, typically silences in speech patterns, from being exploited to generate subtitle breaks. A more complete system could be devised though, for instance by associating our text-based approach with the information provided by a forced alignment toolkit, whenever audio information is available. A simple method along these lines could be the following: 1. Apply our MLM-based segmentation but only generating a unique segmentation tag SEG; 2. Insert EOB markers wherever the
7Examples of segmented subtitles can be found in Appendix A.
silence between two aligned words is above a specified threshold; 3. Traverse the text sequentially and replace SEG with EOL if there exists a previous marker of type EOB, otherwise replace with EOB. We left this use of our method in combination with audio information for future research, as audio alignment for subtitles typically involves additional factors such as non-literal transcriptions.
Additionally, our method is limited in its adaptability to specific segmentation guidelines, which may be company-specific. The main adaptable parameters of our methods are the minimum and maximum parameters of the segmentation window, and the set of predefined punctuation marks over which masking is computed, neither of which could fully model idiosyncratic segmentation guidelines. However, in our experience at least, segmentation in real professional data tends to display varying degrees of consistency with respect to guidelines, and natural linguistic breaks seem to be the dominant factor for subtitle segmentation. A specific evaluation would be needed on data from varied professional datasets to determine the extent to which our method might deviate from specific guidelines.
Finally, other aspects of subtitling, such as the recommendation in some guidelines for subtitles to appear in a pyramidal view, i.e. with the first line shorter than the second line, have not been taken into consideration in this work. Our aim was to evaluate our core LM-based approach without additional variables that can vary across guidelines and may also have led to results that are more difficult to interpret overall. Our approach could nonetheless be easily augmented with constraints on relative line lengths within subtitles, by incrementing the scores of segmentation candidates that respect this surface-level constraint."
58,"Section 5
7 A2. Did you discuss any potential risks of your work? Our work introduces no additional risks on top of the risk associated with the underlying technologies."
59,In the Limitation section after Paper’s conclusion.
60,"The datasets in this paper systematically control lexical cues and world knowledge between critical conditions, allowing us to tease apart the effects of statistical heuristics versus reasoning about causal relations. However, the manipulation brings unnaturalness to sentences when scaling up into large-scale synthetic datasets, and constrains the level of linguistic complexity. As we have seen in Exp3, the small-scale dataset has more complex combinations of conflicting lexical triggers than the large-scale dataset, causing language models to behave differently across datasets. Though we further address the effects of conflicting lexical cues in Appendix A.4, it will be valuable to carry out additional investigation of effects of sentence naturalness, and to consider designing large-scale datasets using naturally-occurring data.
The study raises and leaves open a number of interesting questions: How exactly might counterfactual reasoning benefit from world knowledge? To what extent does GPT-3’s stronger performance reflect robust logical and counterfactual reasoning? While we lay out some possible explanations in the Conclusion and investigate the role of other linguistic and non-linguistic factors in the above experiments and in the Appendix, we leave additional systematic analysis for future work.
Finally, the experiments use English, in which counterfactual conditionals have distinct and systematic linguistic markers relative to other types of conditionals. It would be interesting to investigate other languages in which counterfactual conditionals are not marked linguistically, and require world knowledge to disambiguate. For example, a Chinese conditional could be ambiguous between “if it had rained today” and “if it rains today”."
61,Limitations (after the conclusion)
62,"Our work comes with some limitations. It is uncertain whether our results in two specific settings, multiple choice and extractive QA, would extend to more general settings for NLI, although the use of contradictions for factual consistency by Laban et al. (2022) suggests that they could. Additionally, 3-class NLI is not sufficient to capture all the natural language relations that might be needed to verify an answer. As such more challenging datasets in other settings and more granular NLI settings should be attempted.
Another limitation involves answer ranking and the associated computational cost. The main reason we did not test answer ranking in extractive QA is that we did not generate diverse outputs, but another reason is that such a procedure grows prohibitively expensive as the domain becomes more open. In a fully open domain, ranking would require a quadratic evaluation for each context passage against each reformulated answer candidate (Schuster et al., 2022). Future work should look at comparison approaches that amortize this cost, such as NLI-based dense passage retrieval (Reimers and Gurevych, 2019)."
63,"The last section of the paper.
7 A2. Did you discuss any potential risks of your work? Left blank."
64,in separate limitations section at end
65,"Limitations section at the end of the paper
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
66,"Some of the summarization datasets annotated for faithfulness are relatively small, which makes score estimates uncertain. Furthermore, many datasets contain only output from a limited number of generation systems, which makes it hard to properly account for potential biases towards certain generation systems that may confound scores (see Pagnoni et al. (2021)). These concerns are, however, alleviated to some extent since we study trends across many independently created datasets, which makes it less likely for a single bias to persist in all of them. Furthermore the availability of generation and thus annotated faithfulness data limits our experiments to English. Finally, it remains
unclear whether our results would still provide advantages when applied to larger models such as T5-11B, whose parameter count makes experimentation infeasible on the hardware available to us."
67,"At the time of writing this work, ChatGPT is only available as a proprietary free research preview via a web interface. This is limiting in several ways. (1) Parts of our analysis are qualitative, as quantification is challenging due to limited accessability of the investigated model. (2) Some details about the investigated model are not yet disclosed. This is true for the model design as well as for the data used to train ChatGPT. MultiWOZ is a freely available and widely used dataset, therefore no guarantee can be given that ChatGPT has not been exposed to at least some meta details regarding this dataset. (3) Given the nature of the free research preview, exact reproducibility is not guaranteed, as the model may change any time. However, it is expected that any future version of ChatGPT retains its general abilities and behaviors.
Model-as-a-service. Building a general purpose model such as ChatGPT is extremely costly and an option only for few. However, once it exists, it may be utilized for a multitude of purposes. As a model, ChatGPT does not need to be built for DST in order to be useful for DST. With capable enough general purpose models, fine-tuning towards specific tasks may be avoided. Fine-tuning is challenging for multiple reasons such as the need for adequate data, computational costs, risk of over-fitting and catastrophic forgetting, among others.
Just like its sibling model, ChatGPT will become available as model-as-a-service. The advantage of this is that a massive LM such as this is usable independent of the user’s hardware. But this advantage comes with the disadvantage that it will in all probability remain proprietary. In consequence, it will likely not be possible to ever run, adapt, train or modify ChatGPT on local machines.
ChatGPT as model-as-a-service is likely to remain a black box to customers and researchers, even if just in parts. The model may change any time. In fact, a model update during our experimental evaluation prompted us to re-process a few of our test dialogues. This property impedes backward compatibility and the ability to trust in familiar behavior.
A general purpose model may show too general behavior and converse about more than what is required or requested. This also poses vulnerabilities for adversarial attacks. To this end, models such as ChatGPT have been trained with human feedback to better handle malicious intent and abusive
behaviors. A model-as-a-service is a gated resource. As such, its indefinite availability cannot be guaranteed. Further, recurring costs for access may be too high for certain downstream tasks. As a hosted service, latency might become a bottleneck or hindrance for its use as a component in complex applications."
68,"Limits of Prompt-based Generation. This work specifically proposes improvements to the controllable generation portion of mixed-initiative dialogue systems. However, dialogue policy planning is still an important problem to consider. In order to evaluate generation improvements, we hold dialogue policies fixed — in the static evaluation, we condition on ground truth dialogue intents, and in the interactive evaluation, we follow the same dialogue intents prescribed by the RAP system. To this end, a mixed-initiative dialogue system cannot consist solely of a generation module powered by prompting. There needs to be a set of rules or models that govern how a system can regain control of a conversation; the generation module is just a means of enacting these rules. As discussed in Section 7, prompting is a great option if there is already a pre-existing policy planner.
Due to these limitations, we did not conduct an interactive evaluation in the ESC setting. Emotional support conversations are highly personal, as circumstances vary across individuals. It would have required having study participants pretend to require support regarding a fixed scenario, or for
participants to disclose their personal issues, which can raise other ethical concerns. Moreover, dialogue policy planning is not straightforward for emotional support, due to this highly variable nature. Effective support strategy planning requires expert knowledge.
In Section 7, we also discussed that prompting may be appropriate for developing systems for novel tasks in low-resource settings. However, deploying prompt-based systems may be less useful for the purpose of setting new benchmarks on existing leaderboards with a plethora of data. Such setting already have plenty of well-annotated conversations and simple fine-tuned models can often achieve strong performance.
Guardrails. Proper guardrails should be put inplace prior to productionization of any dialogue system, prompt-driven or not. While we witness strong overall response quality both in terms of human evaluation and automatic metrics, language models can generate contradictions. System builders may consider employing guardrails for dialogue consistency (e.g. Jin et al. (2022)) and coherence (e.g. Ye et al. (2021)), among others.
As with any training set, InstructGPT and other LLMs have been trained on finite amounts of data. InstructGPT has not been trained on data after 2021. This is also true of training corpora such as P4G or ESC; these corpora were published in 2019 and 2021, respectively. Particularly in any sensitive environments, guardrails should be put in-place for factual correctness (e.g. Santhanam et al. (2021); Wang et al. (2020)). RAP attempted to remedy this by incorporating retrieval for factual questions, which we also embedded into our prompting approach, but this knowledge base is also finite. In Section C we discuss one such example (Table A5). A possible solution is internet retrieval (Komeili et al., 2022), but search engines can also yield misinformation, which leads to hallucination.
Computational Cost of Language Models. LLMs are computationally expensive, and in the case of models such as InstructGPT, they are not open source. However, in this study, we did not have access to equally powerful open-source models such as OPT 175B, nor the appropriate hardware to load such a model (loading OPT 175B requires 350 GB of GPU memory). We performed initial experiments with much smaller models which fit our hardware constraints such as GPT-
J 6B, but there was much higher variance in performance. This is supported by the fact that many reasoning capabilities do not seem possible with models smaller than 175B parameters (Wei et al., 2022b,a). Given our limited budget for human evaluation, we opted to use the best performing LLM we had access to, InstructGPT.
Prompt Optimality It is possible that we do not use an “optimal” set of prompts as we did not mine prompts or perform soft prompting. However, prompt optimality itself is a problem in dialogue generation, because open-ended dialogue evaluation is a difficult task. Most automatic evaluation metrics do not align well with human ratings in dialogue (Yeh et al., 2021; Liu et al., 2016). This makes it suboptimal to use as a discriminator in soft prompting, for instance. Most existing work that does search for optimal prompts or tunes prompts works with tasks that have clearly defined automatic evaluation, such as sentiment analysis or table-to-text generation (van de Kar et al., 2022; Li and Liang, 2021; Lester et al., 2021). Moreover, human ratings are expensive and not scalable for systematic optimization."
69,"First, we only access limited computation resources and perform continual pre-training from BERT (Devlin et al., 2018), which is not general enough for every event-related reasoning task. Second, counterfactual reasoning makes our approach conservative in identifying causal relationships, so our
method has a higher precision. However, some potential causal relationships will be discarded. How to achieve a good trade-off between precision and coverage is a problem. In addition, the way we utilize knowledge is relatively simple, and it is very likely that we have not made full use of knowledge. Designing more complex knowledgeenhanced methods may lead to better results."
70,"After Conclusion
A2. Did you discuss any potential risks of your work? Not applicable. Left blank.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 3.1"
71,"Currently, our approach does not effectively leverage syntax tree information via GCNs, a commonly used method for incorporating syntax trees in this task. Further research is required to determine the most effective way to integrate syntax tree information into TOWE models."
72,Second to last section.
73,"While the dataset by Kumar et al. (2021) enabled us to test models for a range of often overlooked groups (e.g., non-binary or bisexual annotators), we ultimately modelled only four specific attributes (gender, age, education, sexual orientation). There are likely to be more factors that could play a role. Additionally, annotators in the Kumar et al. (2021) dataset are exclusively from the United States of America, so that results do not necessarily hold for other countries or cultures (Hovy and Yang, 2021). Specifically perceptions of harmful content online are known to vary across countries (Jiang et al., 2021).
We used only the (Kumar et al., 2021) dataset. This is mainly due to our strict criteria regarding dataset size and availability of annotator-level labels and sociodemographic information. These characteristics were a prerequisite for our experiments across different attributes with sufficient numbers of annotators. Most datasets which include annotator-level labels and sociodemographic information contain much smaller numbers of annotators and attributes. Nevertheless, with the Measuring Hate Speech Corpus there is at least one additional dataset (Sachdeva et al., 2022) with comparable characteristics that could be used in future experiments. Also, additional small-scale, more focused experiments could use datasets like Sap et al. (2022) or HS-Brexit (Akhtar et al., 2021) which was annotated by 6 annotators, each from one of two sociodemographic groups.
We do not study the aggregation of individual predictions or evaluate against majority labels, as these are not directly relevant to our investigation of sociodemographic attributes in models of annotation behaviour. Consequently, we cannot derive a conclusion about performance in those settings from our results. This is a noteworthy limitation, because part of the experiments introducing
multi-annotator models in Davani et al. (2022) compare labels aggregated from multi-annotator models against predictions from a standard classifier (directly trained on aggregated labels).
For computational reasons, our experiments use a comparatively small pre-trained language model (RoBERTa, Liu et al. 2019). Thus, results might differ with larger models."
74,"Limitations, 8"
75,"While we used multiple parsers to avoid biasing the evaluation towards one parser, all parsers used are relatively high-performing parsers—all have la-
belled F₁ scores above 0.8 on the CCGbank development and test sets. This evaluation is thus biased towards especially difficult sentences, since those will be the ones where good parsers produce errors. While we found no correlation between parser scores and judge disagreement, at least suggesting that the judgements were not a function of parse quality, poorer parsers (or good parsers on novel domains) may make different kinds of errors than those that appeared in our sample. It is unclear how F₁ and DF₁ would compare under such circumstances; understanding this better remains an open area of research.
The relatively high disagreement among judges in the second task (24% of sentence pairs) is concerning, but it should be noted that the sentence pairs were sampled from a set of disagreements between two different scoring methods. The extent to which this is a problem in practice is unclear, as judge agreement may not be as low on outputs from different parsers evaluated by the same scoring method—but it could also be lower.
Although the dependency-based evaluations discussed in this paper are standard for CCGbankbased statistical ccg parser evaluations, the reliance on extra resources (namely, the generate program and markup files from C&C) makes it somewhat unique. Because of this, the extent to which decomposed scoring, or the ideas behind it, would be useful for other evaluations scenarios (such as for other corpora, including ccg corpora from other languages) is unclear."
76,"Measurement of translation literalness is neither well studied nor well understood. We rely on a combined interpretation of multiple measurements to investigate our hypothesis and its implications. This limits the extent to which we can make strong claims, since in the absence of a highly correlated metric for translation literalness, it is hard to compare systems. We could only claim that our investigation indicates the presence of a tendency towards non-literalness in GPT translations, but a stronger result would have been preferred to further disambiguate the translation characteristics. Further, we only compare GPT translations in the standard zero-shot and few-shot settings and it is quite conceivable that more specific & verbose instructions could steer the LLMs to produce translations with different characteristics."
77,Section 5
78,"Experiments. We acknowledge the limited scope of our experiments, including only 8 (closeddomain) documents, 3 models and a single language. This is largely due to the limited availability of suitable large LMs and their high computational cost. Still, we believe that our experiments are valuable as a case study that already clearly showcases some interesting features of our methodology.
Computational cost. While we have demonstrated an efficient strategy to obtain predictions for all tokens at all possible context lengths, it still requires running the model N times for a document of length N .
For a k-fold reduction in computational cost, the technique may be modified to use a sliding window with stride k > 1 (instead of k = 1 as proposed above). See Appendix A.1 for details.
Choice of metrics. The proposed methodology allows investigating how any given metric is impacted by context, yet our study is limited to NLL loss and the proposed KL divergence metric (the latter for defining importance scores). These may not be optimal for every purpose, and other choices should be explored depending on the application. For example, to study sequences generated (sampled) from a LM, one might want to define importance scores using a metric that does depend on the generated token, e.g. its NLL loss or its ranking among all candidates. (Indeed, our web demo also supports ∆-scores defined using NLL loss values.)"
79,"Section 5
7 A2. Did you discuss any potential risks of your work? We did not identify any risks. The usual risks related to large language models are arguably not present here since we are not proposing or training a new LM, but a way to explain it."
80,Limitation Section.
81,"Yes. Section 6
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
82,"We list the limitations of our work as follows: • The Query Augmentation is designed for the En-
glish alphabet; therefore, other languages with different alphabets will require further work. • Since the training strategy relies on fine-tuning a pre-trained language model using a large passage retrieval dataset, it may not be suitable for languages with limited resources"
83,"Section 6
7 A2. Did you discuss any potential risks of your work? There is no potential risk associated with increasing the robustness of information retrieval applications to question containing misspellings."
84,"Although FPT shows better control ability, there are two points that need to be improved in the future. First, as in Gu et al. (2022), we need to select hyperparameter α to balance between the control ability and fluency in generated texts. Second, as shown in Table 5, although the time cost of FPT is lower than that of GeDi, it is higher than those of other prefix tuning-based methods and grows approximately linearly by a factor of 2 × number of attributes."
85,"Section 7
7 A2. Did you discuss any potential risks of your work? Our work is a foundational research and does not contain potential risks. Our experiments are fair."
86,"After Conclusion.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
87,"Our study illuminates the deficiencies of the MRF approach and applies statistically-motivated approaches to craft more performant probabilistic models. However, it is admittedly not clear how these insights can immediately be applied to improve downstream NLP tasks. We focused on models over pairwise tokens in order to avoid sampling
and work with exact distributions for the various approaches (MRF, HCB, AG). However this limits the generality of our approach (e.g., we cannot score full sentences). We nonetheless believe that our empirical study is interesting on its own and suggests new paths for developing efficient and faithful MLMs.
Ethics Statement
We foresee no ethical concerns with this work."
88,"Our model only focuses on utilizing text information for recommendation, which is a key limitation of this work. In real-world settings, recommender systems are usually required to handle heterogeneous information inputs. UniTRec is a pure textbased recommender modeling user history and candidate texts as inputs. However, incorporating additional side information (e.g., user profile, text topic labels, and dwell time of user behaviors) could further improve the recommendation performance and alleviate the cold start problem. Furthermore, UniTRec only models two-level relations of user behavior history. Nonetheless, incorporating more user behavior information, such as implicit and negative feedback, could further enhance the recommendation performance."
89,"In this study, we focused on the clustering task in order to assess the real impact of anisotropy on the quality of representations. The conclusion is clear regarding Euclidean and directional clustering but investigating other tasks like information retrieval and anomaly detection would further strengthen the present findings. Also, the set of post-processing methods is not limited to the ones used in this study, and it would be interesting to conduct a more comprehensive study, including more transformation functions. Finally, an important future direction is to assess the impact of anisotropy on other languages, especially on embedding models trained on a restrained corpus, which can be the case of low-resource languages."
90,"Our paper has the following limitations
1. Our class-based influence score cannot improve the performance of GC algorithm. Although class-based version of GD, IF, and TracIn outperformed the original GC, we aim to develop a stronger version of GC. From the analysis in Sec. 4, we believe that a partially normalized GC could have better performance. In partial GC, we normalize the gradient of the clean data point z′(j) only. That will remove the noise introduced by ∥∇θ̂ℓ(z′(j))∥ while retaining the valuable information about the norm of ∇θ̂ℓ(z(i))."
91,The Limitation section after conclusion and before the references
92,"section limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
93,"In this paper, we only focus on investigating and improving gender fairness of pre-trained language models and didn’t touch other fairness issues given the length of the paper. However, we would like to note that with the investigation of other fairness issues in human language more deeply conducted, if the biased words regarding other fairness issues can be more specifically concluded, GEEP can be directly applied to address other fairness problems in pre-trained large language models.
7"
94,"We include a ""Limitations"" section in the paper."
95,Section 8
96,"While our analysis suggests that IT models do not fully utilize instructions but instead learn superficial patterns from instructions, there are some limitations to our experiments. First, we only analyze a SOTA IT method on the NatInst-V2 dataset and T0 dataset. Though Wang et al. (2022) showed that their model can outperform other large models such as Instruct-GPT (Ouyang et al., 2022) and T0 (Sanh et al., 2021), we did not analyze other IT methods, such as RLHF (Reinforcement Learning from Human Feedback) in Instruct-GPT. Secondly, since our analysis is conducted in the training stage, we cannot analyze private models such as Chat-GPT. Also, we did not explore models larger than 7B parameters due to our computation resource limitation. This may miss some emergent abilities of large language models (LLMs) (Wei et al., 2022). Lastly, while we observe the models do not utilize the majority of the instructions by IT, a certain degree of instruction understanding may already exist in pre-trained LLMs, which we did not study in this work. In conclusion, our work is a concentrated analysis to illuminate the potential vulnerability of the current IT models and evaluation metrics. We encourage future works to conduct more comprehensive studies on larger models and propose more reliable IT methods and evaluation frameworks."
97,"Dataset and Experimental Limitations. The datasets and tasks we focus on are from the XGLUE benchmark (Liang et al., 2020). The structured prediction tasks, namely Named Entity Recognition (NER) and Part of Speech (PoS) Tagging, both have a limited number of training samples at 15k and 25.4k samples respectively. This is due to the difficulty in annotating on the token level, however it can still be viewed as a limitation when compared to the remaining sentence-level tasks the majority of tasks have at least 100k samples.
Methodological Limitations. Below are a list of the main methodological limitations we perceive of our work:
• Our method requires a teacher model that is already trained on the downstream task which can then be used to perform knowledge distillation. This is limiting when there are constraints on the computing resources required to produce the quantized model.
• We have focused on the problem of reducing accumulative qunatization errors which become more apparent the deeper a network is. However, this problem is intuitvely lessened when the model is shallow (e.g 3-4 layers) but perhaps wider. Hence the results may be less significant if the model is shallower than what we have experimented in this work.
• By introducing the distillation loss we require an additional regualrization term β to be optimally set, relative to the main distillation loss α. This can be viewed as a potential limitation has it introduced an additional hyperparameter to be searched to obtain best results on a given task.
• Lastly, since intermediate layer outputs of the teacher network are required for self-attention distillation, we have to perform two forward passes during training. Since standard KLD distillation only requires the output logits, it is common to store the training data teacher logits, eliminating the need to perform two forward passes at training data. However, this is not an option with self-atttention outputs as the storage required offline scales with the number of self-attention heads, number of layers and the size of the training data."
98,"Section 7
7 A2. Did you discuss any potential risks of your work? We study open-domain information extraction for researches in this area"
99,"Page 5
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
100,See the limitations section on page 5.
101,"The PhotoBook dataset has a very limited number of images (i.e., 360) and image combinations (i.e.,
14Cross-attention mechanism explained in Appendix B. 15Likely due to limited dataset size and configuration. More
analysis and exploration can be found in Appendix E.
5 per game theme), which may lead to undesirable overfitting behavior as we discuss in Appendix E. Also, since our model depends heavily on CLIP (Radford et al., 2021), it is likely to inherit CLIP’s biases and weaknesses. For example, Radford et al. (2021) mentioned that CLIP fails to perform well on abstract or more complex tasks, such as counting or understanding spatial relationships between objects. Finally, whether our listener model can be easily applied/adapted to productive real-world tasks (e.g., automated customer service with image inputs) requires further exploration."
102,"First, neither Feldermodell (Reis, 1980; Wöllstein, 2018; Höhle, 2019) nor Doppelbaum (Wöllstein, 2018) has obtained complete concurrence among linguists. Also, we limit our scope to the English– German language pair and the IT domain using the WMT 2019 training, validation, and test data sets. A broader scope would not provide confidence in the validity of conducted experiments because there are hardly any standard setups for experimental research (Chatterjee et al., 2018, 2019; Akhbardeh et al., 2021).
In addition, the conducted experiment should take into consideration the effect of randomness that is attended in the process of training artificial neural networks; different techniques, different hyperparameters, and multiple runs of optimizers (Clark et al., 2011) may present different results. However, as previous studies (Chatterjee et al., 2018, 2019, 2020; Akhbardeh et al., 2021), including the study on the baseline model (Shin et al., 2021), do not consider the effect of randomness, we also do not investigate the effect of randomness further, considering that training multiple models (Appendix A) to obtain good estimators (TER and BLEU) will cost a lot."
103,"Section 8.
7 A2. Did you discuss any potential risks of your work? With the standard setup, studies in the field of automatic postediting, including this work, do not involve potential risks."
104,"Section ""Limitations"" (5th section)"
105,Limitations
106,"All our experiments are done on the sequence labeling task, and they can be further evaluated on sentence classification tasks with classifier-based fine-tuning since the [CLS] token used for classification represents the whole sentence. We provide a causal opinion on demonstration-based learning and a simple but not systematic method to alleviate the induced bias. Our demonstration-based learning builds upon previous works (Lee et al., 2022; Zhang et al., 2022a), where BERT or RoBERTa are used instead of Large Language Models, such as InstructGPT (Ouyang et al., 2022), PaLM (Chowdhery et al., 2022), and OPT (Zhang et al., 2022b). Furthermore, our conclusions are drawn from fewshot learning settings and cannot be directly applied to zero-shot inference."
107,Section 6
108,"• We currently rely on gold annotations for attribute marking, which are not always available depending on the dataset. However, RAMP could be easily extended to unsupervised settings through LLM feature attribution (Sarti et al., 2023), i.e., extracting salient tokens driving the attribute prediction. This approach builds upon recent techniques in unsupervised language generation metrics (Fomicheva et al., 2021, 2022; Leiter et al., 2022). We leave an empirical evaluation of its effectiveness to future work.
• Besides the choice of in-context examples, prompting is also sensitive to their ordering (Lu et al., 2022) and the design of the template (Jiang et al., 2020). We refrain from tuning example orders and templates to avoid introducing too many variables.
• Multilingual LLMs perform competitive MT out of the box for languages seen during their pre-training. However, we noticed that BLOOM 175B produces better EN-IT translations than XGLM 7.5B even though IT is not listed as a training language of BLOOM. This could possibly be due to typological similarity between Italian and the Romance languages included in BLOOM training. We leave experiments of unseen languages as future work.
• Multilingual LLMs like the ones used in this paper require larger GPU resources for inference than standard bilingual MT systems.
• One test set we use (MT-GENEVAL) provides only two gender values (female and male), but we do not intend to imply that other genders do not exist."
109,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
110,Limitations
111,"This work has several key limitations. First, we have relied on evaluation against self-reported (questionnaires) assessment of anxiety. Selfreporting the degree of anxiety on a survey instrument is not entirely dependable in diagnostic accuracy. However, it has shown reliable associations with diagnoses, serving clinical assessment treatment purposes beyond diagnosis (Kroenke et al., 2001). For example, anxiety scores from selfreported surveys have been robustly associated with consequential real-world outcomes such as mortality (Kikkenborg Berg et al., 2014). Clinical evaluation of the assessments proposed in this work should be evaluated against clinical outcomes.
Furthermore, the sample may not fully reflect the language use of the general population as it is skewed towards young and female4 and only focused on English spoken by those from the U.S. and U.K., although previous work suggests this dataset contains a diverse representation of socioeconomic status (Matz et al., 2019). Additionally, we do not focus on actual utilization of discourse relations in assessing anxiety, as the scope of this work limits us to showing the viability of modeling anxiety on a continuous scale and the importance of discourse information towards modeling it. Lastly, the strong associations of theoretical discourse relations come from models that themselves are not perfect, with F1 scores ranging from 0.770 for counterfactuals to 0.868 for causal explanations, though one might expect this error to lead to underestimates of correlation with anxiety.
With NLP increasingly working towards better human-focused applications (e.g., improving mental health assessment), we are presented with increasing considerations for human privacy as a trade-off with considerations for open data sharing. In this case, the data used was shared with consent only for academic research use. Open sharing of such data violates trust with research participants (and agreements with ethical review boards). These and additional issues are discussed at length in Benton et al. (2017a). While it would be ideal to
4The self-reported user age averaged 22.6 (SD 8.2), and over half (58.1%) marked their gender as female.
release everything and preserve privacy, in this situation, we believe the fact that the unprecedented data is not universally available suggests an imperative for those with access to openly share our work as best possible within ethical guidelines. We are thus releasing aggregated anonymized features from the secondary evaluation dataset that allows one to qualitatively replicate the associations in our results while preserving the privacy of participants."
112,"We briefly mention some limitations of our work. First, we have only used a single dataset, and a single model family in our experiments. This is mainly due to the fact that the benchmark we use is the only publicly available dataset at this time to the best of our knowledge. We also solely focused on extraction metrics, but did not do a deeper analysis on the extracted sequences. A fine-grained analysis of extracted sequences could yield important insights for understanding memorization and extraction in LLMs. Similarly, we also did not analyze what our prompts converge to, and whether they yield explainable prompts at the time of converge. Such analysis can provide better insights as to why, for example, training prompts with aligned CLM performs better that the basic CLM setting. Finally, we believe the evaluation of our defense could be improved further by measuring other utility metrics (e.g., accuracy) on downstream tasks."
113,See Section 6
114,"Section Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
115,"MOSPC can improve ranking accuracy on each fine-grained MOS score segment, but at the same time, the training method based on pair comparison may impair the generalization performance. As there are unseen-systems in the BVCC validation set, the system-level results of BVCC are affected by the generalization performance degradation. We introduced the C-Mixup algorithm to enhance the generalization performance, which increased the complexity of the experiment to some extent."
116,"Our proposed system was tested on two opendomain TableQA datasets, with one of them (E2EWTQ) being relatively small compared to the other. Also, the current open-domain TableQA datasets are limited to look-up questions. They do not cover more complicated questions that involve multiple cells and complex table operations, such as SUM/MAX/MIN/SUBTRACT in some questions of WikiSQL and WikiTableQuestion. Therefore, the effectiveness of our system should be further evaluated on more complicated datasets of larger scale in the future. Another limitation lies in the token length limit of modern Transformer models. The best-achieving models typically accept up to 1024 tokens (e.g. BART, the base model of TaPEx). This limitation becomes more obvious when tables grow longer and the information being sought go beyond the limit. We believe that, with better approaches addressing this limitation, our system can achieve better performance. The solution can be either applying sampling strategies to pick the rows and columns that are most relevant to answering the question, or increasing the capacity of future Transformer models."
117,"Section 7
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
118,"While the vector representations obtained using XL-LEXEME for different languages are potentially comparable, lying on the same geometric space, the evaluation of cross-lingual semantic changes cannot be performed for lacking crosslingual LSC Detection resources. SemEval 2020 Task 1 datasets consist of small sets of target words, i.e., the number of target words for English, German, Latin, and Swedish is 37, 48, 40, and 31, respectively. The example of the Latin language highlights that XL-LEXEME can perform poorly on languages that are underrepresented in the training set of XLM-R and not covered by the WiC dataset. Generally, at the moment is not possible to state precisely how and how much XL-LEXEME
performance is affected by the language distribution in the XLM-R training set and the WiC dataset."
119,"The key limitation of our work is that, when conducting the review of approximately 60 papers (by searching through the ACL Anthology for works in computational social science since 2010), we encountered a skewed distribution of descriptive versus integrative works. In fact, it was relatively simple to find descriptive works, and that section of Table 1 could have been much longer. We also recognize that, due to the mixed nature of our field, scientific and integrative findings are not the only goal—our ‘big tent’ includes engineers as well, who value gains in performance indicators. Finally, the fact that we have few examples of papers showing a return to theory renders the possibility that our central claim is misinterpreted in a normative way as a mandate."
120,
121,"Our proposed ensemble approach for training the Transformer architecture has demonstrated promising results for the task of AMR ensembling. However, there are limitations that warrant further investigation in future research.
Our first limitation is the lack of generalization, as the approach was only evaluated on AMR parsing. Therefore, the application of an autoregressive ensembling model has not yet been tested on other Natural Language Processing tasks.
Moreover, in order to properly compare each ensemble system under the same conditions, we base all our experiments using the same underlying architecture, i.e. SPRING. There needs to be an exploration of these approaches using more recent, better performing parsers. However, this will require access to such systems.
Furthermore, the computational cost is also a limitation, as even though our proposed merger method, Assemble!, is more efficient than previous ensemblers, it is still computationally expensive, and particularly when we have to ensemble long graphs from multiple predictions. Moreover, as our Assemble! model is based on LongT5, it might be challenged when working with large datasets or when running experiments on resource-constrained systems. Therefore, we encourage the use of ensembling strategies focused on selecting the best graphs instead of merging.
Lastly, as our ensemble approach is based on Transformer, results can be difficult to interpret, as it can be challenging to understand how the generated graph has been ensembled by different predictions, leading to a lack of interpretability.
In summary, the proposed ensemble approach for training the Transformer architecture has shown promising results for the task of AMR ensembling and has the potential to be applied to other tasks, however, further research is necessary to address its limitations and improve performance."
122,Section 5
123,"The section called ""Limitations""."
124,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
125,"Though our method produces full sonnets that are more impressive than all previous approaches, it
is still not at the level of human-generated poetry. It is not clear how to achieve this level, whether it would be using massive large language models, or through our general approach, which is to bend those models around an interpretable framework that knows the rules that sonnets obey. Certainly our approach requires a lot less data – even if one used all the sonnets that have ever been written to train a language model, it is unclear that the language model would learn the very specific rules required of sonnets. However, there may be other ways to obtain these constraints that have not yet been developed."
126,The Limitations section
127,Section A.1 of the Appendix
128,Limitations
129,Section Limitation
130,Limitations
131,"The results we present must be viewed in the context of a few limitations. A limitation is that we only perform experiments in English and on one task at a time. To be more comparable to a LLM few-shot settings, other languages and a multi-task setup could be explored. Furthermore, in order to replicate the results access to none public models is required and inference must be performed on large amounts of data. Another limitation of our work is that it only explores the original CoT prompting approach, but we do not explore subsequent improvements, such a self-consistency (Wang et al., 2022)."
132,Limitations Section
133,section Limitations
134,"We are releasing ScoNe as a diagnostic tool for conducting controlled scientific experiments. This is our primary intended use, and we advise against uncritical use of ScoNe for real-world applications, as we have not audited the dataset for such purposes.
As a diagnostic tool, ScoNe’s primary limitation is its focus on English. Cross-linguistically, we find many strategies for expressing negation. The English-language strategy of using mostly adverbial modifiers for sentential negation is not the only one by any means, and we would expect to see quite different results for languages in which negation is expressed, for example, with verbal suffixes. This highlights the value of potential future efforts extending ScoNe to other languages.
By the same token, we acknowledge that many linguistic phenomena interact with negation even internal to English. ScoNe restricts to negation in the context of lexical entailment, and mostly uses “not” as the negative morpheme. This excludes a wide range of negation morphemes and negation strategies that ultimately need to be brought into the picture.
Finally, we note that there may be undesirable biases in ScoNe that could interact with biases in the models. ScoNe is in part derived from SNLI, which is known to contain gaps, social biases, and artifacts (Poliak et al., 2018; McCoy et al., 2019; Belinkov et al., 2019; Gururangan et al., 2018; Tsuchiya, 2018), and ScoNe may inherit some of these."
135,"Yes, primarily in the Limitations section."
136,"All prompting methods are trying to extract knowledge from the Large Language Models (LLMs).
Our paper compares their knowledge extraction abilities. Thus, the performance of RoBERTa-large can serve as a reference point and provide insights for other LLMs. However, it is still necessary to assess each large language model independently to understand its capabilities comprehensively.
We only tested a handful of simple manual prompt-and-verbaliser pairs which are included in Tables 3 and 4. It is entirely possible that there is a lot of room for improvement in the design of manual prompt-and verbaliser pairs, thus providing us a even stronger baseline. We have opted to use ten trigger tokens in Auto, in alignment with the experiment settings originally presented in the AutoPrompt paper (Shin et al., 2020). However, since the verbaliser domains generated under few-shot learning settings are noisy, reducing the number of trigger tokens may improve performance."
137,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
138,"We evaluated the most widely used distillation objectives including prediction layer transfer, hidden states transfer and attention transfer. However, some objectives are not included in our evaluation due to missing implementation details in their paper. For example, we only implemented the contrastive intermediate layer distillation objective proposed by Sun et al. (2020a) in task-specific setting, since code and implementation details are missing for task-agnostic setting. New objectives are increasingly appearing for model compression in the field of computer vision, such as Wasserstein contrastive representation distillation (Chen et al., 2021) and distillation with Pearson correlation (Huang et al., 2022), which can be included to have a broader scope of distillation objectives evaluation.
This work empirically studied the impact of the teacher layer choice for initialization and training objectives, however, further analysis is needed to understand why lower teacher layers are essential for initialisation, and why attention transfer behaves consistently well under various teacher layer choices in the task-specific setting, while hidden state transfer does not."
139,"A potential limitation of our experiments is the use of oracle validation labels instead of human manual annotation as in the real-world setting. However, all validation sets we used in our experiments were collected based on the manually defined seed set of entities and relations, carefully cleaned and augmented with manually labeled negative samples. Moreover, we chose this more easy-to-implement setting to make our results easily reproducible and comparable with future work.
Another limitation of experiments that use established data sets and focus on isolated aspects of knowledge-graph construction is their detachment from the real-world scenarios. Indeed, in reality knowledge graph completion is done in a much more complicated environment, that involves a variety of stakeholders and aspects, such as data verification, requirements consideration, user management and so on. Nevertheless, we do believe that our method, even if studied initially in isolation, can be useful as one component in real world knowledge graph construction."
140,"In this section, we point out several limitations in this work.
First, our in-domain evaluation experiments focus on passage retrieval for ODQA. While the dense retriever is mostly successful in ODQA, it can be also used in other types of retrieval tasks which may have different input and output format. For example, the KILT benchmark (Petroni et al., 2021) provides several knowledge-intensive tasks other than ODQA. The performance of TASER models trained on such retrieval tasks remain unknown.
Second, compared with traditional sparse vector models like TF-IDF and BM25, the cost of training is an inherent issue of dense retrievers. Although TASER significantly reduce the number of model parameters, the training cost is still high.
Third, in our experiments, we show that the learned routing does not outperform the deterministic routing. This may suggest a better architecture and/or training algorithms for learned routing is needed to fully unleash the power of MoE.
Last, as observed in §4.2, there is still a gap between TASER and BM25 in out-of-domain evaluation. Therefore, how to close this gap will remain
a critical topic for future work on dense retrievers."
141,"Section 7
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
142,"While our work shows promising results in improving the generalization capabilities of Transformers to sequences of arbitrary length, some limitations must be considered. First, our evaluation is confined to synthetic algorithmic reasoning tasks, which may not fully capture the complexity and diversity of natural language. We focused on synthetic datasets since they showed clear and somewhat surprising limitations of Transformer architec-
tures (Delétang et al., 2023). However, the generalizability of our approach to other tasks and domains remains an open question, and additional research, such as evaluation on SCAN (Lake and Baroni, 2018), CFQ (Keysers et al., 2020), COGS (Kim and Linzen, 2020), or the Long Range Arena (Tay et al., 2021), is necessary to understand its potential in real-world applications. Second, our approach introduces a new hyperparameter – the maximum sequence position L. Although our experiments in Appendix B.1 show that our method’s performance is largely unaffected by the precise value of L, practitioners may still have to tune the parameter depending on their specific problem domains. Third, we only isolate and ameliorate one failure mode of Transformer length generalization on synthetic datasets. However, there are other factors contributing to poor length generalization, such as attention becoming less peaked for longer sequences (Chiang and Cholak, 2022). Overall, we believe that our study’s limitations offer several interesting directions for future research."
143,"Regarding the Wikipedia articles used for creating our dataset Wikipedia Table and Image Generation (WikiTIG), some infoboxes may not follow the defined format and rules. This is because various users can freely edit infoboxes. Moreover, the HTML dump data published by English Wikipedia is not based on recent information.
In image generation, due to the standard settings recommended by Zhang et al. (2021); Ramesh et al. (2021); Wang et al. (2022); Wu et al. (2022a), our image generation task requires generating a cropped fixed-size square image instead of the original aspect ratio.
In addition, a table in an infobox may contain cells unrelated to image generation, and thus it may be redundant for image generation."
144,"First, unlike decoders in neural seq2seq models, which can attend to any previously generated tokens, QCFGs have a strong context-free independence assumption during generation. With this assumption, Neural QCFG cannot model some complex distributions. A potential solution is to use stronger grammars, such as RNNG (Dyer et al., 2016) and Transformer Grammars (TG; Sartran et al., 2022).
Second, we assume that both the grammars used by the source-side parser and QCFG are in CNF. Although it is convenient for discussion and implementation, CNF does not suit for modeling the structure of practical sequences. In semantic representations (e.g., Abstract Meaning Representation (Banarescu et al., 2013)), a predicate could have more than two arguments. Ideally, we should represent n-ary predicates with n-ary rules. However, for grammars in CNF, n− 1 unnatural binary rules are required to represent n-ary predicates. In natural language, we will face semantically meaningless spans due to CNF, which is discussed in Sec 4.2.
Third, although using decomposition improves the speed and the memory requirement, our lowrank models still cost much more computation resources than neural seq2seq models for two main reasons. (1) A large amount of nonterminal symbols increase the memory cost significantly. (2) Because finding the most probable string t2 from pθ(t2|t1) is NP-hard (Sima’an, 1996; Lyngsø and Pedersen, 2002), we follow Kim (2021) to use a decoding strategy with heavy sampling. For real data, we may need to sample hundreds or thousands of sequences and then rank them, which can be much slower than the decoding of neural seq2seq models."
145,"In this work, we focus on creating the EnglishFrench tense corpus. These two languages are among the most frequently and widely used languages in the world. In addition, they have several similarities in tenses, which are pretty helpful for research on tense consistency through machine translation. Thanks to the distinctive tense struc-
tures, the study of these two languages makes it possible to examine many common tense issues, but there are also some tense issues in other languages that are not covered by this study. For example, the implicit tense expressions in Chinese are difficult to correspond to the explicit tense expressions in English (Jun, 2020). Hence, our next step will be to extend the tense test set to other language families and even cross-language families to further study tense consistency. Also, as for future work, we will optimize both the tense annotation method and the tense prediction accuracy calculation. Besides, we did not propose a new method to improve the tense prediction accuracy. To be further, we will endeavour to improve the existing machine translation systems according to tense consistency."
146,"limitations the main limitation of the proposed approach is that it would be relatively costly to apply at production time, compared to the conventional lm evaluation. first, it requires drawing a number of tokenization samples, as defined by importance sampling, in contrast to a single pass through the evaluated sequence in the conventional approach. second, the conventional approach can be conducted with teacher forcing and efficiently parallelized, while the proposed approach relies on block-byblock sequential processing. nonetheless, the proposed algorithm is designed for analysis purposes rather than to be used in production systems, for which it is feasible to run it in a reasonable time, allowing users to evaluate the effect of marginalization for any tokenizer and language. broader impact as the work is dedicated to evaluating existing models on publicly available datasets, we are not aware of any potential negative impact."
147,"limitations one limitation of ourwork is that the rnn (meloni et al., 2021) actually outperforms our transformer on the chinese dataset in chang et al. (2022). in addition, as with other neural approaches, our model requires significant amounts of data, which is often not available to historical linguists researching less well-studied language families based on field reports. romance and chinese have relatively many cognate sets because the protoforms are documented5, but a low resource setup with 200 cognate sets would not fare well on our datahungrier transformer model. furthermore, concatenating the entire cognate set may not work on language families with hundreds of languages such as oceanic because the input sequence would be too long compared to the output protoform sequence. finally, we obtain our chinese gold protoforms from baxter and sagart (2014)’s middle chinese reconstruction, which was actually a transcription of the qieyun, a rhyme dictionary. norman and coblin (1995) disagree with relying on such a philological source and prefer comparative reconstructions that begin from daughter data. however, there is no available comparative reconstruction of middle chinese with protoforms corresponding to thousands of characters to use as a gold standard. be that as it may, it seems clear that middle chinese as recorded in theqieyun is not identical to the most recent ancestor of the chinese languages. its preface concedes that it is a compromise between tang dynasty dialects. the situation with romance is, in some ways, comparable. classical latin—the variety on which we train— is not the direct ancestor of modern romance languages. instead, they are descended from vulgar latin or proto-romance, which is not well-attested and is primarily through graffiti and other informal inscriptions. proto-romance reconstructions are also not exhaustive. as a result, it is difficult to find a dataset like meloni et al. (2021) with thousands of such ancestor forms. we are also limited to the faithfulness of espeak-ng’s latin g2p, from which meloni et al. (2021) obtain their phonetic romance dataset. for most language families, protoforms are not attested. in fact, as the term is often used, protoform refers to a form that is inferred only through linguists’ comparative method. we adopt the other usage for simplicity. in practice, our approach would require reconstructions made by a linguist to serve as training labels for cognate sets."
148,"limitations in this work we have tested our approach using spanbert, a relatively small model when compared to, say, deberta_large or gpt. spanbert has been reported to obtain state-of-the-art performance for relation extraction (joshi et al., 2020; tang and surdeanu, 2022), but it is unclear if a larger lm would improve this semi-supervised learning setting. we use both surface patterns (in the tokensregex (chang and manning, 2014) format) and syntactic patterns (odin (valenzuela-escárcega et al., 2016)) as training seeds, but our approach can only produce syntactic patterns as outputs. this is not ideal, since there is empirical evidence showing that the mixed representation for rules may provide better performance. for example, we can easily capture per_title relation with a surface rule such as “{obj_title} {subj_person}”, which simply looks for the two entities being adjacent."
149,"limitations of the original formulation, of being slow for large grammar sizes."
150,"limitations in this paper, we focused on analyzing the properties of textual representations in the few-shot learning scenario. its applicability to broader annotation scenarios could be presumed but is not supported by our empirical results. our experimental setup is based on binary classification tasks using english datasets. while our approach is general and could be easily extended to multi-class scenarios, more work would be required to extend it to other more complex structured prediction settings such as sequence tagging. we see several ways in which this work could be extended. the most obvious extension consists of trying to generalize the notion of alignment to other tasks beyond sequence classification, such as sequence tagging. in this paper, we have used thas to understand the quality of a given textual representation. however, since thas is a function of a labeling and a representation, it could also be used to measure the quality of a labeling (yan and huang, 2018), given a fixed representation. for example, this might be used in the context of hierarchical labeling, to measure which level of label granularity is better aligned with some input representation. the goal of this paper was to provide an explanation for the success of pre-trained word embeddings for text classification in the few-shot learning scenario. we believe that with our proposed methodology we have successfully achieved this goal. however, it should be clear to the reader that we do not provide a method for picking the best representation, i.e. for model selection. this is because our analysis requires access to labeled data and if labeled data is available the best way to select a model will be via cross-validation."
151,"limitations in protocol standards, stress on specifics, and compare with existing protocols or previous versions. several words across different liwc categories (risk, negemo, and adj) highlight such behaviour, e.g., ‘problems’, ‘before’, ‘particular’, ‘specific’, ‘different’, ‘most’, and ‘than’. however, there are many words with dual sense, like ‘trust’ which has a very technology specific usage related to network security instead of conversations involving trust issues between individuals or trust in any given situation. similarly, the word ‘live’ is related with an application or network being live, instead of its conventional meaning. we also observed that some of the liwc categories, such as bio, did not have specific terms that could clearly establish its significance in favour of influential participants (e.g., word ‘problems’ and ‘trust’ reflecting the significance for the category risk), instead such categories had several words with quite weak correlation with influential participants. such words collectively drifted the weight of the category towards influential participants."
152,"limitations we think the following three points are the limitations of this work. (i) as mentioned in section 3.3, the computational cost of our distillation approach increases linearly with the number of gd steps and the distilled data size. it is necessary to explore efficient distillation algorithms to scale our method to more complex tasks or full-scratch training in future work. (ii) to optimize the distilled dataset through the gradient method, we utilized word embedding vectors instead of directly optimizing the text as in the existing work. therefore, the distilled dataset we obtained cannot be applied to models with different word embeddings, such as other pretrained models or full-scratch training. (iii) in our experiments, we evaluated our approach only on text classification tasks. however, our approach can also be applied to text generation tasks as well by applying the attention labels to all input tokens (not only [cls]) and using vocabulary-wise soft labels. in future work, we should investigate its performance and explore more effective approaches."
153,"limitations this work complemented previous analyses on the link between the linguistic and psychological accuracy of a neural language model by expanding the language sample to ten typologically distinct languages. however, our sample of neural language models was limited with respect to the literature focusing exclusively on english (oh et al., 2022; oh and schuler, 2022; shain et al., 2022). this problem cannot be overcome at the present state of affairs, since there are very few available massively multilingual auto-regressive language models, and the only one with sufficient coverage of our language sample was xglm. this problem is an expression of a general difficulty in nlp to conduct experimental research on low-resource languages, due to the extreme skewness in the distribution of available resources (joshi et al., 2020). however, we are confident that future developments in natural language engineering will support an additional test of our hypotheses with a more representative sample of models."
154,"limitations future work can further apply our approaches to other semantic parsing tasks. for example, for parsing texts to lambda-calculus expressions for knowledge base question answering (dong and lapata, 2016), one can similarly preprocess the schema items (e.g., “department_time” into “department _ time”) and typed values (e.g., “dallas:ci” into “dallas : ci”) for more meaningful subword tokenization results. in addition, our experiments are based on t5. to further verify the effectiveness of our techniques, one can apply them to other pre-trained language models such as bart (lewis et al., 2020) and gpt-2 (radford et al., 2019) as well."
155,"limitations the presented classifier and dataset are only from english-speaking sources, a major disadvantage in detecting white supremacist content globally. the dataset also is predominantly sourced from data between 2015-2019 and reflects white supremacist extremist responses to current events from that period, including the black lives matter movement. this limits its effectiveness in detecting white supremacist content from other time periods. though including anti-racist data helps mitigate bias tested by our sample of the hatecheck dataset, an accuracy of 69.2% shows room for improvement. there is still a risk of overclassifying posts with marginalized identity mentions as white supremacist."
156,"limitations we present the first study of generating questions for filling in information gaps. our method is limited in several ways. first, it focuses on information that is explicitly missing, and does not discuss information that is inaccurate or incomplete in other ways. second, it only asks one follow-up question and does not address multi-turn dialogue about a student answer, or multiple student answers. finally, our approach makes somewhat restricted use of the student answer, and it will be better to generate questions that directly uptake information from the student text (demszky et al., 2021). we leave the deep investigation of these for future work."
157,"limitations since rule abduction and inversion utilize the same groundings as the original rules, neuro-symbolic kgc models that are based on grounding the entire rule will not benefit from these augmentations. abduction and inversion also require the model to be trained on a knowledge graph that contains the inverse relations r−1 for each relation r. finally, since rnnlogic+ has a separate rule embedding for each rule, performing rule augmentation increases the number of parameters in the model and leads to longer training times and larger gpu memory consumption."
158,"limitations the cogen model introduced in this paper uses temporal relations as a process of abductive reasoning. although, temporal relations have been shown to be very useful in abductive reasoning (verdoolaege et al., 2000), the measure of the effectiveness of other types of relations about an observation have not been evaluated in this paper. in addition, because of the unavailability of a large number of human evaluators, we randomly selected 100 selected results as opposed to the entire result which would have been ideal."
159,limitations. we take responsibility for any ethical concerns that may arise as a result of our research.
160,"limitations the pre-training privacy policy corpus and the downstream task datasets are unlikely to contain toxic or biased content. therefore, they should not magnify toxicity or bias in the pre-trained and fine-tuned models, although the models may exhibit such behavior due to their original pretraining. the pre-training and benchmark datasets are formed based on privacy policies crawled in the past; as a result, they could be outdated by now. this work focuses on the english language only, and the findings may not apply to other languages."
161,"limitations incomplete representation of all demographic groups we highlight that the names used in our study are not close to a complete representation of every demographic group in the united states or world. in our study, we adopt the definition of race/ethnicity from the us census survey, using us-centric racial and ethnic categorizations that may be less applicable in other countries. we adopt a binary model of gender (female and male), based on the ssa dataset, which is derived from statistics on baby names and assigned sex at birth; this approach limits our ability to study chosen first names, or to study fairness with respect to nonbinary and transgender people. for race/ethnicity, our study is limited to us census categories of white, black, hispanic, and asian. we are unable to include american indian or alaska native in our study, for instance, as we were unable to identify any names from this group that met our inclusion criteria of > 50% membership according to our name data source. furthermore, by using first names as a proxy for demographic attributes, we are only able to study certain demographic attributes that plausibly correlate with names (e.g., race, ethnicity, and gender) but not other demographic attributes that are likely harder to infer from names (e.g., ability or sexual orientation). other demographic attributes that may be discernible to varying degrees from first names were excluded from the scope of this study (e.g., nationality, religion, age). assumption: invariance under name substitution invariance under name substitution, while a valuable fairness criterion for social iqa, may not hold in all other task settings. for example, a factoid qa system should provide different answers to the questions “what year was adam smith born?” (1723) and “what year was bessie smith born?” (1894). extended evaluation time and heavy computational costs due to the huge number of mcq instances we construct for evaluation and a diverse set of names to cover multiple demographic identities, it takes a considerably large amount of time and computational resources to obtain the analysis results. we detail the approximated time and computational budget in appendix b.2. however, it is worth noting that the extensive analysis on a wide range of mcq instances and names makes our observations more statistically robust. a future research direction may be optimizing the implementation of sodapop framework, which we use as a major experiment setup to obtain the analysis, for more efficient evaluation. (in)effectiveness of counter-factual data augmentation it is worth noting that the ineffective result we obtained is not surprising because sodapop has demonstrated that models that are trained with existing state-of-the-art debiasing algorithms continue to treat names differently (an et al., 2023). although we find that controlling the name distribution in the finetuning dataset to be rather ineffective in mitigating the disparate treatment of names, it is an open question if applying cda to the pre-training corpus would be more effective. a recent work proposes to apply cda to the pre-training corpus (qian et al., 2022), and it will likely be a great source to use for investigating our open question here."
162,"limitations our proposed approach requires to train two independent classification models. while the models can be trained in parallel, this requires larger gpu memory. for the experiments, we trained two bert-base models, which have around 220m trainable parameters when trained in parallel. this requires almost twice the gpu memory compared to a single bert-base ner model, having around 110m trainable parameters. owing to a pipeline-based structure, the overall performance of our system is upper bounded by the performance of span detection model which has lots of potential for improvement. on dev set, we find that around 30% of errors for ontonotes5.0 and bionlp13cg, and around 22% errors on wnut17 are just due to minor boundary detection issues. their entity types are being detected correctly. we henceforth encourage the research community to design architectures or new training objectives to detect mention boundaries more effectively. currently, in our span detection model, all entity mentions are grouped into a single class. as a potential future work, we expect to get even better performance by a hierarchical extension of our setup. at the top level, we can detect mentions belonging to some crude categories and gradually break them down into more fine-grained categories."
163,"limitations to the transformer architecture (such as the inability to implement unbounded recursion), our results show that it may have stronger inductive biases than previously believed: with sufficient training, transformers can represent hierarchical sentence structure and use this structure to generalize correctly."
164,"limitations our work has the following limitations. first, we only evaluate generalization on datasets based on english language. second, we show structural grokking on three datasets, and while we believe this to be a general phenomenon, we leave investigating similar behavior on other datasets for future work. next, we also do not study the effect of training data size on structural grokking, and do not investigate whether transformers learn to grok hierarchical structure in low data regimes. finally, all datasets here are based on context-free grammars, either similar to or taken directly from prior work, and we believe constructing similar generalization benchmarks on real language data is a good avenue for future work."
165,"limitations while our proposed projection techniques often improve cross-lingual transfer, the choice of the projection layer and the projection probability in the case of random projection are hyperparameters that vary across tasks and languages. our ongoing work involves identifying a mechanism via which we can parameterize these quantities, enabling the model to directly learn the optimal layer and probability values for projection."
166,"limitations our proposed pre-training approaches require access to large gpu resources (pre-training is performed on 350m training samples for large language models containing 100’s of millions of parameters). even using 10% of the original pretraining compute, the additional pre-training takes a long time duration to finish (several days even on 8 nvidia a100 gpus). this highlights that this procedure cannot easily be re-done with newer data being made available in an online setting. however the benefit of our approach is that once the pre-training is complete, our released model checkpoints can be directly fine-tuned (even on smaller target datasets) for the downstream contextual as2 task. for the experiments in this paper, we only consider datasets from the english language, however we conjecture that our techniques should work similarly for other languages with limited morphology. finally, we believe that the three proposed objectives could be better combined in a multi-task training scenario where the model has to jointly predict the task and the label. at the moment, we only tried using different classification heads for this but the results were worse."
167,"limitations we note two limitations of our paper. first, our work does not extensively evaluate all the available pre-trained models that could be suitable for this task, e.g., electra (clark et al., 2020), biolinkbert (yasunaga et al., 2022), gatortron (yang et al., 2022), radbert (yan et al., 2022), and pubmedbert (gu et al., 2021). the aim of this work is not to report the strongest possible score but rather to address weaknesses of existing radiology report summarization studies (in terms of data and evaluation). yet, we are confident our proposed solutions report a strong baseline for future work. second, although f1-radgraph seems like an appropriate metric to evaluate our new modalities and anatomies (and appears to be consistent with rouge scores), it has only been evaluated subjectively and not systematically."
168,"limitations we replicate three well-known experiments in the gender bias literature, where bias is measured according to a binary female vs. male view. this choice ignores other views of gender but eases the presentation of the frameworks. we only use two corpora and three datasets which by no means capture the biases of all the people speaking or writing in the english language. moreover, we don’t experiment with different corpus sizes, a more diversified set of corpora or more bias types. we hope to explore this in future work. the hyperparameters of the models have not been varied, using their default values. this replicates the standard experimental setting used in the literature. since there are no ground truths when measuring biases (that is, there are no annotations with the amount of bias of words in large corpora), hyperparameters are usually set to their default values."
169,"limitations we only experiment with one type of retrievalaugmented language models, i.e., retro. however, the ways the other models retrieve neighbors and integrate them are not so much different to affect the results in this paper. the experiments in this paper are done with a small size retro model and data compared to the sizes considered by borgeaud et al. (2022), due to computational limitations. according to the same authors, however, the gains should be constant with the increase of the model and retrieval set size. the larger models are mainly different in their behavior when there is no overlap. however, this should not affect the copying tendency of these models tremendously, as it is still the easiest way to generate the next token. it is also worth noting that retro[off], while not using retrieval at test time, is still trained using retrieval – so it is not a complete retrieval- free model. the results presented by borgeaud et al. (2022) however, show that retro[off] is on a par with their retrieval-free baseline in terms of bpb. finally, we note that our evaluations have only considered the perplexity under teacher forcing, and we have not investigated the behavior of the model in free-form generation or with any kind of fine-tuning."
170,"limitations while the training method makes use of user profile description and history, one additional factor that is important is the structure between users and news articles. knowing a user’s social circles can often give hints about the user’s interests and beliefs, which can potentially help the model to infer how a particular persona would respond to an issue. a possible direction is to design a method that explores the social context features (e.g., social network) via graph-based algorithms."
171,"limitations our model does not follow existing sentence embedding models that encode sentences into embeddings. therefore, one limitation of our method is that it is specifically designed for sts task (or more precisely, sentence comparison task) and cannot be easily transferred to other tasks, such as sentence classification. additionally, our approach incurs a slight extra time overhead of approximately 10%, which may be unacceptable for applications that require high time efficiency. our method only takes into account the semantic comparison of individual tokens, rather than considering the meaning of combinations of tokens or phrases. a possible direction for future work is to incorporate the consideration of compositional semantics, for example by grouping tokens into phrases and applying a similar phrase-level matching algorithm."
172,"limitations there are several limitations to this work which should be kept in mind. first and foremost, the datasets for evaluating the measurement of semantic change are relatively small, meaning that any estimates of correlation with human judgements will be relatively high variance. in addition, although the semeval data includes text from four languages, there is no guarantee that these methods will work as well as they do on other languages or other time periods. moreover, our approach depends on the use of pretrained language models, and the quality (or existence) of these and other relevant resources will vary by language. in addition, like all methods, our approach involves numerous small choices, such as the number of background terms to sample, the number of samples taken, and the value of k in choosing top substitutes. we have kept our choices for these consistent across all five datasets, and these values have not been tuned. as such, different choices could result in better or worse correlation with human judgements. it is also worth noting that the human judgements collected by the creators of these datasets may involve errors or noise. it is possible that a different sample of data, or having different people evaluate the same data, would produce different judgements. for exploring the variation in word meanings, we have used the approach of eyal et al. (2022) directly, with the only differences being that we mask terms of interest (allowing us to work with terms that do not exist in the model vocabulary), and do not combine multiple forms of lemmas when getting the top-k terms. we adopt this approach because it is especially easy to combine with our own work, but different methods for word sense induction might lead to different"
173,"limitations the main limitation of our method is that it requires human effort to increase the variety of templates, which makes it difficult to create large datasets. using templates to generate data reduces the time required to create data manually, but the need for human labor remains an obstacle. to resolve this, the templates themselves need to be generated auto- matically, although the tags that constrain the nouns also need to be generated automatically, which is a difficult problem."
174,"limitations one limitation of our work is the experimentation only with languages with shallow orthographies, i.e. relatively simple g2p and p2g mappings. the results might vary for deeper-orthographies languages. although we took extra care to verify our conversions are correct and complete, and designed the rules to be as comprehensive as possible, automatic rule-based processes in languages may not be 100% perfect and some corner cases may introduce errors. these errors may propagate to affect the numerical results. to mitigate this issue, when ambiguities in determining a target phoneme (or grapheme) in a given language occur, we purposefully select the values that occur more frequently in the unimorph data of that particular language."
175,"limitations candidate summaries dependency while we mainly investigate a training objective to select the best summary among a set of candidates, we find that our model has been dependent on those obtained from the generation model. recently, several works have been presented to improve language generation. for example, narayan et al. (2022) and xu et al. (2022) improve decoding methods to generate diverse outputs. it will be beneficial when applying our method to these approaches. one-sentence summary our approach can fail to capture the information from an extremely short summary. since table 2 shows that our approach has a smaller improvement than cnn/dm, we plan to investigate that our model aims to capture more detailed features from an input text."
176,"limitation wherein novel predicates appear in the test set that do not appear in any of the training or validation set. this is a current limitation of our system. since we do not do any online learning during the test phase, there is no way to take these novel predicates into account. the significant effects of amr-originated noise or lack of information can be seen by comparing the second-to-last and third-to-last rows. here we see a significant degradation across metrics and datasets. however, the performance is still comparable or often better than the deep-learning-only benchmark of the first row. comparing the last row (full method with proprioception module) and second-to-last row shows that we can recover most of the performance. we can also see that the metrics are competitive to the model-based approach from game-engine provided logical facts (3rd-last row). this shows the effectiveness of adding the proprioception module comprising both the memory and memory-based constraints."
177,"limitations our approach relies on the performance of the fewshot paraphrasing. this results in two limitations for our approach. one limitation is the difficulty in accessing gpt-3 and opt-175b models. these models currently need to be more widely available. opt-175b has a free version but it is very slow. another limitation is the need for annotated demonstrations for few-shot paraphrasing. while there are available models and tools, like quillbot, that can be used for this purpose, their quality is not comparable to gpt-3 and opt-175b. this can limit the power of these tools in our approach. using human knowledge to paraphrase the demonstration can help these large models generate high-quality paraphrases but it is expensive."
178,"limitations of glosses and common datasets, as well as a standardized evaluation method (§2). in order to make future research on gloss translation more meaningful, we make practical recommendations for the field (§3). we urge researchers to spell out limitations of gloss translation approaches, e.g. in the now mandatory limitation sections of *acl papers, and to strengthen their findings by implementing existing best practices in mt. finally, we also caution that researchers should consider whether gloss translation is worthwhile, and if time and effort would be better spent on basic linguistic tools (such as segmentation, alignment or coreference resolution), creating training corpora or translation methods that do not rely on glosses. limitations our approach to surveying the research literature has limitations. firstly, some characterizations of the published works we survey are subjective. for example, it is somewhat subjective whether a paper “includes an adequate"
179,"limitations there is much variation in literary writing and narrative styles, and our work here deals with a small, curated subset of this domain. the novels we analyze are all in the english language, and were published between the early 19th and early 20th centuries. the authors and novels themselves are drawn from what is considered to be the established literary canon, and are not necessarily representative of all the works of that era, let alone literary works of other eras. the texts we analyze are largely uniform in narrative style. we limit ourselves to only those quotations that are explicitly indicated as such in the text by quotation marks, thereby eliminating more-complex styles such as free indirect discourse (brooke et al., 2016b) and stream-of-consciousness novels. we do not deal with nuances such as letters and diary entries nor quotations within quotations. the models we analyze for named entity recognition and coreference resolution use a fixed, binary formulation of the gender information conveyed by pronominal terms. though the development of fairer, more representative models is constrained by current datasets, we note that there is encouraging progress being made in this area (bamman et al., 2020; yoder et al., 2021)."
180,"limitations biases human data annotation for a sentimentrelated task, e.g., aspect-based sentiment analysis, hate speech detection, etc., involves some degree of subjectivity. while we included important quality control steps in the tbo annotation process, this intrinsic subjectivity will inevitably be present in tbo and learned by the models (see also the"
181,"limitations this work primarily focuses on evaluating the efficacy of existing continual learning (cl) methods for code generation models. it is important to note that many of these methods were specifically designed for natural language processing or computer vision domains and may not directly transfer to the code generation domain. nevertheless, we have made efforts to identify and address any issues encountered during our analysis. it should be acknowledged, however, that the scope of our work is limited by the selection of methods and the benchmark used. while we have utilized the most popular cl methods from various categories, there may be methods that have not been included in this study due to their inefficacy in natural language processing or computer vision tasks but may be effective in code generation. as such, we encourage further research within the community to explore the potential of cl methods for code-generation models."
182,"limitations like lots of deep learning algorithms, our work also needs gpu resources. in common learning problems, models will be trained once on the existing training datasets, using dev sets for tuning the models. then the trained model would be ready for use. in contrast, in active learning, we need to train the model several times (i.e., whenever new annotated samples are added to the current training set, the model should be re-trained), which increases the need for gpu resources. however, the need for gpu, is not related to our proposed method and it is due to the nature of active learning. in addition, one can run the active learning method once (rather than iteratively) for building an acceptable dataset. it should be noted that we have designed the algorithm in a way to be independent of the target language and utilized model. however, we only tested our method on egyptian arabic dialect and the accuracy of the model should be investigated on other languages and dialects using different learning models in further studies."
183,"limitations the benchmark for language identification for the most part contains clean sentences (grammatically correct, single script, etc.). data from the real world might be noisy (ungrammatical, mixed scripts, code-mixed, invalid characters, etc.). a better representative benchmark might be useful for such use cases. however, the use cases captured by this benchmark should suffice for the collection of clean monolingual corpora. this also represents a first step for many languages where no lid benchmark exists. the use of synthetic training data seems to create a gap in performance due to divergence in train/test data distributions. acquisition of original native romanized text and methods to generate better romanized text are needed. note that the romanized lid model does not support dogri since the indicxlit transliteration model does not support dogri. however, since dogri is written in the devanagari script using the transliterator for hindi which uses the same script might be a good approximation to generate synthetic training data. we will explore this in the future. this work is limited to the 22 languages listed in the 8th schedule of the indian constitution. further work is needed to extend the benchmark to many more widely used languages in india (which has about 30 languages with more than a million speakers)."
184,"limitations our work has several potential limitations. first, we determine the threshold γ by manual selection, which may limit the performance of seq2seq models, it will make our work more effective and elegant if we dynamically select the threshold. second, besides the improvement on three widely used tasks, we believe that there are still other abilities, like code generation, of seq2seq models that can be improved by our method, which are not fully explored in this work."
185,"limitations while we show that applying gap can result in a significant improvement in the generalization capability of lms, especially for dialogue tasks, we are only able to show 300 gap runs for each lm size in this work. we leave scaling the number of gap runs, as well as selecting specific text samples to perform gap on for future work. furthermore, a separate validation set of the tasks at interest are needed in order to choose the best checkpoint when performing gap. future work may look for other task-agonostic cues such as language modeling loss to determine the best checkpoint to use for inference."
186,"limitations our dataset and model only covers 201 languages: the ones we were able to test with the flores-200 evaluation benchmark. in addition, because our test set consists of sentences from a single domain (wiki articles), performance on this test set may not reflect how well our classifier works in other domains. future work could create a lid test set representative of web data where these classifiers are often applied. finally, most of the data was not audited by native speakers as would be ideal. future versions of this dataset should have more languages verified by native speakers, with a focus on the least resourced languages."
187,"limitations our results nor evaluation set cannot be used to indicate whether rte models trained for other languages are robust to paraphrases. however, researchers can apply the methods we used to develop p̂ arte to build evaluation sets in other languages to test whether non-english nlu systems are robust to paraphrases."
188,"limitations the proposed pll-word-l2r metric has the same practical limitations as previous ll/pll approaches. most importantly, these scores can be influenced by many superfluous factors, such as the number of available synonyms (computer vs. laptop; holtzman et al., 2021). we therefore expect our method to be most useful in highly controlled minimal pair or multiple choice setups. even more accurate metrics may emerge in the future. for instance, our approach pre-specifies the number of tokens in a word, thus limiting the space of possible alternatives. future approaches might investigate a way to normalize the pll score distribution over words with a varying number of tokens. further, it would be interesting to attempt to estimate the joint probability of all tokens in a word instead of predicting them left-to-right (as in pll-word-l2r) or without any other within-word contextual information (as in pll-whole-word). finally, we test our approach on english text corpora; our results might not generalize to agglutinative languages (due to a high number of tokens per word and, therefore, increased uncertainty) and are of less relevance to isolating languages (where, if enough training data are available, most wordlevel items can be represented as single tokens)."
189,"limitations rooted in inherent properties of general purpose models, preventing these to become holistic solutions to complex nlp problems without further research. we discussed opportunities provided by chatgpt and similar models to advance the development of specialized systems. with our insights and"
190,"limitations our paper has the following limitations: (1) in realworld applications, the label hierarchy may be more than two levels. it is worth extending our method to such a setting and empirically verifying it. (2) our selection strategy simply takes top r% confident samples, which might result in class imbalance problem. alleviating the imbalance problem may further improve our performance. we leave them as future work."
191,"limitations one limitation of this work is that we have included results for only eleven languages. training asr models, even on small datasets, requires significant computing and financial resources. second, there are not that many freely available and well prepared asr datasets that are readily compatible with all four asr architectures. we sought to select a diverse set of languages and datasets with varying features in order to provide, we hope, a reasonable snapshot of how the state of the art performs in low-resource settings."
192,"limitations for inter-parser comparisons. imagine a case where parser a claims to outperform parser b, and closer inspection reveals that a and b differ in their outputs on only two sentences. for one of these sentences, a’s output has a higher f₁ than does b’s; for the other, the opposite is true, but the difference in f₁ is smaller. now, the claim that a outperforms b becomes a claim that the parse that a produces for the first sentence is better than the parse that b produces for the second. and yet, as indicated by the judges’ disagreements in the second task, it is not always possible to make these kinds of judgements."
193,"limitations we find several limitations in this work. first, we acknowledge that the technical novelty of this work is limited: we introduce a sequence classification task, and we investigate rather standard models in our experiment section (i.e., state-of-the-art transformer language models). nevertheless, we believe that there is a gap in the literature for the task presented in this work, hence our introduction of the environmental claim detection task, the dataset, and models. second, we collect data from sustainability reports, earning calls, and annual reports. however, this does not cover the universe of text where environmental claims are made, e.g., company websites and product descriptions. also, environmental claims can be made about environmental improvements on a wide range of topics such as carbon emissions, water pollution, and recycling, among others. we discussed creating different datasets, where each dataset is dedicated to one specific is- 9task force on climate-related financial disclosures sue. however, we leave this to future work. third, sometimes it is necessary to have access to more context to determine whether a sentence is an environmental claim. we discussed whether it would be beneficial to annotate whole paragraphs instead. however, the trade-off would be exploding annotation work and costs, hence our decision to introduce environmental claims as a sentence-level classification task (and we specifically asked annotators to reject ambiguous cases as environmental claims). nevertheless, given a unlimited budget, we would have pursued annotating whole paragraphs instead (or annotating all environmental claims in a paragraph). our data sources, e.g., sustainability reports, are mostly published by european and us-listed companies, which is reflected in our dataset. we crawled these reports from the sec10, hence our dataset contains mostly claims made by (a) big firms and (b) firms from developed countries. it is conceivable that smaller firms and firms from nondeveloped countries make different environmental claims, and models trained on our dataset might not be suitable to detect these claims. moreover, our work is subject to all concerns raised in the"
194,"limitations although the experiment results have illustrated the effectiveness of the proposed imitation-demo method, we have to admit that our work has the following limitations: 1) this article is based on that the readers have some knowledge of prompt-based learning or demonstration learning. due to the space limitation, we can only briefly describe the basic process of the demonstration learning, which may make the article a bit obscure and difficult to follow. 2) imitation-demo does not achieve state-of-theart on all the datasets, but outperforms other strong baselines on 5 out of 14 datasets. besides, it consistently surpasses the demonstration learning-based baseline lm-bff. since imitation-demo is trained without introducing new parameters and explores the working principle of demonstration learning from a certain perspective, we believe the results are acceptable."
195,"limitations we highlight three main limitations of our work. first, although we have explored gradient-based explanations that take the whole network into consideration and have been shown to be faithful in previous work (bastings et al., 2021), we do not explicitly explore how comet combines the sentence representations in the feed-forward that precedes the encoder model to produce the sentence-level score. second, we have shown that combining attention with gradient information results in the best explanations for unite-based metrics. however, from a practical standpoint, running inference and extracting the explainability scores simultaneously may be more computationally expensive than other techniques: gradient-based metrics benefit from gpu infrastructure and require storing all gradient information. third, we have not explored extracting explanations in low-resource settings. that is because high-quality mqm annotations for such language pairs are not yet available. nevertheless, further research in those settings is needed to access the broader validity of our claims."
196,"limitations our method relies on having access to teacher embeddings and prediction which may not always be possible in a black-box distillation setting. retrieval augmentation also requires maintaining a knowledge base that is memory intensive. the cost of the retrieval process is dependent on the size of the training corpus, which can be a limitation when dealing with very large training datasets. conducting dataset distillation (wang et al., 2018b) on the training corpus to further reduce memory cost and retrieval time is an important future step for our framework. acknowledgments this work was done when jianyi zhang was an intern at amazon search. in addition, jianyi zhang and yiran chen disclose support from grants cns-2112562, iis-2140247, and cns-1822085. we thank yuchen bian for the valuable"
197,"limitations rerankner conducts calibration after the regular training, which introduces extra computational overhead. this drives us to further improve the overall efficiency of our method. recent works find that few-shot learning serves as an effective finetuning method of pretrained language models. it is reasonable to investigate our model under fewshot learning to reduce the overhead. although we get competitive results with the state-of-the-art methods, there is still a gap between the oracle score and the best results. we leave them as our future work."
198,"limitations the limitations of this work mostly come from our assumptions: 1) a randomly initialized and frozen tlm, and 2) input tokens are all different and randomly sampled. these two assumptions obviously do not hold true for human languages and pre-trained tlms. therefore, we attempted to empirically verify the existence of lemmas and properties on a pre-trained tlm without positional embeddings in §5. that being said, several methods could be attempted to remove these assumptions. firstly, we can analyze the training dynamics of a tlm to shed light on the model parameter distribution after pretraining. secondly, zipf’s law or a simple n-gram language model could be used to quantify the degree of input token duplication in human languages. this might give us a more accurate estimate of the variance at different positions. we leave these ideas as future work."
199,"limitations due to the lack of multi-intent text revision datasets, we only conduct experiments on iterater. although it is a multi-domain dataset, we only use its sentence-level data, and each sentence pair only contains one editing operation. the robustness of our method is still to be verified by evaluating it on more types of datasets in future work. another limitation of our work is that we only made improvements at the model level. we have noticed that kim et al. (2022) recently improved text revision by leveraging extra data from other text editing tasks and performing editable span detection before revising. similar methods can also be applied to our model and will be tried in our future work."
200,"limitations limited number of operators considered following previous methods (lan et al., 2021), we only consider binary operators (+, −, ×, and ÷). as we adopt a code-style output format, it is possible to introduce other non-binary operators supported by the python interpreter, e.g., sum() and max(). however, obtaining labeled data with such operators may require laborious efforts. we believe it is an interesting research question on exploring how to teach models to solve practical questions e.g., math word problems, by writing code in a low-resource setting (jie and lu, 2023). limited performance due to greedy decoding all the results we report in this work are produced via greedy decoding. a recent work (wang et al., 2023) reports that making large lms generate multiple answers and selecting the answer with the most votes can boost performance by a large margin. however, performing beam search for symbolic neural reasoners, e.g., deductreasoner, can be challenging in that searching space increases exponentially with the number of variables in the question (jie et al., 2022). designing effective beam search strategies for symbolic neural reasoners is a promising direction."
201,"limitations the proposed approach in this paper also suffers from certain limitations, i.e. we adapt apt on the encoder model and lack design for the other architectures such as decoder-only and encoder-decoder. in addition, it is better to generalize the key idea to other parameter-efficient learning approaches. a unified solution for existing work may be worth exploring in the future."
202,"limitations one limitation of this work is that vag does not achieve zero forgetting. although we show solving cil based on label generation can effectively ease forgetting and representation collapse of the pre-trained model, it is still interesting to further explore how to explicitly solve the forgetting issue in this new framework. the proposed techniques in vag are a step in the exploration. another limitation is that we directly use the label sequences provided by the original dataset. this may be suboptimal because the quality of the manually created label is hard to guarantee as it may fail to capture the semantic information of the samples in a class. a potential direction is to study creating label sequences automatically by summarizing the training samples. we leave this for future work."
203,"limitations the identified tendencies towards mentioning object-related features and the reliance on the shape as a contrastive feature might be driven by the grammatical structure of the annotations, mostly presenting object features in sentence-initial subject position, although 40% of exhaustive captions mention either the scale or the object color as the last word in the sentence. therefore, these results call for investigating the biases of model architectures less sensitive to sentence length than lstms, as well as extending the annotations with additional grammars. further, this evaluation provides descriptive results of the models’ pragmatic abilities, leaving the question of whether it is indeed a pragmatic inductive bias or, e.g., structural language drift (lazaridou et al., 2020) causing the observed patterns, unanswered. finally, since the evaluation pertains to the surface form of the predictions, applying decoding schemes other than greedy decoding used in this work might provide different patterns, indicating to which degree potential biases are due to model mechanics in opposition to sampling parameters."
204,"limitations we have demonstrated that an accurate description can perform better for both supervised and weakly supervised event detection. however, the event types from most existing ontologies are not properly defined. for example, in ace annotation guideline (linguistic data consortium, 2005), transfer-money is defined as “giving, receiving, borrowing, or lending money when it is not in the context of purchasing something”. however, it is hard for the model to interpret it accurately, especially the constraints “not in the context of purchasing something”. in addition, many event types from maven, e.g., achieve, award, and incident, are not associated with any definitions. a potential future research direction is to leverage mining-based approaches or state-of-the-art generators to automatically generate a comprehensive event type description based on various sources, such as annotation guidelines, example annotations, and external knowledge bases."
205,"limitations according to us there are 3 limitations of our work which will be addressed in future work. • the impact of layernorm, language tags, and residual connection settings on zst was analyzed in this study. however, other factors, such as the number of layers of the transformer model, may also have an effect and should be further investigated. • our"
206,"limitations in this work we propose an uncertainty-aware bootstrap learning framework for joint extraction. though it achieves state-of-the-art performance compared to other denoising techniques, unbed requires large training resources considering the ensemble loss calculated between two large plms and the probability variance calculated on the plm joint extraction model. in our future work, we hope to incorporate pruning techniques during training to improve the efficiency. we will also consider more complex relations between entities, e.g., relations beyond the sentence boundary, to fit in real-world information extraction scenarios."
207,"limitations actual applications of our model. our work assumes that input sql queries to our model are always wrong. this assumption is more feasible in an interactive semantic parsing framework, where the users are expected to decide whether a sql parse, accompanied by its natural language explanations (elgohary et al., 2020, 2021; narechania et al., 2021; mo et al., 2022), has errors or not. alternatively, to remove this assumption, it would be interesting for future work to study the performance of our error correction model in combination with an automatic error detection model (chen et al., 2023). experiments with more language models of code. we have only experimented with two language models of code, coditt5 and codet5, both using t5-base (raffel et al., 2020) as their underlying model architecture. it would be interesting to test how our"
208,"limitations we acknowledge the underlying assumptions of the social bias benchmarks used in our study. while the presented study aims to point out a key limitation of currently accepted methodologies, the presented investigation could benefit from more diversification. first, this study focuses on english. while we expect similar issues with similarly-constructed benchmarks in other languages, we leave it to future work to formally address the same. also, the bias benchmarks themselves imbibe the notion of fairness with the western value system (bhatt et al., 2022), and future explorations of benchmarks should diversify culturally as well. last but not least, we acknowledge the harm of binary treatment of genders in one of the target benchmarks. the purpose of this work was to bring light to a broader problem regarding the reliability of social benchmark metrics, with the hypothesis that the main idea of this paper would hold for a wider range of datasets with other assumptions or notions of fairness. we also acknowledge that there are larger models that we were not able to train and evaluate due to the limitations on our computational budget. the current study was focused on benchmarks with templated instances. this is no coincidence: the dominant majority of the social bias benchmarking literature relies on sentences with some degree of known structure, even in those collected from the wild (levy et al., 2021). such structural assumptions in datasets are necessary for defining and extracting quantifiable measures of social bias, which as we argue, are the reason behind the brittleness of their decisions. future work should focus on making our bias benchmarks more diverse and robust to small decisions that go into making them. broader impact bias evaluating benchmarks play a very significant role in helping identify potential risks of language technologies. while a large body of work evolves in this area of work, there is growing concern about the ability of the different benchmarks to accurately quantify and identify social biases. we emphasize these concerns by evaluating how robust the benchmarks are to alternate constructions based on simple linguistic properties. it is important to note how inaccurate measurements of social biases can be problematic by underestimating or misdiagnosing the potential harm from language models. we hope our work helps identify such pitfalls."
209,"limitations scope this short paper serves as an initial step toward peft for long-document models. as such, our evaluated scope of models, tasks, datasets, and kernel variations is limited. we acknowledge the need to experiment across broader settings and hope our work provides a foundation for others to build on. future experiments should analyze the validity and efficacy of using prefix-propagation with other long-sequence models to determine whether the prefix modality is suitable for non-sparse attention approximations. for example, would the projection of prefix vectors using a random feature map as in choromanski et al. (2020) result in an excessive loss of information for these critical tokens? regarding tasks and datasets, the performance degradation in prefix methods for wikihop deserves significant attention. verifying whether this extends to other reading comprehension and question-answering tasks will assist in guiding future research efforts. we restricted our research to the encoder-only version of longformer, but using the encoder-decoder version, led would enable analysis of sequence-to-sequence tasks. the scrolls benchmark (shaham et al., 2022) would be a good starting point for this analysis since it includes an led baseline. combining prefix and kernel methods is an ongoing research effort and there are several questions we plan to address: (1) what are the effects of swapping the default exponential kernel with other variants such as linear, polynomial, and rbf? (2) does making the α scale parameter trainable improve performance? (3) can we have a separate scale parameter for each query and should they be trainable? (4) is this approach effective for modalities other than long-document? (5) can we separate other components of attention into modular kernels (e.g. local and global kernels for sparse attention)? robustness the size and nature of long-sequence tasks often resulted in long run times for the larger datasets arxiv, 20-newsgroup and wikihop. consequently, we report results of one seed after doing a hyperparameter search for learning rate. this aligns with the reporting system of the original longformer paper (beltagy et al., 2020) but greater assurance in all long-sequence task performance could be achieved by accumulating results over several seeds. the size of datasets and iteration over several epochs somewhat mitigate this concern."
210,"limitations while we discover that simply applying cnn on top of the score matrix of span-based ner model performs well on the nested ner scenario, there are still some limitations that are worth discussing. firstly, we mainly choose three commonly used nested ner datasets, which may lack generalization. secondly, we only focus on nested ner tasks for the spatial relations between spans are more intuitive and common in nested scenario than those in flat ner. however, the principle of using cnn to model the relations is also applicable to spans in the flat ner task. future work can take flat ner into consideration based on our exploration, and experiments on more datasets."
211,"limitations non-projectivity. the primary theoretical limitation of hexatagger is that it can only produce projective dependency trees. we would like to explore the possibility of extending hexatagger to non-projective parsing for future work. interpretibility. as a trade-off for efficiency, hexatagger does not model dependency arcs directly. compared to graph-based models that explicitly score arc scores between pairs of words, it is more difficult to interpret the output of hexatagger."
212,"limitations because of the nature of our framework design, our work requires a diverse set of targets during training, which is important for target prediction and therefore the stance detection method. it is difficult to be applied to other stance detection datasets when there are limited training resources with regard to targets, such as conforti et al. (2020) and mohammad et al. (2016). besides, the model is trained on news-related debate corpus, so it may need further domain adaptation if applying the model to other domains such as social media. we are using an auto-regressive generation framework, which will also require extra inference time to generate the whole output sequence compared to the classification model. we would encourage readers to compare it with classification methods for efficiency when it will be applied in a time-sensitive scenario."
213,"limitations the major limitation of the present study is that the effectiveness of the proposed method has been confirmed only for a single task. this is because most existing reasoning tasks are relatively simple that they can be solved by a single external tool at most. for example, most existing numerical reasoning tasks provide self-contained questions; that is, all the required knowledge is included in the questions. in such tasks, a calculator is all that is needed as an external tool. however, it would be rare for a single external tool to be sufficient in real-world applications such as medical text analysis. it is crucial for future work to validate the effectiveness in such realistic scenarios that necessitate the use of multiple external tools."
214,"limitations we identify the following two limitations of our work: • different from raw text, constructing mrcstyle data from wikipedia requires the existence of hyperlinks. this idea works well for resource-rich languages, such as english and chinese. while such an idea is less effective for languages with few hyperlink annotations in wikipedia because a small amount of mrcstyle training data is difficult to guide the learning of nlu capability in those languages. a possible solution is to explore other data resources to automatically construct large-scale mrc data for pre-training. • as observed in table 1, the improvements of sequence classification tasks are less significant than those of span extraction tasks. we suggest that the existence of anchors is not a strong relevance indicator between our constructed query and context. such a finding is also observed in chang et al. (2020). therefore, constructing more relevant query-context pairs for sequence classification pre-training can possibly remedy this issue."
215,"limitations one limitation of current token-pair edit matrix based incomplete utterance rewriting models is that they are only able to select tokens that have appeared in the context utterances. thus, these models, including our own, are unable to generate new words, such as conjunctions and prepositions, to improve metrics such as fluency. however, this can be addressed by incorporating an additional word dictionary as proposed by liu et al. (2020) to improve fluency for out-of-vocabulary words (oov). in addition, we will consider combining generative models (gpt (radford et al., 2019), t5 (raffel et al., 2020) etc.) to assist in the recovery of the incomplete utterances in the future works."
216,"limitations one limitation of our method is that when training larger models, it requires more computation resources, whose cost is relatively high. however, after pre-training, we will release our models so that readers can directly use them without pre-training again. broader impacts we provide a new generative pre-trained model on molecules and text. on one hand, the model can be used to speed up scientific discovery, like molecule design, drug optimization, etc. on the other hand, once the model is trained on clinical data (which also describes the usage of drug molecules), it might lead to personal information leaky. we will enhance data filtration to anonymize all personal information, and will design new models to protect the information."
217,"limitation the main limitation of this work is the technical novelty of hybrid retriever. hyrbid-drboost is built on top of drboost, and the interpolation of bm25 with drboost. however, we would like to point out that our study can serve as an important finding for real-life applications. previous retrievers are built on top of indexing-heavy dense retrievers, such as dpr. this limits their applications where memory is a hard constraints, for example, on-devices. our study suggests that a light hybrid retriever can save memory but maintain sufficient performance."
218,"limitations we focus primarily on comparing model efficiencies using a variety of efficiency metrics and do not consider model performance; one can perform a more elaborate analysis of performance-efficiency tradeoffs, which we did not do here. we only profile a total of seven models across three modalities while there are more efficient variants and vanilla transformers proposed in the literature. while we choose our models to be as representative of each modality and efficiency technique as possible, we cannot extrapolate results to other model variants and other modalities. in particular, modalities like video and genomics and efficiency approaches like quantization would be interesting to profile, which we did not do."
219,"limitations the design of the dynamic templates requires knowledge of the event ontology and is timeconsuming. the authors of the paper spent 30 hours designing the exclusive templates that cover all of the possible argument combinations for each argument role in ace ontology. with a more complicated ontology, a much larger amount of time is required. another limitation of our approach is the offset retrieval method. if one sentence contains multiple mentions of the same entities, or even multiple text strings that have the same spellings but refer to different entities, the qga-ee model always retrieves the position where the mention appears for the first time in the sentence as the offset of the extracted target. it may be improved by asking the model to generate contextual text as a position reference."
220,"limitations we address several limitations with regard to our work. first, the publicly available datasets used in our experiments are limited to english. documents in different languages (i.e., chinese) might require different segmentation techniques and may contain unique characteristics in terms of vocabulary size, data sparsity, and ambiguity. secondly, we only evaluate the quality of the topic models in terms of coherence and diversity. future work should explore how our method impacts other characteristics, such as document coverage (i.e., how well documents match their assigned topics) and topic model comprehensiveness (i.e., how thoroughly the model covers the topics appearing in the corpus)."
221,"limitations due to our budget constraint, we only performed pretraining and downstream experiments with basesized transformer models. we also only applied the masked language modeling objective, but there are other effective pretraining objectives (e.g., clark et al., 2020). nonetheless, since we introduced minimal changes in architecture, we hope that subsequent work will benefit from our narrowing operations and conduct a wider range of pretraining and downstream experiments. while pretrained models can be applied to even more downstream tasks, we designed a reasonable task suite in this work, consisting of both glue sentence classification and the conll ner sequential classification tasks."
222,"limitations since the multi-hop texttableqa problem has only one dataset hybridqa, our model has experimented on only one dataset. this may lead to a lack of generalizability of our model. transparency and interpretability are important in multi-hop question answering. while our model achieves the best results, the model does not fully predict the reasoning path explicitly and can only predict the row-level path and passage-level path. in future work, we will design more interpretable texttableqa models."
223,"limitations our limitations are as follow: • data scale: this paper only employ the wikipedia of wizard dataset, a small scale and well-established knowledge conversation dataset, and lack of the validation on largescale dataset. • backbones: this paper lacks the evaluating of other knowledge dialogue model on the proposed method. actually, we have two reasons to employ the plato. first, the plato can better handle the one-to-many phenomenon, which is suitable for learning our expansion samples. second, the plato is a pre-trained dialogue model, and its performance on knowledge dialogue generation task has been proved. we will evaluating the performance of other knowledge dialogue model on our method for our future work. • knowledge expansion methods: this paper only use the synonym and antonym to construct the noised knowledge, which lacks of the comparison of using other data augment method. indeed, we use two tokenlevel data augmentation methods (synonym and antonym augmentation) to prove our statements on hallucination problem in knowledgedialogue generation task. based on this study, we believe that incorporating other data augmentation methods will also mitigate the hallucinations. • manual prompts and responses: this paper designed five prefix prompts, four post-prompts and nineteen euphemistic responses. for ak-more method, we simply randomly choose one prefix-prompt and one post-prompt and concatenate them with the ground-truth response. this leads to some irregular responses. as for ck method, we randomly select one euphemistic response for the incorrect knowledge. however, we found that the response may not coherent with the query. we will design more smooth expansion ways to construct more human-like training samples for our future work."
224,"limitations in this paper, we propose a method named autoconv, which means automatically generating information-seeking conversations with large language models (llm). though it has achieved great performance on both quac (choi et al., 2018) and coqa (reddy et al., 2019), there are still some limitations that should be noticed. limitation of llm. in our experiments, we use opt-13b (zhang et al., 2022) as the llm for generating synthetic conversations due to the limited computational resources. larger models should be considered to further understand the potential ability of autoconv, e.g., gpt-3 (brown et al., 2020), opt-175b (zhang et al., 2022), bloom-176b (scao et al., 2022), and glm-130b (zeng et al., 2022) etc. limitation of implementation. as mentioned in section 2.2 and appendix b, our method needs to finetune llm and generate massive synthetic conversations based on the finetuned llm, which has a high cost for implementation. limitation of synthetic dialogues. as shown in table 2 and section 3.8, there is still a gap between our synthetic dialogues and human dialogues. it is important to improve the quality of synthetic dialogues so that we can further alleviate the dependence on human annotation."
225,"limitations the corpus and therefore also the asr baseline model only cover read speech. we have not tested the model on spontaneous speech, but we expect it to perform significantly worse on this type of data. our data collection process for swiss german speech with standard german transcripts is designed to collect large amounts of data in a costefficient manner. we estimate costs to be 4 to 6 times lower compared to the transcription of existing recordings. however, there is a downside to our approach. because it is based on a given standard german sentence, it can lead to swiss german speech that’s closer to standard german than the swiss german encountered in everyday conversations. the severity of the shift towards standard german depends on the individual speakers and their ability and effort to produce swiss german representations that are close to how they would speak in everyday conversations. while we made every effort to include as many different dialects as possible in the corpus, there are still strong dialects with a comparatively low german-speaking population that are insufficiently or not at all represented, e.g. some dialects from the canton of fribourg. this is due to the huge dialect diversity in switzerland. the gender ratio is not balanced for some dialect regions in the test set, especially not for vs, where the test set is female-only because we did not succeed to recruit any male speakers from this region during phase 1 of the data collection. however, preliminary experiments do not show a significant difference between genders in swiss german asr performance, so we do not expect this to lead to skewed results. our asr baseline model and other models trained on the corpus may perform below average for children and people above seventy due to the lack of training data for these age groups. ethical considerations participants were specifically recruited to record swiss german speech for this corpus. the purpose of the recordings was made clear at recruiting time: a training corpus for swiss german asr models. participants were also informed at recruiting time that information about their dialect, age, and gender will be collected. furthermore, to be able to participate, they had to read and accept our data privacy policy which further detailed the future use of collected data."
226,"limitations, perform poorly as a standalone model for long-tail classification. these results can be improved by priming the model with an entailment predictor through the usage of a prompt. the baseline shows strong performance independent of the llm, as it operates on a closed label space. the capabilities of the baseline can be enhanced by further explicitly priming it with a entailment relation through a llm. rows in which t0pp is initialized, or primed with e are indicated with primed. priming the model showcases improvements across all datasets for macro f1. for accuracy, priming the model shows benefit in two out of three datasets. in figure 4, we show the results of top-5 predictions for the wos dataset. 1we observe a significant drop in performance when we utilize the 3b parameter variant of this model as l. all results are aggregated in table 1. it is important to highlight that prompt variation led to stable results for our llm. the variance upon utilizing bart-mnli is negligible across prompts. the best results are observed upto top-4 predictions on both accuracy and macro f1 for our method, when the entailment prompt is enhanced with a greater number of tokens corresponding to the output of l(e(x)). the variation between our method and the baseline is much greater for top-1 predictions, but top-5 prediction variance is negligible. detailed results for both depth settings of amazon beauty are shown in appendix c."
227,"limitations in this work, we implicitly utilize the contradiction relation. the authors recognize explicitly including it in a prompt template leads to worse performance due to the injection of noise. controlled template generation based on a model confidence is unexplored in this work and appears to be a promising direction. additionally, we recognize the emergence of parameter-efficient methods for training models which are unexplored in this work, which may have utility. these methods are complimentary and may benefit the performance of models as they can be used in conjunction with training paradigms such as contrastive learning to support better representations through explicit utilization of the contradiction relation. in this work, we limit our study to draw attention to the importance of strict zero-shot classification settings with the emergence of llms. our study can be easily extended to recursively operate on large language models, and entailment predictors. as we observe limited performance benefits in doing so, we conduct our study to show improvements after one complete cycle, given by e(l(e(x)) in section 3."
228,"limitations a lot of recent work especially in computer vision has leveraged the unsupervised methods or unpaired multi-modality data to pre-trained crossmodal language model. applying the same idea into speech language model is also discussed in some recent research works. to compare fairly with previous works in st area, we do not build our model on top of such frameworks and discuss how to utilize the raw audio. in terms of the model training, multi-tasks may affect each other due to uneven data distribution, and we have just scratched the surface of this part of the analysis."
229,"limitations one of the limitations of our survey is that it covers a limited sample space of 15 papers from emnlp 2020 and acl 2020. while a larger sample would be helpful in gathering more evidence, access to specific tracks is limited at nlp conferences, unless hosted online via a virtual or hybrid system. with respect to our case study, we evaluate on the asr utterances, but with labels corresponding to the original manual transcriptions. for a perfect comparison, the asr utterances would need to be re-annotated as the talk move could change based on the severity of transcription errors."
230,"limitations in this work, we do not propose any new methods because, as an opinion paper, we focus on raising the problems and making vivid demonstrations to readers. the experiments are limited to linear svm and bert on data sets in the benchmark lexglue. we hope that, within the page limit, our experiments sufficiently convey the points to readers."
231,"Our analysis and conclusions have been based only on a single translation direction (German to English), a single dataset, and a single transformerbased model. The generalization to other languages, data and models is yet to be verified.
Even in this setup, we have seen that some of the proposed methods are very good at detecting fully detached hallucinations. However, none of them were able to well separate strongly detached hallucinations (when only a part of the generated translation is unrelated to the source) from correct translations. Perhaps, such partial hallucinations should be detected on the level of individual tokens instead of the whole sentence.
One of the metrics that we propose, average ALTI source contribution, has an advantage of not requiring any external models except the translation model itself. However, the two best detection metrics (based on LaBSE and on XNLI model) re-
quire additional encoders trained on the source and target languages, which limits their applicability for lower-resourced languages or in the settings with limited computational resources.
Being an internal method is an advantage of ALTI, but it is also a limitation: this method is suitable only for transformer-based translation models. In principle, it can be adapted to other neural architectures, but not to non-neural approaches, such as statistical machine translation."
232,Section 7 (after conclusions)
233,"Despite the promising results obtained in our model, there are still several areas for improvement. Firstly, when dealing with a large corpus, the online retrieval function becomes challenging as
it requires a significant amount of computational resources and time. Additionally, creating a vectorized corpus dynamically every time becomes difficult. Secondly, the process of collecting a large number of reviews from users raises privacy concerns. The collection of data, especially from private and non-public sources, may pose difficulties."
234,"We conduct experiments on public datasets of finite sentence length, while generalizability to extremely long sequences or even streaming data has not been verified. Furthermore, the generalizability of the proposed quantization method to other tasks, including computer vision or speech recognition, remains to be tested. In addition, binarization and ternarization require bit-packing to have actual memory savings and dedicated hardware support for real-time acceleration, which is more of a hardware implementation aspect and not studied in this paper."
235,"We discussed the limitations of our work in the unnumbered limitations section.
7 A2. Did you discuss any potential risks of your work? We only used publically available datasets that are commonly used in research on dialogue systems. We believe there are no significant risks associated with our work."
236,"We discuss limitations of our work that hopefully could inspire future research in this avenue. Task Coverage in ARCADE ARCADE consists of realistic data wrangling and EDA tasks for a variety of ML datasets. In particular, we focus on problems that can be solved using pandas because of its popularity in data science — 90% of Kaggle notebooks use pandas. Still, our annotated problems may not cover all the types of tasks in these two categories. As an example, data visualization is an important part of EDA. Our dataset also includes 59 natural language to plotting problems, which are not used in this paper due to challenges in automated evaluation (Chen et al., 2021b). Future work might consider evaluation of plotting tasks using unit tests (Lai et al., 2022). Additionally, some of the existing datasets in Tab. 1 usually contain broader types of problems other than the wrangling and EDA tasks considered in this paper (e.g., fitting ML models, §7). We leave expanding the task spectrum as important future work. Session-level Evaluation ARCADE features multiple contextually dependent problems in computational notebooks. As the first step towards evaluating code LMs in this interactive program synthesis paradigm, we report turn-level accuracy, and generate notebook context for prompting using ground-truth solutions for the prior turns of a problem (§5.1), following the common evaluation protocol in task-oriented dialogue (Hosseini-Asl et al., 2020; Andreas et al., 2020). Future work could consider a more realistic scenario of session-level evaluation where history contexts consist of model-predicted code instead of the reference (Yang et al., 2020; Nijkamp et al., 2022). However, this evaluation setting is still not ideal without modeling the user (e.g., asking follow-up questions to correct a model’s predictions in a turn before proceeding to the next round, see Austin et al., 2021), which often requires building specialized simulators (Cheng et al., 2022a). Reliance on Large Language Models Our experiments are based on public and in-house large code LMs (PACHINCO), which require adequate computational resources9 and create carbon emissions (Patterson et al., 2021). Their predictions could also be subject to known issues such as misalignment with user intents; for a discussion
9FLOPs usage of fine-tuning PACHINCO is 3.6× 1022.
of these and other risks of code language models, see Chen et al. (2021a, Appendices E-H) and Chowdhery et al. (2022, Section 6.4). To reduce the amount of computational resources required, our initial prompting experiments (§5.2) and error analysis (Appendix J) suggest that leveraging program execution information (e.g., schema descriptions) could be a promising direction to improve sample efficiency and reduce the size of code LMs (Nye et al., 2021), while explicit modeling of code-intent correspondence (Zhang et al., 2022) could be a viable path to mitigate alignment issues in model predictions. In addition, as generative AI coding tools are becoming more available to developers, more efforts are required to understand the potential limitations of those systems and the risks they may pose, such as producing insecure code and over-reliance on model predictions (Chen et al., 2021a). We leave addressing those issues as important future work."
237,"Although our NAR approach can generate fluent and meaningful text, it inevitably suffers from the typical generation problems like in the AR fashion: (1) off-prompt: the provided prompt is very short, which causes the model can not focus on meaningful content and generate reasonable text. Besides, the model usually simply copy prompt text to generate results instead of planning reasonable content, such as the case 3 as shown in Table 13 in Appendix D. (2) incoherent between sentences: When the model is initialized, it does not consider the logical order between sentences, so it can only rely on the training data to learn automatically. We will consider how to generate a suitable initialization to help the model generate coherence results. Our paper’s primary concern focuses on accelerating the generation speed, and we will put how to solve these problems in future work.
Ethics Statement
Our method heavily relies on the pre-trained language models, e.g., RoBERTa, which may inherit the problematic biases (Radford et al.). We have
attempted to mitigate these issues by conducting experiments on comparatively innocuous story generation and opinion generation tasks. Furthermore, we have replaced all the names in those corpora with special placeholders. Although some measures are taken to mitigate the problematic biases, such issues cannot be solved completely. Thus, we urge the users to carefully examine the generation results and cautiously apply our method in real-world applications. Additionally, it is worth noting that all the corpora used in our experiments are only for scientific research.
As for the human evaluation process, we resort to open source web library Django|| to build our own human evaluation interface. Before releasing the human evaluation cases, we carefully check that there is no private information or other problematic biases in the cases. Besides, we did not collect personal information or ask the annotators about their private information during the annotation process. We hired three annotators and paid each of them $0.29 for each case comparison. The payment is reasonable since there are only 100 cases for annotation, and it would cost average 4 hours for one to finish all the comparisons."
238,"Firstly, the training process of our proposed model is dependent on supervised data, thus precluding its application to languages without a supervised dataset for CCG.
Also, the span-based parsing algorithm proposed in this study is implemented in Python and may take a considerable amount of time to parse extremely long sentences (more than 100 words) due to a lack of optimization for implementation."
239,"Section Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
240,Section 9
241,"The high computational complexity is one of the biggest disadvantages of the path aggregation. The time consumption and GPU memory used for multiple operations are expensive. So it is very desirable to use only one time of path aggregation due to attributes of the ABSA task in our APARN.
Another limitation of this work is that the performance of the model is still somewhat affected by the quality of the AMR parsing results. The good news is that the research on AMR parsing is continuing to make progress. In the future, APARN with higher quality AMRs is expected to further improve the level of the ABSA task.
Besides, this model is flawed in dealing with implicit and ambiguous sentiments in sentences. Implicit sentiment lacks corresponding opinion words, and ambiguous sentiment is subtle and not apparent. An example of this is the sentence ""There was only one [waiter] for the whole restaurant upstairs,"" which has an ambiguous sentiment associated with the aspect word ""waiter"". The golden label is ""Neutral"", but our model predicts it as ""Negative"".
Finally, generalization to other ABSA tasks such
as end-to-end ABSA or ASTE is another restriction. Considering the complexity of the task, we only apply our motivation to sentiment classification in this paper. We will further generalize it to more complex sentiment analysis tasks in the future work."
242,"Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
243,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
244,"In this section, we discuss some of the limitations of the Rule by Example method."
245,"Section ""Limitations"" (after conclusion)
7 A2. Did you discuss any potential risks of your work? We analyze the current state of identity inclusion in MT. Thus, our work points to risks of such systems."
246,7: Limitations
247,"In this paper, we explore incorporating multiple constraints to simile generation and attempt to interpret the simile comparisons from the aspect of Cognitive Linguistics. However, the creativity of simile is one kind of subjective feeling and is difficult to be accurately judged, which is also a big challenge for other kinds of creative writing tasks. We hope this task and dataset could provide novel insight into user-oriented text generation, and give the interactive and collaborative generation a closer and more detailed exploration."
248,"There are two limitations of this study that could be addressed in future research. First, this study focuses solely on the ED task. In the future, we seek to extend it to the overall event extraction (EE) task, which also includes the event argument extraction task, where a complete annotation is more challenging than in ED. Second, our study models the partially labeled training data instead of annotators. Indeed, the annotators produce the data, so building a model for annotators may be an essential way to address the partial learning problem. For example, an annotator may be more careless than others and generate more noisy data. Consequently, a robust model for the task should give a lower belief in the data of this annotator to improve learning. Lastly, our research raises no ethical issues because it focuses solely on the technical aspects of a normal information extraction problem."
249,Section 8
250,"In this work, we limit ourselves to object-centric grounding, which ignored that language can ground events, attributes, manners, mental states, etc. The grounded meaning of some groundable words, especially ADVs, NUMs, VERBs, and PRONs, cannot be fully captured by the bounding boxes alone. Future work should explore better task formulations to study the acquisition of their grounded meanings. An exciting future work along this line is to extend the setting from images to videos and physical interactions with the environment, and to incorporate the rich temporal dynamics of the world for language acquisition. In addition, we ignored the social aspects of language learning, where children infer the referents of words from their caregivers through communication (Carpenter et al., 1998; Bloom, 2000). Future work could also investigate grounded word acquisition from natural dialogue."
251,"Section ""Limitations""."
252,"Our implementation of proxy models applies those models after the whole data is generated. Due to this, in the resulting dataset, the number of instances can often be unbalanced between labels. Such a limitation might be addressable by training proxy models from intermediate datasets with a smaller number of instances, and using those models while generating the rest of the dataset. As the data become unbalanced during the generation,
the generation pipeline can try to generate more instances with labels that are a minority in the intermediate dataset. However, when we piloted this approach, we identified potential problems. First, intermediately trained proxy models could perform worse than those trained after all data are generated, due to the lower diversity in intermediate data used to train proxy models. Second, if many data points generated with a specific label (label a) actually belong to another label (label b), there can be cases where most instances of label b come from the prompt with label a. It can skew the linguistic patterns of instances within the dataset, as only a small number of texts for label b might have been from the prompt with label b. Advanced approaches to address these issues can be future work directions.
Our implementation of efficient OOSF was not effective in increasing model accuracy. It might be due to the negative impact of removing instances, such as filtering instances on the decision boundary. As our study of OOSF was not complete, future work is necessary. Applying OOSF to the entire generated dataset and seeing the impact of their removal would be the first step. With a comprehensible understanding of OOSF, we would be able to design better OOSF strategies, such as filtering instances with various criteria.
In this work, we only examined the text-davinci-002 model of GPT-3. Although we believe that the overall trends of results would be similar for other models, examining other models with our approaches is a necessary future work. We also examined only one prompt (Prompt A), while there may be other options. In Appendix F, we present partial results on using another prompt, showing that our approach is generalizable to other prompts. Combining human interventions with automatic annotation error detection (Klie et al., 2023) can be another future direction."
253,Section 8. Limitations
254,"Like all unstructured pruning methods, SMP is hard to achieve inference speedup compared to structured pruning methods. Since SMP prunes model without fine-tuning, this also limits the extension of SMP to structured pruning methods. However, we find that most rows of the sparsity matrices in SMP are completely pruned at high sparsity level. This allows us to directly compress the size of matrices, resulting in faster inference. For example, the 3% remaining weights model of MNLI can be compressed to 47.43% of the model actual size (resulting in around 1.37× inference speedup) without retraining or performance loss. By removing rows of matrices that contain less than 10 remaining weights, we can further compress it to 25.19% actual size (1.76× inference speedup) with 0.9 accuracy drop. We expect that a carefully designed loss function during training could result in even smaller actual model size and faster inference speedup, which we leave it in the future."
255,"Last section in page 9 (unnumber)
7 A2. Did you discuss any potential risks of your work? Work doesn’t have immediate ethical risk"
256,"While Sec. 3 characterizes the effect of general Lhomomorphisms, LEXSYM specifically produces single-token swaps. In images represented as discrete symbol sequences, if a single symbol simultaneously encodes multiple visual features (e.g. color and texture), these features will remain entangled in synthesized examples. It will not exchange substructures larger than a single token, and thus will not synthesize examples longer than those already present in the training set (Lake et al., 2019). This is because LEXSYM targets compositionality but not recursion, which is also required to model the full range of human-like generalizations in sequence learning problems.
LEXSYM is also sensitive to the nature of the tokenization scheme itself. In morphologically rich languages, for example, LEXSYM may need to be applied not on top of words or segments, but instead canonicalized morphemes produced by learned morphological analyzers (Narasimhan et al., 2015; Bergmanis and Goldwater, 2017; Cotterell and Schütze, 2018) (analogous to the use of learned image patch representations rather than pixels in our VQA experiments).
Finally, LEXSYM does not induce some of the generalizations obtained other methods for improv-
ing compositional generalization, especially those that exploit extra structure (e.g. tree-shaped inputs and outputs) in the semantic parsing domain (e.g. Liu et al., 2021a). It might serve as a platform for future versions of those methods that offer greater generality and formal guarantees."
257,8 (Limitations)
258,The limitation section is after the conclusion part of the thesis.
259,"The New Yorker Cartoon Caption Contest represents a narrow slice of humor, deriving from a particular language, region, history, culture, style, and set of conventions. Hence, the results of this study do not represent or cover all types of humor.
Our framing of the quality ranking task could be interpreted as seemingly prescriptive (i.e., that joke A is “objectively” better than joke B), but New Yorker editorial selections should not be taken as ground truth for funniness; disagreement about what is funny is expected and valid. Our tasks operationalize the prediction of only average preferences (rather than individual ones), and these preferences may include a partiality or bias towards items that conform to the characteristics of prior contest winners or published New Yorker cartoons.
Finally, the explanations in our annotated corpus were largely written by a single author of this paper. While a larger pool of the crowdworkers judged these explanations to be of higher quality in comparison to machine generations, future work would be well-suited to compare the person-toperson variance in explaining why particular jokes are funny.
16Or never. Is never good for you?"
260,"6, Limitations"
261,"Lexical variation is not our focus because it is not well-described by systematic, scalable, and generalizable rules. One can derive lexical distributions from data, but many low-resource dialects lack corpora on which to base these insights. This is an important problem for future research.
Multi-VALUE’s strength is its extensive coverage of English morphosyntacic patterns that have been documented in eWAVE by over 80 linguists. Such comprehensive resources are not available for other languages, but we encourage continued collaborations between computer scientists and linguists to build these resources for dialect-robust NLP systems across languages. As it stands, the current iteration of Multi-VALUE provides global value by serving a global contact language, English, and its 50 most documented varieties.
Despite the scope and precision of eWAVE for
English, its catalog ultimately derives from linguists’ oral interviews with native speakers, and here we can identify some additional limitations. First, the orthographic conventions that linguists use to encode spoken dialect may not always align with the speakers’ own writing conventions and usage. Second, our approach can only cover the variation that linguists observe frequently enough to document, and in canonical forms in which they are documented. This means we may not fully capture variation within each feature.
Finally, dialects should not be treated like deterministic speech patterns, but rather like a range of grammatical options or switches that may be turned on and off and adjusted for frequency in various social and personal contexts. Dialects do not always fit into nicely prescribed categories."
262,Section 8
263,"Perhaps the most important limitation regarding ColD Fusion is its deployment. This paper presents a method for multitasking, not a platform. In that sense it solves both multitask learning goals under the constraints resulting from collaboration. However, using ColD Fusion in practice might require much more effort – It would require a place to host the models, a way to make sure no malicious or erroneous model was sent, and other aspects of a platform to support this training.
This is the first method to tackle collaborative multitasking and we scaled it to 35 datasets. However, future methods may be found more efficient or scale better with the amount of data and computation.
ColD Fusion with many iterations and models might require more computational effort for a given amount of data (§6) than regular multitask learning. As a result, while our bottom line performance is encouraging, ColD Fusion might not be the preferred way under every possible scenario. Still, some of the costs may be alleviated by future work – for example the additional iterations when fusing many models, might be reduced by aligning models’ weights before fusing (Ainsworth et al., 2022).
While this paper studied the impact of various ColD Fusion parameters, it is unclear how finetuning or even pretraining parameters affect results. However, we do have a reason to believe the method is relatively robust to these refactors through our initial results and the replication on another architecture (App. §D).
Another limitation is the assumption that the weights of the model change. Some adaptation methods assume the model is frozen and only its inputs change. In those cases, the model would
not be improved by use. Still, even in such cases, multitask learning (Wang et al., 2023) might be applied on the inputs, or the same model might be used in different ways, where some also adapt parts of it (Hu et al.; Jang et al., 2023; Qin et al., 2022; Yadav et al., 2023). In those cases, the method might still prove useful, even if it benefits only from some of the contributions.
As mentioned before, another concern is a possible harmful update done by a contributor. Handling it would require monitoring the updates by regularly evaluating the model, or measuring the updates diff to identify noisy models (too large diff / random weights)."
264,"In this work, we present a general masking scheme for multilingual MLM pre-training on multiple monolingual corpora. Experiments show that our method can work for similar languages (including low-resource and high-resource ones) and dissimilar languages. However, we only experiment with dissimilar language Ne. More experiments are required for dissimilar and distant languages.
When computing [C]x for more than 3 languages, to avoid cross-lingual bias, we adapt our method to a pivoting-based framework, using En as a pivot or anchor point. Although we show this framework can work for cross-lingual classification tasks, this could be a potential problem for further adaptation to other multilingual tasks, which requires further
experiments. Intuitively, we can compute [C]x in random languages instead of only in En with a balanced sample strategy.
Our method provides a general framework to leverage cross-lingual prototypes for multilingual MLM pre-training, but the scope of the study is limited. We believe there are some other solutions. For instance, we can leverage linguistic varieties for masking, but the question is how to obtain linguistic varieties without using parallel corpora. Perhaps, we can consider word frequencies because Zipf’s law indicates that words appear with different frequencies, and one may suggest similar meaning words appear with relatively similar frequencies in a pair of languages. Most importantly, solutions should further consider morphological variations, since in this paper we prove morphological variations are significantly beneficial."
265,Section 7: Limitation after the Section 6: Conclusion and future work
266,"We discuss four limitations of our work: the inclusion of unsafe content, potential biases in data
sources, a limited measure of image quality and generalizability to different generative models.
• Inclusion of unsafe images and prompts. We collect images and their prompts from the Stable Diffusion Discord server (§ 2). The Discord server has rules against users generating or sharing harmful or NSFW (not suitable for work, such as sexual and violent content) images. The Stable Diffusion model used in the server also has an NSFW filter that blurs the generated images if it detects NSFW content. However, we observe that DIFFUSIONDB includes some NSFW images that were not detected by the NSFW filter or removed by the server moderators. To mitigate the potential harm, we compute and share the likelihood of an image or a prompt containing unsafe content using the state-of-theart NSFW detectors (§ 2.3). In addition, we provide a Google Form on the DIFFUSIONDB website where users can report harmful or inappropriate images and prompts. We will closely monitor this form and remove reported images and prompts from DIFFUSIONDB.
• Potential biases of the data source. The 14 million images in DIFFUSIONDB have diverse styles and categories. However, Discord can be a biased data source. Our images come from channels where early users could use a bot to use Stable Diffusion before release. As these users had started using Stable Diffusion before the model was public, we hypothesize that they are AI art enthusiasts and are likely to have experience with other text-to-image generative models. Therefore, the prompting style in DIFFUSIONDB might not represent novice users. Similarly, the prompts in DIFFUSIONDB might not generalize to domains that require specific knowledge, such as medical images (Chambon et al., 2022).
• Limited measure of image quality. We use joint text-image CLIP embeddings between prompts and images to detect generation misalignments (§ 3.5). While the CLIP embedding distance can indicate the degree of alignment between the prompts and generated images, it does not provide a measure of the overall image quality. When constructing our dataset, we have considered including image properties such as entropy, variance, and the most common colors to help users gauge image qualities. However, these metrics do not provide a good measure of the overall image quality as well. To better mea-
sure image quality, future researchers can recruit annotators to rate images in DIFFUSIONDB.
• Generalizability. Previous research has shown a prompt that works well on one generative model might not give the optimal result when used in other models (Borji, 2022). Therefore, different models can need users to write different prompts. For example, many Stable Diffusion prompts use commas to separate keywords, while this pattern is less seen in prompts for DALL-E 2 (Ramesh et al., 2022) or Midjourney (Holz, 2022). Thus, we caution researchers that some research findings from DIFFUSIONDB might not be generalizable to other text-to-image generative models."
267,"The last section (unnumbered), immediately following the conclusion
7 A2. Did you discuss any potential risks of your work? We carefully reviewed the guidelines and could not think of potential risks worth mentioning in the paper."
268,"Models are often developed with specific datasets in mind. Some papers introducing new models also introduce new training sets such as CopyAttention (Cui et al., 2018), SpanOIE (Zhan and Zhao, 2020), and IMoJIE (Kolluru et al., 2020b) which may influence model assumptions. SpanOIE also introduces its own manually annotated benchmark, which may have informed the assumptions SpanOIE makes. The lack of consensus on how to label OpenIE makes it difficult to perform apples-to-apples comparisons because certain models can not extract some relations due to the assumptions they make.
OpenIE has also largely been limited to English. MILIE makes assumptions that allow for different extraction methods depending on the language, but other OpenIE models that support multilingual extraction largely treat extraction from other languages the same as extraction from English. Multilingual OpenIE remains an open field of study."
269,"We have Limitations Section at the end of the paper after Conclusion
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
270,"Although our model achieves good performance in solving the compositional and zero-shot generalization problems, there is still room for improvement on the i.i.d datasets. The fine-grained module in our framework cannot take advantage of explicit composition information when the component compositions in the testing set and training set significantly overlapp. For example, in Freebase, ""Who is the coach of FC Barcelona?"" is answered by the join of relation “sports.sports_team.coaches” and “sports.sports_team_coach_tenure.coach”. Our fine-grained extractor may fail to recall “sports.sports_team_coach_tenure.coach” and instead select “base.american_football.football_coac -h.coach” as the candidate since ‘football coach” is more relevant to the question than “coach tenure” in semantics. The only coarse-grained model, however, can directly memorize the pattern because such composition appears frequently in the training data. Therefore, compared to conventional models that completely memorize composition patterns, our model may only have minor advantages.
Another limitation is that we cannot guarantee the generalization on other KBs such as WikiData because gaps between KBs may bring negative impact. For example, relations in Freebase are often more specific
(ice_hockey.hockey_player.hockey_position, soccer.football_player.position_s), while relations in Wikidata are more general (position_played_on_team). We consider it as a direction for our future work."
271,Section 7
272,
273,
274,"ALERT aims to encompass a wide range of reasoning skills, but some reasoning skills are missing, specifically in regards to symbolic reasoning (last letter concatenation task and coin flip (Wei et al., 2022)) and compositionality reasoning (SCAN (Lake and Baroni, 2018), COGS (Kim and Linzen, 2020) and CFQ (Keysers et al., 2019)). These reasoning skills should be included in future work.
In terms of computing power, we have experimented with models that were accessible to us. We acknowledge that there are larger models that we were not able to train due to the limitations of our computational budget.
During our analysis, we discovered that some datasets contain noise, where even human experts are unable to provide accurate answers for certain instances. While it is important to address this issue, it is a time-consuming process to carefully review and clean each instance in the dataset. We plan to address this in future work."
275,section ’Limitation’
276,"Limitations Section.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
277,"Considerations and Limitations section
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
278,"Our model may have several limitations: (1) As a memory-based model, our model consumes additional space to store typical samples and static prototypes, which causes the performance to be influenced by the storage capacity. (2) Although we propose memory-insensitive relation prototypes and memory augmentation, our model still relies on the selection of typical samples. The selected samples of low quality may harm the performance of our model. (3) The recent progress in large language models may alleviate catastrophic forgetting and overfitting, which has not been explored in this paper yet."
279,"We discussed the limitations of work in section 7 of the paper.
7 A2. Did you discuss any potential risks of your work? Our work does not have any immediate risks as it is related to improving pretraining techniques for code-switched NLU."
280,"Our theory currently assumes that input speech features are quantized into discrete units, as in (Chen et al., 2019), while preserving all the linguistic information in the speech. As a result, our theory does not account for the loss of linguistic information during the quantization process, as often occurred in realistic speech datasets. Further, more recent works (Baevski et al., 2021; Liu et al., 2022) have shown that continuous features, with the help of additional regularization losses, can achieve almost perfect ASR-U. Such phenomena is beyond explanations based on our current theory and require generalizing our theory to continuous speech features. Further, our model assumes that sufficiently reliable phoneme boundaries are fed to the ASR-U system, and kept fixed during training. It will be interesting to extend our framework to systems with trainable phoneme boundaries, such as the wav2vec-U systems, to better understand its effect on training stability."
281,"Our proposed ThinkSum has demonstrated strong performance on thirteen challenging BIG-bench tasks. However, it is important to acknowledge certain limitations of the system.
Firstly, as the number of objects or facts that are reasoned over increases, the computation cost will also rise. However, increasing the number of objects will also make the task harder, and direct prompting may cease to work at all (as we indeed observe in BIG-bench results, such as LOGICAL DEDUCTION with more than five objects), while ThinkSum offers a generalizable methodology, as the atomic Think operations do not increase in complexity as the number of objects grows.
Secondly, when solving a new task, it is necessary to expend human effort to select specific operations in each step, as outlined in §2. This limitation is shared with prompt engineering of all kinds, including direct or chain-of-thought prompting: finding a prompt for a new task requires an often-cumbersome prompt engineering procedure. We have described ThinkSum as a general twostage paradigm, with an external inference step. This generality aims to facilitate the adaptation of ThinkSum to new tasks, with minimal modifications to the Think and Sum steps. Work on automating the prompt engineering procedure (Zhou et al., 2022b) is a promising path towards overcoming this limitation. An alternative to prompt engineering that does not require such human effort is tuning (i.e., differentiable end-to-end learning) of prompts or model parameters; however, this remains impractical for GPT-3-scale models, and attempts to tune models directly on symbolic reasoning chains have met with limited success (Kassner et al., 2020).
Last but not least, ThinkSum has mainly been evaluated with GPT-3 (davinci) and InstructGPT (text-davinci-002) models. To further improve performance, it may be beneficial to apply ThinkSum to more recent instruction-tuned models such as Flan-PaLM (Chowdhery et al., 2022; Chung et al., 2022), text-davinci-003, ChatGPT, and GPT-4, which seem more capable of robustly performing Think steps."
282,"Robustness to perturbations Our empirical study does not explore the connection between the discriminative power of automatic metrics based on the proposed metric preference checklist and their robustness to simple perturbations or other natural language phenomena that may occur in texts or NLG use cases.
Metric Fairness (Social Bias) Our study does not include an investigation of metric fairness or social bias issues that may be introduced by Language Model-based NLG evaluation Metrics.
Single-aspect vs. Multi-aspect Our current empirical experiments mainly explore the discriminative power of evaluation metrics in single-aspect experiment setup (section §5.2). It may also be interesting to inspect to what extend the metrics can identify multi-aspect levels of quality, particularly when there exists disagreement between human evaluation aspects. For example, instead of disjointly splitting samples into {low Engagingness, moderate Engagingness, high Coherence}, samples can be divided based on the joint aspects, such as {low Engagingness and low Coherence}.
Universal input-output structure Our experiments are mainly carried on publicly available author-annotated human evaluation benchmark datasets. Thus, we do not guarantee the universal input-output structure and a uniform naming system across datasets or tasks. For example, UniEval - Topical Chat data (UniEval-TC) (Zhong et al., 2022) and USR - Topical Chat (USR-TC) (Mehri and Eskenazi, 2020) use a different naming system for human evaluation aspects, yet the aspects refer to the same dimension of human-like qualities.
Dependency of NLG Systems When comparing outputs from two different NLG systems, the systems are presumably independent. However, in many NLG use cases, this assumption is not fully accurate. For example, in Controlled Generation task, the systems originate from one pretrained Language Model as an encoder model. In inference or decoding stage, the encoder’s probability outputs are used as inputs for multiple decoding schemes, such as the use of Log-Likelihood ranking, distance scoring as filter, etc (Dathathri et al., 2020), yielding n-systems to compare with. As a result of this setup, the generation outputs from these n-systems are often less diverse and less distinguishable than
the outputs from two independent systems that do not share the same encoding scheme or training objective."
283,"Following instructions, we add Limitations after Conclusion."
284,"Due to the massive combination of relations and times on TKGs, balancing the model performance and efficiency is challenging. Our model TECHS performs well as Section 5.2 and 5.4 discussed. However, there is also a limitation. TECHS is a
two-step approach that can be further improved if we can fuse logical reasoning in the graph encoder like ConGLR (Lin et al., 2022). The model will be more efficient for computational space and time."
285,"Through extensive empirical analyses, we demonstrated that our proposed method can produce highutility synthetic text with strong privacy protection. However, we acknowledge there are limitations.
Our method captures general statistical properties of the original text but is not able to perfectly replicate all details. DP protects the privacy of individual samples in the original training text, but this means that DP also limits the model in learning the tail of the training distribution (Suriyakumar et al., 2021). Overall, strong DP guarantees render the generation of rare patterns in the original data unlikely. This means that the synthetic text generated from a DP-trained model may potentially miss valuable information conveyed in the outliers of the training text.
We observed in our conditional generation studies that DP disproportionally affects classes (corresponding to control codes) with different sample sizes. In particular, tight DP guarantees most negatively impact learning the distribution of small-size classes. Future work may study approaches that mitigate this negative impact for minority populations in private synthetic data generation.
We selected values for privacy parameters ϵ = 4 and δ = 1/(N · logN) based on prior privacyutility trade-off studies for text classification and table-to-text generation (Li et al., 2022b; Yu et al., 2021b). We leave it to future work for a more extensive privacy-utility trade-off analysis for general synthetic text generation.
Our canary extraction experiments demonstrated that strong DP guarantees lead to strong empirical privacy even for “private” information (the subject) that appears across multiple training instances. However, we note that DP guarantees generally translate into strong empirical privacy guarantees only when individual samples have low or no correlation (Kifer and Machanavajjhala, 2011). It is therefore crucial that DP machine learning be applied in conjunction with other modes of privacypreserving techniques (e.g., data deduplication and redaction (Zhao et al., 2022)) for optimal protection. For deployments of DP synthetic text generation, one should also consider meaningful example boundaries."
286,"Training Data Our pre-training data is sourced from 19 existing dialogue datasets. However, it’s important to note that these datasets may contain noise, such as harmful content, irrelevant file names, and URL links. Despite utilizing multiple automatic tools to filter out this content during preprocessing, there is still a chance that some noise may be present in our pre-training data. This could potentially impact the performance of DIONYSUS, making it important to monitor and improve the pre-processing steps continuously.
We also know the potential drawbacks of constructing pseudo summaries using the GSG method, which may lead to unnatural summaries for dialogue data. To mitigate this, we introduced the Summary Helper in Section 2.1, which is specifically trained on two dialogue summarization datasets containing natural summaries. This approach enables more realistic pseudo-summaries and enhances zero-shot performance. Although we employ top-m turns as an additional source of pseudo summaries, Figure 4 illustrates that GSG+ contributes a minor portion of the pseudo summary, with a 0.7 to 0.3 ratio between generated and topm turns. Our method thus minimizes referent and pronoun confusion, ensuring better coherence than solely employing the standard GSG technique.
Training Resource To improve our model’s performance, we employ the “Better ROUGE” strategy, which calculates the ROUGE score for both candidates and selects the best one as the final training objective. This data pre-processing process can be pretty time-consuming, taking approximately one day to complete for our pre-training data when utilizing 100 threads. Additionally, we utilize 16 Nvidia V100 GPUs to train our models, which may not be accessible or reproducible for all researchers. This could present a significant obstacle for those looking to replicate or build upon our work.
Test Data Another potential concern is the test datasets used to evaluate DIONYSUS. The test set size is relatively small, which may not fully represent the breadth of dialogue types that a general dialogue summarization model should be able to handle. This could lead to the model performing well on the test set but not generalizing to other unseen dialogue types. Further, our analysis did not include the assessment of long dialogue summarization, such as lengthy meetings (Carletta et al., 2005;
Zhong et al., 2021; Janin et al., 2003) or screenplays (Chen et al., 2022). However, our study’s approach has the potential to handle these scenarios, even though it was not specifically designed for them. By incorporating LongT5 (Guo et al., 2022) or DialogLM (Zhong et al., 2022), which are known for their ability to process extended input sequences, we expect that they could efficiently tackle this task."
287,Section 8
288,"The limitation of this paper are twofold. First, our method does not provide a recipe for data imbalancement in NLVL task. Thus, our method does not guarantee the effectiveness on edge cases. Second, the choice of feature extractor is considered relatively outdated. Our model does not benefit from the recent development of pre-trained visionlanguage models. On the other hand, using pretrained vision-language models remains in its early stage in NLVL tasks. Not using pre-trained features makes a fair comparison between our model with existing baselines. As a part of future work, we will explore the potential of using more powerful feature extractors in our model."
289,"Section 7
7 A2. Did you discuss any potential risks of your work? Our paper uses public benchmarks and does not have any risk of ethics or infringement."
290,Limitations
291,"Despite the demonstrated effectiveness of selfadaptive ICL, this new paradigm suffers from the following limitations. (I) As we discussed in § 6.4, due to the large search space, we need to trade efficiency for effectiveness. So how to balance the efficiency-effectiveness trade-off is an important decision choice to make when deploying selfadaptive ICL methods. (II) As shown in § 6.1, the gains of our method shrink when the size of the retrieval set gets smaller. To maximize performance, we require a high-quality retrieval set, which might not always be available when dealing with unseen tasks in practice. We also note that both limitations can be alleviated with better selection and ranking algorithms.
The remarkable performance of our method should partially attribute to the powerful TopK selection method, so we also discuss the limitation of TopK here. Despite its popularity, our analysis (§ 6.2) reveals that TopK’s effectiveness is limited to simple NLU tasks with limited label space, and it does not work well with tasks with large or even infinite label space (QA, multi-choice, and NLG). This limitation signals a new direction for ICL research: we need better selection methods to adapt ICL methods to more tasks."
292,section 8
293,Section Limitations
294,Section 7
295,"Section after Conclusion
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
296,"Rather than a complete, systematic probing of the stereotypes and biases related to each demographic group that may occur in the open-ended outputs, our study offers insight into the patterns in the stereotypes that the widespread use of LLMs may propagate. It is limited in scope, as we only evaluate models available through the OpenAI API.
Stereotypes vary across cultures. While our approach can be generalized to other contexts, our lexicon and qualitative analysis draw only upon American stereotypes, and we perform the analysis only on English. Beyond the five race/ethnicity and three gender groups we evaluate, there are many other demographic categories and identity markers that we do not yet explore.
Another limitation of our method is that it currently requires defining which identities are (un)marked a priori, rather than finding the default/unmarked class in an unsupervised manner. The prompts are marked with the desired demographic attribute, and every persona is produced with an explicit group label. Given these explicit labels, we then compare and analyze the results for marked vs. unmarked groups.
A potential risk of our paper is that by studying harms to particular demographic groups, we reify these socially constructed categories. Also, by focusing our research on OpenAI’s models, we contribute to their dominance and widespread use."
297,"Section Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 7 Did you use or create scientific artifacts? Left blank.
B1. Did you cite the creators of artifacts you used? No response.
B2. Did you discuss the license or terms for use and / or distribution of any artifacts? No response.
B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? No response.
B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? No response.
B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? No response.
B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. No response.
C 3 Did you run computational experiments? 5"
298,"The main limitations of our study also provide motivation for future work. First, while we have provided an extensive ablation study for GEC-DePenD, there are many more low-level optimizations that can be done to further improve the results. In a real life application, one would be encouraged to investigate these optimizations.
Second, obviously, non-autoregressive models, including GEC-DePenD, still lose to state of the art autoregressive models. While the existence of this gap may be inevitable, we believe that it can be significantly reduced in further work."
299,"Section 7
7 A2. Did you discuss any potential risks of your work? Our work deals with improving grammatical error correction and does not seem to have potential risks beyond the usual ecological concerns related to using large language models; we do note the model size and training time."
300,After the conclusion section and before the reference section
301,"section Limitations after the conclusion
7 A2. Did you discuss any potential risks of your work? As our model does not generate its own outputs, when used with trustworthy sources, we do not see high societal risks. However, we admit that those biases from the training datasets can be amplified. For example, regardless of improvements, our model can not fully address the deficiency of dense retrieval on rare entities, which can compromise the fairness of retrieval."
302,"Although our approach produces promising results on two datasets, there are certain limitations. In the future, we will continue to dig into these concerns.
Firstly, we evaluate the DaMSTF on two classification tasks. We do not conduct experiments on other NLP tasks, such as machine translation (Yang et al., 2018) or named entity recognition (Jia et al., 2019). Nonetheless, as text classification is a fundamental task, other NLP applications can be specified as a case of classification. For example, named entity recognition can be formulated as a wordword relation classification task (Li et al., 2022).
Secondly, the meta-learning module carries out extra computation overhead. As the bi-level hyperparameters optimization involves a second-order derivate on the model’s parameters, their computation overhead is quadratic to the model’s parameters. In DaMSTF, we use the approximation techniques in WIND to compute the derivate, which is linear to the model’s parameters. In the future, we
will investigate other techniques to accelerate the DaMSTF."
303,"Even the premise of parsing questions to Wikidata queries leads to linguistic and cultural bias, as Wikidata is biased towards English-speaking cultures (Amaral et al., 2021). As Cui et al. (2022) argue, speakers of other languages may care about entities and relations that are not represented in Englishcentric data (Liu et al., 2021b; Hershcovich et al., 2022a). For this reason and for the linguistic reasons we demonstrated in this paper, creating CG benchmarks natively in typologically diverse languages is essential for multilingual information access and its evaluation.
As we mentioned in §4.2, our translation system fails to deal with ambiguities beyond grammar and thus generates wrong translations for a few samples (less than 0.31%). Moreover, although the dataset can be potentially augmented with low-resource languages and in general other languages through the translation framework, adequate knowledge will be required to expand rules for the specific target languages.
With limited computational resources, we are not able to further investigate the impact of parameters and model sizes of multilingual PLM as our preliminary results show significant performance gaps between PLMs.
Broader Impact
A general concern regarding language resource and data collection is the potential (cultural) bias that may occur when annotators lack representativeness. Our released data largely avoid such issue due to the synthetic and cultural-invariant questions based on knowledge base. Assessment by native speakers ensures its grammatical correction. However, we are aware that bias may still exist occasionally. For this purpose, we release the toolkit and grammar used for generation, which allows further investigation and potentially generating branches for other languages, especially low-resource ones.
In response to the appeal for greater environmental awareness as highlighted by Hershcovich et al. (2022b), a climate performance model card for mT5-small is reported in Table 7. By providing access to the pre-trained models, we aim to support future endeavors while minimizing the need for redundant training efforts."
304,"The Limitations section follows the Conclusion section.
7 A2. Did you discuss any potential risks of your work? Our work only provides a benchmark to evaluate semantic parsing models and not an application that can be used for potentially risky purposes."
305,"While our approach effectively mitigates query latency through a cascade ranking paradigm, it necessitates additional computational resources during training due to the need for attention score calculation and alignment in the optimization process. Additionally, our model incorporates passage-level relevance scores into the ranker, generating a cooperative matching representation during document ranking, which could marginally augment the inference time. In our future endeavors, we aim to explore more efficient methodologies that can further improve ranking efficiency. Furthermore, it
is worth noting that our approach has been tested using specific backbone models. To fully evaluate the effectiveness of our method, it is essential to conduct experiments with a diverse range of backbone models, which remains an avenue for further exploration."
306,"Limitation section
7 A2. Did you discuss any potential risks of your work? The topic of the paper deals only with document retrieval"
307,Section Limitations.
308,"Obviously, the work presented in this paper is limited to transcripts of spontaneous conversations in English. Since we are investigating the problem of named entity recognition, we have to point out that there are practically no datasets of human conversations (both audio and transcripts) annotated with entity spans apart from SWNE, OntoNotes and Earnings-21, the three datasets used in our paper. These datasets are relatively small, and the distribution of the frequency of appearance of entity classes is extremely skewed, with several entity classes represented by a handful of examples.
Another significant limitation of the results reported in this paper is the choice of metric. Following the common practice in the NLP community, we have chosen the F1 score as the primary metric of entity recognition. However, this metric is questionable in the context of NER recognition in ASR transcripts because it is highly dependent on two factors: the WER produced by the ASR and the definition of span alignment. Consider a gold transcript annotation ""JohnB-PERSON F.I-PERSON KennedyI-PERSON"" and the ASR output with ""F."" transcribed as ""eh"" annotated as follows: ""JohnB-PERSON eh KennedyB-PERSON."" Should this annotation be considered correct? The original person entity starting at ""John"" is only partially matched, and a new person entity starting at ""Kennedy"" is introduced in the ASR output. Consider another gold annotation of the following transcript: ""secondB-DATE quarterI-DATE twentyI-DATE twentyI-DATE,"" which the NER model tags as follows: ""secondB-DATE quarterI-DATE twentyB-CARDINAL twentyI-CARDINAL"" (NER model trained on written language does not recognize ""twenty twenty"" as a valid date). Again, how should this scenario be scored by an accuracy metric? Unfortunately, the traditional definition
of the F1 score is too restrictive to produce a robust score that could paint a reliable picture of the model’s performance. The design and implementation of a metric that could compute the alignment of entity spans in the presence of ASR errors would be a significant step in the direction of producing more robust NER models for spoken conversations.
We conduct experiments with the ASR on audio files from the Earnings-21 dataset. These files are recorded at 11 kHz-44 kHz, while typical call center conversations are recorded at 8 kHz-16 kHz. Unfortunately, training datasets with recording characteristics resembling real-world usage scenarios are unavailable. We also do not address the problem of racial, gender, and age disparity (Koenecke et al., 2020) due to the lack of availability of sufficiently representative and inclusive datasets. It is, however, to be expected that the performance of the ASR deteriorates for the recordings of speakers other than male speakers of General American."
309,7. Limitation
310,Section 6
311,"We investigated CSJ with SELECT, SIMPLIFY and REWRITE. We adopted HIPORANK as SELECT be-
cause it is a lightweight, unsupervised model that extracts a summary in a discourse-aware manner. However, when we replaced it with other extractive models during the component analysis, we found no significant difference in overall performance. We adopted KEEP-IT-SIMPLE for SIMPLIFY because it facilitates paragraph simplification. We found the model is quite heavy, making it slow during training. To the best of our knowledge, there is no paragraph-based simplification model we could explore in component replacement. The choice among various pre-trained models for REWRITE was quite challenging, as all these models are variations of transformer-based architectures. So we adopted the latest three SOTA models, which are efficient and effective summarization models. We also trained the vanilla sequenceto-sequence model, pointer-generator model and transformer as our baselines to provide sufficient variations of SOTA models. We found mBART is more promising performance-wise in our experiments. However, its training time is also slow for our datasets due to longer inputs."
312,"Section Limitation
7 A2. Did you discuss any potential risks of your work? There is no potential risk."
313,"Section 8
7 A2. Did you discuss any potential risks of your work? This work examines research integrity issues related to model evaluation and does not feature new datasets or models. It is possible the findings of this work will have negative consequences for past and future research, which is a point we discuss in the text. However, because this work does not involve releasing data or model artifacts, it is unlikely that any outcome of this work will be misused with malicious or unintended effects or deployed in any context that is risky, harmful, or negatively impacts privacy, security, or fairness."
314,"Since our primary goal is to study the phenomenon of instruction induction under lab conditions, we focus on tasks that have simple instructions. Future work may extend instruction induction research by including tasks with more complex instructions. These tasks are expected to pose a greater evaluation challenge, especially when considering reference-based methods. Evaluating through execution accuracy, however, may mitigate some of that challenge. Additionally, only one model showed instruction induction abilities, i.e., textdavinci-002. The exact implementation details of the model and its training data are not publicly available, thus we are unable to investigate the reason behind the emergence of this ability. However, we note that our goal is to present the phenomenon of instruction induction and to raise the ambitious possibility of instruction induction as a learning paradigm. Thus, our goal is not to focus on specific models but rather to shed light on this unexplored phenomenon. Finally, we point to a limitation of the execution accuracy metric, namely assuming the existence of a good-enough instruction-tuned model. Due to recent interest and progress in instruction tuning, we believe this to be a reasonable assumption."
315,"In its current formulation, REV might reward a rationale for an incorrect prediction as long as the rationale supports the prediction with relevant additional information. Additionally, our metric does not consider the factuality of rationales. Future work might explore evaluation that penalizes rationales which support incorrect predictions, thus bridging together predictive performance with interpretability metrics. We considered a single declarative construction for baseline rationales and leave analyzing how different baseline construction impacts our metric to future work. Another limitation is that the utility of REV depends on the quality of crowd-sourced rationales used to train the evaluator. Building a good automatic metric REV requires high-quality rationales that provide sufficient new information (e.g., commonsense knowledge) to explain the corresponding labels. The architecture of evaluation models also has an impact on REV evaluation. Using different evaluator architectures may result in varying REV scores, as discussed in Appendix B.3."
316,"We conclude the limitations of our schema into two aspects. Firstly, our method benefits from the assumption that there exists similar semantics between the seen data and unseen samples. However, our work might not own obvious advantages in the case where the correlation among domains is weak, such as medical assistant and movie service. But notably, in such cases, most zero-shot
learning methods will also fail to show well generalization. Secondly, we propose to train semanticindependent DST experts, which is ideal but we believe advanced components could move towards this goal, such as using advanced clustering algorithms and pretrained language models."
317,"After Section 6
3 A2. Did you discuss any potential risks of your work? 6
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 5
3 B1. Did you cite the creators of artifacts you used? 5
3 B2. Did you discuss the license or terms for use and / or distribution of any artifacts? 1,5,6"
318,In the last section
319,"Our proposed KALM has two limitations:
• KALM relies on existing knowledge graphs to facilitate knowledge-aware long document understanding. While knowledge graphs are effective and prevalent tools for modeling real-world symbolic knowledge, they are often sparse and hardly exhaustive (Tan et al., 2022; Pujara et al., 2017). In addition, external knowledge is not only limited to knowledge graphs but also exists in textual, visual, and other symbolic forms. We leave it to future work on how to jointly leverage multiple forms and sources of external knowledge in document understanding.
• KALM leverages TagMe (Ferragina and Scaiella, 2011) to identify entity mentions and build the three knowledge-aware contexts. While TagMe and other entity identification tools are effective, they are not 100% correct, resulting in potentially omitted entities and external knowledge. In addition, running TagMe on hundreds of thousands of long documents is time-consuming and resource-consuming even if processed in parallel. We leave it to future work on how to leverage
knowledge graphs for long document understanding without using entity linking tools."
320,right after the main paper on page 9
321,"Adapting PLMs to our proposed model does not go as smoothly as expected, because there are three different forms of tokenization: the PLM tokenizer, the multilingual tokenizer implemented in our proposed model, and the special annotations of numerical values/entity mentions/long-winded attribute values in the attribute extraction datasets, which are difficult to reconcile simultaneously. Although our model without PLM has outperformed PLMbased ones, this does impose a limitation for future explorations.
Although Re-CNShipNet, one of the datasets used in our experiments, is more accurate with our careful re-annotating, the size of which is still so small that would produce randomness bias during the model training and may affect the final experimental results.
Besides, due to the limitation of computational resources, we did not conduct experiments on large
language models such as T5 (Raffel et al., 2020), LLaMA (Touvron et al., 2023), etc., which may lead to insufficiency of the experiment.
Ethics Statement
This work uses three publicly available datasets, and we respect and adhere to their user agreements and licenses. The content of pre-existing datasets does not reflect our perspectives. We, the in-house authors, re-annotate one of these datasets, i.e., Re-CHShipNet; the purpose of re-annotation is mainly to correct errors and re-balance the ratio of CWA/OWA labels. The annotation may introduce personal judgment and bias, which may bring potential risks. Further, the potential downstream applications of this work include knowledge graph construction, search engine, e-Commerce, recommendation system, etc.; we caution that our proposed method may cause misextraction or false information, and may fail in the case of out-ofdistribution and domain shift, which may harm those applications."
322,Section 8 (Limitations).
323,the limitation section on page 9.
324,"Due to the high computational costs of the method, we tested it only on a very small set of sentences and larger-scale experiments are needed to confirm the results.
Many parameters of the GA algorithm were left unexplored – the results could be improved by grid search over the values for mutation and crossover ratios, using a better list of mutation candidates (for example based on k-NN search), experimenting with different selection methods, combining more metrics in the fitness function or using multiobjective GA like NSGA-II (Deb et al., 2002).
In the experiments concerning held-out metrics, we assumed weaknesses of the held-out metrics are not correlated to the weaknesses of the optimization metrics, which is probably not true, due to similar model architectures and training datasets. This means that held-out metrics are not strictly independent, but we believe combining multiple different held-out metrics should mitigate this issue."
325,"section Limitations
7 A2. Did you discuss any potential risks of your work? All data used in our work comes from public datasets, which ensures that there are no privacy issues involved in our work, so there is no potential risk in our work."
326,Limitations section
327,Left blank.
328,"In this study, we mainly utilised the BERT family of models for Chinese text classification tasks. Given the similarity with respect to transformer language models and pre-training paradigms, as well as the preliminary results on English datasets as discussed in Section 6.3, we may be able to extrapolate the results to other architectures/tasks/languages.
For example, Perplection can be seamlessly apply to decoder-only models (e.g., GLM (Du et al., 2022), LLaMA (Touvron et al., 2023)) to see whether it can boost the performance for those NLG tasks. But further investigation is needed to verify the utility of findings on other model architectures, tasks, and languages. In the future, we expect to see Perplection applied to different NLG tasks such as seq2seq information extraction (Lu et al., 2022b), question answering, arithmetic reasoning, machine translation or even multi-modality tasks.
Also, utilising Perplection may exacerbate the inherent limitations of pre-trained language models. We suspect that, in instances where the model has not been exposed to certain texts or concepts during pre-training, reliance on perplexity for template selection may result in subpar performance. In the future, we expect to explore whether we can alleviate this problem by certain annotation-free methods, such as continuous self-supervised training with downstream data, or extend our method in a few-shot setting where limited label information is available.
Besides, the use of perplexity as a metric has the drawback of favoring long texts, which forces us to design templates of the same length. Therefore, a length-agnostic metric can be considered as an alternative."
329,"Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 4, 5
3 B1. Did you cite the creators of artifacts you used? 4, 5
B2. Did you discuss the license or terms for use and / or distribution of any artifacts? Not applicable. Left blank.
B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Not applicable. Left blank.
B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.
B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Not applicable. Left blank.
3 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Left blank.
C 3 Did you run computational experiments? 4, 5"
330,"The data set presented is still quite small for machine-learning models, as is the number of annotators (and thus the demographic diversity). Since the annotation required a lot of human effort, we chose fewer, but experienced, student assistants as annotators to ensure a high quality of the annotations.
The agreement for effectiveness and argumentative function is low. To address this weakness we used the following strategies: a) An examination of the confusion matrices reveals that the annotation scheme is not exclusive, that is, a story can take on multiple argumentative functions. We therefore include different, aggregated versions of our dataset that include this annotation layer as a multi-label layer (see Section 4). b) We address the subjectivity of the two annotation layers in a regression analysis (Section 6). The interactions between each annotator and certain annotated properties show annotator-specific differences, which should also not be ignored in the modeling.
A crowd-sourcing study could build on the initial findings and collect more annotations for effectiveness to investigate perspectivism in this context. Finally, we lacked sufficient space to analyze the existing annotations of the sub-corpora of our resource (e.g. testimony in CMV and Regulation Room) and discuss them with our new annotations. We see this as an opportunity for future work."
331,"There are several important limitations to this work that can be split into two categories: (1) method applicability to other domains and (2) method scalability to much larger models.
Method applicability to other domains. Utilization rate computation and regularization are possible when there is some external knowledge that can be used to infer which tokens are “important.” In particular, our highest-performing model uses token semantic type to compute utilization rates. This limits our approach to sub-domains where there is an external knowledge source that can inform us about important tokens and give us higher-order semantic information about how to group the important tokens. For example, our approach will likely not be very helpful for open-domain conversations.
Method scalability to much larger models. We have evaluated our approach for models on the scale of O(108) parameters. However, modern state-of-the-art models often involve O(1011) parameters, three orders of magnitude larger than models in our experiments. Large language models (LLMs) often still suffer from the under-generation of rare tokens, but our study is insufficient to determine if our approach would still work. We suppose that utilization-rate-based regularization is most likely to be beneficial in the fine-tuning step of LLMs, but verification of this is left for future work."
332,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
333,"Section 8
7 A2. Did you discuss any potential risks of your work? No potential risks"
334,"No section number, after Section 5 Conclusion
7 A2. Did you discuss any potential risks of your work? We do not see significant risks in our work"
335,After section 8 and before the references - as requested.
336,"In this work, we only focus on designing strategies for PLMs with the MLM-style pre-training objective, and do not account for other types of pre-trained language models such as discriminative PLMs (Clark et al., 2020; Shen et al., 2021). However, as there are recent works that aim to design prompts for discriminative PLMs (Yao et al., 2022; Xia et al., 2022), PATRON can be potentially combined with them to improve the data efficiency.
We are also aware that there exists advanced fewshot fine-tuning techniques for PLMs recently (Hu et al., 2022; Tam et al., 2021; Zhang et al., 2022b, inter alia). We argue that PATRON does not rely on a specific fine-tuning method, and can be combined with them to further improve the performance. Lastly, as prompting methods have been widely adopted to other tasks such as natural language inference (Gao et al., 2021a) and relation extraction (Han et al., 2021), it is possible to extend our method to these tasks."
337,"In our paper, we presented existing and novel training-free NAS metrics for RNNs and transformers. Benchmarks are required to evaluate the effectiveness of these metrics on various architectures. While there exists a robust benchmark for RNN architectures (NAS-Bench-NLP), there is none for transformer models. Thus, we had to create our own NAS benchmark. For our work, we were limited by the computational resources available to us, so we were only able to pretrain and finetune 500 models for our NAS BERT benchmark. A larger sample size would give a more accurate evaluation of the training-free NAS metrics. Furthermore, we only investigated the FlexiBERT search space. While FlexiBERT has a diverse search space, having heterogeneous layers and alternative attention operators, the variation between possible architectures is limited and still dependent on the linear paradigm of BERT. Alternative transformer search spaces using cell-based methods, such as those presented in “Primer” (So et al., 2021) and “AutoBERT-ZERO” (Gao et al., 2022), do not have this limitation. We were ultimately unable to investigate the performance of training-free NAS metrics on this type of search space, as there are no available benchmarks for these search spaces, and their greater variability necessitates a copiously large sample size that is well outside our computational capabilities.
Another limitation is that we only evaluated the effectiveness of the presented metrics on encoderonly transformer architectures, and not encoderdecoder or decoder-only architectures. Furthermore, while the training-free NAS metrics are dataagnostic, the benchmarks they were evaluated on were only trained and evaluated on English datasets and tasks."
338,"Algorithmic Limitations: The current approach assumes each phoneme / grapheme corruption is independent of the surrounding phonemes / graphemes, which can be relaxed to get further insights and model any contextual phonetic shifts. The relative importance between grapheme and phoneme corruptions could also be explored as a hyperparameter to personalize more to the type of errors of a community. Other Limitations (with respect to available data and existing resources): Our coverage analysis is conservative since it does not cover the user generated data from various social media where such
L1-L2 phonetic misspellings are bound to be more common. The coverage analysis also relies on the context not being corrupted. However, this might not necessarily hold and the analysis could benefit from a careful formulation of a relaxed matching criteria that also considers cases with corrupted contexts. With transliteration playing a major role in our solution, it is difficult to immediately extend the work to low-resource languages that do not have models or appropriate datasets to build transliteration modules."
339,"There are two limitations to this work. First, it takes effort to design the prompt to guide the LLMs to generate correct reasoning steps. The GPT-3 models are sensitive to the expressions in prompts. Thus we need to carefully design the prompts. Second, the proposed plan-and-solve prompting can help address the calculation errors and missing-reasoningstep errors, but the semantic misunderstanding errors still remain. We will explore how to address semantic misunderstanding errors by prompting instead of upgrading LLMs in the future."
340,"Section 6
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
341,"Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
342,Limitations section (and throughout)
343,"Our findings are primarily based on ROUGE score, which is a noisy, unstable metric with well-studied limitations (Schluter, 2017). To address this, however, we conduct a human evaluation to support our findings. In both automatic and human annotation settings, we base our evaluations on naturally occurring references, which have been shown to be silver-standard (Gehrmann et al., 2022; Wan and Bansal, 2022; Adams et al., 2022). We hope that our work on PGA–a method to generate high-quality diverse candidates–can be applied to new domains (e.g., (Gliwa et al., 2019; Adams et al., 2021; DeYoung et al., 2021)) and reference-free learning objectives (e.g., RLHF and calibration). Also, our candidate generation method requires two models, which is less elegant and computationally efficient than an end to end solution combining planning and surface realization.
Lastly, PGA treats all content plans as equally likely (each plan is given one abstractive beam). Yet, there is an unexplored trade-off between exploration and exploitation. Should higher-confidence content plans receive more candidates? Future work should explore a generating diverse abstracts from a dynamic nucleus of extracts, which would allow for the generation of many abstracts from only a few extracts when confident (e.g. short documents), while exploring more diverse content when the extractive generator is less confident. We sketch out such a potential system in Figure 5 with a made-up nucleus probability of 0.9."
344,"In this section, we outline the key limitations of our research. Our findings on the ACQ models are not as advanced as the current state-of-the-art, but they serve as a benchmark for others to compare with when using similar datasets. Additionally, to conduct more extensive experiments on larger datasets and more advanced models, we require additional computational resources. Specifically, generating clarification questions is a demanding task as it requires the use of powerful language models."
345,"Experiments on other types of reasoning tasks. In addition to the two representative reasoning tasks (arithmetic reasoning and multi-hop question answering) that we experiment on, there are also other tasks where CoT prompting brings significant improvements over standard prompting shown by previous work, many of which are symbolic reasoning tasks such as Last letter concatenation, Coin flip from Wei et al. (2022) and Temporal Sequences, Tracking Shuffled Objects from BIG-Bench (Srivastava et al., 2022; Suzgun et al., 2022). However, most (if not all) tasks there are highly templatebased and hence the reasoning steps have little variations, both within each example and across different examples. This makes it difficult for us to conduct our ablation studies on these tasks. Take the example of Last letter concatenation, a task about concatenating the last letters of a given sequence of words (e.g., “Amy Brown” → “yn”). Here, every step in the rationale except the last is in the form “The last letter of X is Y” where X is some word in the given sequence and Y is the last letter of X. Hence, the language templates are the same and there is no sense of order among the steps (the order is completely characterized by the given sequence instead), and our ablation settings will not apply well. Extending our ablation designs to these “reduced” cases is one of the items we want to explore in the future. A more systematic treatment of “invalid reasoning”. We manually write rationales with invalid reasoning for the experiments in §4 since automatically synthesizing such rationales turns out to be
challenging, mostly due to the informal nature of the tasks we experiment on (relatedly, the original CoT rationales are also human-written). We intend to give a more systematic treatment of the invalid reasoning setting in the future, e.g., following the categorizations of informal logical fallacies (Copi et al., 2016). Improvements on intrinsic evaluation. Our intrinsic evaluation of the generated rationales is based on the correctness of bridging objects, which, even though is a good indicator of the quality of language templates (Appendix A.2) in our experiments, may not be a good metric in general cases. It also relies on ground truth bridging objects, which are usually not available and costly to annotate. Toward this end, one direction we want to explore further is to develop ways to conduct more comprehensive and reference-free intrinsic evaluations. Recent papers such as Golovneva et al. (2023) have also done promising work along this line."
346,"Other ways of reducing the amount of required supervision could be attempted, but we do not expect that these would change the outcomes significantly. Self-supervised learning via masking / denoising objectives, either in the form of an auxiliary task or via the use of pretrained models, is one such approach. This however generally underperforms backtranslation, which can utilise the same monolingual data to more effect (NLLB Team et al., 2022), as we see in the experiments of Appendix E. Iterative backtranslation might offer an additional boost for data-scarce settings, but is very computationally intensive, complex, and any gains would almost certainly apply to models trained with the addition of seed data too.
The seed datasets that we release bring about large translation performance gains for a number of low-resource languages. We note that, due to budgetary and complexity constraints, the source data we used was sourced from English Wikipedia only. This is likely to have two effects. First, translating English-original data leads to so-called translationese effects one the low-resource side (Volansky et al., 2015), leading to decreased effectiveness for directions that target low-resource languages. Second, the data is unlikely to adequately cover diverse content from multiple cultures. An interesting avenue for future research would therefore involve studying the effects of seed parallel data that is originally translated from low-resource languages."
347,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
348,"Sec. Limitations.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
349,"The section after Conclusion.
7 A2. Did you discuss any potential risks of your work? This work presents a general compression method, which is not tied to particular applications."
350,"The techniques in SpanSub are constructed on the basis prior works of extracting span alignments and clustering words in the training data according to their syntactic role. There is no generic solution for these problem applicable for all of the datasets (this is mainly because the output formats and structures are diverse) at present, which requires users to spend efforts looking for preprocessing techniques applicable for their own datasets. However, the methodology of the proposed SpanSub is rather general to many different datasets and tasks (e.g., Semantic Parsing and Machine Translation). Besides, although we define eligible spans to try to alleviate additionally introducing noisy augmented data, our experiment result on GeoQuery (i.i.d. split) shows that SpanSub can still slightly hurt generalization performance (in comparison with other state-of-the-art methods). Hence we regard that relieving the potentially negative influence of noisy augmentation is important to further improve this work."
351,The Limitation Section on page 9.
352,"Yes, we provide the limitations of our work in Section 7 (conclusion) and Limitation Section.
7 A2. Did you discuss any potential risks of your work? There seem to be no potential risks."
353,"We recognize that our annotation and analysis methods can require considerable human labor, that can limit the amount of annotated data we can collect. Also, despite cycle training being generally accepted as a model-agnostic approach, we were not able to test a wide variety of backbone models due to resource constraints. In addition, though we relaxed the entity constraints and made cycle training for data-to-text generation end-to-end, the nondifferentiability problem remains unsolved. The intermediate outputs generated by the first model of each cycle are assumed to be correct. This is a weak assumption that may propagate misleading training signals to the second model of each cycle, particularly in the early stage of the training.
To address these limitations, future work may focus on the following directions: 1) building differentiable cycle training models; 2) exploring au-
tomated error detection methods and building models that may utilize such signals; and 3) assessing different backbone models, including large language models like GPT-X, with the cycle training approach."
354,"In this paper, we employ an information entropyguided algorithm for purifying the induced biased features. For each dimension of the biased features, the component with less information entropy is priorly regarded as the component corresponding to semantic information, and excluded when deriving the purified biased features. However, there is still the risk that the discarded component still account for part of the dataset biases. This would lead to a decrease in the effectiveness of the debiasing process. Hence, although the prior-knowledge free nature endows our proposed biased features purification algorithm with strong generality, in cases when resources indicating the distribution of dataset biases are available, incorporating these resources would further enhance the purification of the biased features."
355,"Although DEER has shown excellent performance on multiple datasets and tasks, we still found some limitations affecting its usability and efficiency: (1) The latent alignment model (such as CTC) cannot deal with the multi-modality problem in the largescale dataset, which also leads DEER to underfitting the multiple latent alignment targets that need to be aligned. (3) Although DEER does not need to perform length prediction, it relies on the assumption that the input length is large than the output, which causes the model to lose flexibility in length control. (3) We compared sequence-tosequence models such as BART and ProphetNet in the experimental part of this work. In fact, BART only through six layers on each forward pass, while the BERT family model needs to go through 12 layers, leading the inefficient inference due to latency accumulation of multiple iteration steps."
356,"We provide the limitations in Section 8.
7 A2. Did you discuss any potential risks of your work? We think our general training method will not lead to any negative societal impact."
357,"While our theoretical work is broadly applicable to any protected attribute and any dialogue task, our empirical study has primarily tested gender bias on the GuessWhat?! task. Continued experimental study on a wider range of protected attributes and tasks can better support our mathematical findings. Also, users of our theory should verify the assumptions of our theory when using it to draw insights on new datasets. Specifically, as the type of data bias changes, it is possible the assumptions of Thm. 3.2 may no longer be met. Users of our theory should take care in ensuring context-awareness and context-preservation, for example, are reasonable assumptions on new data, prior to applying the insights of § 3.3. Lastly, while all of our gender annotations come from human annotators, only a smaller subset come from annotators primed to
14https://github.com/anthonysicilia/equitable-dialogueACL2023
judge correctness/equity of gender reference. So, more in-depth human evaluation can better support our theoretical results as well."
358,"Legislators show political support in multiple ways. In this work, we operationalised political support as Active and Passive cosponsorship. Active and
Passive cosponsorship represent a strong signal of support between legislators that has been widely accepted in the political science literature (Kessler and Krehbiel, 1996; Wilson and Young, 1997; Browne, 1985; Woon, 2008; Sciarini et al., 2021; Dockendorff, 2021; Fowler, 2006; Kirkland, 2011; Kirkland and Gross, 2014; Lee et al., 2017). However, other forms of political support, e.g., endorsement of public posts on social media, could be considered. Future research might explore the extent to which these forms of support might reveal additional insights about the cooperation between legislators.
Our second limitation relates to the estimation of legislator’s ideology. Ideology is a latent concept. This means that it cannot be directly measured and no ground-truth data exists. Therefore, to validate that our legislator representations encode ideology, we need to prove their performance in a variety of tasks in which the political science literature suggests ideology is important. In our work, we studied three tasks: (i) active/passive cosponsorship prediction, (ii) party affiliation recovery, and (iii) voting prediction. We argue that this is a representative set of tasks. However, legislators are involved in additional ideology-driven tasks, e.g., the release of public statements. Showing that our representations are also predictive of these additional tasks might be considered an even more robust and convincing validation of our results.
Third, in its current form, our model cannot compute predictions for newly elected legislators. This is due to no data being available—newly elected legislators have not given any speeches, or (co)sponsored any bills. We argue that by applying our model as an online predictor, new information on legislators could be incorporated as soon as it becomes available. However, a full exploration of our model’s potential for this application was outside the scope of this work.
Our final limitation concerns how our model can be extended to other data. In our work, we studied four different U.S. Congresses. For these, we obtained consistent and high performance. Therefore, we expect this performance to extend to other Congresses. However, having focused exclusively on the U.S., we cannot make any statements about the applicability of our framework to other legislative systems. Addressing this limitation could contribute to proving the generalizability of our results.
Future Work Our work can impact studies on t latent factors (e.g., ideology) in other domains. For instance, recent works on radicalization (Russo et al., 2022b,a) can take a similar approach to study the relation between ideology and radicalization. Similarly, studies on international relations can benefit (Stoehr et al., 2023b) from this approach in order to study latent states between nations such as “ally”, “neutral”, and “enemy”.
ACL 2023 Responsible NLP Checklist"
359,"The construction of the reasoning tree may be affected by the KG quality since the connection operations are variant with the KG structure. Hence the unsolved problem in Knowledge Graph such as incompleteness or noise could disturb the reasoning process. In the future, we will explore a solution to alleviate the influence of the side information."
360,Section Limitations
361,"Dataset Representativeness Our dataset covers a range of topics of public interest (COVID-19, climate change, abortion, migration, the RussoUkrainian war, and local elections) as well as media from all sides of the political spectrum. However, it should not be seen as representative of the media in any country, nor should it be seen as perfectly balanced in any specific way.
Biases Human data annotation involves some degree of subjectivity. To mitigate this, we created a comprehensive 60-page guidelines document (Piskorski et al., 2023a), which we updated from time to time to clarify newly arising important cases during the annotation process. We further had quality control steps in the data annotation process, and we have been excluding low-performing annotators. Despite all this, we are aware that some degree of intrinsic subjectivity will inevitably be present in the dataset and will eventually be learned by models trained on it.
Baseline Models The reported experiments can be seen as strong baselines as they include fairly small encoder-only transformer architectures. We leave for future work the exploration of other architectures and modeling techniques that are known to improve the efficiency and to reduce the computational requirements of the used models, e.g., fewshot and zero-shot in-context learning, instructionbased evaluation, multitask learning, etc.
Model biases We did not explore whether and to what extent our dataset contains unwanted biases.
5https://propaganda.math.unipd.it/ semeval2023task3/"
362,"We hereby discuss the current limitations of our work: (1) As mentioned in Section 3.1, although our annotated dataset enables the possibility of learning an extractive model that can be trained to predict the span of the text segments of interest from scratch, we focus on the more essential actioncondition dependency linkage inference task as we find that the SRL extraction heuristic currently applied sufficiently reliable. In the future, we look forward to actualizing such an extractive module and other relevant works that can either further refine the SRL-spans or directly propose the text segments we require. More specifically, the extractive module can be supervised and/or evaluated against with our human annotations on the text segment start-end positions of an article. (2) The current system is only trained on unimodal (text-only) and English instruction resources. Multilingual and multimodal versions of our work could be as well an interesting future endeavors to make. (3) In this work, we mostly consider instructions from physical works. While certain conditions and actions can still be defined within more social domain of data (e.g. a precondition to being a good person might be cultivating good habits). As a result, we do not really guarantee the performance of our models when applied to data from these less physicaloriented domains."
363,Limitation is section 6 after conclusion
364,"In this section, we discuss the limitations of this work. First, this study is limited to Englishlanguage tasks, due to English being the common language of the annotators. It is possible that some conclusions from this work may not extend to task definitions written in other languages; we hope that future work can extend this analysis to a multilingual context. Further, the datasets and models used may contain biases reflecting the culture of the English-speaking population, as well as biases relating to gender, race, age, and other socioeconomic factors.
Second, in Section 5, we propose a common structured format to organize the key information for a task. We rewrite the original natural language definitions into triplets after extracting key information in it and observe improved performance. However, a complementary perspective is to write such a triplet from scratch, by filling in the blanks in triplet templates and seeing whether the improvements still hold. This directly reflects whether such
an organizing method works. Our approach serves as a starting point to demonstrate the effectiveness of using a structured and condensed definition.
Third, larger language models can be tested. The largest model we adopt is a T5 model with 3B parameters. As we observe variant behavior as model size grows, later work can further extend our analysis to larger models. Also, new emergent ability of LMs might be discovered with larger models, like mathematical reasoning with larger models following instructions. That is beyond the scope of this paper.
Last, some observations cannot be easily explained in this paper. For example, we saw that removing label information for classification tasks during training eventually also affects the model performance on generation tasks, which can be counter-intuitive and requires further exploration. Later work can pick a few points in the paper and provide deeper analysis on them."
365,Section 9.
366,"Limitation section
7 A2. Did you discuss any potential risks of your work? The topic of the paper deals only with dialogue retrieval"
367,"The study of language model in their alignment to linguistic theories are interdisciplinary and hence usually hard to find explicit connection between language model and theories. In this paper we claim that a generative model, ciwGAN, can model both phonetic and phonology features. However, the two features are learned by two ciwGAN instances from disjoint training data sets. Our finding couldn’t support or deny the following statements that are of researchers’ concern:
1. Generic GAN model can learn phonology features like ciwGAN.
2. CiwGAN can model phonetic and phonology features simultaneously from a single dataset."
368,Limitations
369,"There are two main limitations to this work. First, we focus on the “filtering” approach to controlled generation. While this formulation clarifies what a distribution is, it can be computationally expensive to do rejection sampling in practice. A promising area of future research is the application of these invariance principles to the design of large language models. Second, achieving true invariance, i.e., generalizing to any arbitrary distribution of text, is a challenging open problem. The purpose of this paper is not to solve this problem. Rather, we illustrate that controlled generation is an important application area for invariance methods. An exciting area of future work is to use prompted language models to construct well-defined distribution shift benchmarks for domain generalization methods.
Controlled text generation has the potential to have large impacts on society, both positive and negative. One potential source of risk is misuse. Although we focus on the detection and removal of toxicity, the method we developed can also be applied to the generation of dangerous and toxic content. In addition, this paper does not address other biases (such as gender or social bias) that may already be present in language models. The use of a toxicity filter may compound the problem of decreased diversity in generated text if there is a correlation between social biases and toxicity."
370,Section 7 (Limitations and Potential Risks)
371,"We acknowledge that our dataset is not huge compared to other sentence-level relation extraction datasets. However, HistRED is the first bilingual RE dataset at the document level on the historical corpus. In addition, we constructed 5,816 data instances, and our bilingual model trained on HistRED achieved an F1 score of 63.48 percent when SL is 2. This reveals that our dataset is sufficient for finetuning the pretrained language models. Also, because Yeonhaengnok is a collection of travel records, the domain is not as expansive as other Joseon dynasty records. Additional research on massive corpora covering a broader domain is required in future studies."
372,"We study a limited scope of long-form answers. The questions are either drawn from search queries or from community forums. In the real world, we will encounter many more diverse forms of long form question answering, such as answering questions in education or commercial settings. We only cover the English language, and thus our questions are topically limited to English-speaking culture.
Our evaluation of long-form answers is stationary. Annotators are provided a pre-generated output from the model without being able to interact with the model over multiple rounds. A more interactive evaluation (Lee et al., 2022) of models is a great direction for future work."
373,"Collapsed fine-tuning runs mostly occur in the low resource scenario where PLMs may easily overfit to the small data. The improvement with the proposed technique becomes marginal when the amount of training data scales up, as shown in Table 2. The other limitation is that HyPe introduces two new hyper-parameters: The noise distribution form and the scale of variance. To achieve the best performance, we may need to search for different combinations of hyper-parameters."
374,"Personalized news headline generation has the potential to improve the way users consume and understand the news. However, it is important to be aware of its limitations. The performance of any natural language generation model, including those used for personalized news headlines, is dependent on the quality and consistency of the data used to train it. Similar to personalized recommendation systems, personalized headlines have the potential to create echo chambers. If the model is trained on a biased or unrepresentative dataset, it may generate outputs that are incomplete, inaccurate, or misleading. Therefore, it is crucial to be aware of the limitations of the model and to ensure that it is trained on high-quality data to generate accurate and personalized headlines."
375,"Our current framework does not explicitly consider the temporal order via which word senses have emerged. In particular, in the data collection step, we construct source-target token pairs for each word type by randomly sampling a target sense from its sense inventory. An alternative and more realistic approach would be to sort all senses of a word chronologically by their times of emergence in history, and use the model to incrementally predict each sense of a word based on usages of its older senses. However, we found that it is infeasible to find accurate timestamps of senses in natural corpora at a comprehensive scale. Another approach is to have human annotators evaluate the plausibility of each ground-truth source-target token pairs that are automatically created in our data collection pipeline, which is a potential area for future consideration."
376,"Although our work can effectively model the variability issue in dialogue, we acknowledge some limitations of our study. Firstly, our study can work well on the approaches based on RNN, but cannot be employed to sequence models based on Transformer, which limits the generality of our approach. The reasons we analyze are as follows.
Transformer is not a good architecture for finegrained diversity. The diversity of dialogue includes three granularities of discourse level, utterance level and word level. To model diversity, models will be required to utilize the representation at time t and the relationship between the representation at time t and time t+1 to determine the representation at time t+1. Relationships are computed step by step. If we only consider discourse-level diversity, our approach and variational mechanisms are easily transferable to Transformer architectures. Because we can use the Transformer model to encode the entire historical dialogue sequence. Latent variables or summarizing variables only exist between the entire historical sequence and the responses. This will not destroy the parallel structure of the Transformer. if we employ a Transformer to model diversity at the utterance and word granularity, this will seriously damage the parallelism of the Transformer.
There are great limitations in the variational transformer models. The transformer and variational thinking is not a good match, which leads to less relevant research. The Transformer baselines we compared in the manuscript (i.e. SVT, GVT, PLATO and DialogVED) cover most of the current transformer models that combine variations. Although SVT, GVT, PLATO and DialogVED incorporate variational ideas, these models connect
all the dialogue history utterances into a consecutive sequence. It is inadvisable to model the finegrained diversity relationship in a parallel structure.
Secondly, although our methods can improve the diversity and relevence of responses, there are still gaps in fluency compared with other baselines."
377,"Because decoding symbolism is a challenging new problem, our approach and experimental results have some limitations. First, our work builds on available resources, which may have a bias toward an English/Euro-centric perspective. Second, the evaluative datasets that we curated have a limited coverage of possible symbols even within the English literary tradition. Third, as mentioned in Section 3.1, our study on situated symbolism is limited to symbolic pairs that can be found in static visual advertisements rather than longer form text or videos. Finally, while we have proposed one debiasing method based on re-ranking with PMI, which worked well for our experimental setting, there may be other methods and metrics more suited to different settings. We believe that despite these limitations, our proposed evaluative framework and
methodology offers a good starting point for further exploration."
378,"sec 6
7 A2. Did you discuss any potential risks of your work? No user; no ethic concern"
379,"Since MPCHAT sources the data from Reddit, it has the limitation that it may not be representative of the general population. First, all subreddits of MPCHAT are primarily written in English, and a significant percentage of Reddit users are from English-speaking countries. The four countries with the highest desktop traffic on Reddit are the US, UK, New Zealand, and Australia, accounting for 66% of the total user (Clement, 2022). Moreover, compared to the average US population, Barthel et al. (2016) reported that Reddit users are more likely to be male (67% vs. 49%), young (64% 18-29 years old vs. 22%), college-educated (42% vs. 28%), and politically liberal (43% vs. 24%). Therefore, MPCHAT may reflect such somewhat narrow interests, and the demographic group represented by our model may be biased toward personal conversations suitable for it."
380,In the limitations section
381,"Our work has two limitations. The first is that it may not work well for some specific types of PET. For example Prompt-tuning, which is only added on the input layer. We cannot use CLNorm but only ILProj. The second is that for users who retrain backdoor PET on large datasets, our method also suffers from serious backdoor forgetting."
382,"In section Limitations
7 A2. Did you discuss any potential risks of your work? This work was conducted in accordance with ethical principles. We use the publicly available dataset for the experiments and have no potential risks about credentials or data privacy. No human participants are involved in our experiment. Therefore, we don’t foresee any potential risk of this work."
383,"In our work, we rely on a single Mixture-of-Experts NMT model which is NLLB-200. There is a risk that our conclusions may only hold for this particular model and are specific to the way this model was trained. We believe that our findings still can be of interest to any person willing to use the NLLB200 model because: (1) It was the only publiclyavailable MoE NMT model at the time of submission; (2) It is the only model covering 202 languages and reaching SoTA results for most of those languages.
Moreover, we did not try to finetune the pruned model, which could potentially improve the results (but requires a large number of GPUs) and therefore change some of our conclusions.
This work has similar risks as the original NLLB200 models regarding the misuse of potentially wrong translations. Note that, as observed by Mohammadshahi et al. (2022b), pruning could amplify the biases already present in the full model."
384,"This work only carries out experiments using English as the base training language for domain adversarial transfer. It is possible that domain adversarial transfer has a variable effect depending on the training language from which labeled data is used. Additionally, while typologically and regionally diverse, all but one language used in our evaluation is of Indo-European origin."
385,Section 8
386,"We would like to highlight a few limitations of our work. First, we would like to point out that GENEVA is designed to evaluate the generalizability of EAE models. Although the dataset contains event type and event trigger annotations, it can only be viewed as a partially-annotated dataset if end-to-end event extraction is considered. Second, GENEVA is derived from an existing dataset FrameNet. Despite human validation efforts, there is no guarantee that all possible events in the sentence are exhaustively annotated."
387,"Orthogonal to the speed-ups discussed in this work, Earley (1970) described an extension that we do not include here, which filters deduction items using k words of lookahead. (However, we do treat 1-word lookahead and left-corner parsing in App. G.2.)
While our deduction system runs in time proportional to the grammar size |G|, this size is measured only after unary and nullary productions have been eliminated from the grammar—which can increase the grammar size as discussed in Apps. E and F.
We described how to compute prefix weights only for EarleyFast, and we gave a prioritized execution scheme (App. H.3) only for EarleyFast. The versions for EarleyFSA should be similar.
Computing sentence weights (2) and prefix weights (3) involves a sum over infinitely many trees. In arbitrary semirings, there is no guarantee that such sums can be computed. Computing them requires summing geometric series and— more generally—finding minimal solutions to systems of polynomial equations. See discussion in App. A and App. F. Non-commutative semirings also present special challenges; see App. K."
388,"The theoretical results and the algorithm should be applicable for other knowledge integration models which encode target sentences and associated textual knowledge descriptions in mini-batches. However, this paper does not extensively apply the proposed method to various knowledge integration models to explore its efficiency and effectiveness."
389,"Sec 5
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
390,Limitations Section
391,Limitations
392,Limitation Section
393,section 7
394,"Application to Other Benchmarks A central limitation of our work is that the main experiments are based on a single task-oriented dialogue benchmark. While there are multiple other natural language understanding benchmarks like XNLI, XQUAD, MLQA, and PAWS-X (Conneau et al., 2018; Artetxe et al., 2020; Lewis et al., 2020; Yang et al., 2019) that can also be used to back up our claims, we argue that this is outside the scope of this paper. The main objectives of this paper are
to first come up with a new definition of a crosslingual continual learning challenge and then to give an example using a comprehensive and realistic benchmark like task-oriented dialogue to catalyze more research in that direction.
Choice of Realistic Permutations For more realistic setups of continual learning, we need to come up with an approach to define continual learning annotation scenarios of languages. Rather than using brute force with all possible ways the languages could be annotated at different stages, a principled way would be more desired. Since it is hard to tell if there is any logic or pattern in the annotation process itself and given the sheer amount of realistic scenarios, we chose one scenario experienced by some of the users: a model is built for a user, then the user reveals that more languages are desired. We test in our work the plausibility of continual learning approaches where the sequence moves from one language to another without repetition of the same language. Working on scenarios where the data from different languages are integrated as soon as they are annotated, implying different languages for different hops, is out of the scope of this paper.
Data and Model Size Analysis In this paper, we pick certain model expansion approach variations to analyze the effect of model components (one aspect of model size) and two data distribution scenarios. However, analyzing extensively the effect of the scale of data and model size is beyond the scope of our work. We agree that different data sizes can be used and it is interesting to analyze different supervision levels such as using different proportions of the data for each language and simulating few-shot scenarios. We believe that for lowresource scenarios we need to investigate specific approaches to continual learning like meta-learning. We plan to investigate that in future work.
Application to Other Transformers Another possible limitation of our work is the restriction of the evaluation to a base model on top of M-BERT Transformers. With the advent of Transformer-based encoders as strong pillars for transfer-learning, several Transformers such as XLM-R have been proposed more recently. Although those models have been shown to outperform M-BERT on numerous downstream applications especially on low-resource languages (Conneau et al., 2020), M-BERT is still largely used due
to its reduced number of parameters. In our specific continual learning challenge, efficiency is a top concern as we are training in multiple hops and benchmarking on different models. So, M-BERT has been feasible in our use case. We leave experimenting with other Transformer-based encoders to future work."
395,"We discuss the limitations in the limitations section after the conclusion. There are no strong assumptions, claims or biases we are aware of that are not stated in the paper. We state only claims that are supported by evidence and experimental setup that we design and describe clearly in the main paper and in the supplemental material (appendix and code) (more details on the experimental setup can be found in Appendix C).
A2. Did you discuss any potential risks of your work? Not applicable. To the best of our knowledge, there is no potential risk or harm of any kind that could result from this work. Our research is just an analysis paper about cross-lingual continuous learning where we share our experiments on pre-existing approaches."
396,"The ""Limitations"" section comes after Section 9 (we did not number it).
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
397,Section 8
398,"Although we believe that controlling the forgetting in the fine-tuning phase to avoid forgetting cross-lingual/general knowledge and to reduce the negative interference from misaligned pre-training tasks and downstream tasks can benefit other finetuning settings (e.g. Multi-task setting), we have not yet investigated these settings. In the future, we will try to propose a more general method for fine-tuning a large pre-trained model across various settings."
399,Limitation
400,Section 9 (Limitations)
401,"While we unify diverse IE tasks into token-pair classification tasks and propose a simple but useful architecture to help token pairs interact with each other in an effective way, there are still several limitations that are worth discussing. Firstly, all modules in our UTC-IE are based on pre-trained language models, and experiments proves that different PLMs may influence the performance on the same dataset. Hence, our model relies on the capability of the PLM, which need a lot of GPU resources to complete the experiments. Additionally, although incorporating PlusAttention instead of self-attention can effectively reduce the memory and computational complexity from O(L4) to O(2L3), it still require a little large computation. Future work can leverage the backbone of our unification and model, and focus on the acceleration on each module."
402,"The word embeddings and language models used in this work are trained on contemporary English language, and our social contexts overly contain explicit stereotypes encoded in English. Stereotypes for a specific group can be quite different depending on the language and culture. Although out of the scope of the present work, cross-societal differences in human stereotyping have been shown to be explainable using the SCM framework (Cuddy et al., 2009). Thus, it is fair to posit that our SCMbased framework generalizes to social group biases beyond those in English. Future research is encouraged to replicate our study in non-English languages.
Furthermore, we would like to point out that there exists a catalogue of bias measurements for
word embeddings and language models in the field. However, the current catalogue is far from comprehensive in covering social groups even in the contemporary English/American context, with few resources for the intersectionality of groups and attributes (Subramanian et al., 2021; Dhamala et al., 2021). Additionally, some of these measures have been shown to fail robustness checks. Although our current work uses some of the most recently developed ECT and EQT, we believe that few, if any, of these measurements are completely sound nor complete. In our experiments for language models, we tried to measure bias for the same social group or attribute using multiple benchmarks but still found some substantial differences in results across benchmarks. Therefore, we caution against interpreting low bias measurements as evidence of complete bias removal. While developing a new bias measurement scale is not within the scope of this work, we are optimistic that the social psychological theory in which our approach is grounded provides the bedrock for the current evidence of SCM efficacy to hold on future benchmarks.
Unlike bias mitigation methods for static word embeddings, such as partial projection, the post hoc methods of debiasing for large language models can’t be trivially applied to mitigate biases for multiple social attributes simultaneously. For DPCE, the formulation allows for mitigating biases on multiple social attributes, but collecting enough sentences from each attribute that do not include any words from other attributes or neutral words (i.e. mutually exclusive sentences) was not possible with the corpora we experimented with. This problem is exacerbated as the number of social attributes grow due to the mutual exclusivity condition. For ADEPT on the other hand, the formulation did not trivially handle multiple dimensions. Hence, we employed a coordinate-descent modification in our experiments to apply ADEPT to SCM (more info in §B.4). We encourage future work to devise data-efficient methods that can mitigate biases on multiple dimensions at the same time."
403,"Biases may be present in the data annotator as well as in the data the models were pretrained on. Furthermore, we only include English-language data in our benchmark and analysis. Recent work has noted that language models may be susceptible to learning such data biases (Lucy and Bamman, 2021), thus we request that the users be aware of potential issues in downstream use cases.
As described in Appendix D.1, we take measures to ensure a high quality benchmark. There will inevitably be noise in the dataset collection process, either in the ACU writing or matching step, and high agreement of annotations does not necessarily coincide with correctness. However, we believe that the steps taken to spot check ACU writing and filter workers for ACU matching allow us to curate a high-quality benchmark. Furthermore, we encourage the community to analyze and improve RoSE in the spirit of evolving, living benchmarks (Gehrmann et al., 2021).
For reference-based evaluation, questions about reference quality arise naturally. We also note that the original Pyramid protocol was designed for multi-reference evaluation and weighting of semantic content units, while we do not weight ACUs during aggregation. As discussed above, we argue that our benchmark and analysis are still valuable given the purpose of studying conditional generation and evaluating automatic metrics for semantic overlap in targeted evaluation. We view the collection of high-quality reference summaries as a valuable, orthogonal direction to this work, and we plan to explore ACU weighting in future work."
404,Section 8.
405,"Dungeons & Dragons is a very complex game to capture completely, and there are certain aspects that FIREBALL does not take into account. For example, FIREBALL’s scenarios are recorded independently of the overarching narrative context they take place in, do not record players’ inventory, and do not account for any movement or placement on a map. Our models are not able to play D&D autonomously - but doing so is not the goal. Instead, D&D models can be used to assist and inspire the humans playing.
Our models do not take into account the gener-
ation of profanity or sensitive topics; these were filtered out post-hoc. D&D is a game played by players of all ages that often contains violent or profane descriptions, and unfiltered generations may be unsuitable for young players. There are previous instances of roleplaying games that incorporate language models being used to generate sexual content3 that would require age restrictions and content warnings.
GPT-3 may be prohibitively expensive for everyday use; in our experiments, we were unable to use the full set of data we had available for fine-tuning due to budget constraints."
406,"We note several methodological limitations with our experiments. First, since the evaluation materials were manually crafted, there is a rather small number of items (compared to the size of automatically generated NLP benchmarks). Small evaluation sets can introduce issues of statistical power (Card et al., 2020) and introduce bias based on lexical items. We feel this is not a major concern, because (1) our materials are validated by expert researchers; (2) models can be directly compared to humans in Floyd et al.’s experiments; and (3) in practice, there is enough signal to distinguish between the tested models.
Second, we only evaluate models on Englishlanguage materials, and some of the tasks were designed based on norms of communication and social interaction in Western cultures. As pragmatics can vary widely across language and cultures (Li, 2012; Rubio-Fernandez and Jara-Ettinger, 2020; Floyd, 2021; Brown et al., 2021; Dideriksen et al., 2022), an important direction for future work is to evaluate pragmatics beyond English (Ameka and Terkourafi, 2019; Blasi et al., 2022).
Third, aside from the OpenAI API models, we were only able to test models with ≤11B parameters due to limited computational resources. Models with parameter sizes between 11B and the size of text-davinci-002 could exhibit qualitatively different behaviors.
Finally, we emphasize that it is impossible to predict how models will respond to an arbitrary input. Therefore, we caution against extrapolating from our results and expecting that models will behave “pragmatically” in downstream applications. This is especially true for models behind the OpenAI API,
and text-davinci-002 in particular, for which very little is publicly known about the training protocol."
407,"""Limitations"" section after Section 6"
408,Section Limitations
409,Limitations
410,"There are several limitations to the work presented in this paper that need to be acknowledged.
First, the SREDFM and REDFM datasets are based on Wikipedia and Wikidata, which means they may not cover all possible relation types or entities. In addition, the quality of the annotations in these datasets may be influenced by the biases and limitations of these sources.
Second, the Triplet Critic is trained on a small subset of the SREDFM dataset, which may limit its ability to generalize to other relation types or languages. Additionally, the performance of the Triplet Critic may be affected by the quality of the annotations used to train it.
Third, the authors of this work are native speakers of some of the languages tackled in this work and external native speakers created the annotation guidelines. However, for some of the automaticallyannotated languages, there were no native speakers involved. Additionally, the qualitative error analysis does not include Arabic or Chinese examples, as neither of the authors of the paper is proficient in those languages.
Finally, the mREBEL system is based on a Transformer architecture, which may not be optimal for all relation extraction tasks. It is possible that other types of model, such as graph neural networks or rule-based systems, could outperform mREBEL on certain relation types or languages.
Overall, the results presented in this paper should be interpreted in the context of these limitations. Further research is needed to address these limitations and to improve the performance of multilingual relation extraction systems."
411,"Aside from the still-improvable performance of the classification models we evaluated, our work is limited in two ways: the nature of what is considered appropriate as well as the difficulties that arise during corpus creation in NLP in general.
We point to the subjectivity in perception regarding appropriateness, which is also displayed and discussed in the paper by the inter-annotator agreement. Many sociocultural factors can influence this perception within cultures, such as age, gender, education, or ethnicity. We sought to account at least for gender by including both male and female annotators for all arguments. However, we encourage further studies that focus on other factors, as we expect appropriateness to be seen differently, primarily across cultures with varying styles of debates. Since our corpus contains only arguments written in English and is annotated by native English speakers, it may also be insufficient to generalize across languages.
Moreover, appropriateness perception is likely subject to change over time. Although we collected arguments from different years, we see long-time limitations to our corpus. In general, it also depends on the expectations of the discussion participants, which are to some extent predetermined by the context (e.g., a sales pitch vs. a discussion with friends). In that regard, the context of our corpus is solely that of discussing controversial issues with strangers on the web. Finally, the size of the created corpus we propose in the paper may limit the generalizability of approaches that build on it and should be investigated further in future work."
412,"While our method demonstrates strong performance in our experimental setups, potential issues may arise when the characteristics of the available unlabeled dataset drastically change. For one example, if the scale of the available dataset is too small, the effectiveness of our clustering-based data filtering may fall drastically, leading to poor performance. Or, if the dataset is highly unbalanced, our model cannot acquire information about several specific classes. One way to compensate for this shortcoming is to use an externally imported corpus or dataset, similar to other ZSL or WSL methods. Another drawback of CELDA is that the final performance is highly dependent on the performance of the initial pseudo label, as shown in ablation. Nevertheless, as demonstrated in our ablation studies, we can remedy this issue by labeling a few samples, like active learning."
413,"In this work, we conduct research on event commonsense of open-domain dialogue systems for the first time. While achieving higher correlations with human judgments than existing baselines, ACCENT has some limitations:
First, the ACCENT framework is based on a fixed set of event relations and the commonsense knowledge in ATOMIC2020 which may fail to cover some potential event commonsense aspects. We believe augmenting the current framework with more commonsense resources is a worthwhile direction for the further improvement of ACCENT.
Second, the event-relation extractor in ACCENT framework is a T5 model fine-tuned in a low resource setting. Although the current model can yield fairly strong performance, it is an important research direction to improve the joint event-relation extraction component because the extracted tuples serve as the symbolic representation for commonsense reasoning in ACCENT framework. Since human extracted tuples are very costly to collect, we hope to explore whether we can improve this component through high-quality synthetic data construction or transfer learning in the future."
414,"Sections 6.2, 6.4 and Appendix H include the error analysis of the ACCENT framework. A separate section of “Limitations” is also included in Appendix A.
7 A2. Did you discuss any potential risks of your work? We do not include potential risks of our work, since we believe that none of the components in our model and the dataset by itself can produce offensive results. The responses generated and augmented to DECO dataset are coming from previously proposed state-of-the-art models which are trained on datasets without profanity and inappropriate utterances. Other than that, our work is pertinent to evaluation and has less feasibility of potential risks."
415,"We notice a few key limitations of our approach. Similar to what was shown by previous interpretability studies (Camburu et al., 2018, i.a.), incorporating explanations comes with some penalty on in-distribution accuracy when there is no spurious cue. This penalty decreases as model size increases, potentially because it is less challenging for larger models to generate good explanations. The second limitation is that our artificially constructed training set may not reflect the strength of the studied spurious cues in the real world. In our main experiments, we focus on the case where one spurious cue is perfectly correlated with the target label. For further exploration, we can study the alternative setting where there are multiple weak spurious cues instead of a single strong one. Finally, our work here is limited by the scope of the experiments. We only experiment with generative LMs and binary classification tasks. Also, because of resource constraints, we only consider four datasets and eight types of spurious cues (including datasetindependent and dataset-specific ones). Additional experiments using a wider variety of spurious cues and datasets would help to shed light on how our method generalizes to other scenarios."
416,"Despite the success of our CAME optimizer in training large language models with memory efficiency, there are still some limitations that need to be addressed in the future.
Our proposed memory-efficient optimizer introduces additional computation costs for the nonnegative matrix factorization of the instability matrix in comparison with Adafactor. We observe, however, that the training time of CAME increases only slightly in our experiments. Beyond that, CAME exhibits minor performance degradation in large-batch training of the BERT-Large model versus LAMB, which allows for further improvement in the future. Meanwhile, it is possible to conduct further experiments on other models in other fields, such as Computer Vision and Reinforcement Learning, thereby exploring the effectiveness of CAME
training under more application scenarios. As a final point, it would be much more helpful to provide an in-depth theoretical analysis of CAME to improve comprehensiveness of the paper."
417,"Systematically exploring more prompts Our work uses CoT prompting structure inspired by Kojima et al. (2022). However, small variations to the prompt structure yield dramatically different results. We also do not explore how different CoT prompts affect stereotypes, focusing only on the SOTA “let’s think step by step.” While we qualitatively observe that ""faster"" prompts (think quickly, think fast, not think step by step) are less toxic,
comprehensive work on understanding and evaluating different zero-shot CoT’s for socially relevant tasks is an avenue for future work. For example, priming CoT generation with “Let’s think about how to answer the question in a way that avoids bias or stereotyping” may reduce biased outputs (Ganguli et al., 2023). We also do not explore bias in few-shot settings. Models are very sensitive to few-shot exemplars (Zhao et al., 2021; Perez et al., 2021); furthermore, exemplars trivialize intrinsic bias benchmarks, and are similar to finetuning (Akyürek et al., 2022). Carefully measuring bias in few-shot CoT with respect to these confounds is an avenue already explored by future work (Turpin et al., 2023).
Limitations of Bias Benchmarks Prior work has shown flaws in existing fairness benchmarks; measuring fairness is itself an open problem. Benchmarks often-time have differing conceptualizations of bias (Blodgett et al., 2021), leading to contradictory results (Delobelle et al., 2022; Cao et al., 2022; Goldfarb-Tarrant et al., 2021). We ran our analysis across 3 separate benchmarks, including an extrinsic evaluation of bias in question answering (Parrish et al., 2022). We also conduct a manual, qualitative analysis of failures to tie our quantitative findings to examples of representational harm against protected groups. We believe the general agreement across our analyses mitigates the flaws of each individual benchmark, but the limitations and stated goals of each should be carefully considered when interpreting results."
418,Section 8
419,"Section 4.4, Section 5, Limitations"
420,"Our method cannot discover entities that are mentioned without disappearing contexts since we utilize such disappearance signals.
Although our experiments focused solely on Wikipedia entities, they did not sufficiently cover certain entities, such as stores and food products, for which disappearance is important for us. Therefore, it is necessary to collect and conduct experiments specifically for these entities."
421,"the section following the conclusions.
7 A2. Did you discuss any potential risks of your work? not any known risks."
422,"We discuss the limitations in the ""Limitations"" Section before ""Ethics Statement"" Section."
423,Limitations
424,"One limitation of our study is that we only evaluated our method on the T5 architecture. Further experiments on other architectures could be useful to determine the generalizability of our findings. Additionally, as in previous SOTA, our model also did not produce better results for the hotel domain, even though it did improve performance in general. We have attempted to explain why this domain is more difficult, but more research is needed to fully understand the reasons for this variability and to create methods that can improve performance across all domains."
425,"Limitations (Section 8)
7 A2. Did you discuss any potential risks of your work? It is unlikely that research on zero-shot dialogue state tracking poses any foreseeable risks."
426,"More Complex Domains. We choose the BW for its intuitive and simplistic nature (with one kind of object, three types of actions, and three predicates). Although the generalization experiments suffice currently to challenge transformers, realworld situations are more complicated. With the improvement of the algorithms, the need for a better arrangement of actions domains is emerging. In time, it could be beneficial to include several domains with various levels of complexities.
Balance Between Rigor and Natural. For now, the synthesized English sentences are generated using a fixed template. Whilst being accurate without ambiguity, the resulting text is still quite formal. It would be valuable to add variety in the expressions without losing precision.
Better Solvers. As our demonstrations suggest, current LMs still fall short on the generalization tests. We hope that our work will pique interests in the community towards reasoning about actions and change, and challenge approaches to undertake the fundamental reasoning tasks."
427,"the ""limitations"" section after ""conclusion"" section"
428,"Keeping narration coherent within a movie is crucial for visually impaired people to enjoy the movie. In this work, we move a step forward for this target by setting the ground-truth texts in the Movie Clip Narrating task as narration paragraphs and providing longer video clips as inputs. However, how to ensure description coherence across different clips within a movie has not been studied in this work. This requires a higher-level comprehending ability of models to process the whole movie and connect different plots. We leave this to our future investigation."
429,"Limitations are presented in a standalone Limitation section (after the Conclusion and before the References).
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
430,In Section 6
431,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
432,"""Limitations"" section.
A2. Did you discuss any potential risks of your work? Not applicable. This paper aims to identify risks."
433,"Although our methodology is agnostic to the specific sequence-processing models used as encoder and decoder, its main purpose is to be used together with LPLM. This clearly limits its use to institutions and users with access to computing facilities able to handle such models. Indeed, although the encoder of HSN can successfully be trained while keeping the weights of BERT (or any other LPLM) frozen, a feature which reduces training time, the GPT-based decoder does need to be fine-tuned to learn how to interpret the inferred symbols. This last points further limits the use of our methodology to “not-so-large” decoder models. That being said, fine-tuning HSN decoders via LoRA (Hu et al., 2022a) is an exciting option that can possibly solve (or help with) this last issue. We shall explore using LoRA with HSN in the near future."
434,Limitations
435,"The Goldstein scale is a widely-used measure of the conflictual versus cooperative nature of interactions between countries (Goldstein, 1992). The scale was created by a panel of international relations experts who ranked descriptions of interactions. It was initially created to score action categories in the WEIS event ontology (McClelland, 1984) and was later adapted to CAMEO (Schrodt, 2012).
The Goldstein scale applies only to the action category of an event (e.g., “fight” or “trade”). Thus, two different “fight” events, which might involve two different pairs of actors, occur at different times, or differ dramatically with respect to the number of associated fatalities, will still be assigned the same Goldstein value.The Goldstein scale is thus a poor measure of a conflict’s perceived “intensity”, as it ignores much of the information that contributes to that perception. Recent work in conflict studies, for instance, operationalizes “intensity” primarily using casualty counts (Chaudoin et al., 2017; Zhong et al., 2023), which the Goldstein scale ignores entirely.
In Tab. 1, we show the empirical distribution of assigned Goldstein values alongside the empirical distribution of casualty counts in a dataset of conflict events. The Goldstein scale is very coarsegrained; while it ostensibly ranges between −10.0 and +10.0, only a small number of discrete values ever occur, with many action categories assigned the same value. For the purpose of measuring conflict intensity, a finer-grained and more contextual scale is desirable."
436,Limitations
437,"This study is restricted to the Americas. Therefore the results from this paper can not be generalized, as different indigenous communities or nations might have different pasts. Also, all opinions expressed by the interviewed people are exclusively personal and in should not be interpreted as the general stand of the communities. As discussed in the paper, the main aim of this work is
not to provide a normative for MT researchers. We rather provide a set of questions and open topics that should be considered when performing MT work with indigenous languages. Nevertheless, we also provide general and broad non-normative recommendations that should be carefully applied to the concrete case of each community."
438,"This benchmark compiled and analyzed existing resources collected from diverse methods and domains. Although we demonstrated how careful use of these resources could transfer well to other resources, along with a manual analysis of a varied set of corpora, we cannot guarantee the quality of each resource or validate the methods that the original authors used to create them. We explore each dataset’s linguistic properties in Appendix B. However, we encourage a deeper exploration of the quality of individual resources by researchers that speak the 12 languages included in this benchmark and corresponding data loaders.
Additionally, the human evaluation performed in this study was limited in scope and served primarily to validate the findings by automatic metrics. A more extensive evaluation with more annotators evaluating more sentences would be beneficial in order to draw further conclusions.
Furthermore, some of the resources discussed in this paper were automatically aligned. Although Neural CRF models in English have been shown to yield high-quality alignments (Jiang et al., 2020), other alignment algorithms such as TF-IDF scoring (Nelken and Shieber, 2006) have been shown to result in a high number of false positives (Xu et al., 2015). Future work could include realigning automatically aligned corpora using an embeddingbased sentence alignment model trained on manually annotated alignment data (Jiang et al., 2020). We will continue updating this benchmark as updates are made to the underlying datasets, and new
multilingual resources are released."
439,"Section 9; Page 9
A2. Did you discuss any potential risks of your work? Not applicable. The primary contribution of our work is a benchmark for multilingual text simplification consisting of parallel complex-simple sentences. We provide a collection of existing resources for text simplification and empirical analysis of their utility in fine-tuning and few-shot experiments. We do not release any models, and our work is a collection of existing datasets. In effect, we are not releasing any potentially harmful models or resources that merit a discussion of risk."
440,"The section after conclusion, following the instruction in the Latex file.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
441,"DP training of large models is compute-intensive, requiring per-example gradients and large batch sizes (Li et al., 2021; Subramani et al., 2021). This renders the training of such models difficult and not easily accessible to everyone.
DP-SGD takes records to be single training examples, which in this paper’s experiments
9
correspond to single user utterances. That setup prevents the trained model from revealing much information about any given single utterance, but it may still allow information to leak that is repeated across multiple utterances (Brown et al., 2022).
For both the baseline method and our two-stage method, we trained our model to approximately match the true distribution of private user utterances, ppriv(x), to the extent that this was possible under a differential privacy guarantee. Of course, there are many ways to measure the quality of an approximation, and different approximations are appropriate for different tasks where it might be important to preserve different properties of ppriv(x). The one-stage baseline approach implicitly aims to achieve a low cross-entropy, by applying DP-SGD to the log-likelihood function. In contrast, our two-stage approach aims to encourage an approximation that also roughly preserves the marginal distribution over semantic function types. We did not investigate more direct ways of encouraging such an approximation, for example, one-stage DP-SGD with a modified objective function that explicitly evaluates the marginal distribution in addition to the log-likelihood.
Finally, we trained an approximate model of ppriv from which we can draw utterances to inspect and annotate. But we must acknowledge that ppriv is not the ideal distribution to approximate. Even if we were able to actually use private utterances to improve the system, we would not necessarily want to draw them directly from ppriv. Rather, we would want to select them by active learning—selecting the private user utterances that would be most useful to inspect or to include in the annotated training data. Thus, when training our model by DP-SGD (using either the one-stage or two-stage procedure), we could upweight or upselect the private utterances that appear useful in this way—resulting in a differentially private model that generates useful synthetic utterances. Specifically, traditional active learning by uncertainty sampling (Settles, 2012) would select utterances where the semantic parser was uncertain what to do. We would also want to select utterances where the system suspected for other reasons that it did not do the right thing—because it classified the user’s request as a functionality that the system did not yet support, or because the user objected in some way to the system’s response. We
have left experiments on this setup to future work."
442,In Limitaion Section
443,"In this paper, we propose a continued pre-training method to inject knowledge into large pre-trained language models. There are eight V100 GPUs involved in each pre-training experiment and the whole pre-training process takes 5 days for the base-size model and 13 days for the large-size model, in primary settings. These numbers in data upscaling settings are significantly greater (30 days for the large-size model). Despite its advantage in reducing resource need in inference time, KILM is both time-consuming and computationally resource-consuming during training time.
Similar to any model-based generation system, KILM could be prone to generating factually incorrect statements with regard to entities. These statements might also be prone to be biased based on ethnicity, race, and sexual orientation."
444,"We point out potential limitations and ethical concerns of this work. Limitation: Data and Modeling The dialogues
in our dataset are made by playwright, which are slightly different from daily chat. Second, the automatic evaluation metrics for the response generation task can not perfectly reflect the interactiveness of dialogue system. Lastly, Our autoregressive generative model simply add the segment embedding to the inputs. Similar to the position encoding in transformer, our coarse method does not make good use of the segmentation, and lacks interpretability. Ethics: Copyright and Licensing The data source are publicly available TV series. Its collection and annotation procedure is designed for videogrouned dialogue understanding and generation purpose, and does not involve privacy issues. Following LSMDC (Rohrbach et al., 2016) and MovieNet (Huang et al., 2020), we will polish an agreement and release TV shows content under very strict conditions but will open-source all the scrawling code, pretrained features and sampled images."
445,"While we hope that our approach to data collection can serve as a benchmark for future NLP studies beyond peer review, we deem it equally important to explicitly outline the potential risks and limitations of NLPEER and NLP for peer review in general. Our discussion below encourages future research in ethics and applied NLP for peer review; many of our considerations are not specific to peer review and are equally relevant to the applications of NLP in general.
From the data perspective, we deem it important to clearly state what NLPEER is not meant for. Our data collection campaigns for ARR-22 and COLING-20 included an explicit disclaimer on the risks of author profiling on the peer reviewing data; we stress that such applications violate the intended use of NLPEER. Furthermore, NLPEER enables a wide range of new NLP assistance tasks for peer review. Yet, we encourage future studies of NLP for peer review to reflect carefully about the potential risks and benefits of new task defi-
13https://intertext.ukp-lab.de/
nitions atop of peer reviewing data in general and NLPEER specifically. For instance, the full automation of peer review, i.e. the generation of review reports given a paper, bears risks and dual uses.
Considering diversity in NLP datasets, we stress that even NLPEER only covers a fraction of peer reviewing across all fields of science, and more data needs to be collected to enable fully representative NLP-based study of peer review. Due to the genre standards of scientific publishing our dataset only covers papers and reviews in English language. Multilingual scholarly document processing is overall poorly represented in NLP, and constitutes a promising avenue for future research. While our resource contains data from a wide range of domains, research in arts and humanities is under-represented due to the poor data availability. The trend towards open science and the adoption of responsible data collection practices (Dycke et al., 2022) might bring reviewing data from previously unexplored domains and languages into NLP. We stress that any direct comparison based on our corpus would need to take into account reviewing practices and guidelines adopted by the respective communities. Specifically, potential biases resulting from the donation-based collection for ARR-22 and COLING-20 should be taken into account.
From the task side, we highlight that implementations and resulting models presented here are meant to exemplify the proposed tasks, determine their technical feasibility, and serve as a starting point for developing future NLP for peer review assistance systems. As such, the provided implementations have limitations: for example, sentence-level pragmatic labels derived from structure-based ARR forms might contain noise since ARR forms group text on section level; guided skimming does not make use of implicit links, and explicit links are mostly based on line numbers and quotes, limiting the recall. Since we did not perform extensive hyperparameter search and tuning of the models, our results should not be interpreted as a claim towards superiority of a particular model, approach or reviewing system.
We highlight that high intrinsic task performance does not necessarily translate into the extrinsic utility of NLP support in real-world reviewing environments. We thus deem it crucial to study the factors that affect the success of NLP assistance for peer reviewing. This includes the study of the humanmachine interaction dynamics and its desiderata;
for example, review score recommendations should be accompanied by explanations. We encourage extensive research on risks of biases and errors in NLP assistance models; for instance, a review score prediction model might learn undesirable biases against certain types of papers. Review writing assistance implemented in a real reviewing system should always be accompanied by carefully designed guidelines and policies.
Finally, we invite the community to reflect on the potential societal consequences of the individual NLP assistance tasks, even if NLP models accomplish them well. To provide an example, our newly introduced guided skimming task assists during the effortful and time-intensive, yet crucial step of reading the paper under review. Although the guided skimming for peer review task models an intermediate step during reading and is intended to serve as an additional point of reference during the iterated skimming steps of peer review, such a technology might encourage reviewers to read only the paragraphs suggested by the model. We argue that this risk of ""lazy reading"" is independent of the technology at hand; a reviewer that is institutionally incentivized to perform reviews as quick as possible, may read a paper superficially and settle with heuristics for their assessment (Rogers and Augenstein, 2020) regardless of assistance. A greater risk, however, may be imposed by potential biases and errors of a guided skimming model, which could distract less experienced reviewers. While recent work on skimming assistance in scholarly articles (Fok et al., 2023) suggests a mature and reflected interaction of users with highlight recommendations and possible errors, this needs a specific investigation for the use case during peer review. On the other hand, a critical reading model may serve as a useful point of reference to guide reviewers to employ more scrutiny on the parts of the paper appropriate for this specific paper type, which ultimately may improve reviewing quality. We assess that the opportunities provided by the introduced review assistance tasks outweigh the potential risks in general, yet highlight that a targeted study is necessary to substantiate this assessment.
References 2008–2023. Grobid. https://github.com/ kermitt2/grobid.
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: A pretrained language model for scientific text. In
Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615–3620.
Lutz Bornmann. 2011. Scientific peer review. Annual Review of Information Science and Technology, 45(1):197–245.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.
Liying Cheng, Lidong Bing, Qian Yu, Wei Lu, and Luo Si. 2020. APE: Argument pair extraction from peer review and rebuttal via multi-task learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7000–7011, Online. Association for Computational Linguistics.
Alexandra Chronopoulou, Matthew Peters, and Jesse Dodge. 2022. Efficient hierarchical domain adaptation for pretrained language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1336–1351, Seattle, United States. Association for Computational Linguistics.
Nils Dycke, Ilia Kuznetsov, and Iryna Gurevych. 2022. Yes-yes-yes: Proactive data collection for ACL rolling review and beyond. In Findings of the Association for Computational Linguistics: Empirical Methods in Natural Language Processing 2022, pages 300–318, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
Nils Dycke, Edwin Simpson, Ilia Kuznetsov, and Iryna Gurevych. 2021. Assisting decision making in scholarly peer review: A preference learning perspective. arXiv:2109.01190.
Raymond Fok, Hita Kambhamettu, Luca Soldaini, Jonathan Bragg, Kyle Lo, Marti Hearst, Andrew Head, and Daniel S Weld. 2023. Scim: Intelligent skimming support for scientific papers. In Proceedings of the 28th International Conference on Intelligent User Interfaces, IUI ’23, page 476–490, New York, NY, USA. Association for Computing Machinery.
Tirthankar Ghosal, Sandeep Kumar, Prabhat Kumar Bharti, and Asif Ekbal. 2022a. Peer review analyze: A novel benchmark resource for computational analysis of peer reviews. PLOS ONE, 17(1):1–29.
Tirthankar Ghosal, Kamal Kaushik Varanasi, and Valia Kordoni. 2022b. HedgePeer: A dataset for uncertainty detection in peer reviews. In Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries, JCDL ’22, pages 1–5, New York, NY, USA. Association for Computing Machinery.
Tirthankar Ghosal, Rajeev Verma, Asif Ekbal, and Pushpak Bhattacharyya. 2019. DeepSentiPeer: Harnessing Sentiment in Review Texts to Recommend Peer Review Decisions. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1120–1130, Florence, Italy. Association for Computational Linguistics.
Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer. 2022. DEMix layers: Disentangling domains for modular language modeling. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5557–5576, Seattle, United States. Association for Computational Linguistics.
Xinyu Hua, Mitko Nikolov, Nikhil Badugu, and Lu Wang. 2019. Argument mining for understanding peer reviews. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2131–2137, Minneapolis, Minnesota. Association for Computational Linguistics.
Rob Johnson, Anthony Watkinson, and Michael Mabe. 2018. The STM Report: An overview of scientific and scholarly publishing. International Association of Scientific, Technical and Medical Publishers, The Hague, Netherlands.
Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine van Zuylen, Sebastian Kohlmeier, Eduard Hovy, and Roy Schwartz. 2018. A dataset of peer reviews (PeerRead): Collection, insights and NLP applications. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1647–1661, New Orleans, Louisiana. Association for Computational Linguistics.
Neha Kennard, Tim O’Gorman, Rajarshi Das, Akshay Sharma, Chhandak Bagchi, Matthew Clinton, Pranay Kumar Yelugam, Hamed Zamani, and Andrew McCallum. 2022. DISAPERE: A dataset for discourse structure in peer review discussions. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1234–1249, Seattle, United States. Association for Computational Linguistics.
Ilia Kuznetsov, Jan Buchmann, Max Eichler, and Iryna Gurevych. 2022. Revise and Resubmit: An Intertextual Model of Text-based Collaboration in Peer Review. Computational Linguistics, 48(4):949–986.
Carole J. Lee, Cassidy R. Sugimoto, Guo Zhang, and Blaise Cronin. 2013. Bias in peer review. Journal of the American Society for Information Science and Technology, 64(1):2–17.
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.
2020. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234–1240.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.
Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. 2021. On the stability of fine-tuning BERT: misconceptions, explanations, and strong baselines. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
Barbara Plank. 2016. What to do about non-standard (or non-canonical) language in NLP. arXiv:1608.07836.
Simon Price and Peter A Flach. 2017. Computational support for academic peer review: A perspective from artificial intelligence. Communications of the Association for Computing Machinery (ACM), 60(3):70–79.
Chenglei Qin and Chengzhi Zhang. 2022. Which structure of academic articles do referees pay more attention to?: perspective of peer review and full-text of academic articles. CoRR, abs/2209.01841.
Anna Rogers and Isabelle Augenstein. 2020. What can we do to improve peer review in NLP? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1256–1262, Online. ACL.
Nihar B Shah. 2019. Principled Methods to Improve Peer Review. Available online at http://www. cs.cmu.edu/~nihars/publications/ survey_peerreview_niharshah.pdf (accessed on 22.01.2022).
Chenhui Shen, Liying Cheng, Ran Zhou, Lidong Bing, Yang You, and Luo Si. 2022. MReD: A meta-review dataset for structure-controllable text generation. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2521–2535, Dublin, Ireland. Association for Computational Linguistics.
Lukas Stappen, Georgios Rizos, Madina Hasan, Thomas Hain, and Björn W. Schuller. 2020. Uncertaintyaware machine support for paper reviewing on the interspeech 2019 submission corpus. In Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020, pages 1808– 1812. ISCA.
Ivan Stelmakh, Nihar B Shah, Aarti Singh, and Hal Daumé III. 2021. Prior and prejudice: The novice reviewers’ bias against resubmissions in conference peer review. Proceedings of the Association for Computing Machinery (ACM) on Human-Computer Interaction, 5:1–17.
Andrew Tomkins, Min Zhang, and William D. Heavlin. 2017. Reviewer bias in single- versus double-blind peer review. Proceedings of the National Academy of Sciences, 114(48):12708–12713.
Richard Walker and Pascal Rocha da Silva. 2015. Emerging trends in peer review—a survey. Frontiers in Neuroscience, 9.
Jingyan Wang and Nihar B. Shah. 2019. Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’19, Montreal, QC, Canada, May 13-17, 2019, pages 864–872. International Foundation for Autonomous Agents and Multiagent Systems.
Weizhe Yuan, Pengfei Liu, and Graham Neubig. 2022. Can we automate scientific reviewing? Journal of Artificial Intelligence Research, 75:171–212.
Rongsheng Zhang, Yinhe Zheng, Xiaoxi Mao, and Minlie Huang. 2021. Unsupervised domain adaptation with adapter. arXiv:2111.00667."
446,"In this paper, we introduce Z-Code++, a robust pre-trained model tailored for summarization tasks. However, it should be noted that there are certain limitations to our model. Firstly, the model is not versatile enough as it is specifically designed for summarization. It is unclear whether it performs well on other natural language tasks. Secondly, while FiE can handle document summarization, there are still significant potential for improving cost efficiency. Lastly, the evaluation of multilingual summarization is not thorough enough due to the limitations of available datasets. We intend to address these limitations in our future work."
447,"Refer to Section ""Limitations"" at the end of paper.
7 A2. Did you discuss any potential risks of your work? This paper mainly focuses on improving performance on downstream tasks with external knowledge. We have not found risks of the potential use cases."
448,"A major component of RSMI has been developed with the concept of randomized smoothing which is known to be certifiably robust within a radius of a ball around an input point. Though we have proved the robustness for the perturbed samples within this given ball, there is no theoretical guarantee that a perturbed sample will always lie within the ball. Accordingly, our study is limited to empirical validation of the effectiveness of RSMI, although it has theoretical robustness within a L2 norm ball as shown in §2. Nevertheless, certified robustness is a critical research direction for robust and reliable deployment of NLP systems to address undiscovered attacks. In our future work, we will explore the theoretical understanding of the certified-robustness of NLP systems and textual adversarial examples in-depth."
449,"Despite introducing meta learning for domain adaptive few-shot misinformation detection, we have not discussed the setting of cross-domain adaptation with multiple source datasets to further improve the performance for identifying early-stage misinformation. Due to the lack of early-stage misinformation data, we limit our choice of the target domain to COVID-19, which may hinder the generalization of the proposed method to other domains. Additionally, the proposed method does not leverage efficient approximation or first-order meta learning methods to reduce the computational costs in training MetaAdapt. As such, we plan to explore multi-source few-shot misinformation detection via efficient meta learning as future work."
450,7 Limitations
451,Section 6
452,"In the Section Limitations, we discuss the limitations of our work."
453,"Although our approach, MIME is empirically observed to outperform several other competitive baselines, we do observe some limitations in the modeling capacity towards MEMEX. As depicted in Table 6, there are three possible scenarios of ineffective detection – (a) no predictions, (b) partial match, and (c) incorrect predictions. The key challenges stem from the limitations in modeling the complex level of abstractions that a meme exhibits. These are primarily encountered in either of the following potential scenarios: • A critical, yet a cryptic piece of information
within memes, comes from the visuals, which typically requires some systematic integration of factual knowledge, that currently lacks in MIME.
• Insufficient textual cues pose challenges for MIME, for learning the required contextual associativity. • Potentially spurious pieces of evidence being picked up due to the lexical biasing within the related context."
454,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
455,"Computing Resources. Despite the surprising performance it achieves, our framework needs to be applied to large language models like GPT3 or PaLM. Inference with these models costs more time and budgets than fine-tuning models like RoBERTa (Liu et al., 2019).
Faithfulness. Although DIVERSE can significantly improve the accuracy of final answers, we still cannot guarantee that the reasoning paths produced by the language models are 100 percent faithful. This is the key challenge and future direction for this line of research (chain-of-thought reasoning).
More Training Data. DIVERSE needs more labeled data with well-annotated reasoning paths to construct diverse prompts, and it also needs a training dataset for supervising the verifier. However, from another point of view, this limitation can also be regarded as a contribution that studies how chain-of-thought reasoning can be further improved if we have more training data than just a few exemplars.
Human Evaluation of Reasoning Steps. We use human evaluation to measure the quality of the intermediate steps in reasoning paths since few current works provide reliable frameworks to evaluate the quality of reasoning steps."
456,"Section 9
7 A2. Did you discuss any potential risks of your work? Left blank."
457,"In this study, we provide a survey of reasoning with language model prompting. We discuss the related surveys in Appendix A.1 and will continue adding more related approaches with more detailed analysis. Despite our best efforts, there may be still some limitations that remain in this paper.
References & Methods. Due to the page limit, we may miss some important references and cannot afford all the technical details. We mainly review the cutting-edge methods within two years (mostly in 2022) in §3, mainly from the ACL, EMNLP, NAACL, NeurIPS, ICLR, arXiv, etc., and we will continue to pay attention to and supplement the latest works.
Benchmarks. Most of the reasoning benchmarks mentioned in §5 are gathered and categorized from the experimental part of mainstream works. The definition and boundary of each task may not be accurate enough. Besides, our work may miss some kind of reasoning tasks such as reasoning with generics (Allaway et al., 2022), default inheritance reasoning (Brewka, 1987), non-monotonic reasoning (Ginsberg, 1987) in NLP, and will try our best to fulfill this gap.
Empirical Conclusions. We give detailed comparisons and discussions of language models and prompts in §4, and list some promising future directions in §6. All the conclusions are proposed and further speculated upon empirical analysis of existing works which may not be macroscopic enough. As the field evolves faster, we will update the latest opinions timely."
458,"Section 8 (Limitations)
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
459,"Last section (""Limitations"", no number)
A2. Did you discuss any potential risks of your work? Not applicable. Left blank.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 4
3 B1. Did you cite the creators of artifacts you used? 4
3 B2. Did you discuss the license or terms for use and / or distribution of any artifacts? 4"
460,"Section 6 Limitations
A2. Did you discuss any potential risks of your work? Not applicable. We currently do not identify any potential risks inherently associated with our work."
461,"Though CACR shows significant gains in compositional performance, results are limited in their exploration of only one pre-trained model and compositionality dataset. A significant risk of models is their tendency to be biased by distributions in their training data; vision-language models are
not free from this flaw, but we see our work as teaching VLMs to learn better structured representations rather than memorizing spurious correlations in data. We remain far from solving the vision-language compositionality problem, so biases must continue to be actively mitigated."
462,"limitations, 4.1 Datasets and 4.4 Evaluations."
463,"The experimental methodology employed in this study for both contrastive explanations and NMT is not directly extensible to languages other than English, due to the scarcity of resources such as models and annotations.
The datasets employed in this study to evaluate contrastive explanations across various linguistic paradigms are restricted to sentences that possess a well-defined structure. As a result, it is possible that the conclusions drawn may not be generalizable to the broader distribution of sentences.
Lastly, it should be noted that the method proposed in this study should not be used as a definitive explanation of model predictions in any other context. It is recommended to use the method as a debugging tool and should be employed in conjunction with other methods to gain a comprehensive understanding of model predictions."
464,"While we have argued that our approach to collecting counterfactual data via DISCO is agnostic to the particular task and language, we emphasize that the experiments we report are limited to English and the task of NLI. Given that English is a high-resource language, there could be additional challenges (e.g., finding the tools needed for making span selection) in re-creating our pipeline for other languages. We also emphasize that our data generation experiments were carried out using only a single LLM, namely the publicly available GPT3
model first reported in Brown et al. (2020). As with the related studies we cite (e.g., Liu et al. (2022)), given the high costs associated with largescale prompting, we are unable to ablate all parts of our data generation pipeline (e.g., the effect of systematically alternating prompting styles at scale, alternative span extraction techniques). Similar to virtually all experiments involving LLM prompting, such differences could affect the results and quality of the resulting augmentation datasets. Similarly, given the high costs of human annotation, we have limited our human evaluation to around 500 random instances (each involving 3 annotators), which follows other related studies."
465,
466,"Section ""Limitations"""
467,"The present study has certain limitations that should be acknowledged. Firstly, the RST parsing task itself is known to be highly complex and challenging, and achieving high accuracy in this task is not guaranteed. Although we have utilized the most high-performing parser, there is still room for further improvement in the RST parsing performance, which could potentially enhance the downstream summarization task.
Another limitation pertains to the size of the data used for human evaluation. Due to the nature of long document summarization and the length of
the original texts (often spanning several pages), scaling up the evaluation process, such as through crowd-sourcing, becomes difficult. Consequently, we are only able to evaluate a limited number of 10 documents, which may not be fully representative of the entire dataset.
Furthermore, another potential risk in our study is the limitation in obtaining an unlimited number of training samples. The data samples investigated are often small subsets of real-world data or may exhibit certain biases, which may not accurately reflect the distribution of real-world data. Although we have verified the effectiveness of our model using highly diverse and heterogeneous datasets from different domains, it is important to note that the model’s performance on the specific dataset of interest may not be as robust as its performance on unseen real-world data.
Finally, both training and evaluating the models require significant computational resources. Despite our attempts to optimize the computation by replacing the original attention calculation with the RST attention tensor (as demonstrated in the ablation experiment), we have not achieved satisfactory results. The high computational costs pose a challenge, as they result in increased human and material resources required for the model."
468,Section 7
469,"In an unnumbered section after Section 7
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
470,"1. Computational cost: For our experiments, we used almost 850h of GPUs. In future research, we could try to lower this cost by experimenting with prompting for LRC task in few-shot scenarios, which would also help when conducting the task for low-researched languages.
2. Language: Our experiments were conducted just for the English language. Thus, and with the advantage derived from minimal prompting of being language independent, in further research we would like to expand our experiments to multilingual datasets such as the ones from (Wachowiak et al., 2020).
3. Original dataset limitations: In line with (Lang et al., 2021), we found some misleading annotations in CogALexV dataset. This not only decrease the performance of the model but can also lead to hard-to-detect biases. Once again, few-shot tuning would decrease the annotation cost, making it possible to train with, although less, better-annotated examples. Additionally, synonymy remains the most difficult relation to capture, a more fine-graded annotation of the different kinds of synonyms could improve their classification.
4. Domain dependence: The limitation spotted by (Necsulescu et al., 2015) is persistent in our model. A richer domain annotation would be advised to better research domain bias in the LRC task.
Aknowledgements
Supported by the Spanish project PID2020113903RB-I00 (AEI/FEDER, UE), by DGA/FEDER, by the Agencia Estatal de Investigación of the Spanish Ministry of Economy and Competitiveness and the European Social Fund through the “Ramón y Cajal” program (RYC2019-028112-I), and by the EU research and innovation program HORIZON Europe 2021 through the “4D PICTURE"" project under grant agreement 101057332."
471,page 9 Section Limitations
472,"Although our approach has demonstrated advantages in producing faithful factual error corrections, we recognize that our approach is not capable of correcting all errors, particularly those that require domain-specific knowledge, as illustrated in Table 3. Therefore, it is important to exercise caution when applying this framework in user-facing settings. For instance, end users should be made aware that not all factual errors may be corrected.
In addition, our approach assumes evidence is given. Although this assumption is also true for applying our method to summarization tasks since the source document is treated as evidence, it does not hold for automatic textual knowledge base updates. When updating these knowledge bases, it is often required to retrieve relevant evidence from external sources. Hence, a reliable retrieval system is required when applying our method to this task."
473,Section 8.
474,"The event schemas generated by our model are not directly comparable to those generated by previous work that utilized a close-domain ontology. As a result, we were unable to adopt the same metrics and
evaluate our schemas on type-level event prediction tasks as in (Li et al., 2021; Jin et al., 2022; Dror et al., 2022). Grounding the events generated by the LLM into one of the types in the ontology could be added as a post-processing step to our model, but this would require some ontology-specific training data, which goes against our principles of designing an open-domain, portable framework.
Our event schema does not explicitly represent entity coreference, entity relations, and entity attributes. The current schemas that we produce focus on events and their relations, with entity information captured implicitly through the event descriptions. For instance, the See Medical Professional event is described as “The patient is seen by a doctor or other medical professional” and the proceeding Obtain Medical History event is described as “The medical professional obtains a medical history from the patient”. The “medical professional” and “patient” are implied to be coreferential entities in this case, but not explicitly connected in the schema graph.
Our approach is also quite distinct from prior work (Rudinger et al., 2015b; Wang et al., 2017; Li et al., 2021; Jin et al., 2022) that consider a probabilistic model as an implicit schema where the schema graph, or event narrative chain can be sampled from. Probabilistic schema models have the advantage of being adaptive and can be conditioned on partially observed event sequences, but are hard to interpret. We make the conscious design decision to generate explicit, human-readable schema graphs instead of black-box schema models.
Finally, our model relies on the usage of LMs, which have been observed to sometimes show inconsistent behavior between different runs or when using different prompts with the same meaning (Elazar et al., 2021; Zhou et al., 2022a). However, quantification of consistency has only been done for factual probing tasks while schema generation is a more open-ended task. For example, in our experiments on everyday scenarios, we observe that the model could generate distinct schemas for Buying a (computer) mouse based on whether the purchase was done online or in person. This variance is often benign and we leave it to future work to take advantage of such variance and possibly aggregate results over multiple runs."
475,"Section 7
7 A2. Did you discuss any potential risks of your work? Our model produces human-readable graphs that describe typical sequences of events, which is a form of event-related commonsense knowledge."
476,"The primary weakness of ‘fairly’ averaging model weights for XLT is that sensible checkpoints need to be averaged. This manifests, for instance, in hyperparameter ablation for ZS-XLT on TyDiQAGoldP. TyDiQA-GoldP is a complex task with merely 3,696 training instances that observes unusual training dynamics. On such a dataset, the early checkpoints often underperform models that (nearly) have converged, especially if training utilizes low learning rates with schedulers. Here, SRCDEV could be used to weed out underperforming checkpoints, such that CA then always exceeds the baseline that performs model selection on sourcelanguage validation data. Whenever the English training portion is sizable – like in our other tasks – checkpoint averaging is consistently beneficial. Our experiments also demonstrate that XLT behaves differently by task. Averaging checkpoints consequently might affect other tasks differently like, for instance, document classification that reason about long contexts or retrieval tasks like Tatoeba that jointly require sequence- and word-level semantics. Another dimension we did not explore further due to a limited compute budget is how to ensure best that monolingual models are aligned for run averaging. For instance, it may not be required or even desirable to keep classifiers frozen throughout the second step of our proposed training curriculum (§3), as we would ideally also want to average out idiosyncratic noise of the original classifier."
477,section 6
478,"after conclusion selection, page 9
7 A2. Did you discuss any potential risks of your work? we do not see any ethical implications or risks of our work"
479,"A general limitation of this line of work is that most of the results are primarily confined to artificial datasets. Although such formal languages provide us with a controlled setting and clarity regarding the precise nature of the problem, the relation to practical tasks remains unclear. Hence, while our results highlight the contrast in the performance between the two types of architectures, its precise implications on real-world tasks remain unclear.
There are two negative results that do not support our hypothesis. (a) All the experiments discussed in the main paper are on strings of fixed lengths. We conducted some experiments on tasks with variable length sequences which in some sense have low sensitivity. The tasks can be seen as a variable length extension of sparse parities and sparse majorities. Unlike the fixed length setting, we found both LSTMs and Transformers perform similarly on those tasks. See Section E.1 in the Appendix for more details. (b) Although we found Transformers to consistently converge to low sensitivity functions in the case of Boolean functions, we did not find similar behaviour on sentiment classification datasets such as SST and IMDB (see Section C).
A caveat with empirical studies such as this is that the results depend on the hyperparameters and other aspects of the experimental setup. While we have tried to be as thorough as possible with hyperparameter tuning, there is always a chance that the results or behaviour could differ for some hyperparameter."
480,"The current work marks the first step towards intentconditioned counterspeech generation, and as we noted, even though our model excels in fluency, a larger and more diverse dataset paired with knowledge grounding is necessary to improve and ensure factual correctness. Although the annotators kept the quality of counterspeech as high as possible, it is possible that this data is not at par with other datasets that are annotated by more skilled NGO operators, as is the case with the Multi-Target CONAN dataset (Fanton et al., 2021). A more large-scale annotation of our dataset with higher instances for under-represented target communities would hence be beneficial to learn more accurate distributions of every counterspeech class. Another limitation of the current work is that it exhibits a slightly higher-degree of toxicity compared to the baseline. It, therefore, pertains to accounting for lowering the amount of toxicity present in the generated counterspeeches as future research. Lastly, humor in counterspeech is a very subjective topic, and inspite of including only a few datapoints from that class as compared to the others in our dataset, it is likely that QUARC could generate vague and/or offensive text under the pretext of humor. We intend on keeping the dataset private and only provide access for research and educational purposes."
481,"Similar to the limitations of existing selection methods, our method needs a reasonable feature embedding for accent representation in order to effectively target accents. MFCC features are not the best choice to represent accent information. Some accents may be more difficult to represent than others. This also lowers fairness scores for such accents. For instance, in one of our experiments where Manipuri accent was paired with Rajasthani
or Assamese accents, we observe that acquiring a fair subset using any selection strategy is challenging (see Tab. 5). Although, FLMI was able to achieve a higher TF score than others, it was relatively lower than other accent pairs (see Tab. 3 and Tab. 4). This is due to the fact that the pairwise similarity scores of utterances within the Manipuri accent are lower than other accents. The lower pairwise similarity scores lead to lower marginal gains during greedy maximization and are a consequence of poor feature representations due to insufficient information being encoded about the Manipuri accent. On another note, a risk associated with the targeting ability of DITTO is that it could be misused to create models that are unfair to certain populations. For future work, evaluating the performance of DITTO on larger datasets and other diverse settings (e.g. out-of-distribution accents) will be interesting."
482,Section 8
483,"There are a few limitations to the current framework. Firstly, Verify-and-Edit works the best for open-domain question-answering tasks that require complex reasoning. Less complex datasets or commonsense datasets that do not require knowledge retrieval may not result in high improvements. Secondly, it is most ideal to edit a group of mostly incorrect samples, which we try to select by using consistency. Thus, our method is reliant on the consistency method’s performance and its abilities to separate correct and incorrect predictions. Most often, it can demonstrate a larger improvement with a more challenging set of examples.
To address these limitations, we plan to work on reducing the noise brought in the rationale-editing stage and utilize more knowledge resources, such as knowledge bases, as a follow-up."
484,"• Our work focuses on data from one platform, Kialo, which contains cleaner and higher quality arguments from a diverse range of topics and domains. How our approach performs on data from other platforms or more specialized domains (e.g. deliberations about policy) has to be investigated in the future.
• The vast majority of data available is English which makes conducting and evaluating multilingual experiments not feasible even with language transfer (see Appendix Section A.1).
• The dataset used in the training and evaluation has only one correct position although there might be multiple suitable parents. Given the large scale of the data and the huge number of nodes per tree, annotating all suitable parents would’ve require a very-large-scale unfeasible annotation. This could be investigated in future-work with the support of our models.
• The design of our annotation study does not take into consideration the structure of the tree. This might have made the task more challenging for the annotators. Reconstructing or representing the tree structure without revealing the actual parent (since the majority of the candidates are close relatives) is challenging when limiting the candidate parents to 10. Further refinement of the annotation study is left for future work along with the inclusion of the structure in the modeling.
• Although small models are shown to perform relatively well and are recommended to use when computation resources are limited, the models that perform, in our experiments, on par with humans are large models that are costly to train. Employing parameter efficient fine-tuning methods might be of interest here. • We use only manually designed templates as a simple approach that required no extra training or engineering. How the results compare to using automated template/prompt engineering methods is also left for future work. Including prompt-based fine-tuning might be also of interest to investigate in combination with contrastive training although language modeling training would require more computational resources. • Our task definition excludes the prediction of pro/con relation as less important, but the pro/con template information might be useful for this. More evaluation and analysis is needed to verify that. • Extra analysis that was out-of-scope to include in this paper might be of interest: e.g. the effect of topic, the degree of a node, and semantic similarity to siblings on model or human performance."
485,"One potential limitation of this review is our selection bias which may affect the representativeness of the included papers. Another limitation is the potential differences in methodologies across the
papers we reviewed, which makes it difficult to draw generalizable conclusions. Different studies use different experimental settings and methods for measuring feature importance, which could also impact the comparability of the findings across the included studies. Furthermore, we acknowledge the potential publication bias which might lead to an overestimation of the impact of different factors, as studies with statistically significant results may be more likely to be published than those with non-significant results."
486,"Limitations section
7 A2. Did you discuss any potential risks of your work? During literature review no potential risks could be identified. Additionally, our review focuses on the potential benefits of different factors rather than on the risks."
487,"Unnumbered section after conclusions.
7 A2. Did you discuss any potential risks of your work? Left blank."
488,"In the ”Limitations” section.
7 A2. Did you discuss any potential risks of your work? We only adopt publicly open resources including data and packages. Those resources are commonly used in corresponding domains. Our work does not introduce additional risk."
489,"The limitations of this work are: (1) In this work, we expect to consider more realistic and more applicable settings for class-incremental NER. Therefore, we consider the Unlabeled Entity Problem and provide a more realistic benchmark based on 66 fine-grained entity types. However, there remain some more serious situations unsolved in this work. First, the entity classes in each step might not be disjoint. For example, a new entity type ""Director"" might be included in an old entity type ""Person"". This problem is referred to as the coarse-to-fine problem existing in emerging types of NER. Second, the amount of data or labeled data introduced in each step can also be
limited, referring to the few-shot class-incremental problem. Therefore, the proposed method can be further improved to solve these problems. Third, the current version of the proposed method cannot handle the nested NER or contiguous NER problems. In the current version, we simply followed typical works in NER and adopted the sequence labeling scheme to model the NER task, which is not suitable for more complicated NER tasks. Nonetheless, as the proposed representation learning and re-labeling methods are agnostic to the formation of representations, we believe our method can also be adapted to a span-level version, which might be future works. (2) The proposed method is a rehearsal-based method that requires keeping exemplar sets for each class. Although the number of exemplars for each class is really small, we believe there can be more data-efficient solutions that totally avoid the need of memorizing data and also achieve good results. (3) The proposed method includes several hyper-parameters such as the entity threshold Tentity, relabeling threshold T hNN and T hproto. Although we have shown that the choice of thresholds is relatively robust (Sec.5.4), it still requires efforts to explore the most suitable thresholds when applied to other datasets or situations. There can be further work to improve this problem by formulating an automatic threshold searching strategy."
490,"""Limitations"" section.
A2. Did you discuss any potential risks of your work? Not applicable. We study cross-lingual NER task on public datasets, our work doesn’t have potential risks."
491,Limitations Section
492,"In principle our method is applicable in many domains, for example, one could use a biomedical knowledge graph instead of ConceptNet in a relevant domain. However, in this paper we only evaluate the quality of our approach in argumentative tasks which require commonsense knowledge. Our approach is unsupervised, but its performance depends on the quality of the used knowledge graph and SBERT model.
Similarly, we only evaluate CCKGs for English data, although our approach is not limited to English if one uses multilingual SBERT models (Reimers and Gurevych, 2020) or a multilingual knowledge graph.
Finally, our approach is purely extractive and hence, is limited by the coverage and quality of knowledge graphs. However, improving knowledge graphs is an active field of research and hence, high-quality and high-coverage knowledge graphs are to be expected. Furthermore, our extracted CCKGs could be augmented with generative models if coverage in the knowledge graph is not sufficient. However, that would reduce the interpretability that our approach provides."
493,The proposed AMI component is effective in the low-data regime but cannot be generalized to all cases. Future work will investigate the role of syntax in the structural regularization of attention and the extension of the proposed approach to discrete augmentation.
494,"Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
495,"A limitation of this study is that it is based solely on papers published in the ACL Anthology, which primarily represents the international Englishlanguage NLP conference community. While the ACL Anthology is a reputable source of NLP research, it should be acknowledged that a significant amount of research is also published in other venues such as AAAI, ICLR, ICML, and WWW. Additionally, there are also vibrant local NLP communities and venues, often publishing in non-English languages, that are not represented in the ACL Anthology. As a result, the conclusions drawn from our experiments may not fully capture the global landscape of NLP research and further work is needed to explore the diversity of sub-communities and venues across the world.
This work focuses on the aggregate trends of citing older work in NLP, but does not investigate the reasons for lower citation of certain older papers. There may be various factors that contribute to this, such as the accessibility to these older papers, the large number of recent papers, the applicability of these old works, and the technical relevance of the older work. Determining the relative impact of each reason is a challenging task. Therefore, more research is needed to fully understand the underlying mechanisms that influence the citation of older NLP papers.
This study aims to investigate the factors that contribute to the citation of older works in the field of NLP. We have analyzed different factors such as the mean age of citation, diversity in the age of citations, venue of publication, and subfield of research. Our results indicate that these factors are associated with the citation of older works, but it should be noted that these associations do not establish any causal relationship between them.
Lastly, it is important to note that citations can
be heterogeneous and can be categorized in different ways. For example, some classifications of citations include background, method, and result citations. However, certain citations may be more important than others, as shown by previous research such as ""Identifying Meaningful Citations"" by (Valenzuela-Escarcega et al., 2015)."
496,Section 9
497,We discuss the limitations of our work in section 8 Limitations.
498,"We hereby discuss the current limitations of our work: (1) The SIMMC-VR dataset, similar to the SIMMC 2.0 version, focuses on shopping scenarios (clothing and furniture purchasing domains), one of the most common everyday activities that virtual reality could enable users to do from anywhere, anytime. We have not tested whether the models would generalize to domains outside of the shopping experiences, thus we cannot speak to the transferability of our results to environments with very different visual properties than what our virtual environments provide. (2) In this dataset, we hand-design several possible dialog acts that we assume are common for human buyers, as well as their associated scenarios. This may not exhaust all the possible interactions a shopper can do with the assistant. However, we emphasize that the coverage should be sufficient for common shopping experiences. Additionally, although most of our proposed subtasks should be modeling generic user-assistant multimodal dialogue interaction and thus could be transferred well to other domains, the (our) domain specific MM-DST may not generalize as much. Nevertheless, they should still be transferable to similar (shopping) environments. (3) The audio of the SIMMC-VR videos are generated by automatic TTS, which may fall short to represent the natural human speech. However, we do not foresee this causing problems for multimodal dialog modeling, which this work mostly focuses on."
499,"A limitation of this approach is the trade-off between completeness and noise in the training data. While our method using keywords to extract text from Wikipedia is effective, IMPLICATION likely contains redundant sentences that cannot improve the model’s logical reasoning capability. A better rule-based or neural model might be able to extract a better corpus with potentially higher computational costs. Additionally, using POS tagging limits the application of this approach to languages with well-defined POS taggers. Switching to a more universal semantic tagging system (Abzianidze and Bos, 2017) can potentially alleviate this."
500,"Our synthetic pre-training dataset was automatically generated from manual templates, which inspite of dataset creation scalability and low cost,
may limit the diversity of the generated SQL queries. Our model, MultiTabQA, requires improvement in numeracy understanding and numeric operations. Real numbers are especially challenging and the model may not be able to correctly generate all the digits of the number correctly rending the generated cell incorrect. Furthermore, large input tables pose a challenge as the input sequence may get truncated beyond the model’s maximum sequence length. This has practical limitation in the size and number of input tables which the model can accommodate before truncation which leads to incorrect answers."
501,Section Limitations
502,"We presented Parallel Context Windows (PCW), a simple approach that alleviates context window restrictions for any off-the-shelf LLM, without ad-
ditional training. We showed the potential of this method on a variety of models and datasets. With that, our method does have some limitations.
The number of context windows has a limit, and needs to be predetermined. Similarly to vanilla in-context learning, the number of examples to include in the prompt must be selected beforehand. For PCW, it is also required to select the number of context windows, B. In this paper, most of the results are for B = 3. We experiment in Appendix C with the choice of B. The results are task dependent, but at a high level we find that there are diminishing returns around B in the range of 5 to 7. We leave further investigation of how to effectively benefit from more windows for future work.
Not effective for all types of tasks. As discussed in Section 3, PCW shows impressive gains in ICL for tasks such as multi-class tasks classification as well as information extraction. However, for some tasks, PCW does not improve performance. This might indicate that some tasks are not suited for parallel processing. Section 4.2 demonstrated that PCW is more suitable for cases where the input text could be divided into few independent inputs, but it remains an open question as to whether tasks, such as long text generation, would benefit from PCW."
503,"Linguistic variation Our results are highly dependent on the target language and its morphology. For example, word boundaries might seem like an obvious choice for dynamic segmentation, and in fact they achieve the best performance in English and Vietnamese. However, for some languages like agglutinative Finnish, whitespaces are less frequent, which is detrimental to model performance. Explicit word boundaries are not available for all scripts. For example, in Chinese characters, or in modalities other than text like speech or vision, there is no obvious equivalent to whitespaces. However, segmentation based on stochastic re-parameterisation, subword tokenizers and spikes in conditional entropy overcomes these limitations.
Contiguous segments In its current formulation, dynamic pooling only allows for merging contiguous segments of tokens in a sequence. However, this is not ideal for morphology types like Hebrew where morphemes are discontinuous: vowels are interspersed between consonant roots for inflection. Moreover, future works should consider higher levels of linguistic structure than words, such as dependency trees, for pooling. In this case, discontinuous segments may be necessary to handle non-projective syntactic dependencies.
Independent boundary decisions The decision to emit a boundary at time step t depends on previous boundaries only indirectly through the hidden representation of the first Transformer block, as this preserves the efficiency of the boundary predictor. Instead, a recurrent model could be explicitly conditioned on previous boundary decisions, which however would negatively affect the time complexity of the language model.
Work contribution of authors
The idea of training the models with pooling of variable-length segments was discussed among the authors while Jan Chorowski was at the University of Wrocław. Experiments were performed by Piotr Nawrot while he was employed in a research grant at the University of Wrocław, under the supervision of Adrian Łańcucki and Edoardo M. Ponti. The manuscript was written by Piotr Nawrot, Adrian Łańcucki and Edoardo M. Ponti."
504,"In this paper, we propose DocREDHWE and introduce a new metric to select the most robust and trustworthy model from those well-performed ones in DocRE. However, all data in DocRED are sampled from Wikipedia and Wikidata, which indicates that training and test data in DocRED can be identically and independently distributed (i.i.d. assumption). The i.i.d. assumption impedes our demonstration of the intuition: A model with a higher MAP will obtain a higher F1 score on the test set. Due to the i.i.d. assumption, models can succeed in obtaining a higher F1 score by greedily absorbing all correlations (including spurious correlations) in the training data. To strictly demonstrate the intuition, we need a test set that exhibits different and unknown testing distributions. In addition, expanding the research scope to a cleaner Re-DocRED and analyzing the role of unobservable wrong labels are also crucial and interesting ideas. We leave them as our future work."
505,Limitations section after the conclusion section.
506,In Section Limitations.
507,"We identify some limitations and risks with our current work that can be addressed in future work. • The current work is mainly applicable to log-
ical entailment problems, where one needs to solve a classification problem of whether a goal can be proved, disproved, or neither proved nor disproved based on a theory. Future work can extend LAMBADA to non-classification cases, e.g., where one needs to apply logical reasoning to answer questions such as “What color is Fiona?”. • The current work assumes all the rules are given as input and the rule set is small enough to be included in the prompt. Future work can extend LAMBADA to the cases where not all the rules are provided as input and part of the knowledge has to come from the LM itself, as well as the case where not all the rules can be included in the prompt due to the limitation in the prompt length. • The current work is limited to deductive reasoning with the modus ponens rule; future work can expand the applicability of LAMBADA on datasets with other types of rules such as proof by contradiction, disjunction elimination, etc. • The calls made to the LM modules in LAMBADA are dependent on the value from the previous call. That is, we need to wait for the results from one call before we decide what the next call must be. Since making batch calls to the LMs is typically easier and faster, future work can find ways to implement LAMBADA with batch LM calls. • While we showed that LAMBADA is more efficient than SI in terms of the number of inference calls it makes to the LM, it still requires many more calls to the LM compared to approaches such as CoT, hence increasing the required computation and cost."
508,Limitations section on p9
509,After the Section 6 Conclusion
510,Section Limitations
511,"Our automatic error type system, KAGAS, has room for improvements. Although we got high human acceptance rate for the error type classification results of KAGAS, Our coverage of error types is about 80% to 90% (Table 4). Currently, our system rely on the Kkma POS tagger for Korean. We believe that the improvement of a POS tagger will enable KAGAS to define a more detailed error type classification with high coverage and reliability. Also, there could be other ways (or more efficient ways) to define and classify Korean grammatical edits. However, we would like KAGAS to be viewed as the first step towards the effort of making an automatic annotation tool for Korean GEC, which, though not perfect, have meaningful contributions to the field in its current form.
Future Directions. Currently, the 14 error types of KAGAS is focused to be as specifc as possible, while respecting both statistical characteristics of Korean language and incorporation into a reliable, deterministic system with high agreement of human evaluation. However, definment of a more richer error type classfication system derived from KAGAS such as differentiating between the typographical and phonetic errors would be an important future direction for our research, as both are defined as SPELL errors on our current system. It would require solving additional challenges of accurately disambiguating a writer’s intention behind errors on a grammatical aspect. Another possible future direction would be applying data augmentation techniques on our datasets to boost the size of the training examples and obtain evaluation metric accuracy gains."
512,"Section 8.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
513,"The neuro-symbolic rule learning presented in the paper can handle most generic text-based games. Only in a few specific use cases, additional training of the AMR parser would be required. Since AMR is used for symbolic representation for text-based games, the vocabulary of the extracted triples is limited by the vocabulary of PropBank semantic roles. For applications in a very specific kind of domain where the predicates and entities do not match with this pre-defined vocabulary (for example, specific financial, legal domains, etc.), the AMR semantic parsing engine needs to be retrained first on such specific data before using it for rule learning. However, even in the cases where the testing environment requires additional rules, NESTA allows human-in-the-loop debugging to conveniently add them making it adaptable to generic environments."
514,Section 7 is the limitations section
515,"In this paper, we developed an approach for evaluating how large language models encode social attitudes about gender, and we applied that approach to evaluate BERT-base-uncased. Because the goal
of this paper was ethical in nature, limitations on the generalizability of our approach and findings entail ethical risks. With this in mind, we discuss both limitations and risks in this section. We first discuss limitations related to data, and then discuss those related to models and tasks. For both data and models/tasks, we consider general limitations of our approach, as well as more specific limitations of how we applied the approach here."
516,"Just as it is not possible to create a single benchmark for all language understanding (Raji et al., 2021), it is not possible to create a single, definitive dataset that relates language choices to social attitudes. Human experimental data is always limited by practical considerations and cannot test every condition of theoretical interest; e.g., in the role nouns dataset, there were no conditions with gender neutral names, while in the singular they dataset, there was no comparison to neopronouns (e.g., xe/xem). Additionally, because past work has found that model preferences may vary across similar linguistic contexts (Delobelle et al., 2022), it may be the case that BERT’s predictions would correlate differently with human responses on other variations on the stimuli. Relating model preferences to human behaviour will always be limited by the amount of human data that can be obtained.
Moreover, datasets are always situated in a perspective, emphasizing some people or views over others (e.g., Barrowman, 2018; Chasalow and Levy, 2021). For example, both datasets we consider focus on first language English speakers from the United States, and the specific relationship between social attitudes and linguistic choices captured by those datasets may not generalize outside that context. Languages other than English may have extensive grammatical gender systems, or classification systems that include social roles, among other linguistic devices, which interact to yield rich mechanisms for expressing social attitudes around gender. Even within English speakers in the US, how language signals social attitudes about gender may vary across groups and social contexts. (In fact, Papineau et al. (2022) found that Republicans with progressive social attitudes about gender did not use more gender neutral forms the way Democrats did; other, more fine-grained differences likely also exist.)
Additionally, relating social attitudes about gen-
der to linguistic choices requires some method for measuring social attitudes. Since conceptions of gender are so diverse and culturally variable, no single measurement would be appropriate for all contexts. For example, in one of the datasets we used, a survey for measuring social attitudes about gender asks participants to evaluate statements about stereotypical social roles of men and women, which are likely culturally specific (e.g., “A father’s major responsibility is to provide financially for his children”) (Baber and Tucker, 2006).
In evaluating language technology, a focus on associations between linguistic choices and social attitudes limited to particular linguistic and cultural contexts risks prioritizing the social knowledge from those communities, and imposing that in other communities when language technology is deployed. To support the creation of inclusive technology, the research community will need to prioritize generation of datasets like the two we drew on here – i.e., ones explicitly connecting linguistic choices to social views – across more languages and cultural contexts."
517,"There are also several limitations related to the models and tasks considered. First, we evaluated only one model (BERT-base-uncased), and more work is needed to understand if and how our specific results generalize to other masked language models. This is especially important given that past findings comparing gender bias in masked language models with different architectures and model sizes are mixed (e.g., Sharma et al., 2020; Jentzsch and Turan, 2022; Tal et al., 2022).
Additionally, we only considered the task of masked language modeling. We made this choice because psycholinguistic datasets that pair linguistic choices with results of social attitude surveys are rare, and those available to us used language tasks that were most appropriate for evaluation on the task of masked language modeling. However, given that bias on the intrinsic task of masked language modeling may not relate to (extrinsic) bias on downstream tasks (Delobelle et al., 2022), our results (such as BERT’s language communicating conservative attitudes) may or may not carry over to downstream tasks. In the future, our approach for relating task predictions to social attitudes could be used to evaluate downstream tasks (such as coreference resolution), once appropriate human data is
available.
Another limitation has to do with differences in the information considered by language models, as opposed to humans, in choosing to use gendered vs. gender neutral language. In both tasks we study, participants and language models evaluate the appropriateness of gender neutral forms based only on contextual cues to the subject’s gender, especially gender associations of names. However, when deciding what to say, people can also take into account the referential gender(s) (e.g., the pronouns someone uses, Cao and Daumé III, 2020) of people being referred to. For example, if a person knows that someone named Michael uses feminine referential gender, they would likely refer to her with gender neutral or feminine forms (e.g., congressperson, congresswoman) but probably not masculine forms (e.g., congressman). Focusing on evaluation tasks (and language models) which do not consider information about referential gender risks encouraging the development of language technology that performs worse on data from (binary and nonbinary) trans people, and contributing to their erasure. Note that in the Michael example there are still linguistic choices (i.e., between congressperson and congresswoman), which may reflect social attitudes. Future work should study the relationship between linguistic choices and social attitudes in models which can take referential gender into account, while also recognizing the social implications of language choices that respect referential gender.
Finally, while this work developed an approach for evaluating the social attitudes about gender communicated by language models, it does not propose any approaches for improving language models or adjusting the attitudes they communicate. Past work in NLP has discussed different approaches for how pronouns might be handled in language technology (Lauscher et al., 2022), and has developed gender neutral re-writing tasks (Sun et al., 2021; Vanmassenhove et al., 2021), which replace gendered pronouns and words like fireman/firewoman with gender neutral variants. Contrasting with standard fairness approaches in NLP that remove information about gender from language technology, work in feminist HCI has discussed approaches for the treatment of gender in language generation which are intended to challenge existing norms and stereotypes, and bring about social change (Strengers et al., 2020). Additionally, work on lan-
guage reform has discussed the challenges involved in working towards gender-inclusive language, including how explicitly gendered and gender neutral variants can often take on different meanings (Ehrlich and King, 1992; Zimman, 2017). Future work in NLP should consider each of these lines of research, discussing when and how it may be desirable for models to use or represent language that signals gender, and what attitudes those language choices communicate."
518,ENDERANKER is only tested on DIASUMFACT. Further tests on more datasets are required to establish its general applicability.
519,section Limitations between conclusion and References
520,"First, our work applies hand-crafted action labels as operation hints, which leads to some limitations to represent more complex operation steps. For the future work, we can use a generator instead of a classifier to generate a more flexible set of operation prompts, making them more representative and meaningful Secondly, due to the high controllable generation of our approach, if our approach yields a wrong operation step prediction, it would further mislead the intermediate step generation. To eliminate the drawback where inaccurately generated operation prompts would mislead the next step, we can apply a verifier (Cobbe et al., 2021) to evaluate the reliability of the generated operation prompts. When the reliability is low, we ditch the operation prompt to prevent it from guiding the model into an incorrect path."
521,Section 6
522,"Section 7
7 A2. Did you discuss any potential risks of your work? The data we used are publicly available and do not contain this issue."
523,Limitations section (unnumbered) after the conclusion
524,Limitations Section
525,"4.4, 4.5, Limitation
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
526,Line 587 - 620
527,"Limitations
7 A2. Did you discuss any potential risks of your work? We use the open data to pre-training our model."
528,"This work utilizes generative models trained on large volumes of data, to generate supplemental training data for named entity recognition systems. We do not address any biases, or filter generations of the underlying paraphrasers when using their generated data. This can bias the fine tuned models towards underlying biases of the generative system.
While we do not test or correct the paraphrasing systems for biases, we do not find any evidence for the models deviating unfairly from the underlying training data in any of our human evaluations of the paraphrases.
We recommend human review, and automatic filtering of the generations when applying techniques based on generative models to critical applications, to ensure the black box paraphrasing does not introduce, or exacerbate the biases in existing training datasets."
529,"Estimating human utility is expensive. The core of our work is built on conducting extensive human evaluations, to understand how well lay humans can solve tasks with rationales. In order to replicate these findings to other tasks, one would require the same scale of human evaluations, which are expensive and tedious. These tasks are also difficult to explain to lay crowdworkers, because of which several rounds of turking are required to reach good annotator agreements. Given these shortcomings of human evaluation, a reliable metric that estimates human utility is necessary.
Generating generalization questions is not completely automated. Even though we prompt GPT-3 with varied demonstrations to generate generalization questions of each type, we still have to manually filter them (via crowdsourcing) to obtain a cleaner set of questions. Furthermore, in order to obtain gold answers of these questions, we generate answers by prompting GPT-3 again, which also requires further validation. A completely automated method of generating these questions would lead LM updates to be independent of human involvement.
Even though GEN-U has a better correlation with human utility, the correlation is still low. To train models to produce free-text rationales with more human-utility through Quark (Lu et al., 2022), it is first necessary to have an accurate metric that can serve as a reward function/scoring metric for human utility. In this work, we found that human generalization is good indicator of human-utility. However, given that Quark requires frequent reward scoring, it is infeasible to use human annotations for the same. Our proposed automatic metric GEN-U that simulates human generalization has a good correlation with human utility (better than task accuracy, or BERTScore), but overall, it still has a low correlation with human utility of rationales. Developing a score with better correlation with human utility (perhaps even a stronger version of GEN-U) will decrease the effect of this limitation and lead to training that further increases human utility of generated rationales."
530,"The current framework bears several limitations. First, although a common strategy in the related literature (Brunner et al., 2020; Ek and Wirén, 2019; Jannidis et al., 2018) which we also adopted, the binary annotation at the token-level is limiting. With this schema, the focus is not on speakers’ utterances or turns, but on DS sequences. A subsequent issue is that consecutive turns by different characters are considered as one DS sequence if there is no ""O"" labeled tokens between them. One solution could have been to mark the start and end of a DS turn while paying attention to handle imbricated narration (ie. incises). However, this would have required significant more re-annotation efforts, which we left for a future research cycle within the proposed framework.
Second, because of copyright issues the corpus contains excerpts exclusively from a specific period, 1830-1937. Thus, the models were trained and tested on a specific type of literature and may not generalize well to other forms of narratives, in particular modern and contemporary. In this direction, the curation of the test corpus could benefit from more literary insights considering that the evaluation showed high variance of the performance over chapters. This could help to better determine the application scope of the models, and which kind of narratives require further work.
With regard to the deep neural network baselines, we did not perform an extensive parameter search and model optimisation. This could have further improved the results. However, performances on recognizing full DS spans were clearly lower than token-level metrics, which had most likely other causes. Regarding the evaluation, although we adopted ZME scores from page segmentation to have more qualitative insights, there are still other aspects we have not quantified and could be particularly relevant. For instance, does the model tend to miss the beginning, the end or some other specific parts of a DS sequence? We tried to capture some of these phenomena through our manual analysis, but it is challenging to apply it at scale
without introducing methods to automatically compute metrics."
531,"the Limitations section
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
532,Section 6: Limitations
533,In the Broader Impact Statement.
534,"Despite achieving high translation performance on various language pairs, LOBEF has some limitations, coming from the nature of processing UTF-8 byte sequences.
Speed: As shown in Table 8 in the appendix, the inference times for byte-based models are higher when compared to subword-based models. It is also worth noting that we use the same amounts of model parameters for a total of 6 Transformer encoder layers and 6 Transformer decoder layers for all models in comparison. As shown in Table 7, byte-based models can effectively reduce the amounts of parameters for the embedding layers comparing to the subword-based models, leading to faster training time as shown in Table 8. However, as indicated by Xue et al. (2021), by adding more encoder layers, we can construct byte-based models with comparable amounts of parameters as subword-based models, and these larger byte-based models still require much longer time for training than subword-based models.
Extremely Low-resource Languages: The performance of byte-based models on extremely lowresource languages (e.g., 2.7K training data for Lao-English) is still lower than subword models especially in the multilingual setting. We suspect that byte-based methods require a relatively larger number of training data in order to aggregate information from a combination of byte tokens, comparing to subword-based models that explicitly maintain a subword vocabulary.
Extra Preprocessing: The Byte-WSF model requires an extra preprocessing step that precomputes the attention mask corresponding to the words in each sentence. This adds a slight overhead before training, while training the Byte-WSF model is as fast as the Byte model, as both model use the same Transformer architecture. However,
for languages (e.g., Chinese) that do not have whitespaces to indicate the word boundary, we may rely on an off-the-shell word segmentation tool to preprocess the text."
535,"Limitations section towards the end, unnumbered"
536,"""Limitations"""
537,"Section ""Limitations"""
538,"A well-known shortcoming of transformers is the computational complexity in self-attention lay-
ers (Vaswani et al., 2017). Since the number of required calculations grows quadratically with the length of the input, transformers become prohibitively slow on very long sequences. An unfortunate side effect of processing inputs at the characterlevel is that internal sequences become much longer, so token-free transformers run into these efficiency problems much earlier than subword-based models. Figure 7 illustrates this problem by contrasting the runtime of all poetry generation systems when generating a single quatrain. Even ByGPT5 (small), the smallest model in terms of number of parameters (cf. Table 1) and the fastest token-free transformer, is only marginally faster than GPT-2 (medium), which is almost five times larger. Tay et al. (2022) propose a solution to this problem for transformer encoder blocks by applying a neural pooling operation over input embeddings before feeding them into the model, which could be extended to decoder blocks in future work. Alternatively, Libovický et al. (2022) propose a two-stage decoding architecture in which the transformer decoder operates on character blocks that an additional LSTM model (Hochreiter and Schmidhuber, 1997) decodes into individual characters.
Another shortcoming is that our poetry generation systems can only generate a single poetic form, i.e., quatrains. In general, poetry is a very diverse form of language and stanzas can be of arbitrary length, so this is a serious limitation. In future work, we thus plan to extend our implementation
of style-conditioning to variable length poems. In particular, one could encode a rhyme scheme not as a single special token, but as an arbitrary series of letters indicating which verses rhyme with each other. Alternatively, our current systems could be used to generate longer stanzas through a sliding window approach, i.e., generating one verse at a time with the last three verse as context.
Further, our human evaluation has limitations due to its relatively small scope. We only have a limited number of annotators and only consider a subset of all style combinations. Nevertheless, we have achieved moderately high to high agreement on all tasks, and we have an additional human evaluation of German poetry in Appendix B, which points to the same conclusion.
Lastly, QuaTrain is limited in that it consists of pseudo-quatrains, which are not real quatrains and often have missing contexts. Nonetheless, as can be seen in Appendix D, models trained on QuaTrain are still able to generate meaningful poetry. In future work, we plan to improve the quality of our dataset by obtaining real quatrains from additional sources such as the EighteenthCentury Poetry Archive (Huber, 2022)."
539,"Following instructions, we add Limitations after Conclusion."
540,"Limitations section
7 A2. Did you discuss any potential risks of your work? Our work does not involve any sensitive data or tasks, and there is no potential risk.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 4
3 B1. Did you cite the creators of artifacts you used? 4
7 B2. Did you discuss the license or terms for use and / or distribution of any artifacts? We will discuss the license at GitHub upon publication."
541,"7: Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
542,"In the Limitations section, after the conclusion.
7 A2. Did you discuss any potential risks of your work? Because we are extending existing work on existing datasets with new methods, our work introduces no additional risk beyond existing models for solving those problems. These problems are also specifically far from production uses, so risk is generally low."
543,
544,"Professional coders are trained to find sufficient, as opposed to exhaustive, evidence for each code. Our Profee coders were instructed to find all the evidence for each code. However, given the large number of notes in some MIMIC encounters, they might only manage to annotate most of the evidence. For Inpatient, there might be more bias among coders towards finding sufficient evidence: namely, there were many cases in which one coder found evidence that another had not, but during the adjudication process, both coders agreed it should be included. Thus, although we have opened the door to automatic evaluation of evidence extraction systems, some metrics, such as recall on our dataset, might underestimate the true recall of a system.
We observed inconsistencies and human errors while cleaning up the data. Coders sometimes only annotated partial evidence, leaving out modifiers like ""acute"", ""moderate"" and ""bilateral"". For example, we consider ""bilateral pleural effusions"" as the correct evidence but only ""effusions"" was highlighted, and for ""weakness in his lower extremities"", only ""weakness"" was highlighted. Another source of error is due to the limitation of the annotation tool which does not support highlighting and linking discontinuous spans of text as a single evidence for a code. As a result, some evidence may contain extra tokens between the correct evidence tokens and others may miss part of the evidence when the supporting text spans are far apart. We tried our best to fix these issues, but some errors likely remain in the dataset."
545,"Although our masks in different layers are binary and require significantly less storage compared to other PETL networks, we still need the underlying 32-bit scores for each mask during the training process. Therefore, PROPETL consumes slightly more memory in the training time than existing PETL methods. To fine-tune PROPETL, it takes a similar training time to conventional PETL modules, which means our method will normally take a longer time to converge compared to the fully fine-tuned model."
546,Limitation section
547,"RL4F is primarily targeted at improving final performance. While we have found that the critiques learned by RL4F remain natural, we do not introduce any explicit restraints preventing semantic drift. As though it may raise end-task performance, semantic drift would also hinder interpretability. Future work might study datasets that are not covered by this dataset and quantify semantic drift along with proposing measures to prevent it, as necessary. Moreover, this work does not provide an explicit mechanism to incorporate new critique labels that might become available in the future nor it identifies a framework that could combine critiques from multiple experts such humans and other machines. Lastly, we limit our analysis to GPT-3 and focus on a scenario where it is inefficient or impossible to train the task model while this may be a conservative assumption for other settings."
548,Limitations
549,"At its core, NORMBANK is a collection of logical operations on unique constraints. Consequently, one practical limitation stems from the issue that some situations cannot be reasonably expressed as a set of constraints. While theoretically all logic can be decomposed into AND and OR operations, the logic may be too challenging for an individual to formulate, or the set of constraints themselves might be too large and unwieldy. The latter is problematic, because language models have a finite input token capacity, and for the set of constraints to be digestible, they must fit within that capacity. Relatedly, if the logic to encode constraints become more sophisticated, ensuring that logic is not unnecessarily duplicated will pose a greater challenge. Additionally, certain properties of NORMBANK like the role and behaviors may be challenging to succinctly describe. Further work will be needed to ascertain how these can be incorporated or to more clearly define situations that are out of scope.
Due to limitations on time and computational resources, we have not exhaustively evaluated all downstream applications of NORMBANK, and in future work, we will test additional transfer tasks beyond the moral and social classification tasks considered in this work. Since NORMBANK is the first to encode non-monotonic situational norms, there was no other available benchmark that is directly analogous to ours. Instead, our primary evidence for NormBank’s utility is in Table 3, where human evaluators confirm that models trained on NORMBANK can reliably learn to make new inferences about non-monotonic situational norms.
Other follow up studies should consider training larger normative reasoning models, and/or engineering better prompts for expanding NORMBANK. Relatedly, we have no data to speculate about the long-term evolution of real-world norms relative to this resource, nor the rate of decay in the reliability of NORMBANK. Future work should also expand this resource with perspectives from cultures other than our available annotator pool. The pool was not representative of all cultures and people groups, as we discuss further in the Ethics section."
550,Section 7 (Page 9)
551,"Despite showing impressive performance, our graph-based approach still has several limitations. The first one is related to the construction of the sentence graph. At present, we consider two sentences to be semantically related if they share similar nouns. But coherence can be achieved not only by describing similar entities but also by discourse (rhetorical) relations (Jurafsky and Martin, 2021). So it will be an exciting direction to incorporate discourse relations into the construction of a graph. The second one is that we implemented our method using only a plain GCN. Recent work has pointed out that the original GCN can be further improved with more advanced aggregation functions (Xu et al., 2019a) or attention mechanisms (Velickovic et al., 2018). So another interesting direction is to explore the benefits of more powerful graph neural networks for our method, which we leave for future study."
552,"In Section 6.
7 A2. Did you discuss any potential risks of your work? This is an entirely technical paper. We don’t think it has any risk of bias or otherwise."
553,"The proposed CKL can automatically produce the scores for each utterance in the context and each sentence in the knowledge. However, it is constrained by the total length of the input sequence. CKL takes BART as its foundation, thus the bottleneck of BART limits the upper ability of CKL. BART requests the input length to be 1,024, which means the CKL can receive at most 1,024 tokens at a time. For some samples, the concatenation of the context and knowledge contains far more than 1,024 but is truncated to fit with the length requirement. In those cases, the CKL cannot get enough information, resulting in the sub-optimal performance of the CKL."
554,Section 7 Limitations
555,"While we have endeavored to include a good crosssection of selective prediction techniques in our empirical study, clearly it is not comprehensive of all work in this space. Moreover, this work does not address confidence calibration, nor does it address the behavior of selective predictive techniques under domain shift. Finally, our focus is on selective classification – we do not address confidence estimation for regression or generation tasks."
556,Section Limitations
557,"While the proposed MetaEvent achieves significant improvements in both zero- and few-shot event detection, MetaEvent requires additional computational resources due to the layer- and step-adaptive learning rates and the outer-loop optimization, which may cause increased computational costs for training MetaEvent. Moreover, we have not investigated the benefits of task scheduling techniques and similarity-based meta learning in MetaEvent to fully explore the training event types. Consequently, we plan to study efficient meta learning with advanced training task scheduling for further improvements in zero- and few-shot event detection as future work."
558,7 Limitations
559,"TST BT is simple and straightforward, which brings great improvements against BT baselines. However, comparing with standard BT, TST BT requires an additional style transfer model and additional time to process generated BT data."
560,"This work has one major risk. As the main idea proposed in this work heavily relies on the external 3D scene extractor, the quality of extractor on our used VSD images largely influences the task performance. However, we reveal in analysis that although suffering from the domain shift issue by the out-of-domain 3D scene extractor, our method still improves the VSD task. We show that when handling the in-domain VSD images as used for training the 3D scene extractor, the VSD performance has been boosted remarkedly. Thus, with better a 3D scene extractor, it can be expected that our system will exhibit much stronger capability and advance the VSD task more significantly."
561,"There are some limitations that have yet to be addressed. Since we use the predicted probability distributions of the model output as a medium for continual KD for NMT, the vocabulary of multiple models needs to be consistent. Overcoming it allows continual KD for NMT to be extended to models with different language pairs and different modalities. Also, although our approach is robust to malicious models, there are more diverse and sophisticated attacks in real-world that require more research on defense. In addition, the teacher and student models must be trained on the same language pair. Further studies can consider more general scenarios without the above limitations. There are other approaches worth exploring in order to address the transfer of knowledge from models rather than their training data besides sequential manner. For example, it is also possible to explore various distillation methods like organizing teacher models into batches or pipelines."
562,"There are two major limitations in this work. Firstly, while we showed that query refinement prompts improve the ability of LLMs to generate long-form answers in a closed book and few-shot settings, open-book systems still perform better even when using a lot less parameters. Doing open-book long-form question answering in LLMs is currently not trivial due to their input token length limit and the need to use longer prompts when context passages are given.
The other major limitation is that human annotators only used the gold-standard answers to check for correctness (i.e., VS GOLD in Table 6). As explained in Section 4.5, there can be many ways to disambiguate questions, and therefore the systems can obtain a longform answer that is not different from the gold answers but still should be considered correct. We tried asking annotators to use the Internet to check for correctness, however they found it difficult to do so even for a single example."
563,"As we mentioned, we only focus on event arguments with the assumption that event type is already provided. However, this is not always true for many applications in real life scenarios. But it would be out of the scope of this work to combine them together, so we leave it for future work. Besides, considering the long input of document-level extraction, the computing memory consumption significantly increase to tens of times compared with its sentence-level counterpart. We only consider the 1/2-Doc cases, although in reality more docs are possible. We believe finding a solution for decreasing the memory requirements would be of great impact for future research in this direction."
564,"Please see the ""Limitations"" section."
565,"Although MITQA achieves the best results for TextTableQA benchmarks to date, it still has some limitations, owing to its design, and the type of training data it can access. Design policy: We have designed MITQA as a collection of trainable modules, which are used in a specific sequence. This design has helped us to focus our innovations in specific modules such as multi-row training for RowRetriever, multi-span training for AnswerExtractor, etc., with an eye to boost overall accuracy. However, the modular design also means that MITQA is not fully end-to-end trainable. Therefore MITQA is, in principle, susceptible to compounding error propagation across modules. We view this as an acceptable trade-off while working on HybridQA and OTT-QA, but other data sets may force us to revisit this decision. Types of queries: TextTableQA, being a relatively new task, has only two major benchmarks available (HybridQA and OTT-QA), where OTT-QA is an open domain extension of HybridQA. Therefore, the types of queries to which MITQA during training are limited to effectively a single large benchmark (HybridQA). HybridQA — and consequently OTT-QA — corpora are similar to Wikipedia articles, not confined to any specific domain. Further experiments in specific verticals, such as Finance, Retail, and Health are needed to check if MITQA affords practical cross-domain adaptation.
Moreover, only a small fraction of queries in HybridQA and OTT-QA need aggregation. Due to their rareness, we have not considered handling aggregation queries through MITQA, which needs additional work in future."
566,"Section ""Limitations""."
567,"InsMed is built based on the large-scale pretraining model BART, which requires high computing resources. Besides, the data currently only covers four departments, limiting the usage scenarios of the data."
568,"Firstly, following the work of T0 (Sanh et al., 2022), we mainly focus on NLP tasks that can be formulated as rank classification. This covers classification and multiple-choice tasks, but not other task categories such as generation or regression. We hope to extend our training and evaluation to encompass a wider range of task categories, and hope the research community will collaborate in creating resources for such study.
Secondly, though we showed that FiD-ICL outperforms Concat-ICL, we still lack clear understanding on the source of such improvement. We hypothesized that FiD enables the model to learn from in-context examples more effectively, yet our perturbation experiments show that FiD-ICL mod-
els still learn little from input-label mapping (§6.2). Much more work is needed to further understand of the working mechanism of ICL models.
Thirdly, given the complexity of our study, we limit the scope to encoder-decoder models. We made this decision due to the superior performance of encoder-decoder models in task-level generalization (Wang et al., 2022a) and their compatibility with fusion-in-decoder method. Also, our important baselines, T0 (Sanh et al., 2022) and T-Few (Liu et al., 2022), are implemented with the T5 model family. We include more discussion in §A.4."
569,Limitation section after conclusion.
570,"We empirically conclude two limitations for S2ynRE in the hope of inspiring more future research. On one hand, its advantages are less significant when a large amount of annotated data is available. For example, TACRED training set has 68,142 annotated instances. Under this setting, even if we add another 100,000 synthetic samples, the improvement is only +0.98 compared to +22.02 under 1% training set. This means that the quality of synthetic data, although superior to distant ones, is still not as good as golden ones. Thus they can hardly provide identical utility the same as 100,000 golden data. Nevertheless, with the development of LLMs and their powerful generation ability, we look forward to accessing higher-quality synthetic data.
On the other hand, when training data are limited to a few samples (for example, 1% setting for SemEval only includes 73 training instances), even strong LLMs like GPT-2 can not perfectly fit the structure of relational statements within a few steps of finetuning (See Appendix G for illustration of cases). Therefore, many generated sentences may not contain correct subject or object entity markers as requested and have to be discarded. In general, although the formation of marked natural language sequence proposed in this work made such structured synthesis feasible, we look forward to further improving the synthesis efficacy in future works.
4https://www.wikidata.org/wiki/Property_talk: P609
ACL 2023 Responsible NLP Checklist"
571,Sec. Limitations
572,"In the Limitations section at the end of the paper (Section 7)
7 A2. Did you discuss any potential risks of your work? In our paper, we simply evaluate the performance of models to address the task of Entity Linking in simulated scenarios with noisy data. We cannot think of any potential risks of our work."
573,"Our analysis on temporal drift (§ 5.2) was limited by the fact that the developer of many models in our study did not release the exact time period of the pre-training corpora used. Additionally, models such as BERT and RoBERTa were pre-trained on corpora that could be potentially be temporally close to the CoNLL++ test set.
In the section on test reuse (§ 5.1.2), due to a limited compute budget, we were only able to conduct this experiment on a single new train/dev/test split, so it is possible that the new split happens to be easier than the mean of the distribution. However, our experiments still provide additional evidence models are not overfitting the original CoNLL-2003 test set.
It is worth noting that when using older models trained on the CoNLL-2003 dataset, one additional reason for the performance degradation, especially
in real-world deployment, is that the data used to evaluate the models can be out-of-domain. In our experiments, we attemped to control the domain of the test data on which the models were evaluated to assess other factors for performance degradation. However, we acknowledge that in reality, model performance can be affected by factors such as the emerging text types (e.g. Twitter did not exist when CoNLL-2003 NER task was created), which leads to changes in domain, and therefore affects the generalizability of the models.
We acknowledge that having CoNLL++ will not resolve the problem of generalization to modern data. As new data keep emerging, there will always be the question of how well NER models generalize to that new data. We hope that our paper will encourage researchers in the NLP community to continuously annotate new test set to study this problem, so that we ensure the robustness and generalizability of models."
574,"Section 8
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
575,Last section
576,"""Limitations"" section provided in the template"
577,Limitations
578,"Until now HERMES has only been evaluated on the formula prediction task, but we believe that the three-stage decoding pipeline and the sampling strategy for multi-level expansion in HERMES can further be extended to other forms of code such as SQL and Linux commands. Another limitation is how to unify the experiences of hierarchical expanding and other formula writing orders, because we think that the human reasoning order in writing formulas may be a mixture of top-down, bottomup, left-to-right, etc. HERMES proposed new ideas on variable high-level expansion, but it is desirable to be further integrated with our experiences.
ACL 2023 Responsible NLP Checklist"
579,Limitations Line 666
580,"Section 8
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
581,Section 8 Limitations.
582,"after conclusion selection, page 9
7 A2. Did you discuss any potential risks of your work? we do not see any ethical implications or risks of our work"
583,"The main limitation to our work lies in the handling of unordered n-ary relations. We hypothesize that the bottom-up paradigm performs best when there is one unambiguous logical form to generate for a particular question. While this is quite often true for semantic parsing, in our experience, unordered n-ary relations (e.g., WHERE) can quickly cause this not to be the case. With such relations, there tends to be a large number of correct logical forms for a particular question. In these situations, having so many candidate logical forms can cause significant issues in terms of runtime. A second limitation of our work is that it assumes the logical form will be given as a graph. Thus, there is an annotation burden on the users of this system that
would not be present in systems that treat semantic parsing as a text-to-text problem (e.g., fine-tuned large language models)."
584,Section Limitations
585,The Limitations section on p9
586,Limitations section at end
587,"One limitation of our approach is that we aggregated IC and GC measurements over clusters during the automatic subgroup discovery process, but we did not fully consider the relationships between clusters. A more comprehensive strategy for utilizing beneficial relationships and a more precise approach to potential conflicts between clusters could lead to further improvements in overall performance. Additionally, our MNLI experiments were conducted on large dataset that had multiple clusters with errors. We chose to focus on the top10 clusters with the most errors due to limitations in resources for running a user study. While TDG on top-K clusters has demonstrated effectiveness in improving performance, there is still the potential for further improvements by working on a larger number of clusters. At the same time, we emphasize that TDG should be used as the last step to improve performance in low-performing groups (clusters with high errors). If these groups are numerous, it means the model is likely under-trained, and other techniques (e.g. better data/modeling) should be applied first."
588,"Our work has the following limitations.
Comparisons with more recent models. In §3, we conducted a principled comparison between BERT, ELMo, and GPT-1 under comparable experimental conditions. This comparison notably excludes more recent models that benefit from more parameters, larger training data, or different loss functions, such as RoBERTa, Electra, and T5. Due to the even-higher cost of pre-training these more recent models, we leave a principled comparison that includes these models to future work, although we identified the development of more comprehensive PLM scaling laws as a promising future research direction that would allow us to extrapolate how our findings would generalize to different pretraining data sizes, objective functions, etc. (§4).
Interaction between different factors. In §3, we have conducted a principled comparison by varying only the pre-training objective function and the length of model training, whilst keeping all the other variables constant. In practice, however, the exact choice of these different control variables (e.g., what positional encodings to use, how we pre-process the data, etc.) can interact and affect the findings in a material way. It is conceivable — and rather likely — that our findings on the performance gap between BERT, ELMo, and GPT-1 may change under different experimental settings.
Simulated efficient learning scenario. Our efficient learning scenario in §3 constitutes a simulated one, where we artificially limit the number of updates to 200,000 steps (as opposed to 1M steps in the full setting). We leave the extension to more realistic efficient learning scenarios, such as in languages where there is only a limited number of monolingual data, or where there is a hard limit on what pre-training computational resources we can use (e.g., 1 GPU for 3 days), to future work.
Extension to multi-lingual settings. Our experiments are thus far conducted only in English. We leave the extension to other languages — including low-resource languages with only a limited amount of monolingual data as a realistic and necessary benchmark of efficient learning — to future work.
The increasing prevalence of closed-source / proprietary PLMs. Despite our recommendations and calls for change, we acknowledge the fact that
recent PLM trends have shifted more towards proprietary and closed-source models — a development we attribute to the rapidly increasing commercialization potential of this technology. Under this trend, very little is known about how each PLM is developed, as the vast majority of the technical details (e.g., the amount and source of the pre-training data, the data filtering strategy, the size and hyper-parameters of the model, how the model is implemented, etc.) are kept proprietary. While these trends may mean that our recommendations are more unlikely to be adopted by proprietary PLMs, we argue that our position paper and recommendations are still important (if not even more so) for two reasons. First, open-sourced community models, such as BLOOM (Scao et al., 2022), OPT (Zhang et al., 2022), and Alpaca (Taori et al., 2023), are gaining traction, and have rapidly narrowed the gap with proprietary models. This progress reflects the community’s strong desire to have open-sourced models that can rival proprietary ones in terms of model quality. The rise of these open-sourced models thus gives rise to the question: How can these community-driven models help the community pay off our scientific debt? To that end, our recommendations provide concrete and actionable steps in this direction. For instance, our recommendations call for standardizing the pretraining dataset, which has not yet been done thus far, even though there are plausible, open-sourced datasets that can be used for doing so. Furthermore, we also encourage the community to release the full evaluation results of their models, alongside the relevant hyper-parameter information, etc., such that we can collectively build a more comprehensive scaling law through crowd-sourcing (§4). Second, prior work that conducts extensive ablation studies and rigorous experiments (Raffel et al., 2020; Sun and Iyyer, 2021, inter alia) remains the exception, rather than the rule. Our position paper includes a call for change that will make it easier to pay off this scientific debt going forward, which is ever-more important in light of impressive progress from both proprietary and open-sourced PLMs.
Ethical Considerations
Our experiments replicate prior work under comparable experimental conditions. For this reason, we do not expect our work to introduce any novel ethical issues, although our experiments may inherit a similar set of issues concerning PLM (especially
large-scale ones), as outlined by various prior work (Gehman et al., 2020; Bender et al., 2021; Rae et al., 2021; Dinan et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021, inter alia). We remark, however, that conducting these principled comparisons across different models — which requires a degree of hyper-parameter tuning for each model (both at pre-training and fine-tuning stages) in order to enable a fair comparison — requires a large number of computational resources, which may contribute to increased carbon emissions (Strubell et al., 2019; Patterson et al., 2021)."
589,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
590,"As mentioned in our experimental setup, we provide results of AV-S2ST in LRS3-T with synthesized target speech, similar to the pioneer literature (Jia et al., 2022b) in S2ST. One of our future directions is to develop a better benchmark dataset (e.g., mined or human-annotated data) to improve translation performance.
As mentioned in our results analysis, the BLEU scores heavily depend on the ASR quality, which may not accurately reflect the speech translation performance. Future directions could be improving ASR quality or exploring other evaluation metrics without reliance on ASR models.
AV-TranSpeech lowers the requirements for audio-visual speech-to-speech translation, which may cause unemployment for people with related occupations such as interpreter and translator. In addition, there is the potential for harm from nonconsensual voice generation or fake media. The voices of the speakers in the recordings might be overused than they expect."
591,See Section 6.
592,"Assumptions: This work applies a lower-case transformation to the vendor names during the pre-processing step and assumes vendor accounts ""agentq"" and ""AgentQ"" to be from the same entity. However, in reality, these entities can refer to two different vendors. Additionally, we train our classifier in a multi-class classification setting, assuming that ads correspond to only one individual vendor account. However, our experiments uncover the existence of copycats on Darknet markets. In reality, it is always possible for multiple vendors to co-exist with similar vendor names; hence, any supervised approach will only generate skew results. In the future, we plan to look toward contrastive learning approaches (Pan et al., 2021; Zhou et al., 2021b; Wegmann et al., 2022) to avoid these assumptions.
Architectural limitations: This research establishes a BERT-base-cased classifier to verify migrating vendors across existing and emerging Darknet markets. While we acknowledge that using a bigger BERT model with a sliding window may improve our classification’s performance, given the resources at our disposal, we decided against it. Moreover, as mentioned earlier, most of the ads used in this research are in English, with a few exceptions where the vendors use multiple languages. Therefore, applying a multilingual transformerbased model to the classification task (Wang and
Banko, 2021) can improve our approach’s performance.
Unsupervised and HR settings: As described in the assumptions, the core of our approach lies in the availability of gold labels. VendorLink utilizes the supervised pre-training step to perform knowledge transfer and text-similarity tasks. Therefore, our approach suffers a significant limitation in the absence of these ground labels / unsupervised settings. Furthermore, as described in A.1.3, our approach could not scale well to verify vendor migrants in HR emerging datasets. In the future, we plan to expose VendorLink to contrastive learning approaches to learn universal representations and overcome the problem.
Diverse Advertisements: In the semi-supervised task, we compute the likelihood of two vendor accounts being from the same entity by calculating the similarity between the advertisements of the two vendors. Since one of the novelties of this research lies in the direction of End-to-End training, we have avoided using handcrafted labels for applying content control to generate content-independent style representation. However, as explained in section 4.2, an advertisement from the drug category can be very different from that of the weapon category. Therefore, in the future, we plan to train another classifier to classify Darknet advertisements into different trade categories before performing the vendor-verification task.
XAI limitations: eXplainaible Artificial Intelligence (XAI) is integral in promoting trust and understanding amongst the end-users. From LEA’s perspective, its absence can be viewed as arguably negligent and unreliable. While we acknowledge that our approach currently lacks an XAI feature, in the future, we plan to build upon our experiments in A.1.5 and establish a reliable approach for understanding and explaining our model’s decision."
593,"In terms of the test sets, due to time, labor, and financial limitations, we are unable to construct large-scale test sets of the same size as the original, so the domain balance in the test sets is not fully considered, but the uniformity of writing style might have slightly alleviated this issue. In terms of the method, we empirically explore the possibility of chain-of-thought application in text generation. However, due to the stronger openness of generative tasks compared to pure reasoning tasks, generated summaries might be more sensitive to the form of chain-of-thought, which is a key point worth further optimization."
594,Limitations
595,"Section 8
7 A2. Did you discuss any potential risks of your work? We use the common public datasets, models, and scripts provided by Huggingface to investigate a more efficient computation method for Shapley Values."
596,"Section Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
597,"We acknowledge a range of limitations of our work. As discussed in Sections 3 and 6, overall annotator agreement ranged from fair (frame annotations) to low (entity role annotations). We do not view this as a limitation per se, again pointing to the recent literature on the value of human label variance pointing at a potential loss of valuable information if we overly focus on arriving at a single gold label per instance, with high confidence (Plank, 2022; Pavlick and Kwiatkowski, 2019). Future modeling work involving entity labels should, however, carefully inspect the role label variation, and potentially remove or aggregate selected annotations, before incorporating the labels as signal into predictive models. We explicitly refrained from training model in this paper to avoid the risk of training a predictor on an unfavorable noise-to-signal ratio.
Our data set focuses on English-language news reports, sampled from 2017 to 2019 in mainstream media outlets in the US and UK, and as such focuses on cultures and communities which are already well-resourced and well studied. With climate change being a global challenge, broadening data sets, annotations and models to more languages is an important direction for future work. We explicitly caution against projecting annotations across languages without careful validation as we expect the manifestation of framing, views
on entities (or sheer set of dominant entities) to vary widely across countries and communities.
Even within our English study, we acknowledge that the size of annotated data is small for NLP scales, and an extension in the future is desirable. A related current limitation is the focus on just a single issue (climate change) and validation of our narrative framing framework for other issues is an important direction for the future. Finally, the annotation process was slow and costly, relying on trained, highly educated annotators with constant monitoring, rendering larger scale annotations challenging, on the one hand. On the other hand, we will release upon acceptance our annotation procedure including the full codebook with instructions, which have been optimized over several rounds of annotations and we hope can support more efficient annotation in the future.
Ethics statement
This study was approved by the University of Melbourne ethics board (Human Ethics Committee LNR 3B), Reference Number 2023-22109-37029- 4, and data acquisition and analysis has been taken out to the according ethical standards. We hired four local annotators who were paid an hourly rate of $53 AU in line with the casual research assistant hourly rates set up in the University of Melbourne collective agreement.
We will release the Narrative Framing Corpus comprising of 428 news articles annotated with frame labels, entities, their narrative roles and stakeholder categories. We also publish the raw (nonaggregated) annotations. Our data set builds on news articles from the NELA corpora 2017-2019, which were released to the public domain (license CC0 1.0).14 We release our code and Narrative Frames Corpus under a MIT license."
598,Section 7
599,"Section ""Limitations"""
600,"Though DuNST works well, it has four kinds of limitations as follows:
• Decelerated training process. As with all other Self-training methods, DuNST also needs to reproduce pseudo labels and pseudo text at each ST iteration. Since the pseudo text (both hard and soft) is generated in an autoregressive manner, which is hard to be done parallelly, leading to longer training time.
• Reliance of unlabeled in-domain text. As we discussed in Sec. 4, though our soft pseudo text brings non-trivial improvement, the overall performance of all ST methods still relies on pseudo labels from unlabeled text. When unlabeled text is extremely inadequate or even unavailable (e.g., low-resource scenarios), how to better utilize pseudo text for further improvement is an open challenge.
• Efforts of tuning noise level. As we discussed in Sec. 4.8, the noise level τ is essential for a balanced performance, which should be carefully tuned for each downstream task.
• Task generalization and scalability. We mainly investigate controllable NLG in this work, while it is still unknown whether our method works for other NLG tasks, like NMT and Text Summarization. Besides, as we analyzed in Sec. 3.4, ST actually acts as a kind of regularization and smoothing. How to apply this paradigm to super large PLMs (e.g., GPT2XL and GPT3), where the supervision signals from limited labeled data become extremely weak, is also an open question."
601,"Linguistic studies have shown that respective readings are not necessary to have two coordinate structures in the same sentence (Dalrymple and Kehler, 1995). Both WikiResNLI and NatResNLI have only one sentence in the premise and do not exhaust all possible and complicated realizations of respective readings. However, we are able to discuss and investigate LMs’ generalizability with “respectively” with three constructions, i.e., 1S1O,
1S2O and 2S1O. Our experiments are English-specific and are limited to LMs that can be run with an academic budget. However, our conclusions about the generalizability towards respective readings should be viewed as language-agnostic given there are linguistic constructions under-discussed in many other languages and it is worth researchers’ attention to study them."
602,"Section 7
7 A2. Did you discuss any potential risks of your work? We provide a NLI dataset and we do not see a risk for that"
603,"One limitation of this study is its scope, which covers two downstream tasks and two types of demographics (race and gender). The binary gender definition we used excludes other genders that do not fall under male or female. In the case of race, we explored only African American race (proxied
by African American English), which excludes biases related to other races, and is a US-centric view of racial bias. We did not investigate other types of bias, such as religious bias. Furthermore, our method was tested on datasets with short texts, and it is unclear how it will perform on longer texts. The experiments were conducted on datasets in English, and it is unclear how our method will work on languages that are morphologically rich."
604,"Unnumbered section named ""Limitations"""
605,"Section 4.1 (explicit list of controlled covariates in selecting datasets), Section 5: Results (discussion of both the cases where our objectives improved compared to baselines, and where they did not). Section Limitations (enumerating a list of potential uncontrolled covariates of our experiments)."
606,"The proposed CRINGE loss can be used to mitigate some of the identified problems of large language models, for example, the use of toxic language (Dinan et al., 2019a; Wulczyn et al., 2017; Xu et al., 2021b) or contradictory statements (Roller et al., 2021; Nie et al., 2021). Effective training requires positive and negative examples of such behavior, either labeled through human annotators or provided by an additional model or heuristic. The quality of the data bounds the success of the training approach. In our experiments, we assume non-adversarial label annotation. In real-world interactions with a chatbot, it is likely to experience at least some “trolls” that provide wrong feedback on purpose (Ju et al., 2022). Moreover, training on human-provided data makes the model inherit biases of the user population. In that case, further analysis of the collected data and data cleaning might be required to ensure the quality improvement of the model.
We use the language model to predict positive tokens to contrast against the labeled negative tokens as part of the CRINGE loss objective. Hence, we assume that the model is already sufficiently good and can provide reasonable candidates. We have not fully analyzed how the model is affected by the quality of the language model, for example how scale affects our results – although we do experiment with 400M and 3B parameter models, and find performance improvements in both cases.
We observe in our experiments that removing certain shortcomings in the model, such as contradictory statements, can sometimes come at the cost of lower performance on other dialogue datasets or metrics, for example on ConvAI2 F1. This tradeoff can be controlled by the α-value of the CRINGE loss, or the number of iterations performed."
607,"Limitations
7 A2. Did you discuss any potential risks of your work? Except for the limitations, we do not yet find any other risks the proposed model would have."
608,"This paper aims to make advancements toward automatically identifying fine-grained depressive symptoms from memes shared on social media. Although we used only those memes shared by users who self-declared themselves as depressive, we did not conduct any further clinical assessment to judge whether the user was depressive or not, nor we clinically evaluated their depression severity. Therefore, deploying this system without expert advice could compromise patient safety and lead to undesirable outcomes. We further acknowledge that determining the depression symptoms based on the visual and textual cues present in the meme can be subjective, and therefore the created gold-standard dataset may contain explicit and demographic biases. In this study, we focused on training the models using only the social media data, leaving their performance unchecked if tested on other medical data sources. Finally, our study is not indented to provide any diagnosis; instead, we envision the methods we provide being used as aids by healthcare professionals."
609,"limitations section in page 9.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
610,After Concluding Remarks; before References Pg 9. Section* number unmarked.
611,"Limitations section (unnumbered).
7 A2. Did you discuss any potential risks of your work? Our work analyzes existing models (RoBERTa) on existing datasets, predicting their performance for out-of-domain data. Rather than introducing new models and risks, we hope that these results can be used to reduce the potential risks of applying existing models in cases where they might perform poorly."
612,"The human evaluation shows that our pipeline performs worse than humans in the process of information retrieval and synthesis 67.5% of the time, which still leaves room for improvement (see appendix G for future works)."
613,"Although our proposed method achieves promising performance in the novel direction of ZeroCQG, it still has the following limitations: (1) retrieval-based conversation synthesis is limited to predefined question-answer pairs and may introduce repeated question-answer pairs with small differences (discussed in Appendix B.1). Future work may include exploring generative-based approaches to generate new and diverse questionanswer pairs for better conversation synthesis. (2) Existing question transformation only explore one of the most common conversational characteristics, anaphora. However, other different characteristics, such as ellipsis, should also be considered in the future. (3) The conversation prompting has limitations when the domain gap becomes large (discussed in Sec. 4.4). More robust prompt learning should be explored in the future."
614,"Our work follows the general assumption that the training and test set contain the same list of predefined entities. Without additional or necessary modifications, the few-shot or zero-shot capability of the model is expected to be limited. Future work includes exploring prompt-based architectures to unify pre-training and fine-tuning into the same query-based procedure."
615,Sec 6
616,"Section Limitations
7 A2. Did you discuss any potential risks of your work? There are no potential risks in our paper."
617,"section of its own name
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
618,"Our study has several limitations. First, demographics may not be the best construct for positionality, as there may be variability of beliefs within demographic groups. Assuming that there is homogeneity within demographic groups is reductionist and limited. Rather, capturing an individual’s attitudes
or beliefs may be a more reliable way to capture one’s positionality that future work can investigate.
Study annotators could also purposefully answer untruthfully, producing low-quality annotations. We address this risk by using LabintheWild. LabintheWild has been shown to produce highquality data because participants are intrinsically motivated to participate by learning something about themselves (Reinecke and Gajos, 2015). However, as is the case for all online recruiting methods, our sample of participants is not representative of the world’s population due to the necessity of having access to the Internet. In addition, there is likely a selection bias in who decides to participate in a LabintheWild study.
Pearson’s r may not fully capture alignment as it does not consider interaction effects between different demographics (i.e., intersectionality). Thus, there may be additional mediating or moderating variables that may explain the results that our analysis does not consider. We also took the average of the annotations per group, which could mask individual variations (Talat et al., 2022). Also, having a low number of participants from specific demographic groups may limit how well the results generalize to the entire group; further, it may risk tokenizing already marginalized communities.
As part of our study, we apply NLPositionality to only two tasks which have relatively straightforward annotation schemes. It may be difficult to generalize to other NLP tasks which have harder annotation schemes, especially ones that require a lot of explanation to the annotators, for example, natural language inference (NLI) tasks.
Our approach is evaluated and works the best for classification tasks and classifiers. Generation tasks would need more careful annotator training which is difficult to achieve on a voluntary platform without adequate incentives. Having annotators use one Likert scale to rate the social acceptability and toxicity of a situation or text may not be a sufficient measure to represent these complex social phenomena. To reduce this threat, we provide detailed instructions that describe how to provide annotations and followed the original annotation setup as closely as possible."
619,"There is a fundamental uncertainty in whether Backpack language models will continue to scale with parameters and data and be viable alternatives to Transformers at larger model scales. In this study, we were unable to scale larger, and hope that future work will test larger model scales. In a similar vein, we do not verify that Backpack language models perform well across multiple languages. We also do not consider, e.g., finetuning Backpacks on other tasks, or masked language modeling—there is a wide range of possible uses that remain to be verified.
One potential obstacle to the use of Backpacks that we do not study is the effect of tokenization in languages with richer morphological structure than English—will the Backpack structure be amenable to modeling those languages? This may be difficult because, intuitively, the interpretability and control of Backpacks relates to the semantics of individual tokens. Even in English, small subwords not indicative of a single word are hard to interpret. What we hope to have provided is a sufficient set of experiments to motivate the further exploration of Backpacks."
620,Section 6
621,"We report our limitations in the last section.
7 A2. Did you discuss any potential risks of your work? We do not think there are any potential risks of our work."
622,"In the Limitations section
7 A2. Did you discuss any potential risks of your work? Our work does not involve any sensitive data or sensitive tasks."
623,The Limitations section.
624,"In this work, we have presented results that help inform us what tasks, methods and metrics are best suited for monitoring as well as methodologies and empirical information about the current set of models. We provide detailed information of how these results can be reproduced, to the extend that research have access to the PLMs in question, but these results have limitations, in order to reduce costs, many languages were not evaluated which might have left unforeseen patterns not discussed in this work. Moreover, few-shot learning, in particular, could exhibit large variance if different prompts were chosen, or a different set of exemplars chosen. Because of the high costs involved our work does not explore the performance difference when multiple sets of hyper-parameters were chosen. On the conceptual level, we make the assumption that system-level improvements on our tasks translate to downstream usefulness. While prior work suggests that this is the case, tools like chatGPT have significantly expanded the possible application space beyond the realm of “typical” NLP tasks, and we don’t know how well our findings generalize to this space of tasks."
625,"lilGym uses synthetic visual stimuli, which does not reflect the complexity or characteristics of realistic visual observations. This is critical for our ability to control the environment and provide a lightweight and accessible RL benchmark. Our goal is not to provide a resource for the development of methods that aim to handle realistic visual input, and lilGym is not suitable for this purpose. The limited number of colors, shapes, and sizes used limits the visual and lexical complexity of the data. The synthetic nature of the data and the
modular library of functions we use allow to relatively easily extend the environment (e.g., with new colors). This will require collecting additional natural language data. In this work, we opted to rely on the NLVR data without further expanding it. Some annotators of the original NLVR data adopted annotation strategies that led to repetition of some common phrases (e.g., starting statements with there is). While this creates some implicit patterns in the data, Suhr et al. (2017) showed that NLVR demonstrates high semantic diversity and compositionality. Finally, lilGym includes English data only. Expanding this data to other language is an important direction for future work. Translating the data is a feasible low-cost solution, because the program annotations will not require updating.
Ethics Statement
We paid U.S. standard market wage to our programmers (Appendix B). The rate was determined by the workers. The lilGym environment and data as is are intended to be used for research, including algorithm development and evaluation, and not for development of models to be deployed.
Commented for anonymous submission"
626,"Although we have proven that our work can significantly alleviate concept bias and extract highquality and new concepts, it also has some limitations. In this section, we analyze three limitations and hope to advance future work.
Model Novelty Although KPCE can effectively mitigate the spurious co-occurrence correlations between entities and biased concepts, the proposed framework is not entirely novel. The novelty of our work is to conduct the first thorough causal analysis that shows the spurious correlations between entities and biased concepts in the concept extraction task. After defining the problem and SCM of concept extraction in § 3.1, we propose a promptbased approach to implement the interventions toward the SCM to elicit the unbiased knowledge from PLMs. Previous work in language prompting mostly guides the PLMs with prompts but is unaware of the cause-effect relations in its task, which may hinder the effectiveness of prompts. We hope our work can inspire future work to utilize language prompting from a causal perspective.
Topic Classification Although the topics obtained by clustering are mostly mutually exclusive, there are still cases where an entity can be classified into multiple topics. Therefore, considering only one topic for the entity excludes the correct concepts.
Threshold Selection We only reserve concepts with confidence scores bigger than the selection threshold (§ 4.2), which can hardly achieve a satisfactory balance of precision and recall. If we select a relatively big threshold, we can get more accurate concepts but may lose some correct ones. If the recall is preferred, precision might be hurt.
We suggest that future work consider these three limitations to achieve better performance in the CE task."
627,Limitations
628,"First, our method needs to check all spans in the given sentence and build a table for each sentiment polarity, and is therefore difficult to handle too long sentences. Another limitation of our work is that for the different aspects in the same sentence, we need to rebuild the tables."
629,"In this work, we experiment with GPT3, T5, and DeBERTa. Other large pretrained LMs, such as PaLM (Chowdhery et al., 2022), is not covered in this work. We do not experiment with methods such as fine-tuning GPT3 due to the computation cost. The main purpose of this work is to uncover and analyze the fundamental limitations of LMs on symbolic and arithmetic induction instead of improving their performance of reasoning tasks, so we do not directly compare the mitigation methods with the previous work such as scratchpad (Nye et al., 2021) and (Wei et al., 2022) in our experiments. We leave more advanced methods for future work."
630,"Right after Section 10
7 A2. Did you discuss any potential risks of your work? This work is improving the quality of text generation systems. We believe that the risks of these methods are essentially the same as the risks of broader text generation systems, which have been documented at length in other publications."
631,"There are majorly two limitations: Firstly, in this work, we only consider the current-sentence text context-related prosody. In future work, we will focus on improving the inter-sentence prosody to achieve coherent, expressive TTS for long-form text. Secondly, other variables are not considered during the contrastive pre-training. One can explore similar approaches that connect prosody to other conditions such as speaker, emotion, etc."
632,6.Limitations
633,"This paper takes into account the temporal semantic variations of words and proposes a method to learn dynamic contextualised word embeddings by timeadapting an MLM using prompt-based fine-tuning methods. In this section, we highlight some of the important limitations of this work. We hope this will be useful when extending our work in the future by addressing these limitations.
The learned dynamic contextualised word embeddings are limited to the English language, which is a morphologically limited language. Therefore, the findings reported in this work might not generalise to other languages. However, there are already numerous multilingual MLMs such as mBERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al., 2020), to name a few. Extending our work to multilingual dynamic contextualised word embeddings will be a natural line of future work.
Dynamic contextualised word embeddings represent words as a function of extralinguistic context (Hofmann et al., 2021), which consider both time and social aspects of words. However, in this paper we focused solely on the temporal aspect and ignored the social aspect. Extending our work to take into account the social semantic variations of a word is an important future direction.
Due to the costs involved when fine-tuning largescale MLMs, we keep the number of manuallywritten and automatically learnt templates to a manageable small number as shown in Table 1 in §3.4. However, it remains to be evaluated the impact of increasing the number of templates on the performance of the proposed method."
634,"Section Limitations
7 A2. Did you discuss any potential risks of your work? We utilize existing PLMs and datasets for improving text generation, without creating new models or datasets."
635,"Our methods are currently trained and tested on two SVHR datasets. Gender biases and unethical hashtags could exist in the datasets, which may cause the model trained on these datasets to generate these biases. Besides, although our methods are not language-specific, we only choose the English dataset due to its rich resource. Furthermore, we regard the user tags as the hashtags for SFVD2 in our experiments and there are small differences between the user tags and hashtags. Experiments on more diverse languages and datasets are needed in the future.
As an initial work for SVHR, in our task formulation, our model only take the video and its description as input to predict hashtags and ignore user preference in hashtag recommendations. Exploring user preference is also a promising direction for future work.
We used up to eight A100 GPUs per experiment and it took more than one day to run experiments on SFVD2. More efficient models are needed for real-world applications. Besides, we hope our experimental results can be used as benchmarks for comparison in future work to avoid repeating training."
636,"Section 7 ""Limitation"""
637,"One of the limitations is the selection of hyperparameters. At present, we determine the optimal hyperparameters based on the performance of the selection methods on an existing bilingual dataset. For example, to identify the appropriate utterances to be translated from English to German, we would adjust the hyperparameters based on the performance of the methods on existing datasets in English and Thai. However, this approach may not always be feasible as such a dataset is not always available, and different languages possess distinct characteristics. As a result, the process of tuning hyperparameters on English-Thai datasets may not guarantee optimal performance on English-German datasets. As a future direction, we intend to investigate and develop more effective methods for hyperparameter tuning to address this limitation."
638,Limitations Section
639,"One limitation of our method is that when there are rules of different lengths, the final result is decided by ensemble, not by building a model to generate a single rule with the best length. The second way is more natural and important because figuring out the length of the rule is also a key part of symbolic reasoning. However, it requires more parameterization (for example, the length of the rule could be a parameter) and a more advanced way to optimize. The investigation of the above method is left for future works."
640,"The last section.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
641,"Comparison with GPT-3: There are growing concerns in the research community about the lack of open availability of GPT-3. There are several versions of the model and the details of the training data used for each version are largely unavailable. Direct comparison with GPT-3 is, therefore, becoming increasingly challenging. In this work, we compare against the ‘text-davinci-001’ version of the GPT-3 model and note that newer models might do better. However, extracting the best performance from GPT-3 is beside the point of our work. We believe that as a community, we must investigate alternative approaches that do not only rely on scale.
Undesirable Generations: Language models, large and small, have been shown to be prone to generating toxic text (Gehman et al., 2020). I2D2 relies on GPT-2 XL could also potentially generate toxic statements. While the trained critic model is able to filter out most toxic generations, we estimate the proportion of undesirable generations using the Delphi (Jiang et al., 2021) model. We find that ∼ 1.3% of the generations may not be morally acceptable, either because the statements are not accurate, not verifiable, too restrictive, or they are potentially toxic.
Self-Imitation Iterations: In this work, we only try two iterations of self-imitation due to resource constraints. Exploring the effects of more selfimitation iterations is left for future work. But, based on the performance improvements we observed after two iterations, we hypothesize that the improvements could diminish with each future iteration.
Runtime Efficiency A batch of 32 generations from I2D2 takes 3mins on a single RTX A6000 GPU. NeuroLogic Decoding is the most computationally expensive component. As constrained decoding methods become more efficient, the runtime of I2D2 will also improve. Our focus in this work is to study the quality of generations and we leave runtime efficiency improvements to future work."
642,"7 (after the section of conclusions)
7 A2. Did you discuss any potential risks of your work? Our work cannot produce new contents. Our main goal is to build a state-of-the-art evaluation metric for text generation which shows great generalization ability and interpretability.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 4
3 B1. Did you cite the creators of artifacts you used? 4"
643,"This paper mainly focuses on neural code search models. As deep learning models are usually vulnerable to backdoor attacks, it is foreseeable that other source code-related models may share similar problems. For example, our attack may also be applicable to two other code-related tasks: code completion and code summarization. Code completion recommends next code tokens based on existing code. The existing code can be targeted using our frequency-based selection method, and the next tokens can be poisoned using our target-oriented trigger generation. Code summarization generates comments for code. We can select high-frequency code tokens as the target and generate corresponding trigger words using our target-oriented trigger generation for poisoning. It is unclear how our attack performs empirically in these tasks. We leave the expeirmental exploration to future work."
644,"Our discussion in this paper leaves out the consideration of computability of measures over languages. Specifically, we note that there exist works on computable measure theory developed in the context of theoretical computer science (de Leeuw et al., 1956) and probabilistic programming languages (Roy, 2011). Additional machinery needs to be developed in order for a proper treatment and we leave this for future work.
Another notable limitation is that we exclusively focused on the autoregressive production of language. Importantly, our formalism might not be compatible with other models of language production such as those induced by a PCFG.
Finally, our proofs of Thm. 5.9 and Prop. 5.10 exploit the strictly positive property of the softmax function. Importantly, they do not apply to models with sparse distributions (Martins and Astudillo, 2016; Peters et al., 2019; Martins, 2021)."
645,"Section Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
646,"We discuss the limitations after the conclusion, and before the references.
7 A2. Did you discuss any potential risks of your work? This paper discusses keyphrase extraction, which basically does not bring risks."
647,Section 7: Limitations
648,"In this paper, we propose a new framework that extracts the various aspect of information about given data, relying on the existing model-driven metainformation from the trained models. Hence, if there are some flaws within the used models, such as biased prediction (Sun et al., 2019) or learning of spurious correlation (Liu et al., 2021), then our framework can be directly affected and may have a risk of inheritance or amplification of such problematic behaviors. However, as our framework is not limited to any specific models and metainformation, one can prevent this problem by using the robustly trained models (Sagawa et al., 2020) or introducing more specialized meta-information (Lee et al., 2021a) for these problems. In addition, despite the empirical gains we find, our subset selection method is not theoretically guaranteed to be (or tightly bound to) the optimal set of max informativeness, which remains an interesting direction. A further study is necessary showing that selected samples from infoVerse could lead to low inter-annotator agreement in manual annotation but provide more accurate information than pseudolabels. Abnormality detection using infoVerse, like noisy labels, out-of-distribution, or annotation artifacts, could be interesting future directions.
Broader Impact and Ethical Implications
Our work aims to quantify the data informativeness with multi-perspective for capturing properties that can not be revealed by a single perspective. Especially, infoVerse lends some insight into data by models what we have. Thus, infoVerse has the potential for guiding the construction of high-quality datasets, e.g., removing mis-labeled samples. From these points, it is possible to develop a system or general platform for effectively collecting data like Dynabench6 and Snorkle7. We anticipate that the general platform of infoVerse could be contributing to human-involved machine learning systems.
Although our work empirically demonstrates the improvement over various real-world problems, the current version of infoVerse has a potential risk to be vulnerable to sample undesirable properties (e.g., gender bias (Bordia and Bowman, 2019)) in a dataset, as we construct infoVerse with metainformation measure do not consider such properties. However, it can be easily alleviated by
6https://dynabench.org/ 7https://www.snorkel.org/
adding various measurements which represent ’fairness’ thanks to the extensibility of our framework. Hence, we believe that our proposed method can be personalized to the purpose of data collection."
649,"Although, we uncover and collate a broad-range of stereotypes, it is not without limitations. Firstly, we generate stereotypes using seeds which influence and skew the output stereotypes retrieved. Our coverage could thus be greatly affected and potentially increased with different or more seed stereotypes. Secondly, stereotypes are inherently subjective in nature and even though we do get 6 annotations from annotators residing in different regions, they have a limited world view and might not be aware of all the existing stereotypes. Additionally, certain stereotypes make sense only in context. For example the stereotype (Asians, hardworking) is not offensive by itself but becomes problematic when we compare or rank Asians with other social groups. Moreover, the stereotype (Asians, socially awkward) exists in tandem with the former stereotype which is offensive. Although we do capture regional sensitivity of stereotypes, our work does not capture the contextual information around these stereotypes. For capturing in-region vs out-region stereotypes, we only select annotators from North America but the out-region annotators can belong to any of the other regions as well. That is outside the scope of this work. Additionally, we emphasise that this work is not a replacement to the
more participatory work done directly with different communities to understand the societal context and the associated stereotypes. The complementary usage of our method with more community engaged methods can lead to broader coverage of evaluations of harm (Dev et al., 2023)."
650,"Limitations section, and parts of 8. Discussion"
651,Limitations
652,"First, in this work we limit the experimentation to vertical relational web-tables only, following the format of benchmarks used in TableQA, i.e., WikiSQL and WikiTQ. While we believe that ITR can easily be extended to horizontal entity web-tables, e.g., tables from Wikipedia, we do not expect our algorithm to transparently work on other types of tables that we do not consider, e.g., matrix tables from scientific papers and/or spreadsheets (Wang et al., 2021), where table items can be represented differently. However, this is not a limitation of the algorithm itself and adjusting our assumptions to certain scenarios and type of data can be feasible in the future. Second, ITR selects the relevant table elements by using a question as query. Therefore, it can only be applied for tasks with table-text joint input such as TableQA we showcase in the paper, or also table entailment tasks, e.g., table fact verification. Unfortunately, ITR cannot be used for tasks where table is the only input, e.g., table-to-text task. Finally, while ITR is beneficial for questions that do not rely on table completeness, its effectiveness is limited when, for example, all table cells are required to be predicted. Consider a question that requires cell counting, and the gold cells satisfying the query can be more than what we can feed a model with, e.g., “how many championship did Player A get?” and Player A has won 500 champions. However, this limitation does not arise from our approach and is rather inherited by existing TableQA models in the literature. Indeed, it can be a potential future direction of our work, which requires model innovation and table transformation that focuses on representing the information in a compact form."
653,"(1) In this paper, we tackle the problem of document-level simplification. This consists in simultaneous summarization and simplification. Applying the same model to sentence-level simplification needs to be further evaluated, as sentences naturally due to their shorter length may not require summarization.
(2) In addition, we did not explore various model sizes although we do conduct a fair comparison and show that even with a base-model size SIMSUM performs superior to baselines."
654,Section Limitations
655,"They are in the Limitation Section
7 A2. Did you discuss any potential risks of your work? We do not envision there are potential risks."
656,"We conducted our randomized field study on a single platform (Mental Health America) and in a single language (English). However, MHA is a particularly popular source for mental health resources
with over ten million yearly visitors. In addition, we note that a range of socio-cultural factors might influence how negative thoughts should be reframed and how LMs assisting this process should be developed. Conducting studies on specific communities, including underrepresented communities and minorities, was beyond the scope of this research. Ensuring equitable access of these tools and adapting them to various socio-cultural contexts requires further investigation.
Not all cognitive reframing implementations elicit situations, but we believed it was essential for making the reframe personally relatable. In the future, when an individual uses the system for multiple situations and thoughts, it would be interesting to study how their context can be learned more effectively over time. Due to privacy concerns, we presently do not gather information to link multiple sessions. However, with appropriate ethical considerations and user consent, this approach may be beneficial.
Our focus in this paper was primarily on creating an intervention that is effective in-the-moment. This was motivated by recent clinical psychology research that suggests that such single-session, inthe-moment interventions can lead to significant positive long-term mental health outcomes (Schleider et al., 2022). To integrate a partial longer-term perspective, we assessed the memorability of a reframe, which may be essential for future utility. Nevertheless, evaluating long-term outcomes is critical and forms an important future research direction. Finally, we emphasize that our study does not investigate short-term or long-term clinical outcomes."
657,"Our experimental analysis proved that text regression is a considerably reliable and accurate tool in dating nonliterary papyri. Limitations and challenges stem mainly from the composition of our dataset, which is balanced as far as the dates of the papyri included are concerned, both at the level of the century (approx. 40 records per century) and at the level of the quarter of the century (albeit less strictly and with the exception of the 7th CE). Furthermore, although we retained a substantial text sample of each papyrus, in approximately 1/4 of the records some text was eliminated.
Biases Despite our effort to balance the dataset in terms of dates, biases are present. Since our main concern in collecting the data was for the date distribution, no deliberate selection was made on the basis of the document types. Some types are thus over or underrepresented (e.g. private letters that do not usually bear a date; §6.2). Each type of document has however distinctive linguistic characteristics, such as the level of formality or unusual constructions (e.g. accounts). This uneven typological representation probably affects the performance of the models. Other possible biases in the dataset concern the
15We sampled randomly 100 regressors.
provenance of papyri, the length of their text, and the state of conservation (sizeable portions of missing text or entirely missing parts of the documents).
Chronological analysis of words Chronological analysis of word occurrence is possible if we detect and collect terms only attested in the papyrological material during a limited period. The word ‘denarius’ only appears after the 2nd CE and before the 5th CE, its presence in a text thus means that the text must have been written during this timespan. Likewise a text containing the word ‘indiction’ cannot have been written before the 4th CE. The investigation should also regard the possibility that the models make a prediction for a papyrus based on typical dating formulas present in the text like the name of the ruling emperor. Although our investigation of explanations did not yield any major concerns, a bigger sample of test cases should be created and more explainability methods should be employed (Ribeiro et al., 2016) to make conclusive remarks on this front.
Transcription of papyri is not optional Transcription of the papyri is required (at least partial, but substantial) to reach this high degree of accuracy with our method. Thus, while there are transcriptions available for most already published
papyri, it is less practical for dating unpublished papyri that have not been yet transcribed to a relatively high standard. In that case, image classification on the scripts can provide a less accurate prediction of the date as starting point."
658,"Yes, see the Limitations section after the Conclusion section."
659,"We discuss the following limitations of our work. First, the counterfactual data augmentation procedure we used can only be employed for questions whose answers are named entities. This restricts the applicability of the method as knowledge conflicts can arise for other types of questions, such as Boolean questions (Clark et al., 2019). Extending our framework to other question types will require a new counterfactual data augmentation method.
Second, we conduct our experiments using gold passages – i.e., an oracle retriever. Using retrieved passages, which is often required in real-world applications, may introduce additional challenges when considering knowledge disentanglement. Furthermore, the answerabilty approach presented in section 2.3 mainly serves as a proof-of-concept. It is quite simplistic, because the random context is unrelated to the question in terms of topic and participating entities. The focus of this work is on showing that unanswerable questions significantly boost the disentanglement capabilities of a QA model, and that even a simple approach like the one we took improves the model capability. Future creation of unanswerable examples would include more distracting contexts, that at first glance seem very relevant, but still do not contain the answer.
We note another minor limitation, implied by the high accuracy in the counterfactual case relative to the factual accuracy (see §4.5). This might stem from the model’s ability to identify that the text in the counterfactual examples is somewhat unnatural. It is therefore an indication of a potential limitation of the data augmentation methodology, albeit not a major one, judging by the small magnitude of the differences between the counterfactual and factual examples.
Finally, while our results indicate that models can learn to disentangle contextual and parametric knowledge, it remains unclear what characterizes easy vs. difficult cases for disentanglement. One such attribute, for example, can be the frequency of a given fact in the pretraining data. We view this as an important research question, which we plan to address in future work.
Due to the size of the models, we do not perform multiple trials of training from different initializations to test for significance. However, we do find similar trends across model sizes, which lends further support to the results presented."
660,Limitations section in Appendix A
661,"We present a novel (Target, Stance) pair Extraction task (TSE) for understanding the stance of interesting topics in the wild. There are two potential limitations to our work. First, the mapping module requires a predefined list of targets. Without the predefined list of targets, it is very difficult to understand the correctness of stance labels for the predicted targets in the absence of gold labels. On the other hand, the predefined list of targets makes the entire system end-to-end and automatically evaluable. Second, the process of mapping might become too slow if the number of targets of interest grows bigger. Future works include solving the given limitations and extracting (target, stance) pairs in a unified setting. However, the primary contribution of the work is not to present a fully robust pipeline model but to present a novel, interesting, and challenging task to the community working in stance detection."
662,"Left blank.
3 A2. Did you discuss any potential risks of your work? 9"
663,"Section Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
664,"Unnumbered ""Limitations"" section"
665,"Section 7 Limitations.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
666,"In this paper, we propose a novel SD method, entitled SD-APRR, which expresses negative situations of sarcasm by the potential results and human reactions of the associated events. We employ the COMET to estimate the results and human reactions, and form event-augmented samples with them. We employ those augmented samples as the whole sarcastic texts from the direct access view. We suggest a masked graph-based encoder, enabling to generate discriminative sample embeddings. Experimental results demonstrate that our SD-APRR can achieve competitive performance compared with the existing baseline methods.
We demonstrate two limitations: (1) The datasets used in this work are mostly collected from social media. In the future, we plan to collect sarcastic texts from various sources, such as the literature and films, and conduct more experiments with them. (2) Our exploration of sarcasm theories still has some space to improve. Though the incongruity theory is the mainstream in the community, there are other theories worthy to investigate in the future."
667,Limitations
668,"Our primary limitation is the size of our collected dataset; we have collected a quality dataset which we demonstrated is useful for analysis, but which is too small for training large-scale neural models. However, Section 5 indicates that a training-size dataset may not be necessary, as our question generation model is capable of capturing answer groups
without explicit supervision. Another limitation on our dataset is the relative subjectivity of the task; in completing the annotation, we found that identifying ambiguity and isolating the different underlying questions often involves a Gestalt shift. Once an interpretation of the question is chosen, it becomes increasingly hard to see any other. This makes the annotation task subjective; where one annotator might see ambiguity leading to multiple valid answers, another might see one correct answer group and a number of invalid ones. Thus, the annotations in our dataset represent a high precision subset (rather than a high-recall subset) of all the possible ambiguous datapoints. This subjectivity also risks introducing annotator bias (including the author’s own biases) into the data; we acknowledge that the vetting steps by the authors may have compounded this further. We are also limited by the quality of the underlying data. Our dataset builds on the VQAv2 dataset (Goyal et al., 2017) and the annotations from Bhattacharya et al. (2019), both of which were large-scale annotation efforts intended for training. Due to their scale, individual datapoint quality is often quite low; this was one factor contributing to the need for post-hoc cleaning in the annotation process."
669,"Please see Section ""Limitations"" after the text.
7 A2. Did you discuss any potential risks of your work? There is no potential risks mentioned in ""Responsible NLP Research checklist guidelines - A2"" in our work."
670,"While the LAIT framework can significantly reduce the computation required for large-scale sentencelevel reasoning and classification tasks, we do foresee some limitations in its use. Caching pertoken representations for large numbers of text segments leads to a dramatic increase in memory requirements, which could be prohibitive for extremely low-compute end users. We also note that LAIT can further exacerbate segment-level bias in datasets. While we believe that careful data curation approaches ameliorate this issue, the risk of bias is not always known to downstream users and as such corrective datasets may not always be available. Finally, LAIT can increase the cost of training because the optimal degree of independence is not known until all LAIT-p models are evaluated, though in practical settings (1) it is possible to perform a binary search of LAIT configurations because performance generally decreases monotonically as p increases; (2) even a naive rule of setting p to a quarter of the model’s depth seems to provide some immediate gains while preserving 99% of the accuracy in all our evaluated tasks; and (3) inference-time cost improvements will far outweigh training costs."
671,"We discuss limitations in section 5.2, as well as an un-numbered final ""Limitations"" section."
672,"Although based on the Transformer model, our methods also apply to various DNN modules, including CNNs, Poolings, and their compositions.
The applications of the proposed method in computer vision are left for future work.
An obvious limitation of this work is that we only verify our algorithm on models activated by ReLU. This issue can be alleviated because our algorithm is theoretically compatible with any piecewise linear activation function. For other functions in the ReLU family, such as the GELU (Hendrycks and Gimpel, 2016) used by BERT (Devlin et al., 2019; Liu et al., 2019), we replace the activations with ReLU, then fine-tune on downstream tasks and pretrain tasks (Appendix E). Our algorithms bog down on more complex nonlinear functions (e.g., sigmoid and tanh). It’s intuitive to fit these nonlinear functions with ReLU-activated FNNs. However, this leads to additional computational and space complexity, which degrades performance after fitting."
673,We discuss the limitations in Lines 552 - 599.
674,"Our dataset creation process - introducing unanswerability into a dataset of answerable KB questions by deleting KB elements - limits the nature of unanswerable questions. All of these become answerable by completing the provided KB. However, other kinds of unanswerability exists. Questions may involve false premise, for example, C. Manning works at which European University?, or may not even be relevant for the given KB. We will explore these in future work.
Complete training and inference for each model with our dataset size takes 50-60 hours. As a result, generating multiple results for the same models in the same setting was not possible and our results are based on single runs. However, using multiple runs with smaller dataset sizes we have seen that the variance is quite small. Also, the dataset creation involves sampling KB elements for deletion and as such the generated dataset is one sample dataset with unanswerability. This is unfortunately unavoidable when creating one benchmark dataset."
675,"As this is the first large-scale analysis of client reactions in online mental health counseling, there is huge room for future improvement. Here we only list a few problems that we would like to address in the short future. First, although our annotation framework is comprehensive, the data labeled is quite imbalanced. In some rare classes, there are fewer than 50 instances, making it difficult to conduct an in-depth analysis, let alone train an accurate classifier. Therefore, our analysis mostly focuses on the Extending and Defending behaviors. We will label more data so that rare cases can be better understood and classified more accurately. The accuracy of a classifier is important for real-life applications because it has the potential to mislead counselors. Second, we only have one short post-survey, which limits our coarse-scale analysis. We are adding more and richer post-surveys. Third, while we hope that the lessons learned can be applied to everyday conversations, our analysis has only been limited to psycho-counseling. The lessons learned will be tested against a wider range of use cases. It is important, however, not to overgeneralize our findings as this may harm the naturalness of our daily conversations. After all, the psycho-counseling process is a very special type of conversation."
676,Section 7
677,Limitations section
678,LimitationSection discusses the limitation of our work
679,"Like existing short text clustering methods, we assume the real cluster number is known. In the future, we would like to explore a short text clustering method with an unknown number of clusters. Moreover, the time complexity of self-adaptive optimal transport is O(n2), we are going to seek a new computation to reduce the complexity."
680,"In section Limitations.
7 A2. Did you discuss any potential risks of your work? Our work is a very basic and general research work, and is generally risk-free."
681,"As we cannot control for all confounding variables when examining the correlates of the most effective contrast sets, we only claim to identify trends, not causality, between calibration set characteristics and downstream performance. For instance, the top beams, on average, have higher relevance. As such, for each strategy, we record all key set characteristics and focus our analysis on observing trends between set characteristic values and downstream performance across all experiments, not simply within each Selection Type."
682,"Left blank.
3 A2. Did you discuss any potential risks of your work? 8
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 4
3 B1. Did you cite the creators of artifacts you used? 4
3 B2. Did you discuss the license or terms for use and / or distribution of any artifacts? 4"
683,"Annotation speed for mention detection and coreference is dependent on many variables like annotation interface, domain expertise of annotators, annotation style, document length distribution. So, while our finding that coreference resolution is approximately 2X slower to annotate than mention detection held for two domains (i2b2, CN), there are many other variables that we do not experiment with.
We also experiment with transfer between domains with varying semantic similarity and annotation style similarity. But, our notion of annotation style is narrowly focused on types of mentions that are annotated (i.e. singletons, domain applicationspecific mentions). However, since our method is focused on mention detection, our findings may not hold for transfer to annotation styles with different notions of coreference linking (i.e. split-antecedent
anaphoric reference (Yu et al., 2021)). We also focus on one common coreference architecture Lee et al. (2018) with encoder SpanBERT. However, there have been more recent architectures surpassing the performance of Lee et al. (2018) over benchmark ON (Dobrovolskii, 2021; Kirstain et al., 2021). Our key finding that transferring the mention detector component can still be adopted."
684,"Even though our generalized UD can get comparable performance on some generative tasks, generalized UD may not handle certain complex generation tasks very well (e.g., summarization) We leave expanding UD to solve a broader range of generative tasks and achieve greater performance advantage as our future work."
685,"The contribution of this paper is mainly theoretical. Like most of the POS identification algorithms, the optimization of a criterion among the space of all partitions requires the use of heuristics, and finding the optimum is never guaranteed. Additional work is required before a generalization model that is efficient in practice can be obtained."
686,"section 7
7 A2. Did you discuss any potential risks of your work? We cannot imagine any risk
3 A3. Do the abstract and introduction summarize the paper’s main claims? 3
7 A4. Have you used AI writing assistants when working on this paper? 7
B 3 Did you use or create scientific artifacts? section 5"
687,"Limitation part.
7 A2. Did you discuss any potential risks of your work? There is no potential risk in our work."
688,"Section ""Limitations"" (7th Section)"
689,"SWIPE focuses on the English language. Although it is possible that some aspects of the work – such as the edit categorization – might transfer to the study of text simplification in other languages,
we focus on the English language. As of the writing of this paper, there is no equivalent of Simple English Wikipedia for other languages on Wikipedia, and creating similar resources for other languages would require finding other resources.
Difficulty in Retracing Original Editing. By matching revisions of Wikipedia pages that are factually aligned, and working with SEW editors to annotate the edits, we attempted to match the process used to create the resource. It is however not possible to recruit all 5,000+ SEW editors and for some page pairs the annotations are another editor’s best attempt to reconstruct the intended edits by the original editor.
Improving Annotation Reproducibility. The analysis we conduct in Section 4.2 reveals that our annotators achieve moderate agreement on samples repeatedly annotated. More detailed analysis reveals that agreement is generally strong from common edit categories such as Lexical Edits, semantic deletions, or sentence splitting, but is lower for more infrequent categories. Better training of annotators on tail categories could therefore likely improve annotation. We also found that discussion amongst annotators of a sample often led to eventual consensus. Therefore collecting multiple annotations per sample, and allowing for discussion when multiple interpretations occur could help improve annotation quality, but at an increased cost."
690,"In this paper, we conducted investigation on MPNN-based KGC models. MPNN-based models learn the node representations through aggregating from the local neighborhood, which differ from some recent path-based works that learn pair-wise representations by integrating the path information between the node pair. Moreover, we mainly focus on the KGC task which is based on knowledge graph, and thus other types of graph (e.g., homogeneous graph) are not considered. Therefore, our findings and observations might not be applicable for other non-MPNN-based models and non-KGC task."
691,"Long documents, intuitively, have more possible translations than short documents, so a dynamic number of generated translations may be a better choice when augmenting the data, which balances the training cost and the performance gain. Another potential solution is to sample a few translations and force the MT model to match the dynamic distribution of the DA model using these translations as decoder input, similar to Khayrallah et al. (2020). Such dynamic sampling and matching could potentially be used to increase training efficiency. We do not investigate the solution in this paper and leave the exploration of this topic to future work.
Target-side augmentation can potentially be applied to other seq2seq tasks, where the data sparsity is a problem. Due to the limitation of space in a conference submission, we will leave investigations on other tasks for future work."
692,"The Limitations section
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
693,limitations
694,"Our unsupervised speech recognition approach requires tools to phonemize text for the language of interest. Phonemizers are not available for all languages and this presents a bottleneck. To address this, future work may develop phonemizers for more languages, explore phonemization approaches that generalize across languages, or wav2vec-U 2.0 model training with graphemic text units such as letters.
We train bilingual unsupervised machine translation models with 2.1B English sentences and at least 46M sentences for the non-English language. For extremely low-resource languages, collecting millions of sentences for model training can be challenging. The feasibility of mBART-based online back-translation approach in this setup remains to be validated."
695,"A limitation of our work is that our Graph Traversal Algorithm (Section 3.1) is a heuristic and unlearned algorithm. This leads to a number of nodes after being selected by this algorithm are not suitable for the model to generate conversational questions, and are eventually filtered out by other modules. Future works can focus on more advanced techniques to guide the model to select the nodes such as Graph Neural Networks (Wu et al., 2020). Furthermore, our algorithm to select the relevant turns in the conversational history to generate the conversational questions is a heuristic of selecting a maximum of three previous turns. This heuristic may not be optimal for the model to gather necessary information from history to generate conversational questions in the next turns, as discussed by Do et al. (2022)."
696,"Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
697,"Our f -DISTILL variants are less efficient to train than SeqKD and ENGINE, as we require the teacher’s soft probabilities instead of hard, sampled sequences. However, our methods achieve a significant performance improvement, and more importantly, the additional training time does not affect inference when the model is deployed. This follows the spirit of knowledge distillation in general, i.e., to obtain a small and efficient model for deployment.
Another potential threat to validity is that we have not reported multi-run statistics. In our preliminary experiments, we ran our approach multiple
times and found results were generally consistent. Due to our excessive experimentation (estimated at 2000 GPU hours), it is not possible to run each model multiple times. We instead adopted a wide range of established automatic metrics, consistently showing the effectiveness of our approach. We further conducted in-depth analyses to better understand our proposed framework. We deem multi-run statistics not crucial to this paper, as this paper does not purely focus on empirical analysis. Rather, our main contributions lie in the novel machine learning framework, f -DISTILL, and the theoretical connections between step-wise and sequence-level f - divergence functions."
698,"The limitation section is right after the conclusion in Section 5.
7 A2. Did you discuss any potential risks of your work? Our work focuses on knowledge distillation for small models. It does not impose more risk than other machine learning/NLP research."
699,"Please see section Limitations.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
700,The limitations section after conclusion
701,Limitations
702,"Some Language families in Africa not covered For example, Khoisan and Austronesian (like Malagasy). We performed extensive analysis and experiments on Niger-Congo languages but we only covered one language each in the Afro-asiatic (Hausa) and Nilo-Saharan (Dholuo) families.
News domain Our annotated dataset belong to the news domain, which is a popular domain in UD. However, the POS dataset and models may not
generalize to other domains like speech transcript, conversation data etc.
Transfer results may not generalize to all NLP tasks We have only experimented with POS task, the best transfer language e.g for non-Bantu NigerCongo languages i.e Wolof, may not be the same for other NLP tasks."
703,"In the limitations section.
7 A2. Did you discuss any potential risks of your work? Left blank."
704,"In this section, we make a clear discussion of the limitation of our work. Our work mainly leverages a pre-training scheme to enhance the encoding of speech for video grounding. However, the adopted audio data (i.e. Libri Speech) for pre-training are different from the one in the grounding dataset (i.e. ActivityNet Speech). This could lead to performance degradation due to the domain gap. The findings could inspire the researchers to explore a better pre-training strategy to learn domain-invariant and effective speech representations for grounding."
705,Section Limitations (Line 558-570)
706,"Although the proposed method has verified the feasibility of the idea that constrains both naturallanguage and cross-modality spaces together, it is still necessary to explore more ways to better combine the output of two encoders. Third, our method involves multiple offline knowledge retrieval processes, such as retrieving relevant Wikipedia passages, which will make it difficult to deploy our model as an online model."
707,Limitations Section
708,"This paper has three main limitations worth noting. First and foremost, while our paper aims to model the social context in which a message is said, the current context is limited to only the parties’ relationship. In practice, the social context encompasses a wide variety of other factors, such as the sociodemographics of the parties, the culture and setting of the conversation, and the history of the parties. Even relationships themselves are often much more nuanced and the appropriateness may vary widely based on setting, e.g., statements said between spouses may vary in appropriateness when made in public versus private settings. These contextual factors are likely necessary for a full account of the effect of social context on how messages should be perceived. Our work provides an initial step in this direction by making the relationship explicit, but more work remains to be done. Future work may examine how to incorporate these aspects, such as by directly inputting the situation’s social network as context using graph embedding techniques (Kulkarni et al., 2021), where the network is labeled with relationships (Choi et al., 2021), or by modeling relationships particular types of settings such as in-person, phone, texting, or other online communication, which each have different norms.
Second, our data includes annotations on a finite set of relationships, while many more unique relationships are possible in practice, e.g., customer or pastor. Our initial set was developed based on discussions among annotators and aimed at high but not complete coverage due to the increasing complexity of the annotation task as more relationships were added. Our results in Section 5 suggest that our best model could be able to generalize to new types of relationships in some settings and zeroshot results on two new relationship types not seen in training (a fellow church member and a commercial relationship) match expectations of context sensitivity, (cf. Figure 5 . However, performance is likely limited for less-common relationships without additional training data to describe the norms of appropriateness in this context; and, based on the error analysis in Section 4, models are currently unlikely to generalize to unseen relationships that
have complex sensitivity norms. In addition, new settings such as online spaces may require additional definitions of relationships as individuals interact with each other anonymously.
Third, our judgments of appropriateness were drawn from five annotators total, each of whom had different views of appropriateness based on their values and life experience. While our analysis of agreement with the Adjudicated data (Section 3.2) suggests that when annotators can reach a consensus on a message’s meaning, they are highly likely to agree on appropriateness, we nonetheless view that our annotations are likely to primarily reflect the values of the annotators and may not generalize to other social or cultural contexts where the norms of relationships differ. Future work is needed to explore how these norms differ through additional annotation, and we hope that our dataset will provide a reference for comparison to these judgments. For example, future work may make use of annotation schemes that explicitly model disagreements (Fornaciari et al., 2021) or personalized judgments (Plepi et al., 2022); such approaches may be able to better represent common factors influencing appropriateness judgments."
709,"Our approach is based on meta-learning and is designed for constrained situations where computing resources are limited, such as on-device settings. Therefore, using large and complex feature encoders like LLM may pose scalability challenges. In addition, if the task involves a significant number of new classes, the model may not scale effectively. Lastly, our method is primarily suitable for text classification, such as news category or product review classification. It is not appropriate for text generation tasks."
710,"GPU resources. This work utilizes extremely large language models and thus has a high cost on GPU resources. Concretely, experiments are conducted on the 8 x NVIDIA A100 GPU station. The maximum inference time on each version of COFE (containing 4,785 test cases) is ∼ 8 hours. The maximum estimation of costed computing resources in this study is ∼ 500 x 8 GPU hours.
Synthetic data. As in most previous work on compositional generalization (Lake and Baroni, 2018; Keysers et al., 2019; Kim and Linzen, 2020), the COFE dataset is constructed using synthetic data rather than natural one. The source-side sentences in COFE are from COGS, which account for 70–80% of naturally-occurring English sentences (Kim and Linzen, 2020; Roland et al., 2007). Thus, this synthetic test suite could be close to the real-world application scenarios.
Single run. Due to the high cost on computing resources, we do not take multiple runs with different sets of examples, nor did we take multiple samples with temperature > 0. Observations under different prompt orders (in Appendix 4.4) imply that with desired factors in selecting in-context examples, there could be low variance in experiments."
711,Limitations
712,"While our work displays many strengths, we highlight some important limitations in our analysis. Namely, we pretrain our STAMP models on a range of sources containing structured knowledge, however our analysis is limited to text-to-SQL tasks and does not demonstrate if such pretraining helps more generally in structured information tasks. For instance, STAMP pretrains on tables with (1) masked column recovery as a way to learn the structure of a table using the rows and natural language statement as context, and (2) a context-to-output objective that always includes the table in the context (when available) — since this matches the format of textto-SQL tasks. It is unclear if our objective choices for pretraining on tables perform equally well on the range of structured knowledge tasks, such as table question-answering, table summarization, datato-text, fact verification, and others explored in Xie et al. (2022). Second, we acknowledge that significant GPU resources are required for pretraining, even in continued pretraining approaches like ours which limit the breadth of ablations studies. Conversely, our work explores pretraining at smaller scales where certain phenomena like strong zeroshot performance is unlikely. Pretraining specifically on structured knowledge has an unknown value at larger scales with models having tens or hundreds of billions of parameters."
713,"The ’Limitation’ section.
3 A2. Did you discuss any potential risks of your work? 5
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 3,4"
714,Limitation section
715,"Here we discuss several limitations of our work and point to potential future work directions. First, we focus on single teacher and single student setup to study guidance generation whereas in real life there often are multiple teachers and students. We plan to extend to multi-party goal-driven communication and D&D also provides a proper testbed to study this problem.
Second, there are more nuances in guidance: railroading direct guidance (“make a persuasion check”) and subtle indirect guidance (“the guards seem to be a bit shaken”). We did include them in our human labeling and evaluation interface but did not specifically distinguish them during modeling.
Third, due to the constraints on input sizes for most LMs, we have to set a context window to study dialogue generation in D&D. However, both DM and players have a long-term memory about the comprehensive story progression which might influence how they communicate. As a next step, we plan to use summarization models and adventure books as narrative backgrounds to ground our
G4C task with a larger world setting. We include answers to other Frequently Asked Questions (FAQ) in Appendix A."
716,The limitations are discussed in the first section after the conclusion.
717,"Our work is subject to certain limitations, one of which pertains to financial constraints that hindered the ability to conduct large-scale experimentation with the data annotation methods proposed. As a result, the findings of this study may not be fully representative of larger datasets or populations. Additionally, the utilization of GPT-3 as a model presents challenges in terms of interpretability, as it operates as a ""black box"" system. To further investigate this subject, it would be bene-
ficial to conduct larger-scale experiments and to compare the performances of GPT-3, ChatGPT9, and GPT-4 (OpenAI, 2023) and the open-sourced LLMs like LLaMA (Touvron et al., 2023)."
718,"We compare 12 representative methods, present a unified view on existing prototype-based methods, and propose a competitive unified baseline by combining the advantageous modules of these methods. We test all methods, including the unified baseline, on three commonly-used English datasets using various experimental settings and achieve consistent results. However we acknowledge the potential disproportionality of our experiments in terms of language, domain, schema type and data scarcity extent. Therefore, for future work, we aim to conduct our empirical studies on more diverse event-detection (ED) datasets.
We are fortunate to witness the rapid development of Large Language Models (LLMs Brown et al. 2020b; Ouyang et al. 2022; Chung et al. 2022) in recent times. In our work, we set incontext learning as a baseline and evaluate the performance of LLMs on few-shot ED tasks. We find current LLMs still face challenges in dealing with Information Extraction (IE) tasks that require structured outputs (Qin et al., 2023; Josifoski et al., 2023). However, we acknowledge the ICL approach adopted here is relatively simple. We do not work hard to find the optimal prompt format, demonstration selection strategy, etc., to reach the upper bounds of LLMs’ performance. We view how to leverage the power of LLMs on ED tasks as an open problem and leave it for future work.
In this work, we focus more on the model aspect of few-shot ED tasks rather than data aspect. In other words, we assume having and only having access to a small set of labeled instances. In the future, we plan to explore how to utilize annotation guidelines, unlabeled corpus and external structured knowledge to improve few-shot ED tasks."
719,Limitations and Ethics Statement
720,"Yes, in the Limitations section"
721,"Interpretability. Although ALIGNSCORE shows high correlation with human judgments, it is hard to interpret the reasoning behind its predictions. Therefore, an interesting future research direction is to develop interpretable factual consistency metrics that can accurately identify words or spans in the input that contain factual consistency errors and (or) produce human readable explanations justifying its predictions.
Synthetic data. Our alignment training data contains datasets augmented with synthetic data. While ablation studies show that synthetic data helps improve metric performance, our rule-based method for generating synthetic data could generate noisy data that may not accurately model the error types and distributions produced by real world generative systems. Thus, analyzing the quality of synthetic data and developing more effective ways to generate synthetic data is an interesting research topic.
Language coverage. While we show ALIGNSCORE generalize well to unseen data, it only covers a single language, English. Undoubtedly, factual consistency evaluation is also important for more resource-constrained languages or in a multilingual setting. Consequently, future research could focus on extending the Align metric to multiple languages, including resource-constrained languages."
722,In the Limitations section
723,"In this paper, we introduce the concept of multigranularity temporal question answering and construct a benchmark dataset MULTITQ, which features ample relevant facts and multiple temporal granularities. We also propose a multi-granularity temporal question Answering model MultiQA, serving as a strong baseline for follow-up research.
Limitation. The main drawback of our data creation protocol is that the question/answer pairs were generated automatically, leading the question distribution to be artificial from a semantic perspective. In addition, the KG adopted in the research focuses on a single event domain, and extending the dataset to multiple domains is planned as future work."
724,Section 6 Conclusion and Limitation
725,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
726,"Our proposed approach is based on the premise that faithfulness errors observed in generation systems are due to noise in the dataset. While there is substantial evidence for this from prior work, and our method outperforms existing approaches on the datasets we used, it’s possible the the utility of our approach could drop in cases where we have clean, curated datasets. It’s possible that certain generation errors made by the model could be due to spurious patterns learned by the model that do not generalize well. In such cases, it’s unclear whether using our error attribution approach to remove training instances would alleviate the problem. However, as most large-scale datasets in natural language generation tend to be sourced from the internet, it’s inevitable that these datasets will likely contain at least a few erroneous examples that could lead to undesirable model generations. Therefore, we believe that our approach to using error attribution to clean datasets is still a valuable method to improve generation systems."
727,"In this work, we evaluated neural networks on the task of back-translating mathematical formulae from the PL LATEX to semantic CLs. For this purpose, we explored various types of neural networks and found that convolutional neural networks per-
3For example, LATEXML denotes the binomial as \genfrac{(}{)}{0pt}{0}{n}{k} instead of \binom{n}{k}
form best. Moreover, we observed that the perplexity of the translation of mathematical formulae behaves differently from the perplexity of the translation between natural languages.
Our evaluation shows that our model outperforms the Mathematica software on the task of interpreting LATEX produced by Mathematica while inferring the semantic information from the context within the formula.
A general limitation of neural networks is that trained models inherit biases from training data. For a successful formula translation, this means that the set of symbols, as well as the style in which the formulae are written, has to be present in the training data. Mathematica exports into a very common flavor / convention of LATEX, while semantic LATEX, translated by LATEXML, yields many unconventional LATEX expressions. In both cases, however, the flavor / conventions of LATEX are constant and do not allow variation as it is produced by a rule-based translator. Because of the limited vocabularies as well as limited set of LATEX conventions in the data sets, the translation of mathematical LATEX expressions of different flavors is not possible. In addition, we can see that a shift to a more difficult domain, such as special functions in the DLMF, produces a drop in performance but still generates very promising results. In future work, the translator could be improved by augmenting the data set such that it uses more and different ways to express the same content in the source language. As an example, a random choice between multiple ways to express a Mathematica expression in LATEX could be added. For semantic LATEX, the performance on real-world data could be improved by using multiple macro definitions for each macro. Ideal would be a data set of hand-written equivalents between the PLs and CLs. An addition could be multilingual translation (Johnson et al., 2017; Blackwood et al., 2018). This could allow learning translations and tokens that are not present in the training data for the respective language pair. Further, mathematical language-independent concepts could support a shared internal representation.
Another limitation is that data sets of mathematical formulae are not publicly available due to copyright and licensing. We will attempt to mitigate this issue by providing the data sets to interested researchers.
Note that this work does not use information from the context around a formula. Integrating
such context information would aid the translation as it can solve ambiguities. For example, for interpreting the expression (x)n, information about the specific field of mathematics is essential. Further, context information can include custom mathematical definitions. In real-world applications, building on such additional information could be important for reliable translations."
728,"We utilize safety classifier modules, such as the BAD classifier and Perspective API, as the red team classifier to automatically identify offensive output from the victim model following the practice in Perez et al. (2022). However, automatic classification of offensive outputs can be subject to inaccuracies, which may lead to the identification of false positive test cases (Gehman et al., 2020). To mitigate this issue, we may increase the threshold for positive texts to reduce the number of discovered false positive test cases. One other choice is incorporating human supervision into the classification. For example, we may assume the human-in-theloop scenario that has access to the offensiveness scores evaluated by human annotators within a limited number of queries to the annotators. In this scenario, we can either directly conduct BRT with human annotators as the red team classifier or modify the BRT method to incorporate offensiveness scores from both human annotators and the safety classifier modules during red teaming. Further exploration of these possibilities is left as future work."
729,"On Page 9, the first unnumbered section, ""Limitation"""
730,"after conclusion
7 A2. Did you discuss any potential risks of your work? currently don’t find any risks"
731,"The main novelty of our proposed MIR-GAN is refining frame-level modality-invariant representations via adversarial learning. It is promising to combine this approach with the popular selfsupervised pre-training to learn unified multimodal representations. In this work, we only load pretrained AV-HuBERT for the front-ends and speech recognition model, while the proposed modules (i.e., encoders, generator, discriminator) are still trained from scratch. In future, we may include the entire MIR-GAN into self-supervised learning scheme, together with the adversarial learning to refine better multimodal representations."
732,"While HUQ outperforms individual aleatoric and epistemic UE methods for most datasets considered, for some, the effects are negligible. To understand this pattern, we analyze the difference between the training and test sets. We generate latent representations of instances in the datasets
using a fine-tuned ELECTRA model and fit a logistic regression model to discriminate between train and test sets using these representations as features. Good performance of the discriminator indicates a covariance shift between the training and test data, while bad performance indicates that instances come from the same distribution.
Table 4 presents F1 scores for this task aligned with the performance gains of HUQ-DDU in percentages over the best method from the pair <SR, DDU>. As we can see, high F1 scores often correspond to low values of performance gains (the Spearman rank correlation = 0.8). This means that HUQ is unlikely to provide improvements to the base methods for the tasks with big covariate shifts. In our analysis, this is due to prediction mistakes primarily arising from OOD instances, which are well-handled by epistemic UE methods.
Visualizing the differences between the datasets using a t-SNE decomposition of the latent representations (see Figure 4), we can see that for IMPLICITHATE and TWITTER, where HUQ does not provide improvements, some regions of the test data are not covered by the training set. For PARADETOX and TOXIGEN, on the other hand, the training dataset completely overlays all regions of
the test data, and using HUQ improves AUC-RC on the base methods."
733,
734,"Yes. Section 6.
7 A2. Did you discuss any potential risks of your work? We did not find our work present major risks."
735,"Our experiments reveal that simultaneously incorporating more rules into the loss produces better
performance in the task of interest (Table 4). These results indicate that rules working in tandem significantly complement supervision coming from both sources of direct annotation under a fully declarative loss. Nevertheless, controlling the influence of each term in the loss is crucial for training stability. We found that the system has different sensitivities to each term in the loss, requiring a full search over the λ hyper-parameters (15). From this perspective, the possible benefits of increasing the number of rules in the loss come at the cost of more difficult learning.
Due to hardware limitations of the protected environment server that stores the datasets we use, RoBERTa-base was the best model that could fit in the available GPUs. Although other pre-trained embeddings could provide better performance, we argue that this is orthogonal to our contribution of incorporating indirect supervision under a fully declarative learning framework. Moreover, integrating logic-driven frameworks and prompt-based models like T5 is an interesting future line of work.
Choosing RoBERTa as the underlying embedding foundation of our system introduces all the inherent limitations of large language models (Bender et al., 2021). From this standpoint, we envision the application of these sorts of systems as a humanguided tool used only for counselor training and quality assurance, and never for real counseling sessions."
736,"Our conceptualization of the core task has some important limitations. We highlight three main limitations here. First, in order to tie a character to a place, we require that both the character and the place are explicitly mentioned in the text. This simplyfying approach helps annotation and modeling but is inadequate against the general setting of grounding any character at any time in the story.
Another limitation with our current approach is the assumption that the location of a character is independent at every instance in the story. It is because of this assumption that we can label every character and location co-mention without considering any other labels. In reality, however, location of a character at some time is highly dependent on the location of the character at a previous time.
Finally, the spatial relationship categories are designed to be coarse. This is helpful in setting up the task as a classification task but collapses information that can be useful. For example, if a character is described to be standing outside the southern gate of a building, our current approach will assign the NEAR label retaining only the aspect of distance and not the spatial orientation."
737,right after the main paper on page 9
738,"Given that many special properties of ChildDirected Speech are not present in text, we would have liked to work on a multimodal dataset, where both visual and speech information would be present. More specifically, we would have liked to test the effect of the following:
• Grounding the language models in vision to test the effect of joint attention (Rowe, 2012; Akhtar and Gernsbacher, 2007). Joint attention refers to the phenomena where the caregiver’s and the child’s coordinated attention to each other to a third object or an event.
• Child-Directed Speech is known to have special prosodic properties such as higher variability in pitch (Fernald et al., 1989; McRoberts and Best, 1997; Papousek et al., 1991), lengthening of vowels and pauses (Albin and Echols, 1996; Ratner, 1986; Fernald et al., 1989), context-specific intonational contours (Katz et al., 1996; Papousek et al., 1991; Stern et al., 1982). These properties have been suggested by many researchers to serve as a mechanism for getting the infants attention (Cruttenden, 1994; Ferguson, 1977; Fernald, 1989). This attentive role may be considered to be beneficial for language development in children (Garnica, 1977). As our models only take text as the input, we were unable to test the relationship the between these properties and language acquisition in neural network based models have.
• Caregivers give a lot of feedback when young children are first producing and acquiring language (Soderstrom, 2007). Our current mainstream language models are not interactive.
Therefore, it is difficult to incorporate the feedback loop and the test the effect of the same in models’ language acquisition.
As it is, our findings suggest that many of the most important facilitative features of ChildDirected Speech are relevant to precisely those formal and conceptual aspects of language acquisition that are not captured by text-based language models.
In this paper, we have tested the effect of native CDS in L2 acquisition with 5 typologically diverse languages. However, there is enormous scope to test the effect of the same with many more different languages, which may lead to more pointed implications and conclusions than the findings offered here."
739,"Despite the fact that we demonstrate strong OSSC performance with low generation quotas in Appendix 5.3, CoNAL still is slightly more computationally expensive than vanilla training. It also requires access to a pretrained LLM with which to generate novel examples. To achieve optimal performance, usage of the OpenAI API is required, which poses some concerns around transparency, as details around GPT-3 training and data are not publicly released. Finally, performance varies across datasets, suggesting that types of outliers that are unexpected to LLMs might still confuse a CoNALtrained model."
740,"Although comprehensive, our study of MPT in this work has couple of limitations: • As mentioned in §5.3, because of infeasiblity
to search for optimal hyperparameters for each of the meta learning methods in each of the ten settings, we choose to use the R→R setting as our main representative setting. This could be one of the reasons for MPT underperforming MTL in some non-classification tasks (noted in §6-Q1).
• We mainly focus on how upstream meta learning can improve the performance on target tasks. However, meta learning also enables faster convergence. We leave how it could help reduce the convergence time of PT as future work. Aside from that, meta prompt tuning (MPT) as a
method has a limitation that it is Memory-intensive. Optimization-based meta learning methods, especially MAML, are memory-intensive, which limits the tuning of the inner batch size and inner update steps (§5.3). One potential solution is to build more memory-efficient meta learning libraries."
741,"section 6 and limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
742,"Our work is a comprehensive empirical study of a popular large language model’s capacity to perform in-context learning, relying on both task-specific (via a wide variety of challenging and practically relevant downstream tasks) and task-agnostic (via looking for induction heads) analyses and connecting the two via correlation/overlap investigations. We do not claim a causal link, i.e., we do not claim that an attention head that acquires the capacity to be an induction head will become capable of more sophisticated in-context learning associated with our downstream tasks. Making this claim will require a more deeper investigation that is outside the scope of this paper. We also do not fully understand why most attention heads seem to be unimportant for in-context learning and why there is an overlap in (un)important attention heads across tasks and shots, which warrant further investigation. Other more obvious limitations to our work include our use of only up to 5 in-context examples, random selection of in-context examples for a query input and our choice of all monolingual downstream tasks."
743,"Section 7, after the conclusion section."
744,Limitations section
745,"The limitation of CITADEL mainly shows in two aspects. First, at the beginning of training, the model needs to route each token vector to multiple activated keys for token interaction, which increases the computation cost compared to COIL and ColBERT. This results in slower training speed but it gets better when training approaches the end as more tokens are pruned by the ℓ1 regularization. Another drawback lies in the implementation of CITADEL, or more generally speaking, most multivector retrieval methods. The token-level retrieval and aggregation make them not compatible with established search libraries such as FAISS or Pyserini. Moreover, for time and space efficiency, multi-vector retrieval also requires more engineering efforts and low-level optimization. Recently, XTR (Lee et al., 2023) provides a solution that constrains the document-level retrieval to be consistent with the token-level retrieval during training, which can be used for streamlining CITADEL."
746,Please see Section Limitations
747,"We use RoBERTa-base models trained on a single 12GB memory GPU (we used a NVIDIA Titan XP graphics card) for our experiments. Obtaining annotations for cognitive dissonance are limited by the availability of annotators and is not easily scalable in crowdsourcing platforms due to the required training and expertise in identifying dissonance. Due to this limitation, only two iterations of the AL loop for each setting were feasible for experiments. The transfer learning experiments in this paper were limited to two similar tasks, but there might be other tasks that could further improve or exceed the zero-shot performance of the models to cold start the active learning.
We focus on fine-tuning and active learning selection strategies to improve performance of rareclass classification for a specific task: dissonance detection across discourse units. Therefore, fur-
ther work would be necessary to determine if the findings extend to other tasks. Additionally, the results may be different for other languages or time intervals of data collection. The performance of the neural parser on splitting tweets into discourse units can produce parses that are imperfect but the annotators and our systems worked off its output regardless to keep the process consistent. An improved discourse parser may also lead to improved annotator agreement and/or classifier accuracy. The dataset that we release from this paper, which contains labels of expressions of some cognitive states, was constructed using criteria that may not be fully objective."
748,"Our work is limited by several factors. First, we conduct our work primarily using popular, publicly available dementia detection datasets, all of which are in English. Thus, it is unclear whether our findings generalize to other languages, especially with richer morphology where different predictive patterns may emerge. Second, due to the emphasis on feature-based models in most dementia detection work, we study only feature-based and instance-based DA approaches. Neural DA approaches may yield different findings, although they are less relevant for many current dementia detection approaches. Finally, we only study two backbone classification algorithms in our experiments. These classifiers are among the most common in prior work with our selected datasets; however, it may be the case that with a wider scope, other classification algorithms may yield different results. Collectively, these limitations present intriguing avenues for follow-up work."
749,"While our approach inherits the linear runtime complexity of the backpropagation algorithm, runtime concerns should not be fully neglected. Firstly, the linear runtime is only an analytical result, not an empirical measure. This means that the actual runtime of the backpropagation and thus our algorithm depend heavily on their implementation. For instance, some deep learning frameworks do a better job at reusing and parallelizing computations than others (Goodfellow et al., 2016). Indeed, our code is optimized for good readability and extensibility at the expense of speed, which hints at another limitation of our approach: Our approach requires deep integration with the framework as it needs access to all model weights and the computation graph. For this reason, our approach cannot be easily packaged and wrapped around any existing model or framework and we instead developed our own JAX-based reverse-mode autodifferentiation library, based on the numpy-based Brunoflow library (Ritchie, 2020). While we release our library to enable other researchers to analyze models through their gradient graphs, it faces some computational and memory constraints. In our experiments, running the three semirings together on a single sentence can take several minutes (depending on sentence length) using google/bert_uncased_L-6_H-512_A-8, the 6-layered pretrained BERT from Huggingface (Wolf et al., 2020), totaling our experimentation time on our datasets at about 10 CPU-hours. For improved adoption of this method, we encourage the direct integration of semiring implementations into the most popular deep learning frameworks. Our final point pertains not only to our study but to most interpretability approaches: One has to be careful when drawing conclusions from gradient paths. Cognitive biases, wrong expectations, and omitted confounds may lead to misinterpretation of results.
Ethics statement
We foresee no ethical concerns with this work. Our work aims to make the inner workings of neural network models more interpretable. On this account, we hope to contribute to reducing biases inherent in model architectures, pre-trained model weights, and tasks by increasing overall transparency."
750,Section Limitation at the end of the paper.
751,"Some of our experiments, specifically those in the ablations with large batch sizes, required significant computational resources. We trained these models on Google Cloud TPUv3 Pod slice with 128 chips for a few days. This experiment is important, as otherwise there would be questions on how the models compare at large batch sizes where contrastive models are known to work better. Due to training costs and in the interest of open research, we will open source our code and model checkpoints for the community to use and build upon.
Secondly, VMSST and BITRANSLATION require decoding which which means they need more memory for the decoder and are slower during training. However one advantage of these models is that they can be trained with gradient checkpointing greatly reducing their memory requirements, which cannot be used for the contrastive models as that would reduce the effective batch size for finding negative examples. Moreover, during inference, there is no difference in the memory or speed requirements in CONTRASTIVE, BITRANSLATION, or VMSST as only a single encoder is used in inference and there is no decoding."
752,Section Limitation
753,"Limitations section
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
754,"It should be noted that our contributions are limited to fanfiction documents. Models trained on our datasets might not transfer to other online content like news articles, websites, or social media posts. Particularly social-media texts are shorter and contain fewer descriptions and more verbal expressions, which is a substantial-enough shift to warrant models explicitly trained in the genre. Similarly, the conclusions of our experiments are limited by the models we used, as well as the genre of the text. Furthermore, the trigger warning scheme we used is a simple structure. Further research should investigate more detailed trigger (warning) typologies with a more rich semantics."
755,"Section Limitation
7 A2. Did you discuss any potential risks of your work? Since the number of pages in the text is limited and our model does not have significant potential risks, we do not discuss this."
756,the last section
757,"This section discusses the potential limitations of our work. This paper’s analysis of model effects mainly focuses on common benchmarks for adversarial defence, which may introduce confounding factors that affect the stability of our framework. Therefore, our model’s performance on more tasks, e.g., the MRPC dataset for semantic matching tasks, is worth further exploring. In addition, the present work proposes to conduct adversarial training from the perspective of estimating the overall adversarial loss. We expect a more profound exploration of improving the accuracy and efficiency of such estimation. We are also aware of the necessity to study whether the properties of traditional methods, such as the robust overfitting problem, will also arise in DSRM-based adversarial training. We leave these problems to further work."
758,"section 7
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
759,"Yes, in the ""Limitation"" section"
760,"Due to limited availability of compute resources, we were unable to scale up the model architecture to the large sizes becoming increasingly mainstream today. Similarly, the upstream corpus we used (BookWiki) is 16GB in size, and while it is large enough such that it was used to pretrain BERT (Devlin et al., 2019), much larger pretraining datasets are in use today such as the Colossal Common Crawl Corpus (Raffel et al., 2020). The relative performance achieved by using self-pretraining vs pretraining on upstream corpus can likely vary with the size of the model and upstream corpus, and more compute-heavy large scale experiments are needed to characterize it."
761,"The scope of our approach is intended for our specific task setting, which is proposed as a practical solution to mine open-world attributes without heavy supervision, and has not been studied previously. Our approach does require an external dependency of a POS tagger, and assumes high POS tagging quality on English. Thankfully, there are POS tools publicly available with high performance, and are quite robust against domain shift, mostly fulfilling the assumption.
Our current candidate generation that utilizes syntax-oriented patterns does not check the semantics, which can be another limitation. It introduces noisy spans in the process, such as “supports joint
health & overall” (in Table 15). Future works could consider combining syntax with semantics to alleviate noisy spans."
762,"In this paper, we present a bidirectional generative framework for cross-domain ABSA that has achieved outstanding results on four cross-domain ABSA tasks. Although there is only one stage during inference, our method involves multiple training stages, including text-to-label, label-totext, and final training. These additional training stages not only lengthen the training time but also require additional computational resources, which may hinder scalability for large-scale data and result in a burden for the environment."
763,"As with any natural language understanding task, there are practical limitations and related ethical aspects that must be considered before deploying a system. In particular, our corpus and modeling approach assume that the user-provided REs always refer to one of the two options. If this is not the case, or if the RE is particularly contrived, undesirable or unexpected behavior may occur: For any expression, including for instance one made with arbitrary derisive language, the model would attempt to resolve this to one of the alternative entities. One approach system designers may consider could be to pre-classify any user-provided REs to avoid interpreting those that are off topic or phrased in a negative manner.
A second consideration is that of corpus representativeness. In our case, as this is a first corpus for this task, we have limited ourselves to English Wikipedia, native English speaking annotators, and particular item sampling strategies for practical reasons. However, if used for training a deployed system, the examples present may bias any model to understand specific types of references but not others. Similarly, the items in our corpus are sufficiently popular to have a relatively long Wikipedia entry, whereas items not present in Wikipedia, or with only minimal information, may exhibit different characteristics."
764,Limitations section
765,"The description of limitations has been placed at the end of the paper, please see Limitation section for more details."
766,"Sec. Limitations after Sec. Conclusion.
7 A2. Did you discuss any potential risks of your work? We do not see a substantial risk of our work. Although the output could contain toxic or biased content, we posit that it does not attribute to the search algorithm we propose."
767,Limitations (after the conclusion)
768,"The TAED model has slightly more parameters than the corresponding Transducer model due to the attention modules to connect the speech encoder and AED decoder. They have similar training time for the offline models. However, the optimization of the streaming model would require more GPU memory and computation time due to the chunk-based RNN-T synchronization scheme described in §2.1. In our experiments, the streaming TAED model takes about three times more training time than the offline model on the 16 A100 GPU cards, each having 40GB of GPU memory.
In this work, we evaluate our streaming ST al-
gorithms on two translation directions: EN→ES and EN→DE. The word ordering for English and Spanish languages are based on Subject-VerbObject (SVO) while German is Subject-ObjectVerb (SOV). The experiments validate the streaming algorithms on both different word ordering pair and similar word ordering pair. Our future work will extend to other source languages besides English and more language directions."
769,section 8
770,"In this paper, we have unearthed a variety of problems present in current evaluation benchmarks that favor systems over humans, or that simply make such comparisons unfair. We conclude that there is no real evidence to claim that today’s language models possess superhuman performance. However, without empirical results obtained under the right setups, we cannot even claim the opposite, namely that humans are still better than systems. We leave such demonstrations for future work.
Additionally, while a good portion of the NLP research effort is devoted to natural language generation (NLG) tasks (which includes MT), here we provide only some pointers to NLG/MT. Indeed, as discussed in Section 4.3, these problems exist in the NLG universe as well, but, due to space constraints, we limit our analysis to NLU tasks."
771,"Section 8
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
772,the limitation section
773,"We have limited ourselves to experimenting with only five languages due to lack of data for both the pretrained models and the DeepSpin tokenizer models. Although there are annotated data for some low-resource polysynthetic languages such as Nahuatl, Raramuri, Wixarika, Shipibo-Konibo (Mager et al., 2020) and Kunwinjku (Pimentel et al., 2021), the available data was below 1M and therefore not enough to create pretrained models for our experiments.
Regarding the aforementioned limitation, DeepSpin which has proven to be a good option to mitigate the problem of high TTR languages in closed vocabulary environments is a supervised method that requires the availability of training data. As
can be seen in Table 2, to achieve 90% to better accuracy DeepSpin requires around 350K annotated words. This can be a major drawback for low-resource languages, although the results with less annotated data are still competitive. We have not studied another source of differences in the vocabulary size that could be due to the texts used in pretraining. Ortiz Suárez et al. (2019) found that, in general, the OSCAR samples contain more vocabulary words than the Wikipedia ones. Additionally, the Quechua corpus we have used also consists of educational and legal texts that can increase the number of different types, compared to Wikipedia texts.
On the other hand, we believe it is important to mention that for the Quechua language the training, evaluation, and testing data for NER and POS tasks were obtained from the same corpus used for training the language model. Note that, due to the scarcity of available digital and physical texts in that language, it is difficult to do it otherwise. The limited availability of texts leads to the use of the same corpus for multiple tasks, which could have implications on the evaluation of the obtained results. For instance, if the training corpus contains an unequal proportion of certain types of grammatical structures, it might negatively affect the performance of POS classifiers. Furthermore, if the corpus does not adequately reflect the linguistic variability and diversity of Quechua, the resulting models are likely to be less accurate and less generalizable.
Ethical Considerations
The datasets used in this paper for the training and evaluations of the pre-trained models, DeepSpin models, and fine-tuned models have been extracted from various previous articles and open-access repositories, therefore, we abide by the ethical rules by citing the original authors of each dataset. On the other hand, the annotated Turkish and Quechua data that were constructed by us for the development of the DeepSpin models will be presented in a forthcoming paper for public use. In addition, we encourage authors who use the resources in this article to cite the original sources. Finally, we would like to note that one of the authors of this paper has a long history of working with resource-poor synthetic languages, especially Quechua, which allows us to better understand the problems and concerns of the Quechua-speaking communities."
774,"Limitations have been described as separate section after the Conclusions, as required by the ACL instructions."
775,"In this section, we summarize the limitations of our work as follows:
• There is still a lot more to explore in terms of event co-occurrence for EAE (e.g., iterative extraction, course learning, etc.). We are unable to cover all in this work and will explore further in the future.
• As demonstrated by our ablation study, the high performance of our model greatly relies on the manual prompts. This limits the application of our model to the scenes where high-quality prompts are unavailable and difficult to construct. To address this, we should look into the area of automatic prompt construction.
• Our work ignores the phenomenon of entity coreference commonly existing in narrative documents. This limits the model’s ability to figure out the underlying relation between entities, which is crucial for the task of EAE. And we will take entity co-references into account in our future works."
776,"Section 7.
7 A2. Did you discuss any potential risks of your work? No potential risk is forseen."
777,"Due to the limited ability of the policy model, most theorem still failed to find the proof because of poorly suggested proof steps. Predicting the proof step from the proof state requires substantial reasoning ability. It’s observed in the experiment that the language model tends to produce the same proof step in training data, and is unsatisfactory in generalizing for new states. Another limitation resides in the proof-level value functions. Although the performance of the proof-level value functions shows promising improvement in the value function test set. The end-to-end pass rate diminishes the performance gap. This accounts for two major reasons: 1) weak policy model fails to produce correct action even if our value function correctly located the state to expand. 2) Our value function’s performance is still behind the performance threshold where the value really helps the search drastically. One future direction is to enhance the language model for better reasoning ability by using a larger language model or adding symbolic reasoning into the language model to produce more reasonable proof steps and better evaluate states’ value."
778,"No section number but directly following conclusion.
7 A2. Did you discuss any potential risks of your work? We do not see that our work introduces any new risks over the already published and publicly available previous work. In fact, we believe that better rationale plausibility improves interpretability and fairness, thereby reducing the risk that black-box models pose to the general public, especially to the historically disadvantaged groups.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 5, 8"
779,Limitations section after conclusion
780,"Downstream Tasks In this work, we focused on long-document summarization as we believe it is the task where controllable summarization is most needed. Future work could investigate the effect of SOCRATIC pretraining on other downstream applications beyond those studied here. To handle long document input we could not use the BART model with SOCRATIC pretraining adaptation directly. Instead, we applied the SegEnc architecture on top of BART. This adaptation of the pretrained model may have dampened some of the few-shot performance of SOCRATIC pretraining. We thus believe that tasks with shorter input documents for which the SegEnc architecture is not necessary would see even greater benefits in the low-resource setting.
Base Model Throughout this work, we restricted our analysis to one model architecture the SegEnc architecture with the BART base model. Previous work extensively studied the impact of different architectures for long-document query-focused summarization (Vig et al., 2022). These primarily differ in how they model long documents. The authors found SegEnc, a simple sliding window adaptation of BART, to perform best on QMSum. While the results presented here are specific to SegEnc and BART, our approach is agnostic to the underlying model architecture and is orthogonal to longdocument modeling. We leave it to future work to investigate the effect SOCRATIC pretraining has on other architectures.
Evaluation Metrics As discussed in prior work (Fabbri et al., 2021b; Pagnoni et al., 2021; Gehrmann et al., 2021), there are limitations with the current automated evaluation metrics which do not strongly correlate with human judgments. Our results from these metrics should therefore be interpreted with caution and in combination with the human evaluation we performed to support them. One area in which automated metrics have been reported to perform poorly is factuality. Moreover, current factuality metrics have been designed and tested in the news domain and their performance in the out-of-domain setting (long documents and dialog data) was not systematically evaluated and is hard to interpret (Agarwal et al., 2022). In this work, we therefore choose not to report any factuality metric results.
QG Efficiency We did not optimize the efficiency of the QG component of SOCRATIC pretraining and, consequently, it is computationally expensive. Currently, given equal amounts of resources for QG and pretraining, it takes us about the same time to perform the QG phase and pretraining phase on the same amount of data. We note, however, that in low-resource scenarios, the additional compute can lead to significant benefits, as shown in our results. In addition, we did not experiment with efficient sampling strategies, and believe that improving the efficiency of the QG model inference, for example through model distillation (Hinton et al., 2015), could lead to significant efficiency gains.
Dataset Biases The datasets for pretraining and finetuning used in this work are in English and thus mainly represent the culture of the Englishspeaking populace. Political or gender biases may also exist in the dataset, and models trained on these datasets may propagate these biases. Additionally, the pretrained BART model carries biases from the data it was pretrained on. We did not stress test these models for biases and request that the users be aware of these potential issues in applying the models presented.
Misuse Potential and Failure Mode When properly used, the summarization models described in this paper can be time-saving. However, the current model outputs may be factually inconsistent with the input documents, and in such a case could contribute to misinformation on the internet. This issue is present among all current abstractive summarization models and is an area of active research.
12745"
781,Section 9
782,After the conclusion section (§6)
783,"More generators bring significant benefits to the model performance to our MGR, but the training cost is also increased with the number of generators growing. Although we have verified that we only need to keep one generator during test, there is no denying that the training cost is still an important problem. In the future, we will explore some methods like multi-task learning and model fusion, to reduce the model complexity."
784,In the Limitations section
785,Limitations
786,"The main limitation of this paper is the need for human-labeled reference responses. We will explore automated or human-machine collaboration methods to reduce the cost of annotation in the next stage. Another limitation is that we need to explore whether other auxiliary tasks can also enhance the performance of score prediction. In the future, we also plan to reproduce the proposed method for other, less resource-rich languages."
787,"Our method does not apply to VL models where the cross-modal encoder layers are relatively lightweight. For example, the vision encoder is much more computationally expensive than the cross-modal encoder for VL models like ALBEF (Li et al., 2021) and X-VLM (Zeng et al., 2021), therefore, the end to end inference speed improvement is marginal. Reducing the image tokens inside the vision encoder could further improve the model efficiency, we leave this exploration to future work."
788,"Please see the ""Limitations"" section."
789,We discussed the limitation of our work in a separate section after the main paper
790,"The ""Limitations"" section."
791,"The Conceptualizer we propose consists of two core steps, i.e., forward pass and backward pass. The forward pass identifies the most associated target-language strings for a focal concept. However, due to possible data sparsity of PBC in some low-resource languages and some cases of verselevel misalignment, χ2 scores of the real translations can be indistinguishable compared with some other rare words that also occur in the same verses. Under such rare cases, Conceptualizer will not work well enough. In addition, the genre of PBC is limited to religion and therefore the diversity of the concepts across languages is largely influenced. Nevertheless, PBC, as far as we know, provides texts in the largest number of low-resource languages. PBC is thus a good fit for our goal.
In this work, we select 83 concepts, including the Swadesh32 and Bible51, representing a wide range of interesting crosslingual concepts. The runtime for computing the results for one concept in all languages is around 10 hours on average. The relatively long runtime, however, can prevent us from exploring more interesting concepts.
We find that the concreteness of a focal concept can be a contributor to the stability measure. As we use English as the source language for representing the focal concepts, we naturally resort to concreteness scores from English language ratings only. In addition, the analysis is carried out from an English perspective. Nevertheless, as we want to compare different languages, we have to use a unified source language. Theoretically, we can use any language as the source language and represent the concepts in that language. We therefore plan to use other languages, e.g., Chinese, or some low-resource languages, as the source language in future research."
792,"This work only explores the multilingual VLP model for the image-text retrieval task. We leave the exploration of other multilingual vision-andlanguage downstream tasks such as visual question answering as future work. At the same time, our proposed method relies on a well-pretrained vision Transformer and a multilingual text encoder. Its performance is heavily influenced by the performance of the visual and textual backbones. This hinders the mCLIP from further improvements with the given backbones."
793,"Although we conducted extensive experiments, the exploration scope of this work has some limitations: (1) All data is from one of the largest MOOC websites in China, so the dataset is in the Chinese language, which limits the linguistic features covered in our analyses. We will add comprehensive corpora from other MOOC platforms with various languages such as English, Japanese, French, and so on to enhance the availability and coverage of our dataset. (2) We present two models with highprecision and high-recall behaviors. The severe noisy and incomplete issues could not be coped with simply by combining two technical methods (i.e. co-training and PUL). A more robust training method should be proposed to jointly achieve better overall performance. We encourage future works to address these limitations and get more comprehensive analysis results."
794,"As seen in Section 4.2, sometimes the metrics are unnecessarily penalised due to errors made by the end task models. Filtering these cases would require checking every example in every task manually. We hope our results can provide conclusive trends to the metric developers focusing on segment-level MT evaluation.
We included three tasks to cover different types of errors in machine translations and different types of contexts in which an online MT metric is required. Naturally, this regime can be extended to other datasets, other tasks, and other languages (Ruder et al., 2021; Doddapaneni et al., 2022). Further, our tasks used stricter evaluation metrics such as exact match. Incorporating information from partially correct outputs is not trivial and will be hopefully addressed in the future. We have covered 37 language pairs across the tasks which majorly use English as one of the languages. Most of the language pairs in this study are high-resource languages. Similarly, the examples in multilingual datasets are likely to exhibit translationese - unnatural artefacts from the task language present in the test language during manual translation; which tend to overestimate the performance of the various tasks (Majewska et al., 2023; Freitag et al., 2020). We hope to explore the effect of translationese on MT evaluation (Graham et al., 2020) and extrinsic tasks in future. The choice of metrics in this work is not exhaustive and is dependent on the availability and ease of use of the metric provided by the authors."
795,Section IV
796,"Yes, in the limitation section on Page 10."
797,
798,"In addition to the technical limitations discussed in the previous two subsections, our research has some higher-level limitations. Importantly, the dichotomy between university and industry is not black and white. We captured industry presence by looking at author affiliation and research grants. However, we did not look at university or department funding, which any individual researcher does not receive. Many universities receive (or have received) funding to sponsor their departments and, consequently, their faculty and research. A researcher may not be directly affiliated with a company nor receive funding from any company, yet feel some pressure if their department or university is funded by industry.
This analysis is a snapshot of industry presence up until 2022. Currently, there is no automatic tool to analyze future research and industry presence or interactively set filters and generate sub-views of this study. We invite researchers to use our opensource data and code to create such an interactive system in the future.
Furthermore, we did not consider the effect of governmental or military funding on the research done at universities. Both government and military, like industry, have vested interests and can influence research (Kistiakowsky, 1989; Barker, 2017; Goldfarb, 2008). Exploration of their effect and presence over time is an area we leave to future work.
Although we quantified industry presence at multiple conferences, we did not quantify the amount of industry funding present at each conference as
sponsors. Previous work, conference websites, and personal past experiences make us confident that most large conferences are funded, in part, by industry.
Our analysis did not stratify interaction academic-industry interactions by ethnicity, sex, or many other sensitive attributes. While we believe in the importance of such an analysis, the data to enable this analysis was not accessible to us: it is often not listed on websites, and the information gathered by ACL is unavailable to researchers.
Another aspect that our analysis did not touch on is that many universities are private and also require regular funding to maintain their research work. While student registration fees cover most of the operative business, research funds typically come from state grants, federal government grants, private institutions, and industry. We plan to trace such funding tracks of large private universities and research institutions in the future."
799,"While we conducted an extensive set of experiments to gain a broad picture of whether modeling improvements hold between benchmarks, it is always possible to investigate more settings. While our study covers a representative set of 20 nonpretrained and pre-trained modeling approaches, it is conceivable that evaluating more modeling approaches (or a different set of modeling approaches) on additional benchmarks (or a different set of benchmarks) would have led to different results.
Furthermore, although we evaluate each modeling approach on each benchmark with the same training hyperparameters used for SQuAD, as well as 5 additional randomly sampled hyperparameter settings (20 × 32 × 6 = 3840 experiments in total), it is possible that the SQuAD hyperparameters for some modeling approaches happen to be more general than other modeling approaches. Ideally, each modeling approach would be individually tuned to maximize performance on every benchmark, but doing so requires prohibitive amounts of compute and researcher effort—we believe that our experiments have enough coverage with respect to hyperparameter optimization."
800,"We mainly summarize three limitations of our work. First, the translator only generates a representation, not an actual instruction, making the model less interpretable. Second, we do not include more advanced vision representations such as ViT and CLIP to train the navigation agent. Although only using ResNet, we already surpass prior methods using those visual representations (e.g., HAMT (Chen et al., 2021)), it would be interesting to experiment with those different visual representations. Third, this navigation agent is trained in a simulated environment, and a more realistic setting will be more challenging."
801,"At the end of the paper.
7 A2. Did you discuss any potential risks of your work? Our work studies general information extraction techniques and does not have ethical issues."
802,"Traditional methods for the completion of ATOMIC proposed to score all candidate tail events given the head event and the relation. The GCN for encoding graph embeddings of events induced two shortcomings: 1) it is difficult for a GCN to propagate information due to the sparse graph structure of ATOMIC (Malaviya et al., 2020); 2) it cannot sufficiently utilize semantic information of events."
803,"In Limitations section.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
804,"Section 7.
7 A2. Did you discuss any potential risks of your work? We propose a method for non-autoregressive speech translation, which does not have any risks."
805,"We make use of MS-MARCO, a resource that provides large-scale relevance annotations. However, as with most retrieval datasets, this dataset could contain annotation biases. Given the vast number of documents in the corpus supplied by the dataset, relevance annotations are sparsely distributed, with all other documents assumed to be non-relevant. Consequently, some relevant documents may be inaccurately labeled as non-relevant, leading to false negatives. A notable annotation bias in MSMARCO is that the relevant label correlates highly with the exact matching term (Xiong et al., 2020). This bias poses a limitation during the training or evaluation stages. To appropriately address this annotation bias, we might need to reorganize the labeling process using either a human or a neural annotator, or we could aim to design and train a model that is resilient to such bias. We reserve this task for future research efforts."
806,Limitation Section after the Conclusion.
807,The second to last section.
808,"The last setction
7 A2. Did you discuss any potential risks of your work? This paper does not have such risk since it is a multi-choice question answering setting."
809,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
810,Left blank.
811,"Here, we discuss some limitations of this work to inspire future research in this direction.
Tail phenomena. SELF-INSTRUCT depends on LMs, and it will inherit all the limitations that carry over with LMs. As recent studies have shown (Razeghi et al., 2022; Kandpal et al., 2022), tail phenomena pose a serious challenge to the success of LMs. In other words, LMs’ largest gains correspond to the frequent uses of languages (head of the language use distribution), and there might be minimal gains in the low-frequency contexts. Similarly, in the context of this work, it would not be surprising if the majority of the gains by SELFINSTRUCT are skewed toward tasks or instructions that present more frequently in the pretraining corpus. As a consequence, the approach might show brittleness with respect to uncommon and creative instructions.
Dependence on large models. Because of SELFINSTRUCT’s dependence on the inductive biases extracted from LMs, it might work best for larger models. If true, this may create barriers to access for those who may not have large computing resources. We hope future studies will carefully study the gains as a function of model size or various other parameters. It is worthwhile to
note that instruction-tuning with human annotation also suffers from a similar limitation: gains of instruction-tuning are higher for larger models (Wei et al., 2022).
Reinforcing LM biases. A point of concern for the authors is the unintended consequences of this iterative algorithm, such as the amplification of problematic social biases (stereotypes or slurs about gender, race, etc.). Relatedly, one observed challenge in this process is the algorithm’s difficulty in producing balanced labels, which reflected models’ prior biases. We hope future work will lead to better understanding of the pros and cons of the approach."
812,Limitations section
813,"Although the bias metrics and debiasing methods we study work well, they certainly have limitations. Limitations of this paper are given below:
(i) We are aware that defining a bias in terms of target-attribute pairs can be incomplete and somewhat subjective. Future work could look for a more objective and thoughtful way to define different bias categories or a way that does not require defining bias in advance with some item sets.
(ii) Our dataset contains multiple bias categories, but they are still defined in advance and limited. It is feasible to explicitly define the different bias categories separately, but this also means that we need to use the corresponding subsets of the dataset when studying the different biases. Therefore, a mechanism that can automatically classify biases is necessary."
814,"All of our experiments have taken place by deploying conversational agents on Amazon Mechanical Turk with crowdworkers2, using English-language responses written by workers located in the United States. While these workers are reasonably diverse (Moss et al., 2020), this is quite different to a public deployment with organic users, who are using the system not because they are being paid but because they are genuinely engaged. In that case, collecting feedback will have different tradeoffs which we could not factor into the current work. For example, asking to provide detailed feedback might dissuade users from wanting to interact with the system, lowering engagement and hence the amount of collected data. We believe either more natural free-form or lightweight feedback might be best in that case, which is why we study and compare feedback methods in this work to evaluate their relative impact.
In public deployments with organic users, safety issues also become a much more important factor – in particular dealing with noisy or adversarial inputs and feedback. In the worst case this could mean human conversationalists could teach the model erroneous reasoning, misinformation, toxic or other undesirable behavior. We note that steps to address this issue are studied elsewhere, for example Ju et al. (2022).
2Our crowdsourcing tasks pay workers well above minimum wage. The tasks do not request any personal information from workers."
815,"In the section after the conclusion, without a section number.
7 A2. Did you discuss any potential risks of your work? We didn’t discuss potienal risks, because to the best of our knowledge, the research topic does not introduce additional risks."
816,Limitations
817,Section: Limitations
818,"One unsatisfying aspect of proposed task is that it accounts for distributive coordination structures, but is not able to handle sentences with collective reading where the main predicate applies to the plurality of conjuncts as a whole. In our data collection these account for about 4.9% of the verbal omission cases, and such sentences are left “non-rewritable”. In future work, we would like a solution that allows to resolve also such sentences in a consistent yet easy-to-annotate manner.
Additionally, in the GPT prompting experiment we experimented with a few different prompts, but did not do exhaustive prompt engineering, and it is possible that with more aggressive prompt engineering GPT can perform better on the task than our results indicate. Similarly for the fine-tuning experiments with T5-large, in which we did some hyperparameter tuning, but not aggressively so."
819,"This work relied on previously published datasets to source personas on which to anchor the generated unhelpful thoughts, and thus shares the limitations of those datasets. In particular, they use English-language responses, written by workers located in the United States.10. While these workers are reasonably diverse (Moss et al., 2020), the examples generated may not reflect the thought patterns and personas across cultures and diverse populations. This data is also generated by people who are being paid, as opposed to people genuinely engaging about situations that matter to them. Besides the substance of the thoughts themselves, a more direct limitation is that the models generate only English, so would not be directly usable for speakers of other languages.
In addition, the data collected reflects the understanding of lay people, rather than trained clinical psychologists. While this makes the material more immediately relatable to other lay people, it is possible that the data do not capture what clinical psychologists would consider adequate illustrations of unhelpful patterns. Our data has been spot-checked by a CBT-trained clinical psychologist and found generally sound, but the entire material should undergo further validation.
Another limitation is that the models that we have tested are resource-intensive. In particular, the
9https://github.com/facebookresearch/ ParlAI/tree/main/projects/reframe_ thoughts
10Our crowdsourcing tasks pay workers well above minimum wage.
best-performing model, GPT3.5, is only available through a paid API."
820,"In the 7-th Section
7 A2. Did you discuss any potential risks of your work? The data used for pre-training is based on publicly and widely used wikidata and wikipedia."
821,"First, AGATE can handle DTDNs but does not handle CTDNs. In addition, it currently handles neither labeled nor directed edges. In the future, we intend to extend AGATE to cover a more comprehensive collection of graph types. Second, we assumed that most new nodes have similar attributes to those of new nodes at the previous time step, which were observed in our preliminary experiments. We plan to propose new strategies for the new node appearance task. Third, we tuned the hyper-parameters independently of models close to the input, so it may not be the optimal combination of methods. We plan to employ auto-ML techniques to enhance performance and mitigate the learning process."
822,"Limitation
7 A2. Did you discuss any potential risks of your work? We do regular NLP task and use standard NLP datasets"
823,"We discuss the limitations of the work as follows:
• One major limitation of our work is that we analyze language models pre-trained with the same data, similar training procedures, and the same autoregressive language modeling objective. Our findings may support model families trained in this restricted setting. When comparing models trained with different corpora, such as Neo GPT NEO (Black et al., 2021) and BLOOM (Scao et al., 2022a), different architectures and objectives, such as retrievalbased language models (Khandelwal et al., 2020; Zhong et al., 2022; Borgeaud et al., 2021) and sparse models (Fedus et al., 2022; Artetxe et al., 2022a), the relationship between validation perplexity and downstream task performance could be more obscure.
• For downstream task evaluation, we only evaluate on multiple-choice tasks, where the evaluation protocol is the most similar to the pretraining objective. Evaluating on generationbased tasks is more messy and hard to scale up, and we will leave it as future work. Another risk is that as we always take aggregated measurements over tasks, it might conceal important patterns of individual tasks.
• We do not provide a concrete explanation for the double-descent behavior that consistently occurs during pre-training, nor do we know if it is an artifact of the data, the objective or the optimization process. We consider it an interesting phenomenon and will look more closely into it in future works."
824,Limitation Section.
825,"We highlight two main limitations of our work.
Firstly, instead of focusing on more recent NMT models that use large pretrained language models as their backbone, our experiments were based on transformer base models. That is because we used the NMT models that produced the translations in the datasets we analyze, i.e, the models that actually hallucinate for the source sequences in the dataset. Nevertheless, research on hallucinations for larger NMT models makes for an exciting line of future work and would be valuable to assess the broad validity of our claims.
Secondly, although our method does not require any training data or human annotations, it relies on access to a pre-existing database of source mass distributions. This can be easily obtained offline by running the model on monolingual data to obtain the distributions. Nevertheless, these datastores need not be costly in terms of memory. In fact, in Appendix J, we validate our detectors for datastores that contain less than 100k distributions."
826,Limitations
827,The section after conclusion
828,"ReCode benchmark has several limitations: (1) It contains perturbed datasets based on HumanEval and MBPP which focuses on Python function completion use cases. Therefore, we only perform
evaluation on Python language and not be able to capture robustness in a wide variety of code completion use cases. However, our transformations are generalizable and could be easily extended to other languages and also other coderelated datasets (Athiwaratkun et al., 2023). We encourage researchers to apply and extend ReCode benchmark to additional languages and other coderelated tasks; (2) ReCode benchmark is designed for robustness evaluation and cannot mitigate the lack of robustness. Given that our benchmark can be used to generate comprehensive collection of perturbed data, we believe that it can be used for training data augmentation to enhance model robustness. We will consider corresponding robust training strategy design and evaluation in the future work."
829,"Section titled ""Limitations"" after Section 6 (Conclusion).
7 A2. Did you discuss any potential risks of your work? We do not foresee any potential risks involved in the use of the resource."
830,"the last section
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
831,"The collection and verification of this work has required help from over 250 annotators. This makes the dataset difficult to replicate, as is the case with many dataset papers. We have selected annotators carefully, considering relevant experience and using techniques to determine annotator quality to minimise the subjective variance. We have tried to cover the arguments involved in debating by talking to experts and people from debate circuits across the world, with different experiences and expertise. However, due to the nature of this activity, it is possible that there are arguments and experiences have not been covered in the dataset. These could be experiences of marginalized communities, underrepresented debate circuits, etc. Moreover, some debate motions used are relevant to the time period in which the motion was the most prominent (for example, motions about Trump and his actions, certain policy decisions, wars and their outcomes, etc). Our dataset does not account for the changes that might have taken place pertinent to that issue after the generation of arguments."
832,"SYMBOLICTOM assumes stories are written chronologically, which may not hold for some human-written stories. This may be alleviated using time-stamping models like Faghihi and Kordjamshidi (2021). Furthermore, since we use off-theshelf models (WANLI (Liu et al., 2022) and OpenIE (Stanovsky et al., 2018)) to create and update the graphs, the presented approach may propagate errors as revealed in the linguistic diversity experiments. However, these issues can be largely alle-
5As a part of out-of-domain testing, we also create a more challenging version of the available ToM datasets, available at https://github.com/msclar/symbolictom along with a corrected version of ToMi.
viated by using more sophisticated models, even the LLMs like GPT3 themselves. We do not experiment with them due to budgetary restrictions.
Currently, all NLP datasets available for theory of mind reasoning describe Sally-Anne tests. In these datasets, the concept of large distances is absent, meaning that anyone specified to be in a location is assumed to be a witness of the actions that occur there. This assumption can be violated in realistic settings. For example, “Anne is in the USA” does not imply she is a witness to every action happening in the USA. In future work, this approach can be improved by refining the witnesses detection algorithm to incorporate physical commonsense reasoning. We could also refine the witness detection algorithm by sampling paths between the inserted edge and each node referring to a person, to query an LM directly on that substory by asking if the person witnessed the action. To be able to test both of these ideas, we would need to obtain new theory of mind datasets with significantly more types of interactions and physical commonsense in the stories."
833,"This work is subject to two limitations. First, our experiments were restricted to text classification tasks and we did not evaluate if our methods can effectively defend against adversarial attacks for other tasks like QA, etc. (Jia and Liang, 2017). It therefore remains unexplored if our conclusions transfer beyond the text classification tasks.
Second, the primary contribution of our work, ATINTER relies on using a language model like T5, which is trained on large amount of text in English. It is possible that our approach is not as effective for languages where such a model is not freely available. Additionally, in this work, we did not explore the impact of large language model pretraining on our results."
834,"Section 7, Section 5.6"
835,"Naturalness. Since our dataset relies on the Wikipedia category names and semi-automatically generated compositions, it does not represent an unbiased sample from a natural distribution of real search queries that contain implicit set operations. Further, we limit attention to non-ambiguous queries and do not address the additional challenges that arise due to ambiguity in real search scenarios. However, the queries in our dataset were judged to plausibly correspond to real user search needs and system improvements measured on QUEST should correlate with improvements on at least a fraction of natural search engine queries with set operations.
Recall. We also note that because Wikipedia categories have imperfect recall of all relevant entities (that contain sufficient evidence in their documents), systems may be incorrectly penalised for predicted relevant entities assessed as false positive. We quantify this in section 5. We have also limited the trusted source for an entity to its Wikipedia document but entities with insufficient textual evidence in their documents may still be relevant. Ideally, multiple trusted sources could be taken into account and evidence could be aggregated to make relevance decisions. RomQA (Zhong et al., 2022) takes a step in this latter direction although the evidence attribution is not manually verified.
Answer Set Sizes. To ensure that relevance labels are correct and verifiable, we seek the help of crowdworkers. However, this meant that we needed to restrict the answer set sizes to 20 for the queries in our dataset, to make annotation feasible. On one hand, this is realistic for a search scenario because users may only be interested in a limited set of results. On the other hand, our dataset does not model a scenario where the answer set sizes are much larger."
836,In Section 7 Limitations
837,"Recruiting human subjects for annotation limits the reproducibility of human evaluation. In addition, we have only tested the performance of the proposed framework on the fixed dataset, ArgKP2021, that we described above, and not on a wider range of data. This is because ArgKP-2021 was the only dataset available for use in this task. Finally, we did not filter the arguments in the original corpus, with the result that potentially offensive arguments may come into the framework as input and generate key points which some readers might find offensive. It is worth noting, however, that the identification of offensive language is not the aim of this work."
838,Left blank.
839,"Section Limitations.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
840,"There is a growing interest in investigating human morality in text (Russell et al., 2015; Gabriel, 2020). However, like most technologies, morality classification can be misused, especially targeting sensitive features including ethnicity and political orientation (Kalimeri et al., 2019a; Talat et al., 2022). For instance, authorities in non-liberal countries could use Tomea to identify repressed minorities by detecting moral language that diverges from the expected moral rhetoric. Ongoing research is investigating such issues, e.g., by creating methods that mitigate bias and unfairness by design (Dinan et al., 2020; Vargas and Cotterell, 2020).
We discuss three main limitations of our analyses related to the corpus we use (MFTC). First, MFTC is composed of English tweets, and we employ a version of BERT that was pre-trained on large-scale English data. Our experiments show that Tomea produces insightful results under these conditions. However, the performance of Tomea with models pre-trained on smaller datasets, e.g., datasets for morphologically richer languages, remains to be investigated. Further, the scalability of Tomea to longer text formats (e.g., news articles) and different mediums of communication (e.g., surveys) is yet to be explored.
Second, the tweets in the MFTC were collected using the Twitter API, which only yields public
posts. Thus, following Twitter’s Terms of Service, deleted content will not be available (limiting the reproducibility of any Twitter-based study). Further, the demographic and cultural distribution of Twitter users may not be representative of the general population, In addition, we required the crowd workers involved in the evaluation to be fluent in English, and their demographic distribution (Appendix B.3) is skewed towards Europe. These factors could possibly lead to the perpetuation of Western values and biases (Mehrabi et al., 2021) in our analyses. Additional experiments are needed to investigate whether Tomea would produce insightful results when applied on a dataset collected on a more extensive slice of the population, with a broader set of linguistical expressions.
Third, the MFTC is focused on US-centric topics. However, when recruiting annotators for our crowd evaluation, we did not require familiarity with such topics. Even though the annotators were not exposed to the original tweets but to a processed version of the dataset (i.e., the output of Tomea, see Section 4.4.1), the potential lack of familiarity may have influenced the evaluation results.
Finally, we remind that Tomea’s d-distances measure how (dis-)similar two domains are, and are thus not a (binary) judgment of (dis-)similarity. Further, two corpora collected in the same domain (e.g., two datasets on BLM protests) will likely not have a d-distance of 0. It is left to the user to judge the similarity of the two corpora, supported by Tomea’s quantitative and qualitative metrics."
841,Section 7
842,We discuss the limitations of our work in the Limitation section.
843,Section 7
844,Limitations
845,"In this work, we propose a structure-based pseudolabel generation method for zero-shot video sentence localization and propose a noise-resistant method to reduce the effect of pseudo-label noise. The limitations of our work are: (1) although we generate free-form natural language queries, the distribution of generated queries may still differ from the distribution of queries in the dataset (e.g. queries on the Charades-STA dataset usually start with ‘person’), which may degrade the performance during testing; (2) our pseudo label refinement can correct the noisy event labels, but there is no mechanism to correct noisy queries. These can be studied as future works."
846,"The approach to collect our dataset is expensive and laborious. This along with the dependence on expert annotators makes the transfer of such an approach challenging for other low-resource languages. We however, find this a necessary endeavor to develop initial resources that can help provide a starting point to extend access to more languages and iteratively improve research, technologies and services across languages."
847,"Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
848,"The omission problem is critical in dialogue summarization, but even if this problem is solved, we still cannot guarantee a candidate is appropriate because it might bring hallucination content that is not presented by the source dialogue. Previous works (Tang et al., 2022; Maynez et al., 2020) also concluded that factual inconsistency is a critical
problem in dialogue summarization, and it is not easy to distinguish. How to mitigate the omission problem while avoiding the occurrence of new errors is not discussed in this paper, and we hope to address this issue in future work."
849,"No section number, ""limitations"" section (page 8 and 9)"
850,"We identify crucial financial signals in reports which can help financial practitioners to digest long financial documents efficiently. However, factors such as macroeconomics, stock prices, and public policies may affect how a financial practitioner views financial reports in practice. Confidential intelligence or social media may greatly affect the analysis results. Therefore, we limit our task to the scenario in which the content in the reports is the sole information available to users. Accordingly, to prevent bias in the annotation process, we acquire annotations from annotators under similar scenarios (graduate students majoring in accounting or other related fields) rather than from financial professionals. In addition, language partially constrains our methods since the data we used in stage S2 is in English; adding a machine translation module may have sub-optimal effectiveness of financial signal highlighting. This is mainly because the financial signals highly depend on many languagespecific knowledge or country regulations."
851,"Since the minimax objective requires using two separately trained models, i.e. the learner and the auxiliary, the design of the latter plays a crucial role in the overall stability of the training process. In particular, while having a very capable auxiliary model will naturally result in a more accurate and robust example weight distribution, it will also potentially lead to overfitting to certain training instances with high-losses. Another potential limitation of minimax training is that the existence of noise in the labels may cause the auxiliary to generate erroneous example weights due to high-loss noisy instances co-existing with the “hard” examples containing meaningful patterns that contradict the shortcuts. Furthermore, we explore shortcut mitigation only for NLI in English, and thus our method might not transfer to other tasks and/or languages. Finally, the datasets we consider are well-used and -discussed in the literature, and consequently their shortcuts (and how they are adopted by the models) are well-known. Further testing is needed to establish whether our approach would transfer to datasets containing different shortcuts."
852,"Limitations section on Page 9
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
853,"Despite the progress we made, there still exist limitations in our work. On the one hand, we only investigated some classic dynamic networks and found that the proposed method contribute to the best performance in selected criteria. However, other advanced partition methods that further improve the performance and efficiency may exist, which deserve exploration in future work. On the other hand, since we only consider MoE and DY-Conv in limited tasks, it would be valuable to consider other architectures (e.g., Switch Transformer (Fedus et al., 2021)), machine learning methods (e.g., reinforcement learning (Li et al., 2022)) and tasks (e.g., machine translation (Ding et al., 2020, 2021)).
Ethics Statement
We take ethical considerations seriously and strictly adhere to the ACL Ethics Policy. This paper focuses on the higher efficiency of dynamic networks, e.g., the mixture of experts. Both the datasets and models used in this paper are publicly available and have been widely adopted by researchers. We ensure that the findings and conclusions of this paper are reported accurately and objectively."
854,"We point at some directions for future improvements in automatic instruction generation.
First, as shown in §3, Unnatural Instructions contains noisy examples, in which either the instruction, input, or output are invalid. Future work may focus on developing better filters for such examples - e.g., by annotating a subset of examples as either valid or not and training a classifier for determining the correctness of generated instances (West et al., 2022; Liu et al., 2022a).
Second, future work may employ a human-inthe-loop approach, where humans should recognize challenging patterns, encouraging models to generate more complex examples (Liu et al., 2022a). In another human-in-the-loop scenario, models trained on Unnatural Instructions can be queried by humans to find examples on which these models fail, thus collecting harder examples (Nie et al., 2020).
Finally, language models are known to sometimes reflect undesirable biases present in their training data. Automatically generated data may therefore contain such content. We note that during our manual analysis, we did not notice any harmful examples. Still, future work may consider applying a filtering mechanism to reduce the risk of having biased content."
855,"This work focuses on English QA datasets only. Similar techniques should apply in other languages as well; however, we did not evaluate them. The augmentations generated are difficult to validate for yes/no questions for the few-shot method. Moreover, it can be challenging to generate these augmentations if access to large LM is unavailable. However, under those scenarios, data in the target domain should be annotated, which ideally would perform better than the few-shot setting. Our models also suffer from similar problems as LLMs, like hallucinations, misinformation, etc."
856,"It seems to be difficult to evaluate the efficiency of ODQA models fairly and impartially due to multiple factors that should be considered and need to be traded off. On the one hand, it is not enough to only use accuracy, memory, and processing time to evaluate effectiveness. It is also important to establish what resource, e.g., money, power consumption, carbon emissions, etc., one attempt to constrain (Treviso et al., 2022). On the other hand, how to deploy models and what tools model implementation relies on contributes to inequity growth (Blodgett et al., 2020). It is extremely challenging to unify the deployment of all models and the tools they rely on and to achieve a truly fair and unbiased effectiveness comparison."
857,"Section 5
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
858,"Limitations, at the end of the paper
7 A2. Did you discuss any potential risks of your work? No apparent societal risks."
859,"In this work, we evaluate the proposed models for NLP tasks only. However, tasks in other fields such as Computer Vision may present a very different input inductive bias, thus affecting the performance. Moreover, our models are trained from scratch, hence it is unknown whether the same divide-andconquer strategy works for pre-trained models. We will study these limitations in the future to give a more extensive exploration."
860,"May extend to other domains In this paper, we present a generic framework and evaluate the effectiveness of our proposed model Jointprop on three public datasets. We may further extend the framework to various datasets in different domains. For example, ACE05 (Walker et al., 2006) in social networks, journalism, and broadcasting, as well as GENIA corpus (Ohta et al., 2002) in biomedical research.
May extend to other NLP tasks Our proposed model focus on two tasks, namely NER and RE. We may extend our framework to include more information extraction tasks, such as coreference resolution and event extraction. Moreover, we may contract knowledge graphs from extracted structural information."
861,"Currently, RoHT framework is restricted to incorporating KBs and text. However, since RoHT retrieves answers from each knowledge source in a separate way, it could in principle utilize knowledge from more heterogeneous sources such as tables, and we will study this in future work. In addition, a device with large storage space and memory is needed for the storage and usage of Wikipeida and Wikidata."
862,"To understand the gap between our automatic data generation method and fake news written by humans, we expanded PN-SILVER to different sizes and compared the performance of ROBERTALARGE when trained on these generated datasets and the human-written fake news dataset, SNOPES. Note that since the TIMELINE17 dataset only contains around 4K samples, we additionally crawled New York Times news articles as an input to our generator for the “5 times” to “10 times” experiments. The results are shown in Figure 3. Although the detector performance at first improves as we add more silver training data, it reaches a plateau after the size is increased five-fold. This illustrates that while our approach is more effective compared to baseline generation methods, there is still a clear gap between our generated articles and human-crafted fake news, likely in aspects such as style (as discussed in §5.2), intent (i.e., limited modeling of propaganda techniques), and falsehood (i.e., the generated content is 100% false).
Despite the advantages of our generation approach, as compared to previous methods, it is uncapable of generating other propaganda techniques covered in (Da San Martino et al., 2019), such as straw man. Thus, our method is not generic enough to handle all types of propaganda techniques within a unified framework. Moreover, our approach is limited to generating English-only news articles, and cannot be applied to other languages."
863,Section 8.
864,"In the final part after conclusion
A2. Did you discuss any potential risks of your work? Not applicable. It is fundamental research and not tied to particular applications."
865,Limitations Section on page 10.
866,"Using a medium size fine-tuned teacher. With recent advances in huge LM such as GPT4 and their extraordinary generation capabilities, one may wonder about the relevance of this work which mainly focuses on a medium size fine-tuned teacher. Although we show the distillation of a huge LM (GPT-4), it is often infeasible.
First, when the data cannot be sent to external servers because of privacy constraints or when the domain is unique or specific (e.g., in national security settings or human conversations), huge LMs that cannot be fine-tuned may be less effective.
Second, we have distinguished between two types of costs: computational and financial. While training a student model with a medium-size finetuned teacher may take a few days, the entire process is feasible since training time is typically not a limited resource. In contrast, generating PTs with a huge LM like GPT-4 can easily cost (many) dozens of thousands of dollars. This financial cost is often prohibitive, particularly when training a general high-quality student or several domain-specific ones. While it is possible to utilize a huge LM to obtain a limited number of labeled examples, relying on it for generating PTs for abundant unlabeled data is not feasible. Therefore, a medium size teacher is needed.
Furthermore, research suggests that using mediator/assistant teachers aids the distillation process (Mirzadeh et al., 2020; Wang et al., 2020), as might be the case in distillation from a huge LM to a medium size fine-tuned teacher, and finally to a small student. Considering the aforementioned reasons, our study holds significant relevance as it emphasizes the importance of the distillation process with a medium size teacher, regardless of whether the data is generated manually or by a huge LM.
The scope of our realistic setup. While our results demonstrate the effectiveness of KD for various English-to-English NLG tasks, for the tasks that were part of the study, the output length is relatively short compared to the input (e.g., Summarization and Question Generation) or has a similar length (Abductive Reasoning, Style Transfer and Simplification). The results may differ for tasks with much longer output lengths or for non-English-to-English tasks such as NMT, data-to-text (e.g., table-to-text), multilingual, or multi-modality tasks.
In addition, the results are applicable to our realistic task-specific setups, and some findings may
vary in high-resource scenarios or when unlabeled data is unavailable. Although these scenarios may be less relevant to NLP application developers, they are commonly studied in academic research.
Computational training costs. Another limitation of our research is that we did not consider the computational costs of the KD stages. The training time comparison between the methods was therefore overlooked. This is because we assumed that one-time resource usage for training could be neglected compared to the accumulated inference cost of a deployed model.
However, it is worth noting that generating PTs with the teacher for all the training and unlabeled examples is computationally expensive (it could take one to a few days, depending on the number of unlabeled examples). Furthermore, Joint-Teaching can also be computationally heavier than other KD methods, as the student generates PTs during the training process (although the student is fast).
In addition, different training objectives also have different costs, with some methods being more computationally intensive than others (e.g., Attention-Relation is more costly than Logits KD). Finally, the distillation process can be long, and multiple epochs are required until the student converges - in some setups, we trained the student for more than a few days.
Utilizing huge LMs. Certain limitations arise in our extreme setup, which involves the costly utilization of huge LMs (GPT-4) provided by external companies like OpenAI. First, the comparison with the Joint-Teaching method is not conducted due to the need for repeated costly querying of the teacher model to extract its logits every time a PT is generated with the student. Nevertheless, extracting the logits of the teacher PTs (for Logits KD) and generating multiple PTs is approximately equivalent to generating a single PT. This is because the prompt, consisting of many tokens, is processed only once, and the marginal cost of generating multiple (relatively short) PTs is low.
Another limitation arises from relying on external companies to enable logit extraction (for Logits KD) and there is no assurance that this feature will be supported. For instance, in the chat versions: ChatGPT and GPT-4, logits are not accessible. In this work, we rely on an internal version of GPT-4, which allows us to extract its logits. Fortunately, we demonstrate that even without Logits KD, achieving a strong student model is possible."
867,"In this paper, we evaluate the quality of humanannotated natural language explanations towards the models’ prediction performance on multiple datasets. Although it is a natural step that our evaluation metric could be generalized to evaluate the helpfulness of model-generated explanations, we would like to caution that: our metric and evaluation experiment requires the models to generate explanations for the train split data, then use the data with generated explanations to fine-tune the second model with the Infusion setting, which may not be suitable for those systems that are trained on train split data. In addition, we acknowledge that the human-annotated explanations are very expensive to collect, thus, a better mechanism (e.g., Active-Learning approaches (Yao et al., 2023)) is needed to improve human annotators’ performance."
868,"This paper covers only four NLP tasks. Certain other tasks requiring more background knowledge may show different results. We suggest recruiting language learners when native speakers are not available, but recruiting learners may also be difficult for languages that are not popular for learners. Our results are based on a relatively low number of participants, as we chose to cover three different languages to show generalizability across languages. Many factors that may contribute to the results remain, such as the order of the batch of annotation questions with respect to the question difficulty level."
869,"The focus of the TEMPREASON dataset is to examine language models’ temporal reasoning capability. However, the temporal expressions of TEMPREASON are only in the form of month in textual form and year in numeric form. One limitation of the TEMPREASON benchmark is the lack of adversarial attacks in other temporal formats, such as
all numeric dates and months. The robustness of temporal reasoning is also important in real-world applications. Since the scope of this paper only focuses on the reasoning aspect, the robustness of TEMPREASON will be left for future research. Besides, the knowledge triples of TEMPREASON are from the crowd-sourced Wikidata KB, and these triples are used to construct the question-answer pairs in this paper. Hence, it is possible that errors in the Wikidata KB propagate to the answers in TEMPREASON. However, such errors have minimal effect in the ReasonQA setting, for this task only asks the models to infer from factual knowledge in the Wikidata KB."
870,"This work focuses on the effects of adaptive inference in a low-resource setting, specifically when training data is limited. Our experiments (Section 5) suggest that the negative impact of conflicting gradients may be less prominent when larger amount of training data is available.
Our experiments were conducted using relatively small pre-trained language models (≤ 350M parameters) due to computational constraints, and we defer the replication of our findings with larger, more powerful models to future work. Nonetheless, our results have important implications for the growing trend of increasingly large language models. We hope this work inspires further research on methods to reduce the computational cost of NLP.
This work concentrates on evaluating the speedaccuracy trade-off of Multi-Model and Early-Exit at inference time. We recognize that there are additional factors, such as memory usage, batch processing, and training duration, that could be considered when comparing these methods.
Finally, we experimented with seven text classification tasks in English. We recognize that results may vary for other tasks and languages."
871,"Section 8
7 A2. Did you discuss any potential risks of your work? Our work does not present any risks."
872,
873,"The main limitation of our method is that it heavily depends on the quality of the label description. If a label description does not precisely describe the meaning of the label, our method cannot work. For some classification tasks such as microaggression detection, their labels have abstract meaning that is difficult to be understood by pre-trained language models. Similarly, our method cannot work on the domain that is not covered by the pre-training corpora of language models, such as the medical domain.
Another limitation of our method is that PLCT loss cannot handle short texts. If a text consists of only one sentence, PLCT loss will no longer work because LCT requires a document to be more than one sentence. In this case, PCL loss can still be used for self-training."
874,"As previously mentioned, our study is focused on the generator component of a QA pipeline and ignores the retrieval task. In the experiments presented in the paper, we have used gold facts to report the results. For certain datasets such as HiTab and MULTIHIERTT which were designed for complex tabular structures, this might simplify the end-to-end challenge. In future studies, we hope to explore whether CounterComp can enhance the performance of retrievers.
The datasets used in our experiments were curated using enterprise documents such as financial reports or other corporate disclosures. Quantitative QA over these reports often involves multi-step reasoning that is limited to linear arithmetic operations such as addition, division, averaging, etc. A completely open-domain QA engine might need to cover more complex operators.
Lastly, we designed CounterComp to leverage existing data by sampling from the training set. Nevertheless, combining CounterComp with augmentation-focused methods such as CAD might lead to more robust models."
875,"We identify a few limitations of the current work. Our approach still suffers from biases in the training data and may produce incorrect output or lead to an inaccurate understanding of multi-modal content. And a large-scale audio-visual pre-trained model is a promising direction toward more advanced and cheaper approaches for transfer learning, which we leave for future study."
876,"Three main limitations with regards to certain aspects of this paper are the comparison against very large models, the distribution of the Original set, and the restriction of the output length.
Firstly, due to the lack of computational resources and availability of some models, we were unable to make a rigorous comparison of our finetuned models’ as described in Section 5.2 against very large models like Minerva (Lewkowycz et al., 2022) or even Codex (Chen et al., 2021). However, these larger models can still be evaluated as FERMAT is made publicly available.
Secondly, another limitation of FERMAT is its use of Illinois and CommonCore which have highly skewed distributions of numbers (see Section 3.1.2)
and their answers are mainly integers which is not representative of the real-world. This undesired effect is mirrored in the number types that use the same numbers as Original. However, this was part of our design for FERMAT as the alternative would have been to combined all the ranges of numbers used with the representation, creating too many aspects but mainly conflicting with non-independent analyses between representation and range of numbers. Therefore, we chose to use the same numbers as Original, and since the templates will be openly accessible, they can be used to generate more combinations for wider aspects.
Lastly, when generating training questions, despite our best intentions, we had to limit the length of the output to an arbitrary length of 12 digits, therefore some number combination were not possible, for example 1÷3 = 0.3333... . This practical implication could have been avoided with the use of fractions or rounding. But we judged that it would have added an extra layer of difficulty for the models and decided to restrict the output length instead."
877,"Section 7 - Limitations
7 A2. Did you discuss any potential risks of your work? We do not believe our work to have potential risks, instead we aim to reduce environmental impact by looking at alternative to large models."
878,"There are several characteristics of the presented analyses that limit the scope of conclusions that can be drawn. We discuss how each of these limitations affect the takeaways of our results below.
Number of Chatbots The generalizability of our metric analysis results (Section 7) is constrained by
the fact that we were only able to include conversations from 4 chatbots in our analyses. We did our best to choose chatbots representative of the field and seem to have selected a fairly diverse group of models (Section 8). However, it is possible that not all results we found in our metric analyses will generalize when evaluating other chat models. One possible example is the number of partner contradictions we observed among our 4 chatbots (Figure 4), which may be similar by coincidence. If other chatbot models indeed differ more substantially in partner contradiction rates, our sensitivity metric analysis may have underestimated the sensitivity of our partner contradiction metric (Section 7.3). In general, including a larger number of chatbots in a metric analysis will improve the chance that its results will apply to new chatbot models. Future work that performs metric analyses like those we presented, but with different chatbots than the 4 selected in this work, would aid further analysis of our results’ generalizability.
Use of Surgers as Evaluators We perform our analyses using only a single evaluator group (Surgers). This choice of evaluator group does not harm the replicability of our methods, as other researchers have access to use of SurgeHQ or similar third-party annotation companies. However, several other evaluator groups are more popularly used for chat model evaluation, such as university students and Amazon Mechanical Turkers (MTurkers). We attempted to carry out our study with three evaluator groups (see Appendix E for details), but were unable to proceed with student and MTurker evaluator groups due to time constraints. Consequently, it is unclear to what extent our metric analysis results will generalize to other choices of evaluator.
Number of Collected Conversations As with any study involving a sampling procedure, resource constraints limit the number of collected samples, which in turn limits the statistical power of the study’s analyses. Our study included 400 conversations, which provided more than adequate statistical power for most of our analyses. For example, our investigation of each metric’s predictive validity (Section 7.2) relied on a simple linear regression analyses. At a significance level of α=0.05, our 400 conversation samples would yield a statistical power of 1-β=0.80 to detect effect sizes of f2=0.142 by F-test for each metric’s regression. However, our analyses with the weakest statistical
power are our dialogue-level analyses that compare bots with only 100 samples per bot. At 100 samples per bot, and assuming a standard deviation of 1.0 Likert points,13 a two-tailed t-test of mean Dialogue Likert rating would have a statistical power of 1-β=0.80 to detect differences of an effect size of Cohen’s d=0.40. This is still a reasonable amount of statistical power, but leaves room for our study to produce inconclusive results when the true differences between chatbots are small."
879,Section 10
880,"In the last section
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
881,"Data diversity: As a template-generated dataset, KITMUS does not reflect the full diversity of natural data. However, we do not attempt to emulate the diversity of natural datasets. Using templates over natural data for diagnostic purposes has a few advantages. Templates facilitate control over the source of a certain type of knowledge, which may not be possible to do with more natural datasets like Ontonotes. This allows us to isolate the model behavior we want to probe. We also take several steps to add diversity, like using multiple templates, sampling from large resource pools, random shuffling of entities, addition of noise sentences, and canonical data splits with non-overlapping templates and resources. To prevent spurious factors at lexical level, the templates are hand-crafted to remove surface cues and validated in a study with human participants. Background Knowledge Assumption in LMs: The results of our work is based on the assumption that pretrained LMs have access to background knowledge about real occupations. To verify that the pretrained LMs evaluated in this work contain background knowledge mapping occupations to situations, we ran a LAMA probe (Petroni et al., 2020) on BERT and ELMo. Given the template “The work of a [MASK] is [SITUATION].”, we compared the probabilities the LMs assigned to all single-token occupation names used in KITMUS (probing for multi-token words is not supported by LAMA). BERT assigned higher probabilities to the correct occupation than to any other occupation for 90% of occupations. ELMo assigned the highest probability to the correct occupation for only 45% occupations, which might contribute to explaining why the ELMo-based model C2F generally performs worse than BERT4Coref on the BACKGROUND-PRETRAIN variant KITMUS, which requires such knowledge about occupations. Root Word Overlap: One potential limitation of testing for non-fictional background knowledge like “firefighters put out fires” is that the natural occurrence of the root word “fire” in both occupation and situation might enable models to solve the task without having access to background knowledge. An analysis of trigram overlaps in all occupationsituation pairs shows that 45% of non-fictional occupations have at least one overlapping root word. However, a comparison of performances on those samples with and without root word overlap
showed neither systematic increase nor decrease for any model, indicating that models do not rely on the root word mappings. Results split up by root word overlap can be found in Table 10.
Train Set Size: The size of the train set for KITMUS, 2000, was chosen to mirror that of GAP (Webster et al., 2018). To evaluate whether the failure of models to learn the task is due to the relatively small number of samples observed during training, we re-generated all variants with 5000 train examples and repeated all experiments. We observe an increase in the magnitude of performance both in BERT4Coref and C2F on those variants where performance was higher than random performance with 2000 examples, but not on those that were equal to or below random performance. Consistent with previous results, BERT4Coref performs well on BACKGROUNDPRETRAIN and BACKGROUND-BOTH, but not on all fictional BACKGROUND-INFERENCE variants (Tables 7 and 13). We release the KITMUS generation code to enable experimentation with other train set sizes in future work."
882,Limitations section
883,Final section (9)
884,Section 9
885,"This work represents an initial push to bring dogwhistles to the forefront of NLP and computational social science research, and as such, has many limitations. Our glossary is the most comprehensive resource to date (to the best of our knowledge) but aims to document a moving target, as dogwhistles continuously emerge or fall out of use due to outgroup awareness. We aim to make this resource a “living glossary” and encourage others to submit new entries or examples. We further encourage future research to develop models to automatically detect the emergence of new dogwhistles.
Another major limitation in this work is that we identify as out-group members for nearly all dogwhistles in the glossary and have an adversarial relationship with many of the communities studied (e.g. white supremacists). Although our work would ideally be validated by members of the ingroups, they have very little incentive to share this information, as that would damage the dogwhistle’s utility as a tool for covert in-group communication.
This work, like most prior work, is limited in that we operationalize dogwhistles as a static binary; we assume each term either does or does not have a dogwhistle interpretation and is categorically included or excluded from our glossary and analyses. In reality, dogwhistles are far more complicated constructs. For example, Lee and Kosse (2020) characterize dogwhistles along two dimensions: the size of their in-group and the degree to which their usage is conventionalized. Other axes of variation may include the level of out-group awareness, and the social and political risks of backlash to the communicator if the dogwhistle interpretation is exposed. It is even possible that audience members who hear a dogwhistle further recirculate it even if they themselves do not recognize the covert meaning (Saul, 2018). We hope future work will consider multifaceted and continuous measures of “dogwhistleness"" that account for such nuances.
Finally, the current work is limited in the scope of dogwhistles considered: they are all in English with the vast majority coming from the U.S. political and cultural contexts. However, dogwhistles are prominent across cultures (Pal et al., 2018; Åkerlund, 2021) and we hope that future work will consider other languages and cultures, especially involving researchers who have high awareness of or expertise in non-U.S political environments."
886,"While we aim for a comprehensive analysis of existing methods (such as lemmatization) and model types for Ancient Greek and other Classical languages, there are limits to exhaustively exploring the full space of variations and rigorously evaluating their impact on model performance. For example, we could not comprehensively evaluate the effects of (i) the pre-training corpora, as we did not re-train a BERT model for Ancient Greek, to pin down the exact difference between prior BERT models (which were trained on smaller data before) and our own models, which are based on inherently stronger model types; similarly, we did not induce Latin ROBERTA and T5 models, to confirm the differences between mono- and multilingual models for language-specific Latin tasks. (ii) In a similar vein, we did not compare different model sizes. However, we studied prior work and scaling laws and believe that the base model is appropriate for the size of our training data. Further factors of this type concern (iii) hyperparameter settings and (iv) other factors in isolation.
Not only do we miss sufficient computational
resources to perform such manifold ablations and comparative assessments, we also considered the carbon footprint that such experiments cause and which does not stand up to the insights that could possibly be gained from more experiments.
For these reasons, we focused on two selected dimensions of variants that we believe to be valuable for a community interested in Classical languages:
(i) We tried to answer questions as to when multilingual models can be profitably used, and (ii) aimed to showcase various potential advantages of encoder-decoder models, which by now have not been considered in studies on Classical languages.
Another clear limitation lies in the size of the demonstrated semantic and knowledge probing tasks. (i) They are of small size, and we cannot, therefore, draw firm conclusions as to, e.g., the effect of multilinguality. Also, the synonym/antonym disambiguation task is presumably the most difficult one. As a counter-balance, we used a more tangible task for knowledge probing, by choosing family relationships, which we expect to be frequently found in the pre-training corpora.
(ii) A further limitation we find for the knowledge probing tasks resides in the size of our trained models and the underlying pretraining data. This limitation could be one that is not easy to overcome. But we still encourage the community to create similar probing task datasets. Future work may find appropriate ways of data augmentation, or transfer learning methods that are applicable to historical languages so that further progress and insight will be possible."
887,in Section Limitations
888,"We state two points of limitations and future work in this section. First, the UniVPM combines both restored clean audio and original input audio for downstream speech recognition, while without any trade-off to weight them. For example, under extremely noisy conditions the restored clean audio plays a more important role, while in less noisy scenarios the original audio may provide more valid information. Some weighting strategies to select the most effective audio information could benefit the downstream speech recognition. Second, the proposed clustering and viseme-phoneme mapping are actually unsupervised schemes, so that it could be promising to extend our UniVPM to the popular self-supervised learning framework, in order to make full use of the abundant unlabeled data."
889,"The paper has only focused on graphs with multitype relations (knowledge graphs). When MAGNN shows improvement over baselines, someone may doubt if MA-GNN will do well on single-type relation graphs. The limitations of the representational power of the MA-GNN model should be discussed more deeply."
890,"In this work, we attempt to extend an existing MNMT model to support new language pairs with an acceptable expense. In addition to the advantages, our method has the following limitations:
(1) Additional introduced parameters. We utilize the parameter-isolation based method to support new language pairs. The total parameters of the MNMT model have been increased by pluggable modules to achieve better performance than prior studies. In the future, we will compress the number of parameters to the same size of original models meanwhile preserve the performance on all translation directions.
(2) The gap between our scenario and the realworld scenario. Our proposed method is a whitebox service in incremental learning. Thus, we train
a powerful MNMT model as the original model instead of directly utilizing existing models from the Internet. And we only consider eight incremental language pairs due to the limitation of computation resources. We try our best to simulate the realworld scenario and we will apply our proposed method for large-scale pre-trained MNMT models (e.g., NLLB 54.5B and M2M 12B) to validate the effectiveness in industrial scenarios."
891,We add a limitation section after the conclusions.
892,"TERTiUS is a pilot dataset. In particular, its test set can support segment-level metrics, but is not large enough to support reliable dialogue-level evaluation metrics. Due to resource constraints, we also do not report inter-annotator agreement measurements. While we made effort to make our interface low-friction, the demonstration setting still differs from the test-time scenario it is meant to emulate, and such a mismatch may also result in undesired data biases. Because our dialogues were collected before having a trained interpretation model, trajectories always follow gold interpretations. Because of this, the main sources of errors are ASR misdetections or user speech errors. In particular, TERTiUS contains data on: 1. misdetections and speech errors in transcription, and how to fix them through commands, 2. misdetections and speech errors in edits, and what intent they correspond to. We leave to future work the task of addressing semantic errors and ambiguities which result from incorrect interpretation of user intent. Some of these limitations can be addressed by incorporating trained models into the demonstration interface, which will allow faster demonstration, and capture trajectories that include actual system (non-gold) interpretations.
Though the trained system runs, we have not done user studies with it because it is not production-ready. The T5-base models are efficient enough, but the prompted GPT3 model is too slow for a responsive interactive experience. Neither model is accurate enough at interpretation. We welcome more research on this task!
When a human dictates to another human, interleaved corrections and commands are often marked prosodically (by pitch melody, intensity, and timing). Our current system examines only the textual ASR output; we have given no account of how to incorporate prosody, a problem that we leave to future work. We also haven’t considered how to make use of speech lattices or n-best lists, but they could be very useful if the user is correcting our mistranscription—both to figure out what text the user is referring to, and to fix it."
893,"section ’limitations"""
894,"Even though XY-LENT paves the way towards better general-purpose multilingual representation foundation models, in this section, we highlight the limitations associated with this work. We first expound upon the limitations associated with selfsupervised learning on large web extracted corpora. Then we show that while XY-LENT achieves strong performance on multiple multilingual benchmarks, when the downstream task involves unseen (during pretraining) languages, the performance drops by a substantial margin. Finally, we show the potential limitation associated with a common methodology used for domain adaptation associated with leveraging these multilingual foundation models, illustrating how catastrophic forgetting exacerbates certail issues pertaining to low resource language performance.
Training Data
XY-LENT uses CC-100 which a static multilingual corpus extracted from Common Crawl for 100 languages. As noted by Wenzek et al. (2020), several data filtering strategies have been applied to remove duplicated documents, paragraphs with high ratio of punctuations, digits and profanities, the resultant data may still result in many potential biases requiring further analysis. Additionally, these issues might be aggravated for models that leverage bitext data, since the bitexts themselves are mined from web crawls, and thus potentially have all the associated biases, stereotypes and other associated harms. Furthermore, the raw data was compiled from static Common Crawl snapshots from January, 2020 to December, 2020 and hence may not include information about some of the recent events such as COVID-19.
Performance on Unseen Languages
Given the performance improvements observed with scaling, we investigate how it impacts extremely low resource languages which are not present in the pre-training data. In order to do so, we consider our model’s performance on the AmericasNLI dataset (Ebrahimi et al., 2022) which extends the XNLI dataset to 10 Indigenous languages of the Americas.
Table 5 presents the results on the AmericasNLI
dataset. As can be seen, XY-LENT does outperform XLM-R, indicating that better representation learning also benefits these extremely low resource languages. However, we do not see an increase in performance while scaling our models. Specifically, the performance of XY-LENTBase and XYLENTXL model is nearly the same, and substantially worse that the performance observed on the XNLI dataset. This indicates that, while parameter scaling can help improve performance on languages that the model has seen during pre-training, it does not automatically improve performance in the extremely low-resource regime 6. Thus, while model scaling allows for improvements across numerous dimensions, it is far from a panacea, especially if not done in conjunction with data scaling efforts. To be able to improve performance for unseen languages, an intervention would need to be made at the data collection efforts during pretraining, which we aim to assess in future works.
Continued Training for Domain Adaptation in Pre-Trained Encoders
In recent years, continued training on domain specific corpora has been considered a viable approach for domain adaptation of MLM style pre-trained models (Gururangan et al., 2020; Yao et al., 2021) where the core idea is to continue train the pretrained model on domain specific corpora with the goal of improving in-domain downstream evaluation.
We first show that this phenomenon can be extended to models pretrained with an ELECTRA style training objective. Concretely, we apply domain adaptation in the biomedical domain where we continue to train our XY-LENTBase as well as XY-LENTMLM + TLM model on the PubMed data presented in Yao et al. (2021), and evaluate it on the ChemProt task (which aims at extracting relations between chemicals and proteins) presented in Gururangan et al. (2020) as the in-domain downstream task.
We observe that the continued training approach presented in Gururangan et al. (2020) for the ELECTRA style models, using the same peak learning rate as used during pre-training, results in divergence. Interestingly, this neither happens for the generator of the ELECTRA model nor for the
6Note that since the tokenizer is a sentencepiece tokenzier, there are extremely few UNK words in the low-resource languages. Consequently, the poor performance is not explained by excessive occurrences of UNK tokens
MLM style pre-trained model. Thus, for an ELECTRA style continued training setup, we posit reducing the peak learning rate to be a crucial change. Table 7 shows the performance on the downstream task post the continued training approach and unsurprisingly it helps with improving in-domain performance.
However, given the multilingual nature of such models, we test the multilinguality of these models
before and after continued training; using crosslingual zero-shot XNLI as a proxy for multilingual model quality. Table 6 shows the drop in performance across all languages pre and post continued training. We first note that this drop in performance is present for both MLM and ELECTRA style of models, and thus is not an artifact of the pre-training objective. We observe that the drop in performance is not uniform across all languages and the drop is worse for MLM style models (with using the same peak learning rate suffering more from this issue; Table 7). While we expect the drop in English performance to be relatively less, we do see that the drop is substantially more for the mid and low resource languages (especially Hindi, Turkish, Urdu and Swahili; see Fig. 5). While this can potentially be ameliorated by using techniques like Adapters (Houlsby et al., 2019) etc., we would like to draw attention towards the fact that general purpose continued training does suffer from this issue."
895,Limitations section after section 5
896,In Section: Limitations
897,Unnumbered Limitations section immediately after Conclusion.
898,Section Limitations
899,"Yes. Section Limitation.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
900,
901,Limitation
902,"Supporting more tasks. In this paper, we only consider attacking classification tasks (i.e., sentiment analysis, toxic detection, and natural language inference). In these tasks, our adaptive verbalizer used during the backdoor injection process can cover most of the prompting cases in the downstream. Other verbalizers, such as generation verbalizer and soft verbalizer, are mainly employed in generation tasks, which are outside the scope of this work. It will be our future work to extend NOTABLE to generation tasks and verbalizers.
Extension to more domains. Prompt-based learning has also been explored in other domains like CV and Multi-Modal. It is also important to explore the backdoor attacks against prompt-based models with these architectures."
903,"There are additional limitations and potential risks of LLM evaluations that should be noted, and these limitations are actually well-known problems of pre-trained language models. As listed on the Open AI blog for ChatGPT, ChatGPT sometimes generates answers that sound right and plausible but are totally nonsense. OpenAI also admits that the model’s response may be sensitive to the prompt used to query the model. While in Section 3.3.2, we find that the overall results among different instructions are not significantly different, we cannot guarantee that this is the case for all kinds of modification on the task instructions.
Other than the limitations listed on the OpenAI blog, there are still other limitations. For example, LLMs may not have emotions. Whether AI models have emotion is a more philosophical question and is controversial, so the results of using such models for evaluating emotion-related tasks may be strongly challenged and may even violate research ethics. As we find during our experiments, ChatGPT often replies ""I am an AI system and I do not have emotions like a human"" when asked to rate the likability of a story.
Another important limitation of LLM evaluation is that LLMs lack the ability to process visual cues in task instructions, unlike human evaluation. Human evaluators can use formattings such as special fonts or text styles to focus on important parts of the instructions. Additionally, the way instructions and questions are formatted can influence how human evaluators approach the task. While using special HTML syntax can serve as an alternative for visual cues, such tags are not used in human evaluation, so we do not use those HTML tags in LLM evaluation to incorporate visual cues in the inputs to the LLMs. However, LLMs can only process raw text input and are unable to take in visual cues."
904,"Many limitations of our study are already discussed in Section 5.2, however, we repeat and add to them explicitly here.
Small resource scenario Our study investigates MLP-based architectures for text classification tasks and finds competitive performance with vanilla Transformers while having lower cost in terms of the Green AI equation. However, the scope of our findings is naturally limited to the testing scenario, which is low-resource: Our models are relatively small, not pretrained on large generalpurpose corpora, and trained on datasets with fewer than 1 million examples. We may not say with certainty that our results will also hold on larger scale. For the sake of hypothesis-driven research we consider it more valuable to run many controlled small-scale experiments rather than few large-scale experiments. Nonetheless, scaling up should certainly be part of future research directions, as this is essential for optimal task performance.
Limitation to English pairwise sentence classification tasks Since token mixing is the independent variable in our study, we put our main focus on English sentence-pair classification tasks with textual input only, which we presume (and provide some evidence for) to be most useful to assess differences between token mixing models. Of course, vanilla Transformers are very flexible in the sense that, over the course of many studies, they have been shown to be very effective for a wide range of tasks, languages and data modalities. Whether or not the proposed HyperMixer model possesses similar flexibility cannot be answered in this study. The HyperMixer encoder arguably possesses similar inductive biases as Transformers. We thus expect it to be straight-forward to apply to tasks that are also solved well by Transformer encoders (e.g., span classification). For tasks such as language modeling, which involve a Transformer decoder, significant modeling advancements are required to obtain a HyperMixer equivalent. We consider this a very promising direction for future work.
Limitation to MLP-based baselines Similar to a trend in the computer vision community, our study investigates the suitability of MLP-based architectures for NLP. Due to their conceptual simplicity, these models promise to be easier to train, potentially leading to reduced Green AI costs. To this
end we compare our proposed HyperMixer model to a range of other MLP-based models, and Transformers. Apart from FNet and Linear Transformers, which are efficient Transformer alternatives, we do not attempt an exhaustive comparison to non-MLP-based efficient NLP models. Hence, the scope of our claims does not extend to all efficient Transformer models. However, these models are of course very relevant to this study, as they are targeted towards one of the factors of Green AI cost (single forward pass complexity). Therefore, we regard a comprehensive comparison as valuable future work."
905,"Since two-pass direct S2ST models require linguistic units as the target for the first-pass decoder, they cannot be used when the target language is unwritten. Compared to cascaded S2ST systems, direct S2ST systems require more data preparation steps, including training a HuBERT model, synthesizing target speech with a TTS model, extracting discrete units with the HuBERT model, and training a unit-based vocoder, etc. Moreover, the target audio quality of direct speech-to-unit systems relies on the quality of discrete units generated by selfsupervised discrete models. It further depends on the availability of speech data to train HuBERT models for the target languages.
Because S2ST systems could generate speech that does not necessarily represent the source speech’s content, there is a potential risk of conveying wrong information."
906,"""Limitations” section"
907,"Despite achieving good performance, there are some limitations in our study. The first is how to handle ambiguous instances in the corpus. 3.45% of the implicit data in PDTB 2.0 and 5% in PDTB 3.0 contains more than one label. Currently, we follow previous work and simply use the first label for training. But there might be a better solution to handle those cases. Another is the required time for training. To mimic the annotation process of PDTB, our model needs to pass through the embedding layer and transformers twice, so it takes more
time to train than the RoBERTa baseline. However, our training time is shorter than Pipeline and Adversarial due to those two models’ pipeline setup and adversarial training strategy. Also, note that our method has a similar number of parameters to the RoBERTa baseline since we share embedding layers and transformers between the connection generation and relation classification modules in our approach. Therefore, the memory required to train our model is not much different from that required to train the RoBERTa baseline."
908,"In Section 6.
7 A2. Did you discuss any potential risks of your work? Our paper is an entirely technical work. We don’t think it has any risk of bias or otherwise."
909,"Section Limitations
7 A2. Did you discuss any potential risks of your work? The model is designed and evaluated on established tasks and datasets, which should not cause severe risks."
910,"Due to the restrictions of the adopted benchmarks and resources, our evaluation bears the following limitations: (i) We only focus on social biases in the English language and North American cultures. This is due to the fact that both CrowSPairs and StereoSet are generated by crowd workers from North America. Future work can extend our analysis to other languages and cultures with
7See Appendix B for more details.
BERT/GPT-2 are highlighted in green .
the corresponding resources such as the French CrowS-Pairs (Névéol et al., 2022) and multilingual WEAT (Lauscher and Glavaš, 2019). (ii) Our evaluation has a limited coverage over different kinds of harms according to Blodgett et al. (2020). CrowS-Pairs, StereoSet, and WinoBias all focus on stereotyping, a kind of representational harm, while others like allocational harms are untouched. Developing methods to measure these harms generally requires in-depth interactions between technologists and customers. Blodgett et al. (2021) also point out several conceputalization and operationalization pitfalls in the above three bias benchmarks, which limits the validity of the results evaluated on them. (iii) Due to the incomplete bias attribute word lists, our CDA-based debiasing method is by no means fair enough to cover all the minority groups (e.g., groups with non-binary genders). Therefore the current debiasing method in this paper can only be used to mitigate bias among the demographic groups mentioned in Appendix A. We
recommend more complete resources such as the gender-inclusive word list in (Cao and Daumé III, 2021) for real-world scenarios."
911,"section 8 ""Limitations"""
912,"The primary motivation behind this paper was to provide a comprehensive benchmarking study that explores the impact of model compression techniques on bias in large language models. While our work is among the first efforts to address fairness in compressed language models across multiple com-
pression methods, including exploring multilingual settings, we are aware of the inherent limitations associated with our benchmarking study. Some of the limitations and potential directions for future work that builds on our study include the following:
• Our study primarily focused on benchmarking pre-trained models and evaluating their performance in the downstream text classification task. Expanding our investigation to encompass other tasks, particularly those involving generative models or large language models (LLMs), would be a valuable contribution to the research community. Examining the impact of model compression techniques on fairness in these domains would provide further insights and contribute to a more comprehensive understanding of bias in different types of language models.
• While our work includes a multilingual evaluation component, we acknowledge that there is room for further improvement and comprehensiveness in our benchmarking study, particularly with regard to quantization and pruning techniques. Apart from this, we did not provide a comparative analysis of monolingual and multilingual models using the same extrinsic data, which could provide valuable insights into the disparate impact of compression on the bias across languages. These are potential areas for future research that could contribute to a more thorough understanding of bias in compressed language models.
• Despite showing results for state-of-the-art pruning methods, further benchmarking is necessary to observe how bias varies across different pruning techniques. Similarly, whilst our method serves as a proxy to estimate bias trends in quantized models, a thorough quantization-specific study is needed.
• Different compression strategies yield varied benefits in terms of latency, memory, and so forth. Investigating the tradeoffs between these elements and fairness and accuracy would yield valuable insights for obtaining realistic estimations in real-world scenarios. Additionally, conducting case-study analyses would give practitioners in the field a deeper understanding of the potential harm these methods may introduce."
913,"Yes, in the final section of the paper.
7 A2. Did you discuss any potential risks of your work? No, as our work deals with pointing out the potential risks of certain strategies used to optimize for efficiency that could inadvertently cause models to make more biased decision. We have extensively discussed the limitations and other potential implications of our findings, however."
914,"This work has been studied on the Wikipedia corpus, following the standard experimental setting used in previous unsupervised sentence representation learning studies. We expect to see many important findings by investigating sentence representation learning on various corpora in different domains such as Bookcorpus (Zhu et al., 2015) and the C4 corpus (Raffel et al., 2019)."
915,"A limitation of our work is that we cannot directly apply our methods to the few existing revisionbased corpora from other domains (Yang et al., 2017; Afrin and Litman, 2018; Anthonio et al., 2020) for multiple reasons: On the one hand, those corpora do not contain histories with more than one revision but only before-after sentence pairs). Some also consist of less than 1000 sentence pairs, rendering the quantitative experiments considered in this paper pointless. On the other hand, additional metadata useful for our analysis (e.g., revision types and contextual information) is either not available at all or only for a limited number of instances that is insufficient for training models.
Furthermore, the methods we evaluated utilize distantly supervised labels based on the assumption that each revision improves the quality of the claim and additional annotations provided by human editors. These annotations suffer from being coarse-grained, consisting of mainly three classes. However, each of the improvement types can be represented by several more fine-grained revision intentions. A point that we did not consider as part of this work is whether certain revisions can affect or inform future revisions within the same debate, for example, rephrasing of arguments to avoid repetition or ensuring that all claims use the same wording for the main concepts. Often, such relationships are implicit and cannot be derived without additional information provided by the user performing the revision. We believe that collecting datasets and developing approaches, which enable distinguishing more fine-grained types of edits and implicit relationships, could not only enable deeper analysis and training more fine-grained improvement suggestion models, but also allow for better explanations to end users.
However, it should be noted that some of the considered methods rely on deep learning and have certain limitations when it comes to underrepresented classes, where the number of available training instances is very low. This is especially important when considering the task of claim improvement suggestion. We also point out in this regard that we only use the base versions of the BERT, ELECTRA, and DeBERTa models due to resource constraints.
The results may vary, if larger models are used. While common types of improvements likely differ across other domains and communities, we stress that our approaches are entirely data-driven, and are not tied to any specific quality definition. Therefore, we expect our data processing and filtering methods as well as the considered approaches to be applicable to other domains, where historical collaborative editing data similar to ours is available. When it comes to practice, several issues require further investigation, such as how to integrate recommendations in collaborative editing and educational environments, whether the recommended improvements will be accepted by users, and how they may impact the users’ behavior. We leave these questions for future work."
916,"While our approach does require domain-specific information extraction models to extract structured representations of novel misinformation claims for easy aggregation and review, there is significant prior work on event extraction that can be adapted to extract check-worthy claims (Ritter et al., 2012; Luan et al., 2019; Du and Cardie, 2020). Furthermore, we argue content moderators or factcheckers are likely to be more effective when focusing on one claim type at a time (e.g. COVID-19 treatments, election integrity, vaccine effectiveness, etc.), rather than reviewing a mixture of claims on multiple topics.
Our COVID-19 case study also makes use of “mock” content moderators, rather than employees or contractors working for social media companies or fact-checking websites. However, we believe this methodology still provides valuable insight that would not be publicly available otherwise, as social media companies do not currently publish extensive details about their content moderation processes6 and fact-checking websites vary widely in policy and have been shown to provide inconsistent claim classification (Marietta et al., 2015). Some prior user studies (Nguyen et al., 2018; Pennycook and Rand, 2019; Shabani et al., 2021) have also shown laypeople (e.g., Amazon Mechanical Turk workers) can be good at judging the veracity of claims or reliability of news articles.
As of late November 2022, Twitter has suspended enforcement of its COVID-19 misleading information policies such as the one we target in this paper.7 However, per the Associated Press article, one of the possible reasons for the suspension was that Twitter has “struggled to respond to a torrent of misinformation about the virus” with many “bogus claims about home remedies” still on the site despite the previous enforcement of policies. While we do not have details about the internal automated systems Twitter has in place to assist with content moderation, an end-to-end early detection system might have helped stem the spread of misinformation on the platform. Additionally, despite the lack of official policy enforcement, our system can still be used by third-party fact-checking websites or researchers to measure and report misinformation
6https://www.nytimes.com/2022/05/19/b usiness/twitter-content-moderation.html
7https://apnews.com/article/twitter-e nds-covid-misinformation-policy-cc232c9 ce0f193c505bbc63bf57ecad6
on Twitter. Finally, the main goal of our work is not to create a system for COVID-19 misinformation detection but rather to propose a framework that allows for a fair and realistic evaluation of early misinformation detection systems in any domain."
917,Section 6
918,"There are several limitations in this work. First, we have not explored how to make use of compositionbased augmentations in the supervised setting. A second limitation is a lack of theoretical grounding in the impact of our latent space composition. Finally, we have not explored interoperability with other training objectives."
919,We provided a negative result in Section 4. We also dedicated a section to this end: Section 8.
920,"One limitation of this work is the focus on Englishto-many and many-to-English settings, while previous studies also went beyond English-centric translation (Freitag and Firat, 2020; Fan et al., 2022). Second, we experiment with a WMT based benchmark that has a total of 15 languages and 200M training examples, when translation models were also trained on larger datasets (Aharoni et al., 2019; Arivazhagan et al., 2019; NLLB Team et al., 2022). We leave questions about the amount of scale that will be required to effectively mitigate interference in massively (many-to-many, billions of parallel sequences) multilingual settings for future work.
Additionally, the data collected from high resource languages may be of higher quality compared to that collected from low resource languages. Further research is needed to determine the impact of low quality training data on interference and synergy. Finally, while we explore trends when scaling models width, deeper models (Ghorbani et al., 2022) might help mitigating interference even further."
921,"Section 7
7 A2. Did you discuss any potential risks of your work? Our work does not add new risks involving translation models"
922,Limitations
923,Limitation section after Conclusion
924,"While we cover a wide range of different factors of cross-lingual semantic parsing (e.g., tasks, datasets, natural languages, meaning representations, domains), we cannot include all possible dimensions along with these aspects. Furthermore, we focus on the linguistic generalization ability for semantic parsing because the questions are translated from the English datasets. In the future, we will explore questions raised by native speakers in each language to study the model ability under variations in cultural backgrounds and information-seeking needs."
925,"Despite promising results, we also observe that refreshing and querying the datastore during training
is time-consuming. Our proposed training framework usually takes 3× ∼ 4× training time. In future work, we will explore methods to improve training efficiency. We include a training loop to dynamically use the latest datastore to inject knowledge into neural networks. However, we still find that the kNN knowledge still helps the inference even after our training loops, demonstrating that there still remains space to improve the effectiveness of knowledge injection."
926,"Limitations and bias of pre-trained models: Our work uses detected objects and their attributes in the images to introduce novel insertions in the corresponding text. To this end, it is important to address the limitations of the state-of-the-art object and attribute detection methods. The undesired artifacts of these methods could be categorized as inaccurate or biased. The detected objects could be incorrect, but since we only consider objects that are also mentioned in the text, the effect of incorrect object detections is non-existent in our augmentations. However, we notice that some of the detected attributes in images and BERT predictions reflect stereotypical associations and have been documented in prior works (Li and Xu, 2021;
Kaneko and Bollegala, 2022). We acknowledge that the current state of deep learning research is limited, and the consequential shortcomings are reflected in our augmentations to some extent. Broader social impact: The authors do not foresee any negative social impacts of this work. We believe our cross-modal augmentations will enable an exhaustive evaluation of the robustness of visionand-language models, leading to more reliable multimodal systems. We release the code for our experiments to aid reproducibility and enable future research on this topic. Annotations, IRB approval, and datasets: The annotators for evaluations done in this study were recruited via Amazon Mechanical Turk. We specifically recruited ‘Master’ annotators located in the United States; and paid them at an hourly rate of 12 USD for their annotations. The human evaluation experiments were approved by the Institutional Review Board (IRB) at the authors’ institution. The datasets used in this study are publicly available and were curated by previous research. We abide by their terms of use."
927,"We highlight several limitations of our work:
Unnatural prompting format The choice to separate inputs and targets using a space character has proven effective to multitask finetune our decoder-only models. Nonetheless, poorly formatted prompts may result in undesirable behavior. For example, given the following prompt: “Translate to English: Je t’aime"", the model may continue the input with additional French content before starting to solve the task, i.e. translating the input from French to English. This can be mitigated by improving the prompts with a trailing full stop or a newline symbol. Encoder-decoder models, such as our mT0, do not suffer from this problem, as inputs and targets are fed into different parts of the model.
Limited languages in xP3 The pretraining corpus of mT0 contains more than 101 languages (Xue et al., 2020), however, we finetune on only 46 languages. Likely, finetuning on the full 101 languages mT0 has seen during pretraining would lead to better performance. However, we decided to use only the languages of BLOOM in order to study language generalization (§4.2). Similarly, one could likely attain better performance by enhancing xP3 with more datasets, such as via BIG-Bench (Srivastava et al., 2022; Suzgun et al., 2022), or more
prompts, such as via NL-Augmenter (Dhole et al., 2021). We have released an extended version of xP3 dubbed xP3x that covers 277 languages and is around ten times larger than xP3, but are yet to finetune models on it.
Performance While our models show strong capabilities of performing tasks zero-shot, there remain numerous failure modes that are common in large language models (Rae et al., 2021; Bommasani et al., 2021; Zhang et al., 2022; Smith et al., 2022; Ouyang et al., 2022; Taylor et al., 2022; Chowdhery et al., 2022; Biderman et al., 2023; Allal et al., 2023; Li et al., 2023). In Figure 16 of Appendix §F, BLOOMZ fails to understand the moral of a fable resulting in an undesirable generation. Similarly, in Figure 15, mT0-13B is asked to provide an explanation, but answers with a question. We have made several modifications to the multitask finetuning recipe, such as loss weighting, mixing in long tasks, and various multilingual aspects, leading to the strong zero-shot performance of our models. However, there are many other changes to the multitask finetuning procedure that are worth exploring to get better models (Honovich et al., 2022; Wang et al., 2022b; Longpre et al., 2023a; Liu et al., 2023; Dettmers et al., 2023; Yin et al., 2023). Further, the pre-trained models we use, BLOOM and mT5, are suboptimal in many aspects such as compute allocation (Hoffmann et al., 2022; Muennighoff et al., 2023), pretraining datasets (Longpre et al., 2023b; Touvron et al., 2023; Chung et al., 2023), pre-training objective (Tay et al., 2022b) and possibly model architecture (Komatsuzaki et al., 2022; Shen et al., 2023). Future work should investigate multitask finetuning better base models.
Learning new languages during finetuning While we have investigated generalization to languages only seen during pretraining, we did not investigate generalization to languages only seen during finetuning. Our mT0 models are finetuned on several new languages not seen in pretraining (see Figure 2). Out of those, we only evaluated on code (HumanEval), where mT0 performed at the random baseline (0.00 in Table 10). We point to follow-up work that has investigated the question of teaching BLOOMZ new languages (Yong et al., 2022; Cahyawijaya et al., 2023) and work investigating adaptation of BLOOM (Ennen et al., 2023; Yong and Nikoulina, 2022)."
928,
929,"Our work has a few limitations that we care to highlight. First, it focuses on interpreting models through the vocabulary lens. While we have shown evidence for this, it does not preclude other factors from being involved. Second, we used E′ = ET, but future research may find variants of E that improve performance. Additionally, most of the work focused on GPT-2. This is due to shortcomings in the current state of our framework, as well as for clear presentation. We believe nonlinearities in language modeling are resolvable, as is indicated in the experiment with BERT.
In terms of potential bias in the framework, some parameters might consider terms related to each due to stereotypes learned from the corpus."
930,"In the section ""Limitations"" after Section 5."
931,"It is important to mention some limitations of our work. Firstly, it would be wise to evaluate the impact of the tokenizer on the performance of the models to ensure that this is not the main reason for the observed performance gains.
Furthermore, we can not affirm in this study whether the medical domain transfer observed from English to French using continual pre-training on PubMedBERT can be generalized to other languages or other domains.
Finally, it is possible that training a ChuBERT model with more diverse private clinical data and in a larger quantity could have brought notable performance gains on private tasks.
A considerable amount of computational resources was used to conduct this study, since approximately 18,000 hours of GPU computation were used to create the 7 models presented here, as well as about 7,500 hours of GPU for debugging due to technical issues related to model configurations and poor performance, for a total of 25,500 hours. The total environmental cost, according to the Jean Zay supercomputer documentation4 is equivalent to 6,604,500 Wh or 376.45 kg CO2eq based on the carbon intensity of the energy grid mention by BLOOM environmental cost study also made on Jean Zay (Luccioni et al., 2022). This makes the present study difficult to reproduce and to transpose to other languages when limited material resources are available.
4http://www.idris.fr/media/jean-zay/jean-zay-consoheure-calcul.pdf"
932,Section 9
933,"The limitations of SENDIR include the following two points: (1) It has not extended to documentlevel entity-centric relations tasks. Our work is event-centric, and future work extends it with entity-centric cases. Document-level entity-centric RE needs to consider multiple mentions of an entity and different relations in different directions of the
same entity pair. (2) It does not bring in external commonsense knowledge. Knowledge can be used to enrich events and improve the accurate ERE."
934,"In the section on ""limitations""."
935,"Limitations. The HuBERT model quality is critical to speech-to-speech translation performance, as its extracted units are used by both speech-tounit model and vocoder. We have not explored the optimal strategy of multilingual HuBERT training. One research question is how to choose a group of languages so that a multilingual HuBERT model could be well trained. For example, it is arguable whether Lithuanian (lt) should be included in Slavic or Uralic family. Other questions could be whether a larger HuBERT with more model capacity should be used and how we should deal with language imbalance in multilingual training.
We provide benchmark results of bilingual speech translation with mined data selected by heuristics. One of our future directions is to come up with a better strategy of mined data selection to improve translation performance and training efficiency.
As mentioned in our results analysis, the reported BLEU scores are heavily dependent on the ASR quality, which may not reflect the speech translation performance accurately. Future directions could be improving ASR quality or exploring other evaluation metrics without reliance on ASR models.
Potential Risks. As a technology used for speech generation, the presented speech translation models or the translation models that will be trained with SpeechMatrix dataset might have systemic bias or produce inappropriate outputs."
936,Limitations of this work is discussed in Section 7.
937,"This paper are based on the assumption that Universal Information Extraction (UIE) models have limitations, particularly with regards to over-reliance on label span boundaries and inflexible attention span length. Therefore, the proposed framework may be computationally and spatially expensive as it requires a more complex attention mechanism and additional computing power for training. Nevertheless, this limitation of the span-based UIE model can be overlooked in comparison to that of the generative UIE model, which uses a stronger language model. Additionally, the probability density functions explored in FSL are limited; thus, further research is needed to develop a more targeted strategy for adjusting the correct information distribution."
938,"Section #Limitation
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
939,"While we discuss limitations to our survey design and results in §3 and §6, we expand on the issues here."
940,Limitations section
941,"We present a new dataset for meeting summarization that has the potential to improve the efficiency and effectiveness of meetings. However, we note that the dataset is limited to city council meetings
from U.S. cities over the past decade and licensing issues have restricted our ability to include certain city council meetings in the dataset. For example, we contacted the City Council of San Francisco and were informed that they do not allow the redistribution of meeting minutes. Moreover, our dataset does not include non-verbal cues such as eye gazes, gestures and facial expressions, which may make it less suitable for developing summarization systems that rely on these cues. Despite these limitations, we believe that the dataset is of high quality and will be a valuable resource for the development of meeting summarization systems."
942,"Section 9
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
943,"In this paper, our main contribution is an effective and efficient framework for universal IE. We aim to introduce a new unified IE paradigm with extractive structures and triaffine attention mechanism, which can achieve better performance in a variety of tasks and scenarios with more efficient inferencespeed. However, it is non-trivial to decide whether a sophisticated and artificial prompt is required for complex datasets and large label sets. In addition, we only compare with limited baselines with specific datasets configurations when analyzing the performance of the UniEX in supervised, few-shot and zero-shot settings. In experiments, we implement only a few comparative experiments between BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) due to the limit of computational resources."
944,limitations
945,"Limitations of our task definition Depending on the application, attribution and preservation may not deserve equal weight. For instance, if there are multiple acceptable options for the output, such as in a dialog system, we might trade-off preservation for attribution, similar to how LaMDA behaves in our experiments.
Our evaluation metrics also do not measure all aspects of attribution. For instance, some sentences are self-evident and do not require attribution (e.g., “I agree.”) but would be penalized in our evaluation. It is also necessary to note that linguistic assertions have varying scope: for example, there is a difference between “Frozen is a scary movie” and “I got scared watching Frozen” — while expressing a similar sentiment, the former makes a more general statement that many would disagree with, while the latter is scoped to the speaker’s own experience. In some applications, one could even argue that the latter case does not require attribution, since the speaker is their own source-of-truth. In addition to varying scope, utterances can also make assertions with varying levels of directness. For example, according to standard linguistics, “John ate some of the cookies” yields the implicature that John did not eat all of the cookies, even though it is not logically entailed. This raises the question of which implicatures or implied assertions should be detected and attributed, which should be explored in future work. For more nuances, we refer to Rashkin et al. (2021).
For preservation, we wish to explore other properties that should be preserved, such as discourse or logical coherence. Additionally, if the input text passage is completely misguided or flawed, it can be difficult to revise the text without significant changes, which would be heavily penalized by the current metrics.
Limitations of our model While we aspire to improve attribution for arbitrary text, it is clear that RARR is not yet fully general. For example, the current implementation of RARR would not be well-prepared to edit poetry (where preserving rhyme matters) or long documents, primarily because we do not provide examples of such inputs in our few-shot LLM prompts. However, we do believe that future developers may be able to quickly adapt RARR to such tasks by simply changing the prompts. Second, RARR tends to preserve rather
than delete claims that it cannot attribute. Some of these claims genuinely do not require attribution, but others are hallucination and should be removed. Judging whether a claim requires attribution can be subjective and challenging. Finally, our model is computationally costly, since it is based on prompting a large language model. One potential solution is to leverage recent synthetic data generation recipes to train a smaller model (Lee et al., 2021; Schick et al., 2022)."
946,"limitations for the dataset, although we adopt several methods to assure a high quality of the dataset, mislabeled data still exist due to the subjectivity of the annotators. for example, annotators may have different opinions on whether to regard 屁民(shitizen) as unsafe because 屁民(shitizen) is a rare word in chinese and could be both derogatory and selfdeprecating humorously in most cases. moreover, our dataset is in chinese. directly translating safeconv to other languages with translation tools may induce erroneous labels due to syntactic and cultural differences between languages. we call for endeavors to fix it, such as annotating similar datasets in other languages or improving translation strategies. for the experiments, firstly, in section 6, we evaluate the performance of the rewriter based on chatbots of restricted sizes. however, there are large chatbots that we do not include in the evaluation due to the limitation of computing resources, such as eva-xlarge with up to 2.8b parameters, on which the detoxifying results will lead to more comprehensive results. secondly, as shown in table 8, the overall contextual coherence and informativeness of the responses from current state-of-the-art chatbots in chinese are still not satisfying. evaluating safeconv on more powerful chatbots based on large language models is worth exploring in the future."
947,"limitations one trade-off of limiting the prediction space using an extractive pointer module is that it does not support prediction of multiple slot values which is necessary for some dialogues in the multiwoz 2.3 and 2.4 datasets. to keep the architecture simple we do not consider cases in which slots take multiple values in this work, but we can effectively adapt our model for this setting by introducing sequential query tokens for each slot. another limitation is that the span representation requires a computation of o(n · lans) complexity where n and lans represent the length of context and answer span, respectively. for very long answers this might occur significant computational costs compared to exist- ing span prediction approaches which have o(n) complexity. however, this can be alleviated by adding a simple sampling and filtering step during training and prediction. we plan to further study and address these limitations in future work."
948,"limitations first, due to the lack of datasets to evaluate the mpdrg task, we perform our experiments only on the ubuntu irc benchmark and pre-train our model only on the domain of ubuntu chats. however, the potential of our approach goes far beyond that since it is applicable to any open-domain multi-party dialogue dataset. in the future work, we will consider applying our method in more open-domain conversational datasets, such as the transcripts of tv series or movies. additionally, the pre-training process solely relies on the addressee information of individual turns, disregarding the reply-to relations within the dialogue history. this oversight prevents the model from benefiting from valuable contextual cues necessary for a comprehensive understanding of the multi-party dialogue. in our future work, we will explore the integration of discourse-level reply-to relations into the pre-training process to further enrich the capabilities of the model."
949,"limitations we list down some potential limitations of aclm: 1) plms are restricted by their knowledge to generate entirely new complex entities due to their syntactically ambiguous nature. adding to this, substituting complex nes in existing sentences leads to context-entity mismatch. thus, as part of future work, we would like to explore if integrating external knowledge into aclm can help generate sentences with new complex entities in diverse contexts. 2) we do not conduct experiments in the language farsi from the multiconer dataset as neither mbart-50-large nor xlm-roberta-large was pre-trained on this language. 3) the use of mbart-50-large for generation also restricts aclm from being transferred to code-switched settings, and we would like to explore this as part of future work."
950,"limitations this study focuses only on improving the speed of knn-mt during decoding; other problems with knn-mt remain. for example, it still demands large amounts of memory and disk space for the target token datastore. in addition, our subset knn-mt requires to construct a sentence datastore; therefore, the memory and disk requirements are increased. for example, the quantized target token datastore has 52gb (|m| = 862,648,422) and our sentence datastore has 2gb (|s| = 29,540,337) in the experiment of wmt’19 de-en (section 4.2). although subset knn-mt is faster than the original knn-mt in inference, datastore construction is still time-consuming. the decoding latency of our subset knn-mt is still several times slower than base mt for large batch sizes. the experiments reported in this paper evaluated the inference speed of the proposed method on a single computer and single run only; the amount of speed improvement may differ when different computer architectures are used. ethical consideration we construct both knn-mt and subset knn-mt datastores from open datasets; therefore, if their datasets have toxic text, knn-mt and our subset knn-mt may have the risk of generating toxic contents."
951,"limitations we report the following limitations of mildecoding. mil model still suffers from the tradeoff between detoxification effectiveness and lan- guage model quality (wang et al., 2022). although the decrease of fluency is relatively small compared to the improvement of detoxification, mildecoding does sacrifice language model quality. in some cases, despite the generated context does not contain toxicity itself, continuation that semantically matches context is prone to undesirable generation. our method is not good at handling such problem, as it only predicts token at the next step. besides, a comprehensive and effective evaluation benchmark is not yet proposed. in most cases, toxicity is measured with a trained classifier. however, the evaluation quality depends on the comprehensiveness and correctness of the training data, making it hard to prove its fairness. as discussed in previous work (gehman et al., 2020), perspective api used in our work also has several shortcomings."
952,"limitations of the work given that the training data for most pre-trained models has not been released, further investigation of the frequency effects of control verbs in the corpora, or for that matter, of any other critical word in the sentence (names, adjectives, etc.) is not feasible. this is a shortcoming of our work because word frequency during training is known to be an important factor for model performance (wei et al., 2021). nonetheless, in order to approximate this issue, we run preliminary comparisons of the models’ performance depending on whether the control verb appears or not in the vocabulary (and therefore, assuming that it had enough frequency in the training corpus). very similar results were obtained for both sentences with known and unknown verbs in the main clause. besides, detailed comparisons between models have been left out for reasons of space and scope, since the objective of the research was not to compare model performance, although it is a relevant and interesting issue in itself (for instance, the fact that the lms based on the roberta architecture performed better across tasks, or that the high performance of xlm-roberta contrasts with that of mbert). in relation to this, the comparison of models with different architectures and training objectives (e.g. generative models) was also left for further research. finally, it is worth noting that the two languages evaluated in this study (spanish and galician) are very similar, so that it could be interesting to expand the research to non-romance languages."
953,"limitations of mlms along with the iterative nar inference for open-ended text generation and observed that mlms would collapse for open-ltg. through extensive study and analysis, we found the reason is the inappropriate attention mechanism and inference strategies, and introduced two simple strategies to alleviate such a problem, i.e., dynamic sliding window attention and linear temperature decay. experiments demonstrate that our model achieves competitive performance and significant speedup. we hope our research can make pre-trained mlms as new candidates for the open-ltg community."
954,"limitations one limitation of our study is that interpretable feature spaces are at times only semi-interpretable. we infer from patterns of model behavior that buchanan features such as ‘human’, ‘child’, and ‘animal’ can be signals for animacy more broadly construed. the need to conjecture about what a feature means points to a weakness in our approach. some interpretation will always be necessary, and with a more heavy-handed probing method like ours, it can’t be certain what effects are coming from the model and which are coming from the probe. one way to get around this need for subjective interpretation is to train a separate classifier for animacy more broadly understood, and then use the feature prediction model to examine what features are most relevant to the classifier (chersoni et al., 2021). however, this method is not foolproof either. the classification distinction is wholly determined by the labeled data used to train the animacy probe, and the judgments are subjective. even for a seemingly straightforward feature, the correct label is not always clear. is a clock that sings the hour animate? what about a stony face? subjective interpretation is an important and unavoidable component of both linguistic and neural language model analysis. the goal of datadriven research is to extend the sphere of concern beyond self-reflexive subjective judgments of the researcher to the shared subjectivities of a language community. information about animacy reflected in an annotated dataset still reflects subjectivities, but shared ones. it is important to always be clear about where interpretation is happening, whose interpretations are taken into account, and how they affect what"
955,"limitations though our method does not require iterative retraining, pruning, and rewinding process, one question still remains under-explored: how to selfadaptively find the optimal sparsity instead of trial training, which can boost the training efficiency. also, we plan to further investigate the effectiveness of lottery prompt tuning in other scenarios, including the multi-task learning (he et al., 2022b), prompt ensembling (lester et al., 2021b), etc. furthermore, the proposed learning method should be compatible with other parameter-efficient finetuning methods, such as adapter tuning (houlsby et al., 2019) and lora (hu et al., 2021). we leave these for future research."
956,"limitations we discuss the limitations of our research as follows: • firstly, since the t5-large model has many parameters and our task is document level, one training process will occupy four nvidia v100 32gb gpus; • our paper mainly studies document-level eae task. although we believe our approach is compatible with all document-level extraction tasks, how to adapt it to those tasks still remains an open question."
957,"limitations hyper-parameters selection some hyperparameters still acquire careful selection for wecheck, e.g. p+, p−. also, using different set of hyper-parameters for different tasks and datasets will further boost performance. thus, we need to train the model several time and select the best performing parameters based on validation. end-to-end training wecheck applies the weak annotation and noise-aware fine-tuning twostep pipeline, where the noises in the first step will greatly affect the performance of the second step. by modifying the overall framework into end-toend training will solve this problem."
958,"limitations in this paper, we discuss an important topic in the nlp field, the defense against adversarial attacks in nlp applications. we provide a strong defense strategy against the most widely used word substitution attacks in the nlp field, which is limited in several directions. • we are testing defense strategies using downstream task models such as bert and roberta, and the purification tool is a model with a mask-filling ability such as bert. such a process can be further improved with strong models such as large language models. • we study the concept of adversarial purification in the adversarial attack scenarios with word-substitution attacks on small fine-tuned models. the concept of adversarial purification can be further expanded to various nlp applications. for instance, the purification of natural language can be used in malicious text purification which is more suitable in applications with large language models."
959,"limitations although speech performs well on event-centric structured prediction tasks in this paper, it still has some limitations. the first limitation relates to efficiency. as speech involves many tasks and requires complex calculation, the training process is not very prompt. the second limitation relates to robustness. as seen in the experimental analysis in § 4.5, speech seems not always robust to unevenly-distributed data. the third limitation relates to universality. not all eventcentric structured prediction tasks can simultaneously achieve the best performance at the same settings of speech."
960,"limitations naturally, our work comes with a number of limitations: for instance, we restrict ourselves to testing eight pronoun sets out of the rich plethora of existing options. to ensure diversity, we resort to one or two sets per pronoun group—we hope that individuals feel represented by our choices. similarly, we only translate single sentences and don’t investigate translations of larger and possibly more complex texts and we only translate to a number of languages none of which is resource-lean. our study demonstrates that simpler and shorter texts already exhibit fundamental problems in their translations, even to resource-rich languages."
961,"limitations this work introduces a pseudo-entity recognition (per) task to supervise the model learning overlap knowledge. since no additional entity annotation is available, we manually create a mapping functionm(r), which maps each argument role r to an entity type. with the help of the mapping functionm(r), the eae label can be converted to the per label. however, because the annotation of the eae task is complicated, it is hard to avoid a few exceptional samples in the prior mapping function. some entity words may be attached to impertinent entity types. for example, there is a triple of argument role, event type, and argument in rams’s movement.transportartifact.preventexit event: {artifact, object, two pilots}. the ""artifact"" argument is mapped to ""object"" inm(r), but we expect ""two pilots"" can be mapped to ""person or organization"". we tolerate such exceptional samples, and the occasional noise has not affected the training of ape."
962,"limitations as we tentatively give a successful implementation of leveraging soft-prompt-based manner to benefit both single and multi-attribute ctg, such a paradigm deserves a closer and more detailed exploration. first, we explore multi-attribute ctg in the scenario of two-attribute composition, yet combining more attributes when generating a completion is more challenging and thrilling, and still in its fledgeless stage. besides, while extensive experiments demonstrate that tailor consistently improves attribute-based ctg, applying our approach on a wider variety of plms will evaluate the effectiveness of tailor in a more generally way."
963,"limitations although our datasets are publicly available and gathered from participants in different countries, they cannot entirely represent the moral norms from all the individuals in different cultures over the world or predict how moral norms might change into the future (bloom, 2010; bicchieri, 2005). additionally, we examine a limited set of moral issues for each country, therefore the current experiments should not be regarded as comprehensive of the space of moral issues that people might encounter in different countries. moreover, taking the average of moral ratings for each culture is a limitation of our work and reduces the natural distribution of moral values in a culture to a single point (talat et al., 2021). implementing a framework that incorporates both within-country variation and temporal moral variation (xie et al., 2019) is a potential future research direction. currently, it is not clear whether the difference between eplms’ estimated moral norms and the empirical moral ratings is due to the lack of cultural moral norms in the pre-training data, or that the cultural moral norms mentioned in the pre-training data represent the perspective of an english-speaking person of another country. for example, a person from the united states could write about the moral norms in another country from a western perspective. a person from a nonwestern country could also write about their own moral views using english. these two cases have different implications and introduce different moral biases into the system. potential risks we believe that the language models should not be used to prescribe"
964,"limitations the current system may require the user to have some music knowledge to compose the word boundary prompt from music. hence, more efforts need to be made to fulfill this gap before such a system can operate fully automatically without the human user providing word boundary prompt themselves. we use the back-translation of mono-lingual data to augment the parallel training data, but the quality, especially the text style of back-translations has room to improve. although we have tried using iterative bt to gradually refine the backward direction mt model to adapt its outputs to lyric style, we found some errors gradually accumulated in the back-translated data, which finally made our model perform unsatisfactorily for negative sentences, together with the decrease of controlling effectiveness. further exploration is needed in this aspect. similar to chat text, lyrics are usually composed in short sentences. sometimes it would be challenging to guarantee the consistency of style and meaning for different sentences, if the current sentencelevel translation system are adopted. hence, for building future lyric translation systems, it would be a better option to translate the lyrics directly at the paragraph level or document level."
965,"limitations of only considering the comparator when judging a simile. meanwhile, ppl shows a higher correlation than the other two metrics in evaluating fluency, yet having a remarkable gap with the human score. to furtherly explore the concerns of human when evaluating a simile, we also compute the internal correlation of human scores. as shown in appendix table 11, there is a strong correlation between creativity and consistency. it means that having ground is also important in generating a creative simile, illustrating the necessity of interpretably retrieving tenor-vehivle pair in the vehicle-unknown setup."
966,"limitations while the proposed single-frame training approach shows strong performance on various videolanguage datasets, it does not work well on true temporal tasks like the new ssv2 tasks. compared to multi-frame models, our single-frame model also has a higher demand for pre-training data."
967,"limitations a key limitation in our work is that llms might have seen these math problems. our work theoretically assumes this is not the case. another limitation is that for the sake of simplicity, our work makes some assumptions. for example, we assume all numbers in the range of integers 0 to c = 300. this would not cover every mwp out there. and future work is needed to generalize our framework to other forms of mwps. in this work, we are also constrained by the limitations of the openai policy on the gpt-3 api. this limits the number of perturbations we consider in this work as well as the accuracy with which we can estimate our causal distributions. finally, our work is restricted to english, and extending it to other languages will require us to create an mwp dataset in that language."
968,"limitations although our proposed method performs well in evaluating the open-domain dialogue systems, it also has some limitations. our method identifies the dependencies between context and response. however, according to howcroft et al. (2020), human-evaluated metrics can contain a variety of attributes whilst we only identify the large-scale dependencies of semantics and do not disentangle the texts into the attributes of human-evaluated metrics. in the future, we will conduct disentanglement studies to disentangle the text into various attributes to optimise our model and further improve the interpretability of text evaluation methods based on these disentangled attributes."
969,limitations and how these approaches would impact the data bias and the resulting model performance would need to be further researched.
970,"limitations while muda relies on set of hand-crafted rules for tagging specific phenomena, these rules might involve the use of other error-prone systems (such as coreference resolution and alignment models) and these errors might be susceptible to problems (such as lack of out-of-domain generalization) that could limit the applicability of our tagger. however, this could be fixed by extending muda to use newer and better versions of these systems. the use of f-1 per tag with surface-form matching between reference/translation can also lead to penalizing translations that use context correctly but choose other equivalent words. nevertheless, this should also be mitigable by extending the scoring method to, for example, match synonyms. finally, the benchmarking of context-aware models might not apply to newer, state-of-the-art translation models, especially if these leverage large language models that were trained on long-context data."
971,"limitations when applying causal intervention to remove psycholinguistic bias, we utilize the liwc dictionary to construct the confounder dictionary du. we argue that the debiasing performance could be affected by the quality of the constructed confounder dictionary. in the future, we could try to improve the confounder dictionary with external knowledge."
972,"limitations the limitations of this work are mainly twofold. 1. different modalities are trained with the same optimizer setting, which might cause imbalance across modalities. 2. no theoretical analysis is established to provide insight of the balance between modality independence and dependence."
973,"limitations in this work, we propose to use the denoising score matching function to estimate the gradient of logdensity distribution, then describe the differences between the adversarial and normal samples by the denoising process of langevin dynamics. although our method achieves very good detection performance (nearly 100% under various settings), the actual denoising process requires multi-step iterative updates, resulting in a very slow inference speed compared to previous methods. in addition, the trained score network is highly correlated with the domain data, which makes it difficult to achieve good generalization across multiple domains at the same time."
974,"limitations while we show a clear benefit of data augmentation when the amount of available training data is limited, the performance gain seems to be lower when a larger quantity of manually transcribed speech data is available. whether data augmentation is always beneficial is an open question. we did not measure the effect of sociolinguistic variables on the performance of the models. a risk might be that especially for the models for gronings, which were developed on the basis of speech data from only a few speakers, results might be negatively affected by differences in language background (such as speaking a different variety of gronings, or being from a different social group). we likewise did not measure the effect of nonlinguistic variation (e.g., use of different micro- phones) on the performance of the models. while bartelds et al. (2022) showed that wav2vec 2.0 representations are relatively unaffected by nonlinguistic variation, we aim to further explore this in future work. finally, we evaluated the effect of training data size and data augmentation on four different minority languages or language variants, each using a single test set. of course, using a different test set might have affected the results. however, given that the pattern of results was similar across a range of language varieties we do not expect this difference to be large."
975,"limitations our scheduling strategy only re-arranges the training examples after each training epoch, limiting the flexibility of scheduling them compared with re-arranging the examples after each training step. therefore, the order of the training examples will still be fixed within each training epoch. besides, our method finds it challenging to transfer from the task of idiom usage recognition to that of metaphor detection. therefore, more advanced methods for learning the broad nature of non-compositionality, including those of idioms and those of metaphors are needed. we leave this to a future study."
976,"limitations the methodology and experimental approach presented in this paper have certain limitations concerning their practical application and the availability of language resources. the proposed method estimates uncertainty using monte carlo dropout with k iterations and subsequently performing adaptation j times. these additional computations result in increased inference time in real-world applications. empirical evidence suggests that larger values of j lead to a linear increase in time costs in practical scenarios. although the number of j on the wmt21 benchmark was limited in our experiments, the exact cost associated with achieving successful adaptation for new models or datasets remains uncertain. in terms of language resources, the majority of mt metric benchmarks still focus on the news domain, leaving a dearth of multidomain mqm benchmarks for conducting more meta-evaluation experiments during the preparation of this paper. to address these limitations, it is imperative to explore the performance of the proposed methodology on a wider range of out-ofdistribution benchmarks in the future. furthermore, as highlighted by the reviewer, it is also important to note that the proposed methodology does not consistently exhibit performance improvements on certain specific test sets. one possible explanation for this observation could be attributed to our investigation of the optimal learning rate using the wmt20 dataset. the divergence in scoring perspectives between the conventional wmt score and the mqm score might lead to discrepancies in improvement trends."
977,"limitations and broader impacts of diffusiondb in its data sheet (gebru et al., 2020) (‡ a)."
978,"limitations key point hierarchies may be valuable for summarizing opinions and views in multiple domains, including reviews, survey responses, customer feedback, political debates etc. however, in this work, we only demonstrated their value for business and product reviews, leaving other types of data to future work. also, we only attempted to create kphs for english reviews, for which an abundance of resources is available, including a huge number of written reviews and high-quality trained models, e.g. for nli and key point matching. applying these methods to low-resource languages is expected to be far more challenging. finally, the quality of the resulting kphs depends on the quality of the extracted key points provided as input, which may vary across different domains. to alleviate this problem in thinkp, we manually filtered out problematic key points from the dataset (§4.2)."
979,"limitations although this work aims to be as comprehensive as possible, there are several limitations to this paper. our comparisons only consider neural openie models despite rule-based methods being very popular among downstream applications. this is because of the lack of recent surveys on neural openie methods and the difficulties we personally encountered when trying to determine which openie method was state-of-the-art. we acknowledge that there are many cases where rule-based methods may be preferable to neural models due to being faster or more tailor-made for a specific application. however, we feel that focusing on neural openie methods is not a detriment because we are interested in which methods work best ""out of the box"". based on the results reported in these neural openie papers, we believe they are currently the best out-of-the-box openie models using the metrics we report in this paper on the test sets covered in this paper. the corpora we chose are all limited to english. as a result, our results are not generalizable to any downstream task that relies on different languages. in our experiments, we do not report results for the benchie test set or using the benchie metric. this is because the benchie test set uniquely can only be evaluated using the benchie metric, and the benchie metric can only be applied to the benchie test set. we do not feel that its exclusion hurts our final"
980,"limitations evaluation: we evaluate work as a single-label learning problem (accuracy) and a probability distribution (kl). these metrics do not fully capture the nuances of the crowd (inel et al., 2014). we hope to build on this work by moving beyond general population-level predictions to predictions on subpopulations of interest, such as vulnerable communities. we hope to develop better methods for evaluating and assessing the performance of population-level learning. the range of mixing (w =) of the language features and labels in our experiments could be further delved into. our experiments cover weights ranging from 0 to 100 in quartiles, but this parameter, as a hyperparameter, could benefit from additional experiments in finer ranges. datasets: our experimental datasets have been primarily in english. in addressing the ability to generalize, we hope to explore other offensive or hate speech-related datasets from other languages. the challenge of evaluating our models with other languages is acquiring a dataset with annotatorlevel labels, a rare resource for english datasets and challenging for other languages. finally, we hope our methods open the"
981,"limitations the proposed post-abstention methods require additional computation and storage. despite this additional requirement, we note that this is not a serious concern as current devices have high storage capacity and computation hardware. furthermore, additional computation for training auxiliary model in retop is required only once and just an inference is required at evaluation time which has a much lower computation cost. moreover, the risk mitigation that comes with the post-abstention methods weighs much more than the computational or storage overhead in terms of importance. secondly, human-intervention techniques require a human to be a participant and contribute in the answering process. however, these approaches do not expect the participating human to be an expert in the task. like other empirical research, it is difficult to exactly predict the magnitude of improvement a post-abstention method can bring. our idea of exploring sequential application of multiple postabstention methods addresses this concern and can be used based on the application requirements."
982,"limitations the limitations of our work include: 1) in our work, the structure of lyrics is the chorus and verse parts of songs, and it is learned in a data-driven manner, which highly relies on data quality. 2) the settings of the lyric-to-beat model will limit the effect of our model. for this work, we make an assumption that all songs are with 4/4 time signatures for the lyric-to-beat model. if the time signature is not 4/4, we need to re-train the lyricto-beat model.3) better ”beat construction” can be investigated, such as using a language model to generate the beat sequence. we only explore the simple method and achieve satisfying results. 4) the model trained from scratch may not achieve satisfying results. and a gpu with at least 20g memory may be needed to use the pre-trained language model (mt5) to reproduce our work."
983,"limitations the approach of identifying source domains relies on having a contextual sentence but also a target domain available. the datasets available for evaluation do not always provide precise target domains. for example, the lcc dataset provides the target domain gun ownership for the sentence i just don’t know what it will take for people in this country to embrace gun safety, or the target domain climate change for the sentence the event is billed as the largest meeting of influential figures within the renewable energy field. this mismatch often makes it difficult to provide precise source domains. a similar problem also exists when wanting to use our source domain prediction approach in the wild as we have to somehow provide the model with a target domain. while we can provide a target domain by selecting sentences based on seed-word lists designed for specific domains, we do not know how precisely this matches the target domain occurring in the sentence. in a multilingual setting, the issue becomes more pressing since there are very few multilingual metaphor datasets and for semiautomated approaches the seed-word lists would have to be provided for each language. another challenge is connected to the fact that the model output requires time-consuming manual evaluation to obtain a precise accuracy score. however, deciding what counts as a correct source domain can be difficult and might change depending on how strictly the annotators apply certain rules. for instance, whether an annotator sees a pre- dicted source domain as too general or too specific is a matter of degree. overall, this makes it hard to benchmark different approaches across papers, which is why further investigation of automated metrics, as presented in this paper, is crucial. lastly, there are issues regarding the accessibility of large neural language models, such as gpt-3, and the transparency of openai’s api as described in the"
984,"limitations our analysis is limited to non-autoregressive transformer-based models, fine-tuned with the same set of hyperparameters. hyperparameter optimization would undoubtedly lead to better performance for some models, but we fine-tuned each model with standard hyperparameter values for solving sentiment analysis tasks (deyoung et al., 2019) to reduce resource consumption. this should not affect the"
985,"limitations (1) we did not perform any comprehensive hyperparameter search, which would have further consolidated our results. this decision was made due to the high cost of training multiple models. (2) compared to current very large models, glot500-m is comparatively small. (3) although we have tried to minimize the amount of noise in our data, some noise is still present."
986,"limitations in this paper, we conduct experiments only on the chinese benchmark dataset due to the lack of english datasets and comparisons of related methods. moreover, the model is based on bertbase-chinese, so the maximum input length is constrained to less than 512. however, the numbers of words in some long documents exceed the limit, so we use a sliding window to deal with the problem. otherwise, some documents having too many clauses require large gpu resources after aligning and padding. limited by the memory capacity, we have to set a small batch size."
987,"limitations the pretrained bidirectional distillation transfers language knowledge through the nmt training process, a limitation of this method is that a computational overhead is introduced during training. specifically, there is an extra language model forward pass to generate the pretrained bidirectional distillation objectives. although we significantly reduce the computational overhead by designing a self-distilled language model, the overhead cannot be completely avoided. fortunately, most computations stem from back-propagation when model training, and the introduced computational overhead only affects training time. once the training is completed, the nmt has an identical inference cost as regular translation models."
988,"limitations lmrec is trained on user behavior text data that are collected from diverse service applications. these datasets are preprocessed to users’ behavior sequences as detailed in figure 1 and section 3.1. however, in order to improve the quality of user representations, choosing the item information differently for each application may improve the effectiveness. as such, we can consider domain-specific information for each service rather than using general item information. for example, we may leverage additional domain-specific information such as news topics or categories, names of the press agency, and keywords for the news content rather than using only news titles for the news dataset. this issue is a promising extension for practitioners to successfully apply lmrec to real-world applications. the types of task-agnostic data will largely affect the performance gains of lmrec+agnostic and lmrectl+agnostic. we fully utilize four types of taskagnostic data, i.e., search, e-comm., sns, and news, and achieve state-of-the-art results. however, this paper does not thoroughly explore their optimized combination or mixing ratio of the corpus due to the heavy computational costs, which most large lm studies suffer from. while prior work shows how the pretraining corpus sources and their combination affect diverse downstream tasks (raffel et al., 2020; gururangan et al., 2020; lee et al., 2020; krishna et al., 2022; shin et al., 2022), there still remain limitations in finding the generic relation between downstream performance and corpus properties; measuring the effect of the pretraining corpus on the downstream task is still underexplored. we point out that more careful study is left for future research. regarding reproducibility, it is difficult to open our in-house data due to legal issues caused by privacy and user agreement. therefore, we tried our best to validate the efficacy of our lmrec with the experiments on benchmark datasets in addition to in-house data."
989,"limitations our current freqmlm techniques tend to fail on lid predictions when the linguistic differences between languages are small. for example, english and spanish are quite close: (1) they are written in the same script, (2) english and spanish share a lot of common vocabulary. this can confound freqmlm. the strategy to select the best layer for drawing residual connections in resbert is quite tedious. for a 12-layer mbert, we train 10 resbert models with residual connections from some intermediate layer x ∈ {1, · · · , 10} and choose the best layer based on validation performance. this is quite computationally prohibitive. we are considering parameterizing the layer choice using gating functions so that it can be learned without having to resort to a tedious grid search. if the embedded language in a code-switched sentence has a very low occurrence, we will have very few switch-points. this might reduce the number of maskable tokens to a point where even masking all the maskable tokens will not satisfy the overall 15% masking requirement. however, we never faced this issue. in our experiments, we compensate by masking around 25%-35% of the maskable tokens (calculated based on the switch-points in the dataset)."
990,"limitations similar to other augmentation methods, dialogps demands high requirements for computing resources. the training is performed on up to 8 v100 gpus. on dailydialog: a vanilla transformer only needs 50 minutes while a non-pretrained dialogps takes about 80 minutes when k = 1. other baselines take about the same amount of time as dialogps k = 1. but when dialogps achieves its performance peak (k = 16), the training takes 4 hours. most of time cost comes from sampling which is difficult to be accelerated by gpus."
991,"limitations for representation consistency, we apply the regularization to all the tokens and do not distinguish between the different roles the tokens play. adaptive determination of which tokens or chunks require to be consistent in the representation space is an intriguing research question, which we leave as future work. more effective data sampling strategies can also be explored."
992,"limitations although our proposed nuwa-xl improves the quality of long video generation and accelerates the inference speed, there are still several limitations: first, due to the unavailability of open-domain long videos (such as movies, and tv shows), we only validate the effectiveness of nuwa-xl on public available cartoon flintstones. we are actively building an open-domain long video dataset and have achieved some phased results, we plan to extend nuwa-xl to open-domain in future work. second, direct training on long videos reduces the training-inference gap but poses a great challenge to data. third, although nuwa-xl can accelerate the inference speed, this part of the gain requires reasonable gpu resources to support parallel inference."
993,"limitations and future work we identify two limitations in our work that necessitate further investigation and improvement. first, only empirical results are presented in our work. a theoretical understanding of plms calibration is still lacking. going forward, we are motivated to investigate this problem from the standpoint of feature learning. we see great potential in unifying several problems in ai safety (houben et al., 2021) from a feature-learning perspective, including spurious correlations (gu et al., 2019; wang et al., 2022), robustness (yuan et al., 2021; zhang et al., 2022), backdoor learning (sheng et al., 2022; cui et al., 2022), and calibration (ulmer et al., 2022). second, we propose three simple extended calibration methods based on existing ones. in our experiments, we evaluate the calibration performance of existing and our calibration methods. we make an assumption that we have a large held-out validation set that can be employed as the training dataset for the calibration task. we demonstrate the effectiveness of learnable calibration methods in this ideal situation. however, in practice, we need to make the decision about how to allocate the data for the main task and the calibration task given limited training samples."
994,"limitations demonstration selection methods we assume that diversity can be obtained by choosing demonstrations with different program structures. this is based on previous work that demonstrated the importance of diversifying program structures in semantic parsing tasks (oren et al., 2021; bogin et al., 2022; gupta et al., 2022). we also try to diversify utterance words or program symbols but do not consider more complex utterance features that could be applied to a wider range of language understating tasks. we also assume that recall matters more than precision when designing cover-ls algorithm. that means we aim to choose a set of demonstrations that covers every predicted local structure in sỹtest , since it has the potential to be a correct one. we do not predict whether a specific structure should be covered. furthermore, our approach for increasing gold structure coverage by using additional beam candidates could be improved by employing search methods specifically targeted for diversity (meister et al., 2021; narayan et al., 2022). retrievers we used different retrievers for noft and ft setups based on the retriever that worked best on the development set. future research should be conducted to understand why different retrievers are preferred in different setups. a potential method could be to consider both input utterances and programs for retrieval, as suggested in zemlyanskiy et al. (2022)."
995,"limitations despite obtaining promising results, our proposed approach still has the following limitations. first, although our da2lm approach can generate a large amount of target-domain data with high diversity, the generated words are still limited by the source-domain labeled data and target-domain unlabeled data. how to make the model generate novel target-domain words is a challenging problem to explore in the future. second, our da2lm model is primarily proposed for the absa and ae tasks, which are not directly applicable for the other information extraction tasks with more than two elements, such as aspect sentiment triplet extraction (aste). therefore, cross-domain data augmentation for multiple-element information extraction tasks may be a promising followup direction."
996,"limitations our work on compo is subject to multiple limitations. the first limitation is around its scope when probing compositional operations. we only explored compositional substitution for topical snippets in conversations as an initial effort. however, there are many other types of conversation structures that can be leveraged such as conversation stages or specific discourse acts. second, we used a set of external tools to process the conversations for augmentation, such as the use of c99 for topic split and action extraction. although we choose to select widely-used tools with high precision, error cascades are inevitable. furthermore, our approach may not be applicable to low-resourced languages since these pre-processing tools may not be available even in the first place for these low-resourced contexts. we urge future work to further work on this line of compositional data augmentation without any dependencies on external software."
997,"limitations our approach achieves promising results in crossprompt aes by enhancing the consistency between source and target prompts. we believe that this idea can also be used to other cross-domain or domain adaptation tasks. in addition, as can be seen from table 1, our approach fails to perform well in some cases. we think that forcing the representations of two prompts to be closer during model training may result in more errors when the prompts’ grading rubrics, writing genres, and writing requirements are quite different. therefore, there are two possible directions can be explored for future research: 1) more fine-grained shared features can be extracted to improve scoring performance. 2) scoreaware information can be integrated into model to improve source and target prompts consistency."
998,"limitations we consider the current work has the following two limitations: • we design our lightweight ood detection framework based on the prefix-tuning paradigm. nevertheless, there may be other techniques to achieve this goal, which requires further exploration. • for pto + label, each label focuses on its own prefixes, suffering from prefix redundancy problem. one can design share prefixes across different labels to trigger label-invariant sentence features."
999,"limitations our work focuses on assessing recent english vlms on tasks which require fine-grained understanding. here, we outline limitations that we believe are important considerations for future work. first, we only examined a limited number of models. these include (i) strong coarse-grained models, such as albef, clip, flamingo and blip-2, and (ii) two strong fine-grained models, pevl and x-vlm, that build on albef. while we believe our selection of models is representative of strong components in pretrained vlms (such as dual-encoder and cross-modal interactions), we could not easily evaluate different approaches towards fine-grained understanding (e.g., yao et al., 2022a; li et al., 2022a) as the corresponding models and code are not open-source. we hence hope our study will motivate future work to report zeroshot performance on fine-grained benchmarks. second, we evaluate our models in a zero–shot setting using image–text matching. future work could consider how fine-grained understanding improves when fine-tuning for specific tasks. as opposed to relying on image–text matching scores, alternative methods like input ablations, visualising attention or activations could also be used to gain an understanding of potential failure modes. third, though we note specific areas where model performance fluctuates a lot during pretraining, we look forward to future research that improves performance for various such areas, like existence and counting. finally, some datasets we use are quite small. for example, winoground only has 1,600 data points. we hope that our analysis sheds light on the kinds of skills models struggle with and encourages more and larger datasets that test for these skills."
1000,"limitations our work has the following limitations. first, we only used one evaluation data, namely se23, because it is the only data suitable for the vwsd setting, especially for the oov examples. in addition, our methodology relies entirely on wordnet. therefore, this may be limited the model’s ability when the target word is a proper noun such as a named entity. finally, we depend on the results of gpt-3 definition generation to handle oov words. since the generated definitions may contain errors, as revealed in the qualitative analyses, the errors led to incorrect predictions. ethical consideration the generated definitions were annotated by two annotators. both annotators were fully paid by complying with local minimum wage regulation. in addition, in the sampled definition generations, the authors could not find any statements violating the acl anti-harassment policy. however, generated definitions that authors have not vetted are still at risk of containing toxic or hates contents (e.g. racism, insulting or xenophobic)."
1001,"limitations we identify the following limitations of our work. our current cos’s reranking expert only learns to rerank single-step results. thus it can not model the interaction between documents in case of multipassage evidence chains, which might lead to suboptimal performance, e.g., when we need to rerank the full evidence path for hotpotqa. at the same time, we hypothesize that the capacity of the small model used in our experiments is insufficient for modeling evidence chain reranking. we leave the exploration of learning a full path reranker for future work. also, our current pretraining setup only includes the three bi-encoder tasks, and thus we can not use the pretrained model out-of-box to solve tasks like end-to-end entity linking. consequently, the learned skills from self-supervision can not be chained together to perform configurable zero-shot retrieval. it would be interesting to also include the entity span proposal skill in the pretraining stage, which could unleash the full potential of the chain-of-skills inference for zero-shot scenarios."
1002,"limitation of elabor is lack of exploration beyond gpt-3. we consider investigating this problem as our future work. limitations given the ability of elabor to generate free-text elaborations for commonsense question answering, we still observe some cases where the modelgenerated elaborations are not factually correct, or irrelevant to the question, distracting the answer predictor towards incorrect answers. this reflects a limitation of elabor on the controllability of its generations, which is also commonly discovered when using language models for text generation. we consider this as a possible future direction which aims at verifying the factuality and relevancy of model-generated texts before incorporating them for final inference or as a controlling mechanism during generation."
1003,"limitation, ethical considerations, and social impacts of this paper are in appendix f and g."
1004,"limitations despite the insights obtained through our analysis, certain limitations persist, which we outline in this section. with respect to the re-parameterization of parameters as presented in eq. (3), we adopted the layer-wise setting as proposed by aghajanyan et al. (2021) in order to alleviate memory and computational burdens. nonetheless, such a setting restricts us to only identifying local subspaces, rather than discovering global subspaces within the entire parameter space of a pre-trained language model. the existence of a task-specific global subspace is yet to be ascertained. if such a subspace does exist, the correlation between this global subspace and the identified local subspaces needs to be explored in future research. in terms of experimental settings, the evaluation tasks are limited to natural language understanding tasks, with a lack of natural language generation tasks. on model architecture, decoder-only (e.g., gpt) and encoder-decoder (e.g., t5) models are not included. on model scale, we use basicsize models rather than large ones due to limited computational resources. consequently, the"
1005,"limitations we discuss three limitations of this work as follows. the first one is the instability of reinforcement learning. reward-driven policy learning is an essential advantage of this work because it is better equipped with the positive emotion-driven process of esc than existing works and can model flexible esc expression beyond the training data. however, this flexibility also suffers from instability, which calls for additional knowledge or strategies to refine the learning process. the second one is the need for further reference to psychological theory. an advantage of our work is to learn posterior esc patterns integrating the dialogue context and future feedback in the form of rewards. however, there is still other valuable prior knowledge to be referred from psychology studies, e.g., the cbt (cognitive-behavioral therapy) methods. this kind of prior knowledge can be used as additional knowledge to refine the learning process as mentioned in the first limitation. the third one is that the reward design can be further optimized. the ideal case is to construct a high-quality dataset with human-feedback labels for training reward model (e.g., the constructed example of chatgpt). at the same time, the larger parameter of the reward model, the more conducive it is to learn a robust policy and avoid it overfitting to the reward function. however, such optimizations need a trade-off with cost. ethical considerations in this paper, the esconv dataset used in our experiments is a publicly-available benchmark for emotional support conversation, which does not contain sensitive and personal information as well as unethical language. our work builds on this dataset to study positive emotion elicitation to improve the user’s mental state. therefore, we focus on constructing a dialogue system to provide emotional support from families and friends in the daily scenarios limited by this dataset rather than profes- sional psychological counseling or psychological treatment. for risky non-daily scenarios such as self-harm or suicide-related conversations, we do not claim that the dialogue system we built has a treatment or improvement effect on them. additionally, we also ensure the anonymity of our interactive human evaluation. we believe our work meets acl’s code of"
1006,"limitations as shown in table 2, our approach underperforms the state-of-the-art supervised model on the smd dataset, where the supervised sota labels a search instruction for each sample. in addition, the lemonpicked example in table 8 demonstrates that some- times it is challenging for the query generator to learn complicated expressions automatically. despite our model’s superiority over all unsupervised methods, these gaps reveal some improvement room of qkconv. in appendix d, we try to bridge the gaps by incorporating a few query annotations. another limitation is that our approach suffers from the time-consuming off-the-shelf knowledge selection when given a large dataset and knowledge base. it takes half of the training hours in knowledge selection since it involves heavy computation of retrieval from a large-scale knowledge base and reranking with a cross-encoder."
1007,"limitations our hyde method relies on real-time generation from llms and therefore may not be suitable for tasks that demand high throughput or low latency. however, over the years we have seen the cost of hardware decrease and model compression techniques advance, which may help improve the efficiency of llm inference. meanwhile, as we describe in the"
1008,"limitations mutation. we propose a simple but effective gradient-based mutation strategy. more complex mutation methods can be integrated into our framework to further improve attacking effectiveness. black-box attack. dgslow is based on a whitebox setting to craft samples with fewer query times, but it can be easily adapted to black-box scenarios by using a non-gradient search algorithm, e.g., define word saliency based on our fitness function and do greedy substitutions. adversarial defense. we do not consider defense methods in this work. some defense methods, e.g., adversarial training and input denoising, may be able to defend our proposed dgslow. note that our goal is to pose potential threats by adversarial attacks and reveal the vulnerability of dg models, thus motivating the research of model robustness."
1009,"limitations despite parla being intended for general-purpose linguistic rule learning, we only tested it on arabic and only to learn morphophonology rules. we also recognize the state of the data and the task being on out-of-context standalone tokens and not continuous utterances which is the nature of spoken languages. this is something we plan to investigate in the immediate future."
1010,"limitations imposed by selecting a single curriculum strategy, and instead, focuses on finding and analyzing different curricula that work equally-well for a given model and dataset. in addition, the discovered curricula provide insight into how different portions of the dataset contribute toward learning at different stages of training a model, which, in turn, provide knowledge about the learning dynamics of different models. the task of curriculum discovery could be costly on large datasets, in particular, when the goal is to find optimal curricula for different models and datasets. to mitigate the computational cost, we show that it is possible to rapidly discover a curriculum on a small subset of the dataset (or a smaller version of the model with significantly less number of parameters) and apply the resulting curriculum to the full dataset. there are several promising areas for future work. these include approaches for learning new difficulty indicators from data (e.g., linguistic difficulty including lexical, syntactic and semantic difficulty), prioritizing medium level instances and those with greatest progress during training, and developing challenge datasets that contain diverse data samples with different levels of difficulty. finally, investigating diverse curricula that are suitable for general use and across datasets through curriculum discovery and generalization is a promising area for research. limitations the present work investigates the use of two sample difficulty scoring functions, human-induced annotation entropy and model-induced loss, for nlp models and datasets. the former requires the availability of multiple annotations per sample and the latter requires training an auxiliary model to compute sample instantaneous loss during the course of training. our work does not provide a general solution to the choice or availability of good difficulty scoring functions. however, once such a function is available, our work presents solutions to the problem of finding high-performing curricula in curriculum space. our approach, although effective at finding such curricula, requires a bayesian search of its hyperparameters. we reduce these costs by finding curricula on smaller datasets and smaller models that can then be applied to corresponding larger datasets and models. finally, the proposed method lacks theoretical analysis of the dynamic interactions between data, downstream models, and discovered curricula."
1011,"limitation in comparison to other transfer learning methods of nmt, knn-tl incurs extra time costs and more processes to transfer knowledge from the parent model. this is a result of the requirement to construct a high-resource datastore utilizing large-scale parent data and retrieve it. on the other hand, knntl requires a substantial amount of storage capacity due to the storage of a datastore containing millions of entries. we employ the output representation layer for the alignment and the intermediate representation layer for the retrieval. this method justification is mainly supported by the results of model validation (table 4), which might deserve further investigation."
1012,"limitations common everyday things change over the years. while we try to choose ones that are in children’s vocabulary, over decades, devices evolve and humans change in which things they interact with more frequently, affecting which relationships would be more prominent in an average person’s mental model. so the parts mental models in such a dataset may not stay constant over time (e.g. some entities may be less familiar and certain relations may be less salient to annotators of the future). it would be interesting to use our parrot dataset as a point of comparison when studying mental models of everyday things in the future to reveal interesting insights on how humans’ mental models of everyday things evolve over time. other important future directions include to explore how more coherent mental models can help in complex reasoning tasks about everyday things, combine these parts mental models with mental models along other dimensions e.g. gu et al. (2022a,b), as well as using our dataset of commonsense queries about everyday things as a source of follow-up questions for existing qa tasks e.g., piqa (bisk et al., 2020) and csqa (talmor et al., 2019). this paper only focuses on relationships (spatial orientation, connectivity, and functional dependency) between parts of everyday things. however, our approach parrot-con is easily extensible to other applications such as: • spatial relations in other domains e.g. for geographical distances, we can similarly impose constraints on inverse relations like closer and further • temporal relations e.g. on a timeline, if event a occurred before event b, then event b cannot have occurred before event a (before is asymmetric) we leave the demonstration of the generalizability of our approach to future works."
1013,"limitations notes on key research challenges and decisions that affect the findings of this work. inclusion criteria • venue selection. our systematic review is restricted to papers from major machine learning venues. in order to download and search entire papers, we restrict our review to open-access venues only and exclude all closed-access research. • peer-review focus. we only review peer-reviewed papers, and exclude preprints, technical reports, and other informal articles from our review, even though rouge evaluation frequently occurs in these non-reviewed manuscripts. • archival publications. for completeness, we include all archival acl anthology papers including workshop papers. how- ever, due to technical limitations, we only include the main conference proceedings for non-acl venues. • post-publication changes. historical versions of papers and codebases may contain additional reproducibility information, but we only review current versions (as of january 1, 2023). • external materials. we only review main paper text, appendices, and code linked in papers. we do not review external materials such as websites, slides, videos, or codebases with no link appearing in papers. appendices and supplemental manuscripts distributed separately from the main paper manuscript are not included in our review. • underlying biases. the distribution of papers we review directly reflects the underlying authorship, identity, and content biases (e.g., geography, nationality, gender, language, affiliation, etc.) in papers accepted to machine learning venues. paper annotation • automated annotation. our first paper annotation stage uses automated regular expression pattern matching of paper text. although these patterns are validated and refined through a human-in-the-loop development process, automated pattern matching cannot entirely replace expert human judgement and may incorrectly annotate papers. automated patterns cannot match text in bitmap image figures and tables due to limitations in pdf text extraction. • human annotation. we use a second stage of manual paper review for all papers to identify and correct annotation errors introduced by automated pattern matching. manual review sometimes involves human inference and judgement in challenging cases. (for example, papers that cite “rouge-1.5.5” sometimes use a nonstandard rouge-1.5.5 wrapper instead.) • preliminary search. we perform a preliminary case-insensitive search for “rouge” in all papers. matching papers receive full automated annotation, manual review, and codebase review. however, we are aware of several papers that compute and report rouge scores without specifically naming the metric. they are labeled as non-rouge papers and receive no manual review. • non-english annotation. most reviewed papers are written in english. due to human annotator language limitations and english-oriented automated pattern matching, non-english papers may receive less accurate labels than english papers. • author clarification. contacting authors for clarification may help resolve paper reproducibility questions (for example, see: errington et al., 2021). however, evaluating this aspect of reproducibility is infeasible at the scale of our work. • non-evaluation metrics. some papers use rouge for reasons other than evaluation, such as feature generation or for internal training validation. we do not make any distinction between evaluation and non-evaluation rouge during our review. • assumed correctness. our annotation protocol assumes all papers that use rouge-1.5.5 directly (rather than using a wrapper or reimplementation) report correct rouge scores. however, many of these papers may run rouge-1.5.5 via custom ad hoc wrapper code that (like many wrapper packages) is implemented incorrectly and introduces scoring errors. codebase annotation • codebase linking. we use the papers with code dataset to link papers with codebases. however, this dataset does not cover all papers in our review, which limits our ability to assess their codebase reproducibility. • package inference. many codebases are missing explicit dependency specification, making identifying exact rouge pack- ages challenging. in these cases, function signatures are used to identify the most likely rouge package. • vendored dependencies. in some codebases, rouge package code is “vendored” (copied and pasted into the project code). it is more challenging to accurately identify the source of vendored rouge packages, particularly if the code has been modified. • package aliasing. codebases frequently import very similar versions of rouge packages distributed under different names (examples: ms/rouge and gl/rougescore). we attempt to resolve these packages to a single canonical package for our evaluation. however, slight differences may exist between package aliases that affect our correctness assessment. • multiple packages. when a codebase contain multiple rouge packages, we attempt to identify which packages are used to compute rouge scores reported in the paper. if this is unclear, we list all rouge packages used in the codebase. evaluation experiments • specimen task/model. we choose a single specimen task (cnn / daily mail) and model (lead-3) for measuring rouge scoring discrepancies due to configurations and packages. scoring discrepancies differ for other tasks and models. • summarization focus. although rouge evaluation is used for many different tasks and datasets, our experiments only focus on a single popular task (single-document summarization) and dataset (cnn / daily mail). • english evaluation. rouge was designed for english language evaluation and we perform experiments on the english language cnn / daily mail dataset. while there are rouge packages designed for other languages, there is no universal standard for them like rouge-1.5.5. therefore, we do not cover non-english rouge evaluation in our experiments. • score variants. we only examine three common rouge score variants (rouge-1, rouge-2, rouge-l). we exclude uncommon variants (e.g., rouge-w, rouge-s, rouge-su) rare in papers and often unimplemented in packages. • multiple references. we do not perform any experiments involving multiple reference evaluation, which is not supported by our specimen task (cnn / daily mail) and is not implemented in many nonstandard rouge packages. • bootstrap sampling. bootstrapping is built into rouge-1.5.5 and is often unimplemented or incorrectly implemented in reimplementations. our package experiments operate on individual model outputs and cannot detect bootstrapping errors. • custom implementations. our code review identified several instances of custom rouge implementations, but because we only evaluate packages used by more than one author, it is unknown how correct these custom implementations are. • package versions. many nonstandard rouge implementations change over time (for example: section 5.3). package changes likely affect comparability between papers. however, our evaluation only considers the most recent version of each package (as of january 1, 2023) and does not study these between-version scoring differences."
1014,"limitations perception and reasoning in text-based raven. in this work, one limitation is that we do not attempt to solve the perception problem of analogymaking in rpm, rather we apply perfect perception in solving the reasoning part, and assume the perception problem is simple. by doing so, we find that plms may be a strong solution to the reasoning problem here, which may better direct future efforts toward ai and analogy. obviously, the perception problem for idealized domains is a lot different than more natural domains, and identifying key features across many domains that can facilitate a mapping is still a challenging unsolved problem. we hope that our work sparks more interest in this problem. meanwhile, one may argue that our decomposition abstractions are too strong, and actually contribute to the reasoning problem in rpm, as they make an independence assumption about which features of the task can be teased apart. making such an assumption requires an understanding of the problem that cannot be inferred by only seeing one instance. however, we decomposed the task based on very intuitive and common attributes, e.g., shapes, colors, sizes, and counts of items. we believe that the strength of such an abstraction, which could be applied in many problems, should not be understated. nonetheless, we include decomposition-free forms of results as much as possible throughout the paper to help compare the contributions of decomposition versus naming abstractions, which is more clearly only providing perceptual information. in fact, we find that without any decomposition, plms still achieve very strong performance in many cases, and performance gains from decomposition are not always large. human performance. lastly, we note some limitations in the human performance measurements used as reference points. in zhang et al. (2019a), human performance on raven was measured by giving subjects some task-specific training, then evaluating them on the original visual form of the task. this differs from our results in two ways. first, plms had no task-specific training for raven, given that experiments were zero-shot and the text data we generate is new and thus impossible to appear directly in plm pre-training. this may give humans an advantage. second, the task is presented to plms in text form, not visually. while the essential information from the task is preserved by our conversion, it is possible that this conversion would affect the difficulty of the task for humans (making it easier or harder). as such, it becomes unclear how to contextualize our results with these past human results. future work may carry out systematic human studies to compare the analogical reasoning capabilities of humans and plms in different settings. ethical considerations this work does not use any human subjects or human-generated data. our work deals with abstract visual features that are described with numerical symbols, thus not strongly targeting any language. a possible ethical concern for this work is the amount of computational resources used in evaluating plms. to reduce unnecessary computation in our study, we chose to apply plms to only a subset of 500 testing examples from each sub-task of the raven dataset, while the full testing set is four times as large."
1015,"limitations while our work tries to focus around reasoning over both fine- and coarse-grained cross-document relationships, qamden, the resulted pre-trained model, might still suffer from factual consistency errors while generating information given a query, and there is no guarantee that it will always generate factual and reasonable content without any further fine-tuning. the qasem question generation model that we used may also have been a source of these problems. there is a possibility that qasem produces inadequate questions that could harm the pre-training process of the model. an attempt was made to filter out noise using a question model, but the results were inferior to non-filtering. consequently, if the model is not fine-tuned, inconsistency (hallucinations) may occur more frequently. in addition, by using the newshead corpus as the pre-training data source, we assume that it is comprised of high quality documents. we also take into account the fact that newshead is limited to documents in the news domain, while some of the benchmarks used for evaluating qamden include another topics of interest. future work may further assess the quality of the documents, such as checking for duplications or wrong statements, and diversify the corpus domains. this is crucial for productizing models like qamden in interactive multi-text applications (chatbots) and semantic search applications which are gaining attraction nowadays (hirsch et al., 2021; eirew et al., 2022). finally, the resulted model qamden was pretrained on sets of related documents, by answering questions that matched their content. as in an out-of-domain scenario, qamden’s use over sets of documents that are not related, or over single documents, might be unexpected. such settings may be the subject of another research direction in the future."
1016,"limitations although lgtm has demonstrated superior performance in task-specific knowledge distillation, it is worth investigating the potential benefits of combining lgtm with pre-training knowledge distillation (jiao et al., 2020; wang et al., 2020). additionally, while our experiments have been limited to text classification tasks, which are relatively simple for current pre-trained language models, future work should explore the application of lgtm to more complex text generation tasks."
1017,"limitations one limitation of our dataset, elqa, is that the corpus only contains questions in english and about english. however, stack exchange has sites with questions about other languages and our main data extraction scripts are general enough that they can be used to create corpora for other sites on stack exchange. of course, language-specific processing steps, quality assurance and analysis must be applied before releasing such data. most importantly, the models we have presented here are intended only as baselines for future research, not for deployment. potential biases reflecting the demographics of authors represented in the training data (in terms of native language, level of english proficiency, etc.) also need to be considered if models are deployed for different target populations. moreover, many of these types of questions are found on the web, and a lot of the same topics are brought up by many users, so a model’s ability to generate correct answers cannot necessarily be attributed to abstract reasoning."
1018,"limitation by collecting an addendum to big-c using images taken locally in zambia. in addition, we plan to further expand to other zambian languages such as tonga, tumbuka, chewa, or lozi, by translating the existing dataset (creating an n-way parallel corpus for zambian languages) and by direct data collection. further down the roan we plan to study the dialectal varieties of bemba and the other languages, by collecting contrastive datasets from different regions of the country. 7note that zambia is a land-locked country. limitations we observe the following limitations with the dataset: • language diversity: in terms of number of languages, the presented dataset only covers two languages; bemba and english. • image diversity all the images used in this dataset were obtained from flickr30k image dataset. therefore, in terms image composition, our dataset is limited to the image diversity in the flickr30k dataset. it mostly lacks images that could be considered as ""culturally relevant"" ones for the zambian or generally sub-saharan african context. we plan to mitigate this in future work."
1019,"limitations our approach builds on a task schema that characterizes a task-oriented dialogue system’s domain. for example, the schema captures various attributes of the task. for some domains, when a schema is not pre-defined, it first needs to be extracted, e.g., from a corpus of dialogues. in this paper, we used bert as our lm to be comparable with related work, but more advanced models could further improve the performance. a limitation of our task attribute importance scoring method is that it currently produces a static set of weights, reflecting the domain. in the future, the importance weights may be personalized to the current user’s needs instead."
1020,"limitations despite its robustness, our method has subpar results on the automatic semantic metrics compared to the most recent work. this may be a natural consequence of the perceptibility vs. robustness trade-off (tao et al., 2014; de vleeschouwer et al., 2002): a stronger watermark tends to interfere with the original content. nonetheless, by using some technical tricks (e.g. neural infill model, nli-sorted ordering) our method is able to be superior to all the other methods including two traditional ones and a neural network-based method. techniques from adversarial attack were employed to simulate possible corruptions in our work. however, these automatic attacks does not always lead to imperceptible modifications of the original texts (morris et al., 2020a). thus, the corruptions used in our work may be a rough estimate of what true adversaries might do to evade watermarking. in addition, our method is not tested against paraphrasing, which may substantially change the syntactic component of the text. one realistic reason that deterred us from experimenting on paraphrasebased attacks was their lack of controllability compared to other attacks that have fine-grained control over the number of corrupted words. likewise, for text resources like novels that value subtle nuances, the aforementioned property may discourage the adversary from using it to destroy watermarking."
1021,"limitations our work still has some limitations: 1) due to the lack of research about the bias mitigation of qe, there is only one directly related work in this area, which serves as the main baseline in our experiments. since the bias of qe is a conspicuous problem, we hope there will be more related work in the future. 2) although our experiments are on wmt qe datasets, we do not implement the complicated data augmentation or model ensemble techniques as described in specia et al. (2021) and zerva et al. (2022), therefore our results can not compete with the best results of the wmt qe evaluation tasks. 3) also, our method requires reference as the positive sample. although most qe data includes reference, there are still chances that the qe data is annotated without the absence of reference, and our method would be hard to apply to such cases."
1022,"limitations first, limited by the category of video multimodal fusion tasks, we do not perform experiments on more tasks to better validate the effectiveness of our method, and we hope to extend our model to more various and complete benchmarks in future work. secondly, as shown in section 4.3, our model achieves relatively slight performance improvement on sentiment analysis task. for reasons, our model may be dependent on the scale of datasets to learn noise and redundancy patterns in video, which needs to be further improved and studied."
1023,"limitations one limitation of simlm is that it can not be used as a zero-shot dense retriever, since the pre-training framework does not have any contrastive objective. fine-tuning on labeled data is necessary to get a high-quality model. on the other hand, although simlm pre-training is quite efficient thanks to the replaced language modeling objective, it still requires extra computational resources to train the model. ethical considerations if the retrieval corpus contains some offensive or biased texts, they could be exposed to users under certain queries through our dense retriever. to deal with such risks, we need to introduce toxic text classifiers or manual inspection to exclude such texts from the corpus."
1024,"limitations we train a ufet model and then fine-tune it for target fet tasks. in our approach, the ufet training data is the main source of limitations. first, the large size ufet training data are automatically generated, and thus may contain errors. such errors can propagate to the fine-tuned fet models. another problem is that, for some entity types, there are not many training examples. moreover, some types useful in specific domains (e.g., adverse drug reaction for the biomedical domain) are not included in the ufet type vocabulary at all. as a result, the ufet model will not be as helpful when applied to fet data that contain such types."
1025,"limitations one major shortcoming of feag method is the dependency on creation of counterfactual inputs. if there is an error in counterfactual generation, we might get a wrong feature effect estimate. thus, for simplicity, our evaluation considered tokens as features. the parallel development of counterfactual input generation methods (wu et al., 2021; howard et al., 2022) would hopefully ease this issue and allow feag to be used reliably for spurious correlations on more complex features too."
1026,"limitation extension to multi-sentence tasks. our experiments are limited to single-sentence tasks, as we only retrieve single-sentence nearest neighbors to a test input. multi-sentence tasks such as natural language inference would require constructing pseudo-demonstrations that consists of multiple sentences, which we leave for future work. beyond classification. our experiments are limited to classification. extensions to multi-choice tasks or generation tasks requires going beyond a fixed set of options shared between inputs in the demonstrations and the test input. we leave extensions to non-classification tasks for future work. better construction of pseudo-demonstrations. we think future work can explore better constructing the pseudo-demonstrations. for instance, this paper uses manually chosen synonym labels (see appendix b for more detail). we hypothesize that better pseudo-demonstrations can improve performance, which we leave for future work."
1027,"limitations in this paper, we build the optimal translation policy under all latency by simply setting the search interval, achieving high performance. however, we think that the performance of our method can be further improved by exploring more interval settings. additionally, although we train the agent using a simple architecture and achieve good performance, there exists a performance gap between the learned policy and the searched optimal policy under low latency. exploring more powerful models of the agent may help improve the performance and we leave it for future work."
1028,"limitations our monotonic kd approach requires searching for a hyper-parameter k to strike a balance between monotonicity and translation quality for generating pseudo-targets. the current process requires substantial computational resources to determine the optimal value, which may be different depending on the dataset. more studies are needed to establish an efficient method."
1029,"limitation of sequence length on a single device. we have shown that sequence parallelism can handle longer sequence and is more memory-efficient than sota. in particular, sequence parallelism achieves 3.0× maximum sequence length and 13.7× maximum batch size than tensor parallelism when scaling up to 64 gpus. unlike both tensor and pipeline parallelism, sequence parallelism is not limited by the smaller hyper-parameters (e.g., number of attention heads, number of layers). therefore, our sequence parallelism can be adapted as long as the sequence length is divisible by sequence parallel size. with efficient attention, sequence parallelism can handle sequence with over 114k tokens, which is over 27× longer than existing efficient attention works holding the whole sequence on a single device. we used a language model (i.e., bert) to evaluate our system, but it can also be adapted to vision tasks. this work paves the way to process large images (hou et al., 2019) by vit (dosovitskiy et al., 2020) as a larger image means more patches or longer sequences. limitations in order to perform communication between subsequences during training, the use of sequence parallelism can result in increased communication costs, which in turn can slow down the training process. however, by combining sequence parallelism with pipeline parallelism, this issue can be alleviated and the communication cost can be made comparable to advanced forms of model parallelism such as tensor parallelism. nonetheless, sequence parallelism still incurs higher communication costs than vanilla data parallelism. while sequence parallelism is effective for training of unidirectional attention models as well as training and inference of bidirectional attention models, it poses a challenge for unidirectional at- tention models inference due to the autoregressive decoding process. this means that different devices cannot compute in parallel, resulting in reduced throughput and decreased gpu utilization."
1030,"limitations there are two limitations of the current must model. first, although pre-trained language models can potentially boost the performance in web information extraction, pre-train a must on web documents has its unique challenges. there are several possibilities for our future exploration. for example, we plan to pretrain a must model by incorporating html-specific tasks, such as masking dom nodes and predicting the relations between dom nodes. second, our model focuses on web pages with single-object, where each target field only has exactly one answer. for a multi-object page, e.g. a movie listing page, there are different movie names corresponding to different movies on the page. however, methods like repeated patterns (adelfio and samet, 2013) can be applied."
1031,"limitations due to the limitation of computational resources, we have not evaluated the flan-t5xxl whose number of parameters is 11b, and the opt whose number of parameters is greater than 1.3b. since opt and gpt-neo perform poorly in the zero-shot setting and separating attention scores of each document in the input is tedious for decoderonly models, we choose not to use them as source lms. however, we prove that taking the encoderdecoder model flan-t5base as our source lm is also robust to augment decoder-only models. we will explore new methods to annotate lm-preferred documents of decoder-only models based on their inherent signals."
1032,"limitations in the case of complextable, where table images are generated using an auto html table creator that utilizes a web browser engine for rendering, applying tablevlm directly to recognize the structure of handwritten tables without fine-tuning poses a challenge. this is particularly evident when dealing with handwritten tables found in ancient documents. moreover, the process of annotating the structural information of tables in handwritten documents is both time-consuming and laborious. as a result, there is ample room for further exploration and improvement in enhancing the accuracy of table structure recognition for handwritten tables."
1033,"limitations our work has some limitations. the design of the co-attention in our method can be improved. currently the design of co-attention in our method is limited to four types, which affects its adaptability. in addition, due to the fact that there is only one publicly available dataset in multimodal sarcasm detection, we conduct our experiments based on it. this has limited the evaluation of the generalization of our method."
1034,"limitations we point to several limitations of our work. first, our work considers a popular family of models referred to as “dense retrievers”, but other approaches for retrieval include sparse retrievers (robertson and zaragoza, 2009; bai et al., 2020; formal et al., 2021), generative retrievers (tay et al., 2022; bevilacqua et al., 2022), late-interaction models (khattab and zaharia, 2020), inter alia. while our work draws interesting connections between dense and sparse retrieval, our main focus is on understanding and improving dense models. second, all three dense models we analyze are bidirectional and were trained in a contrastive fashion. while most dense retrievers indeed satisfy these properties, there are works that suggested other approaches, both in terms of other architectures (muennighoff, 2022; neelakantan et al., 2022; ni et al., 2022) and other training frameworks (lewis et al., 2020; izacard et al., 2022b). last, while our work introduces new ways to interpret and analyze dense retrieval models, we believe our work is the tip of the iceberg, and there is still much work to be done in order to gain a full understanding of these models."
1035,"limitations present in them. however, the"
1036,"limitations our getmtl needs to compute the gi for each task i at each iteration and requires a backwardpropagation procedure over the model parameters. every iteration requires one forward-propagation followed by t backward-propagation procedure and computation of backward-propagation is typically more expensive than the forward-propagation. here, we define the time of one forward pass and one backward pass as ef and eb, respectively. the time of optimization process is defined as eo. therefore, the total time e of getmtl is defined, e = ef + teb + eo ≈ teb + eo for few-task learning scenario (t < 100), usually eo eb and getmtl still works fine. however, for large-scale task set (like t 100), usually eo eb or eo teb. consequently, our getmtl may get stuck in the optimization and backward-propagation process at each iteration. therefore, the major limitation of our work is that it can not be applied to scenarios with large-scale task sets."
1037,"limitations in this work, we take the sufficient advantages of the external semantic and syntactic structure knowledge to improve our focused problem. but this could be a double-edged sword to use such features. specifically, our paper has the following two potential limitations. first of all, our method closely relies on the availability of the resources of scene graph structures and syntax structures. while most of the languages come with these structure annotations to train good-performing structure parsers (for example, the syntax structure annotations of penn treebank cover most of the existing languages), some minor languages may not have structure resources. that being said, our idea still works well even in the absence of the targetside structure annotations. with only the structure annotations at pivot-side (resource-rich) language (in this case, the cross-modal semantic&syntactic structure aligning learning are canceled), we can still achieve much better performances than those baselines without using the structural features. besides, our method will be subject to the quality of the external structure parsers. when the parsed structures of scene graphs and syntax trees are with much noise, the helpfulness of our methods will be hurt. fortunately, the existing external semantic and syntactic structure parsers have already achieved satisfactory performances, which can meet our demands."
1038,"limitations although dupmae is to learn representation instead of generative models, it performs pre-training on open web data. therefore, it is also subject to potential ethical and social risks, like bias, discrimination, and toxicity. besides, dupmae is pre-trained with comparatively limited amount of data due to the constraint on computation resources. despite that it already achieves a promising retrieval performance at present, it remains to explore whether the performance can be further improved with the scaling up of pre-training data, by leveraging more high-quality datasets like c4 and openwebtext."
1039,"limitations decompx is an explanation method for decomposing output tokens based on input tokens of a transformer model. although the theory is applicable to other use cases, since our work is focused on english text classification tasks, extra care and evaluation experiments may be required to be used safely in other languages and settings. due to limited resources, evaluation of large language models such as gpt-2 (radford et al., 2019) and t5 (raffel et al., 2022) was not viable."
1040,"limitations several limitations of our study include: 1. only english-language chain-of-thoughts/tasks considered; 2. reliance on gpt-3, which is a closed-source product with an unknown training set (which could itself include some explanations); and 3. focusing only on a single type of student model, opt. more broadly, learning from and with explanations carries some specific risks related to automation bias. while a model might rationalize its predictions using a seemingly coherent string of natural language steps, even if it eventually gets the prediction correct, there’s no guarantee that the eventually predicted output actually results from a process represented by the rationalization. a user might assign excessive confidence to that system based on the chain-of-thought. we observed many cases where the chain of thought seemed promising only to result in models ultimately making incorrect predictions in the final few tokens. caution should be taken when displaying chain-of-thoughts to users."
1041,"limitations that increase the difficulty of developing largescale models for generating or ranking clarification questions. it remains challenging to collect and build large amounts of data. in the near future, researchers should optimize the process of acqs based on the current retrieval technologies (see (trippas et al., 2018) for a description of collecting such datasets). (4) multi-modal acqs dataset. recently multi-modal conversational information seeking has received attention in conversational systems (deldjoo et al., 2021). amazon alexa4 organised the first conversational system challenge to incorporate multi-modal (voice and vision) customer experience. however, there is a lack of existing datasets containing multi-modal information for acqs."
1042,"limitations in this section, we discuss limitations of rmlm with integrity and attempt to provide valuable directions to further improve our method. there are some potential limitations as follows: 1) rmlm does not perform well on the sst-2 dataset, indicating it may not be applicable to phrase-level datasets with data scarcity. and in some extreme cases of short text, rmlm may often give incorrect predictions. we recommend doing more mlm pre-training using our wordlevel transformation if resources are available. 2) the mitigation is mainly contributed by the transformation and the bert defender. however, there is a lack of exploration of different types of them in this paper. it is worth exploring different transformation schemes (e.g., span masking) and a lightweight model (e.g., albert (lan et al., 2020)) as a defender to reduce the computation overhead. 3) the adopted evaluation is for testing the performance of defense against word-level adversarial attacks. rmlm may expose flaws in mitigating character-level or sentence-level attacks. the applicability of the proposed approach needs more investigation."
1043,"limitations inference speed at the same model size, the latencies of grain on different tasks are relatively large compared to the methods like cofi and tinybert. this is because grain generates models with different head size, and the computation of these heads are not parallelized. thus the resulting models are slower than the models with uniform attention structures. this problem could be relieved by introducing model structure regularization at a higher level or by some engineering techniques, such as merging heads with the same or similar size into a large matrix to increase parallelism. backbone models grain is designed for transformer-based models. although the transformer is one of the most popular building blocks of nlp models, there are many other promising structures. the effectiveness of grain on model compression is possibly correlated with hardware lottery or software lottery (hooker, 2020). in addition, we have only tested our method with the standard multi-head attention mechanism. transplanting grain to other attention mechanisms is possible, but the effectiveness has yet to be tested."
1044,"limitations of our work have been mentioned in section 6. here, we propose some attempts to overcome these limitations. control signals. in the acquisition of control signals, there are two main constraints for performance, including (1) the accuracy of control signals and (2) the suitability of retrieval results in the testing step. with regard to (1), the results of the oracle setting demonstrate that our framework has a high ceiling when ground-true control signals are given. therefore, we have tried to enhance robustness by noising the control factors. noising methods contain adding, removing, and replacing random control tokens. however, experimental results show that noising methods compromise the success rate of control, which is contrary to the motivation of this work. in the future, this approach can be tried to further improve language quality in scenarios where the demand for controllability is weak. with respect to (2), we focus on the performance of the retrieval model in the inference stage. the control signals straightforwardly come from the retrieved responses. in this paper, we have proposed a task-specific design that combines semantic and emotional similarity to retrieve but it is still simple compared to those sota dialogue response selection models. in future work, it is meaningful to replace our retrieval model with more powerful response selection methods. as an advantage of diffusemp, both the annotating taggers and the retrieval model are orthogonal to empathetic response generation. it is easy for followers to employ higher-performance response selection models and attribute annotating taggers to empower the diffusemp. diffusion models. finally, the diffusion model requires a lot of gpu computational resources and is slow when inference, which limits its application. there are many attempts to reduce the computational resources (rombach et al., 2021a) required by the diffusion model as well as to speed up the process (vahdat et al., 2021) and inference (song et al., 2021; bao et al., 2022). theoretically, the relevant improvements would also have an enhancing effect on our framework and would be helpful for spreading the diffusion model to the nlp community."
1045,"limitations our method shows impressive performance but relies entirely on beam search during inference. however, it is well known that beam search is a computationally expensive algorithm. with the beam size of 50, the latency increases from 3.6 times (t5/seq-full) to 7 times (t5/seq) compared to greedy decoding. in addition, the re-ranking process causes another latency (about 12ms in our experiments). therefore, it may not be suitable for real-world dst scenarios. we leave this issue for future work. potential directions may include reducing the current two-step pipeline to an efficient one-step process by employing a novel objective function, using data augmentation, or changing the sequential decoding process to a nonautoregressive approach that can be applied in a parallel manner."
1046,"limitations during deployment. our approach has achieved impressive results on multiple natural language processing tasks, including the glge benchmark and three machine translation datasets. furthermore, we have observed that the issue of length prediction consistently limits the performance of the model, especially when dealing with raw datasets. the model struggles to accurately determine the length of the target data, which somewhat affects the model evaluation. in our future work, we will prioritize addressing the challenge of length prediction, aiming to make it more convenient and applicable to a wider range of tasks and scenarios."
1047,"limitations since the appearance of large pre-trained models such as gpt-3 (brown et al., 2020), there has been a wave of using large models without fine-tuning to do in-context learning directly to complete various nlp tasks, or to freeze the parameters of large models and then only optimize task-oriented parameters. the proposed hierverb is a lightweight method especially suitable for the case of insufficient labeled training data, but it is difficult to directly extend to a large-scale language model (i.e, >=175b) because large language models are hard to fine-tune in many situations. in future work, we plan to study our method on a larger scale language model in which only parts of parameters specific to downstream htc tasks need to be learned and further, extend our model to the zero-shot learning scenario."
1048,"limitations although we show that our sov-mas outperforms the vg-mt5 model under different setups, there are some limitations worth considering to study in future work: (1) in this study, we only provide 44 languages and conduct experiments on them, and future work could extend our method to more languages; (2) the used mas model is based on the generative pre-trained language model, i.e., mt5 (xue et al., 2021). the large-scale model size can bring promising performance while it also consumes more training time (all mt5-based models in this work cost about five days under the multilingual training setting) and releases more carbon dioxide, which may be inconsistent with the theme of green ai. therefore, the work related to model compression (e.g., knowledge distillation) may be possibly future work for the multilingual mas task."
1049,"limitations this work presents cats, a large-scale and highquality chinese answer-to-sequence dataset. it is a free and open dataset. one of most important motivations for presenting this dataset is that most of the existing datasets are built for english, which leads to advanced work on d2t generation primarily focusing on english and leaving other languages underexplored. however, cats only alleviates the dataset language bias rather than solving it. and it is limited to the study of chinese methods. regarding methodology, the proposed ugt converts the answer-to-sequence task to a graph-to-text problem to bridge the gap between two heterogeneous input data (sql and table). however, ugt works only for answer-to-sequence task rather than graph-totext task. additionally, though the proposed nse can help the graph-to-text model better preserve the original structural information, the contribution may be limited to the graph-to-text task."
1050,"limitations the purpose of our work is to evaluate the ontological knowledge of plms. however, a sea of classes and properties exist in the real world and we only cover a selective part of them. consequently, the scope of our dataset for the experimental analysis is limited. the findings from our experiments demonstrate an imperfect knowledge and understanding obtained by the models, indicating a tangible room for enhancement in both ontological knowledge memorization and understanding and a need for a better ability to address paraphrasing. these observations lead us to contemplate refining the existing pretraining methods to help language models achieve better performance in related tasks."
1051,"limitation (i) training computation overheads: although having the same inference complexity as any other two-stage retrieval-based dialogue system, our approach requires more computation resources during training as it needs to optimize the two modules in the meantime. (ii) static negatives: we train both modules with a fixed number of random negative samples for a fair comparison with baselines. actually, more effective negatives can be dynamically sampled by the fast retriever to the smart reranker to further improve its performance. ethical statement our paper primarily aims to enhance the training method for constructing retrieval-based dialogue systems that exhibit improved effectiveness. the training corpora we utilize, such as the ubuntu corpus and the response selection track of the dialog system technology challenge, are openly accessible and do not give rise to any privacy concerns. furthermore, the algorithm we propose is designed to be free from ethical or social bias, ensuring fairness and unbiased performance."
1052,"limitations data in this work is limited to the english diachronic word usage graphs (dwugs). our methods themselves are language-agnostic and we do not anticipate serious problems with adapting them to dwugs in other languages (which already exist). at the same time, although flan-t5 is a multilingual lm, we did not thoroughly evaluate its ability to generate definitions in languages other than english. again, definition datasets in other languages do exist and technically it is trivial to fine-tune flan-t5 on some or all of them. generated definitions and mappings between definitions and word senses can contain all sorts of biases and stereotypes, stemming from the underlying language model. filtering inappropriate character strings from the definitions can only help as much, and further research is needed to estimate possible threats. in our experiments with flan-t5, the aim was to investigate the principal possibility of using this lm for definition modelling. although we did evaluate several different flan-t5 variants, we leave it for the future work to investigate the impact of model size and other experimental variables (such as decoding algorithms). the cases shown in §7 are hand-picked examples, demonstrating the potential of using generated definitions for explainable semantic change detection and improving lscd datasets. in the future, we plan to conduct a more rigorous evaluation of different ways to build sense dynamics map."
1053,"limitations both the feedback simulator and the feedback evaluator in our work can be further improved. for example, while we simply fine-tuned a pre-trained t5 model as the feedback simulator, future work can design more specialized architectures for it, such as adding relation-aware attention (wang et al., 2020; elgohary et al., 2021) to augment the schema item linking among input components (e.g., question and template feedback in the tqes variant). alternatively, one can also leverage the feedback evaluator to steer the training of the feedback simulator (e.g., via reinforcement learning). as we briefly discussed, one could also extend our feedback simulator to imitate more fine-grained user behaviors, such as the agenda of how users would engage in the error correction process. finally, an intriguing research direction is whether one can leverage our feedback simulator for continually improving a semantic parser from nl feedback, drawing inspirations from clarke et al. (2010); iyer et al. (2017); yao et al. (2020). although our proposed approaches have not made any assumptions on the type of logical forms and can thus be applied to any of them, in experiments, we have only evaluated them in the task of text-to-sql semantic parsing. future research can further assess our proposed models in other semantic parsing settings such as knowledge base question answering (cai and yates, 2013; yih et al., 2016; gu et al., 2021; mo et al., 2022). on the other hand, as our simulator is primarily designed for interactive semantic parsing, it as- sumes meaning representations of both the groundtruth prediction and the model prediction. therefore, generalizing our methods to other nlp tasks may need additional effort. for example, if we apply our methods to a similar interaction scenario for retrieval-based qa (li et al., 2022), then we will additionally need to define logical forms to describe the ground-truth retrieval process and that of the qa model. for open-ended tasks such as keywordbased story generation (pascual et al., 2021), defining such logical forms will need non-trivial effort."
1054,"limitations this work focuses on informative image captioning evaluation, including an overall score, vision recall, text precision and token-level scores. the effectiveness of our metric is validated on standard image captioning benchmarks. infometic in this work may not perform well in other captioning tasks due to domain gap, but we contend that our general framework can be adapted to other domains such as text-aware image captioning. for example, for textaware image captioning which focuses more on scene texts in images, we could further encode text regions besides the existing object regions for better comparison with captions. in the future, we will comprehensively explore how to adapt our metric to other captioning tasks, such as text-aware image captioning and video captioning."
1055,"limitations of metrics trained directly from overall preference judgments. future work should look deeper into modelling frequent aspects mentioned by expert annotators, such as completeness and ease of understanding, perhaps by taking inspiration from evaluation methods that explicitly localize and categorize errors (freitag et al., 2021; goyal et al., 2022)."
1056,"limitations and trends: 1. language bias. most works used chinese and japanese datasets as testbed for training zp models (song et al., 2020; ri et al., 2021). however, there were limited data available for other prodrop languages (e.g. portuguese and spanish), resulting that linguists mainly used them for corpus analysis (pereira, 2009; russo et al., 2012). however, zp phenomenon may vary across languages in terms of word form, occurrence frequency and category distribution, leading to learning bias on linguistic knowledge. thus, it is necessary to establish zp datasets for various languages (prasad, 7https://catalog.ldc.upenn.edu/ldc2013t21. 8https://zhidao.baidu.com. 2000; bacolini, 2017). 2. domain bias. most corpora were established in one single domain (e.g. news), which may not contain rich zp phenomena. because the frequencies and types of zps vary in different genres (yang et al., 2015). future works need more multi-domain datasets to better model behavior and quality for real-life use. 3. become an independent research problem. early works extracted zp information from closed annotations (e.g. ontonotes and treebanks) (yang and xue, 2010; chung and gildea, 2010), which were considered as a sub-problem of coreference or syntactic parsing. with further investigation on the problem, mt community payed more attention to it by manually or automatically constructing zp recovery and translation datasets (e.g. baiduknows and tvsub) (wang et al., 2018a; zhang et al., 2019). 4. coping with data scarcity. the scarcity of zpt data remains a core issue (currently only 2.2m ∼ 0.1k sentences) due to two challenges: (1) it requires experts for both source zp annotation and target translation (wang et al., 2016c, 2018a); (2) annotating the training data manually spends much time and money. nonetheless, it is still necessary to establish testing datasets for validating/analyzing the model performance. besides, pre-trained modes are already equipped with some capabilities on discourse (chen et al., 2019; koto et al., 2021). this highlights the importance of formulating the downstream task in a manner that can effectively leverage the capabilities of the pre-trained models."
1057,"limitations in the models and the evaluation metrics. therefore, researchers need to pay more attention to mitigate these biases, such as using diverse data sets and debiasing techniques, to improve the accuracy and fairness of zpt methods."
1058,"limitations and future directions the findings reported in this paper have to be seen in light of some limitations and, therefore, they just represent a first step. most of these limitations are related to the ellie dataset itself. first of all, though the predicate-argument combinations used in ellie come from the dtfit dataset and were rated by humans, still the elliptical sentences need human judgements,19 which is one of the future research direction. then, the dataset size is relatively small, especially comparing to other resources on ellipsis (e.g., the 1000 elliptical sentences of the blimp dataset). currently, ellie was mainly conceived as an evaluation dataset but it could be enlarged and become useful for models’ fine-tuning, or for carrying out few-shot learning experiments via prompting. moreover, we tested ellie only with two popular language models, but future works should include the comparison with other systems (e.g., roberta, xlnet, distilled transformer models, gpt-3, etc.) or even with specialized models for ellipsis resolution, to see to what extent our findings are generalizable. concerning the experiments, some changes could be made in the evaluation of task 3. first, we could test the prompts in (4) on the subsets for the other roles, and look for different prompt structures to see if this leads to performance changes. we could also adopt a softer evaluation for this task, by assessing the output in terms of similarity to the target answer. finally, another limitation is related to the strong dependence of our results to the language used for the analysis (i.e., english). from this point of view, a cross-linguistic study on the elliptical structures in ellie could contribute to improve our work from both a theoretical and practical perspective."
1059,"limitations 7 and appendix a.3.4, authors and annotators of mpchat are primarily in the us, uk, new zealand, and australia. these demographic and geographic biases mean that mpchat may not equally represent all groups. meanwhile, wang et al. (2021); lee et al. (2022) reported that preprocessing data with clip can cause gender-bias issues. we use clip to measure image-text similarity in the pre-processing for data collection, so this problem may exist in our dataset. users of our dataset should be aware of these risks. to comply with the reddit api terms of use and to protect the privacy of reddit users, commer- cial and for-profit use of our data is limited. it must be available for academic purposes only."
1060,"limitations although our method is efficient and scalable, we have not conducted pre-training on large-scale corpora due to limited computational resources. the quality and quantity of data are crucial factors for a pre-training model. as our model only covers 36 languages, it cannot provide services for many rare languages. this paper just proposes a new pretraining direction and does not use many training tricks. exploring dap’s full capability is left for future work. besides, rtl task is not the only possible tokenalignment task for our dap framework. other objectives based on token representations are also worth investigating. the best objective form is still under research."
1061,"limitations since our model involves an additional step of ocr, it is less efficient than the end-to-end tit model, although it can achieve significantly better performance. besides, with the incorporation of image information, our model is still unable to completely address the issue of error propagation caused by ocr."
1062,"limitations we summarized the limitations of fedlegal as follows: (1) although fedlegal includes a variety of legal tasks with natural language understanding, more useful legal generation tasks should be included, such as legal court debate, legal case summary, etc. however, the tasks in fedlegal are more commonly used in the legal domain com- pared to these tasks. on the other hand, the manual annotation cost is also a limited factor. we will expand more useful legal tasks and also welcome contributions of new datasets to keep fedlegal up-to-date. (2) we do not analyze the fl algorithm’s robustness attacks (i.e., poisoning attacks). we argue that it is impractical to have malicious court participants when multiple official courts perform federal learning. therefore that"
1063,"limitations first, as we do not rely on specific corpora and avoid the shortcomings of extractive methods, we also lose their advantages. the typed egs generated by our tp-egg is strongly related to the seed predicates and training data of generation modules, while extractive egs can generate domainindependent egs from large corpora and do not require supervised training data to a considerable degree. second, the edge calculator w is time- consuming even we can control the scales of output egs, as the edge num |e(t1, t2)| will be relatively large for tp-egg to generate powerful egs. furthermore, how to effectively select seed predicates still remains a difficult problem which has not been discussed thoroughly in this work by using the validation datasets. we assume that this problem could be solved by carefully confirming how the seed predicates represent corresponding domain knowledge and we leave it to future work."
1064,"limitations the main contributions of this paper are towards tackling over-smoothing issue for learning unsupervised sentence representation. the proposed approach is fairly basic and may simply be extended to improve the performance of other state-of-the-art models. more broadly, we anticipate that the central idea of this study will provide insights to other research communities seeking to improve sentence representation in an unsupervised setting. admittedly, the proposed strategies are restricted with unsupervised training, and biases in the training corpus also may influence the performance of the resulting model. these concerns warrant further research and consideration when utilizing this work to build unsupervised retrieval systems."
1065,"limitations this work has two main limitations. first, we only consider baseline models with similar amount of parameters, and pre-trained on similar scale of text corpus for comparison. while we are aware of recent models including t5 (raffel et al., 2020) and palm (chowdhery et al., 2022), they either use huge corpus like c4 (745gb text) for pre-training or contain significantly more parameters than ours. in the future, we will try to find additional computational resources to scale up our model and pre-train on larger text corpus. second, we leverage spacy to segment sentences into words, which is rule-based using spaces, punctuations and other rules. this approach works well on english and many other common languages such as french, german and spanish. but for a few languages that do not use spaces to split words (e.g. chinese and japanese), it will be challenging to retrieve word boundaries. to address this issue, we consider either falling back to character splitting for these languages (similar to multilingual bert) or employing a more sophisticated word boundary detector in future work."
1066,"limitations one of the primary limitations of nlp modeling in materials science, including this work, is the low quantity of available data as discussed in section 2. this analysis is affected by this limitation as well given that our evaluations were performed in a low-data setting within a dataset that was already limited in size. we believe that future work can improve upon this study by applying larger datasets, both in the number of samples and in the scope of tasks, to similar problem settings. the small nature of the datasets applied in this study also presents the danger that some of the models may have memorized certain answers instead of achieving a broader understanding, which could be mitigated by enlarging the datasets and making the tasks more complex. moreover, we did not study the generalization of nlp models beyond the materials science domain, including adjacent domains such as chemistry and physics. this targeted focus was intentional but imposes limitations on whether the proposed techniques and insights we gained from our analysis are transferable to other domains, including applying nlp models for scientific tasks outside of materials science. another limitation of our study is the fact that we focused on bert-based models exclusively and did not study autoregressive models, including large language models with billions of parameters highlighted in the introduction. the primary reason for focusing on bert-based models was the diversity of available models trained on different scientific text corpora. large autoregressive models, on the other hand, are mostly trained on general text corpora with some notable exceptions, such as galactica (taylor et al., 2022). we believe that future work analyzing a greater diversity of language models, including large autoregressive models pretrained on different kinds of text, would significantly strengthen the understanding surrounding the ability of nlp models to perform text-based tasks in materials science. while the results presented in this study indicate that domain-specific pretraining can lead to noticeable advantages in downstream performance on text-based materials science tasks, we would like to highlight the associated risks and costs of pretraining a larger set of customized language models for different domains. the heavy financial and environmental costs associated with these pretrain- ing procedures merit careful consideration of what conditions may warrant expensive pretraining and which ones may not. when possible, we encourage future researchers to build upon existing large models to mitigate the pretraining costs. broader impacts and"
1067,"limitations in this work, our approach assumes event triggers and argument templates (i.e., ontology) are given. this limits our approach’s applicability, as it requires an event detection system to produce event triggers and event types before llms can be prompted to generate event arguments. we only explore hierarchical events with only 2 levels from the ace05-e ontology and data, which has limited coverage of real-world complex event hierarchy. similar to prior event argument extraction work, our approach relies on a human-curated hierarchical ontology. we leave automatically discover hierarchical ontology for future work. despite llms performing well on eae with few-shot data, compared to existing supervised approaches, their inference is relatively slow and costly11 since the llms we used are generally more than 100x larger in the number of parameters. prior work (zhao et al., 2021; lu et al., 2022) has demonstrated a strong relationship between performance and in-context demonstrations; however, for ease of comparison to supervised baselines, we use the same set of examples from the training set for in-context learning. we expect better selecting (liu et al., 2021) and ordering (lu et al., 2022) incontext examples can benefit code4struct performance, which we leave for future work. ethical considerations since event argument extraction only requires predicting arguments from the given text, the risk of generating toxic languages is relatively low as long as the given test is not toxic. this is because the prediction can be grounded in the input sentence, eliminating potential toxic tokens that did not appear in the original sentence. however, discrimination and bias are possible, as observed in the foundational llms we used (brown et al., 2020; chen et al., 2021; ouyang et al., 2022), which we refer to brown et al. (2020) for detailed"
1068,"limitations there are several limitations to mathgpt, both practical and fundamental. first, the model depends on an external method for converting mathematical expressions to opts, currently being latexml. the conversion method is imperfect, which limits mathgpt’s capabilities as it will be presented with many distorted expressions during training and at test time. furthermore, the conversion process is slow and requires dataset-specific engineering to accommodate, making it difficult to deploy the model across many datasets. second, because mathgpt outputs trees rather than sequences, it is fundamentally difficult to evaluate and utilize in text-based settings without a highly accurate tree-to-text converter. the tree-to-text converter is yet another imperfect process in the pipeline, although it could be improved to a reasonable degree with significant engineering effort. third, because mathgpt has additional components and requires more information per token than gpt-2, it has higher space and time requirements that make training more expensive. finally, because mathgpt is pre-trained on highly formal and structured mathematical content, it may struggle to generalize to student-generated mathematical language, which is often error-prone and may exhibit very different patterns."
1069,"limitations our method depends on a large-scale paraphrasing corpus. we only test our method on the english ls task. excluding english, other languages have large-scale paraphrasing datasets available, e.g., french, german, chinese, spanish, etc. our method can be easily extended to these languages. but, for some languages that cannot obtain enough paraphrasing datasets, our proposed method cannot be used. another limitation is that our method may struggle to generate substitutions for rare or unusual words and phrases, as they may not have encountered sufficient examples of these words in the training paraphrase data."
1070,"limitations in this work, we adopt a data-driven approach to identifying latent relevancy. however, we believe that external knowledge such as knowledge graphs could also be of great help for this purpose, and is thus one of the directions of our future work."
1071,"limitations to better inspire the follow-up work, we summarize the limitations of our method as follows: 1) from our experimental results the appendix d, we can see that the estimation of the number of intents in our proposed can be further improved. 2) we do not try more means to prevent knowledge from forgetting. we can probe into the intrinsic structure of unlabeled data in a more fine-grained way by improving the posterior estimation. 3) according to section 5.3, we have verified that both exploration and utilization are indispensable, but at the same time, we only empirically choose the specific proportion of both, without theoretical analysis of the most appropriate proportion for each dataset. we look forward to making progress in the follow-up research on the above limitations."
1072,"limitations a key limitation of this work is the dependence on a machine translation system to get highquality translations and annotation projections of the dataset. depending on the availability of language resources and the mt model quality for a given language pair, the translations we use for training and evaluation may be inaccurate, or be affected by translationese, possibly leading to overly optimistic estimates of model performance. in addition, since the annotation projection for relation arguments is completely automatic, any alignment errors of the mt system will yield inaccurate instances. alignment is at the token-level, rendering it inadequate for e.g. compounding or highly inflectional languages. due to the significant resource requirements of constructing adequately-sized test sets, another limitation is the lack of evaluation on original-language test instances. while we manually validate and analyze sample translations in each target language (section 4.1) for an initial exploration of mt effects, these efforts should be extended to larger samples or the complete test sets. finally, we limited this work to a single dataset, which was constructed with a specific set of target relations (person- and organization-related), from news and web text sources. these text types and the corresponding relation expressions may be well reflected in the training data of current mt systems, and thus easier to translate than relation extraction datasets from other domains (e.g., biomedical), or other text types (e.g., social media). the translated examples also reflect the source language’s view of the world, not how the relations would necessarily be formulated in the target language (e.g., use of metaphors, or ignorance of cultural differences)."
1073,"limitations our pareto-md doubles computational cost due to training two models simultaneously, which can be a limitation of our approach. however, paretomd obtains significant improvement that is hard to achieve for previous methods of training individual models, thus worthy. besides, our approach would not necessarily result in double training time because these two models can be trained in parallel as implemented by guo et al. (2020). moreover, pareto-md does not affect inference efficiency."
1074,"limitations further research is needed to understand the robustness of our over-parameterization framework properly. the results given in this study are constrained by the natural language processing tasks and datasets used for evaluation. even though we employ standard classifications from the literature, the choice of downstream tasks and datasets is still subjective. furthermore, due to computing limitations, we could not investigate the scaling behavior of the large plms. additional study is needed in this area. in addition, as our approach is based on plms that may learn biased information from pretrained corpus, a potential risk is that our approach may also be affected by it and generates improper texts."
1075,"limitations one limitation of this work is that we are only considering behavioral data which makes it difficult to 12this is also compatible with the observation by sap et al. (2022) that gpt-3.5 performs considerably better than gpt-3 in answering factual questions about object location changes. further, see madaan et al. (2022) for other tasks for which pretraining on code seems to be beneficial. establish a fully causal link between entity tracking capacities and high performance on our task. entity tracking is a high-level linguistic behavior and many other capacities are necessary for achieving high accuracy on our task. therefore, we cannot rule out that differences in some other capacity, such as interpreting sentences compositionally (see bogin 2022 and bogin et al. 2022 for evidence that gpt-3 and gpt-3.5 models differ in their compositional generalization behavior), are the main driver for the differences in behavior we see across models. a possible criticism of our setup is that it requires short-term memory capacities that exceed the memory capacities of most, if not all, humans. that is, if we presented humans with the same input as the model, we would not expect them to be able to keep track of the contents of all 7 boxes due to memory limitations. therefore we are potentially expecting models to do super-human entity tracking, a setup that has been criticized for model evaluations of other linguistic abilities (lampinen, 2022). we nevertheless believe that our task is justified given the architecture of the evaluated models. transformer-based models can look back to any token in the entire input sequence within their context window, so a proper comparison between humans and models would be to present humans with the full description in written form and let them re-read the description after being prompted to state the contents of a box. while we did not formally evaluate whether humans have this ability on a larger population, we personally did not have any trouble tracking the contents of boxes when we had access to the written description. relatedly, we designed our task such that the entire description fits within the context window of pretrained language models. however, as we mentioned in the introduction, entity tracking is an important ability for understanding long contexts and given the limited context window, our results do not apply to texts whose length exceeds a model’s context window, and likely different model architectures will be necessary to perform proper entity tracking for longer texts. further, while we found that gpt-3.5 models as well as finetuned t5 models can track entities in our task with higher accuracy than a strong random baseline, our results also indicate that this behavior is not very stable once several operations act on an entity. our results should therefore not be taken as justification for using these models for critical applications where high accuracy is needed. lastly, we only evaluated english models in this work. given that we showed that even without high lexical overlap between the training and evaluation examples, models can keep track of entities to some extent, it seems likely that our results also apply to other languages. however, whether this actually the case remains an open question."
1076,"limitations data size: sugar is relatively small compared to recently published datasets. this is due to the complexity of our problem setting and annotation pipeline. we prioritized quality over quantity and performed multiple steps of manual intervention to reduce errors, false negatives, and annotation artifacts. these problems have been reported in various nlp tasks not limited to conversational tasks (gururangan et al., 2018; akama et al., 2020; elazar et al., 2020). nonetheless, our experiment has shown that pre-trained transformer models can be trained to outperform a tf-idf ranker by a clear margin, which is encouraging. in addition, we could automatically induce noisy but large-scale training instances from existing resources, for example, by harvesting event pairs that can be used as u and r from event knowledge bases such as atomic2020and generating situation statements using our generator (§3). representation of situation information: in sugar, situation information is represented in textual expressions. in real-world applications, such information could be collected via external apis (e.g., calendar and map) and sensors (e.g., camera) and stored in non-textual forms. our study is a proof-of-concept that shows the understanding of situational information is very important for response selection. future research should explore ways to process situation information that is expressed in other forms of data (e.g., structured texts, numbers, images). even if the value is structured or images, we could transform them into textual forms as done in data-to-text research (shen et al., 2020; miura et al., 2021). besides, we acknowledge that situational information is often under-specified in sugar because some information is considered to be common-sense (e.g., a room has a door) or presupposed (e.g., “please open the door” presupposes that the door is closed.), and such information was not explicitly stated by human annotators during data collection. therefore, response selection systems should be equipped with a mechanism to handle implicit knowledge to solve the task. ethical considerations undesired bias and abusive content: a multitude of sources have reported that data-driven conversational systems can (re)produce undesired bias or abusive language existing in language resources used for development. to minimize such a risk, we carefully curated conversation examples in sugar. our target task is response selection, where systems only produce language in a pre-compiled response list, and therefore, it is not likely that resulting systems yield harmful content. however, users of sugar should be cautious when it is used for developing generation systems in future work. human subjects: crowd workers in amazon mechanical turk (mturk) participated in our data collection pipeline. our annotation tasks were reviewed by the institutional review process before being published in mturk to avoid ethical issues. we did not collect any personally identifiable information of workers other than (anonymized) turker ids. task rewards were decided by several rounds of trials so that workers can receive at least $6.50 hourly. use of external data and tools: we used external datasets such as atomic2020and conceptnet and tools such as spacy and transformers library. we have confirmed that the use of these resources for our research does not violate usage restrictions."
1077,"limitations we discuss here the limitations of the proposed diffusionner. first, as a latent generative model, diffusionner relies on sampling from a gaussian distribution to produce noisy spans, which leads to a random characteristic of entity generation. second, diffusionner converges slowly due to the denoising training and matching-based loss over a large noise timestep. finally, since discontinuous named entities often contain multiple fragments, diffusionner currently lacks the ability to generate such entities. we can design a simple classifier on top of diffusionner, which is used to combine entity fragments and thus solve the problem of discontinuous ner."
1078,"limitations there are two main limitations in this work. first, instead of best st performance given full data, our cross-modal pre-training only aims to demonstrate the effectiveness of our method in the low-resource st setting. we realize that unified pre-training for both speech and text gradually becomes a dominant paradigm for st and our future work is to fuse waco into a joint pre-training framework. second, we note that tang et al. (2022) explores the possibility of pre-training mt models with phoneme tokenizations, though it is unclear if the phoneme-based mt model has an advantage over the bpe-based mt model. we follow the tradition of using the latter one and leave the comparison of them in future works."
1079,"limitations despite outperforming previous best methods, our method still has several limitations and substantial room for future improvement. first, the variety of modules is limited in the current reasoning environment. it would be interesting to introduce a wider variety of modules (e.g., a numerical calculator) to make our method more general. second, our method currently retrieves facts from a fixed corpus. while this is efficient for the specific domain, it may not be sufficient for questions not covered by the fact corpus. it would be more powerful if we retrieve up-to-date information using a modern search engine as our retriever. third, in our experiments, we try our best to select the appropriate prompts to motivate gpt3 and chatgpt to generate reasoning steps and answers. with our prompts, gpt-3 and chatgpt can achieve high answer accuracy. but it is hard to guarantee that our prompts are the best ones to elicit the model’s capabilities completely. finally, although scaling up the size of the language models may lead to emergent abilities (wei et al., 2022a), in this paper, we do not experiment with larger language models (e.g., t5-11b) due to the computational constraints. to the best of our knowledge, our work is foundational research, and we do not find obvious risks related to malicious harmful effects, environmental impact, fairness considerations, or privacy considerations."
1080,"limitations section. we will also apply ot-based alignment to problems related yet with different constraints and objectives, e.g., crosslingual word alignment and text matching. limitations in this study, we used standard and basic word embeddings to highlight the characteristics of the different ot problems on unbalanced word alignment. this limits the capability of phrasal alignment. similar to figure 3 (a), we binned all the test samples across datasets (under the ‘sure only’ setting) according to their phrase alignment ratios and evaluated the performance of the supervised ot-based alignment methods.13 specifically, we regarded one-to-many, many-to-one, and many-tomany alignment as phrase alignment. figure 5 shows the trend of the f1 score according to the phrase alignment ratios. obviously, the f1 score degrades as more phrase alignment exists in a sentence pair. one of the straightforward ways to improve the phrase alignment is exploring pre-trained language models enhanced for span representations (joshi et al., 2020) and sophisticated methods for phrase representation composition (yu and ettinger, 2020). in addition, phrase alignment can be addressed from the ot perspective, too, by conducting structure-aware (alvarez-melis et al., 2018) and order-aware (liu et al., 2018) optimal transport. these directions constitute our future work."
1081,"limitations all samples used in this work are in english, thus to apply the model to other languages, it will require training data on the specified language or using multilingual language backbones. moreover, we are aware that it remains an open problem to mitigate biases in human stancetaking. of course, current models and laboratory experiments are always limited in this or similar ways. we do not foresee any unethical uses of our proposed methods or their underlying tools, but hope that it will contribute to reducing incorrect system outputs."
1082,"limitations in this work, we propose a self-training method which requires unlabeled data in target languages. recall that we remove gold labels from readily available target-language training data from the same public ner dataset, and use them as unlabeled data in our experiments. however, this might not perfectly simulate a real-life application scenario. firstly, most free text in target languages might not contain any predefined named entities. this requires careful data cleaning and preprocessing to produce unlabeled data ready for use. secondly, there might be a domain shift between labeled source-language data and unlabeled targetlanguage data, which poses a question on the effectiveness of our method. furthermore, the ner datasets used in this work contain only a few entity types and different entity classes are relatively balanced. however, on datasets with a larger number of classes, each class will be underrepresented in a batch and a larger batch size might be required for contrastive selftraining to work satisfactorily. also, if the entity type distribution is long-tailed, prototypes for those rare entity types might be inaccurate, and this affects the efficacy of prototype-based pseudolabeling. lastly, as we observe slight drops of pseudo label quality at the end of training for some languages, the pseudo label update strategy can be refined for further improvement."
1083,"limitations this work focused on assessing multimodal degree for recent english vl models. the following limitations can be relevant for future work. we only evaluated a limited number of models in a zero-shot setting using their image-sentence alignment and vqa heads. future work might be interested in assessing more models and tracking the evolution of mm-shap scores during model pretraining and finetuning. this work applied mm-shap to vl encoders. we leave it for future work to investigate autoregressive (decoder-only) vl models. in the time it took to review and publish this work, we already encountered efforts to apply shapley values for interpreting vl models in cafagna et al. (2023). we only applied ml-shap to vl models. future work might be interested in models working with other or additional modalities beyond vision and language. computing all possible coalitions between input tokens for shapley values is infeasible because their number is exponential in the number of tokens (2p). therefore we perform monte carlo approxi- mation by randomly sub-sampling 2p+1 coalitions. this results in approximate mm-shap scores per sample. we argue that as an alternative, one can simply increase the number of sampled coalitions for more exact measurements (as we did 10-fold for fig. 1 and the examples in appendix c) – at the cost of increasing the environmental footprint. but it is not necessary to increase the number of samples when estimating mm-shap at dataset level, because the number of coalitions has very little effect on a data-set wide range – given that approximation fluctuations average out. to compute mm-shap at data-set level, one needs to run models in inference mode 2p+1 times, where p is the number of tokens to mask (around 40 in average for mscoco-sized captions). on an nvidia titan x gpu, computing mm-shap for one image-caption pair can take 2 seconds for albef, 3 seconds for clip. lxmert is the most expensive and needs 15 seconds, because it computes image features with a cnn backbone for every masking configuration. ethical considerations this paper uses publicly available datasets and models and therefore could carry on their potential biases (meister et al., 2022; garcia et al., 2023) and imperfections. however, the method presented in this paper enables model and dataset interpretation and we hope that it can help future work locate harmful biases."
1084,"limitations in this section, we analyze the limitations of this work: • as it is the first attempt to analyze the mixedinitiative interactions in emotional support conversations, the proposed metrics can be further improved for more robust evaluation. • since the knowledge retrieval is not the focus of this work, we did not spend much space on discussing the choice of different retrieval methods. as shown in table 5, there is still much room for improving the knowledge retrieval from a large scale knowledge graph. it is also worth studying more efficient retrieval methods for retrieving knowledge from a densely connected kg. • the proposed method requires an additional mental health related knowledge graph constructed by experts or knowledgeable workers, which is probably difficult to obtain in some applications. however, different from other knowledgeintensive tasks that can be benefited from opendomain knowledge (e.g., wikipedia), it attaches great importance in the professionals of the knowledge for building a helpful and safe esc system. ethical considerations the datasets adopted are publicly available and widely studied benchmarks collected from professionals or well-trained annotators. all personally identifiable and sensitive information, e.g., user and platform identifiers, in these dataset has been filtered out. we do not make any treatment recommendations or diagnostic claims. compared with existing methods for emotional support conversations, the proposed method can be regarded as one step further to a more safer esc system. the proposed method retrieves knowledge from a well-established mental health knowledge graph, which can be maintained by filtering out harmful information when applying into applications. then the knowledge-enhanced approach can alleviate the randomness during the response generation and provide the guidance towards more positive responses. in order to prevent the happening of unsafe cases, the analysis of emotion intensity prediction can also serve as an alarming mechanism that calls for handoffs to an actual psychologist."
1085,limitation of our stimuli is that they were not specifically designed to require tom. new datasets that perform targeted manipulations of tom alongside tests of language comprehension could help reveal how linguistic experience and tom jointly support pragmatic behaviors.
1086,"limitations our proposed method needs to construct counterfactual examples to estimate the natural direct effect of disconnected reasoning during the training phase, thus we need a little more gpu resources and computational time. however, the need of resource occupancy and time consumption of our approach does not increase during inference. another limitation is that we use the learnable parameters to approximate the yk1,k2,k∗ . in our future work, we will explore a better approach to model it."
1087,"limitations we now discuss limitations of causal-debias. in consideration of the fairness, we follow the prior bias mitigation work (guo et al., 2022; cheng et al., 2021; he et al., 2022) and use human-collected lists of gender and racial pairs for counterfactual data augmentation and intervened distribution generation. it is obvious that the bias word lists are inadequate to cover all the bias-related demographic groups, while we believe the general list is exhaustive. we consider there is a possible model improvement that leverages the perturbation augmentation on bias-related sentences along multiple demographic axes (qian et al., 2022). another possible improvement would be to generate bias words by using prompts to probe the biases that may lead to a bad effect. moreover, we also considered the use of external corpora. the external corpora have been significantly investigated in prior works (liang et al., 2020; cheng et al., 2021) and are utilized as an intervention corpora. recently, he et al. (2022) used two natural language inference data (snli and mnli with gender terms) to produce generalpurpose debiased representations. there are several other corpora including news-commentary-v1 (kaneko and bollegala, 2021), wikipedia (zmigrod et al., 2019; webster et al., 2020), and wikitext-2 (guo et al., 2022). a possible future direction of debiasing is how to mitigate the biases without heavily relying on any corpora and just using internal knowledge. moreover, in the paper we primarily focus on studying gender and racial bias mitigation. it is also worth exploring intersectional biases mitigation (lalor et al., 2022) and domain-specific bias mitigation (chuang and yang, 2022; abbasi et al., 2021). we would also like to note that although causaldebias shows a satisfactory performance on seat tests and crows-pairs, these results should not be interpreted as a complete bias mitigation. interestingly, he et al. (2022) expressed the same opinion. the main metrics (like crows-pairs) are mainly against north american social biases and only reflect positive predictive power. they detect the presence of the biases but not their absence (meade et al., 2022). he et al. (2022) did not use the seat tests and evaluated their model on various metrics. from the perspective of different usage scenarios, we need a more general and reliable debias metric for comparison between different models. the lack of universality and agreement in existing evaluation frameworks is a fundamental challenge in this field."
1088,"limitations we acknowledge the main limitation of this work is that we only evaluate our methods on some tasks from the glue and superglue benchmarks due to limited computation resources. and all tasks are not in a realistic few-shot setting, where the number of training samples is less than a few hundred and development sets are not offered. the benefit of peft methods could come from an exhaustive search of hyper-parameters for the development sets, while the realistic few-shot setting could solve this issue and shed more light on peft. it would be interesting to see how our methods and other baselines perform on a wide range of few-shot tasks. in addition, current frameworks are not friendly for sparse fine-tuning methods. most works (diff pruning, fish mask and our pafi) still need to calculate a full gradient of all parameters and selectively update the masked parameters, which makes it cost the same training time as full fine-tuning. last but not least, we only estimate our methods on one single complex task, i.e. wmt 2016 en-ro. one might not draw the same"
1089,"limitations one limitation of our work is that manner only explicitly utilizes the memory to enhance the performance of the entity typing module in target domain. however, we argue that the memory could also implicitly enhances the span detection module through the shared pretrained language model with entity typing module. we leave how to explicitly leverage memory to enhance both entity typing and span detection modules as future work."
1090,"limitations and ethical considerations there are several significant limitations of the massive dataset and of our modeling. starting with the dataset, the per-language data quantities are relatively small at 19.5k total records and 11.5k records for training. second, there are some lowquality utterances, both in the seed data and in the translations. for the most part, these are surfaced through the judgment scores we provide for each record, but if a user does filtering based on these judgments, then the data size decreases even further. third, the data were originally created through crowd-sourcing, not from a real virtual assistant, which introduces artificialities. relatedly, allowing the worker to decide on translation versus localization of slot entities added further noise to the dataset, although we try to store this decision in the metadata. fourth, our labeling schema is relatively simple when compared with hierarchical labeling schemata or flat schemata with more intent and slot options. fifth, our collection system did not have a robust method to preserving or denoting native tokenization practices—some languages do not separate with whitespace, while others do but there is no set practice. this results in potentially easier (larger chunks to predict slot labels) or harder (each character individually predicted) tasks. sixth, it’s possible, though unlikely, that some of our new crowd-sourced records may contain toxic or otherwise objectionable content. we performed analyses to check for such malicious activities and did not find any as such. regarding modeling, we have only investigated base-sized models in relatively standard setups, leaving room for much more sophisticated modeling. the risks associated with this dataset and work are relatively low, given that we have released a research dataset meant to promote better multilinguality in nlp systems."
1091,"limitations the proposed method for improving llms is a post-hoc re-ranking approach, and we do not improve llms themselves due to the difficulty of fine-tuning llms. besides, we improve the ability of constrained language planning for smaller models from the perspective of building task-related datasets, but do not consider investigating the model itself, other than adopting retrieval augmentation. in addition, because automatic metrics for generated text are limited, the automatic evaluation of this paper may result in an overestimation or underestimation of the mentioned methods, though we attempt to mitigate this by incorporating a moderate amount of human evaluation. despite the advanced planning capabilities of newer language models, our work remains significantly valuable to the knowledge distillation of these llms into smaller and more cost-effective models. we also discover several limitations of the proposed coscript datasets. first, the specific goal explored in this work only inherits from an abstract one with one extra constraint. however, in real-life situations, complex planning may involve multiple constraints, which we do not investigate in this work. another limitation of coscript is that our dataset is generated from instructgpt, and thus the data distributions may be biased to favor causal language models. this is a common issue with machine-generated datasets, which we address by manually curating coscript’s validation and test sets. furthermore, there are still some incorrect samples (about 5%) in the training data without manual correction due to the limits of budget and time. last but not least, we only consider whether the script can be executed at the human level. the script execution for robots (huang et al., 2022; lu et al., 2022b) is unstudied in our work, and there still exist huge gaps in transferring complex human language to one that is understandable and executable by robots."
1092,"limitations despite the state-of-the-art performances, our proposed methods still have some limitations for future directions. firstly, multi-view prompting creates overheads of training and inference proportional to the number of views. for efficiency in practice, according to figure 3, mvp with a relatively small number of views behaves decently (e.g., 5 or 7). secondly, we apply a simple yet effective aggregation strategy to combine the results of multiple views. more advanced strategies can be explored. lastly, experiments only verified the consistent improvement on absa tasks, while intuitively, the idea of mvp that leverages multiple views can be expanded to any structure prediction tasks, such as information extraction, emotion-cause pair extraction, and stance detection."
1093,"limitation of large language models that can be widely and easily exploited by malicious end-users. however, we think the benefits of analyzing bias in reasoning prompts, along with possible methods to mitigate effects, may spur improvements in value-alignment work. because the content of our work is offensive, we include a warning at the start of the paper. we only use previously collected or synthetically generated benchmarks and rely on automated scoring, eliminating exposure of offensive text to human participants."
1094,"limitations the outcome on multiple datasets verifies the powerful reasoning ability, which even works on models with only several billion parameters. however, our self-thinking procedure utilizes only one dataset, gsm8k, and the available training set size is only 7.5k. the main reason is the scarcity of high-quality datasets with rich reasoning paths. and, collecting such data incurs huge computation costs and expensive human resources. another limitation is that we have not conducted experiments on bigger language models, such as gpt-3 and palm, due to the expensive usage costs and the fact of no open-source codes. in a nutshell, in the future, we will focus on collecting more highquality labeled data and exploring our method on more powerful language models."
1095,"limitations while we consider our approach more easily applicable to new languages than rule-based forward augmentation, it relies on the existence of sufficient original gender-fair text in the language of interest and it is currently unclear what the minimum amount of parallel data is to learn a genderfair rewriting model. additionally, our survey only targets affinity groups which limits the generalisability of our results to all german speakers. since people who choose to not use gender-fair language can simply not use a rewriting system, we do not think that this lack of generalisability is a problem in this case. another limitation is that we use a specific form of gender-fair german in our survey. we made participants aware of this in a disclaimer at the beginning of the survey. it should be stated that there are many different acceptable gender-fair forms in german (see section f). while using a different gender-fair form could affect the individual ratings in our survey, we do not expect that it would change our finding that rewriter outputs are rated more gender-fair than the original texts."
1096,"limitations in this work, we demonstrate the effectiveness of the proposed diffusionbert. however, the sampling efficiency in unconditional generation still lags behind fine-tuned gpt and we observe a few sampled sentences lacking coherence when the preassigned length is large (e.g., 128). the issue of inference efficiency is more severe in constrained settings in that mbr decoding samples multiple sentences for one source text. though it brings significant improvement in bleu and rouge-l scores, the sampling time of one batch is several times that of unconditional generation."
1097,"limitations the central limitation of minimoe is the increased memory footprint, which we could potentially address in the near future according to appendix g."
1098,"limitations as we have shown, there is much room to improve the learning approach, which incur lower costs than increasing model’s parameters or elaborate data engineering. this paper is an exercise in guiding learning focus, and we argue that focusl is not perfect for the positioning method and the relevanceto-weight transformation method. for example, our positioning method may contain noise, and some words that are not important in given knowledge may be used as our learning focus. we will continue to explore better methods to guide the model’s learning focus. meanwhile, our method only experiments on the basic cross-entropy loss, and still needs to be explored for other learning approaches such as contrastive learning."
1099,"limitations our work provides an effective solution to augment st when source transcripts are unavailable, which could benefit many unwritten languages. however, limited by the publicly available st datasets, we use english as an unwritten language for experiments, which may slightly differ from realworld unwritten languages. since we never use transcripts in our approach, we believe our work can shed some light on st for real-world unwritten languages. we are glad to explore this if there are available datasets in the future."
1100,"limitations this paper aims to investigate a more efficient and effective framework to incorporate the heterogeneous features of both text and graph knowledge. the extensive experiments demonstrate our framework has a superior performance in capturing semantics of input knowledge, thus beating all sota models. however, due to the time and resource limit, we could not conduct further experimentation to compare with promising frameworks in similar areas. in fact, we have observed some other techniques (tang et al., 2022c; yu et al., 2022; wu et al., 2022) may be beneficial to our study, but when considering the difficulty in applying them here (due to additional annotation and knowledge being required), we have to leave them to future work. we also cannot exclude some other factors which may affect performance. for example, we select bart as the base language model in this paper. in practical use, the latest language models (e.g. chatgpt) may have better performance in this task. we have to leave the analysis of these factors to future study."
1101,"limitations this work is currently limited to the action chain as the abstract summary of the complete explanation for the given limited observation. in the future, we will further upgrade this task, e.g., considering the progressive textual descriptions as the complete explanation. we hope our work can advance the reasoning ai system research community."
1102,"limitations of transformers and, 2) serve as a challenge for generalization in rac over text. in the future, we expect to see more interesting work based on trac, such as better solvers with mechanisms to learn both preconditions and effects, and novel generalization tests that call for more specific reasoning abilities."
1103,"limitations we illustrate this paper’s limitations from the following three aspects: 1) limited by the computational resources, we only train udr from the initialization of “bert base uncased” following epr (rubin et al., 2022). we regard explorations based on other competitive pre-trained models like roberta (liu et al., 2019) and deberta (he et al., 2021) as future work. 2) most of current dense demonstration retriev- ers, including udr, are black-box models. although they lead to significantly better performance than bm25, how they find informative demonstrations is still unknown. therefore, a better understanding of the principle of informative demonstration’s retrieval or an interpretable and transparent demonstration retriever may be the next stage of improving demonstration retrieval. xu et al. (2023) propose a more explainable method, beyond-context learning, which first uses the language model to get training data’s next word probability distribution, then assigns test instances with labels of their nearest neighbors with similar next word’s probability distribution. we leave demonstration retrieval with better explainability as future work. 3) in the training stage we use lm to score candidates separately but in the inference stage lm is provided with a sequence of demonstrations. although experimental results demonstrate udr’s effectiveness, we think it is a promising direction to model the dependence between different demonstrations and leave it to future work."
1104,"limitations in this study, the limitations can be summarized into two major aspects: (1) the usage-based approach (dunn, 2017, 2019) being employed in our work has the ability to extract most of constructions, while a small portion of non-contiguous constructions (e.g., comparative correlative constructions) are neglected. these noncontiguous constructions are probably fragmented into multiple independent constructions. we will investigate the approaches to capture these noncontiguous constructions via incorporating more syntactic knowledge in future work for language representation enhancement. (2) as discussed in appendix h, the performances are not significant improved on the tasks that contain a large amount of colloquial expressions. since our constructions are mainly learned from the formal corpus, which has less colloquial expressions. it causes fewer constructions to be accessed in these tasks, which encourages us to learn constructions in more diverse corpus to enhance the language representation for natural language understanding tasks in the future."
1105,"limitations this study has potential limitations. first, it only focuses on answering existential positive first-order logic queries but does not support the negation operation. we will later address this limitation by modeling the negation operation. second, we uti- lize bert as the backbone model for inductive generalization due to computing resource limits. we plan to investigate the use of more powerful pre-trained language models with stronger generalizability in future research to improve inductive logical reasoning over kgs."
1106,"limitations our proposed dimongen task involves generating several diverse sentences to describe the relationships between concepts. however, it does not take into account the number of relationships between different concept pairs. this can lead to problems when applying the model trained on the dimongen dataset to other unseen concept pairs. for example, some concepts may have a small number of relationships, and asking the model to generate a greater number of diverse relationships may lead to hallucinations which can be misleading when using the generative model for educational purposes. we leave this as a future work for the research community. additionally, the performance of the moree model is heavily dependent on the quality of the external corpora used in the retrieval stage. if the corpora do not contain any relevant information for the input concepts, the moree model will perform similarly to a vanilla moe model. an alternative approach is to retrieve information from the web (huang et al., 2022b; lazaridou et al., 2022). last, it should be noted that the base models used in this study were relatively small. recent studies have demonstrated that large language models possess superior reasoning abilities compared to their smaller counterparts (wei et al., 2022; huang and chang, 2022a). future work on exploring the diversified generative commonsense reasoning ability of large language models is encouraged."
1107,limitations this work focuses on binary and multi-class classification settings using data in english. benchmarking faithfulness metrics in sequence labeling tasks as well as in multi-lingual settings should be explored in future work.
1108,"limitations first, off-policy algorithms like q learning are not explored in this"
1109,"limitations we focus on augmenting the hidden representations of a plm. thus most of our baselines, such as dropout (srivastava et al., 2014) and variational information bottleneck methods (mahabadi et al., 2021), do not require unlabeled data. for a fair comparison, we assume that the unlabeled data is not available. therefore, only the limited labeled training set are used to train the autoencoders in our experiments. however, such unlabeled generalor in-domain data (e.g., wikipedia text) are easy to obtain in practice, and can be used to pre-train the autoencoders with unsupervised language modeling tasks, which may help further improve the performance. we leave it for future work. ethical impact deep learning has demonstrated encouraging performance on a wide range of tasks during the past few years. however, neural models are data hungry, which usually requires a large amount of training data to achieve reasonable performance. it is expensive and time consuming to annotate a large amount of data. pretrained language models (plms) (devlin et al., 2019; conneau and lample, 2019; liu et al., 2020) have been proven to be useful to transfer knowledge from massive unlabeled text to downstream tasks, but they are also prone to overfitting during fine-tuning due to overparameterization. in this work, we propose a novel method to help improve model robustness in the low-resource scenarios, which is part of the attempt to reduce neural model reliance on the labeled data, and hence reduce annotation cost. our method has also demonstrated promising performance improvement on cross-lingual nlp tasks, which is also an attempt to break the language barrier and allow a larger amount of population to benefit from the advance of nlp techniques."
1110,"limitations the coco-crola benchmark generating procedure is intended to yield multilingual evaluations that can be scaled to even larger sets of concepts and languages without experienced annotators. in the interests of both concept and language quantity scale, we opted for an automated procedure which leverages machine translation systems, which can introduce translation errors. furthermore, variation in the nuance or normative meaning of concepts, particularly culturally contested ones such as “face,” (engelmann et al., 2022) “person,” or “man” will inevitably drive some variance in expected outputs by users across language communities. this cultural variation will place an unavoidable upper bound on the performance of inherently cross-cultural benchmarks such as coco-crola. additionally, typological variation between languages can introduce complications in applying our framework. for example, while simple template filling for prompting is straightforward in chinese, which requires no word-dependent articles, in english phonological properties of the word govern the preceding article, and in spanish and german grammatical gender do the same. hebrew has gendered nouns, adjectives, and verbs but not articles, on the other hand. overall, it appears that these have limited influence as grammaticality isn’t a crucial feature in the prediction of image tokens performed in t2i models, appendix b. while doing so aids in the scalability of the approach, using clip as a feature extractor for computing the metrics, particularly correctness xc and wc, potentially introduces biases due to the english-primary data that clip is pretrained on. future work could test this hypothesis by comparing the performance of coco-crola’s clip-based features with xc computed using inception features (as in fid) (chong and forsyth, 2020) or with dedicated concept-level purpose-trained classifiers."
1111,"limitations one limitation of our paper is that the exact distribution of the intrinsic tasks in the original corpus and the constructed data is still unknown. knowing the distribution can offer a better interpretation of the effectiveness of picl, even of the strong performance of large language models. besides, although we can find many constructed instances that share obvious intrinsic tasks (see appendix f), there still exist some instances where the intrinsic tasks are hard to identify. how to better evaluate the contribution of these instances to the icl ability or designing better filtering approaches to select more informative data for icl is worth studying. our task-semantics encoder inevitably contains some bias because it is trained on downstream datasets, although we have tried to ensure a large number and diversity of the dataset collection. however, the final language model is pre-trained on the general corpus, and we add the full document loss, which eliminates the bias to some extent. regarding computing power, we acknowledge that our framework takes relatively large training resources in the retrieval and pre-training process. therefore, we did not conduct experiments based on extra-large language models."
1112,"limitations and issues with certain systems (leidner and plachouras, 2017). nlp research can also be used as a political instrument of power, where we can observe mutual relationships between language, society, and the individual that “are also the source for the societal impact factors of nlp” (horváth et al., 2017). in this way, nlp translation can be applied as an instrument to changing the culture of minorities as in traditional translation (cf. section 3.2). so colonizers used translation as means of imperial control and expropriation (cheyfitz, 1997; niranjana, 1992). the asymmetry of power is the cause of domination, where subaltern cultures being flooded with “foreign materials and foreign language impositions” is a real danger for minority cultures (tymoczko, 2006). schwartz (2022) discuss the need to decolonize the scientific approach of the nlp community as a whole, expressing the need for researchers to be cognizant of the history and the cultural aspects of the communities which use the languages they are working with. additionally, he proposes that our research should have an obligation to provide some benefit from our studies to the communities, an obligation of accountability (and therefore be in direct contact with their governing organizations), and an obligation of non-maleficence. the fact that many translation systems nowadays are multilingual8 also result in more multi-cultural challenges (hershcovich et al., 2022). finally, we also want to highlight the importance of discussing mt systems in a text-to-text setup. the usage of text is constrained to certain topics and varies from community to community. for instance, wixarika and quechua, languages that are spoken across all generations, are used in a written fashion mostly in private messaging apps (like whatsapp) but also have a prolific meme and facebook publication generation9. even if a certain community does not widely adopt the written tradition, there are, at minimum legal obligations of the states towards indigenous languages. for example, some constitutions recognize indigenous languages as national languages (e.g., mexico and bolivia), 8multilingual systems refer in nlp to systems capable of translating a set of languages from and to english. in some cases, they are also able to translate between languages where english is not involved. 9for example, wixarika memes: https://www.fa cebook.com/memeswixarika2019, quechua speaking group: https://www.facebook.com/groups/ 711230846397383/ binding the state to the responsibility to translate all official pages, documents, laws, etc., to indigenous languages. this has not been implemented, and this case is a highly valuable application case for machine translation to assist human translation. however, our findings also apply to speech-to-text translation and speech-to-speech tasks that would cover all languages, even with no written tradition."
1113,"limitations. it also sheds light on developing better neuro-symbolic systems in general. limitations despite the strong performance of pangu, we identify several limitations that call for further improvement. the first major limitation lies in efficiency. because pangu requires an lm to iteratively score candidate plans, it is resource-consuming in terms of both time and computing. compared with arcaneqa, which efficiently handles complex questions in kbqa, pangu is about twice as slow for both training and inference and consumes about twice as much gpu memory when using the same lm. concretely, to predict a plan of l tokens, generation-based methods involve using an lm to do l forward passes. for pangu, the number of forward passes is proportional to the number of candidate plans, which can range widely. in the future, algorithms with complexity better than o(n), n being the number of candidate plans, are desired to find the top-k candidates. that being said, we would like to note that both arcaneqa and pangu are more efficient than most existing methods due to their efficient dynamic search design. for example, pangu is 8 times faster than rng-kbqa, according to the numbers reported in gu and su (2022). nonetheless, we list efficiency as a limitation because there is clear potential for further improvement. second, though pangu has shown some promising results with codex, the true potential of enabling few-shot grounded language understanding with pangu has yet to be fully realized. we only experiment with a straightforward scoring function and have not experimented with different prompt designs systematically. in the future, we plan to try different prompt designs, retrievers, and scoring functions, including using latest techniques like chain-of-thought prompting (wei et al., 2022). third, though orthogonal to the general framework of our proposal, in our current instantiation, we assume gold plans for training. however, gold plans can be expensive to collect for some environments. exploring fine-tuning lms with weak supervision can be an interesting direction. in addition to proposing candidate plans to the lm, the agent may also respond to the lm with rewards based on its decisions (liang et al., 2017). finally, one important merit of pangu, controllability, is under-explored in this paper, because it is not very necessary for kbqa. while for tasks like text-to-sql parsing, controllability could be a highly desirable property. intruders may manipulate text-to-sql models to launch database attacks via sql injection (peng et al., 2022). with pangu, we can easily get rid of malicious sql operations in candidate enumeration. however, for generationbased methods, such controls are hard to achieve during generation because the decoding process can be shortsighted—it is difficult to tell whether the current predicted token would lead to a malicious operation several steps later. we leave exploration on pangu’s controllability to future work."
1114,"limitations although the effectiveness of the iou-aware optimal transport mechanism and the assignmentguided multi-granularity loss has been verified by empirical results on three datasets, the proposed iot framework may still suffer from imprecise boundary identification and co-reference handling, as identified in our earlier"
1115,"limitations considering that the golden labels of all instances have been given in the datasets, we directly use these labels as manual labels without performing the manual labeling process. the practice implicitly assumes that all the manual labels are correct. however, with the increase of labeling scale, problems such as (1) inconsistent labeling granularity across annotators and (2) noise in manual labels gradually emerge. how to effectively improve the labeling quality and the robustness of the clustering model are worthy of attention."
1116,"limitations our work demonstrates the feasibility of combining query rewriting and query expansion to reformulate a conversational query for passage retrieval. within our proposed convgqr, the rewriting and expansion are based on two plms trained with different data, which introduce additional training load and model parameters for storage. thus, designing an integrated model that can simultaneously generate the query rewrite and the expanded terms would be a promising improvement to our method. another limitation is that the potential answer acting as expansion terms could be generated from more resources (e.g., pseudo-relevant feedback and knowledge graph) rather than only relying on the generative plms. besides, more alternative methods for knowledge infusion can be tested to connect query reformulation with the search task."
1117,"limitations and ethical concerns of this work. limitation: data and modeling the dialogues in our dataset are made by playwright, which are slightly different from daily chat. second, the automatic evaluation metrics for the response generation task can not perfectly reflect the interactiveness of dialogue system. lastly, our autoregressive generative model simply add the segment embedding to the inputs. similar to the position encoding in transformer, our coarse method does not make good use of the segmentation, and lacks interpretability."
1118,"limitations although mixda achieves promising results on domain adaptation compared with baseline models, there are certain limitations. mixda is a two-stage approach, which is not fully end-to-end. our approach requires training a domain adapter and task adapter, respectively. in the future, we will explore the unifying domain and task adapters by merging them into one."
1119,"limitations one limitation of the proposed method, infinity, is that it is currently limited to fully unsupervised and not incorporating any parallel data which may lead to performance deterioration in uncommon scenarios. furthermore, it only works well with languages having limited morphology such as english and may not perform as well on languages with complex morphology. finally, the method may have low scalability to long text as it requires large gpu resources. these limitations inspire further investigation to improve the performance and applicability of the method."
1120,"limitations one potential improvement to this work is the development of a method to evaluate the accuracy of the severity measure component. we have demonstrated the effectiveness of sescore2 with a severity measure through improved kendall correlations for various types of retrieval augmented synthesis in figure 4. however, there is currently no widely accepted way to quantitatively measure the accuracy of the severity labels. this is because there is no existing dataset that can be used to benchmark severity measures. while freitag et al. (2021a,b) have released mqm annotations with error spans for each segment, these annotations often include compositional errors that prevent the evaluation of individual severity labels without also considering other errors in the sentence. a potential future direction for this research would be to create a benchmark dataset that would allow direct assessment of individual severity estimation or explore alternative methods for evaluating the accuracy of severity measures. second, we have not been able to test sescore2 on low-resource languages due to the lack of mqm-annotated testing sets in these languages. however, we have demonstrated that sescore2 can still perform well without severity estimation by outperforming top unsupervised metrics such as bertscore, bartscore and prism as shown in figure 4. this suggests that sescore2 may be useful for low-resource languages since parallel corpora are not available for most low-resource language settings. to further verify this, a potential future direction would be to create testing sets with mqm labels for lowresource languages, to test the performance of sescore2 and other learned metrics in such scenarios. lastly, since sescore2 is based on proximity between reference and model output, its capabilities for open-ended text generation tasks have not yet been fully explored. this presents an opportunity for future research to investigate the potential of this method in such scenarios."
1121,"limitations it is possible that there is a hidden effect caused by the language pair direction, model selection, or training data and its size. however, our results bear high statistical significance for cases where we desire high correlation and low statistical significance where we expect low correlation. assured by this and concerned by the large cost of training a large number of mt systems, we did not experiment with larger data or other language directions apart from limited additional experiments in tab. 2."
1122,"limitations our method has some limitations that should be acknowledged and addressed in future research. one of the main limitations is the restriction of the plm type to masked lms. while this model type has been widely used in previous studies, it may not be the only option. with the ongoing advancements in pre-trained large language models, it is possible that our method could be applied to a wider range of plm types. furthermore, we have only considered three commonly used perturbation types in this study, future studies could investigate a broader range of perturbations and how they interact with each other in determining the constituents. these limitations provide an opportunity to further improve the method and its applicability in the field."
1123,"limitations considering modality heterogeneity can promote many related multimodal applications, it is worth continually exploring. in this paper, we propose text-guided fusion (tgf) module equipped with sparse-attention to integrate different modalities in representation aspects, which is an implicit way to build the relations of fine-grained features, such as visual objects, and textual words. previous work (khademi, 2020; wang et al., 2020) has proven that graph convolutional network (gcn) (scarselli et al., 2008) shows advantages in modeling the relations among visual and textual elements. inspired by these works, we argue that explicitly introducing the relationship of fine-grained features via gcn can better guide the model to eliminate redundant features. thus it can further narrow the modalities gap and facilitate fusion for multimodal content understanding. in the future, we will bring gcn to learn multimodal relationships and boost the performance of the model."
1124,"limitations, suggesting the possible scope of improvement."
1125,"limitation & risks in this paper, we bridge the gap between discourse markers and the underlying relations. we use distributed discourse markers to express discourse more informatively. however, learning dmr requires large-scale data on markers. although it’s potentially unlimited in corpus, the distribution and types of markers may affect the performance of dmr. besides, the current solution proposed in this paper is limited to relations between adjacent sentences. our model can be potentially used for natural language commonsense inference and has the potential to be a component for large-scale commonsense acquisition in a new form. potential risks include a possible bias on collected commonsense due to the data it relies on, which may be alleviated by introducing a voting-based selection mechanism on large-scale data."
1126,"limitations this paper evaluates language models for their ability to use gender-neutral pronouns and neopronouns using a template-based dataset, misgendered. while this approach is helpful in assessing bias, the measurements can be sensitive to the choice of templates (delobelle et al., 2022; seshadri et al., 2022; alnegheimish et al., 2022; selvam et al., 2022). consequently, our findings should not be considered as the definitive verdict on the phenomenon of misgendering by language models. there are other limitations to our work that should be considered as well. we also only conduct an upstream evaluation on language models and do not assess downstream applications. our evaluation is also limited to a western conception of gender and restricted to english only. we only consider names and genders assigned at birth in the united states. subsequent changes in names or genders are not taken into account in our analysis. furthermore, our work does not take into account individuals who use multiple sets of pronouns, such as she/they combinations (them, 2021), nor does it consider the full range of nonbinary pronouns as the list continues to expand (lauscher et al., 2022). however, additional names (rare, self-created, or non-western) and neo-pronouns can be directly used with our framework to further evaluate llms. we release our full code dataset to make this easier. lastly, there are larger models that were not evaluated due to limitations in our computational budget. further research needs to be done to address these limitations for the complete assessment of accurate preferred pronoun usage by language models."
1127,"limitations in this work, we focused on en→{fr,de,cs} multimodal mt. at the time of writing, our method can only be applied for en→x mmt. it is indeed necessary to have access to a modulated object detector in the source language to extract the features and the image-text relationship exploited by our model. this type of modulated object detector is only available in english for the moment. we leave the extension of our method to non-english source languages to future work. moreover, our method requires large amount of captioning data to perform well. it is therefore computationally expensive."
1128,"limitations we strived to make this work as accessible and applicable as possible. however, as with any other research effort, it suffers from several limitations stemming from preconceived assumptions. we believe that the most important limitation of our work is the assumption of the existence of a pre-trained multilingual language model, to be used as an encoder, that supports both the desired source and target languages. though most modern multilingual language models support over a hundred languages, with over 7000 spoken languages in the world, the vast majority of languages remain unsupported. that being said, language models are trained in an unsupervised manner, meaning that only unlabeled data is required for training purposes. as such, a suitable encoder could be trained provided there is access to enough unlabeled data. this leads to what we consider to be the second biggest limitation of our work: the assumption of the availability of unlabeled target-language data. in general, raw unlabeled data is easy to obtain for most languages. however, it can represent a challenge for extremely low-resource languages. in these special cases, training an effective encoder can be an impossibility which, in turn, limits the applicability of our approach. other limitations stem from our constrained time and computational resources. our method requires a gpu with a largeenough memory to fit the transformer-based encoder which is usually more than what a personal computer gpu provides. depending on the dataset and selected batch size, our model requires between 15 and 32 gb of gpu memory. we performed all our experiments on a tesla v100 gpu with 32gb. finally, additional experiments on a more diverse set of source/target language pairs could certainly provide a more comprehensive overview of our method’s strengths and weaknesses."
1129,"limitations first, we find robustness deficiencies in metrics by comparing the evaluation differences among metrics. this applies to the case when there are metrics that do not have the same robustness flaws. if there are more latent common defects in the metrics, they cannot be identified by mrt. we leave this topic for future research. second, we use beam search to generate candidates during mrt training, but beam search is also known to have deficiencies. for example, beam search suffers from heuristic search biases and shifts statistics away from those of the data (eikema and aziz, 2020). different decoding methods may have an impact on the experiment results."
1130,"limitations first, our model is a method of approximating clustering by contrastive learning, but due to the limitations of the model structure, we cannot directly explore the performance of past clustering algorithms on this task. secondly, due to the large scale of the experiment, our dialogue generator only considers gpt-2. although the ablation study proves the effectiveness of our model, it is a limitation. finally, this paper proposes a complete evaluation framework for personalized dialogue generation. it is very effective, but the specific indicators in it still need to be discussed and further studied. in addition, the model assumes that response and persona are independent gaussian distributions in cvae. although it performs well in the experiment, it does not conform to realistic cognition."
1131,"limitations entity knowledge propagation focuses on updating lms’ knowledge about emerging entities. however, there might be cases where knowledge about existing entities needs to be updated (e.g., regime change, new champion, and renaming etc.). we intentionally exclude these cases since they can easily become intractable due to their complexity. for example, an organization changing its name could theoretically reflect a large number of entities that have relations to that organization. by investigating model behavior when a lm encounters new information which is completely unseen during pretraining, we can experiment in a controlled environment. we find ample challenges unaddressed by current research even in this setting. our experiments are conducted on english language models only. while we believe the results can generalize to multilingual models, it is conceivable that the internal representations of these models make them more or less amenable to the sorts of updating explored here. more work is needed to benchmark these techniques in broader settings such as with larger language models and newer parameter-tuning approaches."
1132,"limitations compared to a standard knowledge distillation process, our method requires additional computation when preparing training data and training the student. first, our contrastive decoding needs to perform forward pass in the teacher model one time more than greedy decoding does to obtain the perturbed plausibility for each token generated (eq. 4). second, our kd process introduces additional training data for training the student with the counterfactual reasoning objective (eq.5). besides computation cost, this work focuses on improving faithfulness of the rationales rather than performance, which is complementary to prior works which leverages rationales for improving the performance only."
1133,"limitations since our approach, tm-hgnn, aggregates every note during icu stays for patient representation learning, it is inappropriate for time-series prediction tasks (e.g. vital signs). we look forward to further study that adopts and applies our approach to time-series prediction tasks."
1134,"limitations our main focus in this work is limited to factoid information-seeking questions that typically prompt short answers. however, lexical matching is adopted by more complicated forms of qa that require complex reasoning. more precisely, qa tasks such as multi-hop reasoning (yang et al., 2018), discrete reasoning (dua et al., 2019), and causal relations (lin et al., 2019) also warrant similar systematic analysis as studied in this paper."
1135,"limitations our analysis has both methodological and technical limitations. while dependency parsers are the most robust semanto-syntactic tools available to us, we are limited both by the quality of the parser’s output and its paradigm. all automated tools make errors, and while our work uses short and simple phrases that are comparatively easy for these tools to handle, it is possible that even systematic errors could seep into the analysis. it is also possible that other semanto-syntactic tools would highlight different phenomena and improve (or worsen) the quality of the analysis. due to the dataset used, which we picked for quantitative comparison to prior art, there is an inherent bias towards concrete concepts, as they are derived from image captions. we are therefore limited in the understanding of how our method applies to more abstract concepts (say, “love” and “dignity”), potentially warranting further study. there are also concerns about the internal validity of attention maps as an interpretability tool. for example, serrano and smith (2019) argue, “[in many cases,] gradient-based rankings of attention weights better predict [models’] effects than their magnitudes.” however, for the analysis of diffusion models, gradient methods are intractable because a backpropagation pass is required for every pixel for all time steps, as stated in section 2.2. therefore, attention scores remain the most feasible method. lastly, we have consciously limited ourselves to purely making analytical observations regarding attribution and entanglement. this has arguably allowed us to cover a very wide range of phenomena and make a large number of observations, but this choice naturally limits us to not providing a method to resolve the issues we have observed with existing models, which is something we have left (and described in section 6) as future work."
1136,"limitations the proposed method is tested for a binary labeling scenario where each instance can belong to one of the labels but not both. the scenario of overlapping labeling space is not tested, nor is the scenario for multi-class labeling space. since we aim to obtain high-quality prompts similar to the base prompt, if the base prompt is very restrictive, then the suggested prompt might be the same as the base prompt. the approach only applies to two moderately sized mlm models, and the extension to other larger models is not tested."
1137,"limitations in this paper, we pre-train cclm with moderate multi-modal data, e.g. cc3m, to make a fair comparison with previous work such as m3p and uc2. we leverage large-scale vision language pretraining simply by utilizing the pre-trained weights of x2-vlm which has been pre-trained on billionscale image-text pairs in english. collecting more image-text pairs in different languages will very likely lead to further performance improvements. moreover, there exists larger public available multilingual datasets, such as multiun (ziemski et al., 2016) and opus (tiedemann, 2012). leveraging more multi-lingual datasets for pre-training should also yield a more powerful multi-lingual multi-modal model. as for social impact, multi-modal pre-trained models can be used in applications that help people with disability in one modality. our work makes these applications applicable to minority people speaking non-english, and potentially low-resource languages. in sum, our work potentially enables deep learning technology to benefit more people, and is unlikely to have direct negative social impact."
1138,"limitations there are several limitations of our work. we tried training the tn-lcfrs on the discontinuous version of the english penn treebank (dptb, evang and kallmeyer, 2011) but failed to induce any meaningful discontinuous structures. this is possibly because discontinuous phenomena in english are much less common than in german and dutch. for example, while 5.67% of the gold constituents are discontinuous in negra, only 1.84% gold constituents are discontinuous in dptb (corro, 2020). the neural lcfrs was also quite sensitive to hyperparameters and parameterization. the instability of unsupervised structure induction is widely acknowledged and could potentially be mitigated by a large amount of training data, as suggested by liang and klein (2008) and pate and johnson (2016). due to this sensitivity, we rely on dev sets for some modeling choices (e.g., rank of the probability tensors). hence, our approach is arguably not fully unsupervised in the strictest sense of the term, although this is a common setup in unsupervised parsing due to the mismatch between the unsupervised learning objective and structure recovery. (however see shi et al. (2020) for a critical"
1139,"limitations of our work in the previous section. we use two existing datasets, sst (socher et al., 2013) and imdb (maas et al., 2011), which are publicly available and commonly used in nlp research. we synthetically generate datasets of formal languages which does not require ethical consideration. we have discussed the experimental details and computational budget in detail in appendix g. the research presented in this paper focuses on analysing the inductive biases of transformers and lstms based on experiments on formal languages and subsequently we believe that our work does not raise any ethical concerns."
1140,"limitations when using our method, we have to fine-tune the upstream nmt model to construct the downstream nmt model and then datastore for the reviser training. hence, compared with the current commonlyused knn-mt variant (zheng et al., 2021a), our method requires more time for training. nevertheless, it does not introduce additional parameters during inference."
1141,"limitations limitations of bartscore++ are three-fold: • in §3.1, we propose explicit/ implicit errors to better distinguish different types of errors in generated texts. however, explicit errors only contain token-level errors that can be detected and corrected by error analysis, not involving all error types mentioned in mqm (e.g. severe fluency errors). we hope future studies can take these situations into account. • in §3.2 we can see that our proposed error analysis framework fully relies on the generation probabilities of bart to decide how to refine the hypothesis. still, we see that this framework may lead to false judgments due to unfaithful content. further research can explore how to calibrate the pre-trained models during error analysis. • in §3.3 we integrate the distance of explicit and implicit errors by simply computing their weighted sum. this can be improved by considering more factors, e.g. the overall quality of the generated text, refining iterations, and external signals. we will leave the exploration of combining these factors and designing better weighting schemes as future work."
1142,"limitations in this section, we will point out the limitations of our work, which can be summarized in the following two aspects. firstly, in the step of answer mapping (section 3.1), we only select those connectives that are tokenized with a single token as answer words, since most masked plms predict only a single word. therefore, those connectives tokenized with multiple tokens will be replaced by the most frequent answer word with the same subtype-level sense tags. we believe that this approach will filter out several meaningful connectives as answer words. in the future, we will utilize the generative model to predict the connectives between argument pairs, which can decode multiple tokens at a single mask position. secondly, in section 5.1, we can observe that multi-prompt ensembling is effective for fusing multiple single-prompts for implicit discourse relation recognition. in the future, we will explore multi-teacher knowledge distillation method for the idrr task, here teacher models are trained with different templates. in this way, we can take advantage of the different prompt templates."
1143,"limitations choice of languages our choice of languages for wikiann and ud probing evaluations were intended to strike a balance between being being typologically diverse and having data in our chosen benchmarks. however, there are major language families and geographical regions not represented in our languages (there is no indigenous language of the americas in any of our benchmarks, and no southern african language in ud or wikiann). while we expect the trends in our results to continue to hold for other languages, we believe that further investigation is necessary on more languages to confirm our hypothesis. choice of evaluation tasks one notable omission from our evaluation suite are sentence-level tasks, such xnli (conneau et al., 2018), xglue (liang et al., 2020) and crosslingual retrieval tasks. one reason is that previous work has shown that character-level models already perform well on these evaluations. in our work, we were particularly interested in situations where prior work showed character-level models underperforming subword-based models. in particular, canine underperformed at ner, especially in the high-resource conll 2003 ner dataset (tjong kim sang and de meulder, 2003). therefore, we chose to focus specifically on ner and extractive qa as typical use cases of encoder-only models. in future work, we will investigate more thoroughly the capabilities of character-level models on a wider range of tasks."
1144,"limitations despite promising, the current work still has limitations. first, the current model mainly focuses on understanding problems. the generation ability of our model has not yet been investigated. it is unclear whether our weakly supervised framework also fits generative models and transfers strong generation capability across languages. secondly, the current work explores multilingual corpora and overlooks the domain gaps in existing image resources. as argued in (liu et al., 2021), the visual appearances of objects are diverse across cultures. bias naturally exists in the distribution of images in existing v-l corpora. to develop a truly generalized multilingual multimodal model, the gap between visual distributions in different cultures should be considered."
1145,"limitations our framework relies on the availability of translation system and unlabeled data in the target language, which can not be applied to languages without any unlabeled text or translation text. the knowledge distillation step requires a certain amount of unlabeled text, while it may struggle in cases where only few hundreds of unlabeled sentences are available. it would be interesting to combine our label denoising framework with data augmentation techniques in such scenarios. besides, the boarder application to other low-resource languages, such as masakhaner 2.0 (adelani et al., 2022), and other cross-lingual sequence labeling tasks are left for exploration in future work."
1146,"limitation, our work focuses on dialect robustness and only briefly evaluates dialect awareness. future works may extend the details and criteria of the dialect-aware nlg evaluation, and we hope our work can serve as a baseline in this new research direction. our encouraging preliminary results lead us to urge researchers to consider and improve the dialect diversity during pretraining. limitations besides the limited size of the evaluation corpora and a brevity of the exploration of dialect awareness that we point out as limitations in §8, we again acknowledge the data acquisition strategy as another limitation of our work. our data acquisition of dialects requires country codes, which exclude many dialects. there is some work on getting dialectal data without country codes: blodgett et al. (2016) build a dataset of tweets that are likely to include a high density of african-american english by linking geolocated twitter data with demographic data from the u.s. census. however, this approach is limited to dialects that have strong geographic associations within the united states and which correlate with census demographics like race. similarly, abdul-mageed et al. (2018) build a dataset of city-level arabic dialects, again relying on twitter geolocation. an alternative approach that does not rely on geolocation is to translate existing corpora into multiple dialects (e.g., faisal et al., 2021; ziems et al., 2022). however, this is labor intensive and therefore difficult to scale up to the amount of data needed for pretraining. we leave to future work the question of how to build largescale corpora for dialects that do not align with easily-identifiable geographical indicators such as national boundaries."
1147,"limitations while our proposed method demonstrates high translation quality and constraint accuracy, it is important to acknowledge that the hard copy mechanism may not be suitable for certain morphologically complex languages, such as arabic. in arabic, phrases or terminologies often involve conjunctions or prepositions and exhibit varying morphological forms. unfortunately, our proposed method is not capable of effectively handling such cases, and addressing this challenge remains an open area for future research."
1148,"limitations the prefixes we use are semantically independent from the test sentences, and also semantically implausible when chained together. this is the opposite of what we typically expect in natural language, where sentences follow from some pragmatically licit prior context. while our findings are theoretically relevant to any nlp task that leverages natural language inputs, we may see qualitatively different trends in more naturalistic settings. our results are currently limited to english. certain languages have grammatical features (such as case marking) that could strongly impact on language models’ acceptability judgments, and this could affect the trends we have observed. future work should investigate similar phenomena across languages to ensure that these findings suitably general."
1149,"limitations our suggested approaches have two primary practical limitations: first, weighted sampling is restricted to languages with available running text sources for extracting frequencies. a project on extremely low-resource languages (e.g., liu et al., 2022) may be restricted to uniform and overlapaware sampling. second, as the number of seeds increases, so do requirements for training time and/or computing power. a shared task, for example, might limit itself to only a few seeds in order to assure on-time submissions. future work would benefit from a wider selection of model architectures, along with more sampling strategies, and of course a wider sample of typologically diverse languages. notably, this work reproduces the effect observed in the sigmorphon 2022 shared task (kodner et al., 2022), which found a substantial performance hit for featsnovel relative to featsattested, but not lemmanovel relative to lemmaattested. however, both this work and the shared task fail to replicate the effect observed in goldman et al. (2022), which reports a 95% performance hit on lemmanovel vs. lemmaattested. this may have something to do with differences in splitting algorithms, unmeasured feature overlap in goldman et al. (2022), or choice of model architectures."
1150,"limitations in this work, we adopt a series of strategies for optimizing the generation models when corpus scaling up. although we successfully train tome on largescale corpora, there is still a performance gap compared to mainstream dense retrieval methods under this scenario. this is also one of the limitations of current model-based retrieval methods, because this retrieval paradigm requires the model to memorize the entire corpus, unlike dense retrievers that have strong generalization capability for different documents in a large corpus. in addition, effective training on large-scale corpus also requires largescale computing resources (up to 32 tesla a100 80g gpu) and long training time, which will indirectly generate risks of energy consumption and emissions."
1151,"limitations a qualitative analysis of distractors generated via mt shows that this method can produce some inadequate candidates (and so do word2vec and bert-based methods). thus, a human-in-the-loop is needed to ensure the validity of the generated distractors. however, human-in-the-loop is standard practice, when producing language exercises and tests (attali et al., 2022). we therefore believe that the proposed approach does not need to be fully automatic to be useful, as it can still help speed up distractor generation to create advanced vocabulary exercises. the mt method can thus be of huge help to human test developers. the mt approach can be computationally more expensive than the methods proposed in prior work such as bert and word2vec. although we make use of pre-trained mt systems, the approach can be still costly, as it requires running two mt systems (forward and backward) with each pivot, and a bert-based word alignment model to align the carrier sentence with each of its 900 back-translations. in terms of cost comparison, it takes 1-2 hours in a single nvidia tesla a100 gpu to generate 900 translations and produce candidate distractors for a single pivot, versus 0.5 hour with bert and word2vec. however, the mt approach can potentially offer advantages that other methods cannot, such as producing a more diverse pool of distractors and, importantly, relating the native language of the learner to the pivot systems used to produce distractors. as our analyses show, each pivot system generates unique distractors. we stress that, while we show that using multiple pivots generates diverse distractors, we leave the question of whether using a pivot based on learner’s first language is useful, to future work. we do hypothesize, however, that using pivots tied to the first language might be useful, however, but verifying this claim is left for future work. this is because verifying whether tying the pivot to learner’s native language would be useful would require a human study with a relatively large group of learners of at least 20- 30 students (all of advanced level) that all share the same first language. in fact, we would need to have several groups of learners, such that students in each group have the same first language background. this would be a large-scale study that is out of the scope of the paper. note that the current work already presents a human study with 32 students that demonstrates that the automatically generated pivots are of the same difficulty as those created manually. we also note that the method requires relatively good mt systems for generating more difficult distractors. finally, our study is limited to cloze items that include single words as targets and does not consider fixed expressions, such as phrasal verbs and idioms. in the language testing community, such expressions are typically tested separately from the generic cloze items. the basic approach is to detect them before the carrier sentence is cleared to be used for cloze exercises. our current work is not focused on carrier sentence selection. but it makes sense to include this consideration in a larger suite of tools for cloze item generation."
1152,"limitations in our study, we address the issue of linguistic forgetting via the injection of the strict non-linguistic skill of quantitative reasoning. although quantitative reasoning with llms is an active research area, as discussed above, further fine-grained studies are required to extrapolate this behavior to tasks that leverage synergies between aspects of both linguistics and non-linguistics - such as math word problems or data-to-text generation. further, investigations into the linguistic forgetting tendencies of different languages would lend an insight into the role of linguistic morphology in this behavior. the restrictions from our in-house gpu resources does not allow scaling this study to more recent models that exceed 100 billion parameters, although, due to the sharing of similar architectures, we forecast our findings to hold despite of model scaling."
1153,"limitations our study leaves room for future work. first, we would like to highlight the difficulty of applying the validity assessment framework from measurement theory to instability measures. for example, in §5.1, our low convergent validity scores may have different interpretations because there are no well-established instability measures. further, in §5.2, because no previous studies have built theoretical foundations of factors that impact the prediction and representation instability, both our tests do not rigorously follow the concurrent validity definition: our first test of successful and failed runs is based on an assumption derived from observations of mosbach et al. (2021) rather than theory, and our second test of differences among test datasets examines the consistency between theoretically indistinguishable groups instead of the differences between theoretically distinguishable groups. second, we only experimented with a limited number of tasks, instability measures, plms, and validity types. future work can use our framework to further validate the generalizability of our observations. for example, to apply our validity testing framework to larger datasets, to include other measures (e.g. functional similarity measures, csiszárik et al., 2021 and jitter, liu et al., 2022), to study generative plms (e.g. t5, raffel et al., 2020 and opt, zhang et al., 2022), and to test other types and validity (e.g. discriminative and predictive validity). third, we focused on general text classification tasks in this paper. one promising direction is to investigate which measures to use for specific settings. for example, to extend our framework to more recent generative models (e.g. bart, lewis et al., 2020 and gpt-3, (brown et al., 2020)). however, in this case, because our prediction measures in §3 are only useful for classification, new prediction measures should be developed, and our tests should be adjusted accordingly."
1154,"limitations of mitigation methods. in this section, we suggest possible analyses, along with illustrative case studies."
1155,"limitations of existing datasets. in turn, fairprism provides a richer lens for diagnosing (1) the types of fairness-related harms that ai text generation systems cause, and (2) the potential limitations of mitigation methods. the process we followed to develop fairprism offers a recipe for building improved datasets for measuring and mitigating harms caused by ai systems. in addition, since we limited the scope of fairprism to stereotyping and demeaning harms relating to gender and sexuality, future work could create similar datasets for other demographic groups, such as those based on race, ethnicity, religion, age, national origin, or disability status. limitations fairprism is limited to fairness-related harms relating to gender and sexuality. it contains only english text, primarily represents varieties of english used in the u.s., and the annotators who labeled the examples were from the u.s. and canada. as a result, it is less well suited to measuring or mitigating harms relating to other demographic groups, harms specific to other countries, and harms in other languages. in addition, the social bias frames dataset, from which we obtained some of the human inputs, consists of text from social media sites, so it may not reflect typical interactions with ai text generation systems. some of the constructs we attempted to operationalize have competing definitions, which may affect the range of harms covered by fairprism. for example, our definitions of stereotyping and demeaning harms may have caused annotators to label some stereotypes, demeaning content, or forms of phrasing as harmful more easily than others. annotators may also have used implicit criteria when labeling examples (e.g., equating explicit language or particular language varieties with harmful text, despite our instructions to the contrary). in addition, our focus on stereotyping and demeaning harms excludes other types of harms. for example, allocation and quality-of-service harms are not covered by fairprism, nor are harms that stem from the use of ai text generation systems more broadly, such as questions of power and agency that relate to who is able to design or use these systems. unintended uses as a result of fairprism’s limitations, we do not intend it to be used for any of the purposes outlined below. access to fairprism is restricted as a preventative measure. to request access, please send an email to fairprism@microsoft.com detailing your desired use case for us to review. as training data for generating hate speech. illintentioned actors could train models on fairprism for the purpose of generating hate speech. as training data for mitigation methods. directly using fairprism to train classifiers for mitigating fairness-related harms prevents it from being useful as a measurement instrument. furthermore, fairprism is not sufficiently large or comprehensive to be effective for training mitigation methods. as a benchmark to be “beaten.” if ai systems are repeatedly trained to improve on any single aggregate metric calculated using fairprism, this will result in overfitting to the dataset, which will make the dataset less useful for measurement and may lead to a greater proliferation of harms that it does not cover due to a false sense of complete coverage. application mismatches. fairprism contains examples of text generated in both reply scenarios (e.g., autoreplies or chatbots) and continuation scenarios (e.g., writing emails or generating stories from a prompt). its efficacy will therefore lessen for applications that are further removed from these scenarios (e.g., it is not intended for measuring harms in human-authored text) and for applications that are highly specific (e.g., medical chatbots). fairprism is also less well suited to measuring or mitigating harms relating to demographic groups other than those based on gender and sexuality, harms specific to countries other than the u.s. and canada, and harms in languages other than english."
1156,"limitations while our approach shows promising results in both automatic and human evaluation, it relies on two significant pillars: a strong entailment model and a strong initial summarization model. the nli model implicitly encodes the biases and other data regularities that were part of the nli training set into the generated summaries of our policy. this is well demonstrated by the gap between human attribution judgements and the automatic nli metric. our rl policies cannot improve on factual consistency errors if they are undetectable by the nli reward. hopefully, as nli capabilities get better, so will the efficacy of rlef and the abilities to automatically flag hallucinations and contradictions. secondly, a strong summarization model is essential for our method in two ways: as an initialized starting point for rl exploration and as an anchor point to a policy. while our rl training does not require any reference data and opens the possibility to use more un-summarized documents, it would probably not succeed as well without initializing from a high-quality supervised model. another limitation is that our experiments suggest that model size is important when using rlef (figure 4): both our summarization and nli models are 11b parameters models. we believe it is important to further understand how to make our approach more robust to smaller models, to increase its computational efficiency and availability."
1157,"limitations since this work relies on the in-context learning ability of large language models, the challenges associated with computational resources to load an llm ensue. due to resource constraints, we could not use larger or commercially available llms to validate if the advantages of x-insta translate to those models as well. as we observed in section 3.5, the static nature of the aligners poses a limitation on x-insta. moreover, these aligners are manually designed. therefore, task-specific, trial-and-error style manual intervention is needed. we believe a better understanding of the pretraining distribution of the multilingual llms can pave the way toward better automated alignment methods. there are multiple shortcomings of monolingual icl that entail its cross-lingual counterpart and x-insta does not address them; issues like knowledge hallucination, limited common-sense reasoning, inconsistency in retrieving factual associations, etc."
1158,"limitations although our proposal enjoys the advantages of validity and generality, there are still two major limitations. first, vlp cannot directly generalize to the inductive setting, since vlp is defined based on the score functions of transductive embedding models. one potential direction is to design an inductive reference selector for emerging entities. second, how to efficiently select more helpful references for prediction is still an open challenge. we expect future studies to mitigate these issues."
1159,"limitations in this work, we have identified two key limitations of coad that can be further examined in future research. the first limitation is that coad only allows for the querying of one symptom at a time, making it unsuitable for scenarios where multiple symptoms are present. however, coad has superior performance in the main metrics for automatic diagnosis. to relieve this limitation, potential solutions include relaxing symptom feedback conditions and allowing the model to produce symptoms sequentially until a stop signal is encountered or querying the top k symptoms in a single turn. additionally, coad has some restrictions on input format, requiring standardized symptoms and values. to make it more applicable to end-to-end settings, an natural language understanding module (nlu) is required to parse plain text and obtain the input symptom sequence, and a natural language generation (nlg) module is needed to translate the predicted symptom or disease to text. the ultimate goal of automatic diagnosis is to support the dialogue between doctors and patients, after coad determines the symptom or disease, rulebased nlu and nlg modules can help to achieve the text to text communication."
1160,"limitations we identify the major limitation of this work is its input modality. specifically, our model only considers textual inputs, ignoring question answering tasks in vision and audio. a multi-modal question answering model under realistic open longtailed scenario is worth further exploration. fortunately, through multi-modal pre-training models (xu et al., 2021; huo et al., 2021) and question answering methods (kim et al., 2020), we can equip our model with multi-modal question answering ability. for future work, learning multi-modal question answering in an open (including out of distribution data (lang et al., 2022, 2023a,b)) longtailed scenario still remains a challenge, and we will continue to work on it."
1161,"limitations while our work displays many strengths, we highlight some limitations. first, we focus on python for programming language evaluation, which is one of the most widely used programming languages. however, we believe that our proposed approach, contraclm, would benefit code lms trained on any programming language. second, the empirical findings presented in this work are mainly based on the smaller versions of gpt-2 and codegen with 124m and 350m parameters, respectively. however, as shown in figure 1b, by continuing to train the pretrained models with our proposed objective, contraclm is able to address not only the isotropy and poor discrimination issue that both gpt2-small and codegen suffer from, but also improve the representation quality of gpt2-large which has a good starting point for both isotropy and discrimination. therefore, we believe the effectiveness of contraclm should be applicable to larger versions of these lms, regardless of whether they suffer from the anisotropy issue (e.g., large codegen models) or not (large scale gpt-2 models). we leave the explorations of larger models as future work."
1162,"limitations unfortunately, we cannot access most sighan2008 bakeoff datasets, which were proprietary but used by many previous works. this makes the comparison in table 2 a little unfair. we argue that we replaced these non-accessible datasets with the ones publicly accessible (including ud, wtb, and zx). we note that huang et al. (2020b) faced the same limitation as us. thus they also replaced datasets just as we did, which makes them the only directly comparable work to ours."
1163,"limitations although our model obtains satisfying results, it also exposes some limitations. first, for a fair comparison to other models, we mainly carry out relevant experiments on pdtb 2.0. due to the lack of baselines on pdtb 3.0, further analysis and comparison cannot be conducted. second, in our experiments, we can find out that the hlr method does not improve the top-level or bottom-level results effectively, indicating that with the increase of the level, the refining method is insufficient to continue to generalize the bottom-level labels and further improvement should be made according to the specific features of the idrr task. third, due to the limitation of space, this paper does not focus much on semantic weight for the refining of sub-labels. this is a very broad topic involving the rationality of the discourse relation annotation and the interpretability of the label embeddings. we will conduct a further study which may appear in our next work."
1164,"limitations we explain model pathology from a classification perspective, but the pathological nature may exist in language models for performing various tasks, such as reading comprehension, textual entailment, and visual question answering. although our proposed regularization technique may be applicable to various tasks, we have only investigated its effectiveness in classification problems. further evaluations are expected to be conducted in future works. the proposed method also leads to more time-consuming training, primarily due to the generation of adversarial examples, while only a minimal amount of time is spent on generating out-of-distribution examples."
1165,"limitations our analysis is primarily limited by the accuracy of underlying nlp models used in our character event extraction pipeline. for example, booknlp does not cluster nominal mentions of characters (""the girl"") with the corresponding character names (""cinderella""). this results in character event chains that do not account for all of the character’s actual events. using allennlp to extract all action verbs in a sentence as the event triggers meant that not all of our events were on the same dimension: some events were intended or thought of, while others actually happened. additionally, narrative events that are described in ways beyond just action verbs are not extracted. (for example, the event of a kidnapping might be described as two separate actions: a character picking up another character and running away.) our salient event identification algorithm might also filter out many events of analytic interest. both characters whose gender are not specified in the story or who are gender-less are classified as “unknown”. there is no explicit way to extract non-binary characters as models tend to label uses of the pronoun ""them"" as plural. thus, the current implementation is lim- ited to comparisons of female and male characters which perpetuates a gender binary. our use of bootstrapping to calculate confidence intervals and determine statistical significance is valid under the assumption that the original fairtytaleqa sample is representative of all fairy tales. as the sample was collected only from popular open-source stories, this assumption may not hold. lastly, bias exists beyond just gender groups and gender itself intersects with other social groups. we plan on expanding this component to include attributes such as race and ethnicity, age, and socioeconomic class. the cultural comparisons and overall analyses were too limited as the fairytaleqa dataset is very eurocentric with most fairy-tales coming from northern and western europe (table 4 in a.3. only some stories income from east asian, southern european, or indigenous north american cultures. meanwhile, almost no fairytales are included from south america, the middle east, africa, south asia, or south east asia. unfortunately, after considering the break down of event chains by gender and culture, the samples were too small to observe robust trends."
1166,"limitations although futuretod achieves significant improvements over existing baselines, there are some directions to explore for future work: (1) in this paper, futuretod doesn’t use any data augmentation strategies to enhance representations. we believe existing augmentation methods will benefit further improving performance. (2) we design a simple technique of constructing the teacher. more complicated methods should be considered, such as multi-teacher and large teacher. (3) futuretod in this paper cares about dialogue understanding tasks like intent detection, dialogue state tracking, etc. we hope to extend the similar idea to the generative dialogue pre-trained models and larger tod corpus. besides, exploiting limited dialogue labels is also valuable to explore."
1167,"limitations we acknowledge a few limitations in this work. first, peacok cannot be comprehensive. persona knowledge is very broad and our resource cannot cover all dimensions of personas, nor all attributes of these dimensions. we select five dimensions of personas that we found salient from background literature in human interaction, and we distill attributes for these dimensions from atomic2020, comet and instructgpt-3. these resources, while rich in knowledge, only represent a subset of possible background resources for the construction of peacok(among other kgs and pretrained language models). furthermore, the primary language of these three resources is english, making peacok a solely english resource. finally, in downstream narrative experiments, the usage of our augmented persona knowledge is constrained by the capacity of baseline model, which leaves for future work the exploration of downstream persona knowledge augmentation on a larger scale."
1168,"limitations the simulated dialogues constructed by kidg are a powerful source of training data for retrieval-free knowledge-grounded dialogue systems. however, there is a clear style difference between the generated utterance and the original document sentences: one is the oral expression and the other is a more formal style. but as shown in table 5, the pdms trained on kidial appear to be more proactive and knowledgeable during conversations. the generated utterances serve as a type of prompt to help the model understand the knowledge. in the meanwhile, our kidg embeds the knowledge into different contexts, alleviating the one-to-many problem in some degree. although generating dialogues needs to cost gpu resources, it is still a cheaper and quicker way to acquire large-scale knowledge-intensive dialogues."
1169,"limitations in this paper, we propose a novel concept of dense retrieval, the matching representation. based on this, we introduce a novel generalizable dense retrieval training method via training the balanced and extractable representation for matching (berm). despite the strong performance of our method in improving the generalization ability of dense retrieval models, more theoretical proof needs to be researched to gain the deeper understanding of generalization improvement. especially for matching representation, more theoretical analysis and implementation will be discussed in future work. we believe that the deeper study of matching representation will promote the development of dense retrieval, because it not only alleviates the problem that query and passage cannot interact in depth during training, but also describes the essence of retrieval task."
1170,"limitations with gpt-3 we only perform a subset of our evaluations of structured prompting on gpt-3, due to the cost of running the models in the api; this also means we do not run comprehensive prompt ablations to better tailor the setup for these models. additionally, the results (i.e., lower performance than comparable gpt-neo models) are difficult to interpret due to the black box nature of the gpt-3 models – it may be due to pretraining data differences (as mentioned in the previous limitation), the lack of prompt engineering for the models, or some other discrepancy. english-only experiments the experiments in this paper focus on english sequence tagging tasks, and it is unclear how well the proposed method generalizes to other languages. we find evidence of task-relevant data in pretraining corpora in nonenglish languages, which suggests there is signal for the approach to work in other languages. however, prior work shows that plms behave much worse when prompted outside of english (lin et al., 2022; shi et al., 2022) but does not address the effect of pretraining data on this phenomenon."
1171,"limitations in this article, we focus only on meeting minutes, speech, and press conference data. many other text datasets such as transcripts from congressional and senate testimonies, beige books, green books, etc can be incorporated to understand pre-fomc drift better. we don’t use audio or video features in constructing the measure, which might contain additional information. it can be an interesting future study to compare measures generated from fomc text with an alternate measure that can be constructed from the news or social media data. in dataset construction, while splitting sentences, we use a simple rule-based approach. we leave it as an open problem for future researchers to find better methods for splitting sentences with opposite tones. in our trading strategy construction, we do not include transaction fees as it involves low-frequency trading. in the future, one can use our model and data to construct a high-frequency trading strategy as well. in addition, a more comprehensive zeroshot and few-shot generative llm benchmark with open-source models can be performed to provide a better comparison."
1172,"limitations elaborated relation descriptions are the foundation of the matching-based methods to achieve superior performance. although we have proposed some ways to enrich the entity information in the descriptions, it is still a promising direction to explore more diversified and effective ways to enrich relation description (e.g. ensemble of multiple descriptions). we leave this as our future work."
1173,"limitations considering the wide spectrum of llms’ applications, not only defining social sensitivity on llm-based generation is not trivial and explicit but also completely addressing all the socially sensitive issues might not be feasible. therefore, our square mainly focuses on socially sensitive questions with three categories and their acceptable responses with six types for safer applications of llms, by in-depth"
1174,"limitations our work focuses on building a two-stage framework for generating and learning from explanations. in our investigation, we are limited by the available computational resources, financial budgets, and datasets. gpt-3 and pet are performant few-shot learners that work well for our use case. however, gpt-3 is not free to use and partly for financial considerations, we did not experiment with gpt3 in-context learning initially. the performance difference between gpt-3 babbage and davinci are aligned with the emergent abilities of largescale language models (wei et al., 2022a; rae et al., 2022). therefore, in the era of research with private large-scale language models, it would be useful for the research community to collectively build knowledge about how large-scale language models work. it would be useful to experiment with other models such as google’s palm (540b) (chowdhery et al., 2022b) and deepmind’s gopher (280b) (rae et al., 2022). it is an important question for the research community to explore productive paths forward. often, prompt engineering requires either significant manual work to come up with good templates (brown et al., 2020; schick and schütze, 2020) or a big budget to run automatic prompt generation methods (lester et al., 2021; wu et al., 2022). in this work, we used a fixed prompt (see appendix c.1) for explanation generation, future work could also investigate from the angle of generating better prompts. we experimented with two natural language inference tasks, which tend to correlate with a certain form of explanations. one way to interpret the difference in our findings and chain-of-thought prompting is indeed that the reasoning in e-snli and e-hans are not the multi-step reasoning used in arithmetic reasoning. as tan (2022) argues, there are diverse types of explanations, which may lead to varying levels of effectiveness from a learning method. future work could investigate the effectiveness of our method on other tasks and different types of explanations. while our method demonstrates effectiveness against strong baselines, there is still a big gap from the upper bound performance and suggests potential for better use of the explanations in future work. for example, future work could incorporate careful example selection into learning with explanations. we picked examples randomly, but research has shown that calibration (zhao et al., 2021) reorder- ing (lu et al., 2022) and example selection (liu et al., 2021) changes gpt-3’s behavior. we also used human explanations to fine-tune the gpt-3 model for explanation generation, but human explanations may not always be high-quality or the best guide for machine learning models. additionally, we use roberta as our backbone model for the classifier used in both the non-gpt baselines and our flame framework. we manage to beat stronggpt-3 baselines that use explanations. while more powerful classifiers (e.g., deberta) could also be used in place of roberta, we believe we have demonstrated the effectiveness of our method by using a simpler classifier. we leave it to future work to investigate the effectiveness of our method with more powerful classifiers. finally, it is worth noting that we use a particular setup of k = 16 for our experiments. while we believe that this is a reasonable few-shot learning setup, results could differ for different k. we leave it to future work for examining the impact of examples, explanations, and number of samples. broader impacts we propose a framework to generate and learn from explanations and conduct in-depth analysis to understand the utility of explanations. our work has the potential to help people understand the behavior or usage of large-scale language models and improve their trustworthiness."
1175,"limitations in this part, we show limitations of our work by categorizing wrong predictions outputed by our method clever into two groups. the first type of error is induced by the unconspicuous biased features of claims. for example, the claim scandinavia includes the remote norwegian islands of svalbard and jan mayen. does not contain obvious biases so that the output of claimonly model cannot represent the biased distribution. therefore, subtracting such output fails to mitigate biases but reduces the beneficial claim information instead. these errors may be avoided by employing different strategies for instances with distinct bias extents, which we leave as future work. the second type of error occurs when high-level reasoning is required, e.g., mathematical computation and multi-hop reasoning, which drops into the scope of model reasoning ability. this work mainly focuses on debiasing fact-checking models that make them concentrate on the intrinsic evidential information. after debiasing, how to enhance the reasoning ability over such information is a promising future direction."
1176,"limitations we discuss the limitations of our framework as follows: (1) in this paper, we take an initial step on the robustness of the summarization system by focusing on word-level perturbations in the input document. however, in practice, the robustness of the summarization models is reflected in many other aspects. for example, the summarization performance towards sentence-level or document-level perturbations is also a kind of robustness. (2) although dasum greatly improves the generation quality compared with other augmentationbased models, it requires more computational resources with respect to the augmented dataset construction process. for large-scale datasets with long text (e.g., bigpatent (sharma et al., 2019)), it is worth considering the time complexity of transformer architecture."
1177,"limitations the primary limitation of the proposed model is computational efficiency. specifically, during the training phase, the input size of the model is more than double that of traditional models, which is due to the inclusion of both predicted and gold templates. besides, the source sentences are transformed into longer sequences, resulting in an increased memory footprint and longer training time. additionally, both during the training and testing phase, an additional step of preparing detection labels for the data further contributes to the increased processing time. in future research, we aim to investigate methods for achieving comparable or superior performance while reducing the input size and addressing these limitations, building upon the foundation of our current work. additionally, templategec does not support the joint training of the seq2edit model. we will further explore how to jointly train the seq2edit model in future work, particularly focusing on the continuous modeling of detection labels based on an end-to-end model."
1178,"limitations that are revealed in our work, such as multiple training procedures or hyperparameter tuning for each method (e.g., how much we allow accuracy drop during layer pruning). limitations although our method well estimates the ambiguity without additional resources as well as boosting model latency significantly, there are a few limitations. first, our method requires additional training procedures, such as training the internal classifiers and kd. for this, we may fine-tune the original model and internal classifiers simultaneously. another limitation is in setting the hyperparameters. we allow the drop of accuracy by 1% to determine the target layer for layer pruning and the value of λ for kd, but this could be subjective and differ depending on the researchers’ experience. finally, we validated our method with a limited number of benchmarks since most of datasets have been released with only aggregated gold labels (uma et al., 2021)."
1179,"limitations the main limitation of this work is the quantity of annotated human reflections. overall, 15 human reflections are annotated, which are outnumbered more than 7:1 by gpt-2 reflections and 9:1 by gpt-3 reflections. if there were more human reflections annotated, we may be able to confirm, among other potential findings, that gpt-3 reflections were indeed significantly more often annotated as coherent compared to human reflections. we also note that the laypeople had a longer between-stage waiting period than the experts, because we could not enforce a similarly long waiting period for the experts due to practical reasons (appendix c). while an ideal setup would keep the same waiting period duration, appendices c and d show that the duration difference is not critical. furthermore, we adopted sequential annotation for reflections within a batch to make the interface easier to navigate for the human annotators, but this also means that the early samples in a batch might indirectly affect the annotation of the later samples. we leave more investigation on this to future work."
1180,"limitations similar to many knowledge graph embedding models, our proposed method is yet to handle link prediction under inductive settings. one possible future extension is to leverage entity description information to generate textual features and use compounde as a decoder to handle unseen entities. also, the affine operators we use are limited to translation, rotation, and scaling and this may limit the number of different relation patterns we can handle. in the future, we can include all affine transformations and investigate their difference. also, because we use 2d givens rotation matrix, the embedding dimension setting needs to be a factor of 2. we can explore higher dimensional transformations such as 3d transformations and compare the modeling power."
1181,"limitations as in-context learning with llm heavily depends on the selected exemplars in the prompt, the performance of kb-binder might vary from different subsets of randomly sampled examples, especially in a low-shot setting. but kb-binder still shows strong performance on thousands of data points on each testing dataset with randomly sampled exemplars, which verifies the robustness of our method to a degree. in the meantime, the performance of kb-binder is restricted with the one-time generated drafts from the perspective of the imaginary frame and schema items of the preliminary logical forms, which can be further improved with interactively generation and retrieval. moreover, we have not explored whether the performance can be further improved with explanation/instruction during the stage of draft generation. we will take these limitations into account and mitigate them in future work."
1182,"limitations we identify two main limitations of programfc. first, despite being complex in their surface form, the claims in the hover and feverous datasets mostly require only explicit multi-step reasoning, i.e., the decomposition can be derived from the claim’s syntactic structure or how the claim is framed. this lowers the difficulty of generating reasoning programs. however, for many real-world complex claims, the reasoning is often implicit. for example, for the claim “aristotle couldn’t have used a laptop”, the reasoning program is: answer_1 = question(“when did aristotle live?”); answer_2 = question(“when was the laptop invented?”); fact_1 = verify(“answer_1 is before answer_2.”); label = predict(fact_1) generating reasoning programs for such implicit complex claims requires a deeper understanding of the claim and also access to world and commonsense knowledge. we conducted preliminary experiments on these types of claims, but we found that our codex-based generator struggled to produce a correct reasoning program. this highlights the gap in applying our programfc to fact-check real-world claims. addressing these challenges is an important direction for future work. second, programfc incurs a higher computational cost than baseline end-to-end fact-checking models. it requires calling large language models for program generation and further calling multiple sub-task models. this results in the actual computational time that is ∼4–5× higher than for an endto-end flan-t5 model. developing more efficient methods for program generation and execution is an important direction for future work."
1183,"limitations in this paper, we simply prepend the retrieved prompt to the input embeddings before encoding. a well-designed method of combining prompts with the input embeddings, such as prefix tuning (li and liang, 2021), may result in additional enhancements. finally, as observed in section 4.2, prompt-based fine-tuning does not present obvious superiority over standard fine-tuning. exploring the prompt tuning on cross-lingual natural language understanding is a challenging task that has recently gained attention (qi et al., 2022; huang et al., 2022b), and we leave it as future work."
1184,"limitation calculating complexity indices for large-scale graphs can be computationally expensive and time consuming. some of the complexity indices show longer turnaround time when computed for denser areas in the graphs. in addition, as we mentioned in the paper, although we made sure our framework and implementation allows adding any number of additional indices in a modular way, there might be other effective complexity indices that are not included in this investigation. furthermore, it should be noted that the model has been exclusively tested on graphs where nodes contain textual content, which may limit its application to more general graph types. finally, the model has not been applied to other graph-based tasks such as clustering and graph-level classification."
1185,"limitations inappropriate initial user questions can negatively affect ner performance. if they are not proper, the qa model returns incorrect phrases, and the phrase embedding queries generated from them will also be erroneous. the absence of a component for controlling this error cascade in our framework should be addressed in future studies. in addition, our method is dependent on the phrase encoder of densephrases. because the phrase encoder is a general-purpose model trained on wikipedia-based datasets, its capability may be limited for domain-specific entities. in fewshot ner, the phrase encoder can be sensitive to the quality of given example sentences. future studies should thoroughly analyze the effect of the phrase encoder’s performance on the resulting ner datasets and ner performance."
1186,"limitation, more results can be found in appendix b. accuracy-oriented methods can still hardly reduce the inference efficiency. yet, our proposed same effectively reduce the speedup ratio by 89%, which is comparable to 93% on base-size models. impact of modification rate: in our main results, we set the allowable modification rate ϵ as 10% of the input words. we further investigate whether same can reduce the inference efficiency under lower modification rate (imperceptible attack). the experiment results across glue benchmark on deebert-base and pabee-bert-base under are summarized in table 7. even constrained with a very low modification rate, e.g., 3%, both variants of same can still significantly reduce the model’s efficiency. in addition, with increasing modification rate, same leads to higher reduction in efficiency. ablation study: to understand the inner mechanism of same, we conduct ablation studies on each component. as shown in table 8, solely using heuristic loss can already lead to significant effi- ciency drop. in addition, using loss combination, and adding layer-wise importance weights can both further increase the high computation ratio. finally, same utilizes all the sub-components, which leads to the lowest inference efficiency. semantic similarity: while we constrain the modification rate in our experiments to keep the semantic meaning consistent, the semantic similarity between benign and adversarial examples is not explicitly constrained. therefore, we further investigate the sentence semantic similarity between original and adversarial examples on sst2 dataset. specifically, we first obtain the sentence representations of adversarial and original sample with a state-of-the-art st5-large embedding model (ni et al., 2022), and then compute their pairwise cosine similarity. with deebertbase and pabee-bert-base as the victim model, the same-word has an average cosine similarity of 0.89, and same-char has an average cosine similarity 0.96. the results suggest that both variants of same can well preserve the inputs’ semantic meaning, at the same time, reduce the efficiency of dynamic transformers. visualization: to illustrate the impact of efficiency-based v.s. correctness-based adversarial perturbations, we present a case study of adversarial samples produced from sst-2 dataset in table 9. for better explainability, we show examples with one-word only modification. due to space limitations, more adversarial samples generated using same can be found in appendix c. as shown in table 9, our efficiency-based method will perturb the word but to bujt, thereby altering the explicit turning relationship between two sentences. while humans can make the correct prediction even without the word but, it can be challenging for dynamic transformers to infer the turning relationship in the early stage. therefore, they fail to satisfy the exiting conditions, resulting in reduced inference efficiency. in contrast, correctnessbased approaches will keep the transition word and adversarially modify the word deeper, e.g., to deper with textbugger. with the transition word but, the model will emphasize more on the latter sentence, and easily get a high model confidence."
1187,"limitations firstly, our proposed same is for the white-box attacking scenario only, which is less practical in real-world scenarios. however, experimental results on black-box transferability show that a blackbox efficiency-oriented attack is highly feasible. therefore, we leave the black box same as a future study. secondly, we mainly study multi-exit transformers for sentence classification tasks in this work. we notice that several recent works extend the idea of multi-exiting to other nlp tasks, e.g., sequence labelling (li et al., 2021), text generation (schuster et al., 2022). for classification tasks, same slowdowns the models by avoiding early exiting. while for text generation tasks, in addition to avoiding early exiting, ones can also slow down the model by forcing the model to produce a longer sequence. we leave the exploration of other dynamic models to future work. thirdly, as the first work that evaluates the efficiency robustness of dynamic transformers. we use a relatively simple permutation strategy. although these permutations can lead to severe performance degradation, they might not be imperceptible enough. yet, they could be easily replaced by other sophisticated permutations under same framework."
1188,"limitations the creation of templates still requires native speaker expertise and an understanding of a language’s grammar. morphological inflection models are imperfect so morphological forms may need to be enumerated to ensure highquality tests. we leave model-in-the-loop template creation and improving morphological inflection models for future work. while we design representative templates with thousands of permutations for each capability, a larger set of templates and arguments may be necessary to ensure a comprehensive coverage."
1189,"limitations wtp performs comparatively worse in some lowresource languages (e.g. welsh, nepalese, punjabi, pushto). this may be attributed to quality issues of mc4 in these languages (kreutzer et al., 2022). in addition, we find that the adapted wtppunct classifiers generally do not transfer well across languages and dataset collections (appendix d). finally, although bias is less obvious in segmentation tasks than e.g. generation, wtp may be biased by performing disproportionately well on text by communities which are overrepresented in the training data, while performing worse on text from underrepresented communities. we try to minimize this form of bias by sampling text from all languages uniformly."
1190,"limitations due to the limited number of available code-related downstream tasks, we did not evaluate our attacks against other code-related tasks. there are several limitations to our designed attack. while the attack can be applied to any downstream seq2seq task for the generation task, compared to those attacks designed for a specific scenario or task (schuster et al., 2021), our backdoor threats are less harmful and can be manually checked to detect and remove bugs or faulty logic introduced by these attacks. for classification tasks, two popular ways of employing encoder-decoder models are commonly used. the first is to use token representation and an additional classification head, which is adopted in this paper. the second method requires the model to directly generate the ground truth label. if the victim users adopt this paradigm, the implanted backdoor will not be activated because the model doesn’t use the ’eos’ token representation for classification. ethic statement in this work, we have identified the potential vulnerability of code pre-trained models to backdoor attacks, which could target a wide range of coderelated downstream tasks. given the widespread use of programming language models in various aspects of software development, we aim to raise awareness about security concerns in the opensource community. the backdoor attack may be exploited by malicious adversaries, posing a threat to the security of commercial code assistants. for example, attackers may implant backdoors in programming assistance models (e.g., copilot), leading to code with vulnerabilities. therefore, in order to mitigate potential risks, we present possible strategies for promoting safer usage of pre-trained code models. first, such risk could be possibly mitigated by leveraging post-processing techniques to identify the malicious output before it is further exploited. detailed"
1191,"limitations one limitation of this work is that both ptw masking and mrd are conducted only on bert due to limited resources, and mlms with other structures may have different reactions to the timevariant masking with different contents and ratios. another limitation is that although we propose mrd for the first time, the strategy of time-variant masking ratio is hard to design like learning rate decay. in fact, other decay methods and choices of starting and ending point are various, where better strategies may exist and further work can be done."
1192,"limitations potential limitations to our work can mainly be attributed to two factors: 1) the fact that we run our experiments using the icelandic language, and 2) inherent biases in the corpora we use. icelandic is north germanic language (along with faroese, norwegian, danish and swedish). as such, it is both germanic and indo-european. while we are fairly confident that our results hold for these languages, different results may hold for other languages, particularly those not using latin script or those using logograms, such as chinese characters. the curated datasets we use only represent a fairly small proportion of all possible demographics and users of the icelandic language. in particular, annotations are performed by a handful of university students, bringing in their biases to the annotated data. even so, the data should serve well to compare the relative differences. the resources we use to develop the models consist of a few high-performing gpus. while these are powerful, this is a relatively low requirement compared to many industry or academic use cases. finally, it is worth re-iterating that the byt5 model we use is slow compared to subword-based models for texts of similar length. inference in our setting was around 2.3x slower on average than for mt5. as such, production use of these methods may be better suited to offline processing, particularly for longer documents."
1193,"limitations. we evaluate the proposed method on multiple datasets and results show that the proposed method yields new state-of-the-art performance. we analyze why our approach attains superior performance by conducting ablation studies and sentence representation visualization. we further apply our model as an aigc detector to distinguish chatgpt-generated texts from those generated by human experts and the experimental results demonstrate that our model outperforms human evaluators in the setting of paired answers. limitations table 6 shows the results of our model and other methods on the agnews benchmark. interestingly, we notice that our approach reports a slightly inferior performance when compared with mdf+imlm (xu et al., 2021). we can see that methods using sentence representations based on token aggregation, e.g., fasttext9 or glove (pennington et al., 2014)-based isoforest, ocsvm, and cvdd (ruff et al., 2019), as well as bert based mdf + imlm (xu et al., 2021), perform especially well on agnews compared to their performance on other datasets. we conjecture that this is because agnews has a much larger variation of sequence length (36.6) than other datasets (around 7 or 8). a larger length variation will lead to more acute fluctuations in perplexities, especially when adopting an autoregressive language model with unidirectional context such as gpt-2-small in this paper, making it more difficult to distinguish between id and ood examples than in other datasets. in contrast, sentence representation based methods benefit from directly estimating the ood score using the information from the whole sentence, thus producing superior performance. fortunately, the limitation of auto-regressive modeling could be eliminated by leveraging transcormer (song et al., 2022) as the base model of our approach, where bidirectional context is used for estimating tokens at each position. we leave this for future work. 9https://github.com/facebookresearch/fasttext"
1194,"limitations we attempted to develop a novel framework for explainable complaint identification in a multitask setting. but the proposed approach is having some limitations as enumerated below: (1) the proposed methodology has been validated on an english language complaint dataset; further training would be required to scale up to codemixed language datasets which are prevalent in multilingual countries. (2) users often post some images along with text while writing complaints. the current system is unable to handle such multi-modal forms of inputs. (3) in some cases, users use an implicit sarcastic tone while writing complaints. in the current setup, sarcasm detection is not considered as a separate task. thus the proposed system will not be capable of detecting complaints with implicit sarcasm."
1195,"limitations besides its merits, this work still has limitations that could be further explored. on the one hand, we collect source data of mmdialog with only english language. thus the applicability to other languages would be restricted. on the other hand, we get rid of gifs and video-modality elements in mmdialog. in the future, we hope to extend mmdialog to multilingual scenarios and include more modalities such as audio and video."
1196,"limitations since there are two transformer-based models in hdld, the major limitation of hdld is the training cost. it costs twice training time than most baselines. here is the training time comparison on opensubtitles: model minutes per epoch adalabel 7.13 d2gpo 7.16 regdg 7.83 ours 14.32 to address this limitation, a potential improvement direction is to fuse the inter-duality approach into one transformer, e.g., sharing parameters between the fxy and fyx. however, we find the significant performance drop when doing so. in the future, the trade-off between performance and cost needs more attention. besides, like most generative models, if there is malicious information in data, there is no guarantee to avoid bad responses to users, which is a potential risk that needs to pay attention."
1197,"limitations although our dualgats simultaneously consider the complementarity of discourse structure and speaker-aware context for more accurate erc, it requires more computation and a longer training time. the performance of discourse parsing could be more satisfying in the current stage. moreover, we directly utilize pre-trained deep sequential models to parse dialogues in erc datasets, which does not address the domain gap problem well."
1198,"limitations there are two limitations in this paper: (1) compared with existing memory-based methods, the proposed prototype memory may bring additional storage space overhead. but since we only require very little additional memory (only one vector per class), we did not discuss it; (2) although we noted that the distortion and forgetting of the prototype are highly correlated, we did not conduct a detailed analysis of the reasons for special prototypes that do not follow this pattern."
1199,"limitations we have considered a variety of different llms in order to study attribution. however we have only considered a small sample of the different llm architectures and training strategies. this has been with a view to using a small but diverse set of llms. of these 10 base models, we tested our approach to attribution on a controlled set of fine-tuned models. while a study that considers a wider variety and larger scale of fine-tuned models would be beneficial to the problem of attribution, the computation resources limited our study. furthermore, in our assumptions in this work we consider that there is a one-to-one mapping between mf and mb. however, this is not necessarily the case. there could be an m-to-n mapping and also a model may be present in one set, but not the other. we believe there is rich space for further research in this area that can address these limitations, and further develop the problem of attribution."
1200,"limitations in this paper, we thoroughly investigate the existing large language models for nl2code, and summarize them from diverse perspectives with our own thinking. however, as this field is evolving so rapidly, there may be aspects that we have overlooked, or some new works that we have not covered. to mitigate this issue, we have created a website to track the latest progress through crowdsourcing, hoping that it will continually contribute to the development of the field. besides, the existing llms possess their own characteristics in terms of model size, architecture, corpus, pre-processing, tokenizer, hyper-parameters, and training platforms. also, some of them are currently not publicly available, such as alphacode (li et al., 2022b) and palm-coder (chowdhery et al., 2022). therefore, it is almost impractical to conduct a completely fair comparison. we tried our best to show a kind of comparison on the popular humaneval and mbpp benchmarks, hoping that it can provide clues to the differences in performance of different llms. in addition, evaluating llms has a high cost in computational resources. we thus have made all files generated by the llms publicly available on https:/"
1201,"limitations firstly, the transferability between different tasks within an mtl system is not well measured in current work. we also find such transferability is asymmetric and thus hard to quantify using symmetric measurements such as cosine similarity between task embeddings or gradients: for example, tsa positively transfers to sc, but sc negatively transfers to tsa (see the “only sentiment” row in table 3); fsrl positively transfer to all other tasks, but other tasks negatively affect fsrl. future work may consider exploring better indicators that address the asymmetry of task transferability (e.g., similar to inter-task affinity scores (fifty et al., 2021) in the cv domain). secondly, some of the"
1202,"limitations the limitations of our work can be viewed from two perspectives. firstly, we have not thoroughly investigated seq2seq architectures for explainable gec. secondly, the current input of the explainable system is the gold correction during training, whereas, in practical applications, the input would be the output of a gec system. we have not yet explored methods to bridge this gap."
1203,"limitations because we focus on off-the-shelf tools in this work, we are necessarily constrained by the availability of such tools in different languages and contexts. while dependency annotation tools are widely available for many languages through projects like the universal dependency project, semantic frameworks, let alone effective, accurate parsers for them are harder to find. in addition, we are constrained by the current state of the art for amr parsing and, more challengingly, alignment. amr parsing continues to improve, but alignment has only recently attracted interest again as a problem, such as in (cabot et al., 2022). additionally, this work, in evaluating six fewshot settings across six pairs of datasets and a number of seeds suffers from a combinatorial problem in terms of the necessary compute infrastructure. as discussed in the paper, our work consumed roughly a month of gpu time. combined with the size of the models, this limits the accessibility of this vein of research. more effort understanding how to narrow down the choice of datasets before studying transfer would go a long way towards alleviating this issue."
1204,"limitations while we observe marked improvements in the proposed multilingual language transfer with adapters, we recognize that there are several limitations still in the experiments. the first limitation is that the entity translation remains difficult, which is especially severe in the generated responses in the e&e setting. we think that name-entity translation is itself a task to be explored in-depth for future works. on the other hand, we think that while knowledge of language is one aspect for the transfer, the structural information of the semantic representation is also another important aspect – models need to acquire the important semantic structural information on top of the language-specific syntactic information. we think that this would further improve the resulting performance."
1205,"limitations we currently give separate definitions of the derivation trees of the four two-level formalisms. ideally, one could give a general definition of derivation tree in a multi-level formalism, and then derive each particular case by plugging in the definition of derivation trees of (ld-)cfgs and (ld-)pdas. we leave this generalization for future work. the d-strong equivalence results only hold for spinal tag and spinal paa. however, full tag and paa are not d-strongly equivalent to spinal tag and spinal paa, and therefore they are not d-strongly equivalent to cfg ⊲ cfg and cfg ⊲ pda, either."
1206,"limitations while our proposed confede method has shown promising results in multimodal sentiment analysis, there are some limitations to consider. firstly, our method is designed for multimodal sentiment analysis that includes three modalities: vision, audio, and text. the performance of the model when one of these modalities is missing is not considered. additionally, as the number of training samples increases, our custom-designed sampling method may require more processing time. however, the similarity calculation can be pre-processed between the unimodal training stage and the multimodal training stage (as outlined in appendix c). therefore, it may not consume a significant amount of time."
1207,"limitation of neupsl dsi. if the domain knowledge introduced is weak or noisy (as in the sgd setting), when the model is provided with more substantial evidence, this additional noisy supervision can, at times, hurt generalization. therefore, enabling models to perform weight learning, where the model adaptively weights the importance of symbolic rules as stronger evidence is introduced, is an interesting future direction (karamanolakis et al., 2021)."
1208,"limitations in this paper, we present a novel backdoor-based watermarking method, embmarker, for protecting the copyright of eaas models. our experiments on four datasets demonstrate the effectiveness of our trigger selection algorithm. however, we have observed that the optimal trigger set is related to the statistics of the dataset used by a potential stealer. to address this issue, we plan to improve embmarker in the future by designing several candidate trigger sets, and adopting one based on the statistics of the stealer’s previously queried data. additionally, we discover that as trigger numbers in the backdoor texts increase, the difference between embeddings of benign and backdoor samples in the cos similarity to the target embedding increases linearly. the optimal result should be that the cosine similarity keeps normal unless the trigger numbers in the backdoor texts reach m. we plan to further investigate these areas in future work."
1209,"limitations the limitations of this paper include the absence of experiments on large language models. previous studies have shown that using high-capacity pre-trained language models can significantly improve the accuracy of answers but also entails an increase in computational overhead. due to (academic) limitations of computational resources, this paper employs a low-capacity t5 model for experiments. our experiments have suggested that the proposed iterative prompting method that works with the low-capacity model can achieve comparable results with baseline methods equipping with large models. in future work, we would like to scale up the proposed model to improve the model’s performance. recent research on large language models (llms) has shown that they can learn from few examples and reason well. we believe that it is worth exploring ways to enhance the prompting of llms to improve their completeness when responding to ambiguous questions and reduce model hallucination in generation (openai, 2023; zhao et al., 2023; sun et al., 2023b,a). another direction worth exploring in the future is the application in lowresource scenarios, such as low-resource languages. low-resources in our study are characterized by limited multi-answer-qa annotations, which aims to examine how data size impacts model performance. other low-resource languages may behave differently with less training data and large models (xue et al., 2020; sun et al., 2021). besides, we would like to explore more effective prompting methods, such as chain-of-thought prompting (wei et al., 2022b)."
1210,"limitations argscichat is the result of a pilot study concerning 31 invited nlp experts. in particular, argscichat contains dialogues about 20 scientific papers regarding a few nlp topics. thus, dialogues in argscichat are only a small sample of the set of possible dialogues grounded in scientific papers. in particular, several design choices have been considered in our data collection methodology: (a) the topic of a paper; (b) the common background of invited nlp experts; (c) the available content of a paper during a dialogue; and (d) the dialogue setting (e.g., in our implementation, dialogues had a time limit which restricted the number of interactions between subjects). we chose the nlp domain in our study since we (the authors) have expertise in this domain. this choice also facilitated the def- inition of a pool of nlp experts to participate in our study through our research network. furthermore, dialogues in argscichat are grounded in scientific papers. in particular, we limit the paper’s content to the abstract and introduction sections. this choice reduces subjects’ effort to act as e, while also providing enough information to sustain a dialogue. thus, we do not collect dialogues between subjects concerning other sections of a paper."
1211,"limitations. while we try to perform multilingual lexical specialization on a set of typologically diverse languages, we are still restricting our analysis to a small fraction of all the languages of the world. in addition to this, our analysis investigates only two mmts – albeit arguably the two most widely used. due to hardware limitations, we experimented with xlm-r base: the results we report may be substantially different for xlm-r large (or other larger mmts like mt5), which possibly encodes more lexical knowledge. ethical considerations. we leverage lexical constraints from babelnet, a resource constructed semi-automatically. babelnet may contain lexical associations reflecting negative social biases (e.g., sexism or racism). biased constraints, when used as training data in our specialization, may strengthen societal biases present in mmts. acknowledgments tommaso green and simone ponzetto have been supported by the join-t 2 project of the deutsche forschungsgemeinschaft (dfg). goran glavaš has been supported by the euinaction grant funded by norface governance (462-19-010) through deutsche forschungsgemeinschaft (dfg; gl950/2-1). we additionally acknowledge support by the state of badenwürttemberg through bwhpc and the german research foundation (dfg) through grant inst 35/1597-1 fugg. we thank our colleague sotaro takeshita for insightful"
1212,"limitations we identify several limitations in this work: (i) false negatives: our current automatic triple extraction pipeline is built using the ds approach followed by filtering using an nli model. however, wikidata is not complete (tan et al., 2022). while some triples may not be completely available in webie, we expect models trained on this dataset can still discover new triples that do not exist in wikidata. (ii) limited relations in annotation: the human annotation is only conducted on the most frequent 200 relations. (iii) limited languages in mwebie: as discussed in §2.3 and appendix c, the languages in mwebie are limited to official languages from geographical regions where there is a reasonable amount of mturk workers to accept the job. an alternative solution would be to use professional translators, especially for low-resource languages. (iv) fixed dataset: facts might change in the world (and wikidata). this can lead to a degraded real-world performance if a system relies exclusively on webie for evaluation when the dataset is not updated accordingly."
1213,"limitation we discuss some limitations of our study. our proposed method needs a pre-trained model to obtain the attention score mentioned in section 3.3. while our method achieved successful attacks with 4∼5 times fewer queries compared to the baselines, the time spent on the attack was approximately half of the baselines. this is because preprocessing is necessary to calculate the score v in section 3.1 and d in section 3.2."
1214,"limitations of existing approaches regarding htc. in particular, tending to minimize structural entropy, we design circa to construct coding trees for the label hierarchy. to further extract textual information, we propose hitin to update node embeddings of the coding tree iteratively. experimental results demonstrate that hitin could enhance text representations with only structural information of the label hierarchy. our model outperforms existing methods while greatly reducing memory increments. limitations for text classification tasks, the text encoder is more important than other components. due to the lack of label semantic information and simplified learning procedure, the robustness of text encoders directly affects the performance of our model. from table 2 and 3, we could observe that bert has already surpassed textrcnn by 4.52% and 6.43% on micro-f1 and macro-f1. besides, bert beats all the textrcnn-based methods on rcv1-v2 and nytimes. however, when applying bert as the text encoder, our model makes slight improvements to micro-f1, especially on wos. a probable reason is that bert was pre-trained on news corpus while wos consists of academic papers."
1215,"limitations psgd is a straightforward constrained-decoding algorithm for the translation suggestion task. how- ever, the early-stopping mechanism involves extra time costs. though psgd is more efficient than vdba in the scene of ts, where only two constraints (prefix and suffix) appear, it could be slower than vdba if there were more short constraints. besides, even if we take both prefix and suffix constraints into consideration for emphasis on the whole translation generation, the decoding process is still auto-regressive from left to right. the algorithm could be improved if we made better use of the information of suffix constraints. for example, how to apply psgd on non-autoregressive models, such as (gu et al., 2019) and (yang et al., 2021b) will be our future work."
1216,"limitations there are several limitations to the current study. firstly, as for now, the corpus used in this study only consists of a single language pair. secondly, the coherence of the mt systems was evaluated using a fine-tuned conference model, as no anno- tations were available for the mt outputs. however, as shown in tab. 8, the fine-tuned conference model is not perfect and may affect the quality of our coherence evaluation. thirdly, this paper focuses on using discourse annotations to reveal and analyze discourse phenomena and the challenges they present to machine translation. using the annotations to improve mt models is beyond the scope of this study and is left for future work. ethical considerations the annotators were paid a fair wage, and the annotation process did not solicit any sensitive information from the annotators. in regard to the copyright of our dataset, as stated in the paper, the crawling script that we plan to release will allow others to reproduce our dataset faithfully and will not be in breach of any copyright. in addition, the release of our annotated test set will not violate the doctrine of fair use (us/eu), as the purpose and character of the use is transformative. please refer to https://www.nolo.com/legal-encyclopedia/ fair-use-the-four-factors.html for relevant laws."
1217,"limitations the cmot method for speech translation is based on the idea of cross-modal knowledge transfer and the paradigm of multi-task learning, so it requires the transcripts of speech during training, which may not be applicable to some languages without transcribed text. besides, this work mainly focuses on the finetuning of speech translation, and applications on the pretraining phase or on other tasks are in need of exploration in the future."
1218,"limitations we analyze the limitations of this work, so as to further improve the performance of our model in future work. based on our empirical observation, we reveal several limitations, which can be divided into two primary categories. (1) first, our proposed spectra method relies on large-scale spoken dialog corpora with explicit word-level speech-text alignment annotation, such as spotify100k. this limits the generality of our model on more spoken dialog corpora. in the future, we would like to develop a semisupervised pre-training method to leverage both labelled and unlabeled datasets. (2) second, our method is mainly designed for speech-text understanding and has not been fully explored for generative tasks. we plan to devise dialog generation per-training objective to empower the model with better generation ability. (3) third, the work only involves speech and text modalities. we are interested in handling more modalities, such as images or videos, to enrich cross-modal information in joint representations."
1219,"limitations since methods based on pre-trained language models on text style transfer requires larger gpu resources and are not mainstream methods, we have not yet tested the effectiveness of our method on pre-trained language models. moreover, since there is no multiple-attribute dataset in existing research, the applicability of our method on multipleattribute tst tasks has also not been verified."
1220,"limitations currently, cone is mainly implemented for proposal-based models as they can generate ex- plicit moment proposals for the introduced interwindow contrastive learning. in contrast, proposalfree methods (e.g., vslnet (zhang et al., 2020a)) directly predict the start/end timestamps without explicit proposals. in the future, it is worthwhile to explore how to incorporate the coarse-to-fine alignment mechanism with proposal-free methods to enhance the generalization ability of cone. furthermore, cone falls short on the groundtruth moment case whose duration is longer than the adopted video window duration. to ease this issue, future work can explore adaptive-duration window slicing to ensure complete containment of scenes and events within windows or some rulebased proposal merging techniques."
1221,"limitations although our method has achieved outstanding performance compared to current kd techniques, it is still a word-level kd method and also suffers from some limitations in vanilla word-level kd, e.g., the exposure bias as analyzed in appendix a. how to design a unified and more powerful kd method from the perspective of the connection between word- and sequence-level kd still remains unsolved. we will leave this for the future work. moreover, our study focuses on the mainstream kd techniques in nmt, which transfer knowledge through teachers’ predictions, while some other kd techniques, like directly distilling the hidden states (wu et al., 2020), are not within the scope of this study and thus not included."
1222,"limitations for hkg one-position link prediction tasks, hahe shows the best performance in all three datasets. however, because hahe is based on hypergraph learning, it improves more on the wd50k high quality hyper-relational knowledge graph link prediction dataset, and less on the wikipeople dataset where triples are the majority, so hahe prefers the fact with more arity numbers. in the future, we will consider extending our approach to triples as a unified architecture. for hkg multi-position link prediction tasks, it can be seen that our model is effective when predicting multiple missing auxiliary information, which is a frequent situation in practical applications. however, the prediction accuracy of our model needs to be further improved in the case of missing primary relations."
1223,"limitations there are several limitations to our framework. specifically, since observations are introduced as guiding information, our framework requires observation extraction tools to label the training set in advance. then, the nodes contained in the observation graph are mined from the training data. as a result, the mined n-grams could be biased when the overall size of the training set is small. in addition, our framework is a pipeline, and the report genera- tion performance highly relies on the performance of observation planning. thus, errors could accumulate through the pipeline, especially for small datasets. finally, our framework is designed for radiology report generation targeting chest x-ray images. however, there are other types of medical images (e.g., fundus fluorescein angiography images) that our framework needs to examine."
1224,"limitations the main limitation of our work is the high memory and computation cost. as both our methods estimate the importance of training examples based on the prompt-performance statistics, we first need to run in-context learning on the dev set multiple times with different prompts in dicl. although icl does not require any parameter updates, llms still require a large amount of memory footprint during inference, especially when the model size is large and the average sequence length is long. for each setup, our dicl contains around 50,000 prompts (see table 8) and 50 dev examples per class, so the most expensive setup (running opt13b on boolq) costs more than 500 gpu hours on an rtxa6000 gpu. our preliminary study shows that the proposed methods need the statistics of at least 10,000 randomly sampled prompts to perform well. future work may use search algorithms instead of random sampling during data collection to reduce the number of prompts in dicl. we also release our collected data of every setup in https://github.com/terarachang/dataicl to support future studies on icl. in this paper, we only study classification tasks, for the sake of easy evaluation. future work may study the influence of in-context examples in generation tasks under different evaluation metrics. due to hardware constraints, we do not study llms of sizes over 13b, and we fix the number of shots and prompt templates for simplicity. in independent work, nguyen and wong (2023) complement these limitations of our paper, showing that similar approaches work well on larger models and a diverse number of shots for in-context example selection. still, the influence of in-context examples for gigantic llms larger than 100b parameters has not been studied. due to emergent abilities of llms (wei et al., 2022), it is unclear whether our methods and findings would still apply when prompting these gigantic llms."
1225,"limitation the unstructured sparse patterns we introduce are not as hardware-friendly as the structured patterns, suggesting the speedup of using unstructured patterns maybe limited due to the implementation. the number of parameters of models we are studying are only at the level of 100 ∼ 300m, and the datasets are focus on glue, e2e, webnlg, and dart. we will generalize to wider choices of datasets in future works."
1226,"limitations we discuss two limitations of this work as follows: one limitation of our work is the lack of taskspecific automatic metrics to evaluate the empathy of generated responses. therefore, the evaluation of empathy relies more on human evaluation. although human evaluation is a golden standard, automatic metrics help to conduct large-scale investigations. this is also a common limitation in current works on empathetic dialogue. the second limitation is the passive response to the user’s cognition and affection. in many scenarios, empathy is used as a strategy for emotional support by responding to the user’s cognition and affection. however, besides passive response, emotional support also requires active emotion elicitation, which can be studied in future work. ethical considerations in this paper, our experiments adopt the widely used empatheticdialogues benchmark, an open-source dataset collected from amazon mechanical turk (mturk) that does not contain personal information. we also ensure the anonymization of the human evaluation. we believe that this work honors the ethical code of acl."
1227,"limitations we acknowledge that the randomness of our noise generation procedure may generate a new entity span that can be considered unnatural (for example, adding prepositions to a city name). such aspect of our method may have some impact on the performance levels measured, as distinct types of annotation mistakes can affect model performance differently. in this case, added noisy words that are improbable to be part of the entity may be more easily “ignored” by the model, while ambiguous additions can lead to a mistake. still, this approach can be used as a baseline and a more sophisticated performance assessment using more complex modeling or even real annotation inaccuracy could be done in future work. in addition, the constraint of expanding the mention boundary by a single token should be also taken into consideration. the reason for this design choice was not only based on narrowing the analysis spectrum by reducing the amount of data we had to investigate but also on time constraints, as the training procedure of multiple models on an even larger number of noisy dataset instances would escalate quickly. however, now conscious of the behavior relaxed annotation has on model behavior, it would be interesting to evaluate how this tendency is transformed by introducing even more unnecessary adjacent context into the annotations. a last limitation that can be pointed out is that we only evaluated the noise effect in a single dataset. there are other widely adopted benchmarks for el, such as msnbc (cucerzan, 2007) and clueweb (cao et al., 2007), which could be used in this work. however, we feel it would be more interesting to juxtapose with other textual domains, especially those with specific jargon and nes, such as the medical domain."
1228,"limitations while we tried our best to maximize the diversity and coverage of our benchmark, it is practically impossible to cover all possible input noises. we acknowledge aspects that we did not get to cover, for example, the impact of different input devices (phones, tablets, as compared to keyboards used in our annotation). also, while we tried to re-construct the real-world input settings as much as possible, there may still be subtle differences between real-world input and our annotation process, for example, we posed speed limits during the keyboard input annotation and this may not capture exactly how users type in real applications. we encourage future work to consider how to increase the coverage of such benchmarks and also possible innovations in the data collection procedure to collect fully realistic user data."
1229,"limitations of the dataset: (1) the handling of example complexity, (2) the coverage of the dataset in terms of domains and entity types, (3) the number of included languages, and (4) the quality of pretrained language models. these limitations are discussed in more details in the dedicated section below. limitations the analysis from subsection 4.6 shows that the number of entities per example has an important influence on results. for now, the handling of examples with more than one entity includes the detection of the mention, but does not consider other criteria. it would be useful to adapt the tsc methods in order to determine whether the sentiment about all entities is expressed by the same part of the example or not. if sentiment is expressed in different parts of the example, a splitting of the example into parts which express the sentiment about each entity would prove useful. despite a more diversified coverage of the political domain compared to newsmtsc, madtsc remains focused on politics. it would be interesting to include other major news domains, such as environment, business, culture, technology, sports, etc. equally important, all targets from mad-tsc are person names. it would be useful to also include sentiment expressed about other types of named entities (organizations, locations, events, etc.), as well as other polarization-prone concepts in each domain. such extensions of the dataset would provide a more complete view of tsc performance. ultimately, they would make the analysis of sentiments expressed in news articles more comprehensive and reliable. while mad-tsc is the first multilingual aligned dataset designed for tsc in news, it would benefit from the inclusion of more languages, including non-european ones. this limitation is due to the unavailability of massively multilingual and aligned news datasets which could be used to include more languages. a potential solution to overcome these limitations would be to enrich the dataset with manual translations in other languages. however, this solution is costly and is left for future work. finally, the comparison of results between languages is biased because the effectiveness of available pretrained language models is variable. this is a limitation which is shared by most nlp works which focus on multilingual datasets and reuse pretrained models, themselves trained on whatever datasets available for each language."
1230,"limitations we only create a high-quality dataset for evaluation and a small-scale dataset for few-shot learning, since the lack of a large-scale chinese parallel ss corpus. the available research methods for chinese ss are limited to unsupervised learning and few-shot learning. we hope a large-scale chinese parallel ss corpus can be released in the future. then we can directly train more supervised models for chinese ss. furthermore, we only analyze whether the current standard metrics are suitable for the evaluation of chinese ss, and leave the work of proposing a new metric for future study. due to time constraints, we do not perform a human evaluation for the output of llms. we hope to conduct a more comprehensive evaluation for llms in the future."
1231,"limitations our work aims to uncover how the distribution of factual and contextual errors in referring expression generation varies based on the familiarity of entities. our proposed experiments uncover this effect using pragmatics-driven heuristics. we need a more general deep-dive into what models “know"" to estimate how language models handle known and unknown information differently, in a way that might even escape human scrutiny. further, our"
1232,"limitations as this work is mainly focused on weakly supervised vision-and-language pre-training, we do not fully explore the factors that may influence the performance of relative representations, such as the use of different unimodal encoders and the source of the anchors. besides, we only validate the effectiveness of relative representations in a weakly supervised setting, while it remains to be explored whether it is also useful for standard vlp and multimodal learning in other modalities (e.g., audio and video). we will further exploit the potential of relative representations and validate it in more cross-modal learning scenarios in the future."
1233,"limitations as depicted in table 4, there are scenarios where argu demonstrates a lack of understanding and instead paraphrases the input variables to generate an incorrect response. it seems likely that the model associates negation with con. however, in exam- ples 5 and 6, the model does not factor the word “stop” in variable 1, leading to arguments that contradict the intended stance. further, in examples 7 and 8, the argument decoder seems to modify the generated template, which changes the overall meaning of example 7. such scenarios might reduce the trust in the model, hurting its practical use. all experiments involving argspan, argspanscheme, and argu only pertain to abortion, minimum wage, nuclear energy, gun control, the death penalty and school uniform. the model performance on any other topics is unknown. although we test argspanscheme on out-of-domain test sets, it still confines the six topics. since argu is trained only on argument sentences with less than 150 tokens, it is more geared towards generating shorter arguments of less than 50 tokens. we further do not benchmark argu’s inference time for practical use."
1234,"limitations the main limitation of our methodology is that the training of generative question answering models requires the usage of large gpu resources, which may not be easily available to all researchers. regarding the performance of the model and quality of the generated answers, our approach can be affected by possible bias induced by the evaluation system we are using. for the experiments in this paper, we only consider datasets from the english language, however, we conjecture that our techniques should work similarly for languages with a similar morphology. automatic qa evaluation systems do not achieve perfect correlation with human annotations, which indicates a gap with respect to human evaluation. for safety critical applications, human evaluation of generated answers still remains the best means for evaluation."
1235,"limitations in this section, we discuss several limitations of our work that are worth future study. first, the performance of the hierarchical transformer retriever is limited since the utterance-level transformer is trained from scratch only on our small-scale dataset due to limited time and computational resources. with more resources, future work can focus on pre-training the utterance-level transformer on large-scale data such as the complete pushshift reddit data (baumgartner et al., 2020). pre-training can potentially improve the performance of the retriever and further improve 8412 the generation quality. second, the two types of retrieved responses in the recap-mixed model are encoded with the same encoder. however, intuitively, the two types of responses should contribute to generation in different ways, so treating them the same way might harm generation performance. this is also reflected in our results. even though the recap-mixed model shows improvement from both types of retrieved responses, the improvement is weaker than that on each separate model. in future work, designing a split encoder for different types of retrieved responses may help maximally preserve the performance boost from both types of retrieved responses."
1236,"limitations though autoregressive span selection effectively weakens the conditional independence assumptions imposed by current span-based parsing methods, this strategy imposes another arguably unreasonable inductive bias of forcing a predefined span selection order. we find using post order performs fairly well but this does not necessarily means it is the best order for span selection, and this work might leave other potentially better order unexplored. future works might consider using set prediction methods (sui et al., 2020; tan et al., 2021) 11for instance, if we want to extend the model of nguyen et al. (2021) to discontinuous parsing (assuming binarization), the number of splitting points is nondeterministic which can be difficult to handle: a continuous parent span could be split into two continuous subspans with one splitting point, or one continuous subspan and one discontinuous subspan with two splitting points, or two discontinuous subspans with at least three splitting points (depending on the fan-out of subspans). the cases of discontinuous parent spans are even more complicated. to eliminate such implausible inductive bias (of forcing orders) while still benefiting from explicit span correlation modeling. another issue is regarding time complexity. though our model needs only linear steps (in sentence length) for parsing, each step takes o(n2) time to select a single span over o(n2) total spans, making the overall time complexity cubic. we remark that for each step the o(n2) operation is parallelizable and—with full gpu parallelization— fairly fast in practice, but it would still be problematic when the sentence is extremely long."
1237,"limitations this work introduces the general idea of incorporating attribution into knowledge distillation, and there are three potential limitations. first, although ad-kd chooses integrated gradients for attribution, there are actually other attribution methods (janizek et al., 2021; sikdar et al., 2021) which can also be fitted in our framework. the question of whether these methods perform better than integrated gradients when combined with knowledge distillation is still unclear. second, we conduct experiments on bert of different scales and have not yet validated the effectiveness of ad-kd on other model structures. third, while we only perform task-specific knowledge distillation in our experiments, applying ad-kd to task-agnostic knowledge distillation is also worth investigating."
1238,"limitations the dataset we collected annotates questionable assumptions based on the current facts at the time of annotation. however, the status of questionable assumptions can change for some questions in the future. for instance, the question what episode does aidan appear in just like that contains an assumption that he appeared in some episode of the show. as of january 2023, this is invalid because the season that he has been confirmed to appear in has not aired yet, meaning that no episode of the show is public. but this assumption will become valid when the show airs. furthermore, annotation errors may arise due to the intrinsically challenging nature of verifying that something did not happen or does not exist. while the annotators tried to provide convincing evidence and used their best judgment to decide on the unverifiability of the assumptions, they could not check every relevant document available online. in theory, this would be necessary for perfect verification of nonexistence, unless there is an explicit statement of nonexistence. however, such statements are inherently rare due to reporting bias (e.g., people rarely explicitly state bananas are not rainbow colored). the annotators often used pragmatic inference, taking omission as supporting evidence that the event did not happen or the entity does not exist (e.g., taking statements like bananas can be yellow, green, brown, or red as supporting evidence for bananas are not rainbow colored). this is nonetheless heuristic and may be a source of annotation errors."
1239,"limitations since our approach identifies common opinions based on frequency of sentence encodings, we require a relatively large number of input sentences. we were not able to experiment with other popular datasets like amazon (he and mcauley, 2016), yelp (chu and liu, 2019) or rotten tomatoes (wang and ling, 2016) since these datasets only include a small number (usually 8) of input reviews. the abstractive summaries are generated solely based on the latent encoding, and our model does not include a copy mechanism or attend to the original inputs when decoding. it therefore does not always generalize well to new domains. however, this limitation is mitigated by not requiring any labelled data during training: hercules can easily be retrained on a new domain. generating output based only on latent encodings means that the model is also susceptible to hallucinating, since the output is less directly linked to the inputs. however, unlike other methods, hercules provides evidence sets alongside the generated summaries, making it easier to check whether the output is faithful. finally, hercules generates summary sentences independently, leading to summaries that are less coherent than approaches that model the summary as a single sequence. we welcome future work on combinining the relative strengths of each approach. we do not anticipate any significant risks resulting from this work."
1240,"limitations since our method constructs on the multimodal transformer, it cannot be migrated to the dualstream model. experiment results show that cfsum can achieve comparable performance with strong baselines. but it still cannot surpass the sota of some dual-stream large models."
1241,"limitations during the creation of the remuq dataset, we simply remove the words in the question that are duplicated in the image caption – in some cases, this may result in grammatical errors in the text query. we performed the experiments for studying optimal masking ratio on a subset of the pretraining data, due to resource constraints."
1242,"limitations in this paper, we explore label-semantic augmentation (la) for multi-label few-shot intent detection via appending label name after utterances, which is similar to instruction learning or prompt learning. however, we don’t further study the relationship between la and instruction learning due to space limitations. we believe that instruction learning integrated with labels will inspire further investigation for multi-label few-shot intent detection."
1243,"limitations in this paper, we mainly focus on developing an amortized model to efficiently achieve a reliable estimation of sv. though not experimented with in the paper, our method can be widely applied to other black-box post-hoc explanation methods including lime (ribeiro et al., 2016). also, due to the limited budget, we only run experiments on bert-based models. however, as we do not make any assumption for the model as other blackbox explanation methods, our amortized model can be easily applied to other large language models. we only need to collect the model output and our model can be trained offline with just thousands of examples as we show in our method and experiments. comparison and training with exact shapley values computing exact sv is computationally prohibitive for large language models (llms) on lengthy text inputs, as it necessitates the evaluation of llms on an exponential (in sequence length) number of perturbation samples per instance. as a result, we resort to using svs-25, which serves as a reliable approximation, for training our amortized models."
1244,"limitations in this section, we discuss the limitations of this work as follows: • peerda leverages labeled spans in the existing training set to conduct data augmentation. this means that peerda improves the semantics learning of existing labeled spans, but is ineffective to classify other spans outside the training set. therefore, it would be beneficial to engage outer source knowledge (e.g. wikipedia), where a variety of important entities and text spans can also be better learned with our peerda approach. • since peerda is designed in the mrc formulation on top of the encoder-only pre-trained language models (plms) (devlin et al., 2019; liu et al., 2019), it is not comparable with other methods built on encoder-decoder plms (yan et al., 2021b; chen et al., 2022; zhang et al., 2021b; yan et al., 2021a). it would be of great value to try peerda on encoder-decoder plms such as bart (lewis et al., 2020) and t5 (raffel et al., 2020), to see whether peerda is a general approach regardless of model architecture. • as shown in table 9, although peerda can significantly alleviate the missing predictions, the most prevailing error in the mrc model, peerda also introduces some new errors, i.e. multiple labels and incorrect label. it should be noted that those problematic spans are usually observed in different span sets, where they would learn different category semantics from their peers. therefore, we speculate that those spans tend to leverage the learned category semantics more than their context information to determine their categories. we hope such finding can shed light on future research to further improve peerda."
1245,"limitations the main limitation of the presented work is the need for significant computing resources to train multimodal models using dynamic uda. it should be noted that the proposed methods, mmbv and dynamic uda, require fewer computational resources than the original version of uda."
1246,"limitations tasks in bgglue the bgglue benchmark is comprised of nine challenging nlu tasks, including three token classification tasks, one ranking task and five text classification tasks. while we cover three different types of tasks in the benchmark, we are restricted by the available resources for bulgarian, and thus we could not include some other nlp tasks, such as language generation. we also consider only nlp tasks and we do not include tasks with other/multiple modalities. finally, some of the tasks are of similar nature, e.g., we include two datasets for ner and two for credibility/fake news classification (see section 2). domains in bgglue the tasks included in bgglue span over multiple domains such as social media posts, wikipedia, and news articles and can test both for short and long document understanding. however, each task is limited to one domain and the topics within the domain do not necessarily have full coverage of all possible topics. moreover, some of the tasks have overlapping domains, e.g., the documents in both cred.-n and fake-n are news articles. baseline models as described in section 5, the baseline models provided for bgglue include fairly small encoder-only transformer architectures. we leave for future work other modeling architectures and modeling techniques that are known for improving the efficiency and the computational requirements of the used models, e.g., few-shot and zero-shot in-context learning and instruction-based evaluation, multi-task learning, etc. model biases in this work, we did not explore whether the datasets in bgglue contain unwanted biases, which could also lead to potentially hazardous behavior of the baselines we trained in our experiments with the bgglue benchmark."
1247,"limitations • our study focuses on word replacement attacks. while these attacks are the most common in the literature, the human perception of attacks that rely on insertion or deletion can differ from our"
1248,"limitations we experiment with a range of adaptation domains that we draw systematically to capture the covariates enumerated in section 4.1. however, future work should acknowledge that these are not all the covariates responsible for the success of adaptation and the robustness of the final model. following is the non-exhaustive list of possible covariates that we do not control in this work. (i) the adapted model size, (ii) the size of pre-training data, (iii) pretraining configuration parameters, but also (iv) the broad variance of adapted language pair(s); (v) the variance of mutual similarity of languages within the pair, and hence (vi) the difficulty of training the translation model. the evaluation of our experiments did not consider the effect of randomness of the training process. despite the fact that our experiments were run with a fixed random seed and initial value, making our results deterministically reproducible, the variance of the results among the experiments of different random seeds was not investigated due to the related infrastructural costs. however, all our results are aggregated over a larger set of checkpoints and/or domains, ranging from 10 (ids in table 1) to 720 (oods in table 2), as described in section 4.2. the alignment scheme proposed in section 3.1 might have blind spots; for instance, in the cases utilizing decontextualized embeddings, where both the hypothesis and reference contain multiple occurrences of the same word, the alignment scheme will make the prediction of the same target token equally good, regardless of the position. in future work, this imperfection could be addressed by using the optimal transport algorithm (kusner et al., 2015) within the alignment, similarly to zhang et al. (2020a)."
1249,"limitations although our proposed method asap is able to outperform baseline estimators, an important factor it ignores is the subjectivity of user satisfaction. in practice, different users may have different degrees of satisfaction with the same dialogue. this implies that asap may be effective for some users, but it may also fail to predict true satisfaction for others. in order to adequately simulate a user, it is essential to take the issue of subjectivity into account. given this, we would like to extend asap for personalized satisfaction estimation by incorporating user profile information in the future."
1250,"limitation while dealing with memes, which will be considered in the future."
1251,"limitations one limitation of this work is the lack of human performance scores on the new tasks. although the baseline performance is far from perfect, and it seems quite likely that human performance is much better, this should be measured in future work. another limitation is that it is unknown how much each task should benefit from access to the audio in addition to text; this could be measured in principle for humans, but again we leave this to future work. broader impact and"
1252,"limitations our work has several limitations. first, our experiments are limited by the multi-domain datasets available for sequence classification tasks, limiting both our task coverage (sentiment classification and nli) and domain type coverage (product categories, temporal splits, and text source domains). future work can evaluate our drift metrics on token classification tasks or even sequence-to-sequence tasks by predicting sequence-level performance (e.g. proportions of correct tokens, or examplelevel bleu scores; papineni et al., 2002) from our example-level drift metrics. past work has already considered dataset-level drift metrics and performance predictions for token classification tasks such as named entity recognition (ner) and part-of-speech (pos) tagging (ramesh kashyap et al., 2021; rijhwani and preotiuc-pietro, 2020), and example-level drift metrics have been used in machine translation for training data example selection (axelrod et al., 2011; wang et al., 2017). we hope that future work will evaluate example-level drift metrics in their ability to predict nlp model performance on this wider variety of tasks. second, we only consider simple logistic regressions to predict whether individual examples will be predicted correctly by different models. more complex classifiers (e.g. xgboost; chen and guestrin, 2016) might improve performance predictions, particularly if more drift metrics are included as inputs, or if raw example features are included (e.g. sequence length; ye et al., 2021). our three dimensions of linguistic drift (vocabulary, structural, and semantic drift) represent just one way of decomposing linguistic dataset drift into distinct dimensions. we hope that future work will explore novel dimensions of linguistic drift, identifying new ways of integrating different drift metrics into nlp model performance predictions across tasks and domains."
1253,"limitations the performance of kpe is also related to the used pre-trained language model (plm), in addition to the proposed framework. kpe could suffer from unsatisfactory performance when the base plm is not strong enough. applying our proposed kpe to stronger plms, such as deberta, may lead to further improvements."
1254,"limitations we are evaluating s2st in an artificial setting given that we have to synthesize the text references. in fact, since there was no metric capable of evaluating the quality in speech, there was no motivation to build such benchmarks either (the chicken-and-egg problem). however, we expect that next benchmarks for the task will have speech references be- cause of the rise of end-to-end s2st systems and their quality increase. blaser paves the way so that we can take advantage of such benchmarks when they appear. our metric works at the sentence-level, by embedding the entire sentence into an intermediate space. we ignore how sensitive blaser is to the length of the sentence, which is a key aspect when we want to extend to the corpus-level metric in the future. moreover, we are aware that sometimes sentence embedding do not discriminate different numbers or words that belong to the same word family, which may disregard impactful errors such as the change of a number in the translation output. ethical considerations translation quality scores were provided by bilingual raters as mentioned in section 4. they were all paid a fair rate. we can not open-source the data form our experiments given that our sources are shared under no-derivative license. small human evaluation detailed in appendix d was done by volunteers."
1255,"limitation of assigning labels to people as being inherently reductionist. as mentioned in §7, using a single likert scale for social acceptability and toxicity is not sufficient in capturing the complexities in these phenomena, such as situational context. we note that quantifying positionality of existing systems is not an endorsement of the system. in addition to making sure that language technologies work for all populations, researchers should also continue to examine whether these systems should exist in the first place (denton and gebru, 2020; keyes et al., 2019). further, we note that understanding a dataset or model’s positionality does not preclude researchers from the responsibilities of adjusting it further. this study was undertaken following approval from the irb at the university of washington (study00014813). labinthewild annotators were not compensated financially. they were lay people from a wide range of ages (including minors) and diverse backgrounds. participants were asked for informed consent to the study procedures as well as the associated risks, such as being exposed to toxic or mature content, prior to beginning the study. research team positionality we discuss aspects of our positionality below that we believe are most relevant to this research. the research team is comprised of computer scientists who study human-computer interaction and nlp and have a bent for using quantitative methods. thus, we approach the topic from a perspective that assumes that positionality can be characterized, fixed, and quantified. the entire research team currently resides in the united states. in alphabetical order, the team members originate from belgium and switzerland, france, germany, india, and the united states; and identify as east asian, south asian, and white. these nationalities and ethnicities are overrepresented in the development of nlp technologies. thus, we acknowledge that our knowledge of how design biases in nlp datasets and models impact people is largely through research, rather than personal experience."
1256,"limitations community survey the winoqueer benchmark is necessarily an imperfect representation of the needs of the lgbtq+ community, because our sample of survey participants does not represent the entire queer community. crowdsourcing, or volunteer sampling, was used for recruiting survey participants in this study as it has its strength in situations where there is a limitation in availability or willingness to participate in research (e.g., recruiting hard-to-reach populations). however, this sampling method has a weakness in terms of generalizability due to selection bias and/or undercoverage bias. we limited our survey population to english-speakers, and the winoqueer benchmark is entirely in english. we also limited our survey population to adults (18 and older) to avoid requiring parental involvement, so queer youth are not represented in our sample. additionally, because we recruited participants online, younger community members are overrepresented, and queer elders are underrepresented. compared to the overall demographics of the us, black, hispanic/latino, and native american individuals are underrepresentend in our survey population. geographically, our respondents are mostly american, and the global south is heavily underrepresented. these shortcomings are important opportunities for growth and improvement in future participatory research. finetuning data collection in an effort to balance the amount of linguistic data retrieved from media cloud and twitter respectively, we had to use additional search terms for media cloud as it yielded significantly fewer results than twitter when using the same search terms. also, news articles from january to may 2022 are excluded from the news article dataset due to media cloud’s backend api issues. due to the size our datasets and the inexact nature of sampling based on hashtags, it is likely that there are at least some irrelevant and spam tweets in our sample. template creation our generated sentences have several limitations and areas for improvement. first, our nine identity subgroups are necessarily broad and may not represent all identities in the queer community. the winoqueer benchmark is limited to biases about gender and sexual orientation. it does not consider intersectional biases and the disparate effects of anti-lgbtq+ bias on individuals with multiple marginalized identities. the names used in templates are taken from the us census, so they are generally western european names common among middle-aged white americans. noneuropean names are not well-represented in the benchmark. additionally, the benchmark currently only includes he, she, and they personal pronouns; future versions should include a more diverse set of personal pronouns. finally, sentences are generated from a small set of templates, so they do not represent every possible stereotyping, offensive, or harmful statement about lgbtq+ individuals. a high winoqueer bias score is an indicator that a model encodes homophobic and transphobic stereotypes, but a low bias score does not indicate that these stereotypes are absent. evaluation and finetuning we used similar, but not identical, scoring functions to evaluate masked and autoregressive lan- guage models. it is possible that the metrics are not perfectly calibrated, and that one category of models may be evaluated more harshly than the other. additionally, some of our finetuned models scored below the ideal bias score of 50. this means that they are more likely to apply homophobic and transphobic stereotypes to heterosexual and cisgender people than to lgbtq+ people. many of these stereotypes are toxic and offensive regardless of the target, but others do not carry the same weight when applied to cis and straight individuals. currently, it is not well-defined what wq scores under 50 mean, in theory or in practice. this definition will need to be developed in consultation with researchers, end users, and the lgbtq+ community. this paper only includes results for a small fraction of available pretrained language models, and our results only represent comparatively small models. we present baseline results for models up to 7.1 billion parameters and finetuned results for models up to 1.5 billion parameters, but many of the models in use today have hundreds of billions of parameters. finally, our results are limited to opensource models and do not include closed-source or proprietary models."
1257,"limitations although we introduce a new gmner task and propose a number of baseline systems and an hindex framework, there are still some limitations in this work. first, our gmner task only requires identifying the visual regions that are correspondent to named entities mentioned in text. however, for each image, many visual regions may contain real-world entities that are not mentioned in text. therefore, it would be interesting to further annotate the entities that only occur in the image and explore a more complete mner task in the future. second, our work is a preliminary exploration of the gmner task, and the proposed approaches are primarily based on previous representative ner or mner methods. we hope this work can encourage more research to apply the recent advanced techniques from both the nlp and computer vision communities to improve its performance."
1258,"limitations there are three limitations on our method. first, we did not verify our method on more generic tasks, such as text classification, yet it is not limited to commonsense qa. extending our method to other downstream tasks is our future work. second, our method requires a longer training time and a larger gpu memory since the knns require forward and backward propagation additionally. third, we do not consider the ambiguity of gold answers, which may affect the quality of knns. for example, “apple” may refer to a kind of fruit or a technology company."
1259,"limitations first, we again emphasise that the lack of highquality non-english image-caption pairs is a primary obstacle to wider-scale multilingual and cross-lingual tti investigations. we hope that researchers in the future can construct and release more high-quality vision-language data for different languages, especially for low-resource ones. second, our work uses 512-dim ‘xlm-r large vit-b/32’ mclip22 and is based on the stylegan2 framework (karras et al., 2020b). since the main focus of our work is to realise multilingual and cross-lingual tti and enable fair comparisons across different models and approaches, we compare all proposed and baseline methods with the same mclip text encoder and the gan framework. however, for readers and potential users interested in ‘chasing’ stronger absolute fid scores, we speculate that the larger 640-dim ‘xlm-r large vitb/16+’ mclip text encoder and the more recent stylegan3 (karras et al., 2021) can be helpful. third, we notice that in addition to lafite, several state-of-the-art large diffusion models such as those from saharia et al. (2022) and rombach et al. (2022) also use clip to condition image generation on text input. this means that we could be able to derive multilingual diffusion models for mtti also by replacing clip with mclip and enhance the mtti performance with our proposed ensad (of course, we would need to redesign our loss functions). however, due to limited computational resources, we leave it to future work. fourth, the ensad boosts cross-lingual transfer for tti by combining the knowledge from multiple translations, which can mitigate potential translation errors. our work does not demonstrate if ensad is applicable and adaptable to downstream cross-lingual tasks besides tti. it is because 1) downstream tasks other than tti are out of the scope of this work and 2) adapting ensad to different tasks will require redesign of model structures and losses catering to the characteristics of each downstream task, making us believe it is not proper to expand the topic and include everything in a single piece of work. therefore, we also leave this to future work. 22https://github.com/freddefrallan/multilingual-clip"
1260,"limitations we discuss the limitations of our work. first of all, our model lyra is build upon pre-trained language models (ptlm) including bart (lewis et al., 2020) and gpt-2 (radford et al., 2019). although our method is much more data friendly than previous methods in that it does not require training on melody-lyric aligned data, our pipeline may not apply to low-resource languages which do not have ptlms. second, our current adoption of melody constraints is still simple and based on a strong assumption of syllable stress and note duration. we encourage future investigation about other alignments such as the tone or pitch variations. lastly, although we already have the music genre as an input feature, it remains an open question how to analyze or evaluate the generated lyrics with respect to a specific music genre."
1261,"limitations of pretrained lms on arithmetic reasoning and symbolic manipulation. we experiment with three simple symbolic manipulation tasks and show that improving the locating and induction capability of lms can be important for further improving their performance. our method that combines abstraction and finest-grained step-by-step tutoring demonstrates its potential to generalize correctly, shedding light on possible directions orthogonal to scaling up lms for future work in this area."
1262,"limitations while our approach is designed to be as broadly applicable as possible, an inherent limitation of our work is that it depends on the usage of causal tfrstyle models, which, though architecturally similar to many existing pre-trained models, require hyperparameter search and fine-tuning to replace non-tfrs on downstream tasks. while we find evidence that such models are as capable as other rerankers, and we believe tfrs can be a default design choice going forward with little loss, this extra requirement may still be a barrier for the broad adoption of our approach. more broadly, our approach is designed for settings where some reranker is available. if it is not possible to train a robust reranker, for example in a low data setting or a setting where evaluation relies entirely on human judgments that cannot be reproduced by models, our approach cannot be applied. however, we believe that the growth in learned functions of human judgments as part of rl from human feedback loops provides a promising avenue to roll out our approach to new settings. our experiments were carefully chosen to represent the capabilities of our models with several base tasks and several reranking objectives. we didn’t, however, explore certain domains involving attribute control such as formality or simplicity, choosing instead to explore more quality-based downstream exploration. we showed the applicability of our approach when reranking outputs on languages other than english, but further results on languages with different typological characteristics may show different trends. while our work already provides strong speedups both with candidate sets from lattice and beam search decoding, these speedups become even more valuable for approaches that combine multiple rerankers, which have been shown to potentially lead to further improvements in reranking (fernandes et al., 2022). while we explore this partially in the form of ensembled eel with model probabilities, more exploration on eel for multiple rerankers may be valuable."
1263,"limitations for cls, giving a more faithful solution. limitations the limitation of this paper can be stated from three perspectives. first, although using our cls annotation protocol can label more faithful data, the annotation cost is higher because annotators need to comprehend the full source text instead of only the source summary. second, convsumx only covers 3 typical languages, while languages from different language families and have different morphology and lexical-/syntactic rules require further investigation. third, although the proposed 2-step method is effective, we simply concatenate the source input text and mono-lingual summary at the token level as the model input but do not make further exploration. we believe that more smart and sophisticated designs to integrate features from source input text and mono-lingual summary can further improve the cls performance, which, however, we leave for future work."
1264,"limitations we view strong performance on our evaluation datasets as necessary but not sufficient to demonstrate human-like learning. thus, if models perform poorly on our datasets (as the models we evaluated did), then we have strong reason to conclude that models are not learning in human-like ways. if future models perform better, such results would be consistent with human-like learning but would not conclusively establish that models learn as humans do, as they might instead be using some shallow heuristic that is not controlled for in our datasets. in other words, a criterion that is necessary but not sufficient facilitates strong"
1265,"limitations our theoretical analysis targets a specific notion of information leakage, and it is likely that it does not apply to alternative ones. while the v-informationbased approach seems natural, future work should consider alternative extrinsic bias measures as well as alternative notions of guardedness. additionally, our focus is on the linear case, which is tractable and important—but limits the generality of our"
1266,"limitations our findings should be interpreted considering a series of problem definitions and design choices. first, our quantitative results on measuring incidental bilingualism at scale are subject to language identification, sentence splitting, and mining errors. our qualitative analysis for the english-french language pair revealed that those errors are reasonably small (see §3.2). however, we expect the accuracy of our tools to vary across languages and, crucially, exhibit unanticipated failure modes on web text and low-resource languages (caswell et al., 2020). second, our findings are restricted to quantifying bilingualism and translations within a limited set of language pairs and only paired with english. thus, by problem definition, we are limited to computing a lower-bound estimate on incidental bilingualism of palm. the above limitations should also be taken into consideration when interpreting our ablation results. although we attempted to remove most bilingual signals in our series of mt experiments, it is still possible that bilingualism slips through due to either model errors or due to bilin- gual signals beyond our focus set of languages. finally, any results and findings of our work are restricted to palm; the single llm studied in this work. however, our finer-grained analysis (see table 11 of appendix e) reveals that incidental bilingualism, including translation signals, is observed across various data sources (e.g., webpages, books, etc.) that are commonly included in the training data of other popular llms."
1267,"limitations we synthesize negative instances by substituting relational phrases with misleading tokens. however, the relational semantics in some instances may be expressed implicitly. that is, there are no key tokens that directly correspond to the target relation. therefore, we cannot synthesize negative instances based on these instances. additionally, we consider substitution ratio ϵ as a fixed hyperparameter. it may be a better choice to dynamically determine ϵ based on the input instance. we leave these limitations as our future work."
1268,"limitations while we evaluate our method on three distinct generation tasks, we acknowledge that we rely on a single language (english) and a single type of structural constraints (on top of syntactic dependencies). further work is required to verify if the proposed approach holds on other languages (e.g., it is unclear how much our method is impacted by low-resource languages where syntactic parsers may be of lower quality) and other types of structural constraints (e.g., semantic roles). this work focuses on relatively smaller langauge models and does not address the impact and modes of usage of structural constraints on larger language models such as gpt-3."
1269,"limitations since our eap builds its graph representation from social media data, our method may carry inductive biases rooted in this type of data. moreover, note that the scope of our study is limited to english social media posts and our approach does not consider inputs larger than 512 tokens. therefore using our approach in long document summarization may be challenging. finally, the general applicability of eap in a different domain is highly dependent on the existence of high-quality lexicons for the domain in question, which may not be available."
1270,"limitations despite the contributions of our work, there are also unavoidable limitations of it. first, our method is based on the setting that each utterance in the dialogue except the first one has exactly one addressee. this setting holds tightly in online forums such as twitter or reddit, yet has its limit in group chats or meetings, where an utterance can reply to multiple or no addressees. however, this scenario is relatively rare in multiparty conversations. considering this scenario is challenging and complicated since the one-to-many reply-to relations can cause the single-turn em algorithm intractable. for this part, we leave it to future works. second, the ubuntu irc benchmark of response generation task is extracted from the ubuntu chat corpus (lowe et al., 2015), where people discuss the technical issues on the ubuntu operating system. due to the lack of human annotators with knowledge of linux and ubuntu, we do not con- duct human evaluations on this dataset. however, we do provide the generated responses in our supplementary materials for those who are interested in the human evaluations."
1271,"limitations in this paper, we present a novel approach to remove the permutation invariance of the attention module. specifically, we propose a weight concatenation operation that exactly follows the word order of a sentence, leading to an increase in dimensionality and the introduction of affine transformations aimed at reducing it. hence, the effect of increased parameter counts cannot be well isolated. while our preliminary experiments show that an increase in the number of parameter counts does not necessarily enhance the experimental results, we acknowledge the increased complexity resulting from direct concatenation and, thus, have utilized the equivalent form of the proposed operation in practice. in the future, we aim to explore alternative operations that implicitly encode positional information based on word order, without resorting to affine transformations, to replace the weight sum operation of the attention module."
1272,"limitations due to the limitation of dataset resources, we evaluate our unified model only with tb-dense and matres. although the experiment results show that our approach can significantly outperform stateof-the-art methods, we still need to experiment on more datasets with various kinds of temporal relations to further prove the generalization capability and robustness of our framework."
1273,"limitations learning an ensemble of multiple source models is expensive, especially for large language models. hence, to adapt to the new target domain, we cast the problem as an iterative-decision making process. while our work reduces the model access frequency to 1 or 2 at each training step, continually updating the language model from a stream of test data is still costly. future work can explore better methods for efficient optimization for a single lm. besides, in some cases, the distribution of test data may change dynamically over the stream, but our work considers only the situation where the test data is from one specific distribution. more complex cases of test distribution can be studied in future work."
1274,"limitations this paper mainly focuses on the generalized intent discovery (gid) task in task-oriented dialogue systems. our proposed decoupled prototype learning (dpl) framework well decouple pseudo label disambiguation and representation learning through protopical contrastive learning and prototype-based label disambiguation, and achieves sota performance on three gid benchmark datasets. however, our work also have several limitations: (1) we only verified the effectiveness of our dpl framework on gid task, but the adaptability of dpl in more unsupervised / semi-supervised settings, such as unsupervised clustering and ood intent discovery, is worth further exploration. (2) we follow standard experiment settings as previous work, and assume that each ood sample must belong to a corresponding intent cluster. however, a more realistic scenario is that there may be noise samples in the ood data. these noise samples do not actually belong to any cluster/category and are some outliers. we leave the noise ood issue to the future work. (3) our experiments in appendix 6 find that the performance of the dpl method decreases significantly when the imbalance factor of unlabeled ood data increases. how to improve the performance of gid model on the long tailed unlabeled data is also a problem worthy of attention in the future."
1275,"limitations the limitation of our work includes the following aspects: 1) the instruction-style question which measures the quality of generated texts from different dimensions still needs manual design. although the questions in our experiment have already involved typical dimensions in text summarization, dialogue generation, and data-to-text generation, we admit that it is hard to cover all the dimensions in various nlg tasks. we believe that this is not a severe problem because we can refer to the definition and human annotation instructions (mehri and eskénazi, 2020) of each dimension, which are commonly formulated as questions. we leave the exploration of automatically constructing instruction-style questions for multiple dimensions of nlg evaluation as future work. 2) due to the limitation of computational resources, the largest base model used in our experiment is flan-t5-xl with 3b parameters. since the ability of instruction following is related to the model scale (wei et al., 2022), we leave the exploration of adopting larger instruction-tuned plms such as flan-t5-xxl and opt-iml (iyer et al., 2022) as future work."
1276,"limitations our study is limited in scope, studying only english question-answering data. we also acknowledge that the long-form answers we study are not always factually correct, as they can be outdated (zhang and choi, 2021) or incorrect as they are crawled from web forums (fan et al., 2019). further, our user study is limited in its scale, evaluating 175 instances, and does not carefully study potentially diverging interpretations from annotators of different demographics. we also do not extensively explore all summarization models, such as the extract-and-abstract approaches mentioned in related work."
1277,"limitations the limitations of our method are as follows: • we find that utilizing multi-view representations in the cross-encoder is an effective method for mvd, however, the ranking performance of the cross-encoder may slightly decrease. therefore, it is sub-optimal to directly use the cross-encoder model for entity ranking. • mention detection is the predecessor task of our retrieval model, so our retrieval model will be affected by the error of the mention detection. therefore, designing a joint model of mention detection and entity retrieval is an improvement direction of our method."
1278,"limitations due to the lack of theoretical support, it is challenging for us to formalize an annotation scheme for implicit persona attributes in the current stage, e.g., extracting an implicit triplet (i, like_animal, dogs) from a sentence “every day, i personally take my dogs out for a walk and lend a hand to my neighbors by occasionally taking their furry friends out for a stroll as well”, besides (i, have_pet, dogs). therefore, our personaext is not compatible with the implicit or multiple persona attribute triplet extraction tasks. additionally, our framework did not exploit complementary information from the context of the current utterance for paed. for an input with multiple dialogue utterances, it is hard for our model to match extracted persona triplets with the exact speaker because of the existence of pronouns and more than one speaker in dialogues."
1279,"limitations the core of promptrank lies in calculating the probability of generating the candidate with a designed prompt by the decoder, which is used to rank the candidates. our experiments have shown that the design of the prompt plays a crucial role in determining the performance of the method. while we have manually designed and selected some prompts to achieve state-of-the-art results, the process is time-consuming and may not guarantee an optimal result. to address this limitation, future research could focus on finding ways to automatically search for optimal prompts."
1280,"limitations of relying on lms’ parameters to memorize factual knowledge and to understand what factors affect factual knowledge memorization. our results show that memorization has a strong correlation with entity popularity and that scaling up models on long-tail distributions may only provide marginal improvements. we also demonstrate that non-parametric memories can greatly aid lms on these long-tail distributions, but can also mislead lms on questions about well-known entities, as powerful lms have already memorized them in their parameters. based on those findings, we devise simple-yet-effective adaptive retrieval, which only retrieves when necessary, using a heuristic based on entity popularity and relationship types. our experimental results show that this method is not only more powerful than lms or previous retrieval-augmented lms but also more efficient. limitations this work focuses on entity-centric factual knowledge and demonstrates that lms’ memorization is heavily affected by the popularity of the entities and the aspect of the entities being asked in the questions. it is important to emphasize that for running controlled experiments, we have relied on two synthetic datasets, and the extent to which our results apply to naturally occurring factual knowledge has not been firmly established. while we can be fairly confident about the relationship between scaling, retrieval, popularity, relationship type, and performance for the kinds of knowledge studied here, the effectiveness of adaptive retrieval will depend on many details of the question answering pipeline. moreover, our work depends on a definition of popularity that is time-dependent and may not perfectly reflect how frequently entities are discussed on the web. wikipedia page views are one possible definition of popularity for which we observe our results, and we invite others to improve upon it in future work. further research can expand upon this simple approach, perhaps drawing on insights from kadavath et al. (2022) to improve the effectiveness of adaptive retrieval. it is an open question if the same findings are applicable to other types of world knowledge such as commonsense. we conjecture that the concept of the subject topic (entity), as well as the aspect (relationship type), can be applied with some minor modifications, which future work can quantify memorization following our scheme. ethical considerations recent work (huang et al., 2022) shows that lms memorize personal information available on the web, which has significant security issues. our evaluation focuses on the memorization of general entity-centric knowledge, but our findings can be applicable to those areas. our findings suggest that lms are likely to have less reliable knowledge of minority groups. parrish et al. (2022) established that models often rely on stereotypes to answer in uncertain cases, so our results indicate that lms are likely to rely on stereotypes disproportionately for minority groups. future work could investigate whether retrieval augmentation reduces bias in these cases."
1281,"limitations though we include 6 systems in our annotation which reflect the current state-of-the-art, all of the models are transformer-based and fine-tuned on just the cochrane dataset, which may limit the diversity of our generated summaries. additionally, none of the systems are generating summaries that approach the accuracy of human-written summaries. as a consequence, though the summaries in our dataset span the spectrum of quality, they may have less coverage on the higher end of quality (summaries approaching the accuracy and utility of human-written review summaries). our analysis of evaluation metrics also assumes the existence of reference summaries. in many real-world summarization scenarios, reference summaries do not exist, and reference-free evaluation metrics are needed for assessment. we refer the reader to related work in reference-free summarization evaluation (vasilyev et al., 2020; gao et al., 2020; luo et al., 2022), which have been found in some settings by fabbri et al. (2021) to exhibit even lower correlation with human notions of summary quality; the performance of these metrics on mslr evaluation is unknown and is left to future work. our notions of summary quality also do not necessarily correspond to clinical utility. as with anything in the medical setting, it is of utmost importance to verify correctness and the quality of evidence before using any generated text to make or guide clinical decisions. ethical considerations as with other applications of nlp in the medical domain, results of mslr systems must be verified by domain experts before they should be considered for use in clinical guidance. we do not intend the system outputs included in our dataset and analysis to be used for such end applications, as this would be clearly premature given the low quality of generated summaries and our lack of ability to assess the prevalence of factuality errors in these summary texts. nonetheless, we believe that medical mds holds eventual promise, and it is of vital importance that we study its challenges and how to measure and detect quality issues in generated text."
1282,"limitations in this work, we generate diverse responses through large-scale sampling in the oversampling stage. although we use the compression and distillation models to speed up, the problem of generation speed still exists. thus, one of the limitations of this work is the additional time cost when generating large-scale candidate responses. in addition, we use existing dialogue models for dialogue generation, mainly used in short text generation and unsuitable for long text generation, which is another limitation of this work."
1283,"limitations our research focuses on the adversarial attack itself and provides a framework that can be potentially used in different adversarial training strategies. we limit ourselves on attacks in this work, but it would be interesting to investigate logic-based attacks in adversarial training. we will leave that as future work. the proposed attack approach is also limited by the limitations of natural logic, while the latter has been a classical logic mechanism. for example, our proposed framework has less deductive power than first-order logic. it cannot construct attacks building on inference rules like modus ponens, modus tollens, and disjunction elimination. as discussed in the paper, some components of the generation and quality control process can be further enhanced."
1284,"limitations exist (discussed in §7.3), compared to traditional approaches the models trained in this study are expected to reduce biases. their value is not limited to predicting dates for individual manuscripts, but they can be applied to any attribute of a group of papyri, e.g. the place of provenance or the text’s type. at the same time, easily accessible open-source metadata exist for most published papyri (§3.1)."
1285,"limitations ircot relies on the base lm to have a zero or few-shot cot-generation ability. while this is commonly available in large lms (over 100b), it’s not as common for small lms (under 20b), which to some extent limits ircot adoptability. given the recent surge of interest (tay et al., 2023; magister et al., 2022; ho et al., 2022), however, smaller lms will likely increasingly acquire such ability, making ircot compatible with many more lms. ircot also relies on the base lm to support long inputs as multiple retrieved paragraphs need to fit in the lm’s input, in addition to at least a few demonstrations of qa or cot with paragraphs. this was supported by the models we used as code-davinci-002 (gpt3) allows 8k tokens and flan-t5-* uses relative position embeddings making it as extensible as the gpu memory constraints allow. future work can explore strategies to rerank and select the retrieved paragraphs instead of passing all of them to the lm to alleviate the need for the lm to support long input. the performance gain of ircot retriever and qa (over oner and zeror baselines) come with an additional computational cost. this is because ircot makes a separate call to an (l)lm for each sentence of cot. future work can focus on, for instance, dynamically deciding when to retrieve more information and when to perform additional reasoning with the current information. lastly, a portion of our experiments was carried out using a commercial llm api from openai (code-davinci-002). this model was deprecated by openai after our submission making the reproduction of these experiments challenging despite our best efforts, just like any other work using such apis. the trends discussed in the paper (ircot > oner > nor), we believe, would still hold. additionally, all our experiments using flan-t5-*, which exhibit similar trends as that of gpt3, will remain reproducible, thanks to its publicly available model weights. ethical considerations language models are known to hallucinate incorrect and potentially biased information. this is especially problematic when the questions asked to it are of a sensitive nature. while retrievalaugmented approaches such as ours are expected to alleviate this issue to some extent by grounding generation in external text, this by no means solves the problem of generating biased or offensive statements. appropriate care should thus be taken if deploying such systems in user-facing applications. all the datasets and models used in this work are publicly available with permissible licenses. hotpotqa has cc by-sa 4.0 license15, 2wikimultihopqa has apache-2.0 license16, musique and iirc have cc by 4.0 license17, and flan-t5-* models have apache-2.0 license."
1286,"limitations of the conventional fact retrieval pipeline, usually consisting of entity mention detection, entity disambiguation and relation classification, which not only requires additional labels for training each subcomponent but also is vulnerable to the error propagation across submodules. to this end, we proposed the extremely simple direct fact retrieval (difar) framework. during training, it requires only pairs of input texts and relevant triplets, while, in inference, it directly retrieves relevant triplets based on their representational similarities to the given query. further, to calibrate the ranks of retrieved triplets, we proposed to use a reranker. we demonstrated that our difar outperforms existing fact retrieval baselines despite its great simplicity, but also ours with the reranking strategy significantly improves the performances; for the first time, we revealed that fact retrieval can be easily yet effectively done. we believe our work paves new avenues for fact retrieval, which leads to various follow-up work. limitations in this section, we faithfully discuss the current limitations and potential avenues for future research. first of all, while one advantage of our direct fact retrieval (difar) is its simplicity, this model architecture is arguably simple and might be less effective in handling very complex queries (sen et al., 2022). for example, as shown in figure 2, even though our difar framework can handle the input queries demanding multi-hop retrieval, the performances on such queries are far from perfect. therefore, future work may improve difar by including more advanced techniques, for example, further traversing over the kg based on the retrieved facts from our difar. also, while we use only the text-based similarities between queries and triplets with lms, it is interesting to model triplets over kgs based on their graph structures and blend their representations with representations from lms to generate more effective search space. also, we focus on retrieval datasets in english. here we would like to note that, in fact retrieval, most datasets are annotated in english, and, based on this, most existing work evaluates model performances on english samples. however, handling samples in various languages is an important yet challenging problem, and, as future work, one may extend our difar to multilingual settings."
1287,"limitations in this work, we have only analyzed the common errors of two models (i.e., gpt-j and chatgpt) in the instructional dialogue task. one open question is whether other gpt-based models or models with other architectures (e.g., encoder-decoder models) also have the same issue in this task. our work and dataset are also limited to the english language. ethical considerations to collect recipe-grounded conversations we hired crowd workers using the prolific platform.6 the study was conducted with the approval of our local irb. the compensation was derived based on prolific’s payment principles. we estimate the hourly pay for crowd workers was $15.49 (details in appendix d). crowd workers were strictly asked not to write any offensive content or personal information. 6https://www.prolific.co/"
1288,"limitations in our proposed model, we introduce a hyperparameter n as the number of event proxy nodes. the value of n needs to be pre-set. setting n to a value larger than the actual event number in a document would lead to computational redundancy as more proxy nodes would be mapped to the null event. however, setting n to a small value may miss some events in a document. we have experimented with automatically learning the value of n based on an input document in procnet. but we did not observe improved event extraction performance. as such, we simply set it to 16. in the chfinann dataset, 98% documents have less than 7 events annotated. this results in the learning of many redundant proxy nodes for such documents. it remains an open challenge on automatically learning a varying number of event proxy nodes based on an input document. reducing the number of redundant proxy nodes can reduce training time further. another shortcoming is the limited capability of procnet in capturing the long-term dependencies of sentences, as have been discussed in the perevent-type results in section 4.2 and 4.3. we observed a relatively worse performance of procnet in dealing with long documents with more than 40 sentences as it does not explicitly model the interrelations of sentences. one possible direction is to explore the use of a heterogeneous graph which additionally models the entity-entity, entity-sentence, and sentence-sentence relations. we will leave it as the future work to study the trade-off between event extraction performance and training efficiency."
1289,"limitation, we report the results of jddc/sr, jddc/d-sts, ic and senti."
1290,"limitations although the proposed method achieves exciting results, there are still some issues that need to be addressed in the future: (1) when designing the structure of hssa layers, we assume that humans tend to understand a dialogue from the local to global perspective, which supports the existence of inner- and inter-segment self-attention layers. (2) we use 2 public chinese corpora, jddc and edc, for post-training. though there are diverse topics in them, it is desired to introduce other corpora from different domains and languages. (3) ssl tasks are arranged in post-training via cmtl (sun et al., 2020) based on the intuitive understanding of their semantic levels and difficulties. therefore, to combine the power of each ssl task more effectively, new training strategies need to be explored."
1291,"limitation of detoxified language modeling, which cannot be avoided unless the provided prompts are rephrased into non-toxic prompts while maintaining their semantic meaning. in addition to developing a safe lms, it is essential to address the issue of lm hallucination, which refers to the generation of factually incorrect texts. while our paper does not focus on this aspect, ensuring both safety and factual valid generation of texts is vital for real-world applications of lms."
1292,"limitations: (1) the datasets used in this work are mostly collected from social media. in the future, we plan to collect sarcastic texts from various sources, such as the literature and films, and conduct more experiments with them. (2) our exploration of sarcasm theories still has some space to improve. though the incongruity theory is the mainstream in the community, there are other theories worthy to investigate in the future."
1293,"limitations we state the limitations of this work from the following aspects. first, we make an initial assumption about the dynamics between exercise difficulty, vocabulary, and student knowledge. while we believe our assumption is sensible in the domain of language learning, we acknowledge that we make some simplifications for the ease of modeling. for example, we measure difficulty using individual performance, whereas a better way could be combining it with inherent problem difficulty, e.g., text complexity. besides, we only consider vocabulary mastery in defining student knowledge and predicting their performance. exploring more dimensions of language knowledge (e.g., syntax) might lead to a finer-grained personalization. second, our model relies on student learning logs to estimate their realtime knowledge states. this model might face the cold start problem when dealing with insufficient history. though it is beyond the scope of this study, techniques like computerized adaptive testing can be used to combat this problem. lastly, due to the lack of a real learning environment, we discuss the educational promise of our model with simulation experiments. in the future, a user study can be incorporated to validate our"
1294,"limitations study scope. while our study only considers three papers, this is by design. as our goal is to study user experience, by fixing papers to be within a specific topic area and time requirement, and having people with different skill levels reproduce the same papers, this allows us to have sufficient samples to understand general behaviors. it also blocks other nuance factors (e.g., introduced by different papers) on beginners’ experience. each of our selected papers presented students with unique reproducibility barriers, and consequently resulted in a wealth of helpful insights. furthermore, finding reproducible nlp papers that satisfied our constraints (as laid out in section 3.1.2) was surprisingly difficult, with only 3 out of 24 considered papers found to be reproducible within our constraints. nevertheless, this study is still on the small scale. engaging a larger community in a large scale study may provide additional insight. related to this, our study only includes a population of mostly graduate students at our university. considering beginners from different educational backgrounds or regions could reveal more comprehensive insights, and we greatly encourage future efforts at a community level toward better understanding the needs of nlp beginners. gpu runtime calculation. it is also worth noting that it is difficult to consistently calculate runtime (as introduced in section 3.2.1) of code on gpu hardware, as fluctuations may occur due to a number of factors, including the specific gpu hardware allocated to a student,27 driver versions, and 26https://arc.umich.edu/ 27while experts used nvidia tesla v100 gpus with up to 16gb memory to reproduce results, nvidia a40 gpus with up to 48gb memory are also available within the cluster. file systems experiments were run with. to minimize the impact of such issues, we chose to reproduce experiments that used small models and had shorter expected runtimes. given that we observed runtimes up to several times larger than expert runtimes, we thus expect that trial and error in setting up experiments accounted for most fluctuation in observed runtimes."
1295,"limitations, we leave exploiting the rewriting model to future work. in table 2 and fig. 5 we demonstrated that our question rephrasing model works well for producing fluent questions that reduce ambiguity. furthermore, in table 3 we showed that the model’s representations contain information about the underlying question being asked, even though this information is not directly present in the training data and we do not include any supervision from our dataset. future work could examine utilizing the rephrasing model in a search-engine environment, where users are actively querying about images. given an ambiguous question identified and a set of answers to it from a vqa model, our model could be used to rephrase the question according to each answer. just as a presenter will often rephrase a question from the audience, the model might present the user with the rephrased question it is actually answering, which would result in better interpretability. this improved interpretability might teach users how to interact with the model."
1296,"limitations currently, to deal with spelling, missing, redundant character errors in chinese text, we jointly pre-train two sub-tasks based on a masked language model with task-specific attention mechanism and utilize re-tagging rules to reformulate the length of text during prediction. the proposed model might be less effective in more complex scenarios: word-level case according to the structure of our model, it could theoretically handle errors that are not limited to character-level, such as redundant or missing words. however, the currently used self-attention matrix works between tokens instead of spans of tokens. a novel attention mask strategy might be considered. if the problem is solved, then our model would be able to handle both chinese spelling correction (csc) and some kinds of grammatical error correction (gec) tasks at the same time. task-specific backbone case the backbone of the proposed model is bert that is not taskspecific, while some errors in sighan happened in entities that might need priori knowledge to solve. for example, the correct sentence is “我 要跟我的朋友去师大夜市” and the wrong sentence is “我要跟我的朋友去市大夜市”, where the mispelled character belonging to an entity “师 大” that is abbreviation of “师范大学” (means “normal university”). to improve the performance of our model in more complicated applications, backbones that learn more task-specific knowledge should be considered. languages mixture case in real world ocr or asr applications, a chinese character might be confused not only by another chinese character, but also by an english character due to their similar pronunciation or shape. for example, the chinese character “丁” is visually similar to the english capital letter “j”, while the chinese “喂” (means “hi”) is phoneticly similar to the english word “way”. furthermore, a same character of simplified chinese and traditional chinese might be visually different. high efficiency case industrial applications often require the prediction time in milliseconds-level under controlled usage of gpus, which would bring troubles to large models. distillation or truncating strategies might be a way to improve the proposed model."
1297,"limitations the primary limitations concern the dataset we created, which serves as the foundation of our findings. our dataset suffers from four key limitations: reliance on papers with code our system is trained and evaluated to retrieve datasets from papers with code datasets (pwc). unfortunately, pwc is not exhaustive. several queries in our test set corresponded to datasets that are not in pwc, such as iwslt 2014 (birch et al., 2014), pascal voc 2010 (everingham et al., 2010), and chime4 (vincent et al., 2017). papers with code datasets also skews the publication year of papers used in the datafinder dataset towards the present (the median years of papers in our train and test set are 2018 and 2017, respectively). for the most part, pwc only includes datasets used by another paper listed in papers with code, leading to the systematic omission of datasets seldom used today. popular dataset bias in the test set our test set is derived from the scirex corpus (jain et al., 2020). this corpus is biased towards popular or influential works: the median number of citations of a paper in scirex is 129, compared to 19 for any computer science paper in s2orc. the queries in our test set are therefore more likely to describe mainstream ideas in popular subields of ai. automatic tagging our training data is generated automatically using a list of canonical dataset names from papers with code. this tagger mislabels papers where a dataset is used but never referred to by one of these canonical names (e.g. nonstandard abbreviations or capitalizations). therefore, our training data is noisy and imperfect. queries in english only all queries in our training and test datasets were in english. therefore, these datasets only support the development of dataset recommendation systems for englishlanguage users. this is a serious limitation, as ai research is increasingly done in languages other english, such as chinese (chou, 2022)."
1298,"limitations we see four main limitations regarding our work. first, we have evaluated our models on a dataset containing events of one type only. it remains to be seen how applicable our formulation and methods are to other historical datasets and event types. second, given the nature of the historical question our dataset targets, it contains documents only from one language family. extending our methodology to languages from other language families might pose further challenges in terms of multilinguality. third, our method relies heavily on automatic translation tools, which are biased toward translating historical texts into modern language. this can negatively affect the performance of our models. lastly, in real-life cases, machine readable historical texts are often extremely noisy, suffering from high level of ocr errors and other text extraction mistakes. conversely, we have tested our methods on relatively clean datasets, with the unannotated dutch material as the only exception. we leave a more thorough study on how well our proposed methods are suitable for noisy text to future work. ethical considerations studying texts about the history of slavery poses ethical issues to historians and computer scientists alike since people of color still suffer consequences of this history in the present, not least because of lingering racist language (alim et al., 2016, 2020). as researchers, we know that an important ethical task is to develop sound nlp tools that can aid in the examination of historical texts containing racist language, while endeavoring at all costs not to reproduce or perpetuate such racist language through the very tools we develop. the enslaved people described in the newspapers adverts used in this study were alive centuries ago, so any immediate issues related to their privacy and personal data protection do not apply. nonetheless, the newspaper adverts studied here were posted by the oppressors of the people who tried to liberate themselves, and contain many examples of highly racist and demeaning language."
1299,"limitations our approach is only a small step towards mining more comprehensive, high-quality topic structures, and there are many more issues that need to be addressed in the future. for example, there are still limitations in the current assessment of the structure of topics mined by different models. examples include assessing the validity of topic hierarchical indicators by topic specialization and the validity of the symmetric structure of topics through clustering as we have demonstrated. all these assessment methods are only a sideways demonstration of the interpretability of the topic structure. besides, there is still a lot of a prior information available in the field of topic modelling, e.g. wordnet, and it may help researchers to explore further in the field of topic modelling if they can combine prior human knowledge and information on topic-words obtained from models to define quantitative metrics that are more consistent with human understanding."
1300,"limitation of token dropping in accelerating language model training. based on a series of preliminary analyses, we find that removing parts of tokens would lead to a semantic loss problem, which causes vulnerable and unstable training. furthermore, experiments show such a semantic loss will hinder the performance of token dropping in most semanticintense scenarios. to address this limitation, we improve token dropping with a novel semanticconsistent learning algorithm. it designs two semantic constraints to encourage models to preserve semantic information. experiments show that our approach consistently and significantly improves downstream performance across all task types and model architectures. in-depth analyses prove that our approach indeed alleviates the problem, and further improves training efficiency. in future work, we will explore the effectiveness of our method on more advanced discriminative language models (he et al., 2020; zhong et al., 2023b). also, it will be interesting to revisit and address the semantic loss problem in efficient training methods for generative language models (such as gpt3 (brown et al., 2020)). limitations our work has several potential limitations. first, given the limited computational budget, we only validate our sctd on the large and base sizes of bert models. it will be more convincing if scaling up to the larger model size and applying sctd to more cutting-edge model architectures. on the other hand, besides the downstream performance, we believe that there are still other properties, e.g., generalization and robustness, of mlms that can be improved by our sctd approach, which are not fully explored in this work."
1301,"limitations one of the primary limitations of this work is that this is essentially an empirical study. although we provide extensive experiments to show that the proposed approach demonstrates significantly better results in different settings, currently we do not provide any theoretical guarantees for this approach. second, many of our experiments would not be easily reproduced in languages other than english, that lack sufficient linguistic resources. during this study we used the gpt-2 and gpt-neo language models, which have been trained on large amounts of english text. finally, anecdotally we observed that this approach can also increase hallucination behaviors, which are a common issue with many text generation models. during application, one would have to take necessary measures to monitor the hallucinations produced by the model."
1302,"limitations this work applies to languages that have a modest amount of data in parallel with english and are represented in pre-trained language models. these are typically high to mid-resource languages. very low-resource languages might not have enough parallel corpora to extract sufficient ner training data. with limited parallel data and/or limited representation in pre-trained lms, it will be difficult to get high-quality word alignments for projection. we use span-based annotation projection to alleviate word alignment errors to some extent."
1303,"limitations inherent debatability in false presuppositions. as discussed earlier, the validity of presupposition is inherently debatable and largely depends on the background context, i.e., even experts in formal semantics and pragmatics observe a high disagreement rate (jeretic et al., 2020). our proposal in using the most upvoted comments partially address the issue, but not perfectly, as discussed extensively in section 5.2. one avenue for future work is to consider extra-linguistic context such as individuals background when judging the validity of presuppositions (zhang and choi, 2021). evaluating massive language models. massive language models such as gpt-3 (brown et al., 2020) have been shown impressive performance in open-ended question answering (nakano et al., 2021). our paper does not include large-scale, systematic evaluation of such models. instead, we conduct a small-scale case study with gpt-3 text-davinci-002. see appendix d for details. most generations are roughly on the right topic, but often contain information that is factually false and do not precisely answer the question. moreover, they rarely explicitly identify false presupposition and provide corrections, indicating that gpt-3 is far from solving our task. we think future work may explore larger-scale evaluation in a more systematic manner. false presuppositions beyond online forums. the domain of crepe is limited to online forums (reddit). while this choice was made due to the availability of large data and its general domain, we argue that false presuppositions are not specific to such domains. for instance, we find that a similar portion (25%) have false presuppositions on information-seeking questions on nlp research papers posed by nlp experts; see appendix e for details. we think future work can explore creating benchmarks on such domains, as well as studying false presuppositions on a broader set of domains that require domain expertise."
1304,"limitations as the experimental datasets are chinese and the word segmentation tool is employed, some parsing errors may exist. also, the token-token matrix is built on all tokens in each document, resulting in a large-scale matrix and the reduction of model training. all these are the limitations of this paper. nevertheless, if the corpus is english, the first limitation does not exist. also, the spatio-temporal efficiency in table 1 is acceptable. importantly, the experimental results obtained in this paper are based on the limitation, which indicates that it is effective to implement our model according to the segmentation results by syntactic tools."
1305,"limitations since the unified graph is very large, it will take more time to construct the subgraphs before the first training. but after saving these subgraphs, there is no need to rebuild the subgraphs in the subsequent training process. on the other hand, the aligned entities among different kgs is a necessary condition for our proposed framework and otherwise, our model can not conduct knowledge transfer among the given kgs without an alignment model or other techniques."
1306,"limitations in this paper, we focus on efficiently and accurately predicting missing links in kgs using low-dimensional features and binary classifiers. greenkgc can achieve impressive efficiency during the inference stage and can be applied to various platforms with memory constraints because of its superior performance in low-dimensional space. however, the whole training process of greenkgc still requires high-dimensional pre-trained embeddings as initial features. therefore, it may hinder greenkgc from being trained on resourceconstrained platforms from scratch. in addition, the current greenkgc model is proposed under a transductive setting, where we focus on a fixed entity and relation set. the generalizability of the few-shot learning capability on greenkgc is yet to be explored. the above-mentioned two limitations can be addressed by leveraging textual information in kgs. in recent years, text-based kgc models (wang et al., 2022a, 2021a,c), which take advantage of entities’ names and descriptions to obtain features, are more and more popular. we may extend greenkgc using word embeddings from pretrained language models as initial features to overcome the current limitations. in addition, continual learning on the classifiers (mai et al., 2021), which aims at learning new training samples without forgetting the old training samples, i.e. catastrophic forgetting, is also an active research topic. thus, greenkgc can incorporate such techniques to improve its generalizability to new data."
1307,"limitations one limitation of the proposed method is that it does not consider domain-specific information to evaluate informativeness. the phraseness module has access to domain-specific knowledge, which are the phrases that occur in similar contexts, i.e. the references. on the other hand, the informativeness module only employs a domain-general sentence embedding model to measure informativeness of phrases. therefore, the integration of both domain-specific and domain-general information for the evaluation of informativeness may be worth further investigation. another limitation of this work is that we only tested the proposed method on short texts. therefore, it is uncertain of the proposed framework’s performance on long text documents. handling long texts could be significantly more difficult than short text, as long texts contain much more information (can discuss a variety of topics). the final limitation of this work is the absence of experiments on using different sentence embedding models to construct the informativeness module. therefore, it might be useful to explore the impact of different sentence embedding models on keyphrase generation performance. we leave this for future work."
1308,"limitations the current dialogue system is mainly based on deep neural network, like transformer structure, which often requires a large number of data sets for training model. however, there are still some deficiencies in our dataset. we will further label and create more dataset to train model. in addition, in order to improve the quality of dialogue, our model parameters are relatively large, which affect the speed of dialogue generation to some extent. we will explore some methods, such as knowledge distillation, to reduce model parameters to improve the speed of dialogue generation on the premise of keeping the quality of dialogue generation unchanged."
1309,"limitations in this paper, we present a novel knowledge injection paradigm plug-and-play knowledge injection for plms. we show existing methods can not be well applied to the new paradigm and propose maptuning as a preliminary exploration of methods. the paradigm plug-and-play knowledge injection has a limitation in terms of its assumption. it assumes that a plm should be fine-tuned for downstream tasks. however, very large-scale plms can perform zero-shot learning or in-context learning on downstream tasks without being fine-tuned. future work may extend the definition of the proposed paradigm to make it meaningful in these scenes. the method map-tuning has three limitations in terms of its applicability. firstly, we did not evaluate map-tuning for plms pre-trained by other language modeling objectives (e.g., casual language modeling) besides mlm. as its spirit can be easily generalized to various language modeling objectives, we leave this evaluation as future work. secondly, we did not evaluate whether the plm can do complex reasoning (e.g., multi-hop reasoning) based on the knowledge injected by map-tuning. thirdly, map-tuning is designed to plug structural fact knowledge. it is also meaningful to plug other diverse knowledge bases, including text corpora, voice, images, and even other plms, which are not covered by our work."
1310,"limitations to further inspire the follow-up work, we summarize our limitations as follows: 1) we only preliminarily reveal the overthinking phenomenon in the open-world scenario, and explore how to mitigate and utilize it during inference. we do not continue to conduct more in-depth research on the broader forms of overthinking in the open-world scenario and do not explore whether there are differences in its performance in different models. in addition, whether it can be solved or alleviated by other ways, such as training methods. 2) from the results of sections 5.2, 5.3 and the corresponding appendix a, b, it seems that there is room for further improvement in the speedup of our method. we leave how to achieve the best accuracy-speed trade-offs to subsequent research. 3) we have preliminarily verified that our method can be compatible with more detection algorithms and models, and look forward to exploring more methods and models."
1311,"limitations an obvious limitation of our work is the considered search space. although we showed that it is well suited for the data used in practice by the nlp community, this may not hold in more general settings. moreover, we only experiment in english. we suspect that similar results would hold for morphologically-rich languages as we expect, in the latter case, that constituents are shorter (i.e. morphologically-rich languages heavily rely on morphological inflection, so we expect more mentions spanning a single word), see (haspelmath and sims, 2013, section 1.2 and table 1.1). however, this is not guaranteed and future work needs to explore the multilingual setting. finally, in this work we do not consider discontinuous mentions, which is an important setting in real world scenario."
1312,"limitations we have not tested all possible recent methods on lemon. we have used expensive gpu resources to speed up the training process on lemon, with 8 nvidia a100 sheets, but consistent results can also be obtained with 8 v100 sheets. our work focuses on chinese. other languages, such as japanese and korean, could benefit from the same technique, but have not been studied in this work."
1313,"limitations the proposed method has several limitations: 1) the current approach achieves hunky context reasoning performance in the cross-modal scene of a single text clue and image, but the context reasoning capability in the scene containing multiple textual and visual clues still needs to be further explored, such as video and long text. 2) from the experimental results, we observed that the visual prefix length greatly impacts the stability of language models infused with visual information. hence, we still need to explore effective and stable vision-aided language models for natural language processing and multi-modal scenarios. 3) we also hope this work could spark further research on improving the long context reasoning capability of pretrained vision-language models. acknowledge we thank the anonymous reviewers for their constructive comments, and gratefully acknowledge the support of natural science foundation of china (no.62006061, 61872107) and the stable support program for higher education institutions of shenzhen (no.gxwd2020123015542700320200824155011001)."
1314,"limitations although our modeling of event centrality is feasible and effective, there is still space for improvement. the performance of event centrality prediction could be higher by using more advanced encoding methods. besides, it is meaningful to further explore the interactions among various types of event relations. existing datasets only cover limited relation types at once, and many works focus on the identification of causal relations alone. in this paper, although we further consider the effect of coreference relations and perform joint classification, there are still some other relations that can be explored, such as temporal relations, subevent relations, etc."
1315,"limitations in this paper, we present a supervised adversarial contrastive learning (sacl) framework with contextual adversarial training to learn class-spread structured representations for context-dependent emotion classification. however, the framework is somewhat limited by the class imbalance issue, as illustrated in section 4. to more comprehensively evaluate the generalization of sacl, it is necessary to test its transferability in low-resource and out-of-distribution scenarios, and evaluate its performance across a wider range of tasks. additionally, it would be beneficial to explore the theoretical underpinnings and potential applications of the framework in greater depth. the aforementioned limitations will be left for future research."
1316,"limitations one major limitation of our work is that our experiments are only conducted on docred and re-docred that consist of documents from general domain. yet, information extraction has many broader applications in specific domains, e.g. biomedical data. we plan to adapt tag to some biomedical datasets, like cdr (li et al., 2016) and gda (wu et al., 2019), in the future. besides, since tag consists of a number of modules and use plm as encoder, the training process takes relatively more time and computational resources than dedicated docre model that only extract relations. we concern that it may affect the scalability with larger amount of either data or parameters."
1317,"limitations as discussed in appendix b, there is still a gap between the synthetic dialogues and the humanwritten dialogues in terms of quality. the synthetic dialogues sometimes do not express knowledge with sufficient accuracy. also, some of the synthetic dialogues are less coherent and diverse than the human-written ones. we believe that these issues can be mitigated in two aspects. first, similar to (zheng et al., 2022), employing larger lms can help generate utterances with higher quality. second, introducing knowledge graph and textual reasoning techniques to produce better dialogue flows. in addition, using large lms inevitably requires more computational resources. however, it is still a cheaper and promising alternative to hiring expensive labor."
1318,"limitations the limitations of this work can be concluded into two points: (1) to obtain the associations between semantic elements, semsin needs to transform the texts into the corresponding semantic graphs. existing methods can only transform single sentences into semantic graphs, and cannot parse texts containing multiple sentences. therefore, this method is not suitable for identifying causal relations between events in different sentences. (2) semsin only exploits the semantic structures of the texts and does not utilize external knowledge. external knowledge is also important for the eci task, and simultaneously exploiting semantic structures and external knowledge is a good direction for future studies."
1319,"limitations although our proposed knowledge transfer methods work well on wos in the few-shot setting, it is less effective on 5-datasets. moreover, all methods fail in the full-shot setting. based on our approach, a more general approach to knowledge transfer is expected in future works. in addition, our approach requires a well-trained language model for task identification and a transformer-based model (well-trained also) for parameter efficient tuning. therefore, it is challenging to cooperate our approach with a language model with random initialization or non-transformer architecture."
1320,"limitations although the proposed framework yields promising results on two fine-grained emotion datasets— goemotions and empathetic dialogues—there remain limitations, including: (1) to the best of our knowledge, there is no such fine-grained emotion dataset in other languages. although theoretically, our method should work fine on languages other than english, we can only show the results in english. (2) the proposed method works best when the label structure contains hierarchy, especially when the semantics of some labels are close and difficult to distinguish. when the label structure is flat and independent, our method may backoff to a conventional classification model."
1321,"limitations while imaginary concepts are encouraged in stylized visual storytelling task, it would be better if these literary imaginations are more related to visual contents. in order to improve semantic relevance, we could restrain models from generating visually unrelated descriptions, or make pseudo images more related to stylized stories. however, the former solution is likely to harm the style expression by decreasing stylistic imaginations. for the latter scheme, we have tried to generate pseudo visual inputs with pre-trained text2image model (ramesh et al., 2022), however, there is a domain gap between photos in vist and images generated with stylized sentences. it would be a challenging and interesting problem to be explored in the future."
1322,"limitations although wspalign successfully outperforms all existing baselines, it is still limited to the accessibility of low-resource language information. for example, the collection of pre-training data requires multi-lingual pos tagging tools to identify which words are common or not. it also requires a multilingual plm and wikipedia hyperlinks to make the alignments, which could be inaccessible for an exceptional minority language. but note that we showed wspalign’s cross-lingual ability in §5.1, which implies that this issue can potentially be addressed in the direction of pre-training on large-scale monolingual data with our future effort. besides, this paper lacks evaluation on real low-resource language benchmarks because there is no existing test set. we will try to collect and annotate low-resource word alignment data in our future work."
1323,"limitations this paper fundamentally considers a scenario in which practitioners rent cloud gpus. in the case of hosting gpus by themselves, the two strategies explored in this study would not be simply comparable. however, in practice, when training a large model (w/ 8 a100 gpus), we conjecture that renting gpus could be preferred in many cases as scaling compute powers is not trivial and prohibitively expensive (izsak et al., 2021; obandoceron and castro, 2021; minixhofer et al., 2022). it is also noteworthy that in the future, computational costs may become cheaper as new hardware advances, the pricing policy by cloud platform services changes, and more optimization techniques are applied. on the other hand, human annotation cost is likely to be the same at least or even more expensive. with cost changes in such a direction, the same"
1324,"limitations in this section, we would like to discuss two limitations of od-rte as follows: (1) in the current table-filling based rte methods including od-rte, the issue of sparse labels in the tables still exists. as cells of the table, the number of positive and negative token pairs is grossly unbalanced. in this work, although we alleviate the problem of unbalanced positive and negative relations by introducing the relation negative sampling strategy, the problem of unbalanced positive and negative token pairs still exists and needs to be addressed. we will try to mitigate the problem in our future work. (2) currently, od-rte can only be applied to the relational triple extraction task. in recent years, the table-filling-based approaches have been widely used for many information extraction tasks besides the rte task, such as opinion mining (wu et al., 2020) and named entity recognition (li et al., 2022). therefore, in future work, we will try to extend the object detection framework to other information extraction tasks to let the model make full use of the information of entity boundaries."
1325,"limitations in this work, we demonstrate the effectiveness of the proposed mp2 with the backbone ptm of cpt-large on a set of chinese nlp tasks. due to the expensive pre-training cost, we did not explore mp2 on other ptms with varying sizes, pretraining objectives and architectures. besides, it is also unknown how does the number of pre-training tasks affect the performance of mp2. for resourcerich languages such as english and chinese, it would be promising for mp2 to be well-performed since one can easily collect sufficient public upstream tasks. nevertheless, for low-resource languages or domains, the effect of mp2 is still underexplored."
1326,"limitations our system employs a modified sequence-tosequence architecture to implement the response generator. since the length of dialogue context increases as the dialogue continues, the generator needs to input multiple long dialogue contexts to the encoder simultaneously, each for a retrieved entity. this may cause redundancy in the input and lowers the proportion of kb-related information. we will explore more efficient architectures for the response generator in future work."
1327,"limitations our analyses are based on models with t5-like architectures and span denoising training objectives. thus, our findings may not generalize to other types of encoder-decoder models (e.g., bart), nor encoder-only and decoder-only models. we believe this is unlikely, given that similar findings have been shown for models with architectures and objectives that differ significantly from t5’s (huebner et al., 2021; warstadt and bowman, 2020). nonetheless, it cannot be ruled out. our analyses are also based entirely in english, and only leverage two syntactic transformations. it is possible that our findings will not generalize to other languages, given that certain grammatical features (e.g., more extensive case marking) induce more syntax-sensitive behavior given a similar amount of training data across languages (mueller et al., 2020; ravfogel et al., 2019); thus, perhaps less wikipedia or c4 data is needed in these languages for models to acquire hierarchical preferences. it is also possible that, within a language, a model could adopt a hierarchical inductive bias for one type of transformation, but not another— especially if one transformation is much more frequent than the other. indeed, the frequency of particular words positively correlates with syntactic evaluation accuracies (wei et al., 2021; newman et al., 2021), and it would be reasonable to expect a similar trend for the frequency of syntactic transformations. thus, future work should investigate more transformations in more languages to ensure that these findings are consistent."
1328,"limitations on western-centricity the majority of the crowdworkers producing the source data (δ-social and delphi) and δ-clarify were located in the united states. due to this, the predictions generated by clarifydelphi are currently limited to representing only the perspectives of western culture (particularly the united states). overcoming the western-centric bias is a compelling direction for future research. on defeasibility we rely upon delphi to produce acceptable judgments given a situation and the modifying context as a measure of defeasibility. we recognize that, however, delphi is not perfect and is characterized by a variety of limitations such as limited cultural awareness and inconsistent predictions (jiang et al., 2022). investigating improved methods for identifying answer divergences that will better capture defeasibility is a topic for future investigation."
1329,"limitations while promising, hint comes with several drawbacks related to its ease of use. first, hint takes advantage of the fact that (a) instructions are often long, and (b) often we want to perform inference over a larger (> 100) amount of examples with the same instruction. if either of these items are not true in a setup, then hint is unlikely to provide a large benefit over simply including the instruction with the input text. this can be seen in the smaller compute savings provided by hint for p3 in table 2. second, while hint is computeefficient at inference time, it is far more costly to train, as it effectively requires running the underlying model together with the hypernetwork for every batch. this means that while hint may be useful for practitioners with limited compute budgets, it may be difficult to train hint models with the same limited budget. finally, we train and test on english data only, and do not explore the generalisation of our approach to multilingual setups. considering the success of hypernetworks in multilingual settings (platanios et al., 2018; baziotis et al., 2022; ustun et al., 2022), we believe this is a promising direction for future research. as such, while promising, hint is limited by certain assumptions made about the length and format of instruction-augmented data, and we hope further improvements of the method work towards loosening these assumptions."
1330,"limitations model coverage. our study is targeted specifically at gpt-3 and it would be interesting to study feature bias patterns on other large language models such as opt (zhang et al., 2022) and bloom (scao et al., 2022); and it is possible that our intervention methods may have different effects on these language models trained with different data sources and scales. task coverage. apart from model coverage, our analysis is focused on only four common binary classification tasks. our main metric, h-accuracy, compares the predictions between a learned function f and a feature function h. for simplicity, we only study binary functions (consistent with prior work) to illustrate the main ideas, but the framework applies equally well if f and h are multi-class classifiers. for example, in the case of the threeway nli task, we might set h1 to predict on the basis of entailment / contradiction / neutral, and h2 to predict on the basis of the genres – e.g. fiction / government / telephone. future work could extend our framework to more tasks, and consider how to apply it to more complex tasks such as generation. feature coverage. our current experiments are limited to a set of hand-crafted features. one potential way to systematically scale our approach is to develop novel techniques for automatic feature discovery, for example, to cluster the data and treat each cluster as having a distinct feature. explaining feature biases. while our empirical findings shed light on the feature bias patterns of gpt-3, we do not yet have a"
1331,"limitations our proposed taxonomy is subject to extension, and we expect new phenomena to be included into its scope as the field progresses and as more document sources are considered. using a taxonomy as an organizational basis for the proposed schema is dictated by our aim to keep the schema simple. the design of future, formalized reporting schemata might adopt an onthology-based approach as it affords more flexibility, and take into account interoperability with the existing proposals in the linked open data community (hellmann et al., 2013). while source analysis is only one of our contributions and is thus limited in scope, we have observed that increasing the number of documents from the same source yields diminishing value: if a source uses a certain non-linguistic textual element, it does so consistently. this suggests that the future qualitative studies of document sources used in nlp should be conducted in a breadthfirst fashion, with few documents samples from many sources, unless quantitative measurement is desired (e.g. ""how often do wikipedia authors use text formatting"") or unless a source is known to accommodate a wide variety of document types with different publication and formatting standards. we do not provide specific details on documenting the text production environment, which represents a promising future research avenue. the study of how the texts in nlp are created is a critical research direction: due to the increased applied use of pre-trained generative language models, documenting the text form and origin is a pressing need. our"
1332,"limitations our work mainly suffers from two key limitations. 1) ignore that the text embedded in the image could also reflect the sarcastic intention. as mentioned previously, we found that our model performs better on non-ocr samples than the ocr samples. this may be due to the fact that our model ignores the text embedded in the image. nevertheless, such embedded text could also indicate the ironic intention, (see figure 3 (a)). we believe recognizing the text of the image can boost the performance of existing multimodal sarcasm explanation models. 2) ignore that different knowledge concepts may contribute differently to the sarcasm reasoning. as shown in figure 3 (b), the related concepts “disgusting” and “pleasant” should contribute more than the concept “night” in the sarcasm reasoning. currently, our model equally treats all the knowledge concepts."
1333,"limitations the limitations of the paper are twofold. first, we need to train the annotators to be familiar with another annotation paradigm: creating counterfactual samples for the labeled factual samples. it is an additional cost for active learning although our user study has shown that annotating counterfactual samples has similar costs to labeling factual samples. second, we require the annotators to manually find and edit the causal features, which is not effective enough. it can be improved by developing tools like generative models to automatically edit features for annotator judgment."
1334,"limitation. the main drawback of our data creation protocol is that the question/answer pairs were generated automatically, leading the question distribution to be artificial from a semantic perspective. in addition, the kg adopted in the research focuses on a single event domain, and extending the dataset to multiple domains is planned as future work."
1335,"limitations our work presents a new dataset based on text data scraped from the internet. hence, the quality of the text depends on the quality of the available websites. most of our data stems from the three websites apo, koe and mdr providing a rich vocabulary in our corpus. while this vocabulary covers a variety of mixed topics, we cannot rule out any negative side effects of data imbalance. moreover, our dataset can only represent topics that were considered relevant to be translated into simple german by the respective website. in section 6.2 we presented the different guis that we used to either manually align the sentence pairs or evaluate a sample of sentence alignments. one drawback of the tool for the second evaluation method is that it focuses solely on the matched sentences and presents them isolated from their contexts. one can argue that evaluators using the tool would have to see the context in which the sentences appear in order to correctly classify partial matches. also, providing more information to the annotators might enable them to also correctly classify additional explanatory sentences. future work and use cases our corpus comprises data in ls and es, two types of simple german. a higher granularity of language difficulty could be achieved by incorporating texts originally directed at language learners that are rated, e.g. according to the european reference system (council of europe, 2020). our work presents a parallel corpus for german and simple german and should be continuously expanded. not only to increase its size, but mainly to increase the number of topics covered in the corpus. yet, as there are no efforts to start a single big corpus like a simple german wikipedia, web scraping from various sources stays the method of choice for the future. an additional option is to compute sentence alignments for existing article aligned corpora to include them in the dataset (e.g. battisti et al., 2020). as for the sentence alignment algorithms, various extensions are imaginable. firstly, it might be interesting to allow one simple german sentence to be matched to multiple german sentences. also, the assumption of the mst-lis about the order of information is very strong, and recall might be improved by softening this assumption, e.g. by allowing matches that are at most n sentences away. other alignment algorithms that impose different biases on sentence order (barzilay and elhadad, 2003; jiang et al., 2020; zhang and lapata, 2017) are interesting for further extensions. our dataset can be used to train (or fine tune) automatic text simplification systems (e.g. xue et al., 2021) which then should produce text with properties of simple german. direct use cases for such simplification systems are support systems for human translators or browser plugins to simplify web pages. further research has shown that text simplification as a pre-processing step may increase performance in downstream natural language processing tasks such as information extraction (niklaus et al., 2016), relation extraction (van et al., 2021), or machine translation (stajner and popovic, 2016). it remains an interesting direction for future research if simple german can help to further increase performance on such tasks."
1336,"limitations the open information extraction methods may amplify the bias of the corpus by extracting any relation occurring in the data. the models with deep learning may learn the relation bias from the training corpus and extract those biased statements. to mitigate the effect of data bias, we try to balance the relations in constrained tuples and the ratio of constraints when constructing the cteb dataset. in addition, the utilization of external auxiliary information increases additional computation time. our ian model has still achieved superior performance when the external auxiliary information is removed."
1337,"limitations limitations of data collection our proposed dataset only targets english language tasks. future work should explore multimodal instruction tuning in a more diverse language setting and augment our multiinstruct with multi-multilingual tasks. in addition, our current dataset mainly focuses on vision-language tasks. datasets from more diverse modalities should be considered such as audio (panayotov et al., 2015; gemmeke et al., 2017; you et al., 2022) and video (soomro et al., 2012; ionescu et al., 2014). while we have built a novel multimodal instruction dataset containing 62 tasks, the number of tasks and associated instructions remains limited. to address this, future research could consider utilizing crowd-sourcing or automatic generation and augmentation techniques to increase the variety of instructions available. limitations of experiments and evaluation our work is the first to explore instruction tuning on multimodal tasks and shows improved performance compared to baseline methods. however, there is still room for improvement, specifically in utilizing text-only instruction datasets. future research could explore alternative architectures and stronger vision-language pre-trained models, or develop additional training loss functions to better utilize these unimodal instruction datasets. additionally, we only used ofa as the baseline model as it was the largest open-source multimodal pretrained model available when we conducted this research. as more and stronger multimodal pretrained models being publicly available, it would be interesting to conduct a thorough comparison between models with different sizes. finally, we take the first step to define sensitivity as a metric to evaluate the robustness of the models on understanding and following human-written instructions, which can be a potential standard metric for all the following instruction-tuning studies. however, it’s only based on the variation of model performance across different instructions for the same task. in the future, we will consider more broad factors, e.g., the model’s capability to understand different instructions for different tasks (inter-task sensitivity), to further improve the sensitivity metric for instruction tuning."
1338,"limitations we identify the following limitations of our work: longer output sequences while outputting the reasoning path as a single short sequence makes the model more interpretable, it increases the challenge of producing a long /coherent sequence when the question is complex (more than 3 hops). producing a longer sequence also increases the inference time. simplifying this output while not sacrificing interpretability is a good future direction entity identification our method needs wikipedia outlinks or a entity linker to construct a localized graph for every question. generalizing this step by pretraining the model to do entity linking (févry et al., 2020; sun et al., 2021; verga et al., 2020) might eliminate the need to use an external module."
1339,"limitations sample efficiency in ar-lms, an nll loss is computed at training time for every token in the sequence of length l (eq. 4). however, in ssdlm, each time a pretraining example is sampled, the loss is computed on only b tokens (eq. 9) leading to a lower sample efficiency than ar-lm. towards improving this efficiency, future work could explore model architectures dedicated to semiautoregressive diffusion rather than the vanilla transformer encoder we use in this work. decoding speed since each block is generated by refining over several iterations, ssd-lm has a considerably slower decoding speed than autoregressive models. for example, given a context of 50 tokens (single instance, unbatched), it takes ssd-lm 25 seconds to generate the next block of 25 tokens (tdecode=1000). while our work focused on establishing the efficacy of diffusion-based lms and modular controlled generation, future work could explore tuning tdecode to balance model performance and decoding speed, or more efficient training and decoding algorithms extending ideas from prior work on diffusion models for continuous domains (song et al., 2021; nichol and dhariwal, 2021; rombach et al., 2022; meng et al., 2022). decoding block size in this work, although we allow setups where btrain ̸= bdecode, the decoding block size bdecode remains the same across m decoding iterations, leaving space for a more flexible decoding schedule. future work can also explore learning bdecode (and btrain) rather than using constant pre-defined lengths. larger scale experiments with different kinds of controls and their combinations can be done, as well as more sophisticated ways to incorporate them (kumar et al., 2021). in addition, we plan to explore alternative methods to continuously represent and add noise to discrete text (bakosi and ristorcelli, 2013). this work experiments with pretraining data that is primarily in english. future work can also explore challenges and benefits of diffusion-based lms in a multilingual setup."
1340,"limitation one limitation of the mcce models is that the number of candidates during training and inference should be the same, otherwise, the performance drops severely. one simple potential solution is to divide or pad the candidates during inference to match the number of candidates during training. for example, divide 128 candidates into two sets with 64 candidates and apply twice forward passes of a filter model if it is trained on 64 candidates and required to filter 128 candidates during inference. we don’t fully explore the solutions to this limitation and leave it as future work."
1341,"limitations there are a few limitations of our work. first, we focus on evaluating state-of-the-art factuality metrics on english newswire datasets. this setting restricts us to english-language data, a formal style of text, and topics consisting of what is discussed in us and uk-centric news sources. moreover, other summarization domains such as dialogue summarization have different common error types such as wrong reference error (tang et al., 2022), which are not fully evaluated under current metrics. as settings like this are studied in future work, we believe that the kinds of analysis we do here can be extended to these settings as well. second, since our work is built on top of previous work, some analysis such as the error type mapping is limited by the quality and annotation agreement from previous work. we chose not to undertake large-scale reannotation to avoid causing confusion in the literature with multiple versions of datasets reflecting divergent annotator opinions. in spite of these limitations, we believe that our reevaluation of these metrics and the analysis of error types under newswire data can bring insights for future works in choosing, designing and evaluating factuality metrics."
1342,"limitations enabling dialogue agents to join multi-party conversations naturally is undoubtedly a crucial step towards building human-like conversational ai, especially as such technology becomes more affordable and portable. more crucially, research on multi-party conversations has the promising potential to improve the interactive experience between humans and machines. although the proposed method has shown great performance and generalization ability across various models and tasks, however, we never lose the sight of the other side of the coin. the proposed method requires full interactions among utterances in multi- head attention of transformers. therefore, computational complexity and inference latency may be worth considering when deploying to online dialogue systems. aside from the well-known difficulties in deployment, the proposed method was only evaluated on the domain-specific datasets, i.e., ubuntu irc, considering the constraints of dataset resources. in the future, we will try to search more open-domain datasets for multi-party conversations, and test if the proposed method can still show great performance on a more challenging open-domain setting."
1343,"limitations the political compass test in this work, we leveraged the political compass test as a test bed to probe the underlying political leaning of pretrained language models. while the political compass test is a widely adopted and straightforward toolkit, it is far from perfect and has several limitations: 1) in addition to a two-axis political spectrum on social and economic values (eysenck, 1957), there are numerous political science theories (blattberg, 2001; horrell, 2005; diamond and wolf, 2017) that support other ways of categorizing political ideologies. 2) the political compass test focuses heavily on the ideological issues and debates of the western world, while the political landscape is far from homogeneous around the globe. (hudson, 1978) 3) there are several criticisms of the political compass test: unclear scoring schema, libertarian bias, and vague statement formulation (utley, 2001; mitchell, 2007). however, we present a general methodology to probe the political leaning of lms that is compatible with any ideological theories, tests, and questionnaires. we encourage readers to use our approach along with other ideological theories and tests for a more well-rounded evaluation. probing language models for encoder-based language models, our approach of mask in-filling is widely adopted in numerous existing works (petroni et al., 2019; lin et al., 2022). for language generation models, we curate prompts, conduct prompted text generation, and employ a bartbased stance detector for response evaluation. an alternative approach would be to explicitly frame it as a multi-choice question in the prompt, forcing pretrained language models to choose from strong agree, agree, disagree, and strong disagree. these two approaches have their respective pros and cons: our approach is compatible with all lms that support text generation and is more interpretable, while the response mapping and the stance detector could be more subjective and rely on empirical hyperparameter settings; multichoice questions offer direct and unequivocal answers, while being less interpretable and does not work well with lms with fewer parameters such as gpt-2 (radford et al., 2019). fine-grained political leaning analysis in this work, we ""force"" each pretrained lm into its position on a two-dimensional space based on their responses to social and economic issues. however, political leaning could be more fine-grained than two numerical values: being liberal on one issue does not necessarily exclude the possibility of being conservative on another, and vice versa. we leave it to future work on how to achieve a more fine-grained understanding of lm political leaning in a topic- and issue-specific manner."
1344,"limitations there are two known limitations of the squadlike annotation approach we used in this work: (1) it can result in higher lexical-overlap between the context and question pairs. (2) it leads to proportionally fewer truly information-seeking questions (gururangan et al., 2018; kaushik and lipton, 2018). the main reason is that the annotators create questions after reading a paragraph, which can induce bias towards recycling words and phrases observed in the context. our annotation guidelines advise against this, but it is difficult to avoid entirely. several approaches have been proposed to mitigate this issue, such as natural questions (kwiatkowski et al., 2019) and tydiqa (clark et al., 2020). however, they tend to be expensive, and comparatively, the squad-like method is resource efficient and a more suitable starting point for low-resourced languages such as tigrinya. finally, the current dataset does not include adversarial examples to measure the capability of models to abstain from providing an answer when it does not exist in the context; this extension is left for future work."
1345,"limitations there are several limitations to this study that should be considered. first, a key limitation is the lack of a variety of language-specific jad. here, we have four different languages namely en, da, fr, and de. this means that our analysis is based on a limited subset of languages and may not be representative of jad data outside of these four languages. in turn, the second limitation is that the esco taxonomy used as pre-training data only covers europe and the datasets used in this work also covers mostly europe. the results may not be generalizable to other regions. however, we see a slight improvement in the bhola dataset, the data of which comes from singapore, which hints that it could generalize to other cultures. the esco relation prediction task aims for learning the relations between elements of the esco taxonomy. we acknowledge that we do not evaluate the effectiveness of the pre-training objective in relation-centered tasks. unfortunately, to the best of our knowledge, there is no job-related dataset containing relations between skill/occupation concepts to benchmark our model on. we consider this interesting future work. finally, we did not conduct an ablation study on the erp pre-training objective, i.e., which errors it makes. as the accuracy of the objective is 60%, we are unable to determine which sampling method is detrimental to this accuracy. however, we suspect that the linked sampling approach might be the hardest to predict correctly. for example, many occupations have a lot of necessary and optional skills, thus it is harder to determine if some skill truly belongs to a specific occupation. nevertheless, we see that adding the erp objective improves over regular mlm domain-adaptive pre-training. despite these limitations, we believe that this study provides valuable resources and insights into the use of escoxlm-r for analyzing jad and suggests directions for future research. future studies could address the limitations of this study by using a larger, more diverse datasets and by conducting ablation studies on the language model to better understand which parts contribute to the results."
1346,"limitations although the proposed multicapclip can generate multilingual zero-shot visual captions without any labeled vision-caption training pairs. we still need the independent set of text for training/translating, which may still be difficult to collect for some lowresource languages. this might be alleviated in the future with techniques such as knowledge distillation from publicly-available pre-trained models, e.g., bert (devlin et al., 2019). besides, our approach uses clip to measure text-text similarities for retrieving concept prompts and conducting input augmentation during training. considering that clip is optimized by image-text global contrast (radford et al., 2021) and intra-modal retrieval of such a model is not as well as its cross-modal retrieval (jia et al., 2021), an improvement direction of our approach is using a vision-language pre-trained model that measures intra-modal and inter-modal semantic similarities well (yang et al., 2022b)."
1347,"limitations one limitation of our approach is that in-sample curriculum learning methods (both tcl-sg and icl-sc) always incur extra overhead during training compared with the vanilla model shown in table 7. nevertheless, the inference time of different approaches is the same as the vanilla model. in a word, it’s worthwhile because (1) icl-sc can perform significantly better than both baselines without additional computational requirements during inference in real applications; (2) icl-sc doesn’t rely on task-specific expertise and has strong generalization ability. due to the limited computational resources, we were unable to do experiments on machine translation. according to the implementation details in liang et al. (2021), all of their machine translation experiments were done on 32g nvidia v100 gpus which are much more powerful than a single rtx 3090. even for the low resource setting with around 133k to 612k training samples, they used dynamic batching with 4096 maximum tokens and trained for 60 epochs. this will either lead to an out-of-memory error or take us several weeks or even months to get the results of a single run on our machine. instead, we tried our best to cover a range of representative natural language generation tasks and corresponding datasets with different characteristics, such as sizes and output lengths (table 1)."
1348,"limitations since product question answering (pqa) is actually a domain-specific application in general qa, the scope of the problem may be limited. however, in recent years, pqa has received increasing attention in both academy and industry. (1) from the research perspective, pqa exhibits some unique characteristics and thus brings some interesting research challenges as discussed in section 3. for example, some studies use pqa as an entrypoint to analyze the subjectivity in qa tasks. (2) from the application perspective, it has great commercial value. online shopping is playing an increasingly important role in everyone’s daily life, so that many high-tech companies develop ai conversational assistants for promptly solving customer’s online problems, including but not limited to amazon, ebay, alibaba, jd, etc. regarding the large amount of research efforts that have been made, there is not a systematic and comprehensive review about this research topic. similar to recent surveys of other domain-specific qa, such as biomedical qa (jin et al., 2023) and legal qa (gil, 2021), we hope that this paper can serve as a good reference for people working on pqa or beginning to work on pqa, as well as shed some light on future studies on pqa and raise more interests from the community for this topic."
1349,"limitations our work is the first step towards unified pretraining for political actor modeling and it is limited in two aspects. in terms of data, we focus on the typical political actors, i.e., the congress legislators, and their statements, without using a larger corpus like political news. but our method can be easily scaled to a larger corpus, where we can aggregate articles of different media and consider their structure information like page links for pre-training. in terms of method, in order to improve the retrieval efficiency in both pre-training and fine-tuning, we use simple methods rather than dynamic selection methods based on embeddings to query and aggregate statements, leaving much room for future exploration."
1350,"limitations and supervision biases. to address this issue, we propose to jointly train with today and its explanation annotations, resulting in improved performances on multiple temporal reasoning benchmarks, namely tracie (+7%), matres (+3%), and today (+10%). we also demonstrate that today can be used to distill gpt3.5 and automatically generate and filter incidental supervision instances with high-quality explanations, which further improves performances. despite these advances, the gap in performance on today still motivates future work toward generic temporal reasoning. limitations this work initially builds on human annotations, which are relatively expensive compared to simple model generations. due to such cost-related reasons, we do not include neutral contextual changes which are hard to annotate, and do not investigate the potential harms of annotated/generated language, e.g. harmful social biases. throughout this work, we only use rocstories as the source data, more diverse sources are reasonable for future work. we use t5 and gpt-3 architectures; however, there are more powerful architectures that could potentially improve our results. lastly, this work only focuses on generalizing temporal reasoning, which is a challenging yet relatively narrow task for large language models. through pilot experiments, we find that similar task formulation, annotation schemes, and model structures can be applied to other tasks, such as natural language inference (nli) and question answering (qa). a sample from the snli training set (bowman et al., 2015) using our formulation for explanation is shown in table 12 in the appendix."
1351,"limitations while our approach is able to optimise over the retrieved shortlist of replies, it does not improve the initial retrieval from the candidate pool, which still scores individual candidates, rather than reply sets, using the matching model. this is a limitation that is shared with prior baseline methods. a further limitation is that we only consider the monolingual setting, whereas many deployed sr applications have an international footprint. learning a multilingual matching model in sr is known to have additional challenges (deb et al., 2021). another limitation is that our model is only tested on public dialogue datasets, due to actual conversations on platforms using sr being proprietary. therefore, while our techniques should work well in the instant messaging setting, our methods have not been directly tested in the email setting. ethical considerations as neural dialogue models have grown in expressive capabilities and fluency, ethical considerations are an increasingly prominent issue. key considerations typically centre around model’s tendencies (1) to produce information that is factually inaccurate (shuster et al., 2021) or (2) to repeat toxic/biased behaviour from the training data (xu et al., 2020). compared to vanilla dialogue models, these risks are mitigated in sr: (1) sr is usually limited to short-form replies that express simple information, and is therefore less likely to lead to the kinds of hallucination seen in longer-form answers; (2) sr typically does not generate tokens sequentially, but retrieves responses from a pool of candidates, which can be vetted in advance. note however, this does not prevent replies that are contextually inappropriate when paired with a particular message, e.g. do you hate people? → yes, i do. the human-in-the-loop, who must ultimately choose and be accountable for whether or not to select one of the suggested replies, can be seen as a risk mitigant compared to vanilla chatbots. conversely however, wenker (2023) identify risks pertaining to a loss of human agency, such as due to a user selecting a sub-optimal reply to save time or being primed by the replies. this could lead to people being more trusting of an sr-generated reply versus receiving a reply from a chatbot, due to the belief that a human ultimately is behind it. we also only experimented with datasets that were released by previous studies, which are publicly available. these datasets (especially reddit) often contain toxic/biased behaviour which developers should bear in mind if using this system in a deployment context."
1352,"limitations we have primarily focused our analysis on similarity or log-probability based metrics for nlg. there are other important and interesting metrics that future work could examine. for example, deng et al. (2021) developed a family of interpretable metrics for various nlg tasks with the concept of information alignment. xu et al. (2022) recently proposed a metric based on stratified error synthesis. in addition, there are several task-specific metrics for paraphrase generation (shen et al., 2022), image captioning (hessel et al., 2021; kasai et al., 2022a), dialogue (mehri and eskenazi, 2020), controlled text generation (ke et al., 2022), etc., which would be interesting to evaluate. in §5.5, we design a number of fluency and consistency tests. it would be interesting to expand this set to be broader or more sophisticated (ng et al., 2014). also, there are other important aspects of text generation to consider, such as factuality (wang et al., 2020; pagnoni et al., 2021). all of our diagnostic data are synthetically created. while it provides valuable insights on the metric’s behavior, it does not have a good coverage of errors in real-world settings. expanding our analysis to real-world errors in a scalable way would be an important future direction. last but not least, we evaluate our proposed stress tests only on english texts. however, many language-specific properties can induce potential blind spots for metrics, especially for low-resource languages (haddow et al., 2022) where plms may provide poor text representations. an important future direction is expanding the tests to multilingual settings (thompson and post, 2020; pires et al., 2019)."
1353,"limitations semantic underspecification has been extensively studied in semantics, pragmatics, psycholinguistics, communication sciences, and cognitive sciences. in this position paper, we review this literature only superficially, although we are aware that a generalized and exhaustive understanding of the phenomenon necessarily requires knowledge of this previous work. we encourage the scholars working on this topic to embrace its complexity and depth. the paper focuses on approaches, tasks, and models within multimodal nlp. as such, it almost completely neglects a"
1354,"limitations in this paper, we limit the proposed whitenedcse for sentence embedding learning. conceptually, whitenedcse is potential to benefit contrastive learning on some other tasks, e.g., self-supervised image representation learning and self-supervised vision-language contrastive learning. however, we did not investigate the self-supervised image representation learning because this domain is currently dominated by masked image modeling. we will consider extending whitenedcse for visionlanguage contrastive learning when we have sufficient training resources for the extraordinary largescale text-image pairs."
1355,"limitations in this work, we address the heterogeneity challenge in the task of fl for semantic parsing, by leveraging the reduction of training loss signal. our work is motivated from the fl training procedure perspective to adjust the contribution of each client during the global model aggregation stage, but how each client’s data contribute to the final global model is still unclear. as the data of different clients contain different information, what kind of information of each client is helpful and can be more directly linked and utilized to facilitate the fl training is worth more efforts in future work. in addition, our proposed re-weighting mechanism is a universal technique for cross-silo fl. thus generalizing our proposed re-weighting mechanism to a broader range of tasks beyond semantic parsing, and further studying under what kind of conditions, lorar can make a huge difference for fl would be interesting future work to pursue."
1356,"limitations we discuss the limitations of our model as follows: 1. due to the natural uncertainty of financial forecast, although we have taken many methods to improve the generalization performance of the model (such as limiting the depth of memory layers and with the assistance of auxiliary data), creating a trustworthy application requires considering many other factors beyond the algorithmic level. we advise that users monitor the model’s performance over time and regularly update it to adapt to everchanging market conditions. 2. this paper uses granger causality based on transfer entropy to make a preliminary attempt to introduce causality between time series to model the similarity between stocks more accurately. but this description is junior and classical, and there are lots of more modern methods to measure precise causality in mathematics (like pc algorithm), which we believe would further improve the performance. 3. we only experiment the performance of model on the task of binary classification, leaving more complex tasks (such as regression task and returns prediction) and simulating actual investment to evaluate the capability and potential of the model comprehensively."
1357,"limitations it should be noted that, as our model and the baseline models in this study were trained using texts from social media and the experiments were conducted on online text, the results may not accurately reflect the performance in a clinical setting. a proper diagnosis by clinical experts necessitates a comprehensive analysis of various factors, including the number of manifested symptoms, the onset and history of symptoms, developmental background, lifestyle, and recent life changes, in order to gain a comprehensive understanding of the patient’s condition. however, it is still challenging to capture detailed information such as personal secrets through online text, as these texts are often composed of fragments of daily life, episodic experiences, and emotive expressions rather than providing a comprehensive view of an individual’s life. despite the domain-specific limitations imposed by the fragmentary text, we hope that our model may still serve as a valuable aid for clinical experts in their decision-making process. furthermore, future research should aim to move beyond predicting psychological symptoms and disorders solely based on linguistic styles and expressions, and instead seek to uncover the underlying features that contribute to these expressions as our model does."
1358,"limitations the main limitation of this work is that the pretraining model can not deal with numerical knowledge well. in future work, we will try to enhance the ability of the pre-training model to better deal with numerical knowledge."
1359,"limitations our proposed model has also some limitations. first, the requirement of large memory power of gpu (here, 40 gb) due to the use of gpt-2medium in the training of pal. further, weight optimization for each of the possible combinations of different rewards may lead to model training and validation time to months. hence, some heuristic is adopted to choose some sets of combinations of reward weights. in case of continuous, short and direct responses during interaction like ‘yes’, ‘i don’t know’, ‘no’, ‘2’, ‘yeah’, the system first tries to counsel client by inquiring about their issue but after three or four turns it starts deviating and may generate repetitive or inconsistent responses. this can be due to the fact that the datasets which are used to train the pal mostly consists of interactive dialogues with long utterances, hence model gets confused when treated with short and direct responses. lastly, it is also observed that sometimes, model asks too many questions to the user. this may dissatisfy the user. hence, the model should be forced to generate only relevant inquiries by discriminating the irrelevant inquiries. this opens up the door for future studies to build a counseling dialogue system."
1360,"limitations in this paper, we focus on open-ended text generation and demonstrate the effectiveness of contrastive decoding. we would like contrastive decoding to also work well for task-oriented generation settings such as summarization and machine translation. however, the idea of contrasting models across different scales (larger expert lm and smaller amateur lm) is not directly applicable, because the modes of both amateur lm and expert lm are of high quality. empirically, having a smaller summaization model (bart-small finetuned on summarization data) as the amateur lm yields lower rouge score than employing a uniform distribution as the amateur lm, which is equivalent to beam search based on log-probabilities. as future work, we aim to study the necessary properties of amateur lm to empower task-oriented generation (e.g. summarization, table-to-text)."
1361,"limitations mentioned in section 8 need to be considered and addressed carefully when using our dataset or models for evaluation or training of a deployed system. in addition, a biased corpus may lead to an evaluation that is unaware of re language forms used in other cultures and languages, or that refer to other types of items. we expect this consideration to be important in practical settings."
1362,"limitations the proposed algorithms allow to speed up an existing model out-of-the-box, without any modification or retraining. however, there are some considerations to bear in mind when using parallel decoding in order to have a speedup in terms of wall-clock time. firstly, as the name implies, the method executes the decoding phase in parallel. therefore, to appreciate the speedup one should be able to run computations in parallel. using parallel decoding without parallel resources or parallel-optimized software may increase wall-clock time due to overheads, leading to a waste of computation. this is further discussed in section 4.3 ""computational scaling"". the reported wall-clock time results are thus to be considered within the scope of the experimental setup proposed in this paper and they may vary depending on the underlying hardware and software. secondly, the method allows speedup of the decoding by scaling on parallel resources. this implies an additional computational cost during the inference phase to achieve a speedup. while using parallel decoding, one should consider a trade-off between the desired acceleration and the utilization of computational resources. thirdly, since our method performs the decoding in parallel, as for nat systems, it is difficult to combine it with beam search. beam search is inherently a dynamic programming algorithm and it is not possible to efficiently maximize the joint probability of the large search space without using sequential intermediate computations. we better explain this aspect in the next paragraph. beam search. beam search is widely employed to enhance the translation quality in mt (sutskever et al., 2014; bahdanau et al., 2015) as well as in other domains such as audio (reddy, 1977; postolache et al., 2023). however, it is an inherently sequential procedure that stores partial joint probabilities of the entire sequence (beams) while progressing with autoregressive decoding. determining the maximal joint probability of all sequences in parallel is a challenging task, equivalent to a full maximum a posteriori (map) estimation. this is an open research problem and it is also an issue for nat methods. nat methods patch up this limitation with sequence-level kd which has the advantage of ""not requiring any beam search at test-time"" (kim and rush, 2016) thanks to learning and distillation from large models. since our method is a decoding algorithm, we cannot use the same approach without learning. nevertheless, the quality guarantee allows our methods to have performance on par with greedy autoregressive and generally better than a nat model. we think of our method, not as a replacement for beam search, but rather as a way to obtain a speedup at inference time that is a middle ground between autoregressive greedy decoding (high quality, no requirements, no speed) and nats (quality compromises, increasing requirements with increasing speed). future works might address the quality gap with beam search by combining parallel decoding with alternative techniques like minimum bayes risk (eikema and aziz, 2020)."
1363,"limitation in hardpt, we focus on training specifically on hard samples while discarding misleading samples. however, it is worth acknowledging that these misleading samples may potentially contain valuable information. additionally, finding quantifiable and interpretable evaluation metrics to accurately assess the model’s ability to identify misleading and hard samples is a crucial challenge. in our future work, we plan to explore strategies for correcting mislabeled samples and develop evaluation metrics that accurately measure the accuracy of sample partitioning. our aim is to maximize the utilization of all available information from the original dataset."
1364,"limitations and ethical issues this work presents some limitations that will be addressed in future work. in particular, i) even if the model for biographical event detection obtained good results, more sophisticated approaches may be devised to increase its effectiveness (e.g., best performing lms, multi-task settings); ii) the intersectional analysis was performed on a specific sample of people, and thus limited to writers. taking into account people with other occupations may lead to different results; finally, iii) only wikipedia biographies were considered: biographies from other sources may differ in style and thus pose novel challenges to the biographical event detection task. the research involved the collection of documents from wikipedia, which are released under the creative commons attribution-sharealike 3.0 license. the annotation of the experiment was not crowdsourced. all the three annotators are member of the research team who carried out the research as well as authors of the present paper. they are all affiliated with the university of turin with whom they have a contract regulated by the italian laws. their annotation activity is part of their effort related to the development of the present work, which was economically recognized within their contracts with the university of turin. a data statement for the research can be accessed at the following url: https: //github.com/marcostranisci/ wikibio/blob/master/readme.md"
1365,"limitations in this work, we propose a decoding algorithm for text generation. we present the algorithm with comprehensive"
1366,"limitations to create a clean and diverse corpus, we have chosen to crawl news articles as our primary data sources. since all the articles are crawled from public domains, the data could potentially encompass the biases which propagate in public channels. currently, the models trained on such data sources could model the inherent biases present within the data. in the current work, we do not perform any debiasing techniques and leave that for future work. 8https://www.meity.gov.in/ 9https://www.bhashini.gov.in/ 10https://www.cdac.in/index.aspx?id=pune language identification (lid) tools are restricted to a limited number of languages and unavailable for some of the very low-resource languages like bodo, dogri, khasi, etc. we made our best effort to clean the corpus using unicode spans, but it is possible that the data sources could have some issues. we leave developing lid tools for low-resource languages as part of future work. from our ablation studies, we see that models are benefited by using in-language training and/or development sets. we call upon the community to work together to create more in-language data resources. finally, there is still work required in terms of building datasets for hundreds of extremely low-resource languages not represented in this work."
1367,"limitations there are two major limitations of the proposed tfsgc. the first one is that the effectiveness of tfsgc depends on the quality of the scene graph. since mscoco does not have sg annotations, we evaluate the parsers in visual genome: for butd/patch and vinvl, the recall@50 of relation/attribute are respectively 65.2/68.4 and 73.4/76.6. we use vinvl’s sgs in tfsgc(butd) and cider improves from 132.3 to 133.1, suggesting better sgs are beneficial. if the scene graph quality is poor, then tfsgc will not achieve good performance. when an incorrect node in the scene graph, it also affects the output of the caption. e.g., in fig. 4 (e), the correct object label should be ""surfboard"" instead of ""train"". in this paper, we use visual genome, which contains abundant and useful scene graph annotations for parsing effective scene graphs, but current performance is not the best, and we will improve the scene graph parser based on the latest scene graph parsing methods in the future. the second limitation of tfsgc is that if the visual features contain abundant attribute or relation knowledge, then the improvement of tfsgc compared with the classic transformer will be weakened. for example, compared with the butd feature case where the relative improvement of ciderd is 3.6 (tfsgc-base in table 1), the vinvl feature is more powerful since it is trained by much more data samples with more semantic labels, thus the relative improvement is lower, which is 2.2 (tfsgc-vinvl(transformer) in table 3)."
1368,"limitations a major limitation of our work may be that our disentangled representation learning framework adopts some heuristic assumptions and designs in data augmentation and counterfactual data construction, and it remains to be seen whether they are applicable to other datasets and other languages. in particular, for the data augmentation of cnaa strategy, we assume that more data can be synthesized by text concatenation and we heuristically decide the quality and content label of synthesized data by some random strategies. besides, for the counterfactual data generation, we mainly generate counterfactual samples and scores heuristically through our intuition and experience, rather than building a generation model based on counterfactual reasoning. considering that some researchers have already developed some counterfactual data generation models for nlp tasks such as neural dialogue generation (zhu et al., 2020), we are interested in whether it is possible and better to build a counterfactual data generation model for our method."
1369,limitations and design more solid and transparent benchmarks that will advance our scientific understanding of nlp systems and humans.
1370,"limitations we discuss here the limitations of the proposed promptner. first, although promptner performs well on flat and nested ner, it cannot recognize discontinuous entities. the discontinuous entity can be divided into multiple fragments, while each position slot of promptner can only fill one. a simple alternative is to expand the position slots in prompts to accommodate discontinuous entities. second, named entity recognition requires pretrained language models (plms) with the essential ability to sense the structure and semantics of entities, which can enhance entity locating and entity typing in lowresource scenarios. however, since plms prefer to learn semantic rather than structured information in the pre-training stage, promptner needs to be warmed up by wiki training when applied to low-resource scenarios. finally, since the number of prompts is determined during training, there is a limit to the number of entities that the model can recognize. if the number of entities in a sentence exceeds the pre-specified value when testing, promptner will perform poorly."
1371,"limitations • these methods have been performed on three sl datasets (section 4.1) as these were the only publicly available natural sl corpora found to contain gloss annotations. therefore, the generalization of these"
1372,"limitations we analyze the limitations of our work as follows. firstly, although applying a million-scale simile knowledge base or large-scale simile sentences as reference makes our designed metric significantly more correlated with humans than prior referencebased metrics (e.g. bleu, rouge, bertscore), our metrics are still reference-based and rely on the quality and scale of referenced data. we have discussed the effect of referenced dataset size in our paper and will design reference-free metrics to further complement our metrics in future work. additionally, since our metrics utilize a million-scale simile knowledge base or large-scale simile sentences as references, the efficiency of our method is slightly lower than the automatic metrics based on a few references. nevertheless, this limitation does not prevent our metrics from performing systematic and scalable comparisons between sg models. ethical considerations we provide details of our work to address potential ethical considerations. in our work, we propose holistic and automatic metrics for sg evaluation and construct an evaluation dataset to verify their effectiveness (sec. 4.1). all the data sources used in our evaluation dataset are publicly available. the details about human ratings, such as the instructions provided to raters, are provided in appx. a. in our case study (sec. 5), the human rankings are discussed by three raters. we protect the privacy rights of raters. all raters have been paid above the local minimum wage and consented to use the evaluation dataset for research purposes covered in our paper. our work does not raise any ethical considerations regarding potential risks and does not involve the research of human subjects."
1373,"limitations and potential risks the two limitations of dynainst are that it requires known task boundaries, and that it does not concern with corrupted or noisy training instances. in a realistic industry setting where the task definition is quite ambiguous, and a non-negligible amount of human bias and noise are introduced during the data collection process, these limitations of dynainst may degrade its performance. however, considering that this is the first time lifelong instruciton learning has been studied, these limitations can be considered interesting directions for future research. like any language model, the model trained with dynainst may output unfair and/or offensive predictions due to the bias embedded in the dataset. improving the fairness of instruction-tuned language models is beyond the scope of this paper; nonetheless, if these problems remain neglected, we will risk deploying language models that are heavily biased and discriminatory."
1374,"limitations our method requires balanced data because all attributes share the same normalizing flow. this means that when the training data for one attribute is much larger than others, we need additional training steps to make up such a gap to prevent the jacobian part of the normalizing flow from too much in favor of that attribute. in addition, although we can achieve good results on the data scale of 2.5k or 5k per attribute, our model does not fit well in few-shot scenarios. we can alleviate this problem by obtaining a sufficient amount of single-attribute labeled data from the style transfer tasks. in our experiments, each attribute is considered equally 8see more analyses in §d, f, and g. important, which may be different from the practical situation. fortunately, our control strategy is flexible and can be customized for different demands."
1375,"limitations in this section, we discuss the limitations of our model. specifically, the selection of k values in the lrl module necessitates human involvement. various types of data or entities may rely on distinct k values. while the majority of k values within a reasonable range lead to improvements in model performance, identifying the optimal value solely through human involvement poses challenges. moving forward, we will investigate the automatic optimization of k values to enhance the model’s capacity for acquiring latent relations."
1376,"limitations it is worth noting that the supportive pretraining data we investigated throughout the work is w.r.t. the current lm, such that a perturbative continued pretraining with the supportive data would improve the final lm checkpoint deployed to downstream tasks. it is possible that for some data which we did not determine as supportive, they had been supportive w.r.t. early checkpoints of the lm. with more computing resources, future work may investigate the trend of supportive patterns across multiple checkpoints of a lm throughout the pretraining process. additionally, another significant limitation of our work is the amount of involved computing resource. the orca-icl method is gradient-based that requires back-propagation. since we iterate through a large size of pretraining data, the cost of computation is similar to training a language model with a batch size of 1 on the considered pretraining data. on our 4 nodes each consists of 8 nvidia v100 gpus, finding the supportive pretraining data for each source task in our experiment would take about a week. one mitigating aspect of such computation is that the gradient calculation can be done asynchronously, therefore enabling the use of idle, leftover gpus scattered across a cluster of nodes. we plan to explore efficient computation of gradient similarity or move from a paradigm of extracting supportive data to generating supportive data in future work."
1377,"limitations and future work our approach makes it feasible to learn the discriminative ability of an expansion embedding for dense retrieval. however, it is unclear how it may be adapted for the single-representation dense retrieval prf model. in addition, in this work, we did not test the effect of the hard negative sampling and the number of negative samples for cwprf. finally, while we have focused on passage retrieval, longer document retrieval can be addressed through splitting documents into passages during indexing, retrieval and prf, and applying a max-passage aggregation (dai and callan, 2019) to obtain a document ranking. for future work, we will consider a hybrid approach to incorporate both the learned weights produced by cwprf and the statistical information in the expansion embedding identification process. while prf approaches typically increase query response time, they can also be used as teacher approaches to realise more effective and efficient student models (e.g., colbert-prf is applied as teacher by kim et al. (2022)). this means that improved prf approaches, such as cwprf, can also have downstream benefits to other retrieval approaches."
1378,"limitations to our dataset and experiments, which we discuss in a separate section following the"
1379,"limitations our study and findings are limited to the specific l1–l2 pair of chinese (mandarin and cantonese)–english. further, the experimental setting we draw our data from is highly controlled, with carefully-chosen lexical items and carefullydesigned (length- and distractor-matched) stimulus sentences. while this enables strong statistical"
1380,"limitations though we have injected math reasoning skills to matcha, error analysis shows that there is still room for improvement on queries requiring complex reasoning. besides, it remains debatable whether doing math calculation in weight space in a purely end-to-end manner is the most promising path forward.9 besides math reasoning, figure 2 shows that plot attributes is an area where matcha underperforms pali. we conjecture that it is due to matcha’s lack of massive scale grounded imagetext pretraining with rich semantics (which pali has using web-scale image-text pairs). while chartto-code pretraining provides certain level of plot attribute grounding, such plot features are mostly using default options in plotting packages but not explicitly written out in code. in terms of experimental setup, the reported number is result of a single run. pretraining is extremely costly especially when there exists more than twenty ablation setups and downstream evaluation tasks. we have collected pretraining and evaluation data points from multiple aspects on various scenarios to verify the robustness of matcha. however, we do acknowledge that the paper can benefit from reporting multiple runs given sufficient compute. last but not least, it is also worth noting that visual language is an umbrella term. there are other visual language systems beyond the ones discussed in this paper. as an example, comics/manga have their distinct visual lexicon or even grammars (cohn, 2013)."
1381,"limitations although bump is, to our knowledge, the first dataset on which to study the consistency of faithfulness metrics on human-written errors across different error types, there are some limitations regarding the"
1382,"limitations we provide a comprehensive study on the efficacy of leveraging pre-trained language models for zeroshot ood detection. our method is thus limited to the setting of abstaining from prediction on all ood data. this is more conservative than selective prediction, where the model must make predictions over as many id & ood points as possible while maintaining high accuracy. despite this, ood detection has lower risks to high-risk and safety-critical applications, where rare and anomalous data is more reasonably flagged to the expert. we believe our work provides new values and insights to the research community, especially on safe handling of distributional shifts when deploying pre-trained language models. as discussed in our ethical considerations, the ood detection problem is of significant use in high-risk settings, and should be incorporated into production-level pipelines. however, for the same reason, the ood detection models must be also reliable to avoid any risk to the downstream applications."
1383,"limitations the limitation of unisumm can be stated from three perspectives. first, the multi-task pre-training of unisumm can be time and cost consuming, which requires large gpu resources. second, the current framework uses prefixes of a fixed length for both multi-task training and few-shot prefixtuning. however, different summarization task may prefer different size of prefixes. third, in this work, we focus on summarization tasks in english. the performance of unisumm for languages that have a different morphology or syntactic structures from english needs further exploration."
1384,"limitation of our datasets, we concatenate one reference and model-generated response, which are then fed to the encoder. employing rade when the reference response is not available. considering the reference is not always available in real-world scenarios, we design two alternatives to enable rade, i.e., constructing a pseudo-reference via retrieval or generative method. we verify the two solutions on the fed dataset and the details can be found in appendix a.3."
1385,"limitations firstly, as analyzed in sec-4.3, our proposed method fails to make a significant improvement on span boundary identification. for one thing, the annotation inconsistency in the dataset hinders the model’s understanding. for another, our span proposal module leverages the contextual information alone with implicit training signals for span boundary information. we will consider enhancing the span proposal module with amr information in the future. secondly, though tara saves up to 56% inference time compared to the previous amr-guided work, its entire training requires more than 7h on 4 tesla t4 gpus. the bottleneck is the incongruity of pre-trained language models and non-pre-trained gnns. we leave the problem for future work. finally, arguments on wikievents and rams are still relatively close to its event trigger (e.g., rams limits the scope of arguments in a 5-sentence window), and thus connecting sentencelevel amr graphs is enough to model the longdistance dependency. otherwise, document-level amr graphs with coreference resolution are in demand."
1386,"limitations our model is trained in an end-to-end manner, resulting in more training time costs than featurebased methods. to eliminate the need for gloss annotations, the ccm process relies on a large amount of sign and translation pairs. the generalizability of the model is restrained by the number of such pairs available. the more ideal end-to-end framework should combine the visual backbone and visual2text encoder into one visual encoder that can be trained end-to-end. in addition, the selection of conceptual words is done according to manually-designed rules now and relies on external toolkits like nltk. we will investigate automatic conceptual word extraction methods in future work."
1387,"limitations this work focuses on pretraining large language models for zero-shot generalization. although our proposed method is more efficient than baselines, it still requires significant computational resources, specifically gpu resources. the gpu resources used and training time are detailed in appendix a.6. our study is also limited by the computational budget, preventing us from training models as large as gpt-3 or t011b. however, our large++ model (775m parameters) already rivals or outperforms previous state-of-the-art models."
1388,"limitations we identify four major limitations of our work. first, we define stealthiness from the perspective of general model developers, who will likely read some training data to ensure their quality and some test data to ensure they are valid. we therefore focus on producing natural-looking poisoned samples. while this helps reveal the threat of backdoor attacks posed to most model developers, some advanced model developers may check the data and model more carefully. for example, they may inspect the word distribution of the dataset (he et al., 2022), or employ backdoor detection methods (xu et al., 2021) to examine the trained model. our attack may not be stealthy under these settings. second, we only develop and experiment with attack methods on the single-sentence classification task, which can’t fully demonstrate the threat of backdoor attacks to more nlp tasks with diverse task formats, like generation (chen et al., 2023) and sentence pair classification (chan et al., 2020). the sentences in our experimented datasets are short. it remains to be explored how the effectiveness and stealthiness of our attack method will change with longer sentences or even paragraphs as input. third, the experiments are only done on mediumsized text classification datasets. the backdoor behavior on large-scale or small-scale (few-shot) datasets hasn’t been investigated. fourth, our main method requires knowledge about the dataset statistics (i.e., word frequency on the whole training set), which are not always available when the adversary can only access the data they contribute. the attack success rate drops without full access to the training set."
1389,"limitations based on internal review and user feedback, we summarized the following limitations to improve and iteratively update our system and framework in the future. problem modeling: new concepts appear yearly in the real world, but the current system cannot generate new concepts. generally, the emergence of new concepts often comes from the fusion of mature technologies. thus, we model the idea exploration as link prediction. note that it is not the only pathway to brew new ideas, but we have verified the effectiveness and rationality of this approach in the experiments. in addition, plm can be taken as an implicit knowledge graph (petroni et al., 2019; wang et al., 2020), which is capable of tackling uncovered concepts in the evolving concept graphs. we will continue exploring the potential of plm in knowledge discovery and innovation. logic, correctness, and concreteness: although the verbalized ideas can deceive many experts, they may still lack logic, correctness, and details, especially in natural and exact sciences. it is also a challenge for natural language generation. we plan to use more academic corpus and introduce constraint (zhang et al., 2020) to alleviate such problems. temporal information: in plm-lp, we simply take the year information as a token in the input sequence. we conduct additional experiments to show that the temporal information is not sensitive to plm-lp, which can be attributed to the negative sampling and the nature of the strictly evolving network. two birds one stone: the current system employs two different plms for link prediction and idea verbalization, respectively. the development of prompt learning (liu et al., 2021b) reveals that most nlp problems can be regarded as generation problems. in the future, we will introduce new training settings using a single plm to address link prediction and idea verbalization simultaneously."
1390,"limitation although experiments on two public datasets show the effectiveness of our proposed method compared with other state-of-the-art methods, we notice that our proposed model fails to distinguish similar emotions effectively going through the prediction results, as frustrated and anger, happy and excited (fig. 5(a)). moreover, our proposed model tends to misclassify samples of other emotions to neutral on meld due to the majority proportion of neutral samples in these datasets. we will address this issue in future work by integrating a component for capturing the fine-grained emotions."
1391,"limitations our framework manually sets thresholds t+ and t− in pseudo labeling by observations of data quality and hyperparameter searching. dynamic threshold tuning (xu et al., 2021) or meta pseudo labels (pham et al., 2021; li et al., 2021) can be implemented to better filter pseudo-labeled examples. and the thresholds for different tasks can be tuned separately to improve the models’ generalizability. recently, large generative language models such as gpt3.5 (brown et al., 2020) and chatgpt2 (ouyang et al., 2022; gao et al., 2022) have demonstrated their strong potential on various nlp tasks including probing abstract commonsense knowledge with in-context learning (brown et al., 2020; xie et al., 2022). due to our limited access, we did not conduct fully-scaled experiments in our paper. a short"
1392,"limitations we note several important limitations of this work. perhaps most importantly, our dataset is ""naturalistic,"" but not actually ""natural"" in the sense of independently occurring in the world. though the interactions between our participants are real, the task itself is ultimately artificially constructed. in a real-world negotiation over something as valuable and significant as a house, the negotiating parties will be much more invested in the outcome than our experimental participants, whose actions change their outcome to the order of a few dollars. this difference in turn could lead real-world negotiating parties to speak differently and possibly employ substantially different strategies than we observe. methodologically, our study has a few important limitations. firstly our analyses are based entirely on language that has been automatically transcribed (with some manual checks), and while this helps with expense and scale, these transcripts could be missing important subtleties that influence the outcome. koenecke et al. (2020) uncover an important limitation of these systems, finding significant racial disparities in the quality of asr transcriptions. the linguistic feature analysis we perform should be treated as largely exploratory, and provides suggestive and correlational rather than causal evidence for the relationship between language in the interactions and negotiation outcomes. lastly, there are further linguistic and interactional phenomena at play that we have not yet integrated into the analysis presented here. for one, we have access to the audio channel of participants’ actual speech, but we have not analyzed it in this work. there could very well be acoustic cues in participants’ speech that are as significant to the interactions as the textual features analyzed here, particularly speech prosody which has been shown to communicate social meanings that could be highly relevant to negotiation like friendliness (jurafsky et al., 2009). this particularly extends to more interactional questions of not simply who said what, but what was said in response to what and in what way. for instance, existing research has shown that acoustic entrainment in dialog (e.g., interlocutor adaptation to one another in terms of prosody) has important social associations with dialogue success (levitan et al., 2012). we leave a deeper investigation of these phenomena for future work. broader impacts this research, collectively with prior and future related work, has the potential to advance our understanding of negotiation, a ubiquitous human activity. our dataset can enable future research into the dynamics of human bargaining as well as interpersonal interactions more broadly. by employing the findings and insights gained from such research, individuals may be able to enhance their ability to negotiate effectively in various settings, such as salary negotiations, personal relationships, and community initiatives. meanwhile, we must acknowledge that while a better understanding of language as an instrument in social interaction can be empowering, it may also be used as a tool for manipulation."
1393,"limitations this study has two main limitations. the first limitation is its reliance on the assumption that teacher logits on augmented data follow a gaussian distribution. this assumption is used in the derivation of teacher logits in section 4.3. however, in practice, teacher logits may not strictly follow a gaussian distribution. it is challenging to estimate teacher logits under more realistic assumptions, which requires thorough investigations on the distribution of teacher logits and more complex computations for logits estimation. the second limitation is that our method still requires access to the training dataset of the downstream tasks. in this paper, we focus on kd when teacher plms only return decisions. however, our method is not capable of kd without publicly available training data, which is a more challenging scenario for decision-based kd. we believe training a data generation model (wang, 2021; zhang et al., 2022; sanyal et al., 2022) might be useful for such cases."
1394,"limitations the main limitation of mccl is the requirement of a sufficiently large batch size in training (32 documents in our experiments), leading to a need for large gpu memory. this is because mccl uses in-batch entity pairs for contrastive learning, and a small batch size does not provide enough instances to form multiple clusters. in addition, we need to store the entity pair embedding of the whole training set for knn-based inference, which is less memory-efficient than ce."
1395,"limitations one of the biggest concern people may have is whether approximate unlearning forget the information of the removal data. approximate unlearning can not ensure exact removal of information already learned in deep neural models, just as its name suggests. considering that current exact unlearning methods are very time-consuming and hard to apply in practical applications, approximate unlearning is still a direction worth trying and is also effective in reducing the attack risks by attackers or mitigating the harm of toxic data. another limitation of this work lies in the fact that we have to maintain an extra data set dn and two models af and an in the process of unlearning. though the extra cost of our kga method is trivial compared to the previous work (e.g., bourtoule et al. (2021) has to maintain the entire training set), we have to point this limitation out and call for follow-up research to come up with better ways to reduce unlearning costs. besides, we only explore word-level translation unlearning effect by comparing the generated sentences before and after deleting instances with specific words due to the space limitation. more interesting experiments with different granularity can be discussed in future work to explore how unlearning method works in different nlp tasks."
1396,"limitation the “narratives” dataset provides a valuable fmri resource, stimulated by language and obtained under naturalistic conditions. further research opportunities can be pursued with the availability of more detailed datasets. for instance, comparative studies between instances of stuttering and nonstuttering in text stimuli can be conducted, as our experiments demonstrate that the model tends to retain frequently-used filler words (such as “um” and “like,”) as a shortcut for higher accuracy. meanwhile, the evaluation strategy applied for current research of open-vocabulary brain decoding presents an idealized condition and and serves as a starting point from which further exploration of how existing methods might perform under more real-world scenarios can commence. although we use this setting for baseline comparison purposes and a testament to the feasibility of our fmri2text task, additional tests under more practical conditions could be an essential step in future work, further elucidating the applicability and robustness of the methods. furthermore, the structure of the snapshot encoder can be explored further, as exemplified by the use of transformer-based vision transformer (vit) in chen et al. (2022) for fmri encoding. ethical considerations in this work, we introduce a new nlp task related to fmri and a unified approach for decoding various types of cognitive signals into human language. we conduct our experiments on the public cognition datasets narratives and zuco1.0 with the authorization from the respective maintainers of the datasets. all experimental datasets involved have been de-identified by dataset providers and used for research only."
1397,"limitations our approach for constructing dense-atomic still has two limitations: 1) to keep dense-atomic simple, we only consider the most reasonable relation in this paper, while the relation between two events can be complex and diversified. we will release versions of dense-atomic with diversified relations later; 2) due to page limitation, we only evaluate dense-atomic on simple commonsense reasoning tasks, and we will further validate the multi-hop reasoning capacity of dense-atomic on more complex downstream tasks in the future."
1398,"limitations currently, the main goal of shrinke is to model inference patterns directly in the embedding space for hyper-relational kgs and we do not explore more advanced training strategies that have recently been proposed. for example, recent works (yu and yang, 2021; wang et al., 2021; shomer et al., 2022) have demonstrated that adding auxiliary training tasks, e.g., the task of predicting qualifier entities, can further improve the overall performance. we believe such auxiliary training tasks can also benefit shrinke and we leave it as future work. another limitation of shrinke, though rarely happens, is that when dealing with semantically opaque contexts, the monotonicity assumption might not hold. in that case, we need ad-hoc solutions. one simple way is to explicitly distinguish semantically transparent and semantically opaque contexts."
1399,"limitations although our ctc-nast model achieves excellent performance, there are still some underlying challenges that remain in the follow-up of our work. here are some limitations that we intend to resolve in the future: • the better designs of reordering augmentation and training strategy. although the proposed cla and clm approaches achieve good results by alleviating the monotonic assumption and relieving the modeling burden, combing them can not bring remarkable improvement. more importantly, these two methods fail to stable improvements in encode-decoder architecture. this drives us to investigate the interference of the optimizations between ctc and cross-entropy. • combination with the pre-training or multitask learning. although our methods bring remarkable gains on both ar and nar models, we do not explore the utilization of external data resources. although we can use the pre-trained models directly, we expect more effective methods in future work. theoretically, we need to design nar asr and mt models that share the same or similar architectures with the acoustic encoder and textual encoder, respectively. in this way, the nast model bridges the gap between pre-training and fine-tuning and has more potential for better performance. • the potential risk for unwritten languages. in our work, we assume that transcription is always available, which is consistent with almost previous studies. although some datasets have no transcription, we can use a well-trained asr model to generate pseudo labels. however, it is hard to handle speech translation from unwritten source speech. the supervision of source text is very important for our model. therefore, we need to develop better methods for stable training."
1400,"limitations our c-stance data is collected from social media, which may be seen as a limitation, as we may not cover all aspects of formal texts that could be used in essays or news comments. we will plan to extend this dataset with other types of text in the future. however, this is a limitation of any other datasets that focus on social media content. ethical statement our dataset does not provide any personally identifiable information. microblogs are collected using generic keywords instead of user information as queries, therefore our dataset does not have a large collection of microblogs from an individual user. thus our dataset complies with sina weibo’s information privacy policy."
1401,"limitations the potential limitations of this work are that wukong-reader has fixed sequence length that may prevent it from modeling long and multi-page documents. therefore it would be promising to handle varying-length inputs for wukong-reader by, for instance, equipping the model with relative positional embeddings of the model backbone. additionally, the pre-training objectives used in this work may not be applicable to all vdu tasks. for instance, it can be hardly applied in abstractive question answering or document summarization."
1402,"limitations to better analyze the limitations of pace, we carry out an analysis of the errors made by pace on the photochat and simmc2.0 test sets. we reveal several reasons for the errors, which can be divided into the following categories. first, since there are many similar images in the datasets, pace fail to distinguish some gold image from similar candidates. this may be because we do not design an explicit fine-grained reasoning module to capture the details of images and texts. for example, for the context mentions “i and my dad both have a camera”, our model can capture the entity “camera”, but fails to reason the fact that there should be two cameras. one possible solution is to introduce a deep reasoning and comprehension strategy to empower the model with excellent reasoning ability. second, due to the lack of fine-grained structural understanding of the images, the sentences generated by pace suffer from identifying the relative positions of entities. for example, pace may have difficulties recognizing the fact that the right side of a yellow shirt is black pants. this issue is particularly severe in simmc as there are many entities in the pictures and spatial descriptions of entities in the responses. one possible idea is to extract the relative positions of objects mentioned in the conversation as auxiliary data to guide the model’s generation."
1403,"limitation this paper presents the mvp-tuning framework, which combines multi-view knowledge retrieval with prompt tuning and incorporates retrieved knowledge in a simple kg-encoder-free paradigm. however, there are limitations to our approach. firstly, multi-view knowledge consists of self-view and consensus-view knowledge, which are one-hop triplets in the knowledge graph. however, not all question-choice pairs have one-hop triplets, leading to null knowledge being retrieved. additionally, excessive consensus-view knowledge can lead to noisy retrieved knowledge. therefore, our knowledge retrieval system needs further improvement to obtain sufficient, high-quality knowledge. secondly, we focus on the empirical study of prompt tuning in commonsense reasoning tasks. although we conduct extensive experiments, including initialization schemes and prefix token length, we do not fully understand the mechanism behind prompt tuning and sometimes experience unstable performance. although prompt tuning has been proven to be an efficient tuning paradigm for commonsense reasoning tasks, it requires further exploration."
1404,"limitations although peit is an end-to-end approach to image translation, in the current form, it needs to be pre-trained in two stages with mt and synthesized data and fine-tuned on the curated image translation data. the training procedure is longer than the standard mt task due to the lack of training data and the cross-modality challenge. for the created ecoit dataset, we used online mt to automatically generate translations and then manually post-edited translations via crowd-sourcing. this significantly reduces the cost of building a large-scale image translation dataset from scratch but may introduce translation noise and “machine translationese” (vanmassenhove et al., 2021) in comparison to professional human translation."
1405,"limitations our framework currently only supports english, thus not allowing us to complete a cross-lingual study. future work should focus on extending this study to a multilingual setup. our method is evaluated on a 16 dataset stance benchmark, where some domains bear similarities. the benchmark should be extended and analyzed further to find independent datasets with varying domains and minimal similarities, allowing for a more granular out-ofdomain evaluation."
1406,"limitations and outlook discomat is a pipelined solution trained component-wise. this raises a research question: can we train one end-to-end trained ml model that not only analyzes a wide variety of table structures but also combines the understanding of regular expressions, extraction of chemical compounds and scientific units, textual understanding and some mathematical processing? this defines a challenging ml research question and one that can have a direct impact on the scientific matsci community. indeed, automating parts of scientific discovery through such nlp-based approaches has the potential for biases and errors. note that wrong and biased results can lead to erroneous information about materials. to a great extent, this issue is addressed as we rely only on published literature. the issue could be further addressed by considering larger datasets covering a wider range of materials."
1407,"limitations the potential limitations of our model are threefold. first, the training process requires more computational cost as the model needs to conduct two forward passes for each sample in the self-distillation module. second, there is still room for improvement to reduce the model’s overcorrection of legal characters. third, the phonetics-aware sequence doubles the length of the original input, which demands extra computation cost at inference time."
1408,"limitation of not utilizing information beyond the training sequence length. fortunately, this is overcome by our new relative positional embedding, sandwich, which is simplified from the earliest proposed sinusoidal positional embedding. finally, sandwich demonstrates a log-decaying temporal bias pattern similar to that previously seen in the design of kerple and t5, and such pattern is likely to be the secret to successful length extrapolation. together these findings supports more effective design of future extrapolatable transformer language models. limitations although sandwich, kerple, and t5 use information beyond training sequence length, their receptive fields still highly favor the most recent tokens. while this recency bias is beneficial to the modeling of human-written text, it is problematic in other scenarios. let us consider the task of parity prediction: a model needs to predict whether a bit string has an even or odd number of ones. for example, the parity of [1, 1, 0, 1] is odd (or 1) and the parity of [1, 0, 1, 0] is even (or 0). unlike human-written text, every single bit is equally important. transformer language models with current rpes still struggle on this simple task (anil et al., 2022). its difficulty can be explained by the recency bias effect that we described. devising a new positional embedding or transformer model architecture that solves this problem is a promising direction for future work."
1409,"limitations in this work, we are the first to uncover the social bias problem in the text-to-sql task. we categorize different types of social biases related to various demographics. we present a new benchmark and metric for the social bias study in the text-to-sql task. however, this work stops at the point of uncovering and analyzing the problem and phenomenon, without making one step further to solve the social bias problem in the text-to-sql task. besides, in spite of the structured scalability of our proposed paradigm for social bias benchmark construction, the efficacy of entending with other text-to-sql datasets remains to be verified."
1410,"limitations this work focuses on mitigating the negative transfer and catastrophic forgetting issue in multi-task dialogue generation. all technologies built upon the large-scale plm more or less inherit their potential harms (bender et al., 2021). besides, we acknowledge some specific limitations within our methods: 1. the construction of pseudo labels requires dependency parsing with spacy, which is timeconsuming. but we only construct pseudo labels offline in the training processing and it causes no latency at inference. 2. we instantiate our modular framework using minilm (wang et al., 2020b) as the backbone of the reader within the programmer, and t5 (raffel et al., 2019) as the backbone for the content operators and linguistic operators. we did not try other instantiations although the modular framework does not depend on the specific initialization choice of modules. theoretically, any generative plm could be the backbone of these linguistic and content modules. 3. we aim at decomposing the response generation into relatively independent and composable operators. currently, the division of dialogue skills and module functions is in a heuristic way inspired by linguistics. thus it remains a future research question about how to design modular architecture in a more data-driven way."
1411,"limitations the intention classification task is not trivial even for humans, especially when the intention is implicit or disguised. the sample size of our study is small, which makes classification more challenging. currently, we are extending the dataset to include more samples in each category. we aimed to use this data as a proof of concept to shed light on using questions as a means to attack someone or disguise intention. future directions involve enlarging the dataset and including a variety of social interactions from different sources such as social media (e.g., twitter), forums (e.g., reddit), and spoken conversations to investigate other emerging categories based on context, topics, and events. moreover, the dataset is imbalanced. wikipedia editors should follow strict rules and avoid explicit hostility otherwise get blocked. the nature of wikipedia"
1412,"limitation of a lack of personalized and specific examples in existing datasets, when teaching cognitive techniques. future work will evaluate whether leveraging models to produce richer training material results in more robust learning and understanding of the types of unhelpful thought patterns in humans.this may serve as the basis for future psychological validation studies of the materials and support future studies of low-intensity self-help interventions."
1413,"limitations in-context learning is an useful ability, this paper only focuses on in-context named entity recognition, leaves the learning of other nlp tasks’ incontext learning abilities for future work. currently, we learn in-context learning via metafunction pre-training, by comparing an in-context extraction function and a fined-tuned surrogate extraction function at the representation level of their encoders. there are two approximation here: one is fined-tuned surrogate extraction function for approximating golden extraction function, and the difference between representations for approximating the divergence between functions. we think the above two approximations can be further improved for better and faster in-context learning."
1414,"limitations the limitation is that we separate node identification and node/edge labeling processes. because joint node identification and label classification should enumerate all possible spans in a sentence, which is too computationally expensive. most previous works also separate the two processes. but an obvious disadvantage of such a pipeline scheme is the error propagation problem. we take joint node identification and label classification with high-order inference as future work."
1415,"limitations our work reported in this paper has two limitations. first, because of the scarcity of treebanks and ner datasets for turkish, our pretrained spacy language models were tested on a limited amount of testsets. second, we trained our spacy models on generalpurpose datasets compiled from wikipedia data and formal written language resources. accordingly, our models may not be very effective in analyzing social media texts such as twitter data."
1416,"limitation while our pipeline is designed to be applicable to any financial text dataset, the evaluation dataset is transformed solely on earnings conference calls. we will expand the scope of experiments to include other financial text sources such as news articles and social media posts. finally, the current trading simulation does not take transaction costs into account. going forward it will be necessary to consider more sophisticated trading policies."
1417,"limitations in this section, we discuss the limitations of our work as follows. first, despite achieving promising results, our model needs to calculate pseudo ranking labels of the teacher which requires additional training time per epoch than the teacher. the training efficiency of rankcse and simcse can be seen in appendix d. second, we directly use simcsebase and simcselarge as a multi-teacher in our implementation and experiments. however, how to choose the best combination of the teacher models is worth further exploration. it could help researchers to better understand the upper bound of improvements. we plan to investigate more along this direction in the future."
1418,"limitations our method utilized pretrained entailed models and adapted them to other domains under zeroshot and self-training settings. there are two limitations that we would like to improve in future work. firstly, we use human-designed suppositions for each task, which is less automatic than a direct, zero-shot adaptation of the models. secondly, the self-training on some multi-class classification tasks is not as high as on binary nlu tasks, indicating the challenge of applying entailment models to multi-choice tasks. we would like to overcome this in the next step."
1419,"limitations while this work represents the first effort towards a perspectivist language resource for irony detection, it has to be noticed that the resource is monolingual (english). moreover, while we tried to maintain a fair balance in terms of demographic profile of the annotators, we limited the resource to five varieties of english tied to five countries, while leaving out other potential locations (e.g., new zealand or nigeria) or even more nuanced distinctions among language varieties. about the self-identified gender dimension, we are aware of the wider spectrum of genders. however, this information is provided by the annotators only in a binary form. another potential limitation is that, in the spirit of constructing a perspectivist corpus, we fully trusted the contributors. while the chosen crowdsourcing platform (prolific) is known for a high quality standard obtained e.g. by vetting its contributors, and we added a layer of checks through attention test questions, random noise in the annotation may still be present and undetected. while this paper mainly presents a new language resource, we also included the results of several analyses and validation experiments. in this direction, a number of dimensions are still unexplored, along which the data could be analysed. for instance, the genre difference between the sources of the data (reddit and twitter) and the distribution of different varieties of english were not yet explored."
1420,"limitation of the current static graph-based dialogue summarization methods and propose a static-dynamic graphbased dialogue summarization method (sdds). it contains two modules, a static graph module and a dynamic graph module. the former injects human prior into the summarization model and the latter encodes the implicit knowledge from a pretrained language model. by fusing these two kinds of graphs with a fine-grained 1×1 convolution, sdds could adaptively adjust the graph weight and learn the graph structure in an end-to-end learning fashion from the supervision of the summarization task. to validate the effectiveness of sdds, we conduct extensive experiments on three public dialogue summarization datasets (samsum, mediasum, and dialogsum) and observe significant improvement over strong baselines. we also carefully examine each key component and gives a detailed analysis of sdds for future research. limitations we discuss the limitations of sdds as follows: (1) although we propose a general framework for dialogue summarization by incorporating both static and dynamic graphs, we only adopt four static graphs to model the dialogue structure. since dialogue structure modeling is still an active research direction, we believe future advances would further benefit our framework. (2) despite the strong performance achieved by sdds across three dialogue summarization datasets, we use a pre-trained language model as the backbone of our proposed method, as a consequence, we can not go beyond the limitation of the maximum sequence length of the plm for the dialog summarization scenario like meeting summarization so it remains a future challenge for dialog summarization in the extremely long format. ethical consideration the dialogue data would inevitably contain private information about the interlocutors. we take careful consideration of this problem: (1) all data in our experiments are publicly available and anonymized by the original dataset provider. the license for samsum dataset is cc by-nc-nd 4.0 and for dialogsum mit license. for mediasum, it adheres to only-for-research-purpose guideline from the national public radio; (2) we do not use online user data to train our model and we would use an additional rule-based system to double-check whether our model output contains harmful and prejudicial discrimination when we use it for production. acknowlegement this work was supported by the national natural science foundation of china (nsfc grant no. t2293773 & no. 62122089 & no. 61876196), the national key research and development program of china (no. 2020aaa0106600), beijing outstanding young scientist program (no. bjjwzyjh012019100020098), and intelligent social governance platform, major innovation & planning interdisciplinary platform for the “double-first class” initiative, renmin university of china. rui yan is also supported by beijing academy of artificial intelligence (baai)."
1421,"limitations; the relevant cases are considered based on official citations as ground truth. however, there might be cases that were not mentioned by the judge (document writer) due to subjectivity involved in the common-law system; finding correct annotation for relevance is always a challenge for a domain like legal, where the number of documents is enormous."
1422,"limitations in this paper, we propose a simple model for prior case retrieval. as shown in experiments and results, the models could improve and score better. there is a big room for improvement. all the previously proposed approaches for pcr have calculated relevance as some form of lexical/semantic similarity between a case and its citations. however, cited case relevance may sometimes differ from lexical/semantic similarity. modeling the document in terms of events only partially addresses this. consequently, what is required is the inclusion of more legal information. we made an attempt towards that via experiments using rhetorical roles. similarly, one could use the information coming via statutes and laws since similar cases are likely to invoke similar statutes. another approach is learning representations using contrastive models that score relevant cases higher than non-relevant ones. in the future, we plan to investigate these approaches to improve the task of pcr. this paper considers a simple structure for an event as a tuple of predicates and arguments. however, more sophisticated formulations are possible, as outlined in the survey/tutorial by chen et al.. moreover, we are taking events in isolation and ignoring the sequential nature of events that help to form narratives. in the future, we would like to develop a model that captures a more sophisticated structure and sequential nature of events in the case. though we covered an extensive set of experiments for the proposed event-based matching technique, many more combinations can be experimented with to understand the role of events in legal documents. this unique finding of events missing from the legal literature would facilitate exploring new directions in the legal domain. in this paper, we evaluated only two datasets as we could not find any publicly available pcr datasets. however, in the future, if we can find more pcr datasets, we would like to evaluate them to see if the trends generalize over other legal corpora. ethical concerns this paper proposes a system for retrieving (recommending) relevant documents. the system is not involved in any decision-making process. the motivation for proposing the system is to augment legal experts rather than replace them. moreover, for training the system, we used publicly avail- able legal documents. we took steps to normalize documents concerning named entities to prevent a model from developing any known biases. to the best of our knowledge, we addressed any biases that the model might learn from the data."
1423,"limitations although our ulra outperforms all unsupervised baseline methods, there are still some limitations. the first limitation is that there is still a gap between the performance of our unsupervised method and that of some supervised methods. although our ulra can complete the aes task without label annotations, it is still worth exploring an unsupervised aes method whose performance is comparable to the state-of-the-art supervised method. the second limitation is that the essay encoder which adopted in our ulra (i.e., bert) is pretrained on the english-based corpora, and the essays for training is also written by english. thus, our ulra works mostly for english, which means a well-trained ulra model may fail to perform well on the essays written by other languages. an unsupervised aes system which supports multiple languages needs to be further explored. the third limitation is that it requires about 25g gpu memory for training, which may fail on devices with small gpu memory. a possible solution is to set a smaller batch size, but this may take longer time. however, the evaluation process only requires about 2g gpu memory, which can run in most of gpu devices, or even cpu devices."
1424,"limitations data and task limitation in this work, we analyze domain-label bias and apply our domaincontext calibration to english. we leave analysis and mitigation methods for multilingual tasks to future works. in experiments, we discuss calibration on classification tasks. the effect of domain-label bias could exist differently for open-end tasks like text generation. our analysis of domain-label bias also emphasizes more on the word-level bias. other types of biases associated with a domain, such as topics and genders, may also impact model prediction. we leave the diverse analysis to future works. due to budget limitations, we conduct experiments on a subset of the 24 reported datasets for gpt-3. one can evaluate all 24 datasets to get a complete picture with enough budget. model limitation for large language models, we only focus on the gpt models and only select roberta as the small-scale language model in experiments. future work could consider expanding to other model types, such as palm for large models and deberta for small models. access to the openai api for gpt-3 is also necessary for parts of our experiments. future work can consider experimenting with open-source llms like the opt-175b or bloom-176b model."
1425,"limitations in this section, we will analyze the limitations of our method. first, we introduce multiple knowledge sources to construct hkg, and encoding this knowledge through lm consumes more gpu resources. second, some useful knowledge may be removed when retrieving knowledge from key entities optimized by dictionary vocabulary. then, we experimentally demonstrate that the paraphrase descriptions are effective in improving the reasoning ability of the model, but due to resource constraints, we are unable to incorporate the paraphrases of all entities into hkg. finally, our method uses the simpler transe algorithm when optimizing the knowledge representation using krl due to gpu constraints, which may not be able to model the complex relationships in hkg well."
1426,"limitations firstly, due to the huge cost of large-scale plms, this paper only employs the t5-base as the backbone plm in our experiments, therefore only limited analysis on the effect of model scale is presented. however, we believe a larger model will benefit our method by providing better language understanding and generation abilities. secondly, the synthesized canonical utterances need manually designed synchronous grammars, which are used to guide raas with knowledge about semantic representation language. although most few-shot/zero-shot semantic parsing studies also rely on synchronous grammars, we leave how to model semantic representations without grammars as an open problem for future work. acknowledgments we sincerely thank the reviewers for their insightful comments and valuable suggestions. this research work is supported by the national natural science foundation of china under grants no. u1936207, 62122077 and 62106251. furthermore, this research was supported by meituan."
1427,"limitations despite the competitive performance, there are several limitations of this work: (1) as discussed in section 6.1, the generation performance relies on the parser performance, which is strong enough for english but still less satisfactory for other languages. dedicated methods need to be considered to compensate for the weak parser performance if we want to extend our method to more languages. (2) in this work, we consider two nlg tasks with semantic equivalence to testify if the proposed method can convey the source semantics accurately by following the target syntactic grammar. other tasks such as summarization and dialogue generation can also be tested, where the semantics are not equivalent between the source and target. (3) to train the neural decoder parallelly, we break down the source-target dataset into a triple set. however, the global dependency of the syntax parse tree is not considered, which can deteriorate generation performance. (4) due to the recursive encoding of the syntax contexts, our model’s inference speed is approximately half that of the seq2seq counterpart (appendix e). (5) future work should include experiments on large language models (brown et al., 2020; openai, 2023; zeng et al., 2022; touvron et al., 2023; taori et al., 2023). to further demonstrate the effectiveness of our method beyond pretrained language models."
1428,"limitation the grm model still has some limitations. even though our model brings some performance improvement to the contextual word embedding model (i.e., bert), this improvement is relatively small compared to the static model. in some cases, grm may hurt the performance of bert slightly, because the primary objective of context-based word embedding models is to infer word meaning from contexts. the approach set forward in our study enhances their initial input word embeddings through word formation, and the benefits brought by this method are modest. how to efficiently improve the performance of contextual word embedding models when faced with oov words remains to be explored."
1429,"limitations in this paper, we focused on english comics only because of their ease of availability. although we have not experimented with non-english text, we expect the proposed model to work well in multilingual settings if we replace gpt-2 decoder with other decoders like bloom (scao et al., 2022)."
1430,"limitations efficiency. to get the optimal performance from pairranker, one may need to call the model o(n2) times for getting the full matrix, thus resulting in a much less efficient solution. we attempted to resolve this limitation by proposing to use multiple rounds of bubble sort methods to reduce the number of inferences needed, and we find it works pretty well. we also want to argue that although the number of inferences can be large for obtaining the best performance with pairranker, those inferences can be executed in parallel because they are totally independent. human evaluation. we agree that automatic metrics have limitations. human evaluation could provide us with more reliable and comprehensive evaluation results. however, due to the number of models as well as the amounts of generation candidates, we cannot afford large-scale human evaluation. we argue that our use of chatgpt for evaluation is a good alternative, according to recent studies. also, we would like to highlight that we show the ground truths when using chatgpt to do pairwise comparisions, which is quite informative than the common practice. *ethical statement this work fully complies with the acl"
1431,"limitations although dcg achieves significant improvements compared with existing baselines, there are still avenues to be explored in future research. (1) dcg in this paper focuses on the compositional generalization for multi-attribute on controllable dialogue generation. we hope to extend the method to other generative tasks, including but not limited to dialogue summarization and story generation. (2) in this paper, we explored the control of coarsegrained discrete attributes and the control of finegrained ones separately, and we intend to study the combination of these two attributes in future research."
1432,"limitations we facilitate fair comparisons and realistic evaluations of recent wsl approaches. however, our study is not exhaustive and has the following limitations. first, it may be possible to perform model selection by utilizing prior knowledge about the dataset. for example, if the noise ratio (the proportion of incorrect labels in the training set) is known in advance, it can be used to determine (a subset of) hyperparameters (han et al., 2018; li et al., 2020). in this case, certain wsl approaches may still work without access to extra clean data. second, in this paper we concentrate on tasks in english where strong plms are available. as we have shown in section 6, training them on a small amount of data is sufficient for generalization. for low-resource languages where no plms are available, training may not be that effective, and wsl methods may achieve higher performance. third, we experiment with datasets from the established wrench benchmark, where the weak labels are frequently assigned by simple rules like as regular expressions (see appendix b for examples). however, in a broader context, weak supervision can have different forms. for example, smith et al. (2022) generates weak labels through large language models. zhou et al. (2022) use hyper-link information as weak labels for passage retrieval. we have not extended our research to more diverse types of weak labels. despite the above limitations, however, we identify the pitfalls in the existing evaluation of current wsl methods and demonstrate simple yet strong baselines through comprehensive experiments on a wide range of tasks."
1433,"limitations in this work, we focus on debiasing the gender bias for plms. in the future, we will try to mitigate social biases other than gender, such as race and religion. in addition, we also plan to extend our debiasing method to more language models, such as natural language generation (nlg) models."
1434,"limitations our method primarily focuses on operation-level specifications, while there are real-world use cases with other specifications. moreover, our method of creating cqas can only be scaled to all python codes that involve heavy api usage. however, if a similar code knowledge graph generator of another language is developed, our method can also be scaled to the corresponding language. our method is also limited in identifying specifications missing from the nld, suggesting potential future work to create cqs about specifications “mentioned but not specified enough” in the nld. ethical concerns one concern about the data is the issue of copyright. liu et al. (2021) have checked the data policy of all 20 kaggle competitions, in which none has copyright issues. furthermore, they have contacted kaggle’s administrator and have made sure that the dataset collection procedure did not violate the platform’s policy. we also check the license of open-source apis when collecting documentation and make sure that there is no concern about copyright issues. another concern about the data is that it might include privacy data. again, we think that our data has a minimum risk of leakage of data with privacy concerns since we only collect data from the 20 kaggle competitions where there is no concern of privacy data. the api documentation also has the minimum risk of containing data with privacy concerns."
1435,"limitations our approach has proven to be superior to previous methods on multiple public benchmark datasets. however, one major disadvantage of the table filling method is the increased training time and memory usage. the computational resources are required for the 2d table representation of word-pair relations for constructing and storing the table. in comparison, using a sequence representation as input could be generally more efficient. our approach also faces the computational challenge."
1436,"limitations we acknowledge that our benchmark dataset does not cover all the existing ambiguities and that ambiguities related to fairness do not cover all the possibilities. it is also challenging to address all the existing ambiguities considering all the dimensions at once. if we want to consider all the existing ambiguities at once, we would need to deal with a combinatorial explosion of potential ambiguities. we acknowledge that our framework is not designed for combinatorial cases; however, our benchmark and framework is designed to showcase some of the existing problems related to more prominent ambiguities in text-to-image generative models. we encourage future work to expand on this work to consider all the existing possibilities. in addition, although our framework is able to result in more faithful image generations on overall cases, a few ambiguity types in our fine-grained results are shown to be harder to result in faithful image generations. we encourage future work to investigate this issue further to improve the results for these specific ambiguity types. ethical considerations in this work, we study and propose solutions to resolve existing ambiguities in prompts given to text-to-image generative models. in addition to resolving ambiguities in prompts, this work not only frames and analyzes fairness from a new and different perspective, but it also results in more faithful image generations aligned with end user intention. these aspects can contribute to numerous positive impacts to the research community. not only one can generate more diverse images through disambiguating fairness type ambiguities, but our framework can also improve user satisfaction by generating aligned images to end user’s intention despite existing ambiguities in the provided prompts. resolving ambiguities can also avoid spread of misinformation and development of fallacies. despite the aforementioned positive impacts, we also acknowledge the limitations associated with this work. we acknowledge that our benchmark dataset is just a very small sample of different types of ambiguous prompts that can be provided to a system. in addition, for the fairness type ambiguities, we only consider gender (male vs female), skin color (dark vs light), and age (young vs old). we acknowledge that these are only a limited number of characteristics that can represent identity of an individual and that we do not cover all the cases possible. we agree that we do not cover all the cases possible; however, our intent is to showcase a few examples through our benchmark (tab) and highlight existing flaws associated with these systems encountering ambiguous prompts. in our experiments, we also utilize human annotators. we ensure to provide appropriate guidelines with a proper compensation to our workers (around 12$ per hour). we also utilize master workers based in the united states with proper expertise (completion of more than 1000 hits with an acceptance rate above 85%). in addition, we provide the workers the opportunity to raise any concerns about our task. based on the feedback, we believe that the task and the pay was satisfactory to the workers. we hope that our study can provide valuable insights to the research community with the positive implications out-weighting its limitations. we also open-source our benchmark dataset for the community to benefit from our work. as future work, researchers can investigate and propose better alternatives than our proposed framework for resolving ambiguities in text-to-image generative models along with extension of our work to semantic ambiguities in addition to the ones studied in this paper. our benchmark dataset can also serve as a valuable resource for research in commonsense reasoning studies in text-to-image generative models which is less explored in our current work. we provide information in our benchmark dataset (whether an interpretation is commonsensical or not) which can be accessible to interested researchers in this area."
1437,"limitations. in addition, we performed a quantitative analysis in terms of efficiency and offered certain suggestions about method selections of open domain question answering. finally, we discussed possible open challenges and potential future directions of efficient odqa models."
1438,"limitations one of the limitations of the current study is the lack of annotated data for all languages. this is also the case of machine translation for which data could only be found for kashmiri, sorani and sindhi, while other languages do not have much parallel data yet. on the other hand, the notion of noisy data is limited to the replacement of the missing characters in a script when compared to another one, i.e. that of the dominant language. as an ablation study, injecting other types of noise, beyond those discussed in this paper, may improve the performance of the models to tackle not only script normalization but several related tasks such as spelling error correction and may also increase the robustness of the models for morphologically rich languages or languages with versatile word boundaries using zwnj. although we did our best to filter out code-switched data in the corpora, our datasets may contain data in other languages (in perso-arabic scripts). in future work, we would like to apply our approach to other scripts and languages in bilingual communities. we also suggest evaluating the impact of script normalization on more downstream tasks, especially transliteration and tokenization."
1439,"limitations the conditional independence assumptions are a limitation for the applicability of our multiset tagging model. for example, the independence assumptions are too strong to apply it to natural language generation tasks such as summarization. from a technical point of view, the independence assumptions are important to be able to induce the latent assignment of output tokens to multisets efficiently. future work may design multiset tagging methods that make fewer independence assumptions. while our method for predicting permutations is comparatively fast and only has a memory requirement of o(n3), inference on long sequences, e.g. with more than 100 tokens, remains somewhat slow. in future work, we plan to investigate other approximate inference techniques like local search and dual decomposition. regarding the importance of trees for compositional generalization, our model has no explicit structural inductive bias towards trees. however, we do not exclude that the pretrained roberta model that we use as a component implicitly captures trees or tree-like structures to a certain degree."
1440,"limitations in this work, we propose managers that allow adaptive aggregation of uni-modal layer representations in each cross-modal layer. inevitably, aaue managers significantly improve performance which slightly increasing the computational budget, as we detailed discussed in appendix c. this needs to be further optimized in the future. analysis and optimization are also needed for the other types of managers as shown in appendix d. moreover, as shown in figure 5, the performance of managertower first increases gradually with the number of uni-modal representations, but then stops increasing and even decreases when the number of uni-modal representations exceeds 6. how to obtain better managertower performance using a lower computational budget while utilizing more insights of uni-modal experts, especially when scaling the model, e.g., 24-layer clip-vit l-224/16 and 24-layer robertalarge, is a question worth further exploration. for example, designing reasonable sparse activation functions for managers in managertower, instead of simple top-n or top-p sampling (which did not work well in our preliminary experiments)."
1441,"limitations in this work, we focus on causal language modeling. it needs additional efforts to integrate the proposed methods into bidirectional attention, such as masked language modeling (devlin et al., 2019). moreover, xpos introduces about 6% inference cost compared with absolute position embeddings, although it accelerates training convergence."
1442,"limitations one limitation of our survey work is that it is focused on the intersection of mathematical reasoning and deep learning over the past decade, which may not encompass the entire field and its history. additionally, our evaluation of existing benchmarks and methods is based on a curated set of papers and may not fully represent the state of the art in the field. furthermore, due to the fast-paced nature of the field, our survey may not reflect the latest developments and advancements which may have come out close to or after the survey was conducted. despite these limitations, our survey still provides a valuable overview of the current state and key trends in the field of mathematical reasoning and deep learning, and can serve as a valuable resource for researchers and practitioners working in this field. broader impact our survey paper on the intersection of mathematical reasoning and deep learning has the potential to significantly impact the field of artificial intelligence. by providing a comprehensive overview of the key tasks, datasets, and methods that have been developed in the past decade, we give researchers and practitioners a clear understanding of the current state-of-the-art and help them make informed decisions about their own research. additionally, by evaluating existing benchmarks and methods and discussing future research directions, we aim to identify gaps in the current state of the art and guide future research and development efforts towards more advanced and effective mathematical reasoning systems. overall, our survey has the potential to contribute to the advancement of mathematical reasoning and deep learning, and have a profound impact on machine learning and natural language processing."
1443,"limitation we verify our method mainly based on the recent robust vlp model albef (li et al., 2021). evaluating it more broadly by incorporating it into other vlp models can further highlight our contribution. given the solid theoretical foundation of our method, the main"
1444,"limitations the proposed t2d dataset has several limitations, which could be addressed in future work. first, it only considers and collects language instructions for the floor plan domain. future work could extend this language-guided design generation task to other design domains such as documents, mobile uis, etc. second, it is limited in the scope of languages where we only collect instructions written in english. future work could assess the generalizability of the t2d dataset to other languages. third, although generating floor plan designs from languages exhibit diversity, we do not consider improving generation diversity at this moment. future works could consider building frameworks that specifically aim at design diversity."
1445,"limitations although our proposed multiemo framework has achieved state-of-the-art performances on both iemocap and meld, there are some limitations with this work: • our proposed visual feature extractor visextnet does not distinguish between speakers and irrelevant people in the scene, which can be problematic in some scenarios. for instance, one scene in meld is the cafeteria, where a lot of background actors sit and drink coffee. the facial expressions of these background people have no impact on the emotion of the speaker since they do not participant in the conversation. however, visextnet captures visual features of everyone appeared in the cafeteria with no differentiation, which may lead to a wrong comprehension of the speaker’s emotional tendency due to the effects of facial expressions from irrelevant people. we plan to explore effective ways to distinguish between interlocutors and irrelevant people in the scene in our future work. • the effects of hyperparameters in the swfc loss (temperature parameter τ , sample-weight parameter α and focusing parameter γ) on model performances have not been fully studied, which will be thoroughly analyzed in our future research. • due to the class imbalanced issue with meld, the swfc loss requires a large batch size on meld to ensure that for each training sample there exists at least one positive pair in the batch, which can be computationally expensive. we will investigate effective approaches to tackle this challenge in our future research. • even though multiemo has achieved remarkable improvements in minority emotion categories, the performances of multiemo in minority emotions are still worse than majority classes. how to further improve performances in low-resource emotion classes will be explored in the future."
1446,"limitations in this work, we focused our exploration of lsls on the encoder. although we ran some initial explorations on the decoder side, further investigation is needed. another venue for research is how lsls affect language expansion. since our approach tries to limit the language-specific weights to just a few layers, in theory, it should be possible to add new languages by only expanding and training the lsls. however, blindly doing so might not work well and the interactions between languages from different families needs further studying. lastly, it is unclear whether our argmax approach to selecting where to place lsls is optimal, how dataset dependent it is, and if there exist alternative approaches that can lead to better results. the fact that it does not take model complexity (i.e., model size) into account can be a disadvantage in practice."
1447,"limitations our propose annotation strategy can be applied to labeling other mrc problems, no matter situated comprehension ones or not. however, when generalizing to other problems other than personality prediction we studied here, the accuracy of the user notes may vary with the difficulty of tasks. additional human verification on the correctness of notes like in our section 4.3 need to be conducted. our unsupervised training technique does not support the longformer reader with character history (char-hist longformer) yet. therefore, the improvement from unsupervised training for our this model is smaller. while longformer is common in benchmarking for long story understanding tasks. there are other families of models (rae et al., 2020; izacard and grave, 2021; ainslie et al., 2020; xiong et al., 2021; pang et al., 2022) handling long text encoding. we leave the comparison with these models to future work. potential risks like the other work that based on the similar set of books (bamman et al., 2019; bamman, 2020; vishnubhotla et al., 2022; thai et al., 2022), the classic literature may be limited by the time of writing, thus raise fairness considerations. however, please note that our dataset construction strategy is not limited to these books, but can work with any books on weread to create a sampled book set without such biases. the main reason we stick with the current list of books is for reproducibility since they are publicly available."
1448,"limitations in style transfer, content preservation and style transfer are adversarial. long texts have richer contents and more abstract stylistic features. we also notice that content preservation is the main disadvantage of storytrans in automatics evaluation results. case studies also indicate that storytrans can maintain some entities and the relations between entities. however, strong discourse-level style transfer ability endangered content preservation. in contrast, baselines such as style transformer have better content preservation but hardly transfer the style. we believe that storytrans is still a good starting point for this important and challenging task. during preliminary experiments, we also manually inspected multiple author styles besides shakespeare, such as mark twain. however, we found that their styles are not as obvious as shakespeare, as shown in the following example. therefore, we only selected authors with relatively distinct personal styles for our transfer experiments. in future work, we will expand our research and choose more authors with distinct styles for style transfer. for example, the style distinction between the following examples is not readily apparent. • everyday story in our datatset: ashley wanted to be a unicorn for halloween. she looked all over for a unicorn costume. she wasn’t able to find one. • ""a double barrelled detective story"" by mark twain: you will go and find him. i have known his hiding-place for eleven years; it cost me five years and more of inquiry."
1449,"limitations an important limitation of our approach vawi is the need for extracting visually-hungry words (vhwords) as the trigger to inject visual knowledge into plms. in real-world applications, it is hard to obtain the annotations of vh-words. therefore, we propose three vh-words extraction strategies. however, the three strategies may be not always proper for all nlp tasks, and we rely on the experimental results to select the best one among them. besides, we adopt the text encoder of clip as the vl-ptm for generating the visually-aligned representation. as a pre-trained model, clip also may contain biases learned from the pre-training corpus, which may result in improper biased prediction on some nlp tasks."
1450,"limitations due to the structure of meetingqa, the answers to questions asked by participants (if any) are present in the transcript itself, making it an extractive task. therefore, we do not extensively explore the use of generative models since the predictions do not stick to the sentences in the transcript and could possibly include hallucinations. however, we aim to mitigate hallucinations by using instruction-tuned generative models with suitably designed instructions and enforce a strict exact match criteria for filtering any possible hallucinations. future work can explore how to adapt or evaluate non-instruction-tuned generative models on this task and better identify hallucinations with a more relaxed filtering to improve performance. we also do not report zero-shot performance of instructgpt (ouyang et al., 2022) as these models are not freely accessible. additionally, we use a simple multi-span qa adaptation technique from segal et al. (2020), but predicting answer spans by classifying each token can be difficult to train leading to slightly lower performance (discussed in section 4.1). we hope our dataset provides additional motivation for future work on multi-span qa. finally, meetingqa only comprises of publicly available meeting transcripts in english, but our methodology of data collection and model training (using multilingual variants) should still be applicable for other languages in future work. ethical considerations the human participants in our work were recruited by an external crowd-sourcing company that ensured annotators provided informed consent, were given fair compensation, and no personally identifiable information (pii) was collected or released. we use existing publicly available meeting transcripts collected by the ami project (carletta et al., 2005) in controlled scenarios and filtered for offensive/toxic content. we also conducted manual inspection of a random sample from annotated transcripts and did not find any toxic content or pii. furthermore, the collected data and experiments are conducted in english and we do not claim generalization of our findings across languages. given the broad nature of meetings, the content can fall into a number of domains, of which only a few are represented in the ami corpus. therefore, we do not expect models trained on meetingqa to generalize to certain domains such as judicial, ethi- cal review, congressional proceedings, etc. which involve specific jargon and rules of engagement."
1451,"limitation dect explores how to adapt black-box ptms on downstream tasks. as we show in section 4.4, our method is not comparable to fine-tuning on hard tasks with increased data points. moreover, we only focus on classification tasks in this work and do not testify dect on free-form generation tasks. in the future, we will work toward more general maas adaptation strategies across tasks. ethical statement as large language models are getting more and more popular in nlp research and application, dect provides a cost-efficient way to adapt these large models. however, we need also to be cautious about the improper adaptation of large language models, such as generating toxic and biased speeches."
1452,limitations section for additional
1453,"limitations of existing methods and providing a more comprehensive view of a model’s predictions. limitations our work shows that crest is a suitable framework for generating high-quality counterfactuals and producing plausible rationales, and we hope that crest motivates new research to develop more robust and interpretable models. we note, however, two main limitations in our framework. first, our counterfactuals are the result of a large language model (t5), and as such, they may carry all the limitations within these models. therefore, caution should be exercised when making statements about the quality of counterfactuals beyond the metrics reported in this paper, especially if these statements might have societal impacts. second, crest relies on a rationalizer to produce highlights-based explanations, and therefore it is limited in its ability to answer interpretability questions that go beyond the tokens of the factual or counterfactual input."
1454,"limitations the annotation task we proposed in this work, i.e., detecting factual errors in summaries and providing human demonstrations and feedback for correcting the identified errors, can be complicated and timeconsuming. during our recruiting phase for mturk annotators, we found that the ratio of annotators who were qualified after finishing the qualification test was relatively low. therefore, it can be difficult to scale up the annotated dataset given the time and budget limitations. as a result, our dataset is of a relatively small scale and we only used one summarization dataset (xsum) and one base summarization model (pegasus). in this work, we view summary factual consistency as an example of user-expected quality to study leveraging natural language feedback for aligning system outputs with user preferences. however, user preferences can be diverse and personal and some user-expected output quality will be less well-defined and objective than summary factual consistency, which further increases the difficulty and ambiguity of data annotation and model evaluation. therefore, it can be challenging to directly apply the methods we proposed in this work to such subjective quality aspects, and we leave it for future work to explore generalizing our methods to more diverse user expectations and preferences."
1455,"limitations our method has the following limitations: datasets: in multi-modal pre-training, we rely on downstream datasets to evaluate the performance of pre-trained models. the commonly used entity extraction datasets are relatively small and lack diversity, so the proposed method may not generalize well to real word scenarios. lack of image modality: in layoutmask, we focus on text-layout interactions, leaving the image modality unexplored. however, documents in the real world contain many elements that can not be described by text and layout modalities, like figures and lines, so incorporating image modality is important in building a universal multi-modal pre-training model for document understanding."
1456,"limitations first, although our approach and existing methods for controllable text generation can improve the constraint accuracies, they are currently unable to achieve 100% accuracies in the vast majority of aspects (e.g., sentiment or topic). this makes them not yet applicable in scenarios with requirements of 100% control fulfillment. second, there is still a gap between the automatic and human evaluation of text generation, which makes there a trade-off between precision and efficiency in the evaluation of controllable text generation. third, although our approach reduces the mutual interference of plugins so that multiple plugins can be combined at a relatively small cost (a decrease in constraint accuracy), this cost will not be zero, which puts an upper limit on the number of plugins can be applied simultaneously. fortunately, for controllable text generation, the number of controls applied simultaneously is generally not too large (e.g., four or five aspects)."
1457,"limitations this work is motivated by the intuition that a mention is more likely to refer to an entity that occurs shortly earlier and refers to a high frequency entity but has not recently used. for the latter pattern, we show examples of topic switching to explain why these phenomena happened, but we have not found a rigorous linguistic explanation to support this finding. we provide empirical results on four benchmarks plus a book. however, the scarcity of long-doc cr benchmarks hinders us from verifying on a larger scale. our major contribution is a new cache design, but we also find the cache design becomes less matter when using a huge cache. nvidia a100 has 80g memory, which means it can handle a document of 100,000 words with a conventional cr model. as the gpu becomes larger and cheaper, the importance of studying cache design is weaker."
1458,"limitations this study aims to detect signs of anorexia, selfharm, and depression in users of social media environments through a double-domain adaptation of a language model. this study presents some limitations, mainly because these datasets are observational studies and we do not have access to the personal and medical information that is often considered in risk assessment studies. for example, we cannot discard that some users who publicly expressed that they have been diagnosed with anorexia are actually non-anorexia cases. however, the identification of positive users from selfexpressions of diagnosis is a common practice in this area (coppersmith et al., 2014), and the test collections built in this way are regarded as solid experimental benchmarks. there are also some limitations given by the nature of the data, as the users in these datasets might differ from users at risk who do not have exposure to social media (e.g., elderly people or individuals who do not have an online account or decided to not make their profiles public)."
1459,"limitations though our approach demonstrates better performances than the baseline models, how to design a good code-format prompt has not been fully inspected. besides, we mainly conduct experiments on the black-box gpt-3 and codex models but they are not open-sourced and querying the gpt-3 model cost the economic budget. and the use of llms may bring environmental pollution. another limitation of our approach is that the code-llms mainly trained on programming language datasets with english annotations. exploring our model on non-english datasets (like chinese datasets) is the future work."
1460,"limitations as shown in table 3, the results demonstrate the effectiveness of our proposed eft, but the performance of the unseen subset is still limited by comparing it with the performance of seen subset, which suggests plenty of room for improvement. data augmentation or generalization methods based on semi-supervised methods could be effective to solve the problem in the future."
1461,"limitations there exist some limitations in our work. livechat is a chinese-originated dataset involving unique cultures and abundant replying styles. however, this intensifies the difficulty of fully understand- ing the content of this dataset. fortunately, the same data construction pipeline can be applied to streaming platforms of other languages, like tiktok. and currently, our livechat is only sourced from 351 streamers on douyin, not sufficient to train a general chatbot. we believe that livechat helps get one’s foot in the door to the wonderful and diversified live scenarios and a dialogue model pre-trained on the considerable amount of videosourced dialogue data among cross-platforms is promising. besides, livechat contains some noisy spoken language segments that are not easy to read after transcribing from the asr tool. the upper bound data quality is limited by such third-party tools. the future work to concatenate such text segments to restore the content of the original expression by streamers is highly anticipated. as for the dialogue-matching method, we simply implement a combination of bow and bert for semantic matching, which needs further optimization. other limitations from the training perspective can also be highlighted. for example, contextual background information is not considered in our modeling. that includes history dialogues in multiturn settings and information from other modalities, like the streamer eating in front of the camera. in addition, we have not explored enough of our annotated basic profiles. in our primary experiments, we found that directly adding basic information such as age, gender, location, and other room information has limited influence on the model performance. we account for the fact that these basic profiles have limited connections with reply styles and contents in livechat. also, note that we remove the repetition part of a streamer’s response before training, while it is useful to maintain this pattern in practical application. ethical consideration this work presents livechat, a free and open chinese dataset for the research community to study personalized open-domain dialogue generation and addressee recognition. our dataset contains wellprocessed dialogues, and annotations (basic profiles and text profiles). data privacy the original live-streaming clips and streamers’ profiles of livechat are collected from douyin, one of the largest chinese livebroadcasting platforms. similar to previous dialogue data from reddit (mazaré et al., 2018) and weibo (qian et al., 2021), livechat is an open- domain dialogue dataset that crossover multiple topics and users. since all streamers must comply with platform rules during their online live streaming under the strict supervision of the chinese government, their topics do not contain any pornographic, violent, reactionary, or discriminatory statements. besides, due to the property of streaming, historically broadcast videos are no longer available when finished. therefore it is not traceable from livechat to the identity of real streamers. moreover, we clean the raw data with transformation, anonymization, and deletion to ensure there is no disclosure of private information and the identity of the streamers or audiences can not be inferred from it. thus, all the collected data (including persona profiles) is publicly available and does not contain any private information of streamers and audiences, such as emails, phone numbers, and real user names. although we collect the age and location information, in our basic profile, the age is expressed as an interval range that doesn’t represent the real age of the streamers, and the location only contains the province’s information. besides, all the attributes of our basic profiles are re-indexed as numbers in the final released dataset. thus, both our raw data and persona profiles do not create additional ethical risks. moreover, we are sure that all the collected data is consistent with the platform usage rules and protocols. livechat will only be allowed to be used for academic research. at last, our construction of livechat was approved by an internal review board (irb). annotators in terms of basic profile annotation and manual evaluation, all the annotators are chinese undergraduates specifically responsible for annotation work in our institution. they are informed of the ongoing research and well known the way the curated data will be used. all the annotated information and evaluation results do not contain any private information."
1462,"limitations as we use only a small number of language pairs, it is not clear how general our"
1463,"limitations the current method works with a large prompt search space t , which means a tremendous number of inference api calls are required. though figure 2 shows that the average cost of finding a lottery prompt for each instance is low, the searching process is highly randomized and there is no guarantee of the performance of searched prompts over the test dataset. therefore, finding strong prompts over the training set can still be laborious. how to use plm inference calls more efficiently and leverage the generalization ability of t ∗ within a reasonable cost is of future research interest. our acceleration strategy can be found in appendix b. another aspect is that not all strong prompts are interpretable as presented in 2. while recently emerged larger models like chatgpt demonstrate superb language understanding ability and can almost always answer yes or no questions correctly given a human-interpretable prompt. this gap observed between small plms like roberta and large language models like chatgpt is yet another interesting research topic. ethical considerations this work shows that with proper plain textual prompts, instance-level desired results can be prompted from plms. this inherent feature of plms means attacks can be launched to produce rude or discriminated words. on the other hand, however, we believe it can also be a technique used for debiasing a plm. overall, this effect depends on the intention of the users and the pre-training corpus of the corresponding plms. the analysis of this study can be used to facilitate the community to develop more specifications for the rational use of plms (especially the super-large ones), and more approaches to effectively prevent potential ethical issues. for example, we can use this technique to analyze which outputs that may have ethical issues are easily triggered by which contexts (prompts) and develop a set of intervention methods to make these tokens unavailable for output."
1464,"limitations our work has the following limitations. first, our proposed facialmmt approach is a two-stage framework that is not fully end-to-end. we plan to propose an end-to-end framework in the future, which integrates face sequence extraction and multimodal emotion recognition in a joint learning manner. second, this work primarily focuses on the visual modality, and has not yet delved into other aspects of the mermc task. therefore, in the future, we plan to leverage the extracted face sequences to explore better cross-modal alignment and multimodal fusion mechanisms to improve the performance of the mermc task."
1465,"limitations as previously mentioned, teast maps relations onto the corresponding archimedean spiral timeline and transforms the quadruples completion to 3th-order tensor factorization. it is required to store the values and this slightly increase the space requirement and training time in the embedding learning process. among all the baselines, tcomplex, tntcomplex and telm are all tensor factorization based models. table 4 compares training time and space requirement between our model and baselines on icews14. tcomplex is the smallest model and takes the minimum training time. compared with tcomplex, our model is about 4.6% bigger than tcomplex, and takes 21.4% more training time."
1466,"limitations as exciting as this work is, it does have several limitations and a lot of opportunities for future improvement. how to scale? we demonstrated our method in a highly constrained environment with very limited concepts, whereas humans are able to pick up new concepts in the noisy world with few shots. how could these representations learned in a clean environment be useful in real world? would comparative learning still be useful outside of the classroom? we followed the baby step of progressive alignment and hoping that establishing a set of clean base knowledge, can ease the acquisition of future more complex concepts through comparisons with existing knowledge, analogy and hierarchical abstraction. this hypothesis remains to be investigated in the future. what about other words? some concepts can be learned through just visual inputs, like color, whereas other concepts require grounding through different sensory types or modalities, like “hot”, “loud” and “fast”. even more concepts are built upon existing words through abstraction and generalization, e.g. “philosophy”, “momentum”. comparisons can still be used to ground these words, but input to these comparisons could vary from data modalities to computation methods, to abstract representations. we leave these for future work. how to put words into sentences? this work only focused on the grounding of individual words into visual representations, whereas sentence syntax, grammar, and article structure are yet to be learned. for future work, we could treat language as its own modality, and learn the structure through comparisons as well. just like in an elementary linguistic class, a teacher would list out several examples “i shower”/“you shower”/“he shower”. humans can learn grammar through what’s changing and what’s constant. this could be an interesting next step to look into. who can offer the supervision? as mentioned at the beginning, human language acquisition is a highly supervised learning process. babies are rarely inventing new words but learning how adults label objects through generations of conventions. a classroom setting with highly structured curriculum and clean dataset takes a lot of curriculum design and heavy annotation. this is the cost that humans are willing to spend in order to educate human children from kindergarten to college. maybe it is a fair price that we have to pay in order for artificial intelligence to learn what we want them to learn. about the current work itself, there are several constraints that we are limited to. first of all, due to limited computation resources and data size, we had to take a shortcut by using a pre-trained clip embedding as a starting point for our models. in theory, we could and would love to train our models from scratch, just like how a new born would learn their first language. a dataset like toys-200 (stojanov et al., 2019) could mimic the process of babies interacting with the objects, get a 360 view and help build 3d mental representations. second of all, like many other continual learning methods, an unbounded memory space is an unrealistic assumption. as more concepts are learned, the memory space would grow fast, so as the search time. an interesting next step could be to reorganize the memory according to the association distances and hierarchical structures. lastly, our work aims at proposing a novel language acquisition definition and the comparative continual learning method. we used somewhat simple model architecture and image generation models for proof-of-concept demonstration on the method. more sophisticated model architecture and training can be switched for different input modalities and applications. listed above are several major limitations and future directions based on current work. we are more than happy to take constructive suggestions and criticism to help improve this and future works."
1467,"limitations diversity of corpora while the newly introduced lexfiles corpus is significantly more diverse compared to the pile of law corpus of henderson* et al. (2022), it is still an english-only corpus covering only 6 legal systems (eu, uk, coe, us, india, canada). despite, the fact that we can train better models (lexlms) and evaluate these models across these corpora, in future work, we should extend our analysis to cover even more languages and legal systems, and a higher granularity in the labeling of legal fields within these systems. not only will this help support the inclusion of other legal traditions but also adding more linguistic and cultural diversity will help us better understand the robustness of existing methods. similarly, the newly introduced legallama benchmark consists of 8 sub-tasks targeting eu, echr, us, and canadian jurisdictions in a very controlled setting; where examples were automatically extracted. while on this benchmark, legaloriented plms has demonstrated a significant degree of “understanding"" of legal language and legal topics, this benchmark should be further expanded with more sub-tasks to evaluate the acquaintance of legal knowledge across more legal systems and topics, and possibly cleansed from both very easy and unsolvable examples. model considerations in this work, we consider encoder-only (bert-like) models up to approx. 350m parameters, while recent work on the development of large language models (llms) (kaplan et al., 2020; brown et al., 2020; hoffmann et al., 2022; chowdhery et al., 2022) is mainly targeting billion-parameter-sized models (10-100bs of parameters) that usually follow a decoder-only, e.g., gpt (radford and narasimhan, 2018), or encoder-decoder, e.g., t5 (raffel et al., 2020), architecture. moreover, new paradigms of training plms have been introduced, such as instructionbased finetuning (wei et al., 2021), and alignment via reinforcement learning from human feedback (rlhf) (stiennon et al., 2020; ouyang et al., 2022). latest gpt models (ouyang et al., 2022) have recently shown significant zero-shot progress on law-related tasks such as bar examination question answering (katz et al., 2023). thus, future work should follow the most recent advances by pre-training much larger auto-regressive gtp-like models that seem to lead to emergent zero-shot and few-shot capabilities. evaluation considerations in section 3, we present how we account for and evaluate multitoken expressions (terms) on the legallama benchmark; we are open to ideas on how we should possibly improve the current approach to provide a fairer and more robust evaluation framework across all models. similarly, in section 4.4, we fine-tune all examined plms for a single epoch to avoid extreme over-reparameterization and better estimate how model’s knowledge affects convergence and performance. nonetheless, there are possibly better approaches to control for these aspects, e.g., adapter-based (rücklé et al., 2021) finetuning, or other approaches, such as lora (hu et al., 2022). beyond performance while we consider a multi-facet analysis, we do not cover other interesting dimensions that should also be explored, especially since law is a very sensitive application domain; for instance trustworthiness-related topics, such as model interpretability (chalkidis et al., 2021b; malik et al., 2021), and fairness (chalkidis et al., 2022b). future work can build from the results reported herein to explore these important topics."
1468,"limitations we have demonstrated that across three standard re datasets, llms achieve sota results. in particular, gpt-3 yields such performance even given only 10s of training sample for in-context learning. we then showed that we can similarly achieve sota performance with the much smaller (and open-source) flan t5 (large) model, when trained using cot generations produced by gpt-3. we also highlighted key challenges for evaluation in this setting. but there are important limitations to these contributions. first, here we considered three standard re datasets with binary relations but— as we discussed—we excluded more complex re datasets. for example, we did not consider corpora containing n-ary relations between entities (taboureau et al., 2010). we were also unable to run experiments on datasets with lengthy texts and a large number of relations, such as docred (yao et al., 2021), due to the necessary prompt lengths for such inputs. second, while we found that cot-style explanations generated by gpt-3 can be fruitfully used as additional supervision to fine-tune smaller language models, we made no attempt to evaluate the quality of these generated explanations which may have an impact on the model performance. third, we did not fine-tune gpt-3 on the re datasets, mainly due to the cost of doing so. it is likely that a fine-tuned gpt-3 would yield performance superior to the results we achieved with flan t5 (which constitute current sota). but, in addition to the costs necessary for fine-tuning this model, the resultant weights would not be accessible to run locally in any case; one would have access to it only via the openai interface, which motivated our decision to fine-tune the smaller and open-source flan t5 instead. finally, we only experiment with datasets curated in the english language and therefore, we do not know that the issues we have highlighted could replicate in the same way in other languages."
1469,"limitations for those label names without semantic meanings, several keywords are still required for npprompt to work well. furthermore, this study focuses exclusively on the zero-shot setting. however, there are potential avenues for exploration in the few-shot scenario, which is prevalent in practical applications. the applicability of npprompt to other tasks, such as ranking and relation extraction, remains uncertain and warrants further investigation. designing a refinement method to jointly search for label words and templates can be a promising direction for future research."
1470,"limitations and ethical considerations of llm evaluation despite the promising results of llm evaluation shown in this paper, there are some limitations of this method. first, llm may possess incorrect factual knowledge (cao et al., 2021), so it is not suitable to use them in tasks that involve factual knowledge. next, llms trained to behave in a certain way can be biased toward certain responses. precisely, an llm that is trained to be safe and non-harmful can result in llms preferring to generate more positive and upbeat responses, which is observed throughout our interaction with chatgpt. additionally, even with researchers’ efforts to make llms safer (bai et al., 2022a,b), llms can still generate harmful and biased responses (ganguli et al., 2022; perez et al., 2022), which are violative of basic"
1471,"limitations of llm evaluation mentioned previously, there is a crucial ethical concern at the heart of llm evaluation. is it ethical to replace human evaluation with llm evaluation? some may question if this paper is suggesting that llms are now ready to replace humans and find this idea unsettling. as responsible and ethical nlp researchers, we understand these concerns but want to make it clear that this is not our intent. as our paper title suggests, we aim to offer an alternative option to human evaluation with the goal of enhancing the reproducibility of nlp research. human evaluation is still essential as the ultimate goal of nlp systems is to be used by human users, so it’s important to gather feedback from them. we highly enjoy the process of discussing the experiment settings and results with the english teachers we hired. we do not recommend that future researchers completely eliminate human evaluation; rather, we believe that human evaluation should be used in conjunction with llm evaluation. both methods have their own advantages and disadvantages, making them both necessary for evaluating nlp systems. we hope the positive results in this paper provide nlp researchers with an alternative method to evaluate systems and encourage further"
1472,"limitations the proposed approach (along with other methods for estimating uncertainty in inconsistent annotations) is only viable when the raw labels from different human annotators for each sentence are provided by the datasets. however, some multipleannotated datasets only released the majority vote or averaged label for each sentence ( i.e. poria et al., 2019). the proposed method made a gaussian assumption on the likelihood function for the analytic computation of the uncertainties. the results show that this modelling approach is effective. despite the effectiveness of the proposed method, other distributions could also be considered. data collection processes for aer datasets vary in terms of recording conditions, emotional elicitation scheme, and annotation procedure, etc. this work was tested on two typical datasets: iemocap and msp-podcast. the two datasets are both publicly available and differ in various aspects: • iemocap contains emotion acted by professional actors while msp-podcast contains natural emotion. • iemocap contains dyadic conversations while msp-podcast contains podcast recordings. • iemocap contains 10 speakers and msppodcast contains 1285 speakers. • iemocap contains about 12 hours of speech and msp-podcast contains more than 110 hours of speech. • iemocap was annotated by six professional evaluators with each sentence being annotated by three evaluators. msp-podcast was annotated by crowd-sourcing where a total of 11,799 workers were involved and each work annotated 41.5 sentences on average. the proposed approach has been shown effective over both datasets. we believe the proposed technique should be generic. furthermore, although validated only for aer, the proposed method could also be applied to other tasks with disagreements in subjective annotations such as hate speech detection and language assessment."
1473,"limitations we discuss the limitations of plugd in this section: (1) we explore decoupling document encoding from concrete tasks in this paper, and propose to represent documents as pluggable modules before task adaptation. therefore, plugd has a higher storage requirement than conventional encodingcoupling methods. we encourage (2) in the experiments, we adopt t5 as our ptm backbone. actually, the proposed framework can also be applied to more pre-trained models with various model architectures. besides, recent trends show that larger models tend to build more expressive text representations. it is worth exploring plugd with larger ptms with billions of parameters to learn informative document plugins. (3) in this paper, we adopt an external retriever to retrieve relevant documents for each input query. recent progress in retrievalaugmented language models shows that training the ptms with an end-to-end textual knowledge retriever can promote downstream performance. we believe document plugins can also serve as the ex- ternal knowledge base and enhancing plugd with end-to-end retrieval is a promising direction."
1474,"limitations by comparing to post-hoc debiasing methods. we hope that our study can make it more accessible for others to debias pre-trained language models with reduced computational requirements, and contribute to fair and inclusive nlp."
1475,"limitations as we stated under future work, one of the limitations is the variance decomposition-based proof. our work is based on simplified settings, i.e., linear squared regression assumption. post-ensemble variance is not evaluated due to the nature of the ens ensemble algorithm. extended experiments using vanilla bagging ensemble would enable analysis of post-ensemble variance. further investigation into refining the two stages would help understand the performance of lpms, e.g. those that are in phase-i but before the peak in figure 1. our results for multirc are based on the instance sampling, however a better sampling technique should be based on stratified sampling based on the ratio of the question types in the multirc set. however, to achieve this, the multirc set needs to be annotated for question types, which is currently missing. sampling techniques by themselves can become a research topic so that a further decrease of variance due to sampling can be achieved. although we list these items as limitations, they are also topics for future research within the greater theme of understanding the new bias-prevalence paradigm for lpms."
1476,"limitations in capturing all dimensions of fairness. further research is needed to develop comprehensive extrinsic metrics across diverse tasks. although our work has been centered around fairness in allocation-based (classification) applications, addressing fairness concerns in other types of language models, such as natural language generation models, is necessary. in generative tasks, the measurement of unfair outcomes would be distinct from the methods we have used. another area of potential future work could involve benchmarking debiasing methods for compressed models and developing new compression-aware methods."
1477,"limitation of the previous sentence representation learning approaches, which are limited to using only the input sentence. rankencoder leverages the distance between the input sentence and the sentences in a corpus to predict its semantic vector. rankencoder is universally applicable to any unsupervised sentence encoder, resulting in performance improvement, and we demonstrated this with three unsupervised sentence encoders. we achieved state-of-the-art semantic textual similarity performance by applying our approach to the previous best sentence encoder. we also showed that our approach is specifically effective for capturing the semantic similarities of similar sentences."
1478,"limitations (discussed below), our results indicate that, in general, revision-based data can be employed effectively for the given tasks, contributing towards solutions for each of the considered challenges. specifically, our suggested sampling strategy revealed that training on claim versions with a higher revision distance between them improves the performance when detecting claims in need of improvement. moreover, we found that the impact of the available types of contextual information is not only task-dependent but also depends on the quality issue that a claim suffers from. we argue that the developed approaches can help assist automated argument analysis and guide writers in improving their argumentative texts. with our work, we seek to encourage further research on improving writing support not only in debate communities but in educational settings as well."
1479,"limitations although our proposed method achieves promising results and outperforms most baseline systems on the st benchmark, it still has some limitations: (1) the performance of our method still lags behind a recent work speechut, although our approach has the advantage of consuming less time and resources; (2) we observe that the modality gap is still not eliminated and the effect of exposure bias on the modality gap still exists; (3) the performance of our approach on larger datasets and larger models remains to be explored; (4) how to apply our approach to other tasks also needs to be further investigated."
1480,"limitations in this section, we discuss the limitations of our proposed framework. our ugdre can reduce the false positive pseudo label by estimating the uncertainty of the model prediction. however, it is difficult to reduce the false negative pseudo labels by uncertainty estimation. our framework also relies on human-annotated data to train the pre-denoising model, which causes the sensitivity of our framework to the quality of human-annotated data. thus, the improvements of models that continue training on the docred dataset are not as well as on the re-docred dataset. moreover, iterative training introduces additional computing overhead, which makes the training process time-consuming."
1481,"limitations amrsim has high prediction efficiency but the training process is time-consuming. in our experiments, one epoch training on one geforce rtx2080ti took about two and a half hours. selfsupervised learning requires a large amount of training data. parsing wiki sentences into graphs requires time, but the advantage is that it can be processed offline. another issue is the length limit. transformers can only handle limited sequence lengths due to the computational and memory complexity of attention calculation. therefore, encoding large amr graphs is challenging. possible solutions include applying sliding window algorithm to split a large amr graph into several subgraphs and merge the scores."
1482,"limitations (see §8), we think the benefits of our work overshadow its limitations. we provide a simple approach and a new set of tools to interpret transformer models and compare them. the realm of input-independent interpretation methods is still nascent and it might provide a fresh perspective on the internals of the transformer, one that allows to glance intrinsic properties of specific parameters, disentangling their dependence on the input. moreover, many models are prohibitively large for practitioners to run. our method requires only a fraction of the compute and memory requirements, and allows interpreting a single parameter in isolation. importantly, our framework allows us to view parameters from different models as residents of a canonical embedding space, where they can be compared in model-agnostic fashion. this has interesting implications. we demonstrate two consequences of this observation (model alignment and stitching) and argue future work can yield many more use cases."
1483,"limitations it is important to note that the unified representation proposed in our study is just one option among many. other linearization methods may potentially yield better results. for example, research by yin et al. (2022) and aghajanyan et al. (2022) has explored using code generation with jupyter notebooks and a hyper-text language model with structured prompting, respectively. further research in these areas, such as converting all structured forms to markdown language or hyper-texts, may yield alternative unified representations."
1484,"limitations since webnlg is derived from 2015-10 version of dbpedia, factkg does not reflect the latest knowledge. also, another limitation of our work is that the claims of factkg are constructed based on single sentences, like other crowdsourced fact verification datasets. if the claim is generated by more than one sentences, the dataset will be more challenging. we remain this challenging point as a future work."
1485,"limitations despite the fact that some measures have been implemented to minimize bias in labeling, we are still explicitly aware that our dataset may contain mislabeled data due to differences in the subjective understanding of toxic language by the annotators. in addition, due to the limitation of data coverage, the samples in our dataset are predominantly in simplified chinese, with very few samples in traditional chinese, as discussed in section d.5. meanwhile, as shown in the error analysis in section 6.3, our benchmark of toxic knowledge enhanced is not practical for all types of toxic comments, lacks sufficient background knowledge, and can easily lead to spurious associations. for reasons of intellectual property, we only capture the comments rather than the full text, which affects the actual semantics of the sentence to some extent. besides that, non-textual features are not taken into account in this work, such as images and meta information about publishers. in future work, we will further research span-level and multi-modal toxic language detection."
1486,"limitations while we establish the “miraculous” ability of character-blind models to induce robust spelling 8the models were trained on the same number of tokens, but only 6% of byt5 training was on english, and we estimate 4 utf-8 bytes per t5 token. information through large-scale web pretraining, our work does not attempt to identify the mechanisms or sources through which this information is learned. possible sources within web corpora include: dictionaries containing phonetic pronunciation guides, alphabetically ordered lists, typos and other misspellings, and examples of spelling words with dashes or spaces between every character. linguistic phenomena that may aide in inducing spelling knowledge include words with predictable morphemic makeup, and cases where meaningform relation is non-arbitrary, contra saussure’s “semiotic arbitrariness”. we refer the reader to itzhak and levy (2022) and kaushal and mahowald (2022) for work in this direction. most of our image generation experiments are limited to english. we present preliminary results in appendix a showing that our byt5-based models have stronger multilingual understanding than t5. however it would be valuable to test this further, and to explore training image generation models on multilingual image-caption datasets, as opposed to merely using a pretrained multilingual text encoder. ideally, it would be possible to conduct controlled comparisons between pretrained text encoders that differ only in one regard, to isolate all factors contributing to performance. however as pretraining large language models is resource intensive, we were only able to use off-the-shelf text encoders, which often differ along multiple axes. in our text-only experiments, we isolated the contributions of character-awareness (byt5 vs. mt5/t5) and multilinguality (byt5/mt5 vs. t5). however, in our image generation experiments, these factors were conflated, as we had limited resources for training new models. still, the fact that byt5based image generation models outperform t5 despite being multilingual (which often degrades performance on english-only tasks) strongly suggests that character-awareness is the key factor for spelling ability. another limitation is that we focused on image generation models that leverage frozen pretrained text encoders. this enabled straightforward experimentation by swapping encoders and retraining the image generation module. however, it remains to be seen whether our results extend to settings where the text encoder is trained along with the rest of the model, as in yu et al. (2022)."
1487,"limitations there are a few shortcomings that we discuss below: underrepresented fine-grained lms: although we had chosen a careful sampling method focused of event-centric informative dataset aiming to increase the likelihood of fine-grained lms’ occurrence (kitamoto and sagara, 2012), we think the low frequency of fine-grained lms in idrisi-ra is a major limitation as it contains solely 25.5% fine-gained lms. human errors: there are some human errors made during annotation due to the difficulty of the task. • annotators sometimes fail in distinguishing between location and organization entities (e.g., “red cross""). • different location types could be used interchangeably for the same locations which forms a difficulty for annotators (refer to section 5.1). • annotators highlight the locations that are mentioned as descriptions within the context of the tweet. we plan to overcome these errors as part of location mention disambiguation (lmd) annotation that aim to remove ambiguity of geo/geo entities (as a sequel to the geo"
1488,"limitations although pgpl is proven to be effective according to our intensive experiments, the current design of the pgp module may not be optimal and could be improved in the future. specifically, on the one hand, to choose the proper number of nearest text instances kc for each class c, we need to obtain the statistic values µyt , µ y <t and computed values µ c t , µc<t, γt for each dataset, which makes it somewhat not easy to scale to many different datasets. on the other hand, we heuristically filter pseudo-labeled texts by the top-k selection, which may also be improved with more systematical approach."
1489,"limitations in this paper, we show that lens shows better human correlation than other metrics on wikipedia and news domains. future research can further experiment and extend lens to other domains, such as medical and children’s books, as the preference for different simplification operations can vary depending on the domain and user. additionally, our work focuses on sentence-level simplification, and future work can extend lens to evaluating paragraph- and document-level simplification. simpeval dataset and lens are also limited to the english language."
1490,"limitations the proposed method ndcr has some limitations as follows: 1) the produced representation of simple proposition sentences in the proposition generator lies in a different space distribution with the image encoding, which affects the performance of their fused representation. although we introduce the reasoning information of compound proposition text to alleviate this issue, we hope to solve it by improving the text understanding capability of pretrained vlms. in addition, adopting the pretrained textual encoder of vlms to perform proposition decomposition is inadequate due to that they present an inferior understanding for the discourse structure of long texts. 2) the performance of samples with highly similar images from video frames is quite different from that of humans. we may improve it from the perspective of image difference modelling. 3) the experimental results indicate that our method is effective at logical inference on examples with medium-length descriptions, but there is still room for improvement for longer descriptions."
1491,limitations section. we hope our analysis of rarr would help with developing new approaches for integrating attribution to lms.
1492,"Possible limitations include the limited PLM architecture/size (although we include additional results with RoBERTa in the Appendix D) and faithfulness evaluation metrics are not necessarily comprehensive. And we only focus on text classification tasks. As a result, we do not investigate other language classification (e.g., natural language inference and question answering) and text generation tasks. If we can intrinsically know or derive the golden faithful explanations (Bastings et al., 2022; Lindner et al., 2023), the exploration of model robustness and explainability will be alternatively investigated for revealing the internal reasoning processes. And future work could include human study (e.g., evaluation about whether explanations help users choose the more robust of different models) and improve the robustness by more diverse ways (e.g., model distillation and data augmentation).
Our findings are also in line with Tang et al. (2022) and Logic Trap 3 (Ju et al., 2022) which claims the model reasoning process is changed rather than the attribution method is unreliable. Different from this two works – output probability perturbation or changing information flow, we view our results as complementary to their conclusion via sourcing the improvement of faithfulness. Although we show the link between robustness and faithfulness empirically, future work can strengthen the conclusions by discussion on a more conceptual and theoretical level. From a theoretical perspective, one possible reason is that the gradient of the model is more aligned with the normal direction to the close decision boundaries (Wang et al., 2022). In the future, we would like to analyze the relationship between robustness and explainability from geometric dimension.
Furthermore, we do not exhaustively experiment with all possible evaluation settings of interest even with the scale of our experiments. For example, saliency guided training methods (Ismail et al., 2021) could have been used as another baseline. We hope this work inspires more future work that develops more effective strategies to make explanations reliable and investigate how our findings translate to large language models, such as GPT-3 model family2, as with the emergent capabilities of these models, fidelity to their explanations or rationale will have societal impacts on accountability of NLP systems.
2https://beta.openai.com/playground"
1493,"While we have included two distinctive dataset (CNNDM and XSUM) in our experiments, more non-news datasets could be included in future studies. Other possibilities for future work include comparing the capability of RL-based reward learning and contrastive reward learning in improving the factuality of abstractive summarization models."
1494,"Our investigation focuses on one aspect of commonsense reasoning restricted to one dataset. There may be numerous other factors in real-world applications. Therefore, our findings may not comprehensively capture the entirety of commonsense reasoning phenomena. Another limitation is that for the sake of simplicity and feasibility, we assumed a fixed threshold of k=200 to categorize frequent and less frequent names. However, this threshold may not be universally applicable to all contexts or datasets, and different thresholds could lead to different results."
1495,"This work introduced a powerful attackability detector but also demonstrated that its success is limited to a matched setting, where the same attack method is used in both training and evaluation of the detector. A second limitation with this work is that all experiments were carried out on natural language classification tasks. It would be useful in the future to extend these experiments to sequenceto-sequence tasks to have a more comprehensive set of results."
1496,"This analysis relies on comparing model predictions with human annotations. One limitation of this approach is the following: we are assuming that the human annotated labels represent a reasonable ground truth. However, it’s likely that the annotations have their own bias issues. A future area
of work is to analyze how reliable the annotations are for some of the top keywords surfaced here, especially for reclaimed speech and for Tweets with AAVE. However, because previous work has found that word choice and profanity are likely stronger contributors to bias against AAVE than linguistic features of AAVE (Harris et al., 2022), we hope that bias mitigation techniques at the keyword level can also help alleviate bias against AAVE without the use of sensitive racial or dialect classifiers. Another fruitful area of future work would be to better understand the relationship between mitigating bias at the keyword level versus the dialect level.
Our methodology is helpful for detecting the most widespread and prevalent problems. However, there may be other serious problems that do not receive the same amount of traffic that still deserve attention. Oftentimes, smaller groups of people, especially those who live at the intersection of multiple marginalized identities can suffer the worst harms from algorithmic systems (League, 2022). Thus, relying on frameworks that focus on bigger segments of the population poses the risk of missing important harms to smaller communities. In this work, we develop a list of keywords for bias evaluation by analyzing a corpus generated from all English Tweets on Twitter. However, because English Twitter is primarily composed of users from the United States and the United Kingdom, our list of keywords for evaluation is likely heavily skewed towards US-centric or Western issues. One way to mitigate this would be to repeat the analysis conducted here, but using separate corpora for each country or upsampling Tweets from countries with smaller populations of Twitter users in order to ensure we are getting appropriate coverage in other countries with smaller user bases. This would help increase coverage for minority groups in the data we use for bias evaluation. Another critical area of work would include expanding the analysis to other languages beyond English. The overemphasis of English has led to the underexposure of other languages in NLP research (Hovy and Spruit, 2016).
This work treats reclaimed uses of slurs as an important facet of the speech of marginalized communities. However, reclamation is not a ""bullet-proof"" process - some may find reappropriated uses acceptable and others may not. Additionally, reclamation may only be deemed acceptable by in-group members or in certain contexts (Rahman, 2012). Since the marginal abuse model only uses the text of a sin-
gle Tweet (and not any information about the Tweet author or conversational context), it is difficult for the model to account for such nuance. Furthermore, because this model is used to moderate all English content on Twitter, the model implicitly assumes the same utterance has the same meaning across the world, which is an extreme oversimplification. In other words, the model does not account for local variations in language use. Reclamation can also backfire, for example the Hong Kong media’s mocking of the reclaimed use of ""tongzhi"" (literally meaning ’comrade’) by the gay and lesbian community (Zimman, 2017; Wong, 2005). This example serves to illustrate the essentially contested nature of reclaimed speech and how language ideologies shift over time. With respect to automatic content governance, shifting language ideologies indicate the importance of 1) meaningfully engaging and consulting with affected communities on models used for content governance, 2) the utility of regular audits and model refreshes to account for change in language use over time, and 3) additional user controls to better accommodate for multiple definitions of harmful content. Lastly, there are inherent limitations to fixing socio-technical problems through purely technical means (Ball-Burack et al., 2021). We hope that our analysis provides an interesting case study of some of the challenges associated with automatic content governance in industry and sparks further discussion."
1497,"Our main limitation comes from dataset size. This was limited because we used human evaluation to label model responses as truthful or untruthful. That is, we have manually confirmed GPT-judge labels on Davinci responses, and extrapolated the system to Ada, Babbage, and Curie. Frankly, the limitations caused by the small size of the dataset were quite evident because the truthfulness detector was often biased towards producing one label (either 1 or 0). We attempted to solve this problem using lower regularization parameters, but this often produced models with lower performances. An ideal solution to this problem would be training the truthfulness detector on a large set of training instances, which is also our future direction."
1498,"Our study covered three types of widely used single attention with different mechanisms of assigning attention weights. However, we did not cover some compound attention mechanisms such as dual attention mechanism [8] and Co-attention [27], which
might contain different patterns affecting the fairness of the models. Also, Transformer [24], the cornerstone of PLMs, should be considered in this study as it is composed of multiple self-attention modules, and the intersection impact of multiple attention mechanisms can be studied by incorporating this model. Apart from the classifier itself, the different word representation models, which are well discussed in Naseem et al. [21], can also be brought into scope since word embedding can also affect fairness. From the dataset aspect, the quality of text can be further improved with pre-processing techniques mentioned in Naseem et al. [20] to ensure better performance and reduce the effect of the irrelevant factors. Also, since the toxicity classification tasks are not easy even for a human, there are noisy data inside the chosen datasets since we found that we disagree with some of the humanannotated labels by manual checking. Furthermore, the HSOL and Jigsaw datasets are imbalanced in terms of distributions of different classes. Therefore, modifications can be made to the loss function in the same way as focal loss [16] or dice loss [15] to mitigate the influence of data imbalance."
1499,"A limitation of our conclusions is that although it is necessary to use several fairness metrics to be able to properly quantify the bias, this is not enough. These metrics must be well chosen according to the context and the task being looked at. The expertise of a person working in the field is therefore always necessary to have the most complete possible interpretation of the bias. More specifically, the different fairness metrics measure distinct properties, and the fact that they are often incompatible has been a core part of the fair ML conversation from the beginning (Barocas et al., 2017). Thus, suggesting to choose a different metric depending on the sample size may sometimes be inappropriate, since this choice may depend on the meaning of the metric in a given application. We must therefore be very careful and see the notion of robustness as additional necessary information and not as a replacement for the metric’s meaning.
We also did not reduce the bias using advanced strategies because this paper focuses more on the analysis intended for a population closer to the law than to machine learning. In this vein, it is interesting to note that more and more tools are available to reduce bias. In particular, (Sikdar et al., 2022) makes it possible to reduce the bias according to several fairness metrics, therefore remaining in our
logic of taking several metrics. The main problem raised by our article comes from the fact that fairness indices are not stable when they are calculated. We should consider them as random variables and look at their law. The first step is to look at the mean and the variance as done in this paper but having the full distribution would be more interesting. Works that compute the asymptotic law can be taken as an example like (Ji et al., 2020; Besse et al., 2022).
Ethics Statement
Natural Language Processing is gaining a considerable amount of attention these days and it is extremely important to evaluate how NLP datasets will impact the gender bias when used to train models that will be used in the real world. This work uses different experiments and fairness metrics to shed light on the shortcomings of these metrics with respect to gender bias made by ML algorithms on textual data. We believe that transmitting knowledge from research to industry on a subject like fairness is essential to make the field of ML more ethical. Hence, this work focuses on issues that most affect the industrial ML landscape and contains a clear message to them on how they should change their current practices."
1500,"The DEPTH+ metric resolves the problem of botgenerated Wikipedia editions that have many botcreated articles and bot-made edits on their articles. Yet, the DEPTH+ metric does not fix the problem of automatically translated Wikipedia editions in the Wikipedia project that their articles have been largely translated by poor direct translation or shallow template-based translation. The quantifications of these automatically translated Wikipedia editions in the Wikipedia project cannot be carried out as systematically as the bot-generated Wikipedia editions, and examining each Wikipedia edition separately is the only way to accomplish such quantification. Another limitation of the DEPTH+ met-
ric is depending on the active users metric, which dynamically decreases the DEPTH+ metric values if there are no editing activities on the articles in the last thirty days. We preferred to use the total unique users who made at least one edit but do not have that figure, so we are approximating it with the already calculated active users metric by the Wikipedia project."
1501,"Despite memory and GPU limitations presenting significant obstacles for our project, we were still able to create high-quality fake scientific papers. Nonetheless, we believe there is room for improvement in addressing such limitations.
Due to the complexity of parsing PDFs, we are currently limited to specific sections (abstract, introduction, conclusion) instead of complete papers. Moreover, processing entire publications would require substantial computational efforts. We believe that selecting sections dynamically at random instead of a fixed choice is worth exploring and will
be the focus of future work. Beyond DetectGPT (Mitchell et al., 2023), other zero-shot text detectors such as GPTZero7 present promising solutions worth testing on our benchmark dataset. However, at the time of writing, such solutions are not available for experiments at scale.
In future work, we aim to address these limitations by exploring dynamic section selection, improving papers’ quality, adding human-LLMs co-created samples, and investigating the potential of zero-shot text detectors like GPTZero as they become more accessible and scalable."
1502,"The main concern is that debiased techniques may remove simple biased features. However, to our knowledge, most debiased techniques (Rathore et al., 2021) can only remove biases across a concept subspace (e.g., the bias direction for gender) in the embedding space. Another setup of data debiasing, e.g., He et al. (2019), requires hypothesized biases to train biased models and is limited to tasks with known hypothesized biases (e.g., lexical overlap for NLI). Also, they remove biased examples rather than identify biased symbols (e.g., label hints). However, we still expect future works to consider other complicated patterns beyond symbol insertions or word substitution."
1503,"We are optimistic that the algorithmic workflow presented in this paper can be generalized to other languages. When the victim models are in languages other than Chinese and English, however, we also acknowledge the uncertainty in achieving a high attack success rate while at the same time achieving fluency in generated examples. In addition, because of the variation in linguistic structures
across different languages, further efforts are required to design language-specific transformation methods (such as the homophone and morphonym transformations for the Chinese language case in this paper)."
1504,"To recruit participants who are more likely to write posts in other languages, our recruitment was restricted to bi/multilingual individuals residing in the US. Due to these criteria, most participants in both studies were Spanish speakers and writers, which is the second most common language spoken in the US. Since Spanish is a rich-resourced language, findings about the quality or accuracy
of MT may be different if we consider a more linguistically diverse participant sample. Future work could recruit global bi/multi-lingual participants to understand broader perceptions of machine translation.
Our discussion section advocates for a number of control features that are currently not available in SNS. Our results suggest that these features would make users more comfortable with MT, but this suggestion would ideally be confirmed with a controlled experiment. Our future research will be focused on designing the proposed controls. In particular, we plan to conduct an experiment on prototypes of three different translation features: one allowing the user to read but not edit the MT, one allowing the user to add a translation manually, and one allowing the user to read and edit the MT if there are any errors. Each prototype will also have an option to include/not include the translation with each post. This experiment will measure which features most improves users’ perceived control, perceived satisfaction, ease of use, and intention to use MT."
1505,"limitations. our modification utilizes sense embeddings. since the senses were not mapped to an external inventory, the senses cannot be interpreted. apart from the lack of interpretability, sense embeddings are superseded by contextual embeddings derived from transformer models with sense awareness (huang et al., 2019; levine et al., 2020; scarlini et al., 2020). while sense embeddings and contextual embeddings are not mutually exclusive, it is necessary to alternate between them for the purpose of privatization and optimization."
1506,"limitations through our work, we analyze various sentiment and toxicity analysis models to determine if they show an ableist viewpoint. the results depict a statically significant presence of disability bias, and we publish our method for any individual to access and use. this step is crucial in the field of nlp to mention the ramifications a given model can have on society. one limitation of this work is that we analyze models that are trained in the english language. we understand that the social concept of disability can change for various cultures and languages. the scope of this paper for now only looks into one language. ethical statement the paper provides a method to parameterize ableist bias in nlp models, but we acknowledge that this is not the sole method that can be used for identification. the work is limited only to identification in sentiment analysis and toxicity detection models. there can be other methods of identification that are rapidly being worked on which may not have been included in this process. we also understand the effects various other forms of social biases can have when viewed alongside disability bias. we, therefore, will be working on measuring the combination of social biases through a cultural lens for the future."
1507,"limitations our work only preliminarily explores the field of textual adversarial attack on chinese minority languages and evaluates the robustness of the tibetan part in the first chinese minority multilingual plm. the textual adversarial attack is a major threat in the information processing of chinese minority languages. we hope our attack method, experiment results, and"
1508,"limitation (see ‘limitations’ section), we would like to underline that our primary goal of this article is to highlight the ample possibility of data leakage and the impossibility of verifying the lack of data leakage with a closed model. as long as the trend of closed models and continuous training loop continues, it will become more challenging to prevent data leakage (training-test data contamination) and ensure fair evaluation of models. therefore, in order to ensure the fair evaluability of the models, we argue that the model creators should (1) pay closer attention to the training datasets and document potential data contamination, (2) create mechanisms through which the training datasets and models can be scrutinized regarding data leakage, and (3) build systems that can prevent data contamination from user inputs."
1509,"limitations in this paper we attempt to understand model responses using multiple prompts, and 2 different set- tings (tokens and full text). the gpt-3 responses were too inconsistent. we attempt at explaining our findings by analyzing the full text responses, but a more thorough analysis of the full text responses would shed more light into how these models behave. this will require extensive manual analysis of each statement and prompt response. currently we do not explore every kind of full text response for each category type and prompt. more work needs to be done to systematically analyze the full text responses and connect them to the token responses and confidence scores. besides, text-davinci-003 was the best performing llm when we started experimentation. recently released chatgpt api and gpt-4 from openai, and other open source models were not analysed in this study, but one could extend our study to any class of llms to assess llm quality as well as the differences among them."
1510,"limitations. we note that directing the substitution to candidates with matching grammatical categories incurs additional information leakage that is not accounted for by our modification. too remedy the unaccounted information leakage, one could recast the candidate selection through the exponential mechanism (mcsherry and talwar, 2007)."
1511,"limitations of its representation of the depth of collaboration in wikipedia corpora. we also quantified the bot activities in the wikipedia project and excluded the bot-created articles and the bot-made edits on wikipedia articles. we lastly proposed the depth+ metric, defined its formal definitions, and highlighted its features, including a better representation of the depth of collaborativeness, a user-centered depth metric, and bot-free wikipedia editions after the removal of the bot-generated articles and the bot-made edits on those wikipedia editions’ articles. we hypothesize that a metric that is a better measure of authentic human collaborativeness will be a better measure of the degree to which corpora authentically represents the language and the culture of native speakers. one key aspect of our future work is to find ways to test this hypothesis. specifically, we aim to examine the performance and societal implications of training llms on unrepresentative and inorganic corpora, particularly on the bot-generated wikipedia articles. reproducibility data collection, implementation of the depth+ metric, and an expanded technical report can be found on github at https://github.com/saiedalshahrani/depthplus."
1512,"limitations presenting significant obstacles for our project, we were still able to create high-quality fake scientific papers. nonetheless, we believe there is room for improvement in addressing such limitations. due to the complexity of parsing pdfs, we are currently limited to specific sections (abstract, introduction,"
1513,limitations and further enhance the utility of our benchmark for the research community. we release a repository containing our benchmark dataset as well as the code used for experimental results8.
1514,"limitations to our knowledge, the current study is the first effort in the ml community aiming to define pi and estimate how much of it is present in two major training corpora, c4 and the pile. however, we recognize that there are ways in which our study can be improved, and directions in which future studies can be conducted. to start with, when annotating the pi found both by using presidio and regular expressions, we observed that new forms of pi have appeared with the advent of the internet, but have yet to be considered in traditional definitions (e.g. facebook events urls), despite their potential for risk. also, given the diversity of types of pi that exist, it is unsurprising that systematically detecting them remains a challenge. as we reported section 4, we found that both presidio and regular expressions were able to detect certain types of pi, such as emails and phone numbers, relatively well, but failed on other types, such as ssns and credit cards; however, without access to ground truth annotations, measuring and characterizing false negatives is impossible. other limitations of both types of approaches is that they are language- and often country-specific, and need to be adapted to contexts of application and languages. this can quickly become complex, because the format of common types of pi such as bank account numbers varies immensely depending on its country of provenance. finally, linguistic characteristics of individual languages make it difficult for multi-lingual pi detection since features that are relevant towards pi detection in some languages are not relevant for others; more work on developing more modular and extensive pi detection tools would be an important contribution to many communities and endeavors, and it is conceivable that ml-based approaches can contribute to these efforts. broader impact statement our work endeavors to help the nlp community better understand and quantify the types and quantity of personal information contained in popular training corpora. in order to strive towards this goal, we manually annotated a subset of the personal information detected in c4, which constitutes a dataset that could be valuable to the community. however, given the quantity of high-risk personal information that this sample contains, we do not feel comfortable disseminating it. we are, however, working on methods for developing synthetic and lower-risk labeled corpora to help develop better methods for detecting pi. as large language model development is increasing dramatically, more models will be trained on these data sources, so its becoming increasingly important to quantify and characterize the personal information present in datasets as well as help practitioners develop better pi detection methods."
1515,"limitations while we endeavor in this work to shed light on the impact of various pseudonymization techniques, we recognize a major limitation of our work – especially the llm-based pseudonymization approach. using closed-source llms may not be an acceptable solution for many settings since it requires sending a (potentially sensitive) text to a third-party api, which, in the absence of appropriate legal safeguards and responsible-use agreements, defeats the purpose of privacy preservation. there are some more technical limitations of the work, such as the following: • while this is a problem that affects sensitive texts in all languages, all the experiments were conducted for data in the english language only. • llms are highly sensitive to prompts, as well as the number and ordering of examples provided for few-shot learning. in this work, we experimented with a limited number of prompts for llm-ps due to api cost constraints. • for the data privacy detection experiment, the flair ner system was trained using the conll-2003 dataset, which might affect its performance for privacy protection tasks. this may also apply to gpt-3 and chatgpt models as the authors do not state specifically on which data they were trained. • we considered only a limited part of named entity types, specifically, person (per), location (loc), and organization (org), whereas it is well understood that pii encom- passes a much broader range of data types (eg. dates, phone numbers, etc.). we also do not consider sentiments associated with named entities used for substitution in the downstream task of text classification. we plan to address these in future work."
1516,"limitations the models used in this work are small, compared to the large language models (llms) used in many language generation tasks today. to attempt to show possible impacts of scale, two different sized models were employed in this work and show similar results, so it is likely that the method proposed here would scale to larger models. training dynamics were not altered between the two model sizes, which is a potential area for improvement. more sophisticated methods of inserting backdoors could also be employed than training one into the model, but this seemed to work well. ethical statement this work attempts to improve the state of watermarking llms in order to demonstrate ownership. our hope is to help improve the space of responsible llm usage by helping model creators assert or demonstrate ownership of their models, although there are probably applications of watermarks that we have not considered that may be detrimental. this work does expose ways to find watermarks, which could be used by a potential adversary who had stolen a model and was attempting to use it illicitly. however, we believe that disclosure of vulnerabilities allows stronger system construction and is preferred over security by obscurity."
1517,"limitation of requiring knowledge of models and training data, we extract simple patterns (e.g., label hints and answer hints) from the min-min optimization to make text unlearnable. although our experiment explores patterns for text classification and question-answering tasks, the pipeline potentially works for any nlp task. reproducibility. to ensure the effectiveness of unlearnable modifications, we slightly tuned the training hyperparameters to achieve well-trained models, such as setting maximum gradient norms and early stopping according to validation sets. we open-source codes with configuration files, which contain hyperparameters regarding model architectures (e.g., the number of layers), batching (e.g., data sampling), and training setups (e,g., learning rate). since these files are configurable in json format, future works can easily reproduce and extend the experiments."
1518,"limitations although we have shown that the overall performance of imbert is superior, we mainly target insertion-based backdoor attacks. however, substitution-based attacks have been recently investigated and proven to be a practical approach in text classification (qi et al., 2021c) and machine translation (wang et al., 2021; xu et al., 2021). it is unknown whether imbert can effectively adapt to these attacks. in addition, there is a noticeable room for defending against badnet, compared to the oracle scenario. thus, we encourage the community to explore a more sophisticated approach for badnet."
1519,"limitations. for authors, the editing of posts would only be possible for output languages that they can read and write in. furthermore, authors’ ability to edit the translations of their posts could result in ethical concerns, as this feature may be used to create language-specific misinformation. a solution to this problem is to present the reader with both the original machine translation and the user-edited version. alternative solutions include allowing users to disallow the automatic translation of posts on a case-by-case basis, or to filter the audience of a post based on its original language. this aligns with the fact that users’ language choices are intentional, and depend heavily on the topic of the post and the target audience [13]: many purposely write posts in a certain language as a means to target those who speak that language. overall, we see a substantial benefit in making sns users aware of how their posts are translated and shared with others, and in allowing them to remove the translation if the post was meant to be targeted to a specific audience or correcting the translation to preserve the original meaning of the post. together, these design solutions would ensure users that their information is conveyed accurately and only to their intended audiences."
1520,"limitations our methodology is currently tested with only english. we conjecture that the methodology should be applicable to other languages, but may be limited by the capacity of llms in those specific languages. it is possible that value-aligned knowledge distillation may be more difficult with languages from countries and regions that do not have a complete set of human value definitions. thus, exploring the value-aligned task in different languages other than english is a promising research direction. our main experimental results are based on a 175b parameter model, which requires large gpu resources or access through an api. this may hinder other researchers from reproducing experimental results. additionally, we explored different sizes of llm including 1b and 6b models, which do not require large gpu resources, and showed they can achieve comparable results. we hope they can be possible alternative options for researchers who may not have access to 100b+ models. although sexism is a suitable case study for us to investigate the feasibility of the value alignment task as we have shown throughout this work, it is still one domain. further expansion to different domains or value-aligned classification tasks such as the detection of racism, toxicity, other than sexism, are needed."
1521,"One of the limitations of our work is the unavailability of context data and unavailability of phrasebased annotations for all languages except Hindi. The unavailability of phrase-based annotations prevents the usage of universal tags because markings that are present on a single word in highly agglutinative languages like Marathi or Tamil get expressed on 2–3 words in isolating or fusional languages like Hindi or Bengali (where markings are present on post-positions). The benefits of using phrase level morphology over token level morphology have been discussed in Goldman and Tsarfaty
(2021). For example, the word ‘sochega’ in Hindi will have MSD tags: future tense and male gender, while in English, it would take two words, ‘he will think’ to express the same amount of morphological information. The presence of contextual data can also help to disambiguate MSD tags. The other limitation of our work is the mismatch between the languages for which pretrained models (especially encoder-decoder models) are available and the languages for which we have the annotated data. For example, UniMorph dataset contains annotated examples for Assamese and Sanskrit, but we do not have multilingual pretrained encoder-decoder models for these languages."
1522,"While this methodology does not require the large amount of text data utilized by more advanced models, it does depend on access to grapheme/phoneme alignment information for all input words. This does limit the usefulness of the model for languages with little linguisticallytagged data available, though the success of the small Deri & Knight corpus does indicate that the model architecture can be made to function effectively with a limited amount of annotated data."
1523,"While the WFST model didn’t perform very well overall, our sense is that it is worth pursuing further. Specifically, there are several moves worth exploring.
First, we should move to a compiled system so that we can test the ""individual variable"" models more thoroughly.
Second, we should try models where we set the variable weights by training, rather than naively in advance.
Third, in an individual variable setting, it would be promising to weight the variables by locality. Specifically, do mismatched variables have more effect when they are closer to where the changes happen? Similarly, we might adjust the granularity of the variables as a function of position, with
single-character variables sometimes and variable spans in other cases.
Fourth, We had individual WFSTs for each lemma, but with a compiled system it makes sense to put them all together into a single WFST."
1524,"Due to time constraints, it was not possible to perform a satisfactory grid search on the large combinations of training hyperparameters, preprocessing techniques, and stem approaches. It is possible that a more optimal system is possible, but we were unable to find it."
1525,"limitations the results of this work may be limited in reliability and replicability due to some hard-to-avoid aspects of the low-resource setting. our numerical results have low statistical power, as illustrated by the wide bleurt confidence intervals in tables 4 and 6. without a large test set, most differences are not statistically significant at the accepted level. they should be treated as trends which can motivate further investigation rather than solid"
1526,"limitations the method proposed in this paper has some limitations: (1) this model learns not only the features of words, word formation, and context but also the relationships between labels. when there is too much noise, it can disturb the relationship between labels. (2) the model needs more training time than other models to learn pretrained label relationships."
1527,"limitations one of the limitations of our dataset is the lower number of wordforms belonging to a closeclass partofspeech as we chiefly focus on nouns, verbs (transitive and intransitive) and ad jectives. on the other hand, we only include in flectional morphology without paradigms of word formation. furthermore, we only address the mor phology of the standard variety of central kurdish, i.e. that of sulaymaniyah. we plan to extend our work to include other varieties of central kurdish along with derivational morphology. given that central kurdish lacks a treebank, it will be com pelling to bridge central kurdish morphology and syntax as well. another limitation of the current work is due to the unimorph schema. using the lgspec tag is not recommended for features that are found across languages but for those that are limited to specific languages (sylakglassman, 2016, p.30). given that some of the features of central kurdish, such as izafe and pronominal copula, are also found in other closelyrelated languages, we believe that the current schema should be extended to use specific tags for such features or a better schema, akin to guriel et al. (2022)’s hierarchical model, is needed for languages with rich morphology like kurdish."
1528,"limitations the speech recognition used was the wav2vec 2.0 model. some of the errors may have been influenced by the fine tuning of the final layers; this could lead to errors being corrected by the language model. furthermore, wav2vec 2.0 produces character output which we transformed to phonemes using a grapheme-to-phoneme tool; this will lead to some loss in the variation. these limitations can be overcome to some extent by using a wav2vec 2.0 phoneme model which we plan for our next experiments. we have only worked on french to date, even though we believe that the method is applicable to other languages. finally, the experiments were done only on timit. while this is a balanced dataset, use of other datasets will likely lead to better insights."
1529,"limitations and ethical considerations. first, the system’s performance is constrained by the use of automated systems to produce pseudo-parallel data. errors in translation and morpheme labeling on the high-resource side propagate to the output and cause mistakes in target side labeling. we have not yet performed the extensive error analyses needed to understand how much error propagation might be affecting the system. second, we have not yet tested the system in an actual documentation project. when working on nlp with endangered and/or indigenous languages in mind, there is a clear risk of perpetuating existing oppression (bird, 2020; schwartz, 2022). we hope to avoid some of these harms by using data from a wide range of non-threatened languages first, waiting to involve language community members and documentary linguists until we have a system with good enough results that we expect it could actually be helpful in real world contexts. we have already developed collaborations with several speakers of endangered languages and linguists working on documentation projects, and we look forward to continuing this work with their guidance and involvement."
1530,"limitations a limitation of this study is the fact that the concreteness ratings of brysbaert et al. (2014) are curated solely from self-identified u.s. residents. and the affectiveness ratings of warriner et al. (2013) are solely curated in english. as such, there is a risk of an anglocentric bias in the created dataset. nonetheless, the goal of this study is to explore the potential of leveraging colexifications to bootstrap cross-lingual datasets in as many languages as possible, including a lot of low-resource languages."
1531,"limitations a major limitation of the current work is the absence of gold alignments for evaluating the different methods. gold alignments would also enable us to provide more reliable estimates of the prevalence of the evaluated phenomena in the three datasets. we are not aware of any other similar corpora that come with gold character alignments. the work of wieling et al. (2009) uses word lists, not entire sentences. furthermore, our work currently only covers european languages in latin script. some of the presented techniques also assume identical writing systems in the transcribed and normalized layers. our setup may therefore not generalize well to the dialectal variation and writing systems present in other parts of the world. for example, the v-c proportion cannot be easily determined in scripts that do not specify all vowels. although there is an extensive amount of research in particular on arabic and japanese dialects and their normalization (e.g., abe et al., 2018; eryani et al., 2020), we currently limit our experiments to data written in latin script."
1532,"limitations the small amount of training data provided in this shared task poses a challenge for models that need large amounts of data to reliably learn linguistic patterns. while we did not employ any data augmentation techniques, we suggest future work to train the models on all the possible feature combinations (weighted with the probabilities as the experimental data) for the stems in the two corpora. owning to the lack of time and computing resources, we did not fully optimize our transformer models and we did not fully utilize and explore i) speaker-specific information, especially for the transformer models, ii) token frequency information in the corpora, as we assumed extension of morphological patterns is based on type, not token, frequency (bybee, 2001; pierrehumbert, 2001). furthermore, we did not experiment with training models with either high-frequency, low-frequency and pseudoword items. it is possible that some speakers’ high/low/pseudoword items would be better served as part of the training set. the ldl model in section 5.1.3 was not able to evaluate the experimental items due to the unattested triphones. this shortcoming can be mitigated by using phonological features (tang and baer-henney, 2023)."
1533,"The presented results only apply to the English language. Both our benchmark dataset and the baseline model target the English language exclusively. Special text sources such as instant messaging or speech-to-text are likely under-represented in our benchmark test set; therefore, we did not evaluate classification performance in those domains. Since we used RoBERTa as the base model, our model inherits the same limitations. Specifically, the length of input sequences is limited to 512 BPE tokens, and additional pre- and post-processing is necessary to run predictions on longer inputs. However, we did not evaluate prediction aggregation methods or classification performance."
1534,"Our work can be considered to have the following limitations:
1. The dataset we introduce contains 10, 000 text instances sampled from a single social media platform. However, we acknowledge this limitation and as noted in section 6, we aim to extend this work by collecting more political data across various social media platforms and using it to model aggressive behavior.
2. We obtained this dataset by crawling for tweets based on 52 keywords (as shown in Table 10). We acknowledge that these keywords may have limited the domains in which political aggression can occur. That being said, we also hope that task generalizability is not compromised due to the presence of pretrained language models at the helm of our experiments."
1535,"Although SCL-Fish achieves improvement over Fish, training SCL-Fish takes longer time than Fish. Empirically, we find that SCL-Fish is approximately 1.2x slower than Fish. Moreover, we believe that the subjective nature of abusive language (Sap et al., 2019) affects the annotation process of different datasets and possibly negatively impact performance."
1536,"In this exploratory study, we focused on Englishlanguage resources. Further, we examined only one social media platform, Twitter. As any other platform, Twitter has a biased demographic representation of users in terms of language, location, ethnicity, gender, age, socio-economic status, and other characteristics. In particular, Twitter is predominantly used in the United States.10 As a result, user attitudes examined in this study primarily represent Western views and may differ significantly from views common in other regions of the world. Future studies on aporophobia need to include other languages and world regions and consider cultural differences while measuring and mitigating this type of social bias.
When searching for aporophobia-related texts, we excluded derogatory terms and slurs associated with the group ‘poor’ as such explicit forms of online abuse tend to be easier to detect by human
10https://www.statista.com/statistics/242606/n umber-of-active-twitter-users-in-selected-count ries/
annotators and NLP models. Nevertheless, when designing tools for measuring and mitigating aporophobia such explicit expressions need to be taken into account. Furthermore, there is a wide variety of linguistic expressions referring to poor and homeless people, and sometimes this target group is not even mentioned at all, but could be inferred from the context (e.g., contexts referring to hunger, food stamps and/or other benefits, ghettos, etc.). To effectively confront aporophobia, NLP resources (lexicons, datasets, classification models) need to have a wide coverage of explicit and implicit linguistic expressions of the phenomenon.
Finally, we targeted only textual data. However, many social media posts combine text with other types of data, such as images and videos. Recent techniques for modeling multi-modal data can be employed to ensure a better coverage of various types of social media posts.
Ethics Statement
Confronting aporophobia, as an application similar to addressing other types of abusive and toxic language, poses a number of risks and ethical issues, including tension between freedom of speech and respect for equality and dignity, biased data sampling and data annotation, dual use, and many others, discussed in previous works by Hovy and Spruit (2016); Vidgen et al. (2019); Leins et al. (2020); Vidgen and Derczynski (2020); Cortiz and Zubiaga (2020); Kiritchenko et al. (2021); Salminen et al. (2021). Future research on this topic should comply with trustworthy AI principles of transparency, justice and fairness, non-maleficence, responsibility, and privacy (Jobin et al., 2019). Special attention should be paid to involving all legitimate stakeholders in the identification and definition of actions to counteract aporophobia, including the affected communities, non-governmental organizations (NGOs) and government officials working on poverty mitigation. In particular, the views and needs of the communities from both the Global North and the Global South should be included."
1537,"The dataset created as part of our contribution leverages hate speech datasets focusing on the English language. Therefore, the model has neither seen, nor been evaluated in other languages."
1538,"Our work has limitations. First, we use the TCAV framework, which assumes that concepts are encoded in the linear space of semantic representations. However, recent works show that in some cases, linear discriminants are not enough to define the semantic representations of concepts in the embedding spaces (Koh et al., 2020). Future work should consider nonlinear discriminants to accurately represent concepts in the hidden layers of NLP neural networks.
In this study, we used simple challenge sets to obtain a baseline for assessing the effectiveness of concept-based explanations in measuring false global sufficiency. Future work should focus on curating challenge sets by annotating user-generated data for the label and the concepts, in order to achieve a stronger baseline.
Our work is limited to pre-defined concepts and requires human input to define the concepts with examples. However, defining concepts in TCAV is less restrictive than pre-defining features in other explainability methods, in that concepts are abstract ideas that can be defined without requiring in-depth knowledge of the model’s inner workings or the specific features it is using. This allows for a more flexible approach where users can test the model regarding their concept of interest.
Our method can only be applied to concepts that are known to be important for the classifier and are prone to being over-represented in training sets. It’s important to check this condition independently before using our metrics. In cases where this condition does not hold true, the metrics we use in our work may be interpreted differently and may not be reliable indicators of global sufficiency. Also, we only considered two variations of emotion-related concepts. Other variations such as expression of negative emotions by the writer of the post should be investigated in future work.
Further, our metrics are limited to cases where different classifiers are being compared since the most important information is in the relative value of the metrics. Our metrics should not be used as absolute scores for testing a classifier.
Testing a classifier for false causal relationships is most valuable for detecting the potential flaws of the models. If our metrics do not reveal a false relationship between the concept and the label, that should not be interpreted as an indicator of a flawless model.
Ethical Statement
As with most AI technology, this approach can be used adversely to exploit the system’s vulnerabilities and produce toxic texts that would be undetectable by the studied classifier. Specifically, for methods that require access to the model’s inner layers, care should be taken so that only trusted parties could gain such access. The obtained knowledge should only be used for model transparency purposes, and the security concerns should be adequately addressed.
Regarding environmental concerns, contemporary NLP systems based on pre-trained large language models, such as RoBERTa, require significant computational resources to train and fine-tune. Larger training datasets, used for fine-tuning, usually result in better classification performance but also an even higher computational cost. To lower the cost of this study and its negative impact on the environment, we chose to use existing, publicly available classification models."
1539,"limitations our approaches largely focus on explicit mentions of identity terms. this does not capture whether the identity term is the target of hate speech, which would require further analysis. this approach also does not capture attitudes held toward high-profile members of those groups, which play a role in circulating associations with identities (such as personal attacks on women in gaming or the use of “george soros” as shorthand for antisemitic conspiracy theories). future work may try to capture and measure these attitudes. incels.is is a large, popular forum for blackpilled incel discourse, which has a unique and extreme ideology that we argue is under-represented in current hate speech datasets. however, our analysis is limited to this forum, and the trends we identify may not apply to more moderate incel discourse (e.g., r/incelswithouthate) or related online male supremacist movements, such as mgtow and puas. though these communities are known to have related, but distinct jargon (farrell et al., 2020), we emphasize that researchers should recognize these lexical innovations in their annotations for hate speech and include a variety of these communities in training datasets for misogyny."
1540,"limitations of such a study - which we discuss in the next section. we release any data (including any raw data, but only in the form of tweet ids and their respective labels for the two tasks), code, and models produced during this study publicly for further research by the community. we license this release under cc-by-sa 4.0. in the near future, we aim to annotate this data for tasks such as sarcasm detection - to develop a deeper understanding of how it is related to aggression and offensiveness. additionally, the motivation for collecting the same data instances marked with aggression and offense labels is for a multitask learning-based model also to be able to identify when 1) the tone of a text is aggressive without being offensive vs., 2) the text is offensive, despite it not being overtly aggressive. we also aim to collect more data and annotate it using weak supervision. finally, we also aim to expand on the theoretical underpinnings of sublime aggression and offense by attempting to identify these within other more tangential domains, viz., comedy."
1541,"limitations this study utilizes a monolingual pre-trained language model (plm) in the english language (bert-base-uncased). although the weaklysupervised classification methods are not limited to a particular language, we have not explored applying the method to another language. social media language use may differ significantly from the data used to train the plm. moreover, the presence of code-switching (doğruöz et al., 2021) may also degrade a monolingual plm’s performance. we explored a roberta checkpoint continually trained with 60m english tweets (barbieri et al., 2020).16 however, it does not yield better performance than bert. we have not investigated whether it is due to the training regime or the dataset. moreover, in this work, we focus on classifying hate speech (hs) categories/target groups instead of hs detection (detecting whether a post contains hate speech or not). to perform hate detection and classification, we can either combine our method with another hs detection model in a pipeline or use an adaptation of weakly-supervised text classification incorporating the “others” category such as li et al. (2018) or li et al. (2021). due to limited space, we prioritized in-depth analysis instead of a comprehensive evaluation. therefore, we selected only two datasets (and two- 16https://huggingface.co/cardiffnlp/ twitter-roberta-base way cross-dataset classification). we are working in parallel on extending this work to a longer-form journal article to cover more datasets and experimental results. recent work on large language models (llms) demonstrated that when the parameters scale to a certain level, language models exhibit a drastically-increased performance in zero-shot classification (zhao et al., 2023). we reported the performance of a moderately-sized bert-large-uncased zero-shot model because of limited computational resources and lack of access to commercial apis. larger language models will likely perform much better than this baseline. lastly, understanding hs sometimes requires cultural understanding or background knowledge. it may be difficult to determine the presence and category of hs when we take the post out of its context. for example, many “sexist” posts in waseem dataset are tweets related to the australian tv show my kitchen rules (mkr), and below is a tweet labeled as “sexist”: everyone else, despite our commentary, has fought hard too. it’s not just you, kat. #mkr"
1542,"limitations while promising, our work presents limitations that need to be acknowledged. firstly, we did not explore the best verbalizers for instruction finetuned language models, which could have further enhanced the performance of the models explored in this study, due to computational cost and the specific goals of the research. secondly, we selected benchmark datasets based on their popularity and diversity, which might not be representative of all possible datasets in hate speech detection. we also acknowledge that, in addition to the languages examined in this paper, there are a number of other languages that may present unique challenges and characteristics for detecting hate speech. our decision as to which languages to include in the multilingual experiment was based on a direct comparison with state-of-the-art research. finally, we utilized the latest open-source language models for our experiments, but we did not explore other recent language models, such as the gpt family, primarily because they are not open and reasonably reproducible3, and therefore the community may encounter challenges in replicating our results. these limitations provide directions for future research to improve and expand upon our work."
1543,"limitations of the use of dalc-v2.0 to detect hate speech. while its abusive dimension can be considered a good proxy, all fine-tuned models systematically fails on core non-hateful functional tests, indicating limitations in the annotated data. future work will focus on extending dalcv2.0 with multiple hate speech datasets and further 2these correspond to f18–19, f21–22 in mhc. validate the functionalities of hatechek-nl. ethical statement limitations hatecheck-nl is based on mhc and it inherits its limits. however, as we have discussed in section 2, we failed to fully implement some functional tests (e.g., reappropriation of slurs) because we were not able to find evidence during our research. to address these limitations, we plan to conduct focused interviews with dutch organizations such as the black archives3. intended use hatechek-nlis a diagnostic tool for hate speech against specific protected groups. we have shown its functionalities and its impact on the evaluation of models trained both on a different language phenomenon, e.g., offensive language, and on related and comparable one, e.g., abusive language. the results have shown critical weaknesses mainly on the non-hateful tests rather than showing the strengths of the systems/models on the hateful examples. similarly, op-nl is a dynamic test for offensive language whose use is to help assessing the robustness and portability of models trained for offensive language detection. goodness of data dalc-v2.0 is the only publicly available resource for investigating the behavior of models on offensive and abusive language phenomena in dutch. none of the annotated dimensions in dalc-v2.0 explicitly address hate speech as we discussed in section 2. the results of the fine-tuned models on hatechek-nl for the abusive language dimension indicate a compatibility between abusive language in dalc-v2.0 and hate speech. the use of offensive training data on hatechek-nl better highlights the limitations of the data, especially as pointed out by the systematic failure on the functions f23–24. at the same time, the results on op-nl for offensive language show a relatively good portability of the models for this language phenomenon."
1544,"limitations graph models require four or more utterances to form meaningful conversation connections and model their dynamics. in some cases, conversations that derail are not sufficiently long and may be best modeled by simpler sequential models. any of these models will work best with asynchronous conversations where there is a time lag between the turns to allow for moderation after forecasting."
1545,"limitations in the available resources and the approaches of nlp researchers towards constructing them. we summarise these and make future recommendations. conceptualisation with a couple of exceptions (e.g. samory et al., 2021; strathern and pfeffer, 2022), the phenomena targeted in the reviewed resources are not clearly defined or strongly rooted in theory or expertise from outside computer science. similar observations have been made for operationalisation of related concepts, such as bias and stereotypes (blodgett et al., 2021), and value alignment (irving and askell, 2019). recommendation: resource creators should collaborate with social scientists to ground them in expert knowledge of the target phenomena. we advocate for the use of gbv as a framework, which encompasses several facets currently operationalised in different ways by computer science researchers. it recognises how all forms of online abuse affect people of every gender both online and off, and has been widely adopted by policymakers. stakeholder participation parker and ruths (2023) propose that computer scientists should: stop thinking about online hate speech as something requiring methods, and start thinking about it as something that demands solutions. this change — treating hate speech less like a task and more like the real-world problem it is — would orient cs research towards the concerns of other stakeholders, and thus begin the collaborative pursuit toward a safe internet. however, we find little evidence of such a paradigm shift having occurred when it comes to designing these resources, with stakeholder participation limited to the recruitment of loosely defined ‘expert’ annotators—where it occurs at all. recommendations: resource development projects should, as far as possible, strive to include stakeholders from the outset by including representatives in research teams. stakeholder participation should be integrated throughout development, and is especially important in the design of taxonomies, guidelines, and at annotation, when judgements about what constitutes gbv are made. due to the risks involved, annotator welfare should be prioritised by following guidelines such as those of kirk et al. (2022), and irb approval sought before any data collection. in documenting resources, authors should provide full data statements or similar (e.g. bender and friedman, 2018; díaz et al., 2022), and, to preserve minority voices, dataset releases should includenon-aggregated labels (prabhakaran et al., 2021). data collection media data for these resources is not sourced from diverse sources, with the majority from twitter, the choice of which does not appear to be driven by stakeholders. furthermore, as the datasets are static in nature, their relevance as reference sources for automated classification decays over time; and, due to data sampling methods, positively labelled (i.e. abusive) examples are skewed towards the more explicit forms of online gbv. recommendation: there is a great need for the development of new methods to surface the diversity of gbv found online. one solution is to create platforms to which victims of abuse and bystanders can submit examples. this could facilitate creation of improved resources on many of the limiting dimensions we outline in this review: dynamic datasets to which new examples are regularly added; stakeholder participation in data and platform selection and labelling; and inclusion of implicit and subtle examples of gbv, as well as multimedia data. limitations and ethical considerations we use a systematic review methodology in order to provide a reproducible and objective snapshot of the current research situation. however, we acknowledge that the choices made (such as search repositories and eligibility criteria) may not have captured every existing relevant resource. we aim to regularly update the repository of gbv resources at https://github.com/ hwu-nlp/gbv-resources and open it to submissions via push requests in order to provide a dynamic and comprehensive record. following d’ignazio and klein (2020), we acknowledge that this research is influenced by the positionalities of its authors. to situate our perspective, we are four computer science and one social science academic researchers working in public institutions in europe. three of us identify as women and two as men, and we are of european and asian nationalities. this work forms part of a project conducted in partnership with charitable organisations that work on combating gbv and supporting its victims. in this paper, we make a number of recommendations that complicate typical nlp resource creation workflows, and could have the unintended consequence of dissuading researchers from working on these problems. however, we appreciate that interdisciplinary work is difficult to instigate, organise, and carry out, and that it is not usually motivated by typical academic or industry reward structures. our intention is to point out practical ways in which resource development can be improved and to encourage researchers to move towards more participatory solutions."
1546,"One of the limitations of our work is that while we are running the MathPrompter multiple times in different ways to increase the accuracy of our results, this does not always guarantee the correctness of the output. Both Algebraic and Pythonic expressions have the potential to produce the incorrect results, even if the prompt outputs match each other. This is the fail case as shown in the last row of Table 2. Increasing the number of prompts will mitigate this issue. We are currently investigating techniques that can address this issue in a more principled manner."
1547,"There are two main limitations to this study. First, because of the lack of downstream datasets, we did not evaluate KG-FLIP on other downstream VL tasks in e-commerce (e.g., substitute recommendation). Therefore, the robustness of the KG-FLIP model on other downstream tasks requires further investigation. Second, the experimental results empirically show that the proposed knowledge-guided pre-training objectives are more effective in producing VL representations that capture subtle distinctions between samples than the standard objectives. However, a theoretical analysis of the effectiveness of our knowledge-guidance strategies is lacking.
†https://aws.amazon.com/batch/ ‡https://aws.amazon.com/sagemaker/"
1548,"While the xPQA dataset is created to be as close to the real-world scenario as possible, it has two major drawbacks. Firstly, the candidate set in the dataset does not include the full candidates for a given product because annotating all candidates is prohibitively expensive. The subjectivity of product questions and candidates also makes it hard to get ground-truth short-span answers, which prevents a straightforward end-to-end evaluation over the full candidate set. A potential fix is to run human evaluations on the top-1 candidate over the full candidate set from each model, but it’d be costly to do so. A more realistic solution is to have an online evaluation for the best model only, which we leave for future work. Secondly, the answer annotation is based only on a single candidate because handling information from multiple candidates requires careful instructions on conflicting information and summarization skills. This might limit the model in answering complex questions that require inference over multiple candidates. However, we find this case to be very rare in real customer questions. Furthermore, as we do not summarize multiple candidates, the returned answer can be biased toward the opinion of a single customer. Our evaluation also has potential limitations in that (1) We did not extensively evaluate the quality of generated answers with manual annotation. It is known that BLEU scores might not correlate well with human evaluations on generation tasks, and they can be misleading in certain cases; (2) We only compared major types of baseline algorithms and did not explore the effects of leveraging existing larger, more powerful pre-trained language models such as mT0 (Muennighoff et al., 2022) and FlanT5 (Chung et al., 2022). Conclusions might change if we hire annotators to perform more human evaluations or change the model architecture."
1549,"Despite the promising results achieved by our approach, some limitations must be acknowledged. First, the use of product graphs as a knowledge source is a double-edged sword. Indeed, while it provides a valuable resource to exploit, the constant evolution of product graphs may create a strong coupling between the algorithm and the knowledge source, thus reducing the method’s robustness over time. Second, our method’s span-based approach makes it computationally expensive, requiring setting a maximum span size to circumvent this issue"
1550,"The proposed KOSBI addresses social bias based on Korean culture with the Korean language. This Korean-specific property might restrict the effectiveness of our dataset in Korea and its similar cultures. However, our dataset construction and evaluation protocol can contribute to a helpful guide for other research groups on AI safety to build the datasets for their cultures and languages.
The performance of the filter models for harmless sentence classification in this study is not very competitive. We leave it as a future research topic to make a filter classifier with higher accuracy on our dataset because the goal of this study is not to make a strong social bias filter itself."
1551,"An important limitation of our contribution lies in breadth of the experimental validation. We decided to focus our experimentation on the setup where we encountered the issue of redundant datasets: NER for a large-scale conversational assistant. While it is true that our approach does not make any assumption neither about the task nor about the model architecture, and while we also provide a rigorous proof to support the estimated gain in terms of training time, the extent to which our approach remains the best time-accuracy trade-off when other tasks are considered is not explored in this work.
An additional limitation stems from the lack of absolute results on the internal datasets, as the latter can only be disclosed as relative improvements over a baseline due to internal policy. We attempt to mitigate this by reporting full results on an open dataset, but as we mention we have to resort to upsampling given the artificial deduplication that manually curated datasets incur before publishing."
1552,"In this section, we discuss some of the limitations of our current model architecture: (1) NonEnglish locales: Currently in our experiments, we have trained and evaluated models only on English datasets. Building models on non-English locales is the direction for future work, (2) Use of pretrained tokenizer: The T5’s tokenizer in our models has been pre-trained on open-domain datasets, and its vocabulary misses out on e-commerce specific terms. For example, the current tokenizer of T5 tokenizes the phrase “skater midi dress” as [“sk”, “a”, “ter”, “mid”, “I”, “dress”]. Here, the meaning of words “skater” and “midi” is not captured in the tokenized text. We believe that we can overcome this limitation by pre-training T5 on e-commerce data which would help tokenizer understanding and tokenizing the e-commerce specific terms more correctly."
1553,"This study is based on a single clinical cohort consisted of 1479 patients, which may limit the generalizability of the results to other clinical cohorts. This specific cohort of patients may not be representative enough of the general population, which may inject certain level of bias brought by the dissimilar distributions of gender, age, race, etc. While we envision the generalization capability of the language models is applicable to other clinical prediction tasks, the focus of this work is majorly about prognostic prediction of cancer immunotherapy, and we hereby have not provided solid evidence to prove that the success can also be extended to other relevant trials. Additionally, we have yet only compared a limited set of transformers and language models, and it is possible that other models may perform better on the tasks evaluated in this study. Finally, it is important to note that while the models in this study achieve high accuracy in clinical prediction, the ultimate value of these models in improving patient outcomes will depend on how well they are integrated into clinical decision-making processes and the impact they have on patient care."
1554,"The performance of the EvolveMT system is contingent upon the reliability of the COMET-QE model in providing accurate labels for the MT requests. Utilizing the encoder’s embeddings as features necessitates that the COMET-QE model performs effectively on blind MT requests. The batch reranking of MT requests after each learning step may result in a computational bottleneck if the queue size is substantial. To mitigate this issue, an asynchronous re-ranking process could be implemented, whereby the queue is only reorganized once the re-ranking is completed. Additionally, before the re-ranking process, a diverse subset of the queue can be selected based on the XLMRoBERTa embeddings, which reflect the novelty of the requests relative to previously processed MT requests. The source embeddings from the XLMRoBERTa model can be cached in parallel during the batch feature extraction process utilizing GPU capabilities, thus facilitating efficient COMET-QE inference. EvolveMT could also be optimized for cost-effectiveness by incorporating the cost of each MT in the ensemble into the algorithm."
1555,"A major limitation of this research work is only item classification, one specific type of NLU tasks, is used in our experiments. To better evaluate our proposed KCL-FNC method, an expanded testing task set will provide more convincing power. In addition, we only used cross-entropy (CE) loss when training models, in both representation and classifier learning stages. It will be interesting to see the compound effect when applying our proposed method together with some advanced loss types, such as LDAM (Cao et al., 2019)."
1556,"We acknowledge that there are certain limitations of this framework. First, generation-based models have latency issue due to the autoregressive generation. Thus, we will explore Non-autoregressive and semi-autoregressive methods in a future study. Second, the knowledge is only stored in model parameters which limits the capacity of the model to make the smarter trigger decision through factchecking and generate a valid rewrite. To this end, we intend to consider a retrieval-augmented generation to incorporate external knowledge to improve performance as well as incorporating more contextual (e.g. if the user is listening music) and personalized (e.g. user preference) signals into the model. Moreover, generative models can also pose quality control challenges, such as hallucinations. To mitigate this issue, we will add constrained decoding (Hao et al., 2022) to control hallucinations."
1557,"There are a few limitations pertaining to the training data we used. Some of them are listed below.
1. RadLing has been trained on English reports only, and therefore will not work out of the box in a multilingual setting.
2. There is data imbalance with respect to imaging modalities and anatomies covered by our training data. For example, regions like extremities, neck, spine and shoulder are underrepresented in the dataset, and expected understanding of observations related to those regions may be limited.
3. There needs to be a study on the diversity of the patients and radiologist expertise represented in the data, and how it impacts the performance of the model for underrepresented communities.
4. Different radiologists (and radiology departments) have different preferences and styles of writing reports. In addition, clinical referrals sometimes dictate to what extent some details are documented the report e.g. the Clinical statement. There was no study on the consistency, uncertainty or information richness of the report.
Asides from the training data, there may be space and time throughputs of the model which could make them unsuitable for at-the-edge applications with limited bandwidth."
1558,"• We use only the transcript as input to the model. This implies the model wouldn’t know that a hold was long unless the customer said “that was a long hold” or something to that effect. The transcript usually contains language indicating the hold is taking place “may i place you on hold?”, “thanks for holding”, etc, but rarely indicates the exact duration of the hold. Similarly, the model doesn’t know the wait time unless the customer complains explicitly about it.
5-w ay
Cl ass
ific ati
on
Bin ary
Ha rd
Lab els
Bin ary
So ft L
ab els
0.64
0.66
0.68
0.70
0.72
0.74
0.76
Cl as
s 1 P
re cis
io n
by using a small model as an initial gating function.
• If a call center doesn’t collect CSAT surveys through our company, their accuracy will be impacted as they won’t be reflected in the training or test set. We ensure customers understand this by training our agents to explain it and including it in help center documentation."
1559,"This paper introduces two heuristic approaches to filter noisy feedback data. Although we showed that these simple methods improve the performance of QA models, they have various limitations and they represent only an initial step for future re-
search on real feedback data. The core of the relevance filtering is based on the assumption that correct feedback occur when the model and the user agree on the labels. This approach may introduce a selection bias towards tuples associated with ""simpler"" q/a pairs, which are already well understood by the model and thus potentially ineffective for training. Although the model can easily discard q/a pairs whose feedback are clearly different, the risk is that uncertain pairs close to the classification boundary (i.e., model score close to 0) are penalized and easily filtered as they will receive a reliability score close to 0.
Regarding the collaborative approach, the main limitation concerns the clustering strategy adopted to aggregate questions. On one hand, we want to reduce as much as possible the number of clusters such that we have a sufficiently high amount of feedback per cluster. This makes the proximity computation between users and the voted feedback vector robust.
On the other hand, the clustering may introduce additional noise by aggregating different and nonequivalent questions into the same cluster. This aspect may reduce the reliability of the voted feedback vector.
Finally, as mentioned in the previous sections, feedback data and q/a pairs used in this work come from real users traffic. For this reason, we only described the high-level approach of integrating feedback and we showed the impact on public benchmarks. A harsh limitation is caused by the private nature of the customer data, which cannot be released for public research."
1560,"Our work explores a dimension of context understanding by Voice Assistants but it is only a small step. Firstly, we only consider 5 categories, while screens have a myriad of other texts and visual
content. We do not include image context into our reference understanding models. But users could use them when formulating references to texts near them. Using image captions or some pixels would improve coverage. Our system leverages entities extracted by upstream and hence is bounded by the performance of that. Also our model evaluates each entity separately while there may be benefit in considering the entire screen holistically."
1561,"Our prescription digitization approach has a few limitations but is still effective for a broad enough application domain and permits future enhancements that address these limitations. First, our system uses an off-the-shelf text extraction tool (AWS Textract) that provides accurate extractions on printed prescriptions but has variable performance on hand written data depending on the legibility of the handwriting. In future, we plan to build a specialized extraction model trained to recognise medical practitioner’s handwriting to replace AWS TextExtract. Further, multiple components in our approach (e.g., attribute extraction) have been trained on primarily English transcriptions. Extension to other language prescriptions requires access to medical vocabulary and training data in those languages. Note that AWS Textract supports multiple languages and can be readily paired with an automated translator to convert the content to English. We did not consider this option since multilingual prescriptions in India tend to have mixed content with medications written in English itself. Lastly, the performance of multiple tasks such as advice block detection, medication attribute extraction and matching-canonicalization depends on the coverage of the available medical catalog."
1562,"limitations this study has potential limitations. when the cwseg model is applied to a new domain, we assume that words and phrases solely related to the domain are available."
1563,"limitations while we conducted extensive experiments and demonstrated the effectiveness of the suggested approach for controlled bandit learning in the context of the skill routing problem, there are multiple directions of improvement for future studies. we believe one of the limitations of the suggested constrained optimization framework is that it relies on expert-defined conditions on an arbitrary segmentation of samples. it entails the need for human intervention and manual constraint definition/optimization which can be challenging. another limitation we faced was during our experiments which showed additional compute overhead of between 2 to 3 times for different constrained optimization methods due to additional optimization objectives, inner loops, and backward passes."
1564,"limitations we identify the following limitations in our work: first, our ftt framework captures and models topic-level temporal patterns for forecasting temporal trends. though the forecasts bring better temporal generalizability, ftt could hardly forecast the emergence of events in new topics. second, ftt considers temporal patterns based on the topic-wise frequency sequences to identify patterns such as decrease, periodicity, and approximate stationery. there might be diverse patterns that could not be reflected by frequency sequences. third, limited by the scarcity of the dataset that satisfies our evaluation requirements (consecutive time periods with a consistent data collection criterion), we only performed the experiments on a chinese text-only dataset. our method should be further examined on datasets of other languages and multi-modal ones."
1565,"limitations our proposed approach is designed explicitly for evaluation of task-oriented dialog systems, and is hence unlikely to generalize well to chitchat systems. most traffic to our platform (and our annotation workflows, including dqa) comes in the form of task-oriented interactions. user turns in the traffic we analyze tend to be quite short (usually less than 20 tokens) and direct, so our model is unlikely to perform as well on dialogs driven by long-form user utterances. ethical considerations we do not envision any ethical concerns with the research presented here. no customer data is released or presented in this paper, and even our internal data sources are fully de-identified and contain no customer personal identifiable information (pii)."
1566,"limitations in this study, we collected feedback on the usefulness of model responses from customer service agents at ar. these agents were recommended based on their availability and experience with conversation assist; however, we did not receive details about the agents such as their level of training or experience, which may have an impact on their preferences using the suggested responses. furthermore, while agents in our study received a flat rate per judgment with no bonus or penalties to how they judged the response, some businesses have existing agent metrics (e.g. actual handle time, aht targets, etc.) that could incentivize the agents to behave differently while performing their jobs. these metrics have the potential to exert pressure on agents in real-life situations to accept responses at a higher rate than in this study. the linear models in section 4.4.2 are based on the judgments of 5 agents on 3 lmm model outputs for 287 conversations. while they have shown a statistically significant relationship between usage rates and perplexity, this is a small pilot analysis. additional data will be necessary to determine how well this generalizes. our cost savings framework also makes a number of simplifying assumptions about workforce optimization. we’ve noted some of these assumptions in section 3.1, and they should be considered when leveraging this framework for different types of products. in addition, while the explicit goal of these models is to make agents’ jobs easier, we expect from previous work studying vigilance tasks (warm et al., 2008) that there can be an upper bound to how much cost could be saved with an excellent llm, as there would be less benefit from the agent acting as a human in the loop as their vigilance wanes."
1567,"limitations the two proposed models (ad fofe and aa fofe mixture) have been tested on more languages than the ones mentioned in this paper, but the comparison with transformer models has not been done for every language. this paper only uses word-level lms. we have done preliminary experiments with subword-level lms but more extensive investigation is needed to draw proper"
1568,"limitations and future work in our study, we focused on a high-resource setting with access to approximately 22.5k hours of labeled speech data. while we compared our models with conformer and transformer-based aed and ctc models, we did not include rnnt models due to their higher compute resource requirements. to accommodate deployment constraints, we employed a smaller model with approximately 60 million parameters, which limited its performance. moving forward, our future work aims to explore the potential benefits of leveraging large unsupervised datasets and larger models to further enhance our system and extend its applicability to other indian languages, which typically have less available data compared to hinglish. building upon our previous success in adapting a non-streaming model for end-to-end speech-to-intent detection in customer support voicebots (goyal et al., 2022), we are motivated to investigate the feasibility of developing a single joint model for automatic speech recognition (asr), end-of-speech (eos) detection, and spoken language understanding (slu). additionally, we are keen on exploring the development of multilingual asr models."
1569,"limitations to ensure high quality extractions for plate, we optimize our annotation process for precision. for example, for the product link attribute, we generally annotate only one product link per product. in an application scenario, the user would not need multiple links to a purchase page, but this could potentially harm the precision of the evaluated models. in addition, we assume that all attributes are text-based. this has the potential of missing additional product information which could be helpful to users, such as images of the product. in future work, we would like to extend plate by incorporating other modalities."
1570,"limitations our work has several limitations. first, our consistency study focuses on our used categorization model and was conducted on only one specific dataset. it might not perfectly generalize to other problems. second, the proposed solutions are based solely on data augmentation without changing the current production settings and model. other approaches such as changing the model’s objective function to take consistency into account might also benefit the solution. lastly, in terms of user perspective, while our solution show significant improvement over the baseline, inconsistencies are still visible."
1571,"limitations as mentioned previously, the main limitation of our approach is that, very complex joins e.g. sequences of joins of different types and joining on columns which have different names in different tables is not straightforward in our approach. one extension to possibly handle this would be using a decoder to generate the complex sequence of joins and column relations. note, however that this does not complete revert to the constrained sequence-tosequence decoding as in semantic parsing, as its not for the entire query but only the table joins or the from section of a sql statement. the select, where, order by and group by can still be done via our approach and we could also continue to use mtl. the second limitation of our approach is subquerying capability which currently we do not have a strategy to handle queries which would require them. however, this is notoriously hard even for existing sota semantic parsing algorithms e.g. the current leader on the spider dataset graphix-t5-3b li et al. (2023) achieves only 50 exact match (em) accuracy on the extra hard spider data subset and 61.5 on the hard subset. overall this model has a"
1572,"limitations of this study. first, we focus on left-to-right code generation without considering right-side and cross-file context, which can be used to determine broader categories of errors with improved precision. second, each static analysis tool has its own limitations. thus, the presented analysis is limited by pyflakes’s accuracy and coverage to detect certain code issues."
1573,"limitations we believe a potential limitation of this work is its reliance of curated samples from historical incidents. due to the complexity of real-world conversational agents, the decision to introduce a new sample to the r/p set requires human expert involvement which could be costly and pose challenges in terms of reliability. another challenge we faced after the deployment of this framework was managing the life-cycle of the collected r/p samples. in a dynamic environment, a regression or progression pattern may lose relevance over time. therefore, we find it challenging to re-actively deal with retirement of such historical samples."
1574,"limitations safer framework is designed for handling bert classification label noise without using any clean data. despite the fact that the bert is one of the most extensively used models in the industrial domain, the influence of label noise on gpt models and prompt should be further studied in light of the recent rapid progress. we believe that our framework is compatible with these models, however, further evaluation is required. another limitation is the types of label noise. we analyze safer using synthetic datasets with uniform and flip label noise which are typical classlevel noise in practice. however, in industrial applications, the model may experience instance-level label noise, which is beyond the scope of our investigation. although safer achieves robust results in our biomedical literature mining task under human label noise, we encourage users to examine the label noise type first in their own application."
1575,"limitations in this paper we compare a shared encoder architecture for slu to a baseline architecture that was chosen based on the specific latency and cost constraints of an industry slu system. since encoder model sizes were chosen based on specific constraints the results may not be directly comparable to model sizes more commonly used in the literature such as bert-large and bert-base. we expect the general benefit and order of magnitude of accuracy improvements shown in our evaluations to transfer to comparable setups with different parameters. the primary focus of this paper is on accuracy improvements and addressing challenges of realworld slu systems such as distribution drift and feature expansion. we do not elaborate on the details of the computational cost and inference aspects. a detailed analysis of compute cost and benchmarks of cpu and gpu inference would better highlight the infrastructure cost benefits of a shared encoder architecture for slu. regarding the multi-lingual aspect of the encoder we only tested a single grouping of similar european languages (german, french, italian and spanish). a more extensive analysis of different language groups would demonstrate that similar trade-offs seen in other works on multi-lingual language models also apply for the shared encoder architecture."
1576,"limitations the study has several clear limitations. firstly, the training and validation datasets used in this study are still relatively small. a larger dataset would give more robust results for comparing different encoders. additionally, the experiments were only conducted on the ability to generalize to unseen entities and not on the ability to generalize to unseen sketch types, which is also of key importance when addressing low resource ckbqa. moreover, the methodology used in this study relies on annotated question-program pairs, which are expensive to collect. learning only from question-answer pairs or even a question with an indicated difficulty based on whether the model was able to answer the question, could be more easier to collect. while the models achieve high accuracy on most of the knowledge base components, overfitting can occur at different stages during training, leading to high accuracy for one component at one training step but poor accuracy for another component at another step. in the future, revising the training procedure or the model setup may help address this issue."
1577,"limitations although our badge framework is shown to be effective in improving the multi-exit model training and early exiting, it still has certain limitations that need to be addressed in the future: (a) blockwise bypasses indeed introduce new parameters and additional flops. we would like to explore more parameter-efficient methods to improve multi-exit model training in future works. (b) in this work, we demonstrate our framework’s performance on sentence classification or pair classification tasks. in future works, we would like to extend our work to broader tasks such as sequence labeling, relation extraction, and text generation. we would like to explore this aspect in future work."
1578,"limitations building this system was nothing trivial. in our understanding the main challenges where to obtain data access, to chain very specialised artificial intelligence models, and to handle the iterations between the (machine) knowledge model, the customer expertise and the algorithms. we detail each of these challenges herein. access to annotated data piracy and ais spoofing are still too frequent, even though not frequent enough so as to result in the availability of datasets to train and evaluate an automatic system. the proposed approach mainly relies on subtask evaluation (notably on the information extraction steps). the coherence check is fully parameterizable in order to choose a sensibility to all possible variations. a stream of work concerning the automatic/statistic evaluation of the full pipeline is still going-on. hyper-specialized ais most of the substasks here are instantiated by trained modules, which inherently contain an adherence to the ontology used for labelling the training dataset. information extraction from texts were fine-tuned for short pieces of news, and limited to english. this cuts off numerous relevant sources of information, typically from local newspapers anywhere on earth. handling business, ontology and algorithms together the trend to fully automatize screening processes seems intuitive for many data scientists, but is actually not desirable for a security point of view: first, because the targeted elements are “black swans” which occur far too little in the training datasets, and more often than not, do not appear twice. moreover, having too much confidence in the machine is clearly identified as a security risk, among other ai-system biases(rastogi et al., 2020). instead, the desired system should help the operator to handle more data about more incoming ships, and enabling them to focus on what is determining."
1579,"limitations similar to other commercial products, embedding apis are subject to changes that could potentially impact their effectiveness, pricing, and usability. thus, it is important to note that our findings are specific to the apis accessed during january and february 2023. nevertheless, we believe our evaluation framework can serve to thoroughly assess future releases of these apis. moreover, we limit our focus to the effectiveness and robustness of semantic embedding apis. nonetheless, safe deployment of retrieval systems for real-world applications necessitates the evaluation of their fairness as well as additional considerations. despite their scale, language models have been found to learn, and sometimes perpetuate societal biases and harmful stereotypes ingrained in the training corpus (bender et al., 2021). consequently, it is crucial to assess potential biases in the embedding apis with respect to protected and marginalized groups. this paper does not delve into this aspect of api evaluation and further research is required to examine these and other issues in real-world applications."
1580,"limitations the main limitations of the proposed architecture are related to the presence of the frozen feature extractor. the accuracy of the classification module is proportional to the quality of features. since the ensemble weak learners are single-layer neural networks, the entire feature extraction process relies on a pre-trained model that strongly limits the upper bound of classification accuracy. such approach reduces the method complexity, but also makes it prone to errors when embeddings have low quality. achieving accuracy at a satisfactory level, which is crucial in real world systems, requires the use of high quality feature extractors. currently, plenty of pretrained sota models are available for free in domains such as text or image classification, but if such extractor is not available, does not produce reasonable features or is too expensive to use, our architecture may not be the best choice. another issue is relatively long training time comparing to the reference methods (see a.3). the introduction of a differentiable soft knn layer resulted in additional computational effort that clearly impacted the model complexity. this limits the use in low latency systems with machine learning models trained online."
1581,"limitations a first limitation of our contribution stems from the fact that to compute the focal distillation term in the loss, predictions from the old model are required. this additional stream of information will therefore cause a slight increase in the required computational power. in this work, we only experimented with fd based on mean-squared error between pre-softmax logits as that approach yielded best results in the paper our experiments are based on, leaving experiments using fd with kullback-leibler divergence between temperature-scaled softmax outputs for future research. due to inference time limitations in a production setting, we did not investigate the reduction of negative flips with ensembles either. finally, we have not tested more principled approaches for ner distillation and focused on tokenlevel distillation leaving sequence-level distillation for future work."
1582,"limitations in this work, we have employed different strategies to identify the important utterances from early cohort. however, since a voice assistant system consists of many components, such as wakeword, automatic speech recognition, nlu, dialogue manager, where errors occurring in one step might result to the final overall incorrect response. we have not discussed or considered the interaction among these components in this study. last but not least, weak signal learning using users’ feedbacks has shown to be beneficial in many studies, it is important to classify and identify the types of feedbacks that are relevant and those that are not relevant to nlu improvement (e.g., a negative feedback might not be caused by an immediate previous request, but be caused by other factors such as unsupported features, asr incorrect recognition, device technical problems)."
1583,limitations of our model based on various question types. 5https://console.cloud.google.com/ 6larger batch size leads to out of gpu memory errors. 7https://huggingface.co/distilbert-base-cased
1584,"limitations as our models are trained on customer-agent conversations in english, they might not be suitable to be used in other domains, types of inputs (i.e written text), or languages. moreover, as we demonstrated in the paper that the model has limitations in certain question types, the user needs to decide which question types to be used when deploying the system in production. though the dialogled model performs better, it requires higher computing resources. on the contrary, even though the distilbert model consumes lower memory, its performance is poorer than the dialogled model."
1585,"limitations in qa dataset. finally, we have tested the model on proprietary ner datasets and radling-kg has yielded 0.92- 0.93 macro f1 on less represented anatomies like neck, while for the highly represented anatomies, f1 is as high as 0.98. this actually shows the potential of using radlex in radiology pretraining. thus, in a real world setting with high imbalances in datasets, radling-kg is more robust. in future we would like to explore more ways to infuse knowledge by (a) using text description of context like (yuan et al., 2021), (b) retrieving context from biomedical knowledge graphs like snomed 4 and umls, and (c) more robust knowledge embedding methods. we would like to experiment with larger datasets and models, and work with more downstream radiology applications. 4https://www.nlm.nih.gov/healthit/snomedct/index.html"
1586,"limitations in this work, we develop a unified model framework that is applicable to different ner tasks. through experiments, we show the effectiveness of our method on different ner tasks, both in english and chinese. however, we recognize that our method is not tested on ner tasks where the input sequences are extremely long. in addition, our method is not tested on few-shot scenarios. we will investigate these issues in future work."
1587,"limitation of neuspell due to sequence labeling is that it can’t handle compounding errors like iphonepro to iphone pro. it’s evident that our diverse synthetic data generation techniques are effective and lead to significant improvement even with a simple 1 layer enc/dec (reparos-base) transformer compared to deeper pre-trained models like bert and neuspell. adding candidates from our novel phonetic transliteration model was beneficial and led to a total absolute gain of 2-3% at query level consistently across the models and 7.5% accuracy improvement at the word candidate level. effect of curriculum learning: while reparosbase itself is good than the competent baselines, our error analysis showed that it still couldn’t address the complex edit/phonetic + compounding spell errors. this led us to design a few new curricula to improve. our first curriculum reparosbase-1, was to simply add more of complex edit/phonetic+compounding training samples. this resulted in marginal improvement over reparosbase. however, with a different curriculum of only fine-tuning reparos-base on tougher mistakes (model reparos-c1) we observe that the performance increases significantly on the improvement set where most of the mistakes lie, however, there’s a drop in the regression set performance when compared to the reparos-base and reparosbase-1. on analyzing this further, we observed that this was due to the over-correction problem where the model is aggressively altering correct queries in the regression set. hence, we change the curriculum and in reparos-c2 we find that a further fine-tuning on weakly supervised user feedback data improves upon all the variants significantly on both the data sets. this is intuitively due to relatively more frequent queries in the regression set on which receiving implicit user-feedback through clicks is possible at scale. thus adding this new curriculum helped acheive the best performance. online evaluation: in production, we adopt a 2-step architecture by adding an ml ranker (yang, 2022) that does the final candidate selection (from multiple candidates from the reparos/googleswbs). this multi-stage setup empirically produced better results than just finetuning the nmt model since top-10 accuracy of reparos-c2 is 76.31% on improvement set and 98.08% on regression set. this helped all the models (and reparos more due to their better top-k accuracy) and is removed from results"
1588,"limitations and future work in this work we did not consider the following aspects, which we discuss below and lay out directions for how to address them in future work. combining reformulation operations: the reformulation operators, except rep, which is applied jointly with other operators, are applied sequentially, in their given order, e.g. roo+gen. this has two potential limitations that we aim to address in future work. first, applying multiple operators sequentially has the negative impact of increased inference latency as the surf model needs to be applied multiple times, which can become a bottleneck for systems that process large traffic volumes. second, by applying sequentially the reformulation operators, the likelihood of cascading errors or the model making mistakes in terms of the target reformulation shape increases. we aim to address this limitation in the future by fine-tuning the model to jointly perform multiple reformulation operators in a single pass. large language models (llm): in this work we relied on bart (lewis et al., 2020) as our seq2seq model, and did not experiment with newer multi-billion parameter llms. recently we have seen rapid progress in the space of llms, both in terms of model size and their capabilities to perform various tasks (chung et al., 2022). however, we note that deploying llms is limited by their high inference latency, particularly in high-traffic, low-latency systems such as ours. furthermore, for experimenting with api-based approaches such as chatgpt and gpt-4, using these systems was not possible due to data confidentiality. while we will explore leveraging llms for this task in the future, current experimental results show that even smaller language models such as bart, with a sufficient amount of training data, can be fine-tuned to perform the task accurately. evaluation on public datasets: our evaluation focused on real-world unanswered user utterances from voice assistants. we did not use public datasets as currently available resources do not accurately represent customer behavior at scale. however, the community is aware of this divergence, and there are initial efforts in different nlp tasks to create public datasets that represent real-world user behavior. for example, in the the task of named entity recognition there has been recent work on bridging the gap between academic datasets and real-world problems by creating new resources that represent contemporary challenges that are encountered in practice (fetahu et al., 2023; malmasi et al., 2022). in future work we will consider evaluating surf on such datasets as they become available. furthermore, the findings from our work may be used to create data that includes the challenges we identified as part of our analysis (either by organically collecting such data, or simulating it to generate synthetic data). multilingual experiments: we only considered english-language questions in this work, and it will be of interest to consider how our approach can be extended to other languages using multilingual models. the evaluation of cross-lingual transfer for this task is another open research area."
1589,"limitations and future work one limitation of our approach is that we do not display or rank multiple reformulations. it is possible that a query can be reformulated into multiple possible questions. for example, the query “apple tv bluetooth” can be reformulated into “how do i connect a bluetooth device to my apple tv” or “does apple tv support bluetooth”. in our future work, we aim to explore the integration of multiple reformulations into the faq retrieval process to further enhance the overall user experience. another limitation is that we do not train an end-to-end faq retrieval model. in the future, we plan to train the faq retrieval model using the reformulations so that the original query can directly be used."
1590,"limitations the data from the pitt dataset (hussain et al., 2017), while useful for our paper, contains many images and annotations that may perpetuate harmful stereotypes according to sensitive characteristics such as gender and carry the risk of amplification by machine learning models. we plan to collaborate with ai robustness researchers to identify such examples and develop methods for improving ml models in terms of robustness and reliability."
1591,"limitation because our architecture only predicts one topic. there are very few cases where we observe that the model learns to put undue emphasis on certain keywords (“back” in topic and question, example 9 in table 5) and gives the highest score to the wrong topic. b taxonomy and tcdl module output"
1592,"limitation of the existing medical ner models is their poor performance on nonus and eu prescriptions due to bias in the training data, which is almost exclusively based on useu centric medical content and vocabulary. in our approach, we have deliberately chosen to have explicit dependence on aspects that vary across geographical regions (e.g., medical catalog), which enhances the applicability of our approach. to further limit the model bias and minimize distributional differences between training and production settings, we have trained our models on prescription images that are randomly sampled from customer uploads. these often include low resolution and improperly positioned images. in future, as the scope of deployment changes, we plan to periodically retrain the model with training images by sampling from the production data. health safety: one of the primary concerns in prescription digitisation is the impact of errors on patient health and adherence to health regulations. to alleviate adverse outcomes, we have multiple guardrails. first, we present the top three suggestions along with scores for each medication for two-fold review by customer and pharmacist. second, to avoid prescription abuse (e.g., manipulation of quantities, prescription reuse) and comply with regulations, there are additional checks based on the prescription date, patient purchase history, and recommended limits on medication quantities. usage for a limited scope: our proprietary system has been trained for a specific-use case, i.e., prescription digitization with acceptable performance on primarily english printed prescriptions for india region. we plan to use the model within this limited scope and expand usage only after adequate benchmarking. to limit the risks of misuse, we do not plan to release this system externally."
1593,"The primary limitations of the current study pertain to the selection of prompts and evaluation metrics. The experimental cost of requesting API responses from OpenAI to assess ChatGPT’s text generation abilities imposes significant constraints on our choice of datasets. Therefore, we have to limit our experimentation to only two related controllable text generation datasets. While we have evaluated ChatGPT’s performance at both the document and sentence levels, we cannot extrapolate that ChatGPT has similar performance for other text generation datasets. Additionally, the experimental cost prohibits us from conducting traversal experiments on the selection of hyperparameters. We relied on the default configuration recommended by OpenAI, and we maintain consistency in all hyperparameters to ensure the fairness of the experiments.
Secondly, although we have studied the impact of prompt engineering on ChatGPT, the selection of prompts is mainly affected by human understanding, and the number of potential prompts is infinite. Hence, we cannot guarantee whether other prompts that we did not select will yield the same conclusions as our experiment. Furthermore, ChatGPT is subject to continuous updates and iterations, which may lead to improved performance, making it difficult to predict if future versions of ChatGPT will have similar results to our experiments.
Finally, to select appropriate evaluation metrics, we have included both domain-related evaluation metrics (such as reading difficulty and text formality) and domain-independent evaluation indicators (such as fact-checking and hallucination detection). However, we acknowledge that the automatic met-
rics may sometimes not capture all aspects of the intended construct correctly."
1594,"This study has the following limitations: • We fixed the vocabulary size of each subword
tokenizer to 30k. Using a different size might yield different results than those in our paper, though the effect of varying the vocabulary size for a subword tokenizer seemed to be small if the size is sufficiently large (e.g., over 16k or more) (Toraman et al., 2022). • We have used the BERT architecture for our comparison, while there are other commonly used model architectures such as T5 (Raffel et al., 2020) and GPT-3. The investigation with these architectures is our future work. • To investigate the impact of tokenizers on the downstream performance of PLMs in scriptio continua languages, we have taken Japanese as a case study. Other scriptio continua languages will be addressed in the future."
1595,"In this section, we discuss two limitations of this study. The first limitation is that aspect and temporal commonsense are outside the scope of our dataset. Here, temporal commonsense refers to knowledge regarding events and the appropriate
duration of those events. For example, the event “I washed my face for three years” is unnatural in terms of temporal commonsense, but this study did not consider such unnaturalness.
The second limitation is that the proposed method is currently applicable only to Japanese. In this study, we used a Japanese case frame dictionary to generate natural sentences. However, other languages such as English do not have resources equivalent to such a dictionary. Therefore, to apply our method to additional languages, we must first prepare a case frame dictionary for each language."
1596,"While our study provides valuable insights, it is important to keep in mind its limitations. Firstly, it was confined to text classification and did not include other NLP problems such as Named Entity Recognition (NER) (Wang et al., 2022b), Question Answering (QA) (Pandya and Bhatt, 2021), etc. Expanding this research to a wider range of tasks would provide a better understanding of the methods’ performance in diverse data scenarios. Additionally, the inclusion of a task shift can be valuable, where the model is trained on a single task but OOD data come from a totally different prediction problems.
Secondly, we conducted our experiments using only RoBERTa model. We chose a widely used language model for text classification, but there are several other architectures worth testing, especially large language models (LLMs) (Zhao et al., 2023) that now becoming extremely popular. A more comprehensive evaluation of the models and methods could provide more insights into whether the development of transformer-based methods contributes to better detection of OOD data.
Finally, due to restricted computational time, we did not perform a hyperparameter search for either model or methods. We just used recommend values from the original publications. This may have affected the obtained results, and it is certainly an aspect worth investigating in the future."
1597,"The MECO dataset (Kuperman et al., 2022) is recorded at different labs following the same strict protocol. Nevertheless, location and experimenter effects may be confounding factors for the NLIR task. The CELER data (Berzak et al., 2022), used by (Berzak et al., 2017), seems to all be recorded at the same lab. Since we confirm their hypothesis, we do not see this as a fatal flaw in our study. There is no other available dataset that would allow us to replicate their finding."
1598,"There are several limitations to the experiments conducted in this project that should be acknowledged:
• Selection of the best pre-trained language model (PLM) for prompt-based learning: The evaluation method used to compare the performance of BERT, GPT-2, and T5 in the context of manual templates and manual verbalizers may not be entirely accurate. The performance of these models did not show significant differences, making it difficult to determine the best model for prompt-based learning. Furthermore, other domain-specific PLMs, such as Bio-BERT, which may be better suited for handling clinical data, were not considered in this project.
• Limited exploration of templates: The experiments utilized a limited number of templates, particularly for soft and mixed templates. These templates were primarily based on prompts derived from manual templates. Further experimentation is needed to explore different patterns, such as varying the position and length of soft token sequences or using soft tokens in mixed templates to replace manual tokens (e.g., ""Question:"").
• Comparison with frozen PLMs: The experiments did not include a comparison between fine-tuned and frozen PLMs, as done in Taylor’s study (Taylor et al., 2022). This comparison could provide valuable insights into the performance trade-offs between these two approaches.
• Addressing the effects of imbalanced datasets, several strategies have gained popularity. 1) Re-sampling techniques, for example, Monte Carlo Simulation Analysis, can be used to balance class distribution by oversampling the minority class, undersampling the majority class, or the combination of these two (Gladkoff et al., 2021). 2) Data augmentation techniques, such as the use of Generative Adversarial Networks (GANs), can generate new examples for the minority class by applying transformations to existing data. 3) Furthermore, machine learning approaches like bagging and bootstrapping can reduce variances
by implementing a ""voting system"" that enables models to make better decisions.
• Finally, it would be advantageous to develop a post-processing step that generates a table displaying all treatments along with their corresponding temporal information. This would create an end-to-end system that physicians could use as a practical tool.
Future research should address these limitations by exploring a broader range of PLMs, templates, and experimental setups to provide a more comprehensive understanding of the performance characteristics of prompt-based learning methods in the clinical domain. Application to some more powerful computational resources will also extend this work."
1599,"The main limitation of our work is the focus on the specific domain and the dataset that is not yet publicly available. However, we should note that the dataset can be requested for further research and replication studies and it will be released in the future.We believe that testing adapters with different settings in the emergency response domain is a valuable contribution but we are also aware of the fact that the dataset used in our experiments is not large or exhaustive enough to cover all the variety of topics relevant for the emergency response. For example, our data cover cases of explosions, leakages of hazardous materials and building collapse but do not include any dialogues for open field rescue operations or car accidents.
Another issue that is worth mentioning is the fact that all recordings were collected during the training sessions and not the actual missions. Hence, the responders might be under less pressure than in a real life-threatening situation and their communication might be more of a textbook case. However, all simulations had a realistic setting that includes several operators, robots and points of interest (objects or locations) and we believe that the recorded communication is representative for the domain in question."
1600,"We explore early, very simple structured event representations. Recent works in visual–linguistic semantic representations which use richer representations comprising predicate–argument structures and event types and argument roles, the general graph-based approaches, as well as scene graphs, are left for future work. Furthermore, the wikiHow articles may reflect the bias of their human authors."
1601,"While we demonstrate the efficacy of TSDSHAPLEY empirically, the current work is limited in terms of theoretical analysis. For example, while we have good empirical performance with a linear SVM, additional analysis could determine if there are optimal ways to select an alternative simple model architecture for the source classifier depending on the target classifier or dataset. Additionally, while we found a strong correlation between number of sampling chains and performance when the subset size was > 2% of the training data size, the lower subset size threshold to observe this correlation may be dataset dependent, which additional analysis could address."
1602,"This work used the moral foundations dictionaries to measure the moral content of text produced by GPT-3. While studies have demonstrated correspondence between results from the dictionaries and human labels of moral foundational content (Mutlu et al., 2020; Graham et al., 2009), dictionarybased analysis is limited in its ability to detect nuanced moral expressions. Dictionary-based analysis could be complemented with machine-learning approaches (Garten et al., 2016; Johnson and Goldwasser, 2018; Pavan et al., 2020; Roy et al., 2022) as well as human evaluation. This study attempted to control for variations in the prompt phrasing by averaging results over several prompt styles (Tables 2 and 3). These prompt variations were chosen by the author. A more principled selection procedure could result in a more diverse set of prompts. The human studies that this study refers to (Graham et al., 2009; Frimer, 2020) were performed on populations from the United States. The precise political connotations of the terms “liberal” and “conserva-
tive” differ across demographics. Future work may explore how language model output varies when additional demographic information is provided, or when multilingual models are used. Documentation for the datasets used herein indicates that the crowd workers leaned politically left, and morally towards the Care/Harm and Fairness/Cheating foundations (Forbes et al., 2020; Hendrycks et al., 2021; Fraser et al., 2022). However, bias in the marginal foundation distribution does not hinder the present analysis, since the present experiments experiments focus primarily on the difference in foundation use resulting from varying political identity. The analysis in Section 2.1 relies more heavily on the marginal foundation distribution; a foundationallybalanced dataset was constructed for this experiment. This study used GPT-3 (Brown et al., 2020), GPT-3.5 (OpenAI, 2022), and OPT (Zhang et al., 2022). Other pre-trained language model families of similar scale and architecture include BLOOM8, which I was unable to test due to compute budget, and LLaMA (Touvron et al., 2023), which was released after the experiments for this work concluded. While the OPT model weights are available for download, GPT-3 and GPT-3.5 model weights are not; this may present barriers to future work that attempts to connect the moral mimicry phenomenon to properties of the model. On the other hand, the hardware required to run openly-available models may be a barrier to experimentation that is not a concern for models hosted via an API.
Criticisms of Moral Foundations Theory include disagreements about whether a pluralist theory of morality is parsimonious (Suhler and Churchland, 2011; Dobolyi, 2016); Ch. 6 of (Haidt, 2013), disagreements about the number and character of the
8https://bigscience.huggingface.co/blog/bloom
foundations (Yalçındağ et al., 2019; Harper and Rhodes, 2021), disagreements about stability of the foundations across cultures (Davis et al., 2016), and criticisms suggesting bias in the Moral Foundations Questionnaire (Dobolyi, 2016). Moral foundations theory was used in this study because it provides established methods to measure moral content in text, and because MFT-based analyses have identified relationships between political affiliation and moral biases, offering a way to compare LLM and human behavior. The methods presented here may be applicable to other theories of morality; this is left for future work.
Work that aims to elicit normative moral or ethical judgement from non-human systems has received criticism. Authors have argued that nonhuman systems lack the autonomy and communicative intent to be moral agents (Talat et al., 2022; Bender and Koller, 2020). Criticisms have also been raised about the quality and appropriateness of data used to train such systems. Notably, crowdsourced or repurposed data often reflects a priori opinions of individuals who may not be informed about the topics they are asked to judge, and who may not have had the opportunity for discourse or reflection before responding (Talat et al., 2022; Etienne, 2021). Some have argued that systems that aggregate moral judgements from descriptive datasets cannot help but be seen as normative, since their reproduction of the popular or average view tends to be implicitly identified with a sense of correctness (Talat et al., 2022). Finally, several authors argue that the use of non-human systems that produce apparent or intended normative judgements sets a dangerous precedent by short-circuiting the discursive process by which moral and ethical progress is made, and by obscuring accountability should such a system cause harm (Talat et al., 2022; Etienne, 2021).
The present study investigates the apparent moral rationalizations produced by prompted LLMs. This study does not intend to produce a system for normative judgement, and I would discourage a normative use or interpretation of the methods and results presented here. The recent sea change in natural language processing towards general-purpose LLMs prompted into specific behaviors enables end users to produce a range of outputs of convincing quality, including apparent normative moral or ethical judgements. Anticipating how these systems will impact end users and society requires studying model behaviors under a variety of prompting
inputs. The present study was conducted with this goal in mind, under the belief that the benefit of understanding the moral mimicry phenomenon outweighs the risk of normative interpretation."
1603,"limitations this study stems from a novel idea for chinese historical phonology studies. as few direct predecessors could offer hindsight, there are quite a few limitations to this study that may be addressed with further work. 1. while the initial-final-tone decomposition is convenient in this context, it also limits the transferrability of the proposed tool to languages outside of the sinosphere. this calls for further exploration of more generalizeable approaches to phonological representation learning. 2. polyphonic characters were not fully utilized in the study, and their alignment perreading and tokenization into separate identifiers should be considered in future work. 3. finally, making full use of the dataset is crucial, and the stochastic train-test split used in this study may leave out important hints. alternative sampling strategies, such as crossvalidation or bootstrapping, could enhance the robustness of the results."
1604,"limitations the current work has several limitations that warrant further investigation. firstly, due to time constraints, we did not conduct experiments using the proposed framework on few-shot settings or a more challenging multi-label classification task. secondly, our ablation study in section 4.4 showed that the framework with the weight assignment resulted in only a marginal improvement in performance, suggesting that simcse may not be the most effective method for addressing prediction bias. therefore, future work will explore alternative modeling approaches for bias reduction. thirdly, in section 4.5, we noticed that several irrelevant words are also generated as keywords with the language prompt, which may negatively impact the final representation. to address this issue, a better solution, such as keyword filtering, should be considered to improve the current framework. lastly, we treated each word as a single atomic entity in the kg embedding space, regardless of its possible different senses or meanings. a more careful treatment of word meanings is necessary to handle the problem of polysemy. the race is on: second private team sets launch date for human spaceflight (space.com) space.com - toronto, canada -- a second\\team of rocketeers competing for the #36;10 million ansari x prize, a contest for\\privately funded suborbital space flight, has officially announced the first\\launch date for its manned rocket."
1605,"limitations this papers focuses on a variety of videoqa - event-level videoqa, we only incorporate event information from the question (textual) side as we think that parsing video frames is inaccurate and could introduce unexpected errors, we should also explore how to inject event-level information from visual side in the future with more competitive visual parsing models. our experiments are only conducted on one dataset due to resource constraint, we should also conduct experiments on more datasets to verify the effectiveness of our approach."
1606,"limitations we used xlm-r for the baseline model to train with our dataset in our experiments because we wanted to make experimental settings as close as the previous study of codebert but for multilingual data. since codebert is based on roberta, we chose xlm-r, which is also roberta-based and already trained with multilingual data."
1607,"limitations in this work, we confirm the effectiveness of the proposed method only on the english-german translation tasks using the multi30k dataset, the most commonly used dataset in the mnmt reserach area. it is not clear whether the proposed method is effective for translation for language pairs other than english and german or translation when a larger training dataset is used (e.g., when using an existing data augmentation method for mnmt). we will leave these verification experiments for future work. the proposed method has improved translation performance of mt, but the performance is not perfect and translation results could include translation errors. accordingly, there still remains a possibility that translation results by the proposed method could convey incorrect information. the proposed method requires an additional process for transforming images, compared with conventional mnmt models. the experiment, including model training and testing, on the proposed model mnmt(conv.) took about 20 hours longer than that on the baseline mnmt model mnmt(orig.) when using rtx3090 gpu × 1."
1608,"limitations the main limitation of our study comes from the extra parameters caused by confidence calculation, in which two separate self-attention operations and biaffine transformation are performed. incremental parameters results in a more time-consuming training process, and a higher hardware demand for storage. to address this issue, we plan to combine parameters from different attentional transformations into shared weight matrices in our future work to reduce the model size."
1609,"limitations the dialogue generated for the player exhibits a higher degree of repetition and has a tendency to- wards looping. this limitation exists as we did not focus on generating player dialogue as that is a different problem of its own. to account for this limitation, both the self-diagnosis and the turing quest only evaluate the npc’s dialogue. currently, the maximum context window for the dialogue history portion is limited by the max tokens of a given model minus the tokens required for the npc header. despite being a rare occurrence, it is possible that the dialogue history becomes so long that the model may not be able to generate any responses as there is no more remaining space. we did not experience this problem; however, a workaround would be to discard the oldest dialogue history entry as needed. this approach however may cause the npc to lose out on information that it would otherwise be able to leverage in dialogue."
1610,"limitations we acknowledge three limitations in our experiments. first, in our second experiment, we fixed the window size for each type of note to be n/2. a more comprehensive investigation could also search for the optimal window size for each note type. second, although we explored one fragmented window configuration p = both, we did not explore other fragmented window configurations due to resource constraints. lastly, we did not investigate more types of clinical notes (e.g., physician notes and ecg notes) because mimic-iii has limited examples for other note types. we expect it to be resolved in future works with mimic-iv’s publication (johnson et al., 2023)."
1611,"limitations that may influence the interpretation of our results and call for future works. first, we did not conduct a hyperparameter search for the regularization strength of corrloss. second, since f1 score decreases are substantial and universal across all experiments on mimic-iii-50, we did not run experiments multiple times with different seeds. third, we did not provide a rigorous explanation of what caused our empirical findings. future works can investigate the plausible hypothesis that the trade-off between the dependency information and the learning complexity causes these findings. besides these limitations, future works can also investigate more scenarios and methods of incorporating the correlation prior."
1612,"limitations the morpho-syntactic parameters used in this study are just a fraction of various other linguistic parameters that have been proposed in theoretical syntax (e.g., roberts 2019). a set of optimal language parameters for language clustering may vary depending on the target task. it remains to be seen whether and how various parameters in theoretical linguistics could improve different nlp tasks. for example, cross-lingual transfer learning may be performed more effectively by carefully tailoring the linguistic parameters to a particular task, like what we have done for ner. related to the above point, one limitation of our approach would be the fact that some languages have not yet been investigated well in theoretical linguistics, particularly some underdocumented or endangered languages. even as for welldocumented languages in theoretical linguistics, some parameters still remain controversial, such as the so-called np/dp parameter (e.g., bošković 2012). thus, our approach proceeds in tandem with the advancement of theoretical linguistics."
1613,"limitations we built buzzer quiz answering systems. however, they do not take into account the time required to respond, and these systems do not have the ability to generate real-time responses, which is essential in actual buzzer quizzes. additionally, the experiments in this study were conducted only in japanese, and it remains unclear whether similar results would be obtained in other languages. particularly, english has a significantly different sentence structure compared to japanese, hence further investigation is necessary to confirm whether appropriate results can be achieved."
1614,"limitations although our research led to improvements in the translation of subject pronouns, object pronouns, and possessive adjectives and pronouns, these improvements did not cover non-binary-associated pronouns, such as they/them/theirs, xe/xem/xyr and ze/hir/hirs. the large underrepresentation of non-binary genders in textual and visual data contributes to propagating the misrepresentation of non-binary people by language models. in this paper, we were unable to work against this issue, thus we hope to contribute to a fairer representation of these disadvantaged groups in the future."
1615,"limitation although our leco framework is shown to be effective in improving the multi-exit bert training, it still has certain limitations that need to be addressed in the future: (a) mha exits and our learned exits indeed introduce new parameters and additional flops. we would like to explore more parameter-efficient methods to improve multi-exit bert training in future works. (b) in this work, we demonstrate our framework’s performance on sentence classification or pair classification tasks. in future works, we would like to extend our work to broader tasks such as sequence labeling, relation extraction, and text generation. we would like to explore this aspect in the future."
1616,"limitations while this research provides valuable insights into using the gan-bert model for authorship attribution, there are also a few limitations to note. we only focused on a limited number of authors from the late 19th century, which may include shortcomings towards model generalisability. future research should consider using the whole dataset of long 19th-century novelists to address this limitation. due to the copyright issues explained in section 4.6 and section 7, we do not release the whole dataset, instead, we release scripts to reproduce the datasets. furthermore, incorporating a rich feature set and comparing performance among different models would be another interesting research direction."
1617,"limitations our studies focus on the differences in how-to guides written for specific audiences only in one language, namely english. a major limitation is therefore that we do not consider other languages. the perspectives provided by the data source we rely on, wikihow, allow us to identify specific phenomena and peculiarities. yet, contemplating only one data source lets us generalize only to a limited extent. for example, the audiences considered in this work depended on the target groups portrayed in the data. they are neither exhaustive nor representative of the diversity of humankind, especially of marginalized social groups. therefore, a wider variety of data sources will be needed to test generalizations. finally, a further limitation of our studies concerns intersectionality. while it seems possible that guides can be tuned by contemplating one specific attribute of the audience at a time, this does not hold with regard to the actual attributes of the readers. such attributes are per se coexistent, and consequently, they are not separable."
1618,"Though our ultimate goal is to build a versatile QA system that can handle all types of questions, our benchmark mainly focuses on extractive questions – those can be explicitly answered by copying from a document in the knowledge source. We start from extractive QA because they cover a wide range of real-world questions and are easier to be automatically evaluated.
Although we addressed the issue of long-form QA evaluation with human evaluation and a range of automatic evaluation metrics, there is still much room for improvements in terms of evaluation of long-form text — human evaluation can be expensive and non-reproducible while current automatic metrics are not without faults. We encourage future work exploring various evaluation strategies of long-form QA.
Furthermore, all questions are in English and possibly collected from English-speaking users. We also use the English Wikipedia as our knowledge source. Thus, our models and dataset may under-represent the non-English speakers."
1619,"In this paper, we present a comprehensive framework for measuring the quality of a dialogue system dedicated to activities of daily living assessments. We have created a new high-quality dataset of human-written questions and answers with corresponding profile information. We are currently working on expanding the dataset by adding more profiles and removing any factual inconsistencies resulting from human error. Although more complex models showed better query classification performances, we need to consider the trade-offs between model size and generation time in the deployment environment to ensure a smooth user experience. We also identify areas where LLM performance can be augmented by a knowledge base filled with human written natural language facts, and that this augmentation need not come at a penalty to sensibleness, specificity, or the realistic quality of conversation. General conclusions based on our initial work here may not be possible given the limited number of evaluators and small amount of evaluated dialogues, and this is a major limitation of our contribution. Future work is needed to develop a more robust and replicatable evaluation framework, especially to perform evaluations of long and complex conversations like the type that assessors perform in the field. Such an evaluation will need to include larger numbers of human raters to improve the statistical power of the surveys. Recent automatic evaluations may also help improve development efforts, as a sufficiently powerful LLM such as GPT-4 may be able to monitor the chatbot for regressions in its ability to speak fluently, sensibly or specifically. This assessment, known informally as the ""Vicuna Assessment""(Chiang et al., 2023), cannot give an evaluation of the chatbot’s fit-for-purpose, but could be used to compare short conversations from several versions of the same chatbot. This could free up
more human resources to evaluate the knowledgegroundedness and fit-for-purpose of future versions. In addition, given more computing budget and more time to engineer prompts, larger language models beyond LLaMA 7B could be further studied or fine-tuned while experimenting with fine-tuning datasets and process. There are also many thresholds and parameters that could be further tested in the development of the knowledge-grounding system, wherein similarity measures inform the system’s decision to answer using a generative model versus responding with language directly from the knowledge base."
1620,"The limitations of this study are primarily due to budget and credit constraints. Consequently, our query rewriting observations are based on a sample size of 2000, leading to limited generalizability of findings. Another limitation of the limited resources was the limited context size of ChatGPT and the relatively long nature of the questions in our dataset. Hence, we could not test prompting ChatGPT with in-context examples for better query rewriting performance. Hence one potential future work is testing the performance of query rewriting using in-context examples. Finally, ChatGPT architecture is not open source, preventing us from testing advanced prompting methods. Hence another future work would be to test query rewriting using open source models and with advanced finetuning methods like Prefix Tuning (Li and Liang, 2021) and Prompt Tuning (Lester et al., 2021).
The study is also subject to the risk of ""hallucinations"" in ChatGPT’s responses, which may lead to imprecision in query rewriting. The study suggests further investigation into these issues to improve the accuracy and reliability of the results. We recommend further investigation into these limitations and any potential societal biases present in our dataset to enhance the reliability and performance of query rewriting."
1621,"The main limitation of our work is its focus on English datasets. While this was due to their popularity and extensive usage (and our limited language skills), it overlooks datasets like DuConv (Wu et al., 2019) and NaturalConv (Wang et al., 2021) (both Chinese) which employ more explicit annotation instructions regarding dialog ‘path’ and topic transitions. Studying the way these restrictions affect conversational attributes, is necessary for a more comprehensive understanding of the problem.
Another limitation is the lack of an empirical investigation on how/if these artefacts and biases affect the final objective of KGD modeling, i.e. response generation. This of course is not easy in the absence of a less biased dataset, but synthetic datasets –which have become much better in quality and flexibility thanks to large language models– can probably provide reliable estimations, which we plan to explore in future studies."
1622,"limitations this work has potential limitations: • we found that on the figure 3 and 4, the entailment of the methods after applying multiple position embedding (red lines) are sometimes lower than origin methods(blue lines). this is not meet our expectations since we don’t want our method to decrease performance. in our opinion, we think the reason might be the embedding method has never been seen before during the pretraining of models, which requires the model’s additional efforts to adapt the embedding, thus hurts the performance.. we leave it as future work to be improved. • we also found that the multiple position embedding does not work very well to alleviate the order effect in the lm loss-only settings4. we have discussed this in previous sections. since lm loss only does not help the model distinguish which parts in the input sequence are knowledge set and thus treat them the same as history. the multiple position embedding will not be trained finely to help the model distinguish. we also left this as a future work to be improved."
1623,"limitation of our contribution. future work is needed to develop a more robust and replicatable evaluation framework, especially to perform evaluations of long and complex conversations like the type that assessors perform in the field. such an evaluation will need to include larger numbers of human raters to improve the statistical power of the surveys. recent automatic evaluations may also help improve development efforts, as a sufficiently powerful llm such as gpt-4 may be able to monitor the chatbot for regressions in its ability to speak fluently, sensibly or specifically. this assessment, known informally as the ""vicuna assessment""(chiang et al., 2023), cannot give an evaluation of the chatbot’s fit-for-purpose, but could be used to compare short conversations from several versions of the same chatbot. this could free up more human resources to evaluate the knowledgegroundedness and fit-for-purpose of future versions. in addition, given more computing budget and more time to engineer prompts, larger language models beyond llama 7b could be further studied or fine-tuned while experimenting with fine-tuning datasets and process. there are also many thresholds and parameters that could be further tested in the development of the knowledge-grounding system, wherein similarity measures inform the system’s decision to answer using a generative model versus responding with language directly from the knowledge base."
1624,"limitations while our proposed method demonstrates promising results and outperforms several state-of-the-art techniques, it is important to acknowledge certain limitations. • dependence on pre-trained llms: our method relies heavily on the pre-trained llm’s quality and the knowledge it has captured. as a result, any biases, inaccuracies, or limitations present in the llm may directly impact the performance of our evaluation metric. • lack of diversity in the dataset: the fed dataset, which we use for evaluation, is primarily derived from conversations with the meena and mitsuku chatbots. consequently, it is possible that our evaluation might not have better correlation with human ratings for other dialogue systems or more diverse conversational contexts. • adaptability to new evaluation dimensions: our method currently focuses on eight turnlevel metrics. extending the method to incorporate additional or novel evaluation dimensions might require further investigation and calibration. • computational cost: the current implementation of our approach is around twice as slow as the baseline nll-based method due to multiple times of the inferences of the language model. the efficiency of the implementation can be improved in the future by re-using the log-likelihood of the dialogue history. • subjectivity in human judgments: our evaluation metric’s correlation with human judgments serves as a key performance indicator. however, human judgments are inherently subjective, which could lead to inconsistencies or discrepancies in the evaluation results. despite these limitations, our proposed method presents a significant step forward in dialogue evaluation, offering a model-agnostic, unreferenced, and training-free approach that captures the human and the system interaction. future work could address these limitations and explore additional dimensions of evaluation, further refining the method and its applicability across a broader range of dialogue systems and text evaluation systems."
1625,"limitations of this study are primarily due to budget and credit constraints. consequently, our query rewriting observations are based on a sample size of 2000, leading to limited generalizability of findings. another limitation of the limited resources was the limited context size of chatgpt and the relatively long nature of the questions in our dataset. hence, we could not test prompting chatgpt with in-context examples for better query rewriting performance. hence one potential future work is testing the performance of query rewriting using in-context examples. finally, chatgpt architecture is not open source, preventing us from testing advanced prompting methods. hence another future work would be to test query rewriting using open source models and with advanced finetuning methods like prefix tuning (li and liang, 2021) and prompt tuning (lester et al., 2021). the study is also subject to the risk of ""hallucinations"" in chatgpt’s responses, which may lead to imprecision in query rewriting. the study suggests further investigation into these issues to improve the accuracy and reliability of the results. we recommend further investigation into these limitations and any potential societal biases present in our dataset to enhance the reliability and performance of query rewriting."
1626,"While our approach is effective at compressing models, it is not the most efficient. In order to discover the most optimal compression approaches and evaluate their performance performed hundreds of experiments. As a result, scaling our approach to every novel language understanding language model is not tractable. Another limitation of our work is we did not track the complete compute utilization of our entire experimentation process but we can provide some estimates. Experiments in pruning during fine-tuning leveraged a
single V100 16 GB GPU and took approximately 14 hours per experiment. The pre-training of structurally pruned models with knowledge distillation required 4 A100 40GB GPUs for approximately 72 hours. Pruning during pre-training with Knowledge distillation required approximately 100 hours on the same setup. Task-specific fine-tuning happened on a single V100 16GB GPU and depending on the size of the task was anywhere from a few minutes to 20 hours. Based on all of our experiments we estimate 400 V100 hours of pruning during fine-tuning, roughly 16,000 A100 hours10 for pretraining, and assuming an average of 10 V100 hours per sparse transfer run, a total of 4000 V100 hours for sparse-transfer and sparse-transfer with quantization."
1627,"While much of our work has focused on showcasing the broad usability of compressed language models, they are not without fault. While our experiments focus on the compression of RoBERTa, the size of its training dataset makes complete exploration of the ability of pruning during pretraining somewhat limited. The work in the paper shows the ability to compress RoBERTa on a smaller pretraining dataset but does not contrast it with the impact of compression on the full dataset.
A second limitation of our work is the high computational demand required for creating public domain sparse language models. Despite amortizing the cost of compression to a few pretraining training regimes, the reduction of other language models like ALBERT (Lan et al., 2019) or XLMR (Conneau et al., 2019) require completely new training, pruning, and transfer experiments."
1628,"While our work makes a broad study on how to improve model efficiency our scope is limited. Our work is limited to the usage of BERT-base and it is not clear how our compression approaches scale to more varied architectures like the sequence-tosequence models used by DocT5 (Lee et al., 2022) or more optimized models like RoBERTa (Liu et al., 2019) or compressed models like MiniLM (Wang et al., 2020)."
1629,"Due to the limited resources, we could not experiment with this approach for larger language models such as roberta-large and bert-large. It would be interesting to investigate the performance of ADEPT with larger LMs.
In addition, we only evaluate the ADEPT approach on seven downstream tasks. It would also be interesting to test it on more broad natural language processing tasks, such as information extraction, natural language generation, question answering, and so on.
Broader Impact
As discussed earlier, fine-tuning large pre-trained models for downstream tasks can be really expensive. The ADEPT approach can help AI practitioners to assess the abilities of LM without using a lot of resources. This approach of prompt tuning can also help smaller end users to take advantage of harnessing the power of LM with the minimal resources they have. It can be used by social scientists, Non-profit organizations, etc. to create a positive impact in society in spite of limited computing resources."
1630,"We present KG-Flex, an end-to-end model that can access new relations at test-time without retraining. Our current KG-Flex model does not perform entity resolution, and so we rely on resolved entities provided by the datasets. However, resolved entities may not always be available, so tools such as automatic entity recognition may be necessary. While it is possible for end-to-end models to jointly learn to resolve entities in questions before relation following (Saffari et al., 2021), we consider this outside the scope of this focused work.
Additionally, KG-Flex is limited in the kinds of reasoning it can do over a Knowledge Graph. Currently, KG-Flex only performs relation following, so it cannot handle questions which require complex reasoning like counts, comparatives, min/max, etc, such as “Who is the tallest NBA player?” We hope to address this in future work.
Further, we test KG-Flex on popular datasets representing possible real human questions. However, we do not deeply investigate the semantic properties of these questions. Notably, McKenna and Steedman (2022) show that searching for similar relations in embedding space (as done in KGFlex) may work better for paraphrastic inference, and only in certain cases for directional inference where semantic precision matters, e.g. DEFEAT entails PLAY, but PLAY does not entail DEFEAT. We
leave deeper investigation of KG-Flex semantics and edge cases to future work."
1631,"Our proposed method MANER for improving NER is best suited for low-resource settings. As discussed in Section 3.3, we measured the effectiveness of MANER in situations where more training data is available and found that MANER boosts F1 performance over the SNER baseline until about 400 training examples, and then both methods perform similarly. The result demonstrated that MANER is best suited for extreme low-resource languages and rapid prototyping because it is easy and cost-effective to obtain very few human annotations to achieve significant performance improvements.
We base the experiments in this paper on a widely adopted model, XLM-RoBERTa, pretrained on multiple languages. It is possible that the empirical conclusions we draw from the observations do not generalize to other pre-trained models."
1632,
1633,"The limitations of this work are as follows:
• English. Our work builds autocomplete models for English language only.
• Accuracy-memory tradeoff only. Our work primarily focuses on deploying models on lower-end edge platforms where memory, as opposed to latency, is the major bottleneck. Hence, our methods may not improve the accuracy-latency tradeoff, which is a focus for future work.
• WikiText-103 dataset Our work explores only WikiText-103 dataset for creating broad prompts. In the future, we will study
6We provide a qualitative analysis of the baseline and proposed character models in the Appendix A.10.
other datasets (e.g., 1 Billion Word Language Model benchmark (Chelba et al., 2013)) that explore the full range of low-frequency prompt patterns, which can arise in real-world situations.
• Transformer-XL architecture Our work studies only Transformer-XL architecture to build word based and character based autocomplete models. In the future, we will study other popular architectures (e.g., GPT-2 (Radford et al., 2018)) to see the generalizability of proposed techniques."
1634,"Limited Experimental Scope Our study’s experimental scope was limited to testing distilled student models against a single teacher model. A more comprehensive evaluation would involve multiple teacher models of varying sizes, fine-tuning tasks, and datasets. Additionally, in our experiment with DistilBERT-based student models, incorporating more checkpoints would enable a more thorough comparison across different factors.
Unexplored Embedding Size Variations We kept the embedding size (768) consistent across student models to maintain variable consistency. Future research could investigate student models with different embedding sizes to determine if the observed trends hold true across models of varying widths.
Lack of Error Analysis A common distillation limitation, as noted by Hooker et al. (2020), is the considerable performance decline for certain data subsets. In our study, we couldn’t conduct a thorough error analysis due to the lack of appropriate tools for comparing individual data points in retrieval tasks."
1635,"limitations as described in section 1, the purpose of this study is to relax the existing parameter sharing strategy which shares the parameters of one layer with all layers (dehghani et al., 2019; dabre and fujita, 2019; lan et al., 2020). experimental results indicate that the proposed simple parameter sharing strategies can be a better alternative to the existing method. as many studies on neural methods, this study also depend on empirical observations. in other words, this study lacks theoretical justifications for proposed parameter sharing strategies. we conducted experiments on various situations. we mainly focused on sequence-to-sequence tasks and trained each model from scratch. our conducted experiments indicated the efficiency of the proposed strategies but we did not conduct experiments on the pre-training and then fine-tuning configuration such as comparison with bert (devlin et al., 2019) due to the limitation of our computational budgets. thus, it is difficult to claim that the proposed strategies are also more efficient in such configuration. in addition, we have to investigate the effectiveness in a more realistic situation. for example, we will investigate the performance of the combination of our proposed method, which is the parameter efficient way for internal layers, and a parameter efficient embedding such as takase and kobayashi (2020). through experiments in various configurations, it is difficult to conclude which strategy is the best. experimental results imply that the best strategy depends on the task and transformer architecture (post-ln or pre-ln). such phenomena are reported in previous studies (press et al., 2020; gulati et al., 2020). in fact, the architecture explored by press et al. (2020) is better in the language modeling task but ineffective in the machine translation task. since it is intractable to investigate a tremendous amount of possible parameter assignment way due to the limitation of computational budgets, there might be a superior way to three simple strategies proposed in this paper. however, we emphasize that all our proposed strategies are more efficient than the universal configuration. because the purpose of our experiments is not to detect the best parameter sharing strategy but to indicate that our proposed parameter sharing strategies are more efficient than the universal configuration, we consider that our conducted experiments are sufficient to verify our claims."
1636,"limitations this study is limited to classification tasks with an encoder architecture, short texts (e.g. utterances), datasets with at least several thousand examples, and a relatively low amount of mislabeled data. in theory, we could apply our method to longer texts, but our takeaways might not directly apply. for similar reasons, our"
1637,"limitations this work has several limitations. first, we only experiment on english datasets. it would be interesting to explore whether the general patterns hold for non-english languages with different structural properties. moreover, we only explore incorporating hard constraints for decoding with local models at testing time. exploring more applications of structural constraints, such as learning with constraints, or incorporating other types of constraints, such as soft ones, would be promising future directions. finally, we only explore three simple sentence-level structured prediction tasks, while extentions can be made to more complex tasks with larger output space, such as text generation or document-level information extraction, where constraints may play more interesting roles."
1638,"limitations the two videoqa datasets used in experiments are associated with relatively short videos. therefore it would be better if more experiments could be conducted on videoqa datasets with long videos to verify the effectiveness of our approach on a wider range of videoqa tasks. although the proposed approach in this paper can also be used in other video-language tasks, our experiments focuses on a specific video-language task - videoqa. experiments on more video-language tasks are needed to show that our approach are also effective in other video-language tasks."
1639,"limitations despite the promising results of our iml pipeline for image captioning, our work has some limitations. firstly, the experiments were conducted on a domain-specific dataset, vizwiz, and may not generalize to other datasets or domains. secondly, our approach may not be suitable for scenarios where user feedback is sparse or unreliable, as the effectiveness of iml heavily depends on the quality and quantity of the feedback. thirdly, our use of episodic memory to retain knowledge from previously seen clusters may not scale well to smaller datasets and other methods may be required. lastly, our approach does not address the challenge of bias in the data, which can lead to biased models. ethical statement as of now, we do not see ethical concerns with the study presented in this paper. we used a dataset that is publicly available. the study is currently not applied to human subjects with personal data; in this case, the use of user feedback in the training process could potentially introduce biases if the feedback is not diverse or representative of the population. lastly, our approach may be used to develop image captioning models that generate harmful or inappropriate content, such as captions that perpetuate harmful stereotypes or stigmatize certain groups of people."
1640,"limitations one limitation of our study is that, due to computational constraints, we use what are now considered as relatively “small-sized” models and corpora, exclusively focusing on the english language and generic domains such as wikipedia articles and books. the generalizability of our findings to larger corpora, other languages, or specific domains such as medical texts warrants further investigation."
1641,"We foresee two limitations to our work. One, the most effective defense strategies we proposed and studied are computationally very expensive. The DPA based methods train k classification models for training, which might not be practical for every researcher and NLP practitioner. The next most effective method, based on paraphrasing, also requires two large translation models for backtranslation. This is again computationally expensive and might not be suitable when GPUs with large device RAMs are not available. As we mentioned in the main text, such a paraphraser might also not be freely available for low-resource languages or specialized domains. Second, we only evaluated the defenses on textual backdoor attacks. Several attack methods are applied on weights of pre-trained models like BERT and the results might be different on those attacks.
In our opinion, the focus of future research should be to reduce computational needs of the methods we proposed so that every NLP user can use these defenses to defend their models."
1642,"Although all pre-training approaches require a sufficient amount of data, given how we defined keywords, longer sequences suit our approach better than short ones for studying the effects of keyword selection. Further, as shown in this study, our findings strongly imply that the strategy we suggested for adapting PLMs can effectively enhance their performance on text classification as the downstream task. To determine whether these findings can translate to other NLP applications, however, further experiments are required."
1643,"The experiments reported were performed on a dataset of French sentences, with a particular organization: sequences of sentences as input, each with a slightly different structure but sharing the subject-verb agreement rule. All sentences in the input sequence are processed together. In future work we plan to separate the distillation of rules from a sentence representation from the processing of the sequence.
We have investigated only part of the parameters in the proposed architectures. In particular, the β coefficient in the encoder-decoder and the dual VAE architectures was set to 1. Higher values may lead to more disentangled representations on the latent layer."
1644,"The adversarial test sets based on masked language models can introduce new noise into the sentence context, as there is no way to automatically ensure grammatical correctness. However, there were many cases where such introduction of noise did not affect the predictions, in all three languages. Further, adversarial datasets are expected to introduce such noise, as is seen in other research on
the topic for other tasks such as sentiment analysis, and the goal of such research is also to understand model robustness in the presence some noise. It is relevant to mention in this context that the NER datasets we considered already consist of other noise and ungrammatical examples such as score cards of sporting matches (conll03-en), social media content (wnut17) and fully lowercased sentences with weakly supervised annotations (mconer21). Further, masking does not alter the entities themselves, and only changes the non-entity tokens. So, the NER models still see the same entities. While there are no established means of quantifying the quality of adversarial datasets to our knowledge, exploring human-in-the-loop approaches to select appropriate examples to include in the final adversarial test set can be one way to address the issue."
1645,"One significant limitation of our work is that we only explored the capabilities of diffusion-based language models under a challenging circumstance
where it is not allowed to use pre-trained weights or grammar parsers, which means we did not utilize this kind of model to its full potential, so a future research direction could be exploring possible ways to further improve the model’s performance by leveraging pretrained weights or word embeddings, and train with enough data to find the full potential of these models.
Another limitation of our work is that we only explored one typical diffusion-based language model, so our conclusions may not generalize to special types of diffusion-based language models (such as ones that uses discrete state space). We also conducted all experiments using the exact same model architecture design. In the future, we plan to experiment with different architectures for the diffusion model, such as more sophisticated conditioning methods (currently we just concatenate the source to the target, but we would like to try other ways of conditioning on the source, such as cross attention, as these conditioning methods for diffusion models have promising performance in the image generation domain).
Lastly, we found that diffusion-based language models work well with limited data and no external knowledge or pre-trained weights, thus these mod-
els may have great potential under low-resource settings, but we didn’t apply them to any real lowresource settings (such as low-resource languages, rare domains, etc) in this paper, and we would like to do that in the future to explore the full potential of diffusion-based language models."
1646,"We obtain notable QA performance through experiments. However, we conduct many experiments to find the optimal candidate for ContrastiveQA. Many of these experiments inevitably consume a lot of time and energy, and we have to heuristically determine the number of candidate sets through experiments in a limited environment. We intend to alleviate the current problems by adding a module that can solve these problems in our future research. For example, the Longformer model takes almost a day to process long text for each epoch to train. Therefore, we use smaller batch sizes with a limited number of GPUs to train the LongFormer model. However, due to the lack of GPU resources, the optimal weight of the proposed framework cannot be learned. Thus, it is necessary for further research on model weight reduction to mitigate computational resource problems."
1647,"Although our method has been shown effective, it has two limitations that may be improved in the future. First, the FA model has advantages in computation but relies on an effective frequency selection strategy, which is difficult to design. We
just simply select some manual frequencies for different datasets by experience. The more effective frequency selection strategy needs further exploration. Second, there is no theoretical guarantee that the orthogonal regularization can generalize to a 3-order tensor. Our OR terms are only formally consistent with matrix orthogonal regularization, which has been empirically shown effective."
1648,"Our initial work on one-shot EM shows promising results but comes with several limitations. By inspecting generated examples, we find evidence of information leakage in the LSR that causes generated sentences to be similar in topic and length to the input sentence. We also require one example at inference time in order to generate a new example for a specific word sense. We demonstrated empirically that the baseline does not have this restriction, because the baseline method generalizes well to words that were unseen during training (see Table 1). Additionally, our best approach, S2Ssemi, still requires WSD training data for pretraining of the BEM, which is used for LSR extraction. Finally, the S2S models in this paper have the limitation that the target word looks identical in both the input and output sentence, whereas the baseline is capable of generating examples with various target word forms for a given target lemma. This could be fixed in the future by lemmatizing the target word before passing it to the S2S BART encoder during training and generation, but we did not run this experiment due to our initial focus on using as little supervision as possible to perform EM."
1649,One limitation is that the importance and visual salience of character instances are not measured directly. We plan to settle these in future work.
1650,limitations of each of the methods and provided guidelines for nlp researchers and practitioners for using these methods. 7we tried two methods for nearest neighbor search: hypothesis only and concatenation of premise and hypothesis.
1651,"limitations of current approaches are discussed in the previous section. apart from this, since the paper focuses on more foundational question of evaluating ner systems in general, we do not foresee any other potential risks involved with this research. broader impact considering the number of practical usecases of ner across industries, and the growth of multilingual nlp, ner evaluation beyond english is more important than ever before. in this paper, we explored a previously unexplored space for named entity recognition, i.e., evaluating ner systems beyond english for their sensitivity to adversarial input, which will hopefully lead into better evaluation strategies when developing ner systems across languages in future. 11https://dx.doi.org/10.6084/m9. figshare.22674079"
1652,"limitation we summarize two limitations which also serve as promising directions to be explored in our future work. rada framework only considers textual domain corpus as the datastore, although this has greatly improve the coverage of domain knowledge as texts are always relatively easy to collect. however, it is widely investigated that structured knowledge such as knowledge base can also serve similar purpose. and such resources are generally in higher quality and are easier to match. therefore, it would be benefiting to further integrate such resources at certain scenario where kb is available. the other limitation regards to the scale of the rada implementation. as large language models have becoming increasingly powerful, they have demonstrated quite impressive capability in memorizing and recalling a wide range of background knowledge existed in the massive corpora they have been pretrained on. this trends of development naturally raises question for the proposed framework: will it still be beneficial when scale up to llms? and on what kinds of scenario does it brings best improvements? these are very important questions to answer, and we can certainly expect them to be explored in future works. ethical statement we evaluate the proposed method on established and publicly available datasets. there is also no human evaluation involved. this paper is not concerned with the above ethical risks. when the proposed framework is deployed into domain specific production, the domain adapted language models might express ethical-related outputs, but just as any other language models do (weidinger et al., 2021), and should be treated with according techniques to eliminate ethical risks such as bias, stereotypes."
1653,"limitation of our work is that we only explored the capabilities of diffusion-based language models under a challenging circumstance where it is not allowed to use pre-trained weights or grammar parsers, which means we did not utilize this kind of model to its full potential, so a future research direction could be exploring possible ways to further improve the model’s performance by leveraging pretrained weights or word embeddings, and train with enough data to find the full potential of these models. another limitation of our work is that we only explored one typical diffusion-based language model, so our"
1654,limitations of repurposed vision encoders and highlights the need for encoders designed specifically for v+l tasks.
1655,"limitations the main limitation of our concatenation-based multi-ve models is efficiency: the models are significantly slower than single-ve models because of the additional visual tokens in the input; the 3- ve model requires almost twice the time to train (in real time, not training steps) compared to the single-ve models. also, in cases where images are not pre-encoded, multi-ve setups are significantly slower at inference time. however, as mentioned before, we concatenate the tokens for analysis purposes only (§5) and leave more efficient alternatives like resampling (alayrac et al., 2022) to the future. several limitations could be investigated in the future, assuming access to a larger computational budget: 1. we focused on single-stream transformers and did not take into account dual-stream or other multimodal transformer architectures. 2. we only experimented with three popular ves (one version per ve class). there are many other ves we could investigate in the future. 3. we do not pre-train our multimodal models on intermediate, auxiliary multimodal tasks (tan and bansal, 2019; lu et al., 2019; chen et al., 2020, inter alia) as achieving state-of-the-art is not our goal."
1656,"limitations the proposed spc framework is model and task agnostic and can scale to different models and tasks. it is adaptable to domain/task shifts and generalizes well with low data. however, the prompt length has to be tuned for each task, and there is no principled way of determining the optimal prompt length. this work does not explore the interpretation of soft prompts or prompt ensemble. future work can investigate other loss forms derived from our spc framework, explore the combination of fine-tuning’s strong in-domain performance and spc’s cross-domain performance, and apply spc and other parameter-efficient tuning approaches to large lms. ethical statement we hereby state that our study adheres to the acl code of"
1657,"limitations kgt5-context relies on the textual mentions of entities and relations (and, optionally, entity descriptions). therefore, it is only applicable to kgs that provide such information. kgt5-context may be able to handle some entities without textual features when well-described by their neighborhood; we did not investigate this though. to use kgt5-context for prediction, the kg has to be queried to obtain context information, i.e., the one-hop neighborhood of the query entity. kgt5context thus cannot be used without the underlying kg. the verbalized neighborhood of the query entity leads to long input sequences, which in turn may induce higher memory consumption and higher computational cost during training. overall, training kgt5-context is typically more expensive than training traditional kge models, which can be tuned (kochsiek et al., 2022) and trained efficiently (lerer et al., 2019; kochsiek and gemulla, 2021; zheng et al., 2020). for inference, kgt5-context first samples relation-neighbor pairs for contextualization, and then samples possible answers from the decoder. these sampling steps can lead to variance in predictive performance. we found this effect to be negligible on wikidata5m, but it may be larger on other datasets."
1658,"limitations the use of auxiliary classifiers at every node of the decision tree is not feasible when the hierarchical tree is huge, such as the large hierarchical terminologies for medical literature indexing (gasco et al., 2021). besides, in table 1, even though the integration of the hierarchical information shows a consistent improvement in both the baseline and mimlroberta models, these improvements are still within one standard deviation of micro-f1. lastly, it is worth noting that we do not focus on large language models since our approach is to explore improvements on a published state-of-theart model. while they might improve accuracy, a careful exploration of those on a new task is beyond the scope."
1659,"limitations syng2g-tr encodes the syntactic dependency graph because the nodes of input and output graphs should be similar. future work could include investigating the use of constituency graphs in the self-attention mechanism of transformer (vaswani et al., 2017b), where the nodes of the input graph (constituency graph) are different from those of the srl output graph. in this paper, we initialise our model with the pre-trained bert (devlin et al., 2019) model. as future study, larger and better pretrained language models will be used for the initialisation of syng2g-tr models, to achieve better performance. additionally, future studies can easily extend our work to multilingual srl benchmarks. 7this leads to a bert-based syntax-agnostic model, similar to shi and lin (2019)."
1660,"limitations of bort (e.g. our explicit demarcation of phonemic and orthographic regions), and is perhaps generally well-suited for the task at hand. an additional area of future work will consist of exploring alternative training strategies. in the present work, we were quite strict with regard to preparing the test split, withholding the most frequent english words because they appeared in our aphasiabank evaluations. as our focus turns more toward downstream tasks, we will update our holdout methods to prioritize a stronger pre-trained model, holding out only as much data as needed for validation. further, seeing how our models did not train for an entire pass through the wikipedia data, we will adjust the training schedule (e.g. a ramp-up in the learning rate) and task configuration (e.g. word transform rates) to ensure we get the most out of the pre-training stage. for our aphasia-specific application, we see room to improve the noise transform with more strategic approaches. phoneme errors could be made more realistic with statistically or linguistically informed approaches (e.g. replacing phonemes with similar phonemes). to better prepare a model for semantic errors, whole-word replacements could be made with semantically similar words. limitations the models presented here were trained with the basic inventory of english phonemes found in cmudict. however, a more fine-grained phonetic analysis would require a pronunciation dictionary with more narrowly defined entries. additionally, while this paper focused on models trained with english-only resources (pre-trained bart-base, english wikipedia text, cmudict, and the english aphasiabank), the techniques should be applicable to non-english language models as well. finally, from a clinical standpoint, the model we describe in this paper assumes the existence of transcribed input (from either a manual or automated source, discussed in detail in § 2.1); in its current form, this represents a limitation to its clinical implementation, though not to its use in research settings with archival or newly-transcribed datasets."
1661,"limitations exist when using certain methods. we also demonstrated how prefix tuning may have more difficulty learning sentence representations of augmented data. we further showed that contrastive learning can be a solution to these issues, and suggest that more work be done to find similar methods that can be more generalizable. limitations although we present our results across multiple datasets and models, we have largely adhered to natural language understanding tasks. how effective the augmentation methods are on generationrelated tasks warrants further study. in addition, another limitation is that we do not study the level of perturbations needed to have a negative effect on prefix tuning. knowing this can shed light on what particular types of transformations are a problem for prefix tuning so that they can be avoided. our hypothesis here is that such perturbation levels are sensitive to many factors during training, such as the difficulty of the task and the characteristics of the data augmentation approaches applied, which could be difficult to be quantified."
1662,"limitations the first limitation of rse comes from data sparsity. unlike word-level relational data, the variety of sentences is much larger than words. therefore, even though we can collect more and more relational data on sentence pairs, it is generally hard to become densely connected between sentences. to alleviate this issue and generate sustainable sentence relational data, we should design automatic tools for sentence relation labelling with human-inthe-loop supervision. second, unsupervised sentence embedding also shows decent performance on semantic textual similarity tasks, and the focus is mainly on the design of unsupervised contrastive samples. therefore, exploring different unsupervised view-augmentation techniques in relational sentence embedding remains an open question. in this work, we only study incorporate dropout augmentation without much"
1663,"limitations this paper hypothesizes that seq2seq models have token-level overfitting and underfitting issues, and provides direct evidence to support the hypothesis in various settings, raising a valuable problem for nlp modeling. however, this paper does not provide a solution to the problem due to the theoretical and practical challenges of measuring the convergence speed of each token. we leave the exploration of this topic to future work."
1664,"limitations the ses in this paper are mainly transformer-based ses, and we are not sure whether the observations hold for other ses. however, considering that transformer-based ses dominate the current nlp community, we think it is fine to only evaluate 59 transformer-based ses. another limitation is that the sentences in heros are converted from reddit, which is an online forum and the texts on reddit may be more casual and informal. this makes the sentence pairs in heros tend to be more informal. users should note such a characteristic of the sentence pairs of heros and beware that the results obtained using heros may be different from the results obtained using more formal texts. an additional limitation is that there can be more diverse rules to create different sentence pairs other than the six subsets included in heros, and our paper cannot include them all. as a last limitation, during the construction of heros, we remove sentences that are ungrammatical based on language-tool, so our results may not generalize to ungrammatical sentences."
1665,"limitation this major limitation of sen2pro lies in the computational cost due to the generation of several samples. thus, improving the efficiency of sen2pro is a future direction. besides, since we choose to concat the representation of different samples, there may be more natural ways to merge information from samples."
1666,"We present a method for question answering using a KGQA retriever and a language model reasoner. Limitations of our method include a lack of an integrated entity resolution system when training our KGQA model: we instead rely on annotated entities from the datasets. While our KGQA architecture is robust to new entities added at test time, it does require retraining when new relations are added to the KG or if a different target KG is used. Additionally, our results are based on training and evaluating on one dataset at a time; training on a mix of datasets could lead to better generalization, however this is not tested."
1667,"Although we showed significant improvements in explanation generation using prompt-based few-
shot learning, our work still has some limitations. First, we experimented only on the e-SNLI dataset: although e-SNLI is a reference for the task, it would be interesting to extend the proposed methodology
to other datasets with natural language explanations (see Wiegreffe and Marasović (2021) for an extensive review).
Second, we did not attempt to automatic prompt optimization: although this may bring further minor improvements, we decided to leave optimization to a next step, as it does not change the core contribution of our work.
Third, we believe there is an intrinsic limitation in comparing our results with SOTA, as there is not a clear consensus on which metric is to be taken as the reference metric for benchmarking, along with the fact that measures sometimes disagree on scoring one system better than another. We hope that in the future this task and its evaluation will consolidate into a shared benchmark.
Finally, as for our our use of e-SNLI, we are assuming that for all sentence pairs in the dataset there is an implicit explanation of the semantic relation between the sentences. Under this assumption we always generate an explanation, even when the explanation is already explicit in one of the sentences. We think that a better capacity to detect those cases would bring relevant insight to our approach."
1668,"Our approach requires access to planning information for each instructional text domain. In general, creating this information requires programming and domain knowledge to formally specify the planning constraints. However for high-value applications the effort associated with generating these planning domain definitions may be justified by their potential to help in generating more valid plan-based semantic parses. Having this knowledge is also crucial to allowing an agent or robot
to execute the resulting plan and may be naturally available in many domains as part of the execution component. In the course of developing our semantic parsing model, we discovered that Codex could generate valid planning domain definitions in a variety of output formats including the Planning Domain Definition Language (Fox and Long, 2003). This may provide a path towards automatically generating planning domain definitions for novel environments or reducing the need for human annotators. Future work could also evaluate our method in other planning domains that contain tasks beyond cooking such as VirtualHome (Puig et al., 2018) or ALFRED (Shridhar et al., 2020)."
1669,"The proposed Reasoning Circuits framework intends to replace the need for thousands of annotated examples with a strong inductive bias of structured rationales. There is two issues with this approach at a conceptual level: 1. It may not always be possible to break down a multi-step reasoning problem cleanly into discrete reasoning steps, and another related issue it increasing complexity of the circuit with the complexity of the task. 2. For the design of these reasoning circuits a researcher must develop a thorough understanding of this reasoning task, so that the final circuit design broadly covers all possible types of reasoning problems expected to be solved. An under- or illdesigned reasoning circuit may cause the system to either not support a certain portion of problems or produce non-sensical outputs.
Essentially, there is trade off between a tighter control over reasoning by investing in a deep understanding of the problem leading to a comprehensive reasoning circuit design and lower annotations budget, versus, less control over logic and depending on a large number of annotations which allow the model to discover this logic on its own at much higher cost of large scale annotations budget.
At the implementation and operations level one of the the key limitations our proposed system is the number of inference steps to solve the problem. The number of times model inference may be needed to solve a single example is equal the length of the longest task sequence chain in the reasoning circuit. One possible solution for this could be by training the model to solve the entire problem by generating all the steps of reasoning and the target string in a single inference step and could massively reduce inference time and costs."
1670,"In this section, we faithfully discuss the current limitations and potential avenues for future research.
First of all, the generation performance of our knowledge-augmentation framework largely depends on the efficacy of retrievers. In other words, if the retriever fails to retrieve the relevant facts to the input question, the prompted LLM, conditioned on the irrelevant facts, is likely to generate the incorrect answer (See Figure 3). Similarly, if the retriever is not designed to retrieve the facts in 2-hop neighborhoods of the question entities, LLMs are less likely to generate the answer requiring 2-hop knowledge. Note that, for the Mintaka dataset (Sen et al., 2022), the number of answerable questions with 1-hop facts is only 40% of total samples. However, when we include 2-hop triples, the number of answerable questions becomes 62%, which suggests the necessity of 2-hop retrievals, which is yet challenging (See Table 2). Thus, future work may improve the retrieval scheme itself to provide more accurate facts including multi-hops to the LLM, or may develop the mechanism to prevent the LLM from being misled by unrelated facts.
On the other hand, the evaluation metric for the generation performance of prompted LLMs may be further improved. Specifically, regarding our target KGQA tasks, the answer for the question is the entity in KGs. However, the prompted LLMs without additional training (i.e., zero-shot) tend to generate the answer as the sentence. For instance, the
label entity for the question (e.g., Where did Alex Chilton die?) in Table 4 is ""New Orleans"", however, the LLMs often generate the sentence-level output: ""Alex Chilton died on March 17, 2010 in New Orleans, Louisiana due to a myocardial infarction"". We currently evaluate the model performance by measuring whether generated tokens contain the answer entity or not; however, it would be worthwhile to develop the additional metric to compare the sentence-level output from LLMs to the word-level answer in KGs in a more effective way. Note that we also try other available metrics (See Appendix B.3), such as F1 and Exact Match (EM) scores (Rajpurkar et al., 2016), however, they largely penalize the longer sentences (e.g., EM of correct examples in Table 4 are 0), thus may not be appropriate for evaluating LM prompting schemes.
Lastly, since we focus on the improvement of knowledge injection in LM prompting, we use the labeled entities in KGQA datasets when evaluating models, following the existing KGQA evaluation setups (Cohen et al., 2020; Sen et al., 2021). However, in real-world applications where the entities in the question are mostly not provided, we first need to extract entities in the question with existing entity linking techniques; therefore, our model performance depends on the efficacy of entity linking. In particular, regarding the result with entity linking in Table 5, the portion of answerable questions from labeled entities in the dataset is 40%, however, the portion of them with entities from the entity linking model (Ayoola et al., 2022) is 22%. Therefore, since the improved entity linking performance would contribute to the performance gain of our KAPING framework, for KGQA tasks, future work may advance such the entity linking scheme."
1671,"While the proposed methods are attractive due to their efficiency, explainability and not needing training data, the limitations are also manifold: The pipeline nature propagates all errors that occur. For instance, the dependency parser in use performs rather poorly on informal texts such as tweets. Further, our definition of positive and negative effect relations is quite shallow and does not always live up to the real world’s complexity. We only capture effect relations that are formulated explicitly within one sentence, and only one effect relation per sentence. Requiring the nodes to link to Wikipedia might be too restrictive while not even truly solving the problem of filtering non-sense nodes. Both the low inter-annotator-agreement in our effect graph evaluation as well as the discrepancy of the crowds’ and the expert’s annotations make it hard to assess the correctness of the extracted effect relations. And lastly, while we showcase some generated explanations, we did not properly evaluate how reliable the approach is in finding reasonable explanations. Indeed, first results suggest that this approach of generating explanations works rather inconsistently, though the ranking helps to a certain degree.
What one might consider another limitation is that we do not check the effect relations for factual correctness, which ultimately leads to contradictions and inconsistencies in the effect graph. While fact checking is a difficult and controversial task, we also purposefully decided against any form of fact or consistency checking. Each edge in the effect graph is meant to represent one effect relation exactly as it was expressed. Including critical effect relations in the graph allows for identifying, analyzing, and potentially disproving them."
1672,"limitations one significant limitation to our study is that, as of march 23rd 2023, openai has deprecated access to code-davinci-0024, thus rendering our results non-replicable for any team not granted special access to these models by openai. we did not anticipate this deprecation while conducting this work and we believe this raises serious questions about the usage of api-based language models in scholarly work. another limitation is that the 12 tasks we selected may not be representative of the broader population of natural language tasks. had we conducted our experiments on a larger selection of tasks there may have been larger-scale trends that we would have been able to uncover. the largest and most pressing limitation with our work is that the models we are testing on have closed-source pre-training datasets. thus, we are unable to verify the extent to which our task datasets have been included in the training or instruction fine-tuning data. given that the training data for most of the models tested in this work cuts off in late 2021, this is a very strong possibility. our results should be viewed with this limitation strongly in mind. finally, while we experimented with different code prompts, the search space of possible prompts is very large. thus, it is very likely that there exists some prompt that outperforms our chosen prompts for each task. drawing"
1673,"limitations our experimental setup excludes free-text rationales explaining the decisions of a model (wiegreffe et al., 2022; camburu et al., 2018), because their output is not based on attribution scores or highlighted spans of the input text, so we argue that they are not trivially comparable. however, there are end-to-end rationalization frameworks that can accommodate arbitrary saliency methods (jain et al., 2020; chrysostomou and aletras, 2021; ismail et al., 2021; atanasova et al., 2022; majumder et al., 2022), but require large language models that are expensive to train and perform inference with, so this is out of scope for this study. however, we also see that high-quality free-text rationales can be more easily generated with llms (wang et al., 2023; ho et al., 2023), and a comparison between them and our attribution-based explanations is an interesting avenue for future work. inferring high-quality explanations from large language models necessitates excessive amounts of compute and storage. although gpt verbalizations are most promising, we urge the research community to look into more efficient ways to achieve similar results. in the future, we will explore if training a smaller model on top of the collected rationale-augmented verbalizations is feasible. emphasizing the concerns of rogers (2023), we do not recommend the black-box model gpt-3.5 as a baseline for interpretability, because the model’s training data or internal parameters can not be accessed and the dangers of deprecation as well as the lack of reproducibility are serious con- cerns. however, we do think it has revealed great potential as a surface realization and contextualization tool for the task of saliency map verbalization. the causality problem explained in jacovi et al. (2023a) is not solved by our verbalizations, as it is an inherent problem with feature attribution and rationalization. future work includes verbalizations alongside counterfactuals, e.g. in interactive setups (feldhus et al., 2022; shen et al., 2023). although multiple models and explanationgenerating methods are available, we specifically focus on one pair for both datasets (bert and integrated gradients), because the focus of our investigation is on the quality of the representation rather than the model. finally, explicitly modelling expected highlights to mitigate misalignments as reported on in schuff et al. (2022), jacovi et al. (2023b) and prasad et al. (2021) is still unexplored."
1674,"limitation of existing lm prompting schemes, which rely on the static knowledge internalized in parameters; therefore, when such knowledge are incomplete, inaccurate, and outdated, llms may generate factually incorrect answers. to tackle this challenge, we introduced a novel knowledge-augmented language model prompting (kaping) framework, which augments the knowledge for the input question from kgs directly in the input prompt of llms, with the fact retriever to inject only the relevant knowledge. the proposed framework is completely zero-shot, and versatile with any lms, without additional parameter updates and training datasets. we validated that our kaping yields huge performance gaps from the lm prompting model relying on its internal knowledge, especially with smaller lms, on the kgqa tasks. we believe our new mechanism for augmenting facts from kgs to the lm prompt will bring substantial practical impacts in generating knowledge-grounded answers."
1675,"limitations concepts in this work, we extract the concepts from semi-structured explanations whose format reassures consistency and non-ambiguity of the exploited concept(s). the selection of datasets and corresponding concepts is primarily conditioned by data availability, as the semi-structured explanations are available merely for a small set of datasets. we acknowledge that our selection of concepts is not representative for a vast variance of concepts that users might expect models to learn from context in interaction. some important concepts’ features that we identify are following: (i) a number of premises or reasoning inference steps needed to map the input to output, (ii) the granularity of the reasoning steps, (iii) a type of the premises; for instance, whether the familiarity with a given concept requires a memorization of an entity property (such as “sun emits light”), or a reasoning mechanics such as analogical reasoning (“if animals can run and cat is an animal, then a cat can run”). we invite future work to identify or propose a taxonomy that would better reflect the wide variance of reasoning concepts that models are expected to comprehend in order to serve a wide scope of unseen tasks. such taxonomy can motivate a more targeted collection of concepts from explanations, or annotation of new explanations demonstrating new concepts. models we acknowledge the limitation in a variance of evaluated models given by their availability and our computational possibilities. we evaluate only two models of the gpt family due to the usage limits of openai api. outside gpt models, we do not evaluate models over 20b parameters, given the infrastructure requirements of such settings. nevertheless, we argue that the relevance of the models with constrained access, or resource requirements exceeding the limits of most organizations also remains a subject of open question. datasets one should note that the sizes of our evaluation datasets, for which we are able to extract concepts from explanations (fig. 2), are too small to compare concept sensitivity between models. the sizes of our sensitivity evaluation datasets are the following: worldtree: 2,204 samples, openbookqa: 792, glue diagnostics: 282 samples, hotpotqa: 182 samples. ethical considerations & broader impact as outlined in section 1, in-context learning recently presents a research direction of broad public interest, where the outstanding results on nlp benchmarks often do not meet the users’ expectations. it is understandable that the focus of development in in-context learning llms goes to measurable improvements on existing benchmarks, as ecologically-valid evaluations (de vries et al., 2020) on end use-cases are timely and challenging to compare to related work. nevertheless, in this highly-exposed and fastpaced direction, we identify the necessity for the emergence of fast proxy measures that can shed light on the decision-making of the llms as expected by their end users. the presented evaluation of models’ sensitivity to demonstrated reasoning concepts introduces a technical framework for quickly assessing models’ compliance with our expected functioning; however, a selection of a comprehensive set of concepts that we can agree our models should be able to learn remains a subject of open"
1676,"limitations while our study provides valuable insights into the impact of finetuning on reasoning performance and the role of explanations during finetuning and prompting with respect to various reasoning skills, there are several limitations to our work. firstly, we only consider a single llm, opt, as our base model. our results may not generalize to other llms with different architectures or pretraining objectives. secondly, we only use a limited set of reasoning datasets for finetuning due to the limited availability of open-source datasets with explanations. however, it is possible that our findings may not hold for models finetuned on larger closed datasets as usually seen in real-world scenarios. thirdly, our experiments only cover a limited range of model sizes due to limitations in computational budget, therefore it is possible that our findings may not hold for much larger models. finally, we only consider finetuning using fewshot prompting conditions in our experiments, and it is possible that our findings may not hold for models finetuned without in-context exemplars. overall, while our study provides valuable insights into the impact of finetuning and explanations on reasoning performance, further research is needed to investigate these factors across a broader range of models, datasets, and finetuning strategies."
1677,"limitations while this work explores the impact of the typical compositional modifiers on entailment relations, we did not consider other fine-grained information that further captures upward or downward monotonicity from the monotonicity calculus of the premise/hypothesis sentence pairs. further, the dataset that we generated is relatively small, at approximately 1,300 sentences. we also did not evaluate the dataset over t5, bart, gpt-x, and other state-of-the-art llms, which may provide more insights. we also did not conduct any evaluation for explanations and interpretation of the evaluated nli models, which could be future work. lastly, we did not include a comparison with existing datasets that were created specifically for negation modifiers and universal & existential quantifiers. we see all these issues as exciting avenues for future work."
1678,"Although LLM-EVAL has shown promising results in assessing open-domain conversations, it is crucial to acknowledge its limitations.
Firstly, the performance of our method relies heavily on the large language models underlying it, which may exhibit biases or generate unexpected outputs. If the language model misinterprets the evaluation schema or prompt instructions, it could lead to inaccurate evaluation scores.
Secondly, the choice of LLM significantly influences the evaluation results, as demonstrated in our analysis. While dialogue-optimized LLMs produce better performance, this selection may limit LLM-EVAL’s applicability for particular tasks or dialogue systems.
Thirdly, our approach employs single-number scoring for each evaluation dimension, which may fail to capture the subtleties of human judgments, particularly for subjective aspects like engagement, creativity, or humor.
Lastly, the effectiveness of LLM-EVAL hinges on the quality and clarity of the prompts and evaluation schemas. Creating such prompts and schemas may require domain expertise and knowledge of LLM behavior, posing challenges for non-experts.
To overcome these limitations, future research can focus on exploring alternative prompt designs, refining evaluation schemas, and expanding the method to cover a wider range of evaluation dimensions and dialogue system types."
1679,"Although cTBLS enhances LLMs with tabular knowledge to generate grounded responses, certain limitations remain to be addressed.
Firstly, the efficacy of cTBLS is constrained by the total number of knowledge sources employed during the augmentation process. Token length restrictions in the OpenAI API limit the knowledge augmentation to the top three cells of the table. Another limitation is the incapacity of cTBLS to handle queries pertaining to the entire table. Figure 4 demonstrates one such instance in which the state tracker module accurately retrieves three rows of the table corresponding to oil and gas industries, yet the response generation module fails to utilize this information when transforming the retrieved state into a response. Generally, cTBLS encounters difficulties with counting, comparing the values of cells, and other mathematical operations, an issue we aim to address in future research."
1680,"The proposed framework has a limitation in terms of the large GPU resources required, as it necessitates double the memory compared to training a CRS alone. Due to this limitation, we have to forego the use of pre-trained language models such as BERT, which could have been beneficial in enhancing language quality, but their extreme memory requirements make it infeasible."
1681,"One of the main limitations of our approach is the use of machine translation to create the IESEMPARSE suite. However, we showed that the overall quality of our dataset is comparable to Samanantar, a human-verified translation dataset. Furthermore, previous studies Bapna et al. (2022); Huang (1990); Moon et al. (2020a,b) have shown the effectiveness of quality estimation in referenceless settings. Lastly, we have also extensively evaluated our dataset with the help of 3 human evaluators for each language as described in §3. We can further take help of GPT4 in future to evaluate the translations in a scaled manner (Gilardi et al., 2023).
The second point of discussion focuses on the motivation for preserving logical form slot values in English. We explore the use cases where querying data in English is crucial, and how this approach can enhance models by reducing latency, limiting vocabulary size, and handling system redundancy. While open-source tools currently cannot achieve this, it would be valuable to evaluate the effectiveness of this task by comparing it with the other two discussed approaches. To accomplish this, we suggest using a dialogue manager and scoring the performance of its responses on the three TOP approaches outlined in the paper.
Another potential limitation of our dataset is that it may contain biases and flaws inherited from the original TOP datasets. However, we contend that spoken utterances are generally simpler and more universal than written ones, which mitigates the risk of cultural mismatches in IE-SEMPARSE dataset. Furthermore, our work is confined only to the Indo-Dravidian Language family of Indic languages due to our familiarity with them and the availability of high-quality resources from previous research. Nonetheless, our approach is easily extendable to other languages with effective translation models, enabling broader applications in various languages worldwide. In the future, we plan to improve our datasets by publicly releasing them through initiatives like NLLB or IndicTransV2, and by collaborating with larger organizations to have the test sets human-translated."
1682,"limitations the dataset used in this work is in italian and there may be language-specific limitations in the model performance. geppetto is the only candidate for auto-regressive models for the italian language at the time of this research. therefore, its performance may be limited due to the small number of parameters. we were unable to experiment with it5-large model due to computation power limitations."
1683,"limitation on the length of input that a model can handle. situational information can typically be obtained from various sources, and often, an excessive amount of information is present. humans can quickly focus on crucial information and discard the rest, otherwise, it would take forever to read, process, and reason over surrounding information. researchers have identified the frame problem (mccarthy and hayes, 1969) that describes the dilemma of a reasoning system in determining which aspects of a situation change and which remain constant after an action. to date, there has been no satisfactory solution to this questions, making the challenge of situated conversation an interesting open challenge. common ground: knowledge about situations is closely related to common ground–the information shared by conversation participants. without common ground, conversation participants would need to convey every parameter of their message, which is extremely inefficient. the importance of common ground is widely recognized, and decades of dialogue research have been devoted to developing systems that can effectively establish common ground with their interlocutors by inferring, presenting, requesting, accepting, and repairing individual beliefs about various information through conversations (traum and allen, 1994; clark, 1996; poesio and rieses, 2010; inter alia). in this paper, we did not delve into the problem of common ground, but the consideration of situations, which is our main proposal, is the first step towards computational modeling of grounding."
1684,"limitations firstly, we did not address the fundamental challenge of determining an adequate amount of situational information. it is very difficult, if not impossible, to describe all the situations required to perform rationale reasoning, so we need to give up somewhere, relying on the reasoning capability of nlp systems. secondly, we did not use large-scale data or conduct an extensive search for optimal hyperparameters and prompts (for gpt-3) in our experiments as the primary goal of this study was to raise attentions to potential issues and benefits associated with situational informaiton. the models may have performed better with different configurations. we did not examined the capabilities of larger plms in conducting situated conversations at scale. in our empirical analysis, we opted for gpt-3 due to its transparency about technical details compared with later versions of gpt. finally, while situational information can aid in the development of truthful and creative response generation systems, it does not address well-known issues associated with conversational technologies, such as safety and bias. in fact, poorly chosen situational information may even amplify undesired bias by linking two irelevant concepts together. to mitigate this problem, researchers and developers should exercise caution when collecting data and carefully monitor system output."
1685,"limitations as mentioned in table 3, one limitation of the current study is the small scale of the test sets with modern parsers. we encourage future work to emphasize the development and evaluation on these test sets, specifically those which more closely reflect the current sota in text-to-sql (e.g. t5). additionally, though we have shown using an auxiliary schema prediction model greatly improves the performance of a text-to-sql system, the addition of a model for the text-to-sql task is a limitation given the time and training resources required."
1686,"limitations our work is limited in the following senses. first, all presented results relied on the ground truth number of intents to initialize the number of clusters for conducting k-means to retrieve prototypes (§3.1) and infer latent intents (§3.4). in practice, however, the ground truth number of intents is unknown and needs to be estimated by examining a subset of utterances. however, our ablations in §5.2 investigated the impact of overestimating the number of ground truth intents by a factor of two, and found that idas’s performance did not degrade much. while we did not explore this for the final k-means to infer latent intents, future work could investigate cluster algorithms that do not require the number of dialogue states as input, e.g., 2https://github.com/maarten-deraedt/idas-inten t-discovery-with-abstract-summarization. dbscan (ester et al., 1996), mean shift (comaniciu and meer, 2002), or affinity propagation (frey and dueck, 2007). second, we generated labels with the gpt3 (175b) text-davinci-003 model, which may be prohibitively expensive and slow to run for very large corpora. in our initial experiments, we tried using smaller-sized models such as text-curie-001, text-babbage-001, and text-ada-001, as well as flan-t5-xl (chung et al., 2022), but found that the generated labels were of lower quality compared to those of text-davinci-003. in future work, it would thus be interesting to further explore how to more effectively exploit such smaller-sized and/or opensource language models."
1687,"limitations the main limitation of our proposed method relies on the additional cost of retrieval. even if the size of our character style indexes is small it still adds latency to our overall pipeline as retrieval must occur once per token. we expect that incorporating the recent work of he et al. (2021) on improving the efficiency of nearest neighbor language models should decrease this latency significantly. as in most nlg work, another important limitation is in quality evaluation. we found qualitative evaluations to be too imprecise for appropriate inter-annotator agreement, and the quantitative evaluations that we present in this paper are all proxies that cannot be said to capture character style or fluency in full. another limitation of our work is the exclusion of models that are only accessible by calling or finetuning powerful external language model apis due to the excessive monetary cost involved. it is almost certain that these larger models would outperform the 6b parameter model we use, and this may also change the relative performance of the techniques that we present. while we feel that this constraint is appropriate at this moment in history and that our position as major aaa developer gives us the authority to make such a claim, shifts in third party model availability and pricing could change the landscape. our work deals with data of a singular domain, video game scripts in english, but represents a wide variety of nationalities and ethnicities over the span of a large catalog of games."
1688,"limitations are. current directions with promising results include using llms for conversation synthesis (wei et al., 2023; chen et al., 2023), where high-quality multi-party conversations are synthesized through prompting, and the conversations can be grounded in specific characters or personas. such synthesized conversations may also help adapt methods for conversation analysis and response generation to rarer domains that may not be well-represented in natural corpora."
1689,"The dataset used in this work is a personal narrative corpus in English collected in-vitro (e.g. subjects in a lab setting). Further work will be needed to extend it to other languages, genres, and naturalistic conditions. The reproducibility of the annotation task may be subject to variability due to the fact that the task is done by five internal annotators and not through crowd-sourcing techniques."
1690,"Due to various constraints, our experiments are only able to cover five non-linear novels and nine linear novels, listed in Table 7. This pales in comparison to the thousands of novels typically expected of large-scale studies in digital humanities, whose scale allows them to make generalizable claims regarding narratives or literary history (Piper et al., 2021). We hope to make up for this gap in our future work. One key challenge to scaling our dataset would be data availability. The use of non-linearity in fiction is predominantly a 20th century phenomenon, which suggests that many non-linear novels will not be in the public domain for some time to come.
In terms of experiment design, an important limitation of the quantitative evaluations in Section 4.2 is its assumption that a novel’s chapter divides provided by its author could be thought of as a form of “gold standard” labels for model validation. This claim of authorial control and “authority” over the text has been thoroughly problematized in literary studies since the emergence of poststructuralism (Barthes, 1967; Foucault, 1969), while analogous suspicions have been raised in natural language generation against the assumed reliability of human evaluators (Clark et al., 2021). Unfortunately, the author’s input is the only operationalizable criteria for ground truth available to us within the scope of this study."
1691,"limitations that need to be acknowledged. first, our approach relies on a reductive representation of the narrative texts, overlooking all traditional stylometric measures. the perception of literary quality is an intricate concept that relies on numerous factors, ranging from the stylistics, characters, plot development and pace, to cultural contexts. by reducing each narrative text to a subset of chosen features, our approach inevitably discards much of the richness and subtlety of works, while the narrow range facilitated by goodreads’ scores forces the models to discern nuanced differences in perceived quality among texts that may be considered generally good by readers. this clearly limits our understanding of literary quality, especially when it comes to the more linguistically or stylistically virtuous titles. secondly, the reliance on goodreads scores as the sole metric of quality introduces bi- ases, as these scores are inevitably influenced by factors such as genre preferences and reader demographics. finally, the analysis is based on a limited sample of english-language texts from the 19th and 20th centuries, potentially limiting the generalizability of our findings to other periods, languages, or contexts. for the same reason, our study cannot consider the potential impact of translation and its effect on the reception of the texts. at the same time, given the inherent complexity of these constraints and the subjective nature of literary evaluation, the performances achieved by our models in terms of r2 scores and mean squared errors, which would be modest for easier tasks, can be considered rather promising. naturally, there is much that can be done from here. in the future, we intend to compile an even larger data set, in terms of both texts and features. integrating stylometric and syntactic features, for instance, could provide additional insights into the complex nature of literary quality. furthermore, we plan to investigate genre-specific patterns, as observing the performance of our models across different genres may reveal unique patterns and relationships that are specific to particular types of literature. finally, we intend to use more diverse and sophisticated metrics than goodreads: exploring alternative sources such as anthologies, awards, and canon lists. leveraging a richer set of indicators for literary quality/qualities, we hope to gain clearer insights into the complex interplay of factors that contribute to the perception of literary quality."
1692,"limitations corpus. in this paper, we use fiction novels from multiple languages in project gutenberg. one assumption of this work is that the text is representative of the culture surrounding the language. while this may or may not be true (e.g. handler and segal, 1999), our investigation’s focus is on the structure, or narrative arc, of stories and how arcs may differ across languages. naturally, our findings may differ for other genres, such as history or self-help. we focus on fiction because the vast majority of research on narratives has focused on fiction, though we believe non-fiction and other genres would be interesting for future work. future work can also consider the addition of other corpora to enhance project gutenberg, such as megalite moreno-jiménez et al. (2021), a corpus of about 5,000 spanish, french, and portuguese narrative texts, poetry, or plays. however, multilingual corpora of this kind are few and far between, even for high-resource languages like spanish and french. dictionaries. this work heavily relies on liwc, which is proprietary software. many researchers (including ourselves) may not have access to all liwc dictionaries. in addition, as a dictionary of psychometric properties, liwc is constantly evolving and improving with new research in psychology and linguistics."
1693,"limitations this paper presents the conceptual foundations of a novel architecture for narrative-based language understanding, along with an illustrative proof-ofconcept implementation. as such, it has been operationalised on a small scale only. scaling up the approach to real-world applications is a highly non-trivial task that would not only require large investments but also significant innovative research efforts. moreover, important aspects of the theoretical model have not been included in the proofof-concept implementation, in particular when it comes to modelling the confidence of an agent with respect to its beliefs and narratives."
1694,limitations this is a position paper thus we do not see what the potential limitations could be. the only potential limitation might be the incompleteness of the list of relevant publications.
1695,"limitations some of the limitations of this dataset include that of much of the time periods and locations given are simply approximations of the time period that the work is actually set in; this is most notable in the case of library of congress and wikipedia labels which make up the majority of the work. these datasets offer more coarse-grained settings of a work, such as years and geolocation, which have limitations for some purposes. an additional limitation is that the works are in english and also are more commonly set/written in the west, which should be taken into account when used for analytics."
1696,"limitations several limitations relate to the study setups. one shortcoming is the small number of comics used in the experiments. while this is adequate for initial exploration into animate entity identification, these results cannot be generalised - comics in particular have an incredible number of potential types of animate entities and beings, and four comic stories do not touch on most of them. another limitation is the reliance on crowd-sourced recruitment and remote annotation. the researcher is not able to instruct the annotators in person and check their understanding of the annotation scheme. although the annotation task are seemingly relatively simple at this point, word-of-mouth recruitment of annotators who are more familiar with annotation processes are likely a better choice in future work, especially as the annotation scheme develops to include more complicated concepts. more significantly, the outlining method for identifying animate entities does not capture inanimate entities that become animate, as discussed in section 1.1. an annotator using the scheme tested here would not outline the sofa in figure 1, although the sofa should be included in an annotated corpus to capture an important update to the reader’s mental model. while these experiments show that these updates are somewhat obtained through interpreting reader feedback about their outlines, the limitations in developing an annotation scheme solely from the reader’s perspective are apparent. developing a comparable annotation scheme from the creator’s perspective may facilitate fuller analyses of narrative structures. since a creator knows that the sofa and kamala kahn are linked through coreference, both would be outlined and given the same reference label. integrating these two perspectives into one corpus could give insights into how creator’s intentions to communicate larger narrative structures are expressed in lower-level configurations of image and text."
1697,"limitations of our pipeline’s output on mrs. dalloway shown in figure 1, we follow the approach of wang and iyyer (2019) to present the outcomes for literary close reading alongside quantitative metrics."
1698,"limitations since dall•e mini is trained on english-language material, and since our input text is english only, our proposed methods will only be able to measure the imageability of english isolated words and connected text. the text-to-image model we use, dall•e mini, requires gpus or tpus to generate images. while we used 4 gpus (see section 4 for more details) to obtain the results in this paper, we were able to use a single gpu to successfully run the same experiments with longer runtime."
1699,"Despite its promising results, our study has limitations. Our model primarily works with English text, limiting its applicability to other languages. Its focus on sentence-level extraction doesn’t consider document context, which could be investigated in future research. The employed training dataset is relatively small, potentially not encompassing all possible event types, thus affecting the model’s performance and generalizability. Additionally, our two-stage inference framework, while enhanced by a ranking module, is prone to error propagation. If a trigger isn’t identified in the first stage, its associated arguments cannot be extracted. Future work should address these issues for improved performance and broader applicability."
1700,"Our experiments focus solely on English-language entity linking. Similar models have been trained to perform entity linking in multiple languages (De Cao et al., 2022), but we do not consider performance beyond English. The issues faced in other languages are likely to be similar, but the multilingual element of other models might lead to different results. Further, how to select keywords in the multilingual setting is unclear.
In addition, we are limited by the available annotated entity linking datasets. Given that we need a large amount of data to train these models, they are inherently reliant on Wikipedia. These entity linking datasets are skewed towards specific types of matches, including ones that are frequently exact matches. The effectiveness of this model might change when trained on a dataset with different characteristics, even with a large amount of data.
Finally, the computational resources required to train these models are large, and our final results do not reflect numerous other preliminary experiments. This restricts our ability to run multiple experiments, train models from scratch easily, and potentially leads to underfitting of our final models."
1701,"In this section, we faithfully discuss the current limitations and potential avenues for future research.
First of all, the generation performance of our knowledge-augmentation framework largely depends on the efficacy of retrievers. In other words, if the retriever fails to retrieve the relevant facts to the input question, the prompted LLM, conditioned on the irrelevant facts, is likely to generate the incorrect answer (See Figure 3). Similarly, if the retriever is not designed to retrieve the facts in 2-hop neighborhoods of the question entities, LLMs are less likely to generate the answer requiring 2-hop knowledge. Note that, for the Mintaka dataset (Sen et al., 2022), the number of answerable questions with 1-hop facts is only 40% of total samples. However, when we include 2-hop triples, the number of answerable questions becomes 62%, which suggests the necessity of 2-hop retrievals, which is yet challenging (See Table 2). Thus, future work may improve the retrieval scheme itself to provide more accurate facts including multi-hops to the LLM, or may develop the mechanism to prevent the LLM from being misled by unrelated facts.
On the other hand, the evaluation metric for the generation performance of prompted LLMs may be further improved. Specifically, regarding our target KGQA tasks, the answer for the question is the entity in KGs. However, the prompted LLMs without additional training (i.e., zero-shot) tend to generate the answer as the sentence. For instance, the
label entity for the question (e.g., Where did Alex Chilton die?) in Table 4 is ""New Orleans"", however, the LLMs often generate the sentence-level output: ""Alex Chilton died on March 17, 2010 in New Orleans, Louisiana due to a myocardial infarction"". We currently evaluate the model performance by measuring whether generated tokens contain the answer entity or not; however, it would be worthwhile to develop the additional metric to compare the sentence-level output from LLMs to the word-level answer in KGs in a more effective way. Note that we also try other available metrics (See Appendix B.3), such as F1 and Exact Match (EM) scores (Rajpurkar et al., 2016), however, they largely penalize the longer sentences (e.g., EM of correct examples in Table 4 are 0), thus may not be appropriate for evaluating LM prompting schemes.
Lastly, since we focus on the improvement of knowledge injection in LM prompting, we use the labeled entities in KGQA datasets when evaluating models, following the existing KGQA evaluation setups (Cohen et al., 2020; Sen et al., 2021). However, in real-world applications where the entities in the question are mostly not provided, we first need to extract entities in the question with existing entity linking techniques; therefore, our model performance depends on the efficacy of entity linking. In particular, regarding the result with entity linking in Table 5, the portion of answerable questions from labeled entities in the dataset is 40%, however, the portion of them with entities from the entity linking model (Ayoola et al., 2022) is 22%. Therefore, since the improved entity linking performance would contribute to the performance gain of our KAPING framework, for KGQA tasks, future work may advance such the entity linking scheme."
1702,"Although our dataset presents a significant advancement over previous benchmarks, it is still limited in that it only contains entities already known to Wikidata. One could argue that the very long tail is what is even beyond Wikidata.
In the second stage, our method harnesses an LM pre-trained for entity disambiguation. Therefore, our methodology, in its current form, cannot predict objects that are not already known to that LM and its underlying KB."
1703,"limitations while there is a lot of work on creating and making available large pre-trained language models for a range of languages, there is to our knowledge not that many knowledge graphs for other languages than english — especially general knowledge ones, like conceptnet. this is a major limitation, as it restricts research to one single language and the structured representation of knowledge found in the culture associated with that specific group of language users. creating commonsense kgs from unstructured text is a costly process that requires financial resources for annotation as well as available corpora to extract the graph from."
1704,"limitations of existing methods. our model uses a pretrained language model module and graph neural network module to jointly represent event graphs. in addition, we make the event embedding space geometrically meaningful by imposing two constraints on event embeddings: event temporal order should be in accordance with event embedding norm, and event temporal relations should only exist between events whose embeddings are close enough. experiments demonstrate that our method significantly outperforms baselines by generating accurate and globally consistent temporal event graphs. in the future, we aim to incorporate external background knowledge and commonsense knowledge into our framework. we also plan to make use of the generated temporal event graphs in downstream tasks, such as future event prediction and question answering. limitations in the current design setting, our proposed model is only able to classify temporal relations between event pairs into one of three classes: before, after, and no relation. our model should be more practically useful if we can extend it to predict more relation types in addition to temporal relations, such as parent-child and causecaused_by relations. we believe that our model is able to make such extension without too much modification. in addition, as mentioned in the previous section, our model does not make use of any external knowledge, e.g., commonsense knowledge of event temporal relations. our framework should be more powerful to deal with domain-specific articles if utilizing such knowledge in the framework. ethical considerations we acknowledge that our work is aligned with the acl code of the"
1705,"limitations domain shift: in the current implementation, our prompting model relies on the availability of a training set. this assumption may not hold in cases where the relations to be discovered exhibit a significant domain shift from the training set. to address this limitation, future work should explore fully unsupervised prompting approaches that can better adapt to new domains and mitigate the impact of domain shift. limited number of relations: in this study, our analysis is restricted to a total of 25 relations. while this allows for a focused exploration of these specific relations, it also limits the scope and potential applications of our model. to broaden the applicability and effectiveness of our approach, future work should aim to utilize wikidata more com- prehensively, incorporating a larger number of relations for more extensive and diverse analysis."
1706,"limitation of existing lm prompting schemes, which rely on the static knowledge internalized in parameters; therefore, when such knowledge are incomplete, inaccurate, and outdated, llms may generate factually incorrect answers. to tackle this challenge, we introduced a novel knowledge-augmented language model prompting (kaping) framework, which augments the knowledge for the input question from kgs directly in the input prompt of llms, with the fact retriever to inject only the relevant knowledge. the proposed framework is completely zero-shot, and versatile with any lms, without additional parameter updates and training datasets. we validated that our kaping yields huge performance gaps from the lm prompting model relying on its internal knowledge, especially with smaller lms, on the kgqa tasks. we believe our new mechanism for augmenting facts from kgs to the lm prompt will bring substantial practical impacts in generating knowledge-grounded answers."
1707,"limitations we focuses on resolving various mentions from different domains. although we have tested our framework on multiple datasets, it relies on a humanannotated dataset and effort should be taken to investigate how the model performs with emerging domains without human-annotated data. our model works with mentions that have been extracted from raw text. it would be more practical if the model could work with raw text directly and interact with another mention-extraction module. the performance of the model is largely affected by the surface form of the mentions, although our framework is robust to variations in the surface form, it would be more beneficial to further investigate how adversarial turbulence in the mentions could affect the behaviors of the framework."
1708,"While we have done our best to create high-quality evaluation data, there are limitations that should be kept in mind when using these datasets. It is known that creating translations by post-editing may bias data towards the output of the MT systems used for initial translations; however, many transcription and translation vendors now exclusively use postediting rather than translation from scratch and so direct translation may not be an option in all cases. This could influence metrics toward similar MT systems. The presented evaluation sets are moderately sized compared to datasets in other domains with plentiful mined data, and may be best used in conjunction by reporting on both the development and evaluation sets for statistical significance. The evaluation sets also have a necessarily limited set of speakers which may not be fully representative. Systems which tune to the development set run the risk of over-fitting to specific speakers or content. We do not perform a comparison to human evaluation here, but refer interested readers to the IWSLT’23 evaluation campaign findings paper which runs this comparison for a variety of systems with the ACL 60/60 data (Agarwal et al., 2023)."
1709,"There are several limitations of this study. First, it is done on one language pair although we believe this should not qualitatively change the results. Second, only one set of standard model sizes was evaluated for AST student and NMT expert; we expect it be in line with reported findings for NMT (Ghorbani et al., 2021). Finally, while alluding to the potential of using large pre-trained ASR models instead of manual transcripts for IL-based AST, our current work must be seen as a proof-of-concept experiment where we train ASR models on a few hundred hours of audio, and discard the manual transcripts in IL training, showing the feasibility of our idea."
1710,"The data that we analyzed are limited to only one English-German language pair, 5 SST systems from IWSLT 2022, and three domains. All the systems were trained in the standard supervised fashion on parallel texts. They do not aim to mimic interpretation with shortening, summarization or redundancy reduction, and they do not use document context. The used MT metrics are good for evaluating individual sentence translations and that is an important, but not the only subtask of SST. We assume that some future systems created with a different approach may show divergence of CR and the offline MT metrics.
Furthermore, we used only one example of human interpreting. A precise in-depth study of human interpretations is needed to re-assess the recommendation of translation or interpreting as reference in SST."
1711,"The scores reported in the SI test were lower than those in the offline test. Reporting results on other SI data would support seeing the effectiveness of our method. To our knowledge, this is the first work to use SI data as speech translation data. There are no other language pairs SI data than EnglishJapanese pairs those source speech and target text aligned."
1712,"Apart from all the advantages that our work achieves, some limitations still exist. Firstly, in this work, we investigate the efficacy of applying our proposed DePA approach on the representative vanilla NAT, the highly competitive fully NAT model GLAT and current SOTA CTC-DSLPMT for fully NAT models, but we have yet to apply DePA to iterative NAT models, such as Imputer (Saharia et al., 2020), CMLM (Ghazvininejad et al., 2019), and Levenshtein Transformer (Gu et al., 2019). Hence, the effectiveness of DePA on iterative NAT models still needs to be verified. Secondly, we have not yet incorporated reranking approaches such as Noisy Parallel Decoding (NPD) (Gu et al., 2018) into DePA. Thirdly, our proposed method FBD requires multiple additional training phases before NAT training, resulting in longer training time and using more GPU resources. Reducing the computational cost of FBD training is one future work that will be beneficial for energy saving. Last but not least, NAT models have limitations on handling long text. They suffer from worse translation quality when translating relatively long text. We plan to investigate all these topics in future work."
1713,"Our training schedule introduces a language discriminator loss to impose constraints on the intermediate translation in the back-translation period. The experimental results suggest that our method can alleviate the copying problem when the involved languages are distant language pairs or lack training data. However, for language pairs that are not distant, and especially high-resource languages, our model does not show improvement over the baseline. Due to time and resource limitations, we do not further explore whether the optimal weight
for the language discriminator loss can have a connection with the size of the dataset and the involved language pairs. For example, for WMT En-De or En-Fr pairs, the languages are not distant language pairs and therefore we might obtain better results if the weights are slightly smaller. We believe that future research could explore this direction: to adapt the weight to different language pairs and the size of the training data. In addition, we do not conduct hyperparameter search for other hyperparameters, instead directly using suggested values.
In this work, we propose a novel training schedule that tries to address the copying problem, which is common among distant language pairs in UNMT. We experiment with high-resource languages English, German, French, Russian and Chinese, and low-resource languages including Gujarati and Kazakh. The training data we use is monolingual text extracted from online newspapers and released for the WMT series of shared tasks. As far as we know, all the monolingual corpora do not contain any metadata and therefore it would be unlikely that anyone can use the concerned data to attribute to specific individuals."
1714,"limitations of these self-supervised pre-trained speech models while fine-tuning them on downstream speech translation tasks. acknolwedgements we are thankful to the organizers of the iwslt 2023 low resource and dialectal shared tasks. this work was generously supported by nsf grant iis-2125466 and by a meta sponsored research award. we are also thankful to the office of research computing at george mason university (https://orc.gmu.edu), funded in part by grants from the national science foundation (awards number 1625039 and 2018631), for the computing resources we used to train our models."
1715,"limitations while our method is effective in zero-shot settings, we find that it has limited implications in supervised settings. this is because improving zero-shot translation presents a tug-of-war between language-agnostic and language-specific representations, each of which has a distinct effect on the model. another major downside is reduced training speed relative to the baseline many-to-many model. we note that this is an artifact of the agreement loss (kldiv.) which entails two forward-passes for each update. finally, in the present work, we compute k-nns for every source word in a sentence. although this has yielded strong results, we would like to explore a more explainable setting where k-nns can be applied to specific source words. we leave such explorations to future work."
1716,"limitations and future work since our auxiliary target-side lm decoder is spawned with the same configuration as the mt decoder, this significantly adds to the model size at training time. this makes it difficult to scale/slower to train with translation models of large size. while this problem can be easily mitigated by using a gpu of larger memory, we would like to explore more efficient ways of incorporating the target context which we leave for future work. secondly, even though our method gives a significant boost to translation quality in the early latencies, it relies on the mma (ma et al., 2020) policy that has some limitations in terms of latency because of a suboptimal decision making using multiple heads (indurthi et al., 2022). while our policy shows improvement, it could be further optimized, for instance, in following reference alignments more closely which would have a positive effect on latency. finally, using additional monolingual data is also a viable direction for future work to strengthen the language model used in the approach."
1717,"limitations the proposed method has several limitations that should be taken into consideration when employing it. first, the method relies on an existing model, e.g., k-means, which creates a dependency between the performance of the initial and the robust models. second, the flow is not trained end-to-end, which can also limit its performance as end-to-end training allows improvement of the robustness of the whole representation. lastly, to fully assess the effectiveness of the method, multiple metrics need to be examined. this can be a limitation as interpreting the results from multiple metrics may not be straightforward. however, it gives a more complete picture of the model’s performance."
1718,
1719,Left blank.
1720,"The performances except for the proposed tasks. We presented the result of neologism and the performances on two downstream tasks (i.e., word similarity task and short text classification), which are closely related to the understanding of word semantics. The selected downstream tasks are challenging for the contextualized models; they can use only a few contexts to make a representation.
The performance in general benchmarks (e.g., GLUE) is almost the same as the vanilla BERT because our model suffers catastrophic forgetting while learning definition information. Sophisticated modeling and training processes to overcome the problem could be interesting future work.
The use of other models Other pretrained models like RoBERTa could be a base model of our method (e.g., DefRoBERTa). However, we think that BBPE tokens scarcely have semantic meanings, which makes it hard to find appropriate tokens to inject definition information. Therefore, integrating human-written definitions with other types of tokens (e.g., Byte-Pair Encoding and Byte-level BPE) is also a future direction.
The use of all the loss function& Collect more definition data. Presenting more experiments with other models, other collections of definition data, and other loss functions will further support our idea. Nevertheless, we want to show the performances with the widely-used basic model of pretrained language models (i.e., BERT), using definition data from the previous work, with various loss functions (e.g., W-D, D-E, [+W’], [+E]) as many as possible. A fine-grained combination of all the loss functions could make further improvements."
1721,"There are several perspectives from which we need to consider the ethical considerations of this work.
Privacy: Lifelogs are personal data and should only be used and shared given user authorization. The lifelogs presented here are fictitious and do not reveal the personal information of any individual. No personal data is used to create this benchmark. This work is intended to unlock development in the creation, maintenance, querying and usage of lifelogs, and additional work will certainly be needed to ensure that they are secure and being meaningfully and responsibly used.
Comprehensiveness and diversity: We recognize that the lifelogs generated in this work are far from representing the full range of human experiences. While we strived to make the lifelogs complex enough to benchmark and compare current stateof-the-art, these lifelogs would not be considered diverse in the sense that a social scientist would note, and are likely biased by the life experiences of its creators. We encourage future work in creating lifelogs that are more inclusive and faithful to all walks of life. This includes further work in making lifelogs that are more diverse in terms of life experiences, personas, time scales, and queries as well as more granular and complex in detail. The strength of the benchmark is in identifying patterns of questions on lifelogs rather than the specific events described in them.
Inferring episodes: TimelineQA is a collection of time-and-space boxed episodes, and not the raw data itself from which the episodes are inferred (e.g., a wedding photo, or video snippet from smart glasses). Naturally, more research would need to be devoted to understanding how to extract important information in natural language and infer episodic events from this raw data before performing question answering. As mentioned previously, this also involves sometimes grappling with the linguistic variation amongst the language used in the episode description and the query itself.
Intended use: We clarify that the benchmark should not be used to train models for making key decisions that will impact people’s lives (e.g., job matching, insurance approvals or building personal assistants). The intended use of TimelineQA is as a benchmark to reveal potential limitations of QA systems over lifelog data. Even if the benchmark is determined to be sufficiently comprehen-
sive, a detailed study should be conducted to understand the potential representational harms of using TimelineQA before using it for training models. Conceivably, TimelineQA can also facilitate research in evaluating the biases of QA systems by creating counterfactual pairs in the dataset: two timelines which are exactly the same, but differ by the demographic group or a specific life event (e.g., having dropped out of college or committed a crime). The QA system can then be systematically probed for differences in performance between the two timelines."
1722,"Our approach to universal text perturbations suffers from linguistic inconsistency, which makes them easier to detect. Therefore, as the next step of our research, it would be interesting to investigate
the possibility of improving the naturalness of adversarial triggers without degradation of the attack performance in terms of the fooling rate.
While the proposed approach outperforms the UATs of Wallace et al. (2019) in the transferability task, we should highlight that the additional hyperparameters adjustment plays a crucial role, and one could suggest validation procedure refinement for a more fair comparison. Also, for both direct and transferability settings, a more comprehensive range of models should be examined, including recurrent (Yuan et al., 2021) and transformer architectures, e.g., T5 (Raffel et al., 2020), XLNet (Yang et al., 2019), GPT family models (Radford et al., 2019; Brown et al., 2020).
Another direction of improvement is related to the fact that sometimes the found triggers can change the ground truth label of samples they are concatenated to if, e.g., they contain words contradicting the true sense of a sentence. It would be interesting to analyze how often this happens and develop an approach to tackle this issue.
Finally, it would be interesting to investigate the dependence of attack efficiency on the size of a training set and compare it with the so-called data-free approaches, such as the one proposed by Singla et al. (2022)."
1723,"In this work, we first formulate the scene-robust NLVL problem and propose our solution. However, our generalizable NLVL model is still tested on existing close-world datasets, and the actual performance in real-world scenarios needs to be further explored. A real-world, large-scale dataset is required to develop a practical, generalized, openworld query-based video retrieval model."
1724,In the Sec 6 and Appendix.A
1725,"Time Granularity: The granularity of time we test DynaMiTE on ranges from spans of four years to months. After testing multiple ways to bucket our temporal corpora, we observed that the granularity of time only affected DynaMiTE when there were insufficient documents in each time step. Specifically, we found that there must be at least 100 documents per time step to expect reasonably good results. Runtime: One drawback of DynaMiTE is that its runtime depends on the number of terms required at each time step. However, this can be avoided by mining more than one term during each iteration of the framework. We also observed that DynaMiTE, along with all other dynamic topic mining baselines, had a slower performance on datasets with longer text documents. Risks: DynaMiTE is intended to be used as a tool to discover topic evolutions in temporal corpora suited to a user’s interests, represented as category seeds. We only experimented with DynaMiTE in domains with trustworthy information. If DynaMiTE was used in document collections that contain misinformation, it could have the potential to mine inaccurate terms."
1726,Section 8
1727,"There are majorly two limitations: Firstly, we collect a Chinese singing voice dataset and test our method only on this Chinese dataset due to the difficulty of recruiting professional singers in different languages. In the future, we will attempt to collect the singing voices dataset including more languages and test our method in multilingual settings. Secondly, our method adopts the diffusion model in pitch modeling and the postnet, which require multiple inference steps. We will try advanced acceleration methods for diffusion models in the future."
1728,"The present study is limited to exploring biases in MLMs for the gender dimension only. For future work, important dimensionalities can be explored, especially for non-western contexts like Caste, Ethnicity, etc (Ahn and Oh, 2021; Bhatt et al., 2022). We also used Machine Translation on English counterfactuals to obtain CDA data in each language in our dataset. Translations are prone to errors and issues like Translaionese (Gellerstam, 1986), especially for the lower resource languages, and therefore can lead to the unreliability of the quality of generated counterfactuals were generated. In the future, we would like to explore learning generative (Wu et al., 2021) or editing models (Malmi et al., 2022) for automatically generating gender counterfactuals given text data in different languages. This can help us scale our counterfactual generation process to a much higher number of samples while also avoiding any losses in quality that may arise due to machine translation. Our multilingual DisCo metric is currently limited to 6 Indian languages and we hope our work will inspire further extension to cover different language families for improving the focus on multilingual biases evaluation."
1729,"Section 6
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
1730,"Our proposed graph-guided SQL generators are superior in generating complex SQL queries. How-
ever, the model has a large number of parameters and requires more computational resources, which is a common problem with current methods of generating complex SQL queries.
In addition, the proposed knowledge-enhanced re-ranking mechanism is proposed to leverage the knowledge in PLM to choose the best SQL query from the beam output. However, it does not take into account the database schema which can be the source of domain knowledge.
In the future, we will design lighter models for complex and cross-domain text-to-SQL generation and explore some other re-ranking mechanisms to incorporate the prior knowledge of database schema."
1731,Section 7
1732,"Domain KGs are the premise of KnowledgeDA, while open and high-quality domain KGs may be rare in some domains. Therefore, the method will be limited in the domains without suitable KGs. Besides, we use a similarity-based method to map entity mentions in the text to the corresponding entities in the KG. Although this method performs efficiently, it ignores the problem of entity ambigu-
ity (Vretinaris et al., 2021). For instance, the abbreviation, CAT, can stand for ‘catalase’ or ‘COPD Assessment Test’ in healthcare. To address this problem, it is necessary to use contextual information to clarify the specific meaning of the mention (Phan et al., 2017; Orr et al., 2021; Vretinaris et al., 2021). Last but not least, KnowledgeDA may be not good at tasks of paragraph-level texts and the efficiency will reduce. Because long texts probably contain more entity mentions and have more complex syntax, it is more difficult to retrieve the entities and acquire their relations from the KG."
1733,Limitations
1734,"The scope of this work is limited to sentence-level detection due to the absence of any span-level annotated datasets for hyperbole detection. Also, we could only partially annotate the metaphor datasets due to resource constraints. Finally, we did not try sophisticated large language models in our work as our goal was to demonstrate the effectiveness of multitasking using a simple model, rather than to test the performance of more sophisticated models."
1735,Limitations
1736,Limitation section at the end.
1737,"Our work has focused strongly on the formal aspects of BPE. NLP practictioners should not be dissuaded from using BPE for subword tokenization, despite our presentation of examples where greedy BPE fails. Indeed, in contrast to synthetic examples on toy alphabet, on real data we made an observation that greedy BPE may be close to optimal."
1738,"For web-augmented models including our work, the deterioration of search results from search engine highlights the importance of deriving an effective method to interact with the huge web. Search engines are often perceived as black-box and non-transparent for end users. Therefore, many works proposed “leaning to search” to decompose complex questions into simpler queries, which may improve the performance of web-based models (Nakano et al., 2021; Komeili et al., 2021).
In our model, we used a commercial search engine as the retriever to work with the whole web as a knowledge source. Since the web is not curated and well-structured like Wikipedia, we may encounter unexpected safety issues, including misinformation and harmful contents. While we have relied on the security control of the search engine, more attention should be paid to better understand the risks and provide effective ways to mitigate them. We hope our simple approach and strong results could encourage more future work by the community to tackle these questions. To encourage the community to investigate the question and ensure reproducibility, after the reviewing process, we will release the search URLs used in our experiments.
As for the potential concern, since we use the search engine to access real-time information, we do not have a tight control over retrieved results as traditional end-to-end retrieval (Guu et al., 2020; Lewis et al., 2020b). Not only the changes of search engine logic, but also the newly published information, might create discrepancies over the course of time. This is also an issue we have to tackle to build a stable web-based solution for PLMs."
1739,"Although our proposed method is effective in three vision-language tasks, we still have some limitations. Firstly, we utilize T5 to convert the questionanswering format into the declarative sentence in VQA and it works well in most cases, but it still faces out-of-coverage problems, which will affect the following zero-shot prediction of CLIP. We need to design more rules for these special cases for better conversion. Secondly, our clustering algorithm for SNLI-VE can achieve strong zero-shot performance, but the clustering centroids are close to each other and the algorithm is sensitive to these centroids. The robustness of this algorithm should be improved. What’s more, we leverage FasterRCNN in visual fine-grained information extraction, so the detectable object attributes and classes are constrained in a relatively limited object set of Faster-RCNN, which may hinder further improvement from visual fine-grained information. The Faster-RCNN can be replaced with a better vision module. Besides, since we only utilize CLIP in our paper, we can explore the zero-shot ability of other
contrastive pre-training models in future work."
1740,"The current work does achieve better performance than previous methods, but processing only one slot type at a time also reduces the efficiency of the model. In the future, we will explore how to maximize model efficiency. It would be an interesting challenge to generate answers for all the slots at once without degrading the effect of the model. Also, we will also try to apply our framework to more scenarios, such as NER and other tasks to explore the adaptability of the proposed method.
ACL 2023 Responsible NLP Checklist"
1741,Section 4(line 288 293) and Appendix E.
1742,"Compared to vanilla prototypes, the advantage of HyperProto would also rely on the additional radius parameter. Under the 1-shot setting, however, hypersphere prototypes will face challenges in estimating the radius in support sets, this is because the initial radius may be biased by the randomness of sampling. When the radius is set to exactly 0, the model will resemble a traditional prototypical network. Nevertheless, although not as large as the boost in the multi-shot setting, we find that having a consistently optimizable radius parameter at the training stage in the 1-shot scenario still delivers non-trivial results and exceeds most baselines (Table 1, Table 2, Table 3). This further points to the positive influence of the added radius parameter to learning prototype representation and hints on the possible research direction in learning a transferable radius in 1-shot scenario."
1743,"As shown in Table 1, our method is experimentally demonstrated to be effective for two LLMs. However, OPT, a decoder-only model, is more suitable for the prompts generated by Co-Prompt. This seems to be because T0, the encoder-decoder model, requires a separate generator such as GPT2. The performance of prompts may vary to the generator involved in the vocabulary and training process. Also, there is a trade-off between search time and performance. While increasing the beam size and the number of document-query pairs enhances the probability of finding a more optimal
prompt, it makes the search time proportionally longer."
1744,"I discuss the limtations at section ""Limitation""."
1745,"Despite showing non-trivial improvements in the multi-hop capabilities of T5 models, our work has multiple limitations.
Restricted to 2-hops First, we chose 2WikiHopMultiQA (Ho et al., 2020) as our primary dataset since it uniquely maps each question to a chain of triples that contain the precise, noiseless single-hop knowledge required to answer the question. However, this comes at the cost of our analyses only being restricted to 2-hops (though see arguments by Press et al. (2023, sec 3.5) who suggest 3-and4-hop questions to be too convoluted to understand even by native-speakers). Nonetheless, our random walk training method is general by definition, and can be extended to multiple hops, though its effectiveness on QA tasks requiring more than 2-hops of reasoning remains to be measured.
Knowledge Graph size Our focus in this paper was to allow models to chain together their internalized knowledge in order to answer complex 2- hop questions. However, this critically requires them to possess the world knowledge required to answer the questions, for which we had to memorize the KG constructed using the structured triples provided in the dataset. This trade-off between focusing on knowledge composition vs. fully encoding world knowledge restricted our KG to be small in size (only 98,284 entities and 29 relations), which could be impractical in most real-world applications. In future work, we will experiment with larger sized KGs (Vrandečić and Krötzsch, 2014), by adding a substantially larger amount of additional triples to the existing KG, and measure their impact on multi-hop reasoning.
Lack of diverse QA tasks Finally, we were unable to consider popular datasets with CBQA versions such as TriviaQA (Roberts et al., 2020), NaturalQuestions (Kwiatkowski et al., 2019), etc., due to their lack of links from questions to structured knowledge. Future work can apply entity and relational linking techniques (Balachandran et al., 2021; Agarwal et al., 2021) in order to augment such QA datasets with (possibly) noisy links to structured knowledge, which will allow us to paint a more holistic picture of our methods. Additionally, this would also overcome the above limitation (of KG size), as it would substantially increase the amounts of entities and relations to be encoded
within models.
Implications for Larger Models Although we show clear improvements in triggering 2-hop reasoning in the largest T5 LM (T5-XXL), with 11B parameters, contemporary work has shown that multi-step reasoning capacities naturally emerge in LMs that are two or three orders of magnitude larger (Brown et al., 2020; Chowdhery et al., 2022; Wei et al., 2022b,a). However, these LMs benefit from examples in-context (especially since tuning them is non-trivial and expensive), and therefore it is unclear whether our methods can improve such models’ capacities even further. We have not tested such LMs in our work, due to resource limitations."
1746,
1747,"Regarding data collection, we crawled the English WikiHow website from Jan 2021 to May 2021. The number of available activities is limited by the data we crawled from WikiHow. We currently only
choose Gardening and Crafts categories as case studies. Because we focus on multimedia imagestep pairs, we remove steps that are not attached to any illustrative images. We also observe that a small portion of activities in the dataset do not follow chronological order.
Since our task focuses on the daily stereotypical tasks which usually require the model to understand the visual environment, the model design can be directly applied to support other domains, such as steps in the cooking videos. In addition, our model can also adapt to scenarios without visual images because the performance of our model only decreases slightly if no caption is provided. We plan to expand our model to other categories written in other languages."
1748,"The model might generate incorrect nouns because of the occurrence of patterns (e.g., “refrigerate the slane for up to 1 year” instead of “refrigerate the purslane for up to 1 year”). In addition, our model sometimes tends to generate generic step descriptions because of insufficient input information, e.g., given the last step “lay the t-shirt out on a clean, flat surface.”, the model generates “cut the shirt out” which is vague compared to ground truth “carefully cut around the sleeve”. Moreover, the pretrained model might focus more on language modeling instead of inherent logic: for the activity of “make paint can planters”, after “removing the label” from the paint can, the BART+CAP generates “read the label”. In addition, there is still a small chance that the model generates the same output for various similar inputs.
Because we rely on image captions and retrieval results for step prediction, the upper bound of our generation quality is limited by the performance of the image caption and sentence retrieval modules. Our framework also needs to improve on imbalanced topics in the dataset. For example, the dataset contains more activities about tree for the gardening domain than other gardening-related plants. Because our multimedia generative script learning is a new task, we cannot compare our model with other established state-of-the-art models. Moreover, because WikiHow is a crowd-sourcing website, some everyday activities might have better human annotations than the remaining activities. We plan to include a fine-grained human written step prediction as an upper bound to address this issue."
1749,"The automatic metrics we chose, including BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014), BARTScore (Yuan et al., 2021), self-BLEU (Zhu et al., 2018), and unique n-grams (Fedus et al., 2018), might not be the best metrics to evaluate our results. Some other metrics, such as semantic similarity and multimodal-retrieval based metrics, are based on pretrained models, including Augmented SBERT (Thakur et al., 2021), SentenceBert (Reimers and Gurevych, 2019), and CLIP (Radford et al., 2021). Those metrics might not align with human judgment and might be biased toward pretrained datasets. While we complement it with human evaluation, we only focus on relevance to ground truth and diversity. Although we found fluency is not an issue, it is likely we still need to cover all aspects of generation results."
1750,"While our approach is shown to be effective in improving the zero-shot adaption ability of these PLMs, the scope of this work has only been extended to English languages and has not been tested on other languages. In addition, another limitation of this work is the scope of the aspect. Aspect is defined across 3 main categories of intent, sentiment, and topic in the work. However, given the massive space of text label interpretations, our aspect range can be refined and expanded even further, lending to more analysis of the stability of implicit & explicit training as the number of aspects grows. We do not investigate this scenario in this work."
1751,"Like other controllable text generation methods (Dathathri et al., 2019; Krause et al., 2021; Liu et al., 2021; Lu et al., 2022; Arora et al., 2022; Adolphs et al., 2022), CLICK also relies on automatic neural classifiers when constructing DCL in some tasks (language detoxification in § 3.1 and sentiment steering in § 3.2 in our work). It may unavoidably inherit the biases and limitations of these classifiers. For instance, for the task of language detoxification, the toxicity may be overestimated when the input prompt or the continuation contains minority identity mentions. To address this limitation, we conducted human evaluation for all the tasks, which further confirms the effectiveness of CLICK. As more accurate, inclusive, and reliable classifiers are built (e.g., for toxicity detection), we expect that CLICK would inherit those improvements as well."
1752,"There are still some limitations of our work. In the future, we plan to enhance the procedure of extracting candidate keyphrase, to improve the upper bound of the performance of keyphrase extraction. One possible way is to generate candidate phrases of the document by utilizing the high-level semantic relatedness (e.g., attention weights) instead of using the surface-or syntactic-level information."
1753,"We collect the dictionary from the Internet, and although we make effort to reduce replicate explanations, there is noise in the dictionary. Besides, not all the words are included in the dictionary. In other words, the quality and amount of entries in the Chinese dictionary are to be improved. Additionally, our method is pre-trained on the Bert-like transformers to enhance the corresponding PLMs, and can not be applied to LLM directly whose frameworks are unavailable. In the future, we will use the retriever for disambiguation and dictionary knowledge infusion to LLM."
1754,limitation section
1755,"Although INSTRUCTOR significantly improves the baseline GTR performance, we were only able to use four negative examples during the model finetuning process due to computation constraints. However, negative examples have been shown to play an important role in contrastive learning (Robinson et al., 2021). We hope that future work will scale up the number of negatives used during finetuning and investigate various methods for mining hard negatives. Additionally, we do not have enough computation resources to apply multitask instruction finetuning to GTR-XXL (4.8B parameters), which is also an area for future exploration.
At the core of INSTRUCTOR is the instruction design. While our current unified instruction format has demonstrated effectiveness, future research can explore other instructional elements to further improve performance. For example, previous work (Wang et al., 2022) have shown that incorporating demonstration examples and explanations can be beneficial for instruction-finetuned language models."
1756,"section 7, after conclusion.
7 A2. Did you discuss any potential risks of your work? Models and datasets are all open-sourced and used consistently with their intended use. We do not see potential risks beyond these open-sourced artifacts."
1757,Left blank.
1758,"While we show that C2A successfully improves the effectiveness and efficiency of PEFT in FL, we have mainly focused on improving the effectiveness of the vanilla adapter. However, it is an open question whether our framework can improve other PEFT approaches, such as prompt tuning(Lester et al., 2021), and LoRA (Hu et al., 2022). Although we didn’t analyze whether our framework can generate parameters for alternative PEFT, one recent approach reveals that hypernetworks can generate parameters for various types of PEFT in multi-task learning (He et al., 2022; Üstün et al., 2022). Likewise, as C2A generates parameters with hypernetwork, we believe that C2A is highly expected to improve the performance of any alternative PEFT modules.
Ethics Statement
This study covers work that utilizes PLMs, which have a wide variety of positive applications, such as the application to summarization, or language understanding. At the same time, there are a number
of ethical concerns with PLMs in general, including concerns regarding the generation of biased or discriminative text (Bordia and Bowman, 2019), the leakage of private information from training data (Carlini et al., 2021), and the environmental impact of training or tuning them (Strubell et al., 2019).
Our framework attempts to train PLMs with minimal changes made to their pre-existing parameters in FL scenarios. Our work is believed to bring some insights into the two ethical dimensions: privacy and environment. First, with respect to private information leakage, although our work has not addressed address the privacy issue in the pre-train process, our FL framework can mitigate the data privacy issues in the fine-tuning stages. In addition, with respect to environmental impact, our work may obviate the need for full fine-tuning, which may also significantly reduce the cost in terms of memory or deployed servers."
1759,"Many of the reviews that were gathered for constructing BANGLABOOK are discarded because they lack a corresponding rating. A manual annotation process would have yielded a much larger dataset, which was not feasible due to resource constraints. Moreover, one of the challenges for validating the dataset is the lack of statistical models and word-embeddings pre-trained on the Bangla language. Some pre-trained Bangla-BERT models, yet to be trained on extensive corpora, have only recently been proposed. Improving transformer-based models for Bangla can enhance sub-word level contextual understanding which will consequently help in more accurate identification of the sentiments in BANGLABOOK (Islam et al., 2022)."
1760,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
1761,"Cooking recipes constitute a single genre within ProcDocQA, with a well-grounded task and large range in RISK OF HARM and user EXPERTISE. Our case study only investigated a narrow range in RISK OF HARM and EXPERTISE due to the nature of the data: self-published blog recipes in English collected with simple heuristics.
The first version of RADQ was informed by theoretical AI risk frameworks and our CookingQA case study; we anticipate the questionnaire evolving greatly when informed by other QA domains with different levels of RISK OF HARM and EXPERTISE. This work only considers immediate risks to humans; longitudinal risks such as the propagation of information are an open research topic.
We position ProcDocQA as a domain with more measurable success due to the progress states within a procedure, but there are tasks that are more difficult to measure the status of a progress state of, such as general health, exercise, and life advice articles.
This work contributes to risk mitigation by concretizing risks in user-aware scenarios. Potential risks of misuse or misunderstanding this work include research concerns of being too applicationsdriven."
1762,"Although introducing chart value prediction objective, it only provides minor improvement to the model’s performance on doing complex reasoning. There is still a large room to improve the model’s capability in math calculation. Our model also suffers from the noisy OCR prediction of off-the-shelf object detector, whose performance will depend highly on the extracted OCR text qualities. Another possible limitation of our approach is the quality of the pre-training data, which only contains synthetic images. Although the proposed model works fairly well on the ChartQA dataset, it is unclear if the improved performance can be generalized to other realistic chart images."
1763,"SoftMV is specifically designed for cross-lingual natural language inference. We believe that some of the ideas in our paper can be used in other tasks of XLU, which remains to be further investigated by subsequent research.
In addition, we conduct experiments on the XNLI dataset which consists of 15 languages. SoftMV outperforms the baseline methods under the cross-lingual transfer settings. However, the cross-lingual ability of SoftMV on other languages, especially those lacking relevant datasets, needs to be verified in future work."
1764,Section 7
1765,"The biggest limitation of our framework is that persona prompting increases the input length of the model, increasing inference time. Through the analysis of real online user logs, the inference time will be 1.82 times that of the original. In the future, we will optimize the inference performance by compressing the length of the prompting part."
1766,Section 8
1767,"In our experiment, we presented results for three diverse sequence-to-sequence tasks, namely, machine translation, text summarization and knowledge graph question answering. While for these three tasks, we managed to observe common trends (i.e. some methods consistently outperformed other methods) a more large-scale study of various sequence-to-sequence tasks is needed to further confirm this observation and robustness of the best-performing method as identified in this work."
1768,"Although the proposed method achieves significantly improved OOD detection performances compared to the baselines, but POE can not be applied to a naive LSTM, and RNN because our OOD construction is based on an attention score of the PLM. We leave this issue for future work, but we believe that our proposed method can be used in various NLP tasks as PLMs are now adopted in most fields of NLP tasks. While we adopted a masking method using attention scores in this paper, it is not clear that tokens with high attention scores have the most direct impact on the model’s predictions (Wiegreffe and Pinter, 2019). To provide readers with more information, we include additional experimental results in the Appendix to discuss the impact of different masking strategies on OOD detection performance."
1769,We have discussed the limitations in Section Limitations.
1770,"We identify the following limitations for our work:
1. Due to limited access to a wide network of native speakers from the majority of languages, we were able to manually inspect only a subset of languages present in our pretraining data. Specifically, we could only manually evaluate Afrikaans, Yorùbá, Igbo, Hausa, Luganda, Kinyarwanda, Chichewa, Shona, Somali, Swahili, Xhosa, Bemba, and Zulu. Future work should focus on increasing the subset of languages evaluated manually in order to ensure quality. We believe automatic analyses are not sufficient before development of models that get deployed in particular applications.
2. Another limitation is related to our inability to perform extensive analysis of biases and hateful speech present in our pretraining data. Again, this is due to relatively restricted access to native speakers (and even automated tools) to perform this analysis. As a result, we cannot fully ensure that our models is free from biases and socially undesirable effects. Therefore, it is important that these models be used with care and caution, and be analyzed for biases and socially undesirable effects before use.
3. Additionally, due to unavailability of sufficient computing resources, we were unable to evaluate large language models such as BLOOM, even though it covers 22 African languages.
4. Finally, even though AfroNLU has diverse tasks at the word and sentence level, these tasks only cover few African languages. We therefore encourage the creation of more datasets for downstream NLU tasks in more (and more diverse) African languages. We believe broader benchmarks will continue to be important for future progress in African NLP."
1771,"Due to the absence of large evaluation databases, we only evaluated our method on three publicly available datasets that can be used for the MSMO task. The popular video databases, i.e., COIN and Howto100M datasets, can not be used in our task, since they lack narrations and key-step annotation. So a large evaluation database is highly needed for evaluating the performance of MSMO approaches.
As the nature of the summarization task, human preference has an inevitable influence on the performance, since the ground-truth labels were provided by human annotators. It’s somehow difficult to quantitatively specify the quality of the summarization result, and current widely used evaluation metrics may not reflect the performance of the results very well. So we are seeking some new directions to find another idea for quality evaluation.
The current setting is short videos & short documents, due to the constrain of available data. To extend the current MSMO to a more general setting, i.e., much longer videos or documents, new datasets should be collected. However, this requires huge human effort in annotating and organizing a high-value dataset, which is extremely time-consuming and labor-intensive. Nevertheless, we believe the MSMO task is promising and can provide valuable solutions to many real-world problems. So if such a dataset is collected, we believe it could significantly boost the research in this field."
1772,"Section 7
7 A2. Did you discuss any potential risks of your work? To the best of our knowledge, we do not foresee any harmful uses of this study"
1773,"We propose and solve the feature space shift problem in text augmentation. However, there is a limitation that remains. BOOSTAUG cannot preserve
the grammar and syntax to a certain extent. We apply the perplexity filtering strategy, but it is an implicit constraint and cannot ensure the syntax quality of the augmentation instances due to some breaking transformations, such as keyword deletions and modifications. However, we do not need precise grammar and syntax information in most classification tasks, especially in PLM-based classification. For some syntax-sensitive tasks, e.g., syntax parsing and the syntax-based ABSC (Zhang et al., 2019; Phan and Ogunbona, 2020; Dai et al., 2021), ensuring the syntax quality of the augmented instances is an urgent problem. Therefore, BOOSTAUG may not be an best choice for some tasks or models requiring syntax as an essential modeling objective (Zhang et al., 2019). In other words, the syntax quality of BOOSTAUG depends on the backend."
1774,"Despite the promising results achieved by our proposed method, there are certain limitations that need to be noted. Firstly, our approach relies heavily on the use of Transformer models, which can be computationally expensive to train and run. Additionally, the lower performance of our aligner for languages other than English is still a substantial shortcoming, which is discussed in Section 5.2.
Furthermore, our method is not adaptable to nonTransformer architectures, as it relies on the specific properties of Transformer-based models to extract alignment information.
Lastly, our method is based on the assumption that the decoder will attend to those input tokens that are more relevant to predicting the next one. However, this assumption may not always hold true in practice, which could lead to suboptimal alignments.
In conclusion, while our proposed method presents a promising approach for cross-lingual AMR alignment, it is important to consider the aforementioned limitations when applying our method to real-world scenarios. Future research could focus on addressing these limitations and exploring ways to improve the performance of our aligner for languages other than English."
1775,"Even though our method is an excellent alternative to the current AMR aligner system, which is standard and task-agnostic, we notice some drawbacks when moving to other autoregressive models or languages:
Model In this work, we studied how Cross Attention layers retain alignment information between input and output tokens in auto-regressive models. In Section 5.1, we examined which layers in state-of-the-art AMR parser models based on BART-large best preserve this information. Unfortunately, we cannot guarantee that these layers are
optimal for other auto-regressive models, and so on. As a result, an examination of cross-attention across multiple models should be done before developing the cross-lingual application of this approach.
Sentence Segmentation It is necessary to apply LEAMR’s Spam Segmentation technique to produce the alignment in LEAMR format (Section 3.4). However, this segmentation method has several flaws: i) As stated in Section 7, this approach does not deal appropriately with phrasal verbs and consecutive segments; ii) the algorithm is Englishspecific; it is dependent on English grammar rules that we are unable to project to other languages. Therefore we cannot extract the LEAMR alignments in a cross-lingual AMR parsing because we lack a segmentation procedure. However, although LEAMR alignment has this constraint, ISI alignment does not require any initial sentence segmentation and may thus be utilized cross-lingually.
ACL 2023 Responsible NLP Checklist"
1776,Section 9
1777,"In theory, the method proposed in this paper can be applied to different types of transformer language models for both pre-training and fine-tuning. Due to limit of computational resource, we currently haven’t had the chance to test our proposed method in the very promising setting of large-scale language model pre-training yet. In future work,
we plan to further test our proposed logical transformer architecture on large-scale language model pre-training to see how much performance boost it can achieve."
1778,
1779,"Though the formulation of the task allows exploring several different settings (by varying the configuration parameters), in this work, we investigated only the label-balanced setting. Exploring the labelimbalanced setting is another very interesting research direction, and we leave that for future work. Another limitation was the limited exploration of novelty detection methods, as a number of methods have been proposed in the recent times. However, we study only a limited set of methods since the focus of this work is on formulating and exploring NoveltyTask. Lastly, we note that NoveltyTask is a controlled task/framework for evaluating a system’s ability to deal with novelties and not a method to improve its ability."
1780,"There are a few limitations we would like to address. First of all, the number of clusters needs manual configuration. This is a limitation of the clustering algorithms (Xie et al., 2016) since we
need to set a threshold for convergence, which consequentially pinpoints k. An expedient alternative is to analyse the dataset for the realistic settings or probe into k for the optimal setup, which is, however, beyond the scope of this paper. Another limitation is the pre-requisite for millions of unannotated data. The autoencoder needs enormous data to learn bottleneck representations. Its performance would be hindered without access to abundant corpora. Lastly, the performance of the acquired clustering-friendly representations depends on the similarity metric chosen. Efforts need to be made to find the best option, whether it is Euclidean distance or cosine similarity etc."
1781,"Section 7
7 A2. Did you discuss any potential risks of your work? Our work does not introduce a novel dataset."
1782,"In this paper, we only discuss the SSAs in English, as this has been the most predominantly studied in adversarial attacks in NLP. The authors are not sure whether SSAs in a different language will suffer from the shortcomings discussed in this paper. However, if an SSA in a non-English language uses the transformations or constraints discussed in this paper, there is a high chance that this attack will produce low-quality results for the same reason shown in this paper. Still, the above claim needs to be verified by extensive human evaluation and further language-specific analyses.
In our paper, we use WordNet as the gold standard of the word senses since WordNet is a widely adopted and accepted tool in the NLP community. Chances are that some annotations in WordNet, while very scarce, are not perfect, and this may be a possible limitation of our work. It is also possible that the matched sense synonyms found by WordNet may not always be a valid substitution even if the annotation of WordNet is perfect. For example, the collocating words of the substituted word may not match that of the original word, and the substitution word may not fit in the original context. However, if a word is not even a synonym, it is more unlikely that it is a valid substitution. Thus, being a synonym in WordNet is a minimum requirement and we use WordNet synonym sets to evaluate the validity of a word substitution.
Last, we do not conduct human evaluations on what the other substitution types in Table 1 are. As stated in Section 3.2.1, while we do not perform human evaluations on this, the readers can browse through Table 6 in the Appendix to see what the others substitutions are. It will be interesting to see what human evaluators think about the other substitutions in the future."
1783,"Since our results regarding a fine-tuned similarity method are limited to the SBERT fine-tuning introduced by Bexte et al. (2022), our findings are limited to this specific similarity-based setup and cannot exclude that other similarity-based methods might behave differently. We also did not consider training sizes larger than 1000 instances of ASAP, and can therefore not speak for how the relative performance of the different methods would be affected by using even more training data. Regarding the experiment on larger training data sizes, we also limited our analysis to ASAP, so it is necessary to compare the observed effects to those that occur on other data sets. The same goes for our cross-prompt experiments, which were also limited to ASAP . Other data sets cover other content domains and can thus produce different effects. Finally, while we do discuss the advantage of a more straightforward explainability of similarity-based models regarding feedback, this is an entirely theoretical argument that goes beyond the scope of this paper and would therefore have to be investigated further in future work."
1784,"Limitation Section, after Section 6
7 A2. Did you discuss any potential risks of your work? No, we do not foresee potential risks of this work."
1785,"• The interdisciplinary element – at the heart of this work – mandates that our results be interpretable and relevant to scholars from the opposite side of the methodological divide (i.e., biblical scholars). This, in turn, introduces constraints to our framework – the foremost is choosing appropriate text-embedding techniques. As discussed in §2.4 and §2.8, the ability to extract specific lexical features (i.e., unique n-grams) that are important to the
classification, to quantify them, and subject them to complementary philological analysis (see Appendix E) – requires that they be interpretable. This constraint limits the ability to implement state-ofthe-art language-model-based embeddings without devising the required framework for their interpretation. Consequently, using traditional embeddings – which encode mostly explicit lexical features (e.g., see §2.4) – limits the complexity of the analyzed textual phenomena and is therefore agnostic of potential signal that is manifested in more complex features.
• In text stylometry questions, especially those related to ancient texts, it is often problematic (and even impossible) to rely on a benchmark training set with which supervised statistical learning can take place. This, in turn, means that supervised learning in such tasks must be implemented with extreme caution so as not to introduce a bias into a supposedly-unbiased analysis. Therefore, implementing supervised learning techniques for such tasks requires a complementary framework that could overcome such potential biases. In light of this, our analysis involves predominantly unsupervised exploration of the text, given different parameterizations.
• Our ability to draw insight from exploring the stylistic differences between the hypothesized distinct texts relies heavily on observing significant overlap between the hypothesized and unsupervised partitions. Without it, the ability to discern the similarity between the results of our pipeline is greatly obscured, as the pipeline remains essentially agnostic of the hypothesized partition. Such a scenario either deems the parameterization irrelevant to the hypothesized partition or disproves the hypothesized partition. Breaking the degeneracy between these two possibilities may entail considerable additional analysis."
1786,"We concede that there are differences in the number of parameters between the BART models when compared to the RoBERTa and LUKE counterparts. However, as per our result discussions and observations, the gains are orthogonal to the encoder used and the differences in the base models are not as significant when comparing the larger counterparts. We note that we also explored seq2seq
pre-trained knowledge-enhanced models like KeyBART and GENRE, however both resulted in underwhelming performance compared to BART. Further exploration is required in improving performance for such models. We also note that while we demonstrate gains by switching to a classificationbased approach in RINE, such models are limited in other generation task capabilities such as translation or summarization. We will release the data and code used for this work, but emphasize that some processing was done over the raw TOPv2 dataset, namely reconstructing source utterances directly from the provided target instead of using the provided source, as we encountered mismatches when constructing pointers. The source was then lowercased."
1787,"Our system distills PLMs into a less expressive but trustworthy set of templates. In developing this method, we explicitly trade off linguistic diversity for faithfulness guarantees. While this approach works well on academic benchmarks, in more complicated real world settings sacrificing linguistic diversity may impact different groups to a different extent. This raises the question of fairness and we hope to investigate such problems on more realistic datasets in future work."
1788,"Our approach for training the Transformer architecture using self-knowledge distillation is promising, but there are still some limitations that need to be addressed in future work. One limitation is that our approach is only tested on the task of AMR parsing, and more evaluations are needed to see if it generalizes well to other tasks, such as Relation Extraction. Additionally, our approach, as is also the case for other current methods, exhibits performance degradation as the number of words in the sentence increases. This may be an indication of the current methods’ limitation or lack of robustness to longer sentences.
Another limitation is the added complexity and extra parameters required by the use of Transformer adapters, which increases the overall complexity of the architecture and training time. Even though our approach still achieves state-of-the-art results and it is as lightweight as previous systems at inference time, this fact should be considered by researchers if they should decide to adopt it for other tasks.
In summary, our approach presents an innovative way to train the Transformer architecture and achieve state-of-the-art results in AMR parsing. However, more work is needed to further improve the performance of the model and to apply it to other tasks as well."
1789,Section 10
1790,"For modeling simplicity, we adopt the classic LDA methods to get the topic ID for each video segment. We plan to investigate more advanced topic clustering methods and check how it can be applied to multilingual cases. Also, we propose a twostage framework that first extract topic and style features, based on which the emotion classifier will be trained. In the future, we hope to extend this work to learn features in an end-to-end manner."
1791,Limitations section
1792,"While good performance has been achieved, there are still limitations in our work. First, though QAVGAE extracts enhanced features and are fast to train, it is an independent module from the main framework. Second, as a post-processing step, the performance of SERR module on simple question is better than that of complex questions.
In the future, we would like to explore the possibility of fusing relation constraints into the representation module directly and inject strong facts identification mechanism as guidance signal of multi-hop reasoning process, aiming to integrate QA-VGAE and SERR into the main framework."
1793,Section 7
1794,"We think this work has the following limitations: The first limitation is that our method involves additional computation for identifying noun phrases and determining which phrases should be normalized. The second limitation is that our method is only performed on noun phrases.
Other phrases may also introduce spurious features. Extending our method to other types of phrases is a potential research direction. The third limitation is that due to the cost limitation, we did not test on the more powerful GPT-based PLMs, which proves to be more powerful and leads to heated discussions recently."
1795,
1796,"Limitations and Future Work: As the first study to assess the adversarial robustness of prompt-based FSL methods, we focus on representative methods that cover different design choices. Future work could expand the set of prompt-based FSL methods considered in this study. Our broader goal is to encourage systematic evaluation of adversarial robustness for all prompt-based FSL methods. Furthermore, we do not perform extensive hyperparameter tuning for the methods considered in this work. It is worth noting that “true” few-shot learning setting has been argued not to involve any development set (as that would involve collecting more labeled data) (Perez et al., 2021; Schick and Schütze, 2022). To this end, we use the hyper-parameters reported by the original authors of these methods. Future work could explore settings where access to a limited development set is assumed for exhaustive hyperparameter tuning. Finally, for adversarial evaluation of prompt-based FSL approaches, we utilize a pre-constructed dataset — AdvGLUE (Wang et al., 2021a). Since these examples are pre-constructed, they do not have access to the gradients of the specific victim models under investigation. Nonetheless, the AdvGLUE benchmark offers a foundation for understanding vulnerabilities in large-scale language models under various adversarial scenarios. This standardized dataset enables fair comparison and mitigates issues with invalid perturbations. For
instance, Wang et al. (2021a) found that over 90% of adversarial perturbations generated using the gradients of victim models for NLP tasks are invalid. Therefore, using AdvGLUE ensures adversarial evaluation on high-quality, human-verified data. Future work could extend the study by considering adversarial examples generated using the gradients of victim models and validating them for correctness. Broader Social Impact: The authors do not foresee any negative social impacts of this work. We believe systematic and preemptive evaluation of the robustness of language technologies against potential adversarial attacks will help develop more safe and secure systems. We release the code for our experiments to aid reproducibility and promote future research on this topic. Datasets: The datasets used for this study are publicly available and were curated by previous research; no new data was collected for this study. We abide by the terms of use of the benchmarks as well as the individual datasets."
1797,"Our search was conducted exclusively in English, and we may have missed relevant papers written in other languages; this may have influenced the heavy English skew in our data.
Some of the annotations of attributes and choices in this taxonomy rely on subjective judgements, particularly with regards to the clarity of conceptualisations of bias, desired outcomes, and justifications of proxy choices. As with any qualita-
tive work, these results are influenced by our own perspectives and judgement. We did our best to address this through regular discussion, identifying disagreements early on when designing the taxonomy, and adopting a “generous” approach."
1798,"In this work, we find that robust instances are helpful for model robustness and propose a metric to select them. However, we only applied one single criterion, i.e. the training dynamic of adversarial loss, as selection metric. More instance features can be inspected in terms of the relation with model robustness and further serve as metrics for robust data selection. Moreover, in this work, we use the selected data for standard fine-tuning with simple regularization, while the impact of data robustness on adversarial training is not studied. These two problems will be explored in future work."
1799,"The limitation section is after the conclusion part of the thesis.
7 A2. Did you discuss any potential risks of your work? Our work don’t have potetial risk."
1800,"While the previous VQA methods that retrieve from PLMs all use GPT-3, we do not experiment with GPT-3 in this paper due to the additional cost. We only focus on applying text-generation models as answer selectors, while classification models could also potentially be good answer selectors. The multi-modal CLIP embedding has already been surpassed by several recent studies (Alayrac et al., 2022; Singh et al., 2022; Lu et al., 2022) and therefore ClipCap cannot represent the performance of multi-modal answer selectors."
1801,"The Pne value decreased in all datasets. While this is not problematic for question generation, where the presence of a named entity is not always necessary, it does pose an issue for NLG tasks where the inclusion of named entities is important. In these cases, we recommend using alternative techniques that we have proposed. Additionally, using delexicalization and over-generation in our approach leads to a high training and inference time."
1802,"We identify the potential limitations of our work as follow: (1) Distant labels may not be available in every application domain (e.g., patient notes in clinical application), although domain adaptation can be applied in these scenarios. We also believe that distantly supervised contrastive learning can be exploited in tasks involving image and video where surrogate labels are abundant. (2) We also acknowledge that the offline NPMI matrix of our proposed CCL method depends on a dataset (distantly) labeled with multiple classes in each sample. To alleviate this limitation, we explore an alternative method that uses learned class embeddings to calculate the inter-class relations in Section 5. This weighting approach achieves sizable improvement over RoBERTa on 16 in-domain datasets, though it underperforms our NPMI-based approach. (3) Our framework does not always work on tasks outside SM. For example, our model underperforms self-supervised CL models, i.e., SimCSESelf and Mirror-BERT, on semantic textual similarity task in Appendix E.6. As we showed, however, our framework exhibits promising performance on some other tasks. For example, our hashtag-based model acquires best performance on the topic classification task, as shown in Appendix E.5."
1803,"There are certain limitations that can be concerned for further improvements. First, the posterior inference relies on the prior estimation of positive class prior α and true positive ratio β. Our experiments show that a data-driven estimation based on end-to-end model training produces worse results than a hyperparameter grid search. An automatic prior estimation is desirable for real-world applications. Moreover, in nPUGraph, we approximate the probability of negative/positive facts being collected/uncollected via neural networks, which lacks a degree of interpretability. In the future, we plan to utilize a more explainable random process depending on entity/relation features to model the collection probability distribution.
Ethical Impact
nPUGraph neither introduces any social/ethical bias to the model nor amplifies any bias in data. Benchmark KG are publicly available. For Twitter interaction data, we mask all identity and privacy information for users, where only information related to user interactions with tweets and hashtags
is presented. Our model is built upon public libraries in PyTorch. We do not foresee any direct social consequences or ethical issues."
1804,Just Limitations; no section number
1805,"In this paper, we propose a novel set-wise framework to extract keyphrases globally. To verify the effectiveness of the new framework, we design simple yield effective neural networks for both the neural keyphrase set function and the keyphrase set extractor agent modules. In general, a complex neural network should yield better performance. Moreover, for the sake of fairness, our model adopts the same pre-trained language model (i.e., BERT) as the recent state-of-the-art baselines (Liang et al., 2021; Ding and Luo, 2021; Zhang et al., 2022). Actually, other pre-trained language models can be applied to our model, such as RoBERTa (Liu et al., 2019). These pre-trained language models may yield better results, which also demonstrates that there is much room for improvement in our proposed framework. Therefore, we believe the power of this set-wise framework has not been fully exploited. In the future, more forms of document-set matching models can be explored to instantiate the set-wise framework."
1806,"When evaluating our model in a cross-dataset adaptation setting, our experiments indicate the importance of using a retrieval dataset. It is challenging to procure high-quality and volume retrieval datasets, especially in low-resource domains such as the medical field. Fortunately, the VQA-RAD and SLAKE datasets we evaluate on contain professionally annotated medical images. We also overcome the lack of data by creating a synthetic dataset from the medical ROCO image-captioning dataset.
Additionally, our model struggles with questions requiring multi-step reasoning, such as knowledge graph, abnormality, and position questions. Although performances in these question types are not far below the overall accuracy, future work may consider supplementary knowledge-based retrieval to assist in these challenging question types."
1807,"Our work has some limitations. Although we use the same verbal stimuli in the previous IAT tests for creating text prompts, it is very likely that some stimuli that can represent the concepts are underrepresented. The approach we adopted for comparing the images’ distance might be biased as well. The current bias test procedure applies the visual encoder of OpenAI’s CLIP model to measure the distance between images. However, it is unclear whether the image encoder may inject additional biases into the latent visual representations.
Ethics Statement
The scope of this work is to provide a principal procedure for measuring the implicit valence and stereotypical biases in image generations. The experiments conducted involve generating images that pertain to demographic groups, and all images were generated in compliance with the terms of service and guidelines provided by the stable diffusion’s license. The AI-generated images are used solely for research purposes and no identities are explicitly attributed to individuals depicted in the images. People’s names are used to generate images. We justify that these are common American names publicly accessible, and do not contain any information that can uniquely identify an individual."
1808,Section 7
1809,"All source sense embeddings we used in our experiments are only covering the English language, which is morphologically limited. Therefore, it is unclear whether our results and conclusions will still be valid for meta-sense embeddings created for languages other than English. On the other hand, there are WSD and WiC benchmarks for other languages such as SemEval-13, SemEval-15, XL-WSD (Pasini et al., 2021) and WiC-XL (Raganato et al., 2020), as well as multilingual sense embeddings such as ARESm (Scarlini et al., 2020b) and SensEmBERT (Scarlini et al., 2020a). Extending our evaluations to cover multilingual sense embeddings is deferred to future work.
Our meta-sense embedding method requires static sense embeddings, and cannot be applied to contextualised sense embedding methods such as SenseBERT (Levine et al., 2020). There have been some work on learning word-level and sentencelevel (Takahashi and Bollegala, 2022; Poerner et al., 2020) meta-embeddings using contextualised word embeddings produced by MLMs as the source embeddings. However, contextualised sense embedding methods are limited compared to the numerous static sense embedding methods. This is partly due to the lack of large-scale sense annotated corpora, required to train or fine-tune contextualised sense embeddings. Extending our work to learn meta-sense embeddings using contextualised word embeddings as source embeddings is an interesting future research direction."
1810,section 6
1811,"Our framework is a two-phase process, which has its inherent defects, that is, the results of the second phase depend on the results of the phase 1. Because the sequence annotation algorithm in the first phase cannot achieve 100% accuracy, it will predict the wrong position that should be rewritten when the second phase is followed, which will further lead to the error of the final result.
On the other hand, T5 model is only used to predict the words that should be filled in blank, rather than generate the whole sentence, which may lead to the decline of the overall fluency of the sentence."
1812,"We would have liked to evaluate the generalization of our cross-lingual approach on more languages. For instance, we partially rely on machine translation models for Chinese-to-English translation. Available translation models for other language pairs, especially from/to low-resource languages have much lower quality, and it would be desirable to measure the effect of that in our experiments.
The ontology used for new languages is derived by translating the Chinese ontology. As a result, the entities are not localized. Creating local ontology requires manual effort as one would need to identify websites or databases for scraping or collecting the entities. Once the local entities are collected, we can automatically replace translated entities with local ones to localize the dataset.
Another limitation is the lack of human evaluation for agent responses. BLEU score does not correlate well with human judgment (Sulem et al., 2018), and SER only accounts for the factuality of the response but not grammar or fluency. In future work, we wish to address this by conducting human evaluations in addition to automatic metrics."
1813,"During inference, kNN-MT have to query the datastore at each decoding step, which is timeconsuming. Although up to 45% datastore entries can be safely pruned by our method, deploying a high-quality kNN-MT system with fast inference speed is still an open challenge."
1814,section 9
1815,"Our augmentation approach is model-agnostic, meaning that it can be applied to any lexical substitution model. However, this also means that it inherits any limitations of the underlying model. For example, in the case of LexSubGen, it can only produce single-token words as substitutes which might prevent it from generating valid longer words or phrases as substitutes that are present in the gold annotations. Additionally, the substitutes are also limited by the vocabulary of the pre-trained language model that LexSubGen uses.
Another limitation of our method is that it relies on the presence of target words in a lexical resource, such as WordNet, together with their synonyms and glosses. If this sense-specific information is missing from the lexical resource, it cannot be used to improve the performance of a lexical substitution system.
Our entailment criterion for lexical substitution is defined for the binary classification task, rather than for generation or ranking tasks. However, if a probabilistic model is used to determine the probability of mutual entailment between sentences, this score can be utilized to rank substitutes if necessary. As explained in Section 3.2, the binary definition can also be adapted to the generation task by iterating over candidate substitutes."
1816,Section 7
1817,"Our paper assesses procedural knowledge augmentation using a limited number of highly structured instructional documents. Naturally, the results presented may vary for unstructured guidelines. Additionally, due to the limited size of publicly available TOD datasets, we have not tested how our method may scale to settings with larger document
spaces (> 100 documents). For larger document sets, more efficient methods of computing similarity such as Maximum Inner Product Search (MIPS) algorithms may be necessary to approximate documents with the highest relevance scores."
1818,"The proposed model, Recurrent Attention Network (RAN), effectively models long sequential data by propagating information window-by-window through the sequence via its well-designed recurrent architecture. However, the multi-head selfattention applied to each window is still limited to local attention, which prevents it from providing a global dependency relationship for the entire sequence. This limitation restricts RAN’s application in scenarios where a global dependency relationship is necessary, such as visualizing attention weights for the entire document via a heatmap. This limitation potentially reduces the interpretability of the model, although it does not affect the model’s performance. Hence, exploring ways to incorporate global attention mechanisms into the RAN architecture is a promising research direction to improve its interpretability and expand its range of applications."
1819,"We worked with only five language pairs, all involving English and another language: Arabic, Spanish, French, Russian and Chinese. This is due to using the multiUN dataset (Ziemski et al., 2016) for evaluating alignment and performing realignment. We also used the opus100 dataset (Zhang et al., 2020), which contains more pairs and is the dataset that eventually figured in our paper, but we stuck to the same language pairs for a fair comparison with multiUN in Appendix F. This narrow choice of language limits our ability to understand why realignment methods work well for some languages and not others. And we believe that making a similar analysis with many language pairs, not necessarily involving English, would be a good lead for further research investigating the link between the success of the realignment method and how two languages relate to each other.
We chose a strong alignment objective with contrastive learning for our realignment task. Several other objectives could have been tried, like learning an orthogonal mapping between representations (Wang et al., 2019) or simply using a ℓ2-loss to collapse representations together (Cao et al., 2020), but both methods require an extra regularization step (Wu and Dredze, 2020b) since they do not leverage any negative samples. For the sake of simplicity, we focused on a contrastive loss, as trying different methods would have led to an explosion in the number of runs for the controlled experiment. This also explains why we used the same hyperparameters and pre-processing steps of Wu and Dredze (2020b). A more thorough search for the optimal parameters, and realignment loss, might lead to better results."
1820,"Section 8
7 A2. Did you discuss any potential risks of your work? Our work does not introduce new methods. It is anlysing existing ones and trying to provide a better understanding of their inner working. Hence, we do not believe that our paper present a direct risk."
1821,"This work proposed a dataset, a simulator, tasks, and models for Aerial Vision-and-Language Navigation. Since satellite images are needed to simulate the drone’s observation, risks of privacy leaking may exist. By using the open-source satellite dataset xView (Lam et al., 2018), we mitigate the risks while also being able to develop a simulator for training our model. Additionally, using satellite images for simulating top-down visual observation of the drone introduces the shortcoming of having only 2D static scenes while adopting the strength of the satellite images where rich labels and visual features are included.
Broader Impact
We recognize the potential ethical problems during the dataset collection, where human annotators are involved. The data collection of this project is classified as exempt by Human Subject Committee vis IRB protocols. As a result, we utilized the Amazon Mechanical Turk (AMT) website to find workers willing to participate in the project. With AMT, our data collection is constrained by legal terms, and the data collection protocol is under AMT’s approval. The agreement signed by both requesters and workers on AMT also ensures a transparent and fair data annotation process and that privacy is well protected."
1822,"Although our model achieves competitive results with baseline models, some limitations are summarized as follows.
1. The process of extracting data distributional signatures is time-consuming, especially for datasets with more diverse dialogue patterns. The process of calculating adjacent n-grams is slow. In addition, repeated string manipulation for long texts also needs to be optimized
2. The experiment results are easily affected by the fluctuation of hyper-parameters, especially the signature block hidden size. There is some noise in the distributional signatures. Under different hyper-parameters, noise may have different effects and directly affect experiment results.
3. Our model performs poorly when the training set is too small. The distributional signatures of small data interfere with the model."
1823,"We presented a method to automatically generate and label affective events by co-prompting with large language models. The data generation process does not involve creating or training new language models. There are some limitations to our approach. One limitation is that language models are not guaranteed to generate truthful or sensible information, which could introduce noisy information to our model. For example, we observed that the Emotion Prompt sometimes generates highly unlikely polarity labels for some events. Language models can also produce biased results, which could introduce biased information to our model. Another limitation is that it may be non-trivial for researchers who want to apply our method to other NLP problems to design prompts that are effective for their task. We believe that this method should be fairly general, but it has not yet been evaluated for other tasks. Lastly, our method requires a moderate amount of computational resources, including GPU cards with substantial memory and access to large language models. As a result, groups with limited resources might find our method too computationally intensive."
1824,"There are two limitations of our work: 1) As the overall loss function (12) comprises five components, we propose to directly add these components together. Although this simple summation already yields better results than the SOTA methods, we believe that it is better to tune the weights of these components based on expert knowledge, empirical experiments, or other machine-learning techniques. 2) In ZeroAE, we use three PLMs, including two BERTs and a GPT-2. Moreover, contrastive learning typically requires a relatively large batch size in order to collect a sufficient number of negative samples and achieve satisfying performance (Chen et al., 2020). The batch size in our experiments is typically 32. As a result, training ZeroAE incurs relatively large resource cost. In practice, we find that using four NVIDIA TESLA V100 GPUs with 32G memory works well, and further reducing the resources hurts the performance."
1825,"PRAM effectively handles the ZRCL-NER task but has certain limitations. Firstly, since PRAM relies on the cross-lingual inference ability of mPLM, its transfer ability may be restricted if the target language is not among the pre-trained languages of mPLM. Secondly, the high memory requirements of PRAM may occurs when the task is the multisource transfer, where we need to set a large batch size to ensure the stable update of prototypes on different source languages. This drives us to enhance the space efficiency of our method in the future."
1826,"Section 6
7 A2. Did you discuss any potential risks of your work? We don’t see any potential risks in your work."
1827,"Regarding our work, we summarize the following three limitations.
(1) TLDT is proposed under the assumption that entities in a sentence are independent of each other, that is, they do not overlap. In the case of overlapping entities, TLDT cannot capture the label dependency of these entities.
(2) We tested TLDT on four public datasets and these datasets contain a part of the same labels. This may help TLDT to learn the general knowledge of the label dependency. We did not give experimental proof that if the label sets of the datasets all totally different, whether the TLDT can maintain good robustness.
(3) To overcome the label discrepancy problem and generate adjustable hyperparameters, we model for the individual transition score in the specific task. However, the mechanism neglects the correlation between parameters, which limits the ability to update rule learning."
1828,Section 7
1829,"In this work, we contribute an evaluation benchmark for classical Chinese NLP tasks. We did our best to create as comprehensive a well-defined task set as possible, something no one has done before. However, our work has several limitations due to lacking expertise knowledge and data.
When designing the tasks, we got a lot of inspiration from the middle school Chinese test paper. thousands of test papers are collected in order to ex-
tract data for NLP tasks. During the work process, we learn that it is difficult to extract a sufficient number of questions of a single type. The main difficulty is due to the variety of questions on the test papers and the mixture of the language of classical and modern Chinese. Finally, we create Xuci task, WYWRC task and IRC task from the test papers and related literature but failed to create solvable natural language inference tasks.
When working on some datasets which have less corpus, i.e, the Xuci task, we find it very difficult to calibrate existing samples or create new ones, resulting a small dataset size.
Meanwhile, the category rule we followed in the GJC task is not certified by authoritative experts, so this method is not completely reliable if viewed by experts of classical Chinese.
In this work, tasks for more aspects of grammar phenomenon are lacking. It’s expected that more classical Chinese experts and researchers join this work in the future to solve the above problems.
On the other hand, we lack a diagnostic dataset compared to other benchmarks. This is because similar data (NLI corpus generally) are even more difficult to retrieve. However, this benchmark works for NLP researchers even though the diagnostic dataset is missing. This issue is also expected to be solved in future work."
1830,The limitation section on page 9.
1831,"The largest limitation of this study is that our annotation pipeline is automated. This makes it possible
that there are errors in the noise annotations that we base our analysis on. Additionally, since we capture a naturally occurring noise distribution, our findings are coupled to the datasets we study here. Our findings may not generalize to distributions of noise in other datasets."
1832,"Our summary generation and clustering steps have some key limitations. First, there is an API cost involved in using instruction-tuned models such as InstructGPT. It is thus preferred to use publicly available large language models, which have the additional advantage of results being reproducible by other researchers. With the recent release of large models, this is a promising avenue for further investigation. Second, it is difficult to control the granularity of steps predicted by the summary generation step as they are obtained through free-form text generation. Third, the clustering approach involves hyperparameters that need to be manually set such as the number of clusters. We believe these limitations are best addressed by tighter coupling between the different components (i.e., summary generation and clustering components which inform each other)."
1833,"This paper lacks a formalized analysis of the relationship among perturbed contexts/pretraining/model generalization. Although we try to analogize pre-training and prompt in Appendix B to explain how the perturbed context works, it lacks a rigorous mathematical description.
The validation of the perturbed context is limited to relation extraction. Although we show its potential on other applications in Appendix D, the experiments are still primitive. A more systematic evaluation on different NLP tasks is still excepted."
1834,"One limitation is that the efficacy of our method is shown with massive LLMs in this paper. However, we note that our method is based on only model inference with them and is already considerably cheaper than other adaptation methods.
Furthermore, our method seeks to improve LLMs: while the technology itself is ethically neutral, we acknowledge that there are various social and ethical risks of potential misuse, especially given the powerful generative capabilities of the LLMs that have become increasingly accessible to a broader audience (Weidinger et al., 2021). We argue that both the prospective end-users and researchers should be aware of these concerns when using our method in order to mitigate these risks.
Methodologically, we note that an integral component of our algorithm is self-consistency. We rely on the expectation that it reliably predicts accuracy, which essentially places an expectation that the model uncertainty should be reasonably wellcalibrated. While we have indeed found this to be the case for almost all considered tasks and models, additional investigations might be required to ascertain their general applicability. Given our reliance on self-consistency, for tasks where selfconsistency does not lead to significant gains, the performance improvements with our method may be limited. An example of this could be tasks with very small label spaces (e.g. binary classification) where “consistency” in outcomes may be achieved much more easily even if the model simply outputs random predictions. A potential remedy, for which we defer a thorough investigation to future work, is to not only consider the consistency over outcomes but also over the intermediate rationales which are generated but not currently used for selfconsistency computation.
Second, while COSP improves performance in an overwhelming majority of cases and is significantly less sensitive to the original zero-shot model performance compared to baseline methods like AutoCoT, there still exist exceptional cases where it fails to improve, especially when the tasks are too challenging in zero-shot setup – we argue that this is also due to the general, inherent limitations of the LLMs. However, both continual improvements on the foundational models and provision of some human guidance (e.g., using COSP-FS) should alleviate this issue."
1835,"This work has only focussed on numerals from 10- K documents mandated by SEC. Our dataset, at present, does not include any annotated words as we focus only on numerals. It also does not include any tabular data. We also find that companies often annotate text with their custom labels which are not included in our dataset. We also find that often, it is difficult to label a numeral based on just the text of the sentence; the context might depend on surrounding paragraph, associated tables, etc. To this end, we have not benchmarked the performance using this information. However, we provide certain metadata along with the data points, including the company name, the year document was published, and the surrounding text which may be used to develop improved models."
1836,"Section 6
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
1837,"One limitation of this work is the lack of diversity in the downstream tasks. In our experiments, five of the tasks are named entity recognition and one is a parts-of-speech task. As OOV words make up a small portion of a sentence in the text for our downstream tasks, (the number of OOV words with valid characters ranges from 3% to 12%), analyzing the impact of higher quality OOV estimates is not trivial. For example, in document classification, the predictions depend on each word in the document, and thus the evaluation of OOV estimation will not just be based on the quality of the OOV embeddings, but also on their effect on the result compared to known embeddings. This makes assessing OOV estimation quality more challenging. As such, it is better to focus on tasks with word level output, so the quality of the OOV estimates can be directly judged. However, this limits the types of downstream tasks being analyzed. A second limitation is the fact that all tasks use the English language. As subword impact is dependent on the morphology of a language, the contri-
8For easier comparison, we report the actual values of the CRW Ablation in Appendix C.
butions of subword pretraining and attention will vary with the language. However, previous OOV works evaluate on English tasks, and as a result for comparison this paper does the same."
1838,"Section 6.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
1839,"Although our model can achieve better results compared to other works, there are some limitations of our model:
• Our model cannot use shared dictionary and embedding on the encoder and decoder.
• Though the encoder and decoder can use PLMs, the coordinator cannot use PLM.
• The half-layers knowledge distillation still uses additional frozen BERT."
1840,"There are two limitations of this work, one of which is that our work is trained on the sequenceto-sequence model. However, we have not verified our approach on the sequence-to-edit architecture. In future work, we will verify our approach on the test bed of the sequence-to-edit model. The other limitation is that using translationese as input of data augmentation can not bring absolute improvement to grammatical error correction task. Specifically, our approach still has some room for improvement such as correcting rare words, word order, and deletion errors."
1841,"Section 1, and Limitations section.
A2. Did you discuss any potential risks of your work? Not applicable. There are no potential risks associated with this paper because all tasks we used are public ones that have been verified for years."
1842,"Although we have shown the robustness of multisource NMT to transcription errors in a fullsentence and simultaneous settings, our work has the following limitations:
• Our work does not address the case where the additional source, typically interpreted, is available after a delay. A delayed source may reduce the gains seen by multi-sourcing.
• We have only focused on the Local Agreement (LA-n) approach for simultaneous translation and exploration of other simultaneous approaches such as wait-k remains.
• Human evaluation of translations is pending.
• Evaluation on other language pairs is pending."
1843,"We completed this evaluation based on the Beta version of ChatGPT, and the relevant results may change as OpenAI continues to improve ChatGPT. In addition, we use the ATT&CK Technique datasets collected from third-party institutions and publicly available data on the Internet. The large size of these datasets makes it difficult for us to verify their accuracy manually. Therefore, errors in these datasets may affect the conclusion of this paper."
1844,"We would like to claim our limitations from two perspectives: technical-wise and application-wise.
Technical-wise: We currently only experiment with BERT-Base, BERT-Large, and RoBERTaLarge as the basic encoders. For larger language models, due to limited resources, we have not implemented them.
Application-wise: The experimental data comes from the Open Data repository released by governments of various countries. Although many domains are covered, some domain-specific data, such as biomedical, have not been considered. Furthermore, our tabular data are all from English, open data research in other languages can be considered as a future research direction."
1845,"As mentioned in the previous section, up until now, there has not been a fully pre-trained seq2seq model with a BERT-style self-attention mechanism in the decoder, while the vanilla seq2seq model tends to use a left-to-right or right-to-left unidirectional self-attention. Therefore, utilizing our proposed Bidirectional Transformer Reranker (BTR) to rerank candidates from a pre-trained vanilla seq2seq model requires additional pre-training steps, which cost both time and GPU resources. Because the BTR masks and predicts only 15% of the tokens in Eq. (7), it requires more training steps than a vanilla seq2seq model. In addition, during fine-tuning, the BTR also requires additional atrain negative samples, which makes the fine-tuning longer. Furthermore, tuning atrain will be inefficient if the training is slow. In other words, training an effective BTR requires much more time than training a vanilla seq2seq model.
As a reranker, the performance of the BTR depends on the quality of candidates. There is no room for improvement by the BTR if no candidate is more grammatical than the original selection."
1846,"In this research, though we employed automatic evaluation of our multi-style transferred text, we acknowledge that multi-style transfer is challenging to observe with the existing metrics for style transfer evaluation, and human evaluation should
be done as well. As this research paper focuses on exploring the impact of style distributions in the training data on style-transferred output rather than developing a superior multi-style text transfer model, we use quantitative evaluation in this iteration of our paper. We hope that the large sample size and the consistency of applied metrics make our automated approach a reasonable way of evaluating the style transfer output.
This iteration of our paper aims to achieve multistyle transfer across multiple micro styles taken into consideration together as our contribution would aid in constructing a training dataset for multiple micro-style style transfers. We did not explore another exciting question of how balancing multiple micro styles in the training dataset might influence individual style transfer, which could be a promising future direction for our study.
We acknowledge that the classifier’s quality sets an upper bound on the best style transfer accuracy that is obtainable. However, the target task is quite complicated without a parallel dataset. Our objective was not to have the most accurate classification of micro styles but to find a means to get acceptable pseudo labels for the micro styles. Individually, all our micro style classifiers had a classification accuracy of 80% F1 and higher, and we deemed this good enough for pseudo-label creation.
We also focused on utilizing the present styles in the training data and classifying them to derive inherent training style distributions instead of dynamically tuning the proportion of styles present in the training dataset. However, tuning these style proportions using techniques such as PPLM (Dathathri et al., 2019) would give us greater control over our experimental pipeline and is an appropriate next step."
1847,"Despite being easily adapted to current deep learning architectures, one concern about multipleforward sampling methods is efficiency, since it has to repeat T processes to evaluate uncertainty in the stage of inference. We leave efficient variants of sampling methods for future work.
Another glaring issue is the focus on only English. Different languages may have different effects on uncertainty estimation due to e.g., distinct forms of morphology. Thus, some conclusions may vary according to the language in question. We hope that follow-up works will refine and complement our insights on a more representative sample of natural languages."
1848,"In Section 3.2, we show that our training mixture with multiple-choice QA tasks, although small, is highly effective for multitask training. However, it is still unclear why multiple-choice QA tasks are particularly effective. Identifying the key factors towards positive or negative transfer from different tasks in the multitask training mixture would greatly help improve zero-shot task generalization. Future work includes investigating why certain mixtures are more effective than others and expanding the evaluation set to a wider range of tasks. Computation overhead is another noticeable limitation of semi-parametric models which is discussed in detail in Section 3.5. Moreover, future work on developing more efficient and accurate retrieval methods for retrieving from large-scale task-agnostic corpus can definitely improve semiparametric language models."
1849,"The major limitations of our work are as follows: (1) We show that the neuron structure of MoE reveals the presence of modularity in pre-trained Transformers. However, the MoE structure is not the only possible modular structure. To better understand the modular structure of Transformers, we need to explore more types of structures. For example, the number of neurons in each module could be different, and the modular structure could
be hierarchical, where modules are grouped into larger modules. (2) We study three typical functions for language processing: semantic function, knowledge function, and task function. There are many other functions that could be studied, such as the syntactic function, discourse function, etc. Moreover, our categorization of functions may be not suitable for pre-trained Transformers because there are some overlaps between studied functions. A new Transformer-based function categorization may be needed. (3) We transform T5 into its MoE version to study its modular structure while not all dense pre-trained Transformers can be studied in this way because the adopted MoEfication technique (Zhang et al., 2022b) can only transform ReLU-based Transformers. Studying the modularity of other dense pre-trained Transformers, such as BERT, is also important for future research."
1850,"After the conclusion.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
1851,"SETI only considers veridical inference and natural inference (including both lexically-based inference and structure-based inference). However, our benchmark SETI can be flexibly extended to more varied reasoning patterns, such as negation, quantifiers, or others. In addition, we evaluate the systematicity capabilities of PLMs on semi-synthetic datasets, which are limited in language variance. Extending our benchmark on manually annotated compositional inference datasets might be a promising future work.
Recently, Hupkes et al. (2020) dissect the notion of compositionality and define five theoretically grounded tests for generalization, in a taskagonistic manner. Our work is limited to evaluating the systematicity of PLMs in textual inference. While the systematicity test is one of the most im-
portant tests, the remaining ones (e.g., productivity and localism) are still worth to be explored in future works."
1852,"Section 7
7 A2. Did you discuss any potential risks of your work? We do not find any risk in our work so far. We use an open-sourced NLI dataset."
1853,"There are several limitations of this work: (1) In this work, we leverage a cluster-based method for improving the representations during coarse training. Regarding the clustering, we simply perform a K-means process to obtain cluster assignment. The problem is, the cluster assignment with one-time clustering, especially based on the fine-tuned BERT model, might be incorrect. Training with incorrect cluster assignments will pull together the instances from different fine classes, thus hindering the performance. Therefore, the proposed method can be further improved by: (a) leveraging a more robust learning method that can tolerate incorrect cluster assignments; (b) Improving the clustering process to obtain more accurate cluster assignments. (2) In this work, we aim to extend existing few-shot NER to a practical yet understudied setting. However, we limit the setting to the few-shot coarse-to-
fine transfer learning setting as in previous works (Bukchin et al., 2021; An et al., 2022; Yang et al., 2021; Ni et al., 2022). In practice, there might be more complex situations that require to be explored in further works."
1854,"Although we use data from dialogues, we do not model collaborative reference, i.e., we do not model continual mutual adaptation. Instead, we focus on the speaker’s adaptation to the listener in a single turn, which is certainly a simplified setup. Furthermore, our plug-and-play approach still requires the training of simulators per listener type. However, as we keep the speaker and listener models frozen and use the output obtained from them to train the simulators, this allows us to reduce the required amounts of training. We train the models from scratch using PhotoBook data and do not make use of state-of-the-art large pretrained visionand-language models that are nowadays commonly based on Transformers, which could be considered a limitation. We opted for this setup as it is more aligned with our research questions, allowing us to control the domain-specificity of the models. We also acknowledge the imbalance in the set sizes of the domains, as well as the possible lexical and visual overlaps in the samples across domains. The overlaps may facilitate the adaptation of certain sentences from one domain to another (asymmetry is not controlled in a fine-grained manner), and this is not uncommon in human communication."
1855,We discuss them in the Limitations section.
1856,"Computational Complexity. Under our proposed MBRD framework (see Eqn. 9), we need to make O(k2) comparisons to find the candidate with the lowest expected risk, assuming |C| = k. If we are using lightweight, non-neural alignment metrics, we might not necessarily need to worry about the overall computation time; however, if we are using a learned metric such as BLEURT as our utility function, then the generation time might be slower, especially if we have a large candidate pool.12 We leave the in-depth exploration of more efficient methods for computing or estimating the textual similarity matrix of candidates for future work.
Generative Model Quality. In practice, MBRD operates under the assumption that we have access to a good generative model. In our experiments, we primarily made use of Codex, which we know to be a model with impressive generation and in-context learning capabilities, but we also considered a few other text generators such as InstructGPT, Pegasus, BART, and T5—and we found that these models too benefit from the use of MBRD. Due to the
12If the utility function util(·, ·) used in MBRD is symmetric, that is, util(a,b) = util(b,a), ∀a,b ∈ V∗, one can then reduce the computations by half.
scope of this work, we left the investigation of the efficacy of MBRD under smaller language models such as GPT-2 and GPT-J for future work.
Choice of τ . The choice of τ for temperature sampling determines the diversity of our candidate generations. In our experiments, we set τ = 0.7 as it yielded good results across multiple datasets, but as Table 6 also indicates, there might be more effective values of τ—or even better stochastic sampling methods—for different tasks and datasets. More broadly, the choice of τ reflects the larger and partially inescapable trade-off between diversity and generation quality (Holtzman et al., 2019)
English-Language Focus. In all our experiments, the outputs were English-language texts; similarly, we used learned metrics that can be used to compare English-language texts. The general MBRD framework, however, is applicable to any language or modality. We hope to explore using multilingual tasks and alignment metrics in the future.
Impact and Potential Misuses. Large language models may exhibit a variety of biases due to their pre-training data. As with many other NLP tools and applications nowadays, it is conceivable that malicious actors might use these models to generative negative, harmful and hurtful content about certain groups of people. Our MBRD approach does not enhance or support the generation of toxic content in any way. On the contrary, MBRD can be used to eliminate biased and toxic outputs and to mitigate hallucination issues in language models if appropriate utility functions are used in Eqn. (6). Such investigations are, however, out of the scope of this present paper. We hope to investigate those dimensions in our future work."
1857,"We recognize that several limitations remain with SENTECON and SENTECON+.
(1) Despite the gains in performance obtained by using a fine-tuned Mθ with SENTECON, we note that this version of SENTECON has significantly worse agreement with human evaluation than when a pre-trained Mθ is used. It is not immediately obvious why this should be the case. Although it is always possible to use SENTECON+ with a pretrained Mθ in cases where agreement with human evaluation is particularly important, future work should examine why this degradation occurs and explore whether it is possible to maintain human agreement while also seeing those same performance gains (possibly through a secondary loss term that prioritizes human agreement).
(2) When building a sentence embedding dictionary, the base lexicon of SENTECON(+) may map lexically similar sentences to the same categories, regardless of attributes like negation. Despite this, SENTECON produces meaningful repre-
sentations for sentences that require compositional understanding, which we attribute to the large number of sentences mapped to each category (recall that each contextualized word embedding mapped to a category can be viewed as a summary of all sentences in the language model pre-training corpus containing that word). For example, the number of negated sentences in the sentence embedding dictionary is far smaller than the number of non-negated sentences—and likewise for other attributes requiring compositional parsing. Consequently, each category’s centroid is still approximately an average of the non-negated sentences.
The same principle applies to SENTECON+ if a reasonably-sized reference corpus is used. If, however, only a very small reference corpus is available and the task dataset is known to require strong compositional understanding, SENTECON should be used instead of SENTECON+."
1858,"The main limitation identified for our RL model is decreased performance as the vocabulary size increases. Our RL model also has a higher variance than some other topic models to which we compared. While our RL model performed well on all the data sets tested, this performance may not
generalize to different data sets. The insights from the policy dropout sweep conducted may not apply to other topic models. The performance difference for NPMI coherence compared with Bianchi et al. (2020a) may be overstated since the model in that paper used a deprecated SBERT model that produces sentence embeddings of low quality2. For the comparison to Nguyen and Luu (2021), we used slightly different preprocessing than the authors. While the model can work on any languages with associated embedding models, all data sets used in this paper were in English. Our model has additional hyperparameters compared to some other models. So, it may require more tuning and, therefore, more GPU computing. The initial model was developed on a system with 8GB of RAM and a Nvidia GTX 1060 with 3GB of VRAM for a total of approximately 100 GPU hours. Experiments using the New York Times data set were run on a system with 256GB of RAM and a Nvidia RTX 3090 for a total of approximately 100 GPU hours. All other experiments were run on a system with 128GB of RAM and a Nvidia TITAN RTX for a total of approximately 600 GPU hours."
1859,"In our work, when facing long sentences, a large number of synonym candidates can decrease the convergence speed of the lower and upper bounds. Therefore, in the experiments, we set up limitations on the length of the input sentences and the number of synonym candidates. Please note that it is still feasible to process long input sentences because of the anytime nature of our tool, however doing so would increase the unverifiable region, which essentially trades the tightness of bounds for efficiency."
1860,"First of all, our OASum inevitably contains inappropriate summaries not strongly correlated with certain aspects since it is automatically curated. The model trained on it could furthermore hold such misinformation and affect other downstream tasks. But we hope the large-scale training can alleviate such effects to a minimum. At the current stage, we are not responsible for any products directly built on our results. In the future, a potential denoising mechanism could be designed to further reduce the noisy summaries.
Secondly, we only opt for end-to-end extraction, which requires large computational memory and cost that may not be afforded by everyone. Thus, a meaningful direction would be investigating other extract-then-summarize two-step methods for dealing with long document summarization. Besides, our vanilla dataset contains millions of summaries that are difficult for certain researchers with limited computational resources to directly reproduce results on. We recommend using a small subset of our corpus if enough computational capability is not immediately available.
Finally, we only explore a simple strategy for controlling the summarization based on input aspects. However, we find it can not always guarantee aspect-focused generation. How to efficiently and accurately generate specific summaries by confining aspects is not only challenging for model design but also difficult for humans to evaluate. We leave these issues for future work."
1861,"vMF distribution has a unit constraint. This limits the variability of latent space, which in turn reduces the gains as the number of topics increase. We can try other distributions with richer variability, such as Bivariate von Mises distribution and Kent distribution.
Also, in weakly supervised cases, vONTSS may not perform as well as those methods that leverage pretraining language models in classification. In the future, we can combine the structure of this model with existed language modeling to further improve its classification performance.
Lastly, in semi-supervised cases version, our formulation of vONTSS requires each topic to have at least one keyword. This limits its practical usage to some extent. To solve it, we can first preselect topics before doing the topics and keywords mapping, or we can modify the optimal transport loss using Gumbel distributions.
ACL 2023 Responsible NLP Checklist"
1862,"The models chosen in this work are selected to represent the state-of-the-art at the time the work was conducted, and in some cases omit weaker models. For example, our exemplar selection experiments do not cover those LLMs trained with vanilla language models objectives, namely OPT and davinci, as we find their performance substantially lags code-davinci-002 and text-davinci-002. For the same reason, we only consider the substantially large language models, omitting LLMs of smaller scales (e.g., text-curie-001). Running experiments using smaller LMs or vanilla LMs may provide insights into how scale or instruction finetuning impacts the ability of LMs in learning from explanations, but our investigation mainly focus on selecting exemplars to achieve the best in-context learning performance with state-of-the-art models.
In addition, certain aspects of our approach are computationally intensive, particularly using LMbased similarity scores. However, we think this is still feasible in practice: if practitioners are deploying a real-world system, investing more computation upfront to improve its performance is likely in reach for those deploying LLMs in practice.
Finally, our experiments consider a certain subset of NLP reasoning tasks written in English. While we believe the results here transfer to other tasks in this vein which have been frequently used to evaluate LLMs, it is unknown how well they handle other languages, dialects, or genres of text such as social media data."
1863,"Section 8.
7 A2. Did you discuss any potential risks of your work? This work focuses on analyzing what makes effective explanations for in-context learning. We believe that the risks of these methods are the same as the risks of broader in-context learning literature."
1864,"One limitation of our work, which is also an avenue for future work, is that it is not fully understood yet why the mismatch error types help much more in some tasks than others. Trying to develop a more task or even instance-specific understanding of the benefits of mismatch error types will be very useful. We also want to try our proposed approach on a wider set of tasks, using different foundational models, and under the distribution shift setting to see if the mismatch error types as auxiliary supervision can improve robustness of natural language processing systems."
1865,"In this work, we investigate the use of pre-trained language models for long-term English conversations. While we expect a modular approach may be effective for other languages when given a capable language model, it should also be noted that further research is needed to confirm the applicability of our findings to other languages. For instance, though BLOOM is trained as a multilingual language model, we only implement MPCBLOOM in English and evaluate its English capability as a open-domain dialogue agent.
Meanwhile, a modular system can create additional inference overhead or error accumulation. The system performance would become much bet-
ter if we optimally choose the LM for each module. For example, we could use GPT-3 td2 for the memory processor, while we employ OPT-175B for the utterance generator. We would need to evaluate every module to find the best model for each, which we leave to future work.
In terms of evaluation methodology, our human evaluations of MPC and its analysis face the same challenges as previous studies on evaluating interactive conversational tasks. As demonstrated by Smith et al. (2022), there is currently no definitive evaluation method for determining the best chatbot model. Additionally, there are several factors that must be taken into account during data collection and interpretation, such as annotator subjectivity, instruction bias, and crowdworker working conditions. For a more in-depth discussion of human-LM interaction, we refer the reader to Lee et al. (2022).
As described in Appendix C, to gather a diverse range of evaluations, we have collected qualitative data from two groups: English-speaking annotators on Amazon Mechanical Turk (MTurk), and qualified university students who were capable of speaking English. To some extent, this evaluation setup reduces cultural bias and platform homogeneity compared to using MTurk workers alone. However, the limitations of this approach should be acknowledged and this may further complicate the analysis when controlling for MPC’s performance on different subgroups.
Lastly, we note that running MPC requires at least as much memory as its underlying language model, making MPC infeasible to even load on a single node for heavy models such as BLOOM176B."
1866,Limitations
1867,"While ProToCo works well with our consistency training for improving fact verification under fewshot and zero-shot settings, our work has some limitations. Due to limited resources, currently we were unable to conduct comparison with larger PLMs and examine if extremely large models have already developed the similar or better level of consistency for fact verification on their own. In addition, our experiments show that consistency training brings improvements in both settings using only gold evidence. However, the retrieved evidence in realworld setting can be noisy and incomplete. That said, the performance of ProToCo on non-oracle evidence requires further study. To utilize consistency constraints, ProToCo still needs to fine-tune the PLMs. Also, in zero-shot setting, the labels of logical variants are assigned with the predictions of the original claim by the base model, which could be inaccurate and thus affect the consistency training."
1868,
1869,"In all the experiments conducted in this paper, Perspective API was used to evaluate the level of toxicity in generated outputs. While using it, two substantial limitations were noted. The first limitation related to the frequency with which the API is updated, often resulting in a different scoring for the same text across newer versions of the tool. This leads to considerable issues with reproducing results – values presented in our paper are therefore only a snapshot in time of the version of Perspective API used during experimentation. Table 7 in the Appendix shows the results found when using Self-Debiasing (λ = 10) before an update and the same test with same input and specification done after an update.
The second limitation is that while Perspective API evaluates based on the negativity in language, it does not evaluate its positivity. The ramifications of this are that when it is evaluated on neutral text, a reflection of improvements may be more difficult to arise, because while the output may be adding more positive language, if there was not any negative language to remove, then Perspective API shows it as no change in scores. Of course, these are by no means limitations of Perspective API as a tool as they are the very limitations of using a single evaluation measure for toxicity/bias, which is undoubtedly both arbitrary, but also temporally varying, as well as culturally-bound. Our own societal measures for toxicity undergo “version updates” over time. In the future, more robust testing should be performed by using multiple toxicity evaluation tools such as Moderation API4. Furthermore, work should be pursued on developing ways of including and accounting for these nuances and variations."
1870,"One limitation that we observed in relation to the language model (LM) concerns the possibility that a debiasing method may depend on specific LM characteristics and may not be universally adaptable. It is crucial to clearly acknowledge this limitation. This became apparent during the development of the Instructive Debiasing method, which relies on an LM’s comprehension of context and polarity for its functioning. Interestingly, while GPT-3 exhibited these capabilities, GPT-2 seemed
4https://platform.openai.com/docs/guides/moderation/
to lack them. If a debiasing method is found to be inconsistent on the current LM, transitioning to more advanced LMs is a critical subsequent step. A unique advantage of Instructive Debiasing over other, more complex debiasing methods that modify the LM’s mechanisms, is its ease of application to closed-source models such as GPT-3, as demonstrated in our paper. We are currently exploring its applicability to successors and counterparts such as PaLM (Chowdhery et al., 2022) and ChatGPT (Schulman et al., 2022). Our preliminary experiments with ChatGPT using the Instructive Debiasing approach have yielded intriguing results, with the model persistently refusing to follow the instruction to continue the text. This behavior might represent the most significant leap in debiasing capabilities to date; after all, you can’t say something bad if you say nothing at all.
Another model limitation is the observed tendency of a language model to repeat itself when given a prompt. This was especially prominent in GPT-2 outputs but less so for GPT-3, however certain prompts did still elicit GPT-3 to show the same behaviour. Table 13 (Appendix) reveals outputs for given inputs to GPT-3 and debiased using Instructive Debiasing with nonsensical specifications, it can be clearly seen that GPT-3 will sometimes repeat the input as well."
1871,"Our protocol carries with it assumptions that may not allow it to be applied to all possible debiasing methods. For example, it does not account for debiasing methods that do not use specifications or for those whose specifications do not have corresponding opposites. In future work, we are interested in exploring the adaptability of our protocol for the recent debiasing methods mentioned in Section 1 that have been proposed in parallel to our work.
Moreover, the protocol isn’t a universal tool; its application is limited to debiasing methods that employ both a mechanism and specifications with polarity. For researchers working on new debiasing methods that either don’t use specifications or employ more complex specifications that may not represent a specific polarity, we encourage them to leverage our findings (along with our shared code) and investigate potential adaptations to suit their methodology. The protocol was not only designed to be compatible with appropriate debiasing methods, but also to serve as a foundation for the
development of novel protocols.
Ethics Statement
Those who wish to use this protocol should be wary of some possible ethical implications around the usability and validation that the protocol gives to a debiasing method.
First, the protocol was designed to be used as an evaluation tool for consistency and is far from representing all tests and considerations that must be taken before deploying a debiasing method into public or private use. The use of the protocol is encouraged to gain insight into possible shortcomings of a methodology, but there are risks to this as there may be considerations and inefficiencies that the protocol does not account for. Thus, the protocol is meant for research purposes only and is not meant to be a foolproof ethical check for software deployment.
Second, the protocol was built using, and only considering, English with North American definitions of specifications. This means that any results found using the protocol, and even the protocol itself, may not work or be as effective if used with different languages or different social and cultural definitions of specifications. However, we sincerely hope that our work helps open the doors for future work into testing the effectiveness of the protocol on different languages and within different cultures and values and adapting it accordingly."
1872,Section Limitations
1873,"Effectiveness of Task and Model Scopes. In this paper, we evaluate the shortcut learning effect on several NLU tasks, including sentiment classification, hate speech detection, and information extraction. Our task selection is mainly based on the robustness and effectiveness of in-context learning on
2Our implementation is grounded in LIME. GitHub: https://github.com/marcotcr/lime
certain tasks. Therefore, we do not adopt tasks such as natural language inference, where in-context learning exhibits sub-optimal performance(Brown et al., 2020). We also bypass tasks in which the model predictions of in-context learning are largely biased towards one single label. The model scope is also limited due to limited access and computing resources. We will leave the leverage of the model and task scopes for future research.
Calibration of Shortcut Learning Effect. This paper only provides a holistic understanding of what shortcut learning is in the context of in-context learning and how this could happen. Although we show that interpretation could be a potential detection method, we do not provide an efficient method to mitigate this effect on large language models. We will leave it for future research."
1874,"As established, medical coding is an important task for the healthcare industry. Efforts toward its successful automation have wide-ranging implications, from increasing the speed and efficiency of clinical coders while reducing their errors, saving expenses for hospitals, and ultimately improving the quality of care for patients.
However, with this goal in mind, our study presented here should be contextualized by the two main limitations that we identify and outline below.
As with other data-driven approaches, the evaluation performance discussed in our paper is limited by the choice of the (static) MIMIC-III data set. This data set could be seen as lacking diversity, as it only features a fraction of all possible ICD-9 codes and contains medical notes collected in English from a specific department of patients belonging to a specific demographic. While our approach does not make any explicit assumptions about the nature of the data other than the hierarchy of labels, in absence of formal guarantees, we cannot make rigorous statements about the efficacy of our (or indeed any related) approaches on clinical data gathered in different settings, such as other languages, countries or departments.
The second limitation is of a more practical nature, since 2015 the ICD-9 coding system is being phased out in favor of the more expressive ICD-10 system, thus ICD-9 coding has limited applications in practice. However, as with its predecessor, the ICD-10 codes are organized in an even richer hierarchy, which should enable the successful application of our proposed approach."
1875,"The proposed asymmetric Shapley interaction value could estimate asymmetric feature interaction in explaining the prediction of deep models. There are two major concerns regarding the time complexity: estimation of marginal contribution and construction of hypergraphs. In computing value function we have to consider more permutations to reduce approximation errors. Also, before estimating the contribution of asymmetric interaction, interaction graph with different orders should be constructed. We could resort to effective approximation methods in computing marginal contribution and prior knowledge in building hypergraph."
1876,"Our work focuses on domain generalization for abstractive summarization through prefix averaging. However, we do not experiment with larger backbone models due to computational constraints. Based on previous works we expect our approach’s performance to improve with model size. Also, a larger sequence length for prefix tuning increases the computational costs at inference.
Another limitation of our work is that we do not test it on natural language understanding tasks. This can be part of a future work."
1877,Section 7
1878,"One limitation of our proposed method is that it requires the pre-identification of the target MWE in a sentence before paraphrasing it, a task that is not a walk in the park. In particular, it is very challenging to identify what is the “correct” span of a given MWE, which our model critically relies on. For instance, given the MWE lip service (“insincere agreement”), our model predicts more attention as the best paraphrase, likely because the MWE is usually used as pay lip service to (something), and attention is one of the few words that fits well in this context (in terms of collocation). Therefore, the whole phrase pay lip service to should be identified as an MWE instance23 when it is used in sentences like They pay lip service to the idea; however, lip service can also serve as one lexical unit in sentences like It wasn’t just lip service. A similar problem arises when we deal with nominal MWEs that follow indefinite articles (a or an) as discussed in Section 4.1, or verbal MWEs that are often followed by specific prepositions (e.g. turn a blind eye to ... means “deliberately ignore ...”) because the MLM prediction is affected by the syntactic constraint.24 MWE span identification is also important in our sentence collection process; e.g. as discussed in Section 4.1, the phrase small fry can be used as small fry pan rather than as the MWE meaning “insignificant”, and hence collecting sentences based on string match resulted in one additional cluster that is not relevant to either its literal or idiomatic senses.
Another limitation is that our model cannot handle discontinuous MWEs such as throw someone under the bus and not ... in the least because it is not clear which parts to mask and paraphrase in such cases. Similar problems arise when continuous MWEs undergo either internal modification (e.g. go completely cold turkey) or drastic syntactic transformation (e.g. the beans are split). However, note that all of these types of expressions, as well as the pre-tokenisation problem discussed above, become a pain in the neck for any approach that regards an MWE as a lexical unit and learns its holistic embedding.
Lastly, our method heavily relies on the quality
23In fact, it is registered as such in some English dictionaries.
24In languages where words have grammatical gender such as Portuguese and Italian, this problem can be more pronounced because context words including adjectives and determiners are affected by gender.
of the clusters and is thus prone to error propagation. For instance, our model using BERT always generates large fish as the best paraphrase for the MWE big fish and fails to capture its idiomatic sense (“an important person”), likely due to its rare occurrence in monolingual corpora (compared to its literal sense). One possible solution to this problem is to derive more senses by allowing the clustering method to create more clusters with fewer instances, but that institutes a trade-off between accommodating rare senses and creating too many clusters for common senses; hence, there is no silver bullet. In fact, this problem pertains to the longstanding question (with no single correct answer) among lexicographers: how to “split” and “lump” senses of words, and how fine-grained the sense distinctions should be (Hanks, 2000, 2012)."
1879,"While our Solar has demonstrated its superior performance on three benchmarks, it still has several limitations. Firstly, Solar relies on accurate recognition of image caption and object-attribute detection models. If the features of these two parts are not correctly recognized, it will cause subsequent cascading errors. Secondly, it only demonstrates that language can serve as a unified representation in multi-modal QA, but has not been tested in other more multi-modal tasks, which we will leave as future work. Lastly, the experimental results do not delve deeper into which cases a unified language representation is better and in which cases a multimodal model performs better. We speculate that an integration of language models and multi-modal models will yield better results."
1880,Section 5
1881,"Despite strong performance compared to few-shot our self-training methods still contain significant room for improvement compared to the fully supervised benchmarks. It would be interesting to try larger language models to see if it is possible to close this gap with more knowledge embedded into the pre-trained models. Our evaluation of free text rationales are limited by the automatic metrics, which are necessary but not sufficient to analyze quality of an explanation for decision making of the model. From example explanations (a few of which are shown in Appendix), it is evident that we still lack understanding on multiple dimensions such as, when an explanation is factually wrong, is it due to
the model believing in the wrong knowledge or is unable to retrieve the correct one. Works that probe a language model with various prompts could be useful for investigating in these directions."
1882,"Our study includes some limitations that must be addressed. Some test examples might have wrong predictions made by the homograph disambiguation module. Specifically, in positive examples where lexical constraints should be imposed, its errors result in wrong corrections (i.e., the elimination of necessary lexical constraints). Table 12 shows how these erroneous corrections affect the results.
We can observe an overall decline in CSR; however, it does not hurt the translation quality. We
verify that the differences in BLEU resulting from wrong corrections are not statistically significant for all the methods. Considering the gain achieved in negative examples, as seen in Fig. 2, our proposed homograph disambiguation might serve as a useful starting point to address homographs in LNMT; however, there is still room for improvement. Our current homograph disambiguation module is designed as a stand-alone system outside the LNMT. However, building an end-to-end system can be beneficial, which can be addressed in future work."
1883,"Section Limitation
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
1884,"In our research, we have focused on one language pair, English↔Hokkien, and experimenting in both directions. In the future, we plan to apply the same methodology to additional unwritten languages to evaluate its broad applicability.
Our approach leverages parallel speech-to-text data between the unwritten language and a linguistically similar written language. There remains a question of whether there are unwritten languages without similar written languages."
1885,"Left blank.
7 A2. Did you discuss any potential risks of your work? The speech to speech translation application has been broadly study in the field. In our work, we expand the application to a new unwritten language, Taiwanese-Hokkien to English, and except that we did not enable new application in our work. Therefore, we did not discuss potential risk of our work."
1886,"The study presented in this paper has three main limitations. (1) While the design of the framework does not prohibit the utilization of longer textual forms, the two case studies presented deal with short texts. When dealing with longer text forms, we need to consider the cognitive load of having experts look at groups of instances. In our ongoing work, we employ strategies such as summarization, highlighting and other visualization techniques to deal with these challenges. (2) In the studies presented, qualitative researchers worked in groups to identify themes. Our goal in comparing two independent groups of researchers was to evaluate the degree of subjectivity by observing if the themes identified by the two groups would diverge. This setup might not always be realistic, as a lot of times qualitative researchers work independently or asynchronously. In the future, we will explore the effect
of the crowd in minimizing subjectivity, as well as the role that the computational tools play in more challenging settings. (3) Finally, we did not include a comprehensive user study to gather input from the experts about their experience with our framework. We consider this to be an important next step and we are actively working in this direction."
1887,"We have studied the task of socio-cultural norm discovery based LDC2022E20 dataset, which consists of everyday situational interactions in Mandarin Chinese. Although we believe that our approach can used in other cultural settings, the current state of the model might not be generalisable to other cultures, unless further tuning is possible. Our model’s ability in discovering such norms can help to improve conversational agents, however, real-world scenarios involving duplicitous or ambiguous terms might confuse our proposed approach. In addition, our model is limited to the textual modality, and we believe incorporating audio and visual features into the model can improve identifying socio-cultural norms. Nonetheless, the reliance of our model on large-scale pre-trained language models might result in some deployment challenges in situations with limited resources. Besides, all the reported results are by fixing a random seed running all experiments once."
1888,"Auxiliary attack and supports: VoteTRANS without support works well with an auxiliary attack which is the same with the target attack. In contrast, VoteTRANS with support achieves stable results with any auxiliary attack but it runs slower.
Short text and susceptible text: A short text is more difficult to detect than a long text. Susceptible text may bypass VoteTRANS as mentioned in Appendix I. However, the short text and susceptible
text are often unnatural and unclear meaning, respectively, so they are easily recognized by humans. Therefore, we recommend that humans recheck suspicious text with an abnormal ratio in the voting process of VoteTRANS (line 19 of Algorithm 1).
Beyond word-based attacks: We detect adversarial text up to word-based attacks, which change a few characters or words and are often imperceptible to humans. Other attacks remarkably affect the naturalness with a large change such as sentencebased attacks as in Iyyer et al. (2018).
Beyond text classification: We evaluate VoteTRANS on adversarial attacks targeting text classification. In contrast, the other tasks do not well-define a standard for generating adversarial text. For example, attacks targeting sequence models need to determine a threshold for BLEU score, which is aimed to minimize, but whether the score is sufficient for an adversarial text is still in question."
1889,"One of our limitations is that the data is split for short-term planning and long-term planning at fixed positions which on one side shows the overall planning capability on different datasets unbiasedly but on the other hand mixes the planning ability of the datasets with the overall performance of the embeddings. We have demonstrated in section E.2 that this can lead in many cases to unplannable examples. While this means that our embeddings should overall perform better than our results suggest, in the future, we should create either a human-filtered dataset where planning is always possible or either create a human benchmark as a further baseline. Furthermore, we rely in short-term planning (transformer guidance) on the generated utterance distributions by transformers where we have to balance between semantic diversity and the likelihood of utterances. We control these with temperature and nucleus sampling (top p) and found the best
‡In tribute to our fellow researchers in the field of physics for their inspiring work on the curvature of spacetime
trade-off with a temperature of 0.8 and a top p of 0.8. Nonetheless, this can still lead to utterances that might lead to the goal but that would be not considered by humans as very likely based on the given context as we explore in E.2. Furthermore, in the next utterance selection, we utilize the publicly available checkpoints which have been evaluated in the paper (Gao et al., 2020) on DailyDialog but both were seemingly not trained on an MDC-like task-oriented corpus. Since we find that the next utterance selection based on the curved property of the context in a task-oriented setting like MDC is almost always worse than just taking the last utterance, we have not expanded experiments in this domain."
1890,"The current work requires that knowledge modules be written by hand. Commonly used axioms, such as general knowledge like the commonsense law of inertia expressed by event calculus, can be reused easily, but there are vast amounts of other commonsense knowledge that are not easy to obtain. LLMs could be used to supply this information, but we have not tried. Knowledge graphs, such as Con-
ceptNet (Liu and Singh, 2004), COMET (Bosselut et al., 2019) and ATOMIC (Hwang et al., 2021), can be utilized to populate ASP rules. Like code models, we expect that LLMs could generate ASP code, which we leave for future work.
Also, when using large language models, despite various efforts, sometimes it is not understandable why they do not behave as expected."
1891,"Currently, our model only exploits the direct relations between nodes in the AMR graph. In other words, only one-hop neighborhoods can be considered. However, there are a few cases where an opinion word and a related aspect word can be in a k-hop neighborhood. In the future, we will design a model that can capture long distance relations in the AMR graph. Another limitation is that the errors of the pre-trained AMR parsers and AMR alignment models are propagated to the model as a whole. What is required is to improve the performance of those modules."
1892,"Section 8
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
1893,"The limitations are that our use of GPT-3 sometimes generates hallucinated texts, thus reducing
the effectiveness in generating valid paraphrases. The dictionary of toned-down words could include more semantic rules or could be built automatically, which will be left as future work."
1894,"Section 7
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
1895,"While we do explore a range of models in the 1- 20M parameter space, our work does not constitute a complete study of downscaling. In this work,
we aimed to explore the more fundamental components of model shape, model size, and input data.
However, our findings may not generalize to other models with alternative applications of downscaling methods. Considering it to be out of scope for this study’s assessment of pre-training effects, we did not compare our results to knowledge distillation methods of similar model shape and size. Furthermore, our exploration of model shape and size was limited to a model’s hidden size, number of hidden layers, embedding size, and intermediate size, and number of attention heads as these are the most commonly-tuned hyperparameters.
Our usage of vocabulary filtration as a means of downscaling input data size may not be the most effective means of limiting input data. While shown to be effective, alternative approaches for input data manipulation such as curriculum learning, and data pruning merit study beyond the scope of this paper.
Ethics Statement
Our exploration of smaller language models presents a number of implications for accessibility, environmental impact, and cost. By exploring models in the space of 1-20M parameters, our findings can inform language modeling work for those without access to large, GPU-enabled environments. This is important as it can encourage further research work in this space by those who are otherwise unable to work with SoTA LLMs. We acknowledge that our resources enabled the breadth of study in this paper; most of this study was conducted using a single GPU. This consideration underscores our commitment to improving accessibility for under-resourced technologists throughout the world. Furthermore, in working with downscaled LLMs, we hope to encourage methods that reduce overall carbon footprint and bolster sustainable practices in NLP. These considerations are especially important given the particular burden placed on those with limited access to electricity and technology. The cost of running and experimenting with these models may prove quite costly in terms of person-hours and compute resources. As such, we hope our work at smaller scale can help lessen these burdens, and positively impact the lives of technologists, and others. Any model from the study can be trained in less than a day on a single consumer-grade GPU."
1896,"We discuss some limitations of our current work which can be further explored in the future.
On Data Format. We specifically use fictional short stories as our primary data for the study since we require gold standard labels for this document classification task. Moreover, fictional short stories are easier to find as they often come with a specified grade level compared to other types of literary texts such as magazines or web articles written in any of the three Philippine languages. We do not claim that our models are able to generalize on these other types of literary materials or on other types of closely related language pairs unless a full study is conducted which is outside the scope of this work.
On Handcrafted Features. We were only able to use traditional handcrafted features covering countbased predictors such as sentence or word count and syllable pattern-based features for training the Random Forest models. We did not extract other feature sets one may find in the previous work on
English such as lexical density or discourse-based features since such features require NLP tools that are able to extract POS, named entities, relations, and discourse patterns that do not yet exist for all three Philippine languages used in this study. The work of Imperial and Ong (2021b) covered a small set of lexical features such as type–token ratio and compound word density for readability assessment in Tagalog. Still, we cannot use this approach since all languages would need to have the same number of features as is a standard practice in model training.
On Model Training. Our choice of the Random Forest algorithm for training the ARA models is based on the substantial amount of previous work supporting the application of this method to low-resource ARA, e.g., to Tagalog and Cebuano in a monolingual setup (Imperial and Ong, 2020, 2021a; Imperial, 2021; Imperial et al., 2022), where it achieved better results than other algorithms such as SVM or Logistic Regression. One can consider these algorithms for comparison but the analysis of each ARA model trained with various algorithms to the same level of depth and focus that we have given to the Random Forest classifier in the present study would require a considerable amount of time as well as a higher page limit.
On Current Measures of Mutual Intelligibility. The majority of existing literature in linguistics, specifically on the topic of mutual intelligibility in Philippine languages, discusses examples in the context of speech communication. As such, one might claim that Cebuano and Tagalog are not mutually intelligible by giving an example where a Tagalog speaker may not fully comprehend (or only recognize a few common words) another speaker if they are talking in Cebuano. While this is certainly true, in this study, we specifically focus on the mutual intelligibility of languages at a word and character level via written texts such as children’s fiction books. From this, we see a substantial degree of closeness between Tagalog, Cebuano, and Bikol compared to English. Thus, based on our results, we posit that mutual intelligibility may be used as an additional feature (see CROSSNGO in Section 4) for text-based tasks such as readability assessment. We leave the exploration of our proposed novel feature in the speech communication
area to future work."
1897,"Section 7
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
1898,"One potential limitation of our method is that it induces an extra cost of estimating training dynamic statistics of the data samples to characterize them (e.g., easy-to-learn or ambiguous) based on how they incorporate into the model’s learning. This may be more expensive for tasks and datasets with a large number of classes. In the future, we will focus on approaches to characterize the training examples on the fly."
1899,"Certain parts of our proposed methodology, for example, template-based replacement and n-grambased prompting are applicable only when stylespecific linguistic attributes could be identified between the source and the target text. And due to the cost of human labor and the lack of publicly available client-therapist dialogues, the sample size drawn in the study is small and thus may have an impact on the conclusions drawn. Our methods have only been tested for the English language. But we believe similar methods could be applied to other languages given they have unparallel corpora tagged with Advise without Permission and Advise with Permission labels. The rephrasing methods described in this paper are tested for short sentences with a maximum sentence length of 98 tokens. Thus, the scalability of these methods for long text still remains to be tested.
When testing the rephrasers, there are some combinations that could be tried other than the ones already tested. For example, more models can be fine-tuned and tested separately on templatereplaced and retrieval-based PP and PPA corpora but incorporating generic and N-gram prompting. In this work, we first combined these two types of corpora before attempting prompting since we could observe better performance on Blender when the corpora were combined.
In order to have more data, we combined the Advise with Permission and Advise without Permission responses present in CounselChat and RED datasets. But studies show that there are differences in the language used by counselors and peers (Lahnala et al., 2021; Mousavi et al., 2021). So, there can be linguistic differences between the same type of response in CounselChat and RED datasets. Future work should attempt to identify these differences and ideally rephrase the responses given by peers to reflect the language of the counselors."
1900,"Although Gender-tuning succeeds in reducing the gender bias scores in the pre-trained language models, there are some limitations to performing debiasing. Gender-tuning only works on gender-related words list. Thus Gender-tuning cannot cover the probable gender biases that do not exist in its’ list. We defer the gender-related word list modification to future research. All our experiments ran on English language texts with English gender-word morphology."
1901,"We summarize the limitations of our method as follows: (1) TextObfuscator was designed to protect word privacy in the inference phase, and we did not verify its ability to preserve other privacy attributes and training phase privacy. (2) Although we have done empirical experiments and visualizations to demonstrate the effectiveness of our method, a mathematical proof would enhance its privacy guarantees. (3) Our method requires more training steps than fine-tuning, resulting in an increased computational cost."
1902,"Task configurations are entangled with the full model parameters. In our ablation study of task configurations at training time (Table 3), we see that when training without task type, the model fails to generalize to FETAQA. Upon examining the model output, we find that although we change the output configuration to “long answer”, the model still produces a short-form answer. This indicates that model behaviors are not always aligned with a single configuration, leading us to question the extent to which each individual configuration influences the model. In order to have better and more interpretable control over the models, one potential avenue for future research is to develop pluggable task configurations, where each configuration controls a more atomic function of the model and can be plugged, unplugged, and combined to yield different model behaviors.
Our exploration scope is limited to table-to-text tasks. Due to the constraints of the computational resources, we haven’t explored joint training with a broader range of other NLP tasks. We think with some modifications, such as the inclusion of dataset domains in the configuration set, it would be possible to extend our approach to additional datasets and tasks."
1903,Section 7
1904,"While in-context learning methods for DST are promising in their data efficiency and flexibility to new domains, they typically require very large models to perform effectively. At 175 billion parameters, OpenAI Codex (Chen et al., 2021) is much larger than some of the fine-tuned approaches to DST, though with better performance and ability to adapt to new domains without re-training. Despite our advances, there are still significant errors when applying ICL for DST. As such, ICL may not necessarily be relied on in safety-critical settings."
1905,"The major limitations of this work are:
• We show results on two public datasets, from bio-medical and bio-chemical domains. These results may not generalize to other domains.
• Our results indicate benefit in low resource settings, while no appreciable benefit is seen for medium or high resource settings.
• Our method relies on GPT-2, a large language model that needs humongous compute resources and a long training time. It takes about 2 hours to generate 50 samples, versus the baselines like vanilla GPT-2 (ECG-LM) taking 30 mins or EntInj taking about 10 mins to generate same number of examples with much less memory requirements.
• We use quantitative measures to evaluate the quality of text generation, which might not be enough to capture the quality of generated text. Gold standard of measuring the quality is human evaluation, which is expensive."
1906,"Authoring transduction rules is relatively easy but may still be labor-intensive for complex domains. Future work might explore (semi-)automatically deriving transduction rules from data, learning to synthesize them from domain specifications, or curating a collection of domain-general transduction rules that can be imported into new domains.
Our experiments in this paper generated text only in English. It would be interesting to apply the framework to datasets in other languages, e.g., GlobalWoZ (Ding et al., 2022). While our framework is intended to be agnostic to the output language, our notation for response templates might need to be slightly extended (along the lines of Appendix A) to be more convenient to use with morphologically complex languages or free-wordorder languages. In these settings, presumably, the QCFG should systematically generate many inflections or orderings for the LM to choose among.
To support multilingual dialogue systems, future work could consider automatically translating the response templates into additional languages—perhaps by working backwards from automatic translations of natural language responses that use those templates.
Relatedly, we have only tested the proposed approach on dataflow graphs. Future work could apply the method to generate textual descriptions of other graph-structured inputs, such as graph databases or abstract meaning representation (AMR) graphs.
While QCFG productions were unweighted in this paper, giving them weights would allow the QCFG to express its own preferences about which productions to use for a given input. For example, in a product-of-experts architecture, the probability of a given response y, would be proportional to the LM probability of y times the weights of all productions used in the QCFG derivation of y (summed over all such derivations). Beam search (§4) could then be carried out using prefix weights (Opedal et al., 2023). The weights could be trained using gold responses.
Weighting the QCFG raises the possibility that the dataflow transduction rules could encode pragmatic context-dependent policies. For example, a dataflow transduction rule could call a neural network to assess the suitability of applying the rule to a given node in the dataflow graph, and then weight the resulting QCFG production ac-
cordingly.
Ethics Statement
Our proposed approach strongly outperforms a purely neural model at truthfully describing the result of a computation and its provenance. However, our approach can still make pragmatically unhelpful omissions, making it potentially risky to deploy in some scenarios. Additionally, we leverage pre-trained neural language models such as CodeT5, and as such, we acknowledge that our approach might inherit some biases present in these pre-trained models."
1907,"Section 8
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
1908,"In the current work, we adapt one-step gradient descent training for the outer loop based on our bi-level optimization framework. Since this outer loop optimization doesn’t have a closed-form solution, determining how many steps to perform for the outer loop for better outer optimization is still important to explore."
1909,"section 7
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
1910,"Recent work has shown that models of a certain size (upwards of 3B parameters) exhibit learning properties that cannot be observed in smaller models. Due to practical limitations and environmental concerns, in this study we chose not to train models larger than T5-Large. It is thus not possible to know how emergent properties in larger models may have affected the comparison between the different approaches compared here. We believe that our findings will nevertheless be useful to NLP practitioners who operate on a constrained compute budget and may thus opt for moderately-sized models anyway.
We compare encoder-only and encoder-decoder models for multi-label classification. Decoder-only models (Radford et al., 2019) are omitted since at present there are no decoder-only methods for multi-label classification in the literature. While we could have adapted the Seq2Seq approach in our experiments to operate in a decoder-only context, we deem this unsuitable for the datasets we work with, as they contain long documents which will quickly cause problems for standard decoder-only models like GPT-2.
Domain-specific pre-trained language models exist for both the legal and biomedical domain, which outperform their generic counterparts when used for classification tasks. These models all have an encoder-only architecture, however, which renders them unsuitable for a comparison of encoderonly and encoder-decoder approaches to multilabel classification.
Our experiments consider datasets from the legal and biomedical domains first and foremost because there are publicly available datasets with hierarchical labelling in these domains, unlike others. Moreover, we believe that working in critical application domains is a worthy purpose and covering two such domains with two different datasets in each domain gives us a good view on how the examined methods are expected to work in such domains."
1911,"For now, the superiority of the proposed two-stage inference speed-up method cannot adapt to inductive datasets since the generated sequences are difficult to map to unseen entities. Therefore, we will explore how to efficiently perform KGC under the inductive setting in the future.
Like the other text-based KGC methods, our GHN lag behind embedding-based methods on FB15k-237 dataset. Cao et al. (2021) claims that many links in the FB15k-237 dataset are not predictable based on the information in the KG and we hypothesize this may harm the training of textbased models. In the future, we intend to examine this more thoroughly."
1912,Yes. We have. It’s not a numbered section and comes right after the conclusion.
1913,"The main limitation of HRT is that its upper bound on the inference speedup is lower than that of single-iteration NAT under the same network architecture. As demonstrated in Appendix A, the average speedup of single-iteration NAT (i.g., CMLM1) is 4.7x/3.2x on GPU/CPU, respectively, while that of HRT is 1.7x/1.6x. To achieve higher acceleration, HRT needs to employ the deep-encodershallow-decoder architecture. Increasing the chunk size is a simple way to reduce the autoregressive cost, yet it results in severe BLEU degradation (see Table 5). Further research should be conducted to maintain high translation performance with fewer autoregressive prompts."
1914,"In this work, we have been focusing on improving the performance of tagging-based Grammatical Error Correction. Our work has the following limitations: (1) We work on three recent Chinese Grammatical Error Correction datasets. But there are many emerging datasets from various languages. We will add support for these languages on our GitHub repository and make all resources publicly accessible. (2) We point out a limitation of inference tweaking, but it remains to be explored how to
explain the phenomenon and derive better tweaking methods."
1915,"In this work, we only consider the pre-train then fine-tune paradigm which assumes that model weights are tuned for adaptation to specific tasks. Future work, once more capable multilingual LLMs are released, may also consider the few shot, and in-context learning-based setups to accommodate for more recent approaches towards adaptation in NLP. Future work may also consider setups more relevant to different, more diverse tasks (e.g. including webtext)."
1916,"This paper proposed a novel pre-training model COMAVE which aims at textual AVE tasks, while in this field, multi-modal AVE tasks also widely exist in many e-commerce platforms. We expect that the following works can leverage COMAVE as a powerful word embedding pre-training model for text encoding combined with image feature representation in multi-modal AVE tasks in the future. Meanwhile, the same as the previous AVE works, we assume that each T is an independent extraction object, without considering the context-dependent of the whole data resources, such as long documents and instructions, which exceeds the length of an allowable single input."
1917,Section 7: Limitations
1918,"We acknowledge that the StereoSet and CrowS datasets and metrics are not ideal evaluation measures for debiasing work (see Blodgett et al. (2021) for more details about their pitfalls). We advise practitioners to conduct careful analysis for their specific use case rather than interpreting the scores from our experiments as clear measures of bias mitigation or removal.
Furthermore, we realize that in discussion of harms, we should also ensure that allocative harms do not arise from dependency on a PCGU-debiased model. In this paper, we do not report experiments on models finetuned for other downstream tasks, as finetuning is generally more prone to spurious correlations and accidentally encoding bias, so evaluating such models obfuscates the procedure’s effect on the pretrained model. Instead, we focused only on the masked language modeling task such that intrinsic and extrinsic evaluations both use the pretrained model directly and only. In the modern age of large language models, this is arguably more applicable, but this setting doesn’t take into account the effects of prompts on the prediction distribution. An interesting extension of this study would be to debias using some form of PCGU in the pure generation setting and evaluating with high quality generation-based resources such as HolisticBias (Smith et al., 2022). However, the base form of PCGU is not directly applicable due to the difficulty in attaining and using minimal pairs/tuples in generations.
Another related limitation is that our experiments were only conducted in English. However, many languages, such as Spanish or other Romance languages, have a much richer concept of grammatical/lexical gender sometimes affecting multiple words per sentence.
Unfortunately, a fundamental problem with interpretability arises if we wish to evaluate the language model’s bias implicitly. For example, the prediction in Figure 2 suggests that the debiased model is less biased than a model predicting the full probability mass for the female term. Discrete metrics fail to account for this behavior, so better evaluation metrics would also give us a better sense of the efficacy of our proposed method.
We further note that gender, which has historically been treated as a binary construct, is likely to be a relatively easy domain to work with. Other more complicated social biases like racism and
classism are similarly harmful, and an ideal debiasing procedure should work for all of them. Similar questions may arise about if we can ever comprehensively cover all domains without a better way to generalize across domains. It is also to be seen if PCGU can be directly used for other domains, as our experiments only touched on the intersection of gender and profession biases while observing that this has effects on other domains. Further work would be required to understand why, and in what contexts, PCGU can affect unseen domains; are the cross-domain results in the main paper artifacts of intersectionality (between seen and unseen domains) or is this truly generalizations across a broader notion of bias?
Due to the complexity of social bias, it is not obvious if a properly modeled dataset for such other domains of bias can be easily constructed for usage with PCGU. A natural thought would be to attempt to generate training data for PCGU. We attempted this but found that the generations were not reliable in terms of providing signal for what constituted bias. By using a templated dataset like WinoGender, we can ensure that every instance in the training set encodes bias by an assumption of gender based on only the profession.
Obviously, partitioning at the most granular level where each single parameter is its own part would make our directional comparison meaningless. However, we did not extensively study how important the specific partitioning method was. An interesting class of experiments would be using some sort of random partitioning, where each individual parameter is assigned to its group of parameters not according to any architectural reason but according to some sort of randomness. Our implementation of this made the gradient selection extremely expensive because it required too much indexing into tensors as opposed to a full replacement of specific dimensions. A better implementation or experiment would be needed to draw actionable conclusions about different partitioning methods. However, our baseline experiments for this matched with the intuition that sampling each weight as being a bias or non-bias weight using a Bernoulli distribution yields a similar effect as regular training with dropout, similar to the k=All experiments in Table 2."
1919,See the Limitation part.
1920,"One of the main limitations is that we focus solely on the IETF. Consequently, we can never be completely sure how well our findings generalize to other similar organizations without further annotation.
We are also limited by not conducting a hyperparameter search on our models. We omit this step as the main goal is not maximizing performance, but rather data annotation and analysis. In a similar vein, it is likely possible to increase performance by using a more advanced model that is either trained on dialogue-like data or is specifically designed to exploit phenomena specific to dialogue (e.g., having speaker embeddings).
We also acknowledge that many emails are longer than 512 tokens which is the limit of our BERT model and thus might have been cut short. However, most of the emails do fit into this limit."
1921,"We point out the limitations of large language models (costly to train, deploy, maintain, hallucinate, opaque). The vision of POSTTEXT shows promise of less costly training, maintenance, and more explainability. However, no actual system is built yet to validate these claims and it is also not clear that a system with POSTTEXT architecture will be easier to deploy since it has more components."
1922,"Limitations to our work are as follows: (1) We only study the three magnitude effects for the number word and digit denotations of the numbers 1 to 9. The effects for the number 0, numbers greater than 10, decimal numbers, negative numbers, etc. are beyond the scope of this study. Future work can design behavioral benchmark for evaluating whether LLMs shows these effects for these other number classes. (2) The mapping of LLM behaviors to human behaviors and effects might vary for each effect. Thus, we might require a different linking hypothesis for each such effect. (3) We only use the models built for English tasks and do not evaluate multi-lingual models. (4) We report and analyze aggregated scores across different dimensions. There can be some information loss in this aggregation. (5) Our choice of models is limited by certain resource constraints. Future works can explore the use of other foundation / super-large models (1B parameters +) and API-based models like GPT3 and OPT3. (6) The behavioral analysis of this study is one-way: we look for human performance characteristics and behaviors in LLMs. Future research can utilize LLMs to discover new numerical effects and look for the corresponding performance characteristics in humans. This could spur new research in cognitive science. (7) The results show similar outputs to low dimensional human output and show that we do not need explicit neural circuitry for number understanding. We do not suggest models actually are humanlike in how they process numbers."
1923,"PRAGMATICQA is collected via crowdsourcing on English-language material from Fandom.com, where community-maintained wiki pages are used as reading materials and basis for answering questions. Therefore, it cannot be guaranteed that the excerpts from Fandom will be factually correct or stay unchanged over time, and in turn the answers in PRAGMATICQA are also not factually verified. Furthermore, techniques or models developed on PRAGMATICQA might not be generally applicable to non-English languages or non-entertainment topics without further adjustment or evaluation.
More importantly, the crowd workers that participated in PRAGMATICQA are geographically limited to primarily English-speaking countries, and therefore might not represent typical pragmatic reasoning behaviors of people that speak different first languages or come from different cultural backgrounds. Therefore, it should not be treated as a universal standard for pragmatic reasoning in information-seeking conversations, but rather a single reference point."
1924,"An important limitation of our work concerns the definition of the protected attributes in the datasets used for evaluation. In particular, gender in BIOS and PAN16 is limited to the binary female/male, lacking an inclusive and nuanced definition of gender. Similarly in FDCL18, we consider only two dialects of African American and White American, while clearly this definition is limited and noninclusive. Furthermore as in previous work (Sap et al., 2019; Ravfogel et al., 2020; Zhang et al., 2021), the labels of this protected attribute are assigned through a probabilistic model, and hence the dataset might not represent the nuances and traits of the real-world.
The second limitation regards reaching strong conclusions on the generalizability of the multiattribute setting for MODDIFFY over any possible number of protected attributes or subset of them. Our multi-attribute experiments are conducted on one dataset with two attributes of gender and age, particularly due to the lack of available suitable datasets. Hence, Further studies (as well as more suitable datasets) are required for achieving a more comprehensive picture on the topic.
Finally, we should also highlight two general limitations, shared with the other related studies in the area of model bias mitigation. First, we should consider that the aim of representation disentanglement optimizations is to reduce the existing correlations in the model with the protected attributes based on the observed data. These data-oriented approaches might lack effective generalization, particularly when the model is evaluated in other domains or out-of-distribution data. Second, our bias mitigation evaluation is grounded in the notion of fairness through blindness, and the debiasing optimization methods are designed to support this form of fairness. The effects of our method on other possible definitions of fairness are therefore left for future work."
1925,Left blank.
1926,"Even though we performed a rigorous literature search to try to cover all existing work on scientific fact-checking, there is possibly work that was left uncovered due to different keywords, naming conventions (e.g., fact-checking vs. claim verification). Whenever possible, we tried covering all related work and all relevant cited papers.
All approaches for automated scientific factchecking described in this work are still not safe for widespread adoption in practice due to constraints to their performance. Having deployed automated fact-checking systems that would produce incorrect verdicts could lead to mistrust in their usefulness and the process of fact-checking itself, including the work of dedicated manual fact-checkers."
1927,"One major limitation of Uni-Encoder is its suitability only for generation-based dialogue systems in which the number of responses is small. A twostage approach is necessary for retrieval-based systems: Context-independent encoding methods like Poly-Encoder first filter out a small set of candidates from the large pool, then Uni-Encoder can pick out the best response from the pre-filtered collection. Moreover, as discussed in Section 5, Uni-Encoder could be a good component of the RLHF approach. However, the increasing research of pure generation methods with alignments bakedin (Arora et al., 2022; Liu et al., 2023) may gradually replace the SFT+RL method. Consequently, Uni-Encoder will have a smaller and smaller impact in terms of application. Nevertheless, because Uni-Encoder unified all other ranking paradigms, we believe it remains helpful even as a theoretical framework."
1928,Please refer to Section 6
1929,"Our proposed method is an offline system in which the input is a dialogue containing all utterances rather than a single utterance input in chronological order. An online system for emotion recognition can be applied in real-time conference systems or human-computer interaction, so the online system has potential value for future research. Our method can be built into online systems by creating buffer systems such as history windows. However, all the baseline methods in the past are offline systems, such as COGMEN, DialogueRNN, etc. In addition, the form of datasets also leads us to construct an offline system for training and testing. On the other hand, the offline system also has application scenarios such as analyzing emotions of posted videos, opinion mining in social media, etc. Therefore, our method only builds an offline system under the offline experimental setting that can be compared and evaluated.
Besides, the input of our method is feature-based. The original text, audio, and video files will first pass through feature extractors to obtain multimodal features, which may cause information loss and hurt performance. We focus on feature-based training methods because training based on the original files costs a lot. For example, training a video encoder generally requires several V100 GPUs and days of training time. Therefore, we, including the baseline methods we compare, adapt the feature-based training methods. When the cost permits, training based on source files is worth exploring in future work. With feature-based training methods, different baseline methods use feature extractors to obtain features, leading to a lack of fairness in method comparison. In this regard, we reimplemented all open-source methods and compared them using a unified feature file to ensure the fairness of the experimental results. At the same time, we also conducted evaluations with different signature files to verify the generalization of the method."
1930,Section 6.
1931,"One limitation of our method is that it requires multiple generations to achieve the best performance on stance detection datasets. While the best student model significantly outperforms strong baselines, it takes longer training time and requires extra memory for the teacher model. This is a common limitation for knowledge distillation in generations. Another limitation of our method is that the improvements brought by knowledge distillation saturate after a few generations, which can be also observed in previous work. We will explore how to improve the performance saturation in the future."
1932,Limitation Section after the Conclusion.
1933,"As shown in Table 1, an overwhelming number of catering reviews on Yelp makes the countertemplates with obvious catering information. For example, “This is a great place for a quick bite to eat. The food is delicious and the staff is very friendly. They have a good selection of beer and wine. The place is always busy, but it’s worth the wait.” In this case, the pattern information in the text is not consistent with other businesses irrelevant to catering. As shown in Table 6, although our method has a slight improvement over the previous methods in the mean and standard deviation, it is only comparable to the SOTA at the best performance. Since the counter-template is exactly the same text for the whole data set, the performance of the model is affected perhaps when the pattern information from different texts in the data set has large differences. When we extract the restaurantrelated parts of the dataset as Yelp-Res that have more similar pattern information, our model performs better."
1934,"Inapplicability to reference-free evaluation: Since our MRE supposes that there is an available reference question to be augmented (paraphrased), it is not applicable to reference-free question evaluations such as QRelScore (Wang et al., 2022a) and RQUGE (Mohammadshahi et al., 2022).
Inapplicability for answer-unconditional QG frameworks: MRE can’t be applied to answerunconditional QG frameworks because it only augments the reference question by paraphrasing without considering other possible questions of supposing other answers.
Large computations: To generate multi-reference questions, our method requires inference of large language models, which results in huge computational costs. Therefore, this can become burdensome as the test dataset grows."
1935,"In this paper, the proposed XtremeCLIP framework is mainly focused on CLIP-based deterministic VLU tasks. In future work, we will extend XtremeCLIP to other Pre-trained Vision-Language models and apply XtremeCLIP to generative tasks such as image captioning, visual grounding or visual relation extraction."
1936,"Despite the significant advancements made by the proposed FACTUAL-MR representation in addressing the limitations of current scene graph parsing datasets, there remain several areas for future research.
First, FACTUAL-MR currently relies on heuristic rules to resolve the collective-distributive ambiguity as introduced in Section 4.2. However, the limitations still remain due to the ambiguity of language. To obtain a perfect parser, rich-world knowledge from multi-modalities or textual context (Li et al., 2020) is required, which is left as our future work.
Second, there is currently no explicit alignment between objects represented within FACTUALMR and the corresponding bounding boxes in the image. To fully utilize multi-modal information, collecting such alignments may be necessary.
Third, the proposed method utilizes ORACLE scene graphs of the image, however, in practical applications, extracting a scene graph from an image remains a challenging problem. Further research is required to determine if utilizing a visual scene graph parsing model to extract scene graphs from images would negatively impact image retrieval performance.
Lastly, our current approach utilizes a large pretrained language model to train the parser. However, the issue of robustness in parsers (Huang et al., 2021; Zhuo et al., 2023) has always been a significant concern. The captions in the VG dataset mainly consist of short sentences with simple patterns. It remains unclear whether the parser is robust enough to handle sentences with more complex linguistic variations, which calls for further investigation."
1937,"For the Top K operation in target relation calculation, we set K’s three hyperparameters (i.e., ks, kt and k) to determine the number of the related targets. To explore the influence of the selection of K on model performance, a grid search on these three hyperparameters needs to be conducted to iterate each combination. However, due to the time and resource limits, we explore the impact of one hyperparameter in K by controlling the other two hyperparameters. Based on the empirical findings from this, we then set the value of K so as to achieve an appropriate performance."
1938,"We have identified several limitations in our work and propose future directions to improve them:
(i) The sources for UR-QA in this paper are limited to the document corpus and QA-history, but our unified reader is not restricted to take specific sources. Further research can explore the generalizability of UR-QA to more diverse sources, such as linearized knowledge sources as proposed in (Oguz et al., 2022). Future work can also explore the optimal method for considering LM likelihood, answerability, and consistency together.
(ii) Though it is not the focus of this work to optimize readers, our proposed UR-QA can orthogonally benefit from improvement in retrieval. Further study on the retrieval for UR-QA can be conducted, including the direction to co-optimize the reader and retriever as proposed in (Izacard and Grave, 2020)."
1939,"Although our model allows users to interpret which parts of the input document are most relevant to the model’s prediction, our model does not allow users to interpret which text spans of the input summary contain errors. We use the summary in Table 4 as an example. If the model can indicate the text span “a school in northern ireland” contains errors, it will be easier for users to correct the summary, potentially benefiting factual error correction systems (Fabbri et al., 2022a; Huang et al., 2023). Kryscinski et al. (2020) introduced an auxiliary task to extract erroneous text spans in summaries, but their method requires expensive text span ground-truth labels. Locating incorrect text spans in the summaries without requiring spanlevel training labels remains unexplored. Another limitation of our model is that it does not allow users to interpret the uncertainty of the prediction results (Deutsch et al., 2021)."
1940,"Additional unnumbered section after Section 8 Conclusion
7 A2. Did you discuss any potential risks of your work? We provide a framework to correct for errors made by other automated systems. The application scope is extremely limited, as such we expect very low potential for malicious use compared to other works."
1941,"One limitation of CoT-KA is that it performs finetuning based on the PLMs, and the input sequence length limit of the PLMs allows us to add only a limited number of CoTs. Therefore, it is important to explore and develop a CoT selection strategy in future research. A good CoT selection strategy would enable the identification of highly effective CoTs from a set of CoTs, enhancing the efficiency of KADL."
1942,"In this paper, we focus on the hybrid QA task, where the answers to most questions can be extracted from cell values in tables and linked passages using a reading comprehension model. Although TACR performs well in cell selection, one of its limitations is that it lacks numerical reasoning
ability across different cells, such as counting and comparing. To enable TACR to answer numerical questions, we will further develop its numerical reasoning capabilities in future work. Another limitation of TACR is that it shows a strong ability in column selection while performing relatively worse in row selection. For future work, we plan to try to improve its row-selection accuracy."
1943,section 6
1944,"Cross-Cultural Inference Beyond Codenames Our work explores sociocultural pragmatic inference in a very limited setting, using a core vocabulary of just 100 words. Despite this limitation, we find significant diversity in our dataset; furthermore, our models successfully capture these diverse inferences. While a limitation of our work is its focus on a single setting, we expect domains outside of Codenames to see similar variance. Understanding and highlighting miscommunication in dialog—due to culture-dependent misinterpretation—is one such extension. These domains are likely much nosier than Codenames; we urge future work to further investigate them.
Spurious Correlations across Sociocultural Factors Across all tasks but one (Target Rationale Generation §4.1.3), jointly modeling all sociocultural priors does not result in the highest performing model. Because our sociocultural factors already correlate with each other (§3.4), we suspect that modeling all features may be redundant, adding spurious correlations and resulting in overfitting. Improved modeling methodology and careful regularization may address these issues; we leave these experiments for future work.
Bigger Models and Task Specific Modeling Currently, we evaluate small Seq2Seq models due to computational constraints; however, evaluation of 0-shot and few-shot performance on larger language models (e.g. GPT-3) is necessary. Given the changing state of the Codenames board—along with evidence that LLMs struggle with theory-ofmind-esque perspective taking (Sap et al., 2022)— our dataset can serve as a challenging benchmark for sociocultural understanding. However, successfully encoding game state into prompts for LLMs may require experimentation.
Finally, our current task formulation and modeling setup are straightforward: we simply encode all information in-context and do not assume recursive reasoning like in RSA (Goodman and Frank, 2016). Future work can explore these directions.
Human Evaluations Our evaluation is limited to automatic metrics and qualitative analysis. Evaluating cross cultural generation depends on the evaluator’s own culture. Each generation depends on the player’s sociocultural background; finding evaluators who match the player may be prohibitive."
1945,Section 8
1946,"Since our generative approach to product attributevalue identification autoregressively decodes a set of attribute-value pairs as a sequence, the inference is slow (Table 5) and how to linearize the set of attribute-value pairs in the training data will affect the performance (Table 6). The best way of composing an attribute-value pair and ordering the pairs will depend on the characteristics of the datasets such as the existence of canonicalized values and the number of attribute-value pairs per example. Those who attempt to apply our method to their own datasets should keep this in mind."
1947,"Although these experiments were performed on only one dataset, it is indeed large with data from 82 participants. That said, it will be nice to perform experiments with more listening datasets.
We experiment with a linear encoder – Ridge regression. We plan to experiment with more complex encoders as part of future work.
This work was done on data related to English stories only. Several other languages belong to the same language family as English (Malik-Moraleda et al., 2022). While we can expect the insights
and learnings to hold across languages in the same language family as English, empirical validation needs to be done. For languages in other language families, syntactic structure may be very different from English. Hence, more work needs to be done to check which of these insights hold for datasets in other language families.
This work was conducted on a dataset where the participants were involved in the listening task. However, the stimuli was represented in the text form. We believe that an audio form of the stimuli can lead to improved insights. Thus, more work needs to be done to design representations (like prosodic features) for auditory stimuli."
1948,"In our experiments, as NRMs with cross-encoder are widely used, we focus on evaluating the textual adversarial robustness during the re-ranking stage and do not currently take into account the effect on the retrieval stage. But actually, in a “first retrieval then re-ranking” ranking paradigm, the attack is effective only when the adversarial documents are passed into the top retrieval results. Meanwhile, dense retrieval (DR) models have been widely studied, and they may also inherit adversarial vulnerabilities due to the basics of PLMs. Besides, due to limitations in our computing resources, we only tested adding adversarial text to relatively short documents (i.e., passage-level), but the document content in real-world applications could be much longer. Therefore, further comprehensive investigations on examining the NRMs with different architectures, the effects of attacks on the retrieval models, and the manipulations on longer documents are left for future work. Finally, it is important to note that mitigation and defense methods against adversarial ranking attacks are currently understudied, making it a significant area for future research."
1949,"Although our proposed method exhibits great performance to generate more smooth and natural emotional support than baseline models, we argue that the research on this field still has a long way to go. We conclude three aspects that may inspire further exploration. First, the automatically annotated emotion labels may be a little bit coarse and may not accurately manifest the emotional states of the seeker. Second, since various types of commonsense knowledge are not introduced, the current chatbots always generate general and safe responses, failing to provide specific and personalized suggestions to help the seeker get over the dilemma. Finally, current automatic evaluation metrics are still not rational and proper to measure the ability of chabots to provide emotional support. It is desirable to build better evaluation metrics for this."
1950,Section 8
1951,"Despite the simplicity and strong empirical results, RESIDUAL PROMPT TUNING still has few limitations. First, its performance is still not on par with fine-tuning on (e.g. 7.8 points difference with T5L model and 100-token prompt on SuperGLUE average score). Also, our method uses slightly more parameters than prompt tuning to train the reparameterization network. However, this is not a significant limitation given the full language model size.
We have tried to cover several model architectures, but so far we have focused on encoder-decoder (T5) and encoder-only (BERT) models. In future work, we would like to investigate decoder-only methods (e.g. GPT). Another limitation is that our method (similarly to other prompt tuning-based methods) strives to reduce the number of trainable parameters, but uses a longer sequence than the original input text (due to the injected prompt)."
1952,"Our model currently requires high-quality word boundaries for both speech and sign videos. However, as demonstrated by our preliminary results in Table 5, we can overcome such limitations by incorporating more powerful unsupervised segmentation algorithms to our system. Further, while our dataset is sufficient to model the variability in speech and videos, all experiments to date have assumed that spoken and signed sentences share similar word order, which may not be true of natural spoken and signed communications. A future direction of this research will seek to develop methods for spoken-sign language pairs with very different syntactic structures. Lastly, the vocabulary size under our study on word-level SSR-U is relatively small (<1000), and a promising future direction is to extend the current approach to deal with much larger vocabulary size in more diverse conversations."
1953,"Our work is not without its limitations. First of all, our annotated data is relatively small. However, given the relatively straightforward task (as reflected in high IAA), and since we are using this data only for evaluations, we believe that this amount of data is sufficient for the research questions we are asking/answering in this paper. Second, our data entirely comes from the politics domain and social media, situated in the US context. This choice was driven by our downstream use case of a large scale social science analysis in the US political domain. While we have not established how well our tagger performs in domains other than politics, given that our tagger relies on contextualized language models trained on web data and since it is performing a basic linguistic task, we believe that the performance is robust across domains used in Section 3 and 4. However, we expect performance degradation with genre or dialectal shifts with substantial differences in syntactic patterns. Third, we have not fully exploited the utility of the dataset in this work. As mentioned in Section 2.2, our aim in this paper is not to build the best tagger possible, and hence we did not explore state of the art modeling techniques such as few-shot learning. Finally, our work is done entirely on English language data. While we believe that similar approach could work in other languages without vocative markers, more research need to be performed to verify that. While we acknowledge these limitations, we reiterate that these are outside the scope of what could be meaningfully done within this short paper."
1954,Left blank.
1955,"Limitation section and Appendix H.
A2. Did you discuss any potential risks of your work? Not applicable. We provide an evaluation metric for QG task. limitations are provided in Conlusion and Limitations sections."
1956,"Language-related limitations. For the ease of the analysis, we conducted experiments using only the English dataset in this study. Although our proposed method can be applied to any language, its performance must be evaluated on languages other than English. For example, the SemEval2020 Task 1 dataset includes Latin, German, and Swedish language datasets, in addition to English, and can be used for this purpose. In particular, our proposed method requires only pretrained MLMs and does not require additional training data for the target languages, which makes it easily scalable to many languages.
Availability of MLMs for the target language. Experimental results show that the quality of the MLM is an important factor determining the performance of the proposed method. For example, the proposed method reports good performance with vanilla BERT model in Table 2 but further gains in performance can be obtained with the fine-tuned BERT model on masked time stamps. However, since our method assumes the availability of pretrained MLMs, a problem arises when trying to adapt our method to minor languages where no pretrained MLMs are available. This limitation could be mitigated to an extent by using multilingual MLMs. For example, Arefyev and Zhikov (2020) demonstrated that satisfactory levels of accuracies can be obtained for semantic change detection by using multilingual MLMs. Our proposed method can further benefit from the fact that new and larger MLMs are being publicly released for many languages in the NLP community."
1957,"Section Limitation
3 A2. Did you discuss any potential risks of your work? 3"
1958,"The limitations of this paper mainly lie in the following folds: (1) We do not provide any theoretical analysis for the correlation between long-range dependencies and repetition loops, as well as solutions to avoid repetition loops with maximizationbased decoding. (2) We do not discuss the source of LMs’ learning bias, which may be caused by multiple factors, such as the Transformer architecture (Vaswani et al., 2017), the MLE loss, or the auto-regressive generation manner. (3) We conduct experiments based on GPT2 due to resource limitations. The conclusions may differ for extra-large LMs (such as GPT3). (4) We do not experiment with RNN-based models, which are also shown to prefer repetition (Elman, 1990). (5) We do not perform the manual evaluation to compare SELFCONT with baselines since we focus on repetition in this
paper, which can be automatically evaluated reliably. Perplexity and mauve scores are also shown to correlate highly with manual evaluation for evaluating fluency and overall quality, respectively."
1959,"Below, we outline several limitations of our work. Data coverage. Our claims are only valid for the datasets accessed in our study. We use the Microsoft Academic Graph (Sinha et al., 2015) and S2ORC, which is larger than other publiclyavailable scientific text corpora (Lo et al., 2020).
However, these sources can differ from other collections of scientific text, because which journal/venues, sources, and resource types constitute “science” differs across academic literature search systems and databases (Gusenbauer and Haddaway, 2020; Ortega and Aguillo, 2014). In particular, since a substantial portion of S2ORC comes from scrapes of arXiv and PubMed, its coverage of computer science and medicine is better than that of other fields (Lo et al., 2020). Also, our coverage is limited to English articles. Past work has shown that citation-based metrics of impact favor articles written in English, and articles from non-Englishspeaking countries have different citation patterns compared to others (Liang et al., 2013; Liu et al., 2018; González-Alcaide et al., 2012). Finally, we recognize that MAG field of study labels are contestable and imperfect. For example, less than twothirds of ACL articles are labeled as natural language processing, and the most popular subfield in ICML is mathematics rather than machine learning.
Token-level analyses. Another limitation of our study is that many scholarly terms are not single words or tokens, but rather phrases. Phrases are somewhat accounted for by measuring words’ senses, since senses induced by language models reflect words’ in-context use, including their use in discipline-specific phrases. For example, Table 3 shows that title has a sense specific to stereochemistry, and in abstracts, this word often occurs in the phrases title reaction or title compound. Phrases containing distinctive words are also somewhat accounted for by measuring individual words in the phrase. However, phrase-level measurements of jargon would likely still be useful for improving interpretability and downstream applications of our metrics, and so discipline-specific phrases are a promising avenue for future work.
Compute. Science of science is interdisciplinary and involves a range of organizations and institutions. Not all researchers will have easy access to the computuational resources needed to replicate our study or apply our approach to data of the same scale. The most resource intensive step of our pipeline is when ScholarBERT predicts each instance of a vocabulary word’s top 5 substitutes across CONTEMPORARY S2ORC and WIKISAMPLE. This took approximately 90 GPU hours split across Nvidia RTX A6000 and Quadro RTX 8000 GPUs. ScholarBERT itself is a 770M-parameter BERT model (Hong et al., 2022), and generally our
compute infrastructure included machines with 64 to 128 cores and 512 to 1024 GB of RAM.
Social implications. In §6.2, we define “success” in two ways, both of which are based on citations. However, though citations are an important currency in science, they are imperfect signals of credit or impact. One article may cite another for reasons that span a range of significance, from brief mentions of related background to core motivation (Jurgens et al., 2018). In addition, associations between jargon use and scientific success may differ as success is redefined using indicators beyond citations. For example, success could be defined beyond scientific communities, such as findings that lead to societal change, products, and use (Bornmann, 2013). Finally, our study on the relationship between jargon and success is not causal, but associational and descriptive."
1960,Section 9
1961,"Our experiments were performed under some limitations. Since our work deals with both privacy and bias, we tried to keep the individual concepts within bounds, and thus only focused on the oftentreated case of gender bias. Other works, however, also consider cases of, for example, stereotypes towards members of the LGBTQIA+ community or different religions (Barikeri et al., 2021; Nozza et al., 2022). Additionally, we adopted the simplified assumption of binary genders without considering other existing identities such as non-binary or trans*7.
7https://www.gendercensus.com/results/ 2022-worldwide/
Furthermore, our computational resources were limited. Training with DP requires a lot of GPU memory (cf. Yu et al. 2021a; 2021b), which is why we could not train the entire GPT-2 medium with DP. Moreover, we could only train with a batch size of 2. Compensating this by increasing the gradient accumulation steps was also only possible to a small extent due to the limited memory. However, it is likely that DP could have a higher effect on some of the evaluation frameworks when applied to all layers of the model. It would have been of great interest to see if the effect on fairness would have been different. Furthermore, the dataset we used for training was relatively small. Due to limited computational resources and the overall good compatibility with Opacus (Yousefpour et al., 2021), we worked exclusively with GPT-2. For future work, it could be interesting to determine the studied effects in other models.
In the experiments, we found that both dropout and CDA did not provide unambiguously reliable mitigation results. We agree with the finding of other authors that the reliability of SEAT is not beyond doubt, as no bias with statistical significance is found even in the pre-trained GPT-2 model (cf. Kurita et al., 2019; May et al., 2019; Meade et al., 2021). For the other two approaches (StereoSet and BEC-Pro), the model must make predictions with respect to very specific stereotypes, and these predictions may not necessarily be changed by training on a counterfactually expanded data set or increased dropout. Moreover, we evaluated our models on the GLUE benchmark, without focusing on individual tests. More closely examining this would be interesting scope of future research."
1962,"The adversarial learning still requests a proper selection of hyperparameters, otherwise the training procedure could be unstable. Besides, training speech diffusion probabilistic models typically require more computational resources, and degradation could be witnessed with decreased training data.
Our proposed model lowers the requirements for high-quality speech synthesis, which may cause unemployment for people with related occupations, such as broadcasters and radio hosts. In addition, there is the potential for harm from non-consensual voice cloning or the generation of fake media, and the voices of the speakers in the recordings might be overused than they expect."
1963,See Section 8.
1964,"Research on the speech-to-singing conversion is important for human voice study and useful for practical applications such as computer-based music productions or entertainment. However, current STS approaches require an input condition of a fine-grained target F0 contour, which is always unavailable. In addition, the F0 contour of a singing utterance often possesses rich speaker-related infor-
mation, which still needs further disentanglement. Finetuning F0 contours in real applications brings significant extra work. One of our future directions is to simplify the input conditions, such as musical scores. Furthermore, the preliminary attempt at the zero-shot STS task may lead to a better perspective.
Except for positive applications, STS systems may face ethical concerns. With the development of speech/singing voice synthesis technology, the cost of faking an utterance of a specific individual is gradually declining. Researchers need further consideration of the regulation and recognition of speech/singing voice synthesis."
1965,"section 6, after the conclusion"
1966,Broader Impact
1967,"Although we discussed different task formulations and evaluation protocols, the few-shot settings are simulated by downsampling according to existing works, which is slightly different from the real scenario."
1968,"The scope of this work is limited by the available data. The OpenPI dataset (Tandon et al., 2020) is derived from WikiHow 2, and focuses on everyday scenarios and contains English only. We would like to see resources that span more domains (e.g. scientific domains) and more languages."
1969,"In this work, we mainly leverage control guidance such as action triples, dialogue acts, and discourse relations in structured forms that are extracted automatically from the corpus for training. We encourage future work to explore how to incorporate control information in natural language forms (for example, the natural language descriptions of the action information instead of triples). We also compose multiple modules (like the prototype generation, discourse classifier, etc.) to generate the final conversation which might lead to a larger error cascade if there is some early noise. So future work might explore how to make the pipeline learned in an end-to-end manner. What’s more, we mainly focus on using three major conversation structures to help the entire conversation generation, future work might continue to explore other types of linguistic and human knowledge to further improve the conversation generation qualities."
1970,"Section 6
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
1971,"For image captioning, we used the pre-trained OFA model for zero-shot inference. We did not explore every state-of-the-art model or fine-tune OFA specifically on the imSitu dataset. Other image captioning systems could yield better results. The gap between automatic object recognition and using gold nouns confirms that correctly identifying the objects in an image is very important for activity recognition. Also, we are not certain that mapping the Jiang and Riloff (2021) function frames to the imSitu frames is strictly necessary."
1972,"Although our method has been shown effective, it has two limitations that may be improved in the future. First, the FA model has advantages in computation but relies on an effective frequency selection strategy, which is difficult to design. We just simply select some manual frequencies for different datasets by experience. The more effective frequency selection strategy needs further exploration. Second, there is no theoretical guarantee that the orthogonal regularization can generalize to a 3-order tensor. Our OR terms are only formally consistent with matrix orthogonal regularization, which has been empirically shown effective."
1973,"section 7, before the References
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
1974,"We define the adversarial attack task as a sequential decision-making problem and apply policy-based reinforcement learning to model it. This work must follow this assumption: the decision process conforms to Markov decision process (MDP) that the conditional probability distribution of the future state depends only on the current state. Meanwhile, reinforcement learning training requires additional time costs and the results may be unstable.
We only conduct the experiments on two NLP tasks with six selected datasets, which are all English corpus. Furthermore, our experimental results are mainly for BERT, with RoBERTa supplemented in the analysis. Thus, we lack the evaluation of other novel pre-trained language models, such as ELECTRA (Clark et al., 2020) and XLNET (Yang et al., 2019). Therefore, our work lacks multi-task, multi-model and multilingual verification in terms of generalization and transferability."
1975,"In this work, we introduce LaSQuE, which models and learns the differential semantics of linguistic quantifiers present in natural language explanation to train a classifier guided by these explanations. We evaluate the efficacy of LaSQuE over baselines on the CLUES benchmark.
This work assumes that only a single quantifier is present in the explanations. However, in real-world settings, explanations may contain multiple quantifiers. Modeling the composition of quantifiers can be an interesting direction for future work to make the paradigm of learning from explanations more robust toward fuzzy concepts expressed in real-world explanations.
For our experiments, we assume perfect extraction of quantifiers and limit our analysis to a limited set of quantifiers in this work. Furthermore, we assume that the effect of quantifiers in a sentence is the same irrespective of the domain of the sentence. For example, consider two sentences
‘pungent mushrooms are usually toxic’ and ‘people who smoke regularly usually suffer from cancer’. Here the effect of ‘usually’ is not exactly the same for two sentences that are from different domains. However, LaSQuE is not sensitive to the task domain while modeling the semantics of the quantifier. Future work can investigate variations in the
semantics of the same quantifier across different domains and also how to incorporate/learn such domain-specific differences (for example, by modeling the semantics of a quantifier as a probability distribution rather than a point value)."
1976,"Our system has been trained on everyday conversations from Spanish-English bilinguals and may not be applicable to other domains. Additionally, the accuracy of the classifier varies depending on the label type. We use human-created transcripts, so results may not apply for automatic transcripts. There is a risk that incorrect conclusions can be drawn if the system does not meet the performance requirements."
1977,"Our work is base on the existing sequence-tosequence NER model, since its way of decoding has been shown effective for knowledge transfer between different classes (Chen et al., 2022). However, it might also be valuable to consider other token-classification-based or CRF-based (Sutton et al., 2012) NER models. Especially, it would be interesting to employ the existing CRF-based distillation method (Wang et al., 2020b) to cope with the problem of heterogeneous tag sets for NER."
1978,"Our method is first limited by the proposed grammar that doesn’t cover all the realistic cases. As shown in Table 1, there are still a few cases in the randomly sampled 100 examples that none of the defined rules can explain. Secondly, the time complexity of our method is the cube of the sentence length, limiting its direct applications on long documents. So we have to classify the document based on classification of individual sentences, which might be problematic since the sentiment of different sentences in the document may affect each other.
All the experiments in this paper are conducted on public available datasets, which has no data privacy concerns. Meanwhile, this paper doesn’t involve human annotations, so there are no related ethical concerns."
1979,"This work introduced NeQA, a question answering dataset for evaluating the ability of large language models to process negation. While our NeQA attempted to cover diverse types of negation (e.g., different negation phrases and positions) and multiple data sources (e.g., OBQA, LAMA), it is possible that the dataset construction misses some types of negation or domains of text. Our future work will extend the dataset to cover more comprehensive types of negation and domains of text, beyond OBQA and LAMA. Additionally, NeQA is an English dataset, and it would be interesting to extend it to non-English languages and conduct a more comprehensive evaluation of language models, including multilingual ones.
Another potential limitation is sensitivity in language model prompting. Language model performance is known to be influenced by the specific prompt used to query the model (e.g., a rephrased prompt may lead to different model outputs), and prompt engineering—finding the “right” prompt— may be needed to obtain reasonable outputs from the language models (Jiang et al., 2020; Ruis et al., 2022; Wang et al., 2022). As our language model evaluation protocol uses prompting (§3), the evaluation results may inherit such prompt sensitivity. It would be an interesting future work to incorporate techniques to mitigate prompt sensitivity in language model evaluation (e.g., Burns et al. 2022)."
1980,Page 6.
1981,"The current work is an initial attempt at studying the problem of zero-shot classification of semistructured documents. There are two key aspects that this work does not cover and we encourage future work to explore.
First, as pointed out in §2.1, we choose LayoutBERT as our document encoder, Φdoc. This work does not experiment with the variety of encoding strategies in the literature that combines textual, visual, and layout information (Appalaraju et al., 2021; Xu et al., 2021; Huang et al., 2022). It is likely that richer document representations derived from these diverse encoders will further push the limits of zero-shot classification when combined with our proposed unsupervised contrastive pretraining procedure.
Second, results in this paper are on a single dataset, i.e. the RVL-CDIP dataset. While we mitigate this to a large extent by creating four nonoverlapping test splits (see §3.1 and Appendix A), results on more datasets might yield more useful insights. In practice, the lack of datasets for this task (of semi-structured document classification) is what makes this exploration difficult and might require creation of new resources"
1982,"Theoretically, our method might benefit from comparable corpora across languages, where words and compound words might have similar distribution because Zipf’s law might be satisfied only for similar domains. For instance, as presented in Figure 3, word distributions of De and En on Wikipedia are similar after applying BPE. In our experiments, we only confirm the effectiveness of our methods on Wikipedia corpora in different languages, which are comparable across languages. This might limit the scope of our method. However, multilingual models are commonly pre-trained on comparable corpora, e.g., Wikipedia and CC.
Another limitation is about the combined objective in Eq. 4. In our experiments, we try to
eliminate the MLM objective, only considering global regression modeling LGC . The result is not promising, and it seems that LGC can not work well without the help of the MLM objective. However, our experiment is very simple. This might be further confirmed or designed in future work."
1983,"The drawbacks of our method are the same as those of LoRA: it is tricky to batch inputs to many tasks with varying A and B in a single forward pass, and the rank may be greater for tasks that are more challenging. Moreover, we believe that weights obtained during a single task may be used for a better initialisation. Finally, the use of a different sampling policy on a different dataset may also be appropriate, however this choice is not obvious."
1984,"Though we have found a tight connection between probabilistic transformers and transformers in Section 3, this does not mean that our model can be directly used to interpret or modify transformers. For instance, in Section 3.3, we find that WK and W V in transformers both correspond to U in probabilistic transformers. However, if we tie WK and W V in transformers, then we may observe a performance drop on some downstream tasks.
The performance of probabilistic transformers lags behind transformers on large datasets (>100k), which suggests that our model may not be as scalable as transformers. We have discussed this in Section 6.
The way of positional encoding for probabilistic transformers leads to slower training and inference speed. On masked language modeling tasks, our model is about 3 times slower than transformers with either absolute or relative positional encoding, though it has much fewer parameters than transformers."
1985,"mT5, compared with previous Hebrew LMs, is bigger, pretrained on more multiligual data, and learning to segment and tag in an end-to-end manner. While it was beyond the scope of this paper to pretrain new LMs and study which factors contributed to the improved performance, identifying these factors will be useful for determining the most effective approach for future work.
While larger mT5 models perform better than available LMs, they require more powerful hardware accelerators and take longer to train and infer. However, this is a reasonable trade-off from pretraining designated monolingual models from scratch, a more expensive task by itself. Additionally, the inclusion of data from 101 languages in the training of mT5 may have negatively impacted its performance on Hebrew, as some of the data may not have been relevant or beneficial to this particular language. Future work will need to address
this issue by training a monolingual Hebrew LM in order to further improve performance for Hebrew.
An inherent risk in sequence-to-sequence models is that they can generate inconsistent text with respect to the input text (Lee et al., 2018; Rohrbach et al., 2018). While potentially sensitive in different applications, a number of evaluation frameworks have been suggested to reduce the number of such “hallucinations"" (Honovich et al., 2021, 2022). Another limitation of our evaluation framework is that, for lack of available datasets, we did not evaluate mT5 on purely generative tasks such as summarization and paraphrasing."
1986,"While our approach effectively predicts the relationships between entities in a knowledge graph, there are limitations in the scope of knowledge graph resources that can be modeled. The knowledge graph contains a vast array of resources, including attributes, descriptions, and images, which are not easily captured by embedding-based methods, but can be effectively modeled using PLMs. To improve the compatibility of KGC with actual needs, it is necessary to consider a broader range of data types in the knowledge graph and develop complementary methods to effectively incorporate them."
1987,"Despite the effectiveness of our proposed method, it still has two main limitations: (1). Generative data augmentation methods need to use the original
HTC training set to fine-tune the backbone generative PLMs. Then, they need to go through an inference stage to generate data. Both the training and inference stage need more GPU resources, which increase carbon emissions. Although the data generation is usually complete offline and does not improve the time cost of online progress, we leave how to relieve the need for GPU resources as future directions. (2). Although we conduct experiments on three widely used HTC benchmarks, the language of all these benchmarks is English, which has limited morphology. The effectiveness of our proposed method on language with varied morphology needs to be further confirmed."
1988,"In this section, we examine the limitations of our approach. Even though our training methodology runs faster and uses less memory than retraining, there remains potential for further scalability optimization. One potential avenue for improvement could involve optimizing the estimation of the Fisher Information Matrix. Furthermore, op-
timizing the parameters related to the incremental training such as buffer size and regularization coefficient is dependent on the entire time steps rather than the current time steps. Devising a time-efficient way for hyperparameter optimization could be extremely beneficial for this task. Additionally, while our full model has demonstrated some mitigation of the problem of catastrophic forgetting, a significant gap remains between the upper performance bound and the performance of our approach. Further research is necessary to bridge this gap and improve overall performance. Finally, our current focus on continual learning is limited to the emergence of new events and does not currently consider the possibility of new relations or entities. This limitation is in part due to the base model (RENET) not being inductive and is a problem that is inherent to the model itself. Future research in the field of continual learning may aim to address this limitation by considering new relations and entities, even in the context of base models that do not support these features."
1989,"The main limitation of this paper is the one applying to any opinion piece: it is subjective and personal, as the views of the authors are inherently limited by their expertise and experience. More specifically, this paper argues for an increased interaction between the speech and NLP communities, but the author is more strongly embedded in the latter, and thus addresses this audience primarily. Additionally, the short paper format imposes significant constraints on the amount of nuance, detail and discussion of relevant literature, and thus readers may find some of the claims to be less strongly supported and less hedged than would be ideal, or proper in a longer treatment of this topic."
1990,"The main limitation of the proposed study is the relatively small scale of the dataset it is based on. The proposed method is scalable and computationally undemanding (all of the analyzed models can be trained on a single GPU with 12G of memory), and it is feasible to apply it to other countries in the CMP dataset. However, in order to arrive at interpretable results that could be verified in terms of policy substance based on the experts’ knowledge of the political spectrum, we had to focus the evaluation part on the materials of a single election cycle in one country. Potentially, the method can be applied to any country whose manifestos have CMP annotations, however, further investigation with data from other countries needs to be carried out to verify that.
While most policies are recurrent in manifestos, there may be a few topics appearing in upcoming elections, adding some variability in debate across election years. The policy domain labeller might need to be updated every now and then with current topics of interest (e.g. Covid, a sudden expansion of the military). Therefore, the effect of news electoral programs in the classification step requires more investigation namely, the feasibility of further training with new topics of the current debate or the necessity to re-train the whole classifier with new manifestos over again. That being said, the CMP codebook has remained the same for over two decades now. We take this as evidence that the policy domains do not need to change, only the ability of the classifier to correctly identify sentences with unseen topics."
1991,"The current study is has some limitations. First, we used a single audio-transformer model, the pretrained Wav2Vec2.0-base, as a test bed to validate the fine-grained ANs and couple them to their BN signatures. On the one hand, various audiotransformers have been proposed in the literature. On the other hand, the parameters of a pre-trained model are fine-tuned by downstream tasks and previous studies have shown that fine-tuning may lead DNNs to increase their brain similarity (Millet and King, 2021; Tuckute et al., 2022). Thus, it would be interesting to explore whether there are consistent AN-BN coupling patterns across different models, either pre-trained or fine-tuned. In addition, it is necessary to investigate these patterns across different languages (e.g., English VS Mandarin).
Second, existing studies have shown that audiotransformers are able to learn sound-generic, speech-specific and language-specific representations and those hierarchical representations are akin to the cortex (Li et al., 2022; Millet et al., 2022; Vaidya et al., 2022). Thus, it would be interesting to explore whether the fine-grained ANs carry such multi-level representations, and link them to brain responses.
Third, the reproducibilty between the two sessions was high regarding to most of the results (e.g., the global BNs and the phoneme-selective AN-BN pairs), but it was relatively low in some results (e.g., the local ANs in some layers). We speculate that this is the consequence of relatively smaller fMRI training samples but much larger amount of VS-DBN model parameters in the session of Forgot, in which the number of subjects is smaller but the fMRI spatial resolution are higher. Higher
spatial resolution results in much larger number of valid voxels (120,506) compared to that in Pieman (50,065) and consequently more visible units in the VS-DBN model.
Last but not least, the analyses presented in this study are intrinsically limited by the coarseness of spatial (voxels in millimeters) and temporal resolution (volumes in seconds) of fMRI data. Mapping from sound to an interpretable representation involves integrating neural activities on different spatial-scales down to sub-millimeters and on different timescales down to milliseconds. Thus, it would be of great interest in the future to apply the fine-grained ANs to auditory magnetoencephalogram (MEG) dataset to disentangle the symbiosis of model computation and brain responses in both space and time (Bhaya-Grossman and Chang, 2022; Gwilliams et al., 2022)."
1992,"We discover that for datasets with a relatively large number of categories, our method requires a more delicate setting of epoch under different shots. Figure 5 shows the average results on Sun397 and ImageNet of different epochs. It can be observed that for datasets with a large number of categories (such as Sun397 and ImageNet), as the number of shots decreases, the performance deteriorates with an increase in the number of epochs, which is not evident on the datasets with a small number of cat-
egories. We will delve further into this problem to find the reason and solution."
1993,Section 6
1994,"The limitations of our work can be stated from two perspectives. First, the proposed context-free opinion grammar is designed manually. It can be the future work to explore how to automatic generate the grammar. Secondly, we focus on opinion tree parsing in one major language. The performance of other languages remains unknown."
1995,Section 9.
1996,"Prosody-TTS adopts generative diffusion models for high-quality synthesis, and thus it inherently requires multiple iterative refinements for better results. Besides, latent diffusion models require typically require more computational resources, and degradation could be witnessed with decreased training data. One of our future directions is to develop lightweight and fast diffusion models for accelerating sampling."
1997,See section 6
1998,"Our work seeks to gain insight into what pretraining knowledge is transferred and useful for downstream fine-tuning in NMT using synthetic tasks and data. We note that changes in the data generation methods do require re-running the pretraining stage, which is computationally expensive compared to the fine-tuning stage.
Our current synthetic data generation methods are somewhat crude. Although they are designed to encode varying degrees of lexical and structural translation knowledge, they do so in a rather simplistic way. For example, sampling phrases from the normal distribution ignores distributional frequencies which represent information that is likely useful for the synthetic data generation task. In this paper we have presented some interesting initial findings regarding the suitability of synthetic
pre-training for NMT. We plan to explore more sophisticated data generation models in future work.
We acknowledge that synthetic pre-training is unlikely to surpass the quality of real-world massively multilingual pre-trained models in performance, especially if synthetic data is the only data used for pre-training. However, good performance can probably be achieved by combining synthetic pretraining and real-data pre-training. Of course, this risks exposing the model to toxic and sensitive or private content. Therefore, concerns of both model quality and data quality should be considered when evaluating the impact and benefits of synthetic pretraining. We view synthetic pre-training as a complimentary approach to finding an optimal balance rather than as a replacement for previous state-ofthe-art NMT pre-training methods."
1999,"First of all, IDOL relies on a customized dataset that is filtered out from Wikipedia pages with the help of many pre-defined logical indicators. Inevitably, this will introduce a certain amount of artificial bias. If an automatic method for logical indicator extraction based on something like hidden representations from neural network models is put forward, it would be beneficial to narrow the gap between the dataset preparation and logical pre-training.
In addition, in the field of pre-training task design, there have been a lot of different but effective approaches proposed. For example, in Cui et al.
(2022), the authors presented a pre-training task named PERT which requires the models to recover the original token sequences under the background of that different token permutation within a certain range would not affect Chinese text understanding. This method only depends on the original texts, but IDOL introduces one more special token, which widens the gap between pre-training and fine-tuning to some extent."
2000,"There are two main limitations of our work. Firstly, since there are no known baselines for Indian language DC except Kundu et al. (2022), other architectures might perform better than our model. Our claim that Seq-GAN-BERT tries to maximize the information gained from unlabeled sentences is supported by superior performance over baselines defined in this work and other related models. Secondly, due to the lack of good quality labeled datasets, our test sets contained only 100 sentences. However, we believe that the consistency of our high-performing models across languages and multiple seeded experiments presents a positive sign for DC in low-resource settings."
2001,Limitations have been mentioned as section 8 of the paper submitted
2002,"This work puts forth a position: by the nature of a position paper, the work is deliberately intended to be evocative and opinionated, in some places not having unequivocal evidence for certain claims. This presents a clear limitation: the analysis presented may diverge from the realities of NLP at present or in the future, namely if the assumptions/conditions presented themselves prove to be untrue in practice. Nonetheless, we believe centering power and change, and understanding evaluation as a political and sociological phenomenon, is likely to be useful under all conditions.
Further, in understanding the qualities of evaluation relative to other social forces, we directly suggest that evaluation is more readily operationalized in more pluralistic ways than other key forces (primarily resources). While initial efforts indicate the potential for such holistic approaches that reflect many different desiderata (Liang et al., 2022b) as well as participatory approaches that permit contribution from different entities (e.g. Srivastava et al., 2022), it is still unclear how much adoption such approaches will get, and therefore how much power they will acquire. That is, the extent to which evaluation can realize this pluralistic vision still largely remains an unresolved aspiration than a readily realizable certainty. And, conversely, we do note that while current practices potentially put pluralism and resources at odds, they may be mutually compatible in other regimes (e.g. decentralized training through the pooling of shared/volunteered compute (Yuan et al., 2022), open-source software development (Wolf et al., 2020; Gao et al., 2021; von Werra et al., 2022)).
Finally, we do not discuss other forces that we believe have not exhibited strong influence on NLP research thus far, in favor of allocating focus to evaluation and resources, which have had clear influence. To enumerate some of these other (potential) forces, we specifically note (i) research norms, (ii) policy and regulation, and (iii) auditing/advocacy. For (i), we note that while the NLP research community has many established norms (e.g. reproducibility checklists, peer review guidelines, conference organization structure, policies on respectful conduct), most of these do not directly/significantly influence what research topics different researchers work on. We do note that is possible in the future that certain norms (e.g. the access to training data or model checkpoints;
Liang et al., 2022a) would influence what research is conducted (e.g. we may have not seen as much work on the learning dynamics of language models and/or memorization of training data due to the relative inaccessibility of intermediary checkpoints and training data until recently). For (ii), we note that policy and regulatory efforts have had little to no salient impact on the deployment of most language technologies, let alone NLP research, to our knowledge. With that said, much as efforts like GDPR and privacy legislation has impacted scientific research on privacy (e.g. work that operationalizes the right to be forgotten as in Ginart et al., 2019), similar trends could occur in NLP research (e.g. in response to the EU AI Act).3 Akin to (ii), for (iii), we also have seen fairly little impact from auditing/advocacy work on NLP research to our knowledge. But, much as work on auditing/advocacy around face recognition (Buolamwini and Gebru, 2018; Raji and Buolamwini, 2019; Raji et al., 2020, inter alia) influenced research in the computer vision community, we could see similar trends in NLP (e.g. in response to auditing/advocacy intervention around language models)."
2003,Limitations (pg 5)
2004,"First, despite our pursuit of attempting to understand figurative language use across cultures, we have barely scratched the surface in terms of diverse representation. Due to limited scope, budget, and resources, we collect data from 2-3 annotators per language, for seven languages. Further, culture can vary greatly within a language (Hershcovich et al., 2022). Therefore, until we can represent all of the worlds’ people and their languages, there will always be room for improvement. We also acknowledge that the syntax captured in the dataset may not be the most diverse, as many examples follow the template “<X> is like <Y>”. However, we create these simpler examples as a first step, since extension to more complex and naturalistic language can be included in future work. Second, to analyse concept shift, we machine translate test sets into English. However, these translations can be erroneous to varying degrees, which may have resulted in an over-estimation of error attribution to concept shift. This could not be avoided however, due to limited resources of obtaining human translations. Third, English may not be the best language to transfer from in zero-shot evaluation of multilingual models. While we were constrained by training data availability, past works have shown that
machine-translating train sets can help, an avenue we haven’t explored here. Even though we experiment with few-shot evaluation, there may exist an optimal combination of source languages which best transfer to our target languages. Fourth, the English authors recognized culturespecific terms that were not marked as cultural by annotators in the commonsense categorization across all languages. This may be because annotators, being mostly familiar with their own cultures, attributed culturally specific facts and terms as being common sense. Likewise, the Englishspeaking participants may have viewed a separate set of facts as common sense which would not be agreed upon by people from a different culture. It is thus difficult to disentangle common sense and culture in many cases."
2005,"Our approach requires training a discriminator with an attribute classification dataset, which may be expensive in some scenarios. However, it is still applicable by collecting a small set of attribute-sensitive training instances and applying data augmentation techniques.
Our method is hard to achieve fine-grained control. We aim to address attribute-based generation that conditions on a given style, sentiment, toxicity, or topic. However, it cannot condition on a piece of content to control the generation. We encourage future works to explore retrieval-augmented generation with fine-grained control signals.
ACL 2023 Responsible NLP Checklist"
2006,"We conducted extensive experiments with three datasets from different domains to substantiate the results thoroughly. We observe the best performance when we also leverage the body of the articles. So, we did not evaluate the performance on the datasets that do not have the full text (or equivalently, long text) of the articles.
Ethics Statement
The datasets we used in experiments are publicly available. In our work, we provide a comprehensive analysis and present data augmentation strategies specifically to address keyphrase generation in purely resource-constrained domains. We do not expect any direct ethical concern from our work."
2007,"BIGVIDEO is collected from two video platforms Xigua and YouTube. All videos are publicly available. However, some videos may contain user information (e.g., portraits) or other sensitive information. Similar to VATEX and HOW2, we will release our test set annotation and the code to reproduce our dataset. For videos without copyright or sensitive issues, we will make them public but limit for research, and non-commercial use (We will require dataset users to apply for access). For videos with copyright or sensitive risks, we will provide ids, which can be used to download the video. This step will be done under the instruction of professional lawyers.
Though we show that our model with video inputs helps disambiguation, we find that our model could yield incorrect translation due to the lack of world knowledge. For example, model can not distinguish famous table tennis player Fan Zhengdong and give correct translation. We find this is due to video pretrained models are often trained on action dataset (e.g., Kinetics-600 (Long et al., 2020)) and hardly learn such world knowledge. In this work, we do not further study methods that leverage world knowledge."
2008,"In this section, we draw conclusions for the limitations of our proposed model in this paper. Our proposed model mainly focuses on the sentencelevel procedural graph construction. The scenario that two actions in the same sentence cannot be considered in our proposed model. It is challenging to handle multi-grained (i.e., entity-level and sentence-level) dependencies between actions. We will consider this limitation as our future work."
2009,"The way the method applies to larger datasets needs further exploration. As the number of training examples increases, the accuracy gain over vanilla finetuning reduces, indicating that our method best works in low-resource scenarios. Another limitation is that we performed experiments only in one language. It will be interesting to apply our method to tasks in other languages and understand the impact of task-dependent similarity structure on the model’s performance in those scenarios. BFTSS Top-K and BFTSS U-V methods perform similarly. Scenarios where BFTSS Top-K and BFTSS U-V differ in performance, should be further explored. We plan to address them in our future works."
2010,Left blank.
2011,"While EVALM demonstrates that vocabulary augmentation with LRL task performance as objective requires different priorities from vocabulary augmentation for improving representation for its
own sake, our work opens up several avenues for exploration. Our understanding of the potential conflict between fidelity of LRL word representation from wordpieces and LRL task class discrimination requirements remains far from complete, particularly when we extend from sequence-tosingle-label applications to sequence labeling (as in POS and NER tagging) and further to sequenceto-sequence applications (such as translation). Perhaps, further experiments with mBERT and other MLLMs will further our understanding of these trade-offs. While initializing an LRL word embedding using InitHRL or InitMix, we depend on automatic machine translation, which can be errorprone. Ranking by ∆H and picking a prefix fails to discount informative but correlated features. A more sophisticated formulation of loss of information owing to fragmentation, taking multiple LRL words into account simultaneously, may alleviate this problem. In the short term, these two limitations may deserve closer scrutiny."
2012,"The proposed CAST framework carries the same limitation of self-training-based methods, which is the requirement for multiple rounds and multiple splits of training. As a result, the GPU computing hours of CAST are longer than those of vanilla baselines and NS."
2013,"We proposed a solution to the cosine similarity underestimation problem associated with contextualised word embeddings of highly frequent words. Our evaluations used only a single contextualised embedding model (i.e. BERT) with a single dimensionality (i.e. 768). Therefore, we believe that our proposed method must be evaluated with other (more recent) MLMs to test for its generalisability. Moreover, our evaluations were conducted only on the English language, which is known to be morphologically limited. Although in our preliminary experiments we considered discounting schemes based on the part-of-speech of words (instead of considering stop words vs. non-stop words), we did not find any significant improvements despite the extra complexity. However, these outcomes might be different for more morphologically richer languages. In order to evaluate similarity predictions in other languages, we must also have datasets similar to WiC annotated in those languages, which are difficult to construct. Although having stated that using a single MLM and single language as limitations of this work, we would like to point out that these are the same conditions under which Zhou et al. (2022) studied the cosine similarity underestimation problem.
We used only a single dataset (i.e. WiC) in our experiments in this short paper due to space constraints. Other contextual similarity datasets (e.g. Stanford Contextualised Word Similarity (SCWS) (Huang et al., 2012)) could be easily used to further validate the proposed discounting method in an extended version."
2014,"It’s worth noting that this study has certain limitations. One of the limitations is the limited scope of the training data employed. The AltCLIP model is trained on open-source parallel corpora and publicly available unfiltered text-image pairs. A more careful study of the training data, i.e. filtering textimage pairs by relevance and text/image quality may help to further improve the overall performance of the model. Another limitation is the challenge of evaluating the model in a multilingual setting. Despite our best efforts to include as many benchmarks as possible and to translate from English datasets, the evaluation of the model’s performance in other languages is not as comprehensive as it is in English. For example, there may be fewer tasks available such as OCR or action recognition in videos in other languages. In addition, the use of machine translation may introduce biases that could affect performance. Future research should focus on creating a more robust and scientifically rigorous multilingual evaluation framework."
2015,"Although we have demonstrated the superiority of our RHGH model compared to previous work on four real-world datasets, there are still two limitations that should be addressed in the future:
(1) As our RGC layer employs the whole graph to learn the embedding of entities and relations, like most GCN’s frameworks, the computational resources and time required by our framework increase linearly with the size of KG. To make our RHGH model effective on the KG with millions of entities, it is desirable to apply some graph chunking techniques, such as Cluster-GCN (Chiang et al., 2019), to reduce the size of the KG for our RHGH model to improve computational efficiency.
(2) Currently, our RHGH model treats each relation individually. However, relation paths consisting of multiple relations will contain more complex semantic information in KGs. Relation paths enable entities to obtain higher-order neighbor information, but it is also more difficult to align relational paths in different knowledge graphs. In future work, we will explore more efficient ways
to utilize the relation path in entity alignment, such as the relation path matching in different KGs."
2016,"Section 5.4, Section 6 and Section 7
7 A2. Did you discuss any potential risks of your work? We do not use huge models and our experiments are fair."
2017,"Our work has only considered pairwise interactions, but linguistic structure can also manifest through higher-order interactions. We show that our results on small-scale, formal languages, are different from our results on a natural language task.
It would be premature to conclude that small-scale, synthetic tasks can not be predictive of behaviour on more complex tasks, and a more detailed investigation into the properties of the task that play a role is a viable next step. Some of the FIDAMs we considered, most notably SII and STII, are intractable for larger inputs (scaling O(2n)), and a necessary step in employing these methods to larger models is to construct better approximation procedures, e.g. by adapting SHAP to SII as has been done before for tabular data by Lundberg et al. (2018). More generally, although we believe our probabilistic formal language setup provides a important step forward, solving the Attribution Generalization problem – i.e., showing that results for small setups generalize to very large model – remains a key open problem."
2018,"Section 9
7 A2. Did you discuss any potential risks of your work? Our work is of a more theoretical nature, providing a new way of measuring the faithfulness of feature interaction methods."
2019,"Despite our efforts to collect as many generation tasks and datasets as possible, we only evaluate the generation quality and generality of our models on a small number of tasks and datasets. The interpretability and robustness of our models require further analysis. Besides, there exists subjectivity when collecting downstream tasks and intratask datasets, albeit our attempts to employ widelyrecognized categorizations from the literature. Due to the limitation of computing power, we do not study the performance of our method at different model scales. The effectiveness of multi-task pretraining from scratch, similar to ExT5 (Aribandi et al., 2022), also merits an in-depth study.
Broader Impacts
In this paper, we pre-trained a language model MVP using labeled NLG datasets. According to the research (Bender et al., 2021; Bommasani et al., 2021), PLMs tend to “remember” what they have “seen” in the pre-training corpus. This could result in the reproduction of undesirable biases from pretraining data on downstream tasks. Training data intervention could be a solution to alleviate this issue (Lu et al., 2020). It is also interesting to investigate whether supervised pre-training produces fewer biases than unsupervised pre-training.
Environmental impact is another factor we should consider. We attempt a more efficient pretraining strategy and released our PLM for future work. In contrast to large PLMs with tens of billions of parameters, such as T5 (Raffel et al., 2020) and GPT-3 (Brown et al., 2020), we pre-train only a small model with hundreds of millions of parameters. In addition, we utilize supervised pretraining data and initialize our model with pretrained BART, both of which improve the convergence of our model. Ultimately, our model is pretrained for about 20, 000 steps, whereas the BART of the same size is pre-trained for 500, 000 steps."
2020,Section Limitations
2021,"We note a few limitations of our work: a) while we systematically investigate the choice of in-context examples for both in- and out-of-domain settings for higher-resource language pairs (EnglishGerman, English-Russian), it is unclear how this in-context ability of the PLM varies for the lowerresourced language pairs; b) We only experimented with one pre-trained language model, XGLM. Our preliminary experiments suggested XGLM-7.5B to result in better translation quality than Bloom7B (Scao et al., 2022) under the same settings. However, further investigation is required to understand how these results vary across different model scales; c) We analyze different orderings for the few-shot task-level prompts but only examine limited sets of ordering (most similar to the left or right) for the example-specific prompts. As the PLM is shown to be sensitive to the ordering of these in-context examples, it remains an open question to study how to best combine the information from multiple example-specific prompts, with prompt ensembling being a viable option, which we leave to future work."
2022,Section 9
2023,"In the process of conducting experiments, we find our method has some limitations. First, CIF-PT needs to be performed on the dataset with speechtext pair. For some small-scale dataset that only contains speech and SLU labels, our method needs to use external ASR dataset to conduct the pretraining, leading to the increase of complexity of model building. In addition, in CIF-PT, we need to ensure that the tokenizer of the pre-trained language model is consistent with the tokenizer in the ASR task. However, there is usually a gap between the two in terms of vocabulary size. In consideration of performance, it is necessary to modify the tokenzier of one or both sides."
2024,"In section 7
7 A2. Did you discuss any potential risks of your work? We believe there is no risk in our work. We only use the open-resource codes and publicly release data from the community, and no misuse of any resource and no other stateholders involved in our work."
2025,
2026,"One of the limitations of our framework is we need to design the rough range of hyperparameters to search the best setting. In our future work, we will explore the strategy to avoid hyperparameter tuning."
2027,"Although our approach exhibits great speedups in encoder-only settings, it doesn’t yield as impressive speedups in encoder-decoder setting. This is due to the autoregresive decoding steps in the decoder, that has to be conducted sequentially. Accelerating that with DCT requires to incrementally update DCT outputs step by step based on outputs of pre-
vious timesteps, which is theoretically possible but not easy to optimize its efficiency. We plan to further accelerate it in this direction in future work."
2028,"Language Our experiments are conducted on English, as all Code-LLMs we know are pre-trained on English programming languages. Fundamentally, most popular programming languages are English-based, but international programming languages (which work in multiple languages) like Scratch, or non-English-based programming languages like Qalb also emerge. We look forward to the appearance of Code-LLMs on these programming languages.
Prompt Engineering We manually design the prompts without prompt engineering techniques such as prompt search. The searched prompts may outperform the ones we used, but our experiments on interventions show that CODEX is fairly robust towards format perturbations.
Model LLMs update quickly. From the time we submitted the paper until now, several new LLMs have been released. We try to compare their performance with ours. We select three new LLMs: CHATGPT, GPT-4 (OpenAI, 2023), and BARD4, and feed the text prompts to them. Because we do not have access to some of their APIs, we only experiment on a subset of 100 instances and report
4Experiments are done with models updated to May 10, 2023.
the results in Table 5. CODEX outperforms all these models in the automatic evaluation, but part of the reason is that these models provide more detailed outputs than the reference. We provide a case study in Appendix A.5.
Since CODEX is no longer available to the public, we provide CODEX generation results in our GitHub repository. We also looked for alternatives and tried two open source Code-LLMs CODEGEN (Nijkamp et al., 2022) (version CodeGen-16BMono) and STARCODER (Li et al., 2023) with our code prompts. However, as shown in the case study, their performance is not comparable to CODEX, probably because they are more than ten times smaller in size."
2029,"Our approach is based on the assumption of a limited data budget, and the observation that general multi-task training may not be the most efficient method when one cares about single target tasks. As such, DEFT is not applicable to “true” zero-shot settings where one has no information about the target task, since it relies on the existence of at least some unlabelled examples. Furthermore, for some tasks it may be possible to cheaply gather many examples for finetuning beyond the point where DEFT is useful. In some cases, gathering unlabelled examples may not be so much cheaper than gathering labelled examples that it is worth considering whether to gather unlabelled or labelled examples. Additionally, the recent rise of sparse
dataset. Questions were written after reading paper abstracts, and evidence selection required reading entire papers.
mixture-of-expert models (Shazeer et al., 2017; Fedus et al., 2022) may reduce the negative interference effect observed throughout our work, where DEFT models often outperform models trained on all multitask data and random subsets of the multitask data. Finally, we note that in pilot experiments we found that task diversity was a key element of strong held-out task performance. However, DEFT does not explicitly correct for task diversity, and we leave further exploration for extending DEFT to account for this to future work."
2030,"We discuss the limitations of our method from three perspectives.
First, our method is based on pre-trained language models, so compared to rule-based data augmentation methods (synonym replacement, shuffle within segments, etc.), our method requires higher time complexity.
Second, the entity matching process (Section 4.3) will discard sentences which cannot match entities in the entity list, which will affect the utilization of data.
Third, our data augmentation method based on the pre-trained language models, whose generalization ability is limited since the augmented knowledge comes from the pre-trained language models. However, the knowledge in pre-trained language models is limited and not domain-specific. How to improve the generalization ability of the data augmentation methods is a future research work."
2031,Section 7
2032,"MATHWORLD is limited to cover math story problems using the four basic arithmetic operators. Furthermore, within the space of such problems, it does not cover “second-order” MSPs (as discussed in § 3.4). Neither does it cover negation nor inequalities.
We only consider datasets with MSPs written in English in this work. However, MATHWORLD should in principle be able to cover the same type of problems formulated in other languages as well.
An obvious limitation of this work is the low performance on the task of solving MSPs. The focus of this work is to introduce the world model formalism and its use cases, and we leave for future work to build stronger MATHWORLD parsers."
2033,"Given our focus on finding efficient MoE models under computational constraints, AutoMoE search space and evaluation has been restricted in scale to big-sized Transformer models for benchmark MT tasks. A natural extension of this work is to explore the limits of MoE models like SwitchTransformers (Fedus et al., 2022b) and GShard (Lepikhin et al., 2020) that are significantly larger containing billions to trillions of parameters; as well as designing sparse and transferable efficient expert models (Zoph et al., 2022) for diverse types of tasks like reasoning, summarization and understanding.
The limitations of this work are as follows:
1. Sandwich sampling (Yu et al., 2019), inplace knowledge distillation (Yu et al., 2019), and gradient conflict reduction (Gong et al., 2022) are popular techniques to improve the training procedure of supernet. It would be interesting to study the impact of these techniques to improve AutoMoE’s supernet.
2. AutoMoE uses the hidden dimension of intermediate feedforward network (FFN) to modulate the capacity of each expert. It would be interesting to study other techniques to modulate expert capacity such as stacking variable number of hidden layers in FFN.
3. The backbone of AutoMoE’s supernet uses Switch Transformer, which adds FFN based expert layers and routes each token to exactly one expert (top-1 routing). It would be interesting to: (i) search for the number of tokens to route, and (ii) search for the Transformer component (e.g., FFN, self-attention projection layers, LayerNorm) to add expert layers.
4. AutoMoE’s search space contains classical Transformer components such as multi-head attention and FFN layers. It would be interesting to add components that are efficient by design such as convolutional layer, FLASH (Hua et al., 2022), and g-MLP (Liu et al., 2021)."
2034,"In this work, we did not conduct a detailed analysis of how language-specific characteristics contribute to our model’s cross-lingual generalization capabilities. Future work may address this question through extensive matrix experiments — traverse the training on each possible language pair combination and evaluate on all languages."
2035,"In this work, we collect extensive and comprehensive human feedback with high qualities to facilitate our human-in-the-loop conversation summarization framework. While the learned rewards and models are showing good generalization abilities, further attention is still needed to deeply understand what types of feedback or what amount of feedback is necessary. Our current work only considers human feedback collected using the required forms (i.e., rankings and highlighting). We encourage future work to explore how to incorporate human preferences with more open-ended feedback such as through natural languages. Furthermore, we mainly focus on conversation summarization with human feedback in this work, and other types of summarization tasks (e.g., multi-document summarization, email to-do summarization, meeting summarization and etc.) could be further explored to incorporate human knowledge."
2036,"Limits on visual variability and naturalness. The Pentomino domain can only serve as an abstraction for referring expression generations in visual domains. The amount of objects is limited to 9 different shapes and the number of colors is reduced to 6 as well. The positions are chosen to be discrete and absolute while real-world references might include spatial relations. Furthermore, the pieces show no texture or naturalness, but are drawn with a solid color fill. We choose this simplified domain to focus on the interaction between the follower and the teacher and left the evaluation of the proposed models on more realistic looking scenes for further work. Nevertheless, we think
our approach can also be applied to photo-realistic environments (Ramakrishnan et al., 2021; Kolve et al., 2017).
Limits on variability of the referring expressions. We only explored expressions that are generate by the Incremental Algorithm. Moreover, we choose a fix property value order (color is mentioned before shape is mentioned before position) for the realisation of the template’s surface structure and left the exploration for a higher variability to further work.
Limits on variability of the feedback signal. In this work we used a heuristic teacher with a fixed behavior to provide the intermediate feedback to the follower. We choose this Oracle speaker for better control over the experiments and to focus on the research questions of which feedback is most helpful and how it should be presented (contain which information). We are aware that in natural interaction the teacher’s responses might be more dynamic and can be potentially learnt in a much more complex multi-agent RL settings which would go beyond our focused contribution here. Still this is an interesting prospect for future research."
2037,"Our work is one of the first to perform a detailed empirical investigation of transformer guided chaining but is clearly preliminary. The following are some key limitations: - Evaluation of Interpretability: A fair evaluation
of interpretability is not straightforward. In this paper, we reported results from a preliminary study with limited human labor. - Analysis of negations: LogicNLI dataset uses
negations in the facts, rules and statements but it is difficult to disentangle them for a fair investigation. Hence, we were unable to rigorously analyze the ability in handling negations.
- Evaluation on Real-life data: Our reported
work focused on a synthetic dataset. For a more rigorous evaluation, it is imperative to consider more datasets including real-life ones."
2038,"Section 5.1
7 A2. Did you discuss any potential risks of your work? No potential risks anticipated."
2039,"While introducing a framework which deals with multiple languages and multiple figures of speech, this work is still only dealing with three figures of speech and seven languages. Many more phenomena and languages can still bring substantial challenges and insights if considered (once the data availability bottleneck is addressed). Also, we deal with figurative language as labelled at the sentence level, but the word level is also not only interesting but important for broader natural language understanding and could yield different insights than those observed in the present work.
We only mention in passing the influence that different cultural contexts have on figurative usages, and we make some observations on idioms, but this aspect would require a much bigger unpacking. We actually believe that (failure) of cross-lingual computational models can be an excellent diagnostic tool towards a finer-grained analysis of the interplay between culture(s) and figurative language.
We propose a successful method based on prompt learning and present experiments using a specific pre-trained model. Choosing different (and possibly larger) models and investigating even more than what we already do in this paper the influence of specific prompts would also be necessary to further generalise the efficacy of our approach.
Finally, as with most language technology, the limitations of our approach, also in terms of accuracy (especially for some phenomena and some languages), could lead to substantial inaccuracies which could be propagated in further processing. Considering that figures of speech are associated with emotional language, a word of warning is necessary regarding the direct deployment of our
models. We do hope that writing about risks explicitly and also raising awareness of this possibility in the general public are ways to contain the effects of potential harmful consquences. We are open to any discussion and suggestions to minimise such risks."
2040,"In our study, we have demonstrated the effectiveness of our proposed method on FLAN-T5 with different sizes. However, we have not yet evaluated its performance on LLMs, which possess an even greater number of parameters and have been pre-trained on larger corpora, thus potentially providing more accurate feedback for both caption adaptation and reinforcement learning. Meanwhile, it is worth noting that PLMs may contain certain biases, and training based on their feedback may amplify these biases. As future work, we aim to investigate the scalability of our method to LLMs, as well as strategies to mitigate the potential negative effects of biases present in PLMs.
Ackownledgement
This work was partially supported by National Natural Science Foundation of China under Grant No. 62222215, Beijing Natural Science Foundation under Grant No. 4222027, and Beijing Outstanding Young Scientist Program under Grant No. BJJWZYJH012019100020098. Xin Zhao is the corresponding author."
2041,"Our study here focused on the most capable GPT3.5 model, text-davinci-002, at the time the experiments were conducted. We believe that models like ChatGPT and GPT-4, as well as those in the future, are likely to perform at least as well as these, and if they improve further, the metrics we have developed here will be useful in benchmarking that progress. However, significant further paradigm shifts could change the distribution of errors in such a way that certain of our factors (e.g., genericity) become less critical. In addition, the latest iterations of GPT have a much greater input window size, which help them digest much larger swaths of text in one go and potentially make our pipelined approaches less needed in certain settings.
Furthermore, the text-davinci-002 model is fine-tuned with data produced by human demonstrations. The precise data used is not publicly available, so it is difficult to use our results to make claims about what data or fine-tuning regimen leads to what failure modes in these models.
Recent work has noted that language models may be susceptible to learning biases from training data (Sheng et al., 2019; Wallace et al., 2019; Shwartz et al., 2020), and this phenomenon has also been observed for GPT-3.5 (Lucy and Bamman, 2021). We did not stress test the models studied for biases and furthermore only experimented on English-language data.
When properly used, the summarization models described in this paper can be time-saving. However, as noted above, summary outputs may be factually inconsistent with the input documents or not fully representative of the input, and in such a case could contribute to misinformation. This issue is present among all current abstractive models and is an area of active research."
2042,"GUMSum is designed to constrain summaries to one sentence for all 12 genres, which raises the question of whether one-sentence summaries are useful for all possible genres or long-document summarization. This is a complex topic that needs in-depth investigation. For GUMSum, as mentioned in Section 3, document length is limited to 167–1,878 tokens. Moreover, in analyzing human evaluators’ responses to two open-ended questions ([1] and [2] in Appendix C), we noticed that virtually all evaluators mentioned that limiting the summary to one-sentence is very difficult and that some genres were easier than others. For example, one evaluator who was given a vlog and a travel guide commented that,
“The travel guide was much more difficult than the vlog, likely because it was longer and denser. [...] the travel guide packed a lot more information into its pages and within each sentence.”
This indicates that genre differences at the summary-level is not trivial due to the style of the original text.
Additionally, this paper examined a specific subset of pre-trained systems and one version of GPT3’s pretrained language model (i.e. GPT3-text-davinci-002), producing findings which may not generalize to other settings. The dataset used for the evaluation is also substantially smaller than those used in most work on summarization, due to the fact that it was carefully crafted based on both general and genre-specific guidelines to be substitutive and to avoid hallucinations and faithfulness issues, rather than originating in a found dataset, in order to conduct a more targeted evaluation, as recommended by Liu et al. (2022a). While it is inevitable that more data would lead to different results, we do not believe that system rankings or overall findings would be substantially different, so long as the guidelines and genres examined here remain stable.
Finally, we must raise a further limitation involving text type and language: our study encompasses 12 specific written and spoken genres available in the UD English GUM corpus, but does not capture findings for other genres, or indeed other languages, which deserve more attention in future studies."
2043,
2044,
2045,"In Section 4.1, where we show that our approach doesn’t provide benefits on improving models that have the same data and compute complexity as the teacher.
A2. Did you discuss any potential risks of your work? Not applicable. Our approach is more of an up-to-date analysis into some of the existing techniques used, and does not pose any risks beyond the risks of general improvement of AI."
2046,"Our proposed two-stage training recipe is beneficial under the assumption that a pre-trained model is needed for generative as well as sequence labeling tasks. We believe that is typically the case, as one tries to offset the pre-training investment by using the model for as many tasks as possible, but this assumption might not apply in all cases. While we assess the effect of randomness on fine-tuning results by using multiple seeds, we have not done that for the pre-training itself. Even at our mediumsize scale, it is already prohibitively expensive to do so. The evidence for the effectiveness of the twostage approach is also limited by the number of tasks evaluated (2 sequence classification tasks, 2 sequence labeling tasks, 2 generation tasks), but we believe it is a reasonable trade-off between robust results and compute investment."
2047,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable."
2048,"The way we use the intermediate sequences is to concatenate new sequences and the target sequence as the new target. As a result, the length of the target increases linearly with the number of intermediate sequences introduced, which increases the cost of inference. In the meantime, Minimum Bayes Risk decoding needs to do prediction multiple times under different control tasks, which further increases the computational cost. However, there are potential solutions to compromise between the computational cost and quality, e.g. learning a student model by distilling the domainrobust knowledge from Progressive Translation."
2049,"Our work is limited in capturing the unintended dependencies of attributes. It is possible that maximizing certain attributes like positive sentiment may maximize attributes like gender bias. A formal study to capture the dependency of the bias with varied attribute control is an important future direction. The efficacy automated metrics used to measure the linguistic qualities and attribute alignment of the generations is limited (Jozefowicz et al., 2016). Devising more exhaustive and explainable metrics is also an important future-work."
2050,"One limitation is that the importance and visual salience of character instances are not measured directly. We plan to settle these in future work. Another limitation of our work is that we only evaluate our visual coherence loss on a single dataset. Whether the VCL can generalize to other datasets remains unexplored. The reason is that many other datasets are collected in a way to exhibit less visual coherence (lower rates of recurring characters). The VIST dataset (Huang et al., 2016) contains fewer human characters per story than VWP. Also, some of the features we are using for character reidentification may not be suitable to other datasets to the same extent (for instance, if the clothing of characters changes between images)."
2051,Section 8
2052,"The type of field metadata tasks is limited in this paper and it can be explored more. There are far more types of analysis metadata to be discovered and inferred. On the one hand, inspired by data profiling metadata, the dependency between multifields in one table plays an important role. There are several common dependencies or relationships among columns. How to identify them is future work. On the other hand, in Table 8 only a limited taxonomy is provided. A more comprehensive one is future work.
Our initial research explored the ability of large language models (LLMs) to extract metadata from tables. The results were not optimal, likely due to a lack of exposure to metadata during the training process of the LLM and limitations in the design of the prompts used. Further investigation is nec-
essary to improve the performance of LLMs in extracting metadata from tables."
2053,"LAVS is proposed to overcome the off-target problem among languages that share alphabets because those languages tend to have more sharing tokens after the sub-word tokenization process. As for language pair that does not have shared tokens, LAVS might not have a direct influence on the zero-shot translation though it can also increase the overall performance for those languages, which might need further exploration."
2054,"We identify the following limitations:
1. Although we strive to include tasks in all Arabic varieties, available downstream datasets from certain countries such as Mauritania and Djibouti are almost nonexistent and so are not covered in ORCA. In addition, there is a need in the community to create more datasets for several Arabic dialects. This includes, for example, dialects such as Iraqi, Sudanese, and Yemeni. With the introduction of more datasets for such dialects, ORCA’s coverage can be further extended. Regardless, as Figure F.1 (Appendix F) shows, ORCA datasets are quite diverse from a geographical perspective.
2. Although ORCA currently covers both dialectal Arabic (DA) and MSA, it does not pay as much attention to the classical variety of Arabic (CA) due to historical reasons. That is, the community did not invest as much efforts creating and releasing datasets involving CA. However, as more unlabeled datasets become available and with an undergoing positive change in the culture around data sharing, this is likely to change in the near future. Again, this will make it possible to extend ORCA to better cover CA in the future.
3. Although benchmarks in general are useful in encouraging standardize evaluations and
meaningful comparisons, and can help motivate progress within the community, they also run the risk of contributing to a culture of leaderboard chasing that is not necessarily useful. That is, although scientific research advances due to competition, it also thrives through partnerships and collaborations that bring the best from diverse groups. It is in the context of this collaborative culture that we hope ORCA will be perceived and used."
2055,"Section of Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
2056,"NATCS is partially annotated with dialogue acts, intents, and slots, which are annotated independently from the initial collection of the conversations. While decoupling annotations from collection was intended to facilitate natural and diverse dialogues, the methodology is more timeconsuming and expensive than previous approaches that use pre-structured conversation templates to avoid the need for manual annotation. In particular, NATCSSPOKE requires multiple participants engaging in synchronous conversations, followed by independent manual transcriptions and annotations, making the approach particularly time-consuming and difficult to apply for large collections. Furthermore, this decoupling of annotations from collection has greater potential for annotator disagreement.
While the complexity types and annotations are mostly language-agnostic, NATCS is restricted to EN-US customer-initiated customer service conversations between a single agent and customer in a limited number of domains (multi-party conversations beyond two participants or agent-initiated conversations are not included). The annotations included are primarily intended for applications related to task-oriented dialogue systems.
Further, we note that NATCS closes the gap from real conversations along many metrics, but still falls short along some dimensions. We find that real conversations are more verbose, more believable, and less predictable. We also note that comparisons in our paper focused on a limited number of taskoriented dialogue datasets with different collection approaches, and did not exhaustively include all pre-existing dialogue datasets for comparison."
2057,"Due to limitations in time and computational resources, we limited our experiments to using GLM and SuperGLUE benchmark3. While transformerbased language models and the SuperGLUE benchmark are representative, further validation is necessary when applied to a wider range of models and tasks. Additionally, we found that the performance of GLMD−vc (10B→2B) at 85.28% was marginally lower than that of GLM-2B at 85.91%. However, it’s noteworthy that GLM-2B leverages a substantially greater scale in the pre-training stage with a batch size, iterations, and GPU count of 7168, 17k, and 224 respectively, far exceeding the respective parameters of 64, 15k, and 8 employed by GLMD−vc (10B→2B) in its distillation during the pre-training stage. We plan to further investigate these potential limitations in future work.
3Given the requirement for grid search and seed averaging, we have run over a thousand SuperGLUE averages."
2058,"This work focuses on a specific view of the whole neuro-computational modeling field. We exclude specific angles of research such as non-linear models (Ruan et al., 2016; Qian et al., 2016; Bingel et al., 2016; Anderson et al., 2017; Oota et al., 2018) since we want to evaluate the accumulated evidence for structural similarity (isomorphism) between neural responses and language models. (Ivanova et al., 2022) mention several advantages of using linear mapping models, they are more interpretable and more biologically plausible. They also provide an insightful discussion on mapping model choice, emphasizing the importance of estimating models’ complexity over categorizing them as purely linear or nonlinear.
Another limitation is that we do not include speech models (Vaidya et al., 2022; Défossez et al., 2022; Millet et al., 2022) that have been used to map brain representations mostly due to coherency and page-limit restrictions. The survey is also limited to fMRI and MEG data instead of other modalities for two many reasons: (i) fMRI and MEG are used as a combination in many studies (Caucheteux and King, 2022; Schrimpf et al., 2021; Toneva et al., 2022a), and (ii) they offer high spatial resolution and signal reliability (fMRI) and better temporal and spatial resolution (MEG), making them suitable for NLP (Hollenstein et al., 2020). For a survey in encoding and decoding models in cognitive electrophysiology, see Holdgraf et al. (2017)."
2059,"Since this is one of the first studies on understanding the effects of continued finetuning of multilingual models, the focus of this paper was to lay the groundwork by establishing the experimental setting on a set of representative NLP tasks and languages. The resulting set of languages chosen in our setup for evaluation (en, hi, bn, zh, ta, ja, ar, de, es, fr, th), although diverse, are still relatively higher resource. Extending the analysis to languages which were severely underrepresented (or even absent) during the pretraining of the underlying model may provide interesting insights and would be an important future work to pursue."
2060,"In this section, we make a clear discussion of the limitation of our work. Our work mainly study the setting where each dataset serves as an independent domain. However, the adopted datasets (e.g. UNC, UNC+) for query-based image segmentation are mostly collected on MS-COCO (Lin et al., 2014) and have limited domain gap between visual modality. The findings could inspire the researchers to explore other settings, e.g. each class serves as an independent domain."
2061,"There are three predictable limitations in the developed EmbedTextNet. First, while we have performed a thorough evaluation of EmbedTextNet on various downstream tasks, it is still a generalpurpose approach and its effectiveness on specific tasks or in specific domains may vary. Thus, further research is needed to fully understand its capabilities and limitations in different contexts.
Second, as mentioned, EmbedTextNet is most suitable for scenarios where the embedding is saved during inference, such as text retrieval or similarity measurement when the fixed embedding is saved with a vocabulary (e.g. GloVe). However, it may not be as effective in scenarios where the embedding needs to be decoded back to its original form, such as text generation.
Third, the effectiveness of EmbedTextNet is evident on a large embedding dimension, and it may decrease when working with a small embedding dimension even if it was still better than other SOTA in our experiments (e.g. GloVe-50D → 10D in Table 1). This limitation is due to the fact that EmbedTextNet is based on a VAE architecture, which is known to perform better on high-dimensional data. Therefore, it is better to compare the performances of EmbedTextNet with other SOTA and choose the right one according to the researchers’ usage."
2062,"The term culture has many meanings, and before attempting to incorporate commonsense with culture, one needs to establish a well defined definition and boundary along which test cases and examples would be constructed. By focusing exclusively on food and culinary customs, we have greatly restricted our domain of inquiry. However, culinary topics are universal, and span multiple domains of common sense reasoning (physical, interpersonal, societal). Nonetheless, we hope this work will inspire future work to investigate cultural bias along many axes beyond the culinary.
Incomplete representation of all cultures: There are limitations with using countries as a proxy for culture. As noted in § 2, mappings between cultures and countries are many-to-many, not one-to-one. The majority of questions in our test set FORK focus on culinary cultures and customs of only a few countries, and we do not expect the results to generalize to all the countries of the world. We choose to focus only on one topic and a small number of countries so that we may initiate research on this broad, challenging problem with a narrower, more well-defined task. We selected these cultures based on the cultural backgrounds of the authors and authors’ colleagues who were available to provide direct feedback on/validate the data. We hope this work paves the way for follow-up work investigating a broader set of cultures.
Small Annotator Pool: The validation study in § 2.1 is done on a small pool of annotators from a few countries represented in FORK. While the study gave useful feedback about the dataset and question quality, a larger and more diverse set of annotators would reflect a broader range of perspectives within each country, and further reduce the potential for biases or inaccuracies in our data."
2063,Section 7: Limitations
2064,"Explainability Most current MWP solvers are only able to generate solutions. In our work, although we achieved better generalization ability, it is still hard to explain how the model solves MWPs both correctly or incorrectly. These automated solvers would be much more helpful for tutoring students if they could explain their equation solutions by generating reasoning steps."
2065,
2066,"Our model is developed to tackle the structural difference between the ontology and instance views of a KG. However, many modern KGs are multilingual, where different portions of a KG may not only differ in structure but also differ in the text semantics. How to jointly capture these differences remain unsolved. Also since box embeddings naturally provide interpretability to the granularity of the learned concepts, how to use the current model to discover unknown concepts from these embeddings is also challenging."
2067,"There are two main limitations to our works. (1) Grammar constraint: The results of the StructSP method at the 25 SPIS setting in the TOPv2 dataset (Table 3) suggest that the results
of using grammar with low-resource data can be uncertain. The reason is that the extracted grammar from training data for low-resource setting is not general enough to capture the grammar of the new coming data (validation or test set). Therefore, for our StructSP method to work effectively, the provided grammar should cover all grammar rules if possible.
(2) Prediction time: A recursive insertion-based strategy is used for prediction. This means that the output of the previous parsing step is used as input for the current parsing step, and this process continues until a terminal signal is encountered. As a result, parsing a complex tree with multiple intents/slots (labels) can be a lengthy process due to the recursive nature of this method. Future work includes improving parsing prediction time by predicting all labels at the same level in the parsed tree rather than predicting them one by one."
2068,"We would like to claim our limitations from two perspectives: application-wise and technical-wise.
Application-wise: GDA needs annotations to finetune T5, which requires more computing resources and manual labeling costs than the rule-based techniques.
Technical-wise: Our “original sentence restructuring” and “original sentence pattern approximation” tasks rely on the efficiency and accuracy of pre-ordering rules (Wang et al., 2007) and parsing methods (Chen and Manning, 2014). Although current GDA show effectiveness, we still need to find more efficient pre-ordering and parsing methods."
2069,Section 6
2070,"The similarity of TMs is an important factor influencing the translations of TMPLM. However, high-similarity TMs are not always available in practical applications. It is worth studying methods to make use of relatively low-similarity translations in LLM-based translation systems."
2071,Limitation In Page 5
2072,"The coreference annotations of the MovieCoref dataset exclude plural character mentions because the annotation guidelines did not cover them (Baruah et al., 2021). It contains few singleton coreference clusters (65). Our model only identifies singular characters and cannot retrieve singleton clusters. All the movies in the dataset have a linear narrative. Non-linear stories can confuse a coreference model because of time skips and flashbacks which is not explored in our work. Both our inference approaches require at least 10 GB of GPU memory for finding coreference clusters from full-length screenplays."
2073,Section 8
2074,Limitations section at the beginning of page 9
2075,"There are several limitations to our experiments: we work only with English data and with datasets concerning hate speech and toxicity. Frequently such data do not represent i.i.d. samples from the data that we might encounter in real life. In addition, experiments are all conducted in the simulation with these existing datasets. The annotations in the simulated experiments were already checked for quality by the original dataset creators (Sachdeva et al., 2022; Wulczyn et al., 2017). In real-world deployment, further steps would need to be taken to ensure that the entropy in annotations truly comes from disagreements and not other kinds of noise.
While DAAL is designed to capture disagreement due to annotator positionalities, the datasets used may not have had a diverse enough pool of annotators to fully test this. In the portion of the MHS dataset used in our experiments, 67.9% of annotators were cisgender, straight, and white, while only 0.4% of examples targeted this same popula-
tion. The Wikipedia Talk dataset does not provide demographic information about its annotators.
A classifier for toxic text or hate speech trained on a pool of annotators whose backgrounds do not reflect anywhere near the full diversity of human identities (and especially the identities of the targets of the text being classified) is inherently limited. Applying such a classifier, whether it predicts a single label or a distribution, to text from and about marginalized populations not represented in the annotator pool carries inherent risks to the wellbeing of these populations. Such a classifier could systematically fail to flag content that annotators from privileged groups do not find harmful or incorrectly flag innocuous speech written by members of marginalized groups."
2076,Section 7
2077,"Introducing the regularizers inevitably incurs additional computational cost in the training of PETs. To show their impact on the training speed, we plot the time-performance curves for both PDF and SDE regularizers on full-set GLUE in Figures 5, 6, 7 and 8.
On different PETs, the regularized PETs with PDF regularizer has similar running time to the vanilla PETs. On the two large datasets, QQP and MNLI, regularized PETs with SDE regularizer take about 2 to 3 times longer to achieve the best performance than vanilla PETs. However, on medium-sized (QNLI, SST-2) and small datasets (CoLA, MRPC, RTE), the time to achieve the best results with SDE regularizer is comparable to vanilla PETs.
Overall, the PDF regularizer can effectively improve the performance of PETs without introducing much computational cost. In scenarios where there is relatively more focus on the inference performance of PETs and less concern about the slightly longer training time, or when the dataset is small, SDE regularizer should be a good choice.
Our method does not introduce additional risk to the original risks of PETs."
2078,"When building LMentry, an important criterion for a task is the ability to easily create many examples. One of the benefits of this approach is in handling model memorization: if a model achieves very good results on some task(s), one can quickly create additional examples to verify whether the success is indicative of task competence or originating from example memorization. That being said, a few LMentry tasks are inherently limited in this regard. For example, the number of possible examples of word starting with letter is limited by the number of letters in the English alphabet. We opt to include these tasks in LMentry as we judge their benefits (e.g. broader capability coverage) to outweigh this limitation. To mitigate this disadvantage, LMentry task data also includes a “canary string” (Section 2). While a canary string helps in filtering out data from training corpora, it is not a catch-all solution.
Additionally, our experiments include some models whose exact implementation details are yet to be publicly available (e.g. TextDavinci002). We include these results as these models have the best LMentry performance at the time of writing. To shed as much light as possible, we provide the predictions of these models along with all available metadata from the API. This includes the logits of the final layer for the top 5 tokens at each position."
2079,"We now explain the limitations and potential risks of our work. First, it seems the Knowing-how & Knowing-that task is a bit unfriendly to supervised learning methods as we only annotate the testing set. However, towards practical industry-scale applications, we encourage future work to utilize the current annotations and contribute to more efficient heuristic, unsupervised, self-supervised, or weakly supervised methods, etc. Second, each user manual in OHO only contains one user (agent). However, there are a number of user manuals involving more than one agent, e.g., “invite your friend as a new user and get cash back”. This motivates us to explore multi-agent user manuals in our future work. Third, in addition to the textual content, many user manuals contain visual information like images and GIFs. Hence, it will be more desirable to add such user manuals and study the Knowing-how & Knowing-that task in multi-modal settings."
2080,"To some extent, DSpERT pursues performance and interpretability over computational efficiency. The major computational cost of a Transformer encoder is on the multihead attention module and FFN. For a T -length input and a d-dimensional Transformer encoder, the per-layer complexities of the multihead attention and FFN are of order O(T 2d) and O(Td2), respectively. When the maximum span size K ≪ T , our span Transformer brings additional O(K2Td) complexity on the attention module, and O(KTd2) complexity on the FFN. Empirically, training a DSpERT consumes about five times the time for a shallow model of a same scale. However, this issue can be mitigated if we use fewer layers for the span Transformer (Subsection 4.3).
As noted, we empirically choose the maximum span size K such that it covers most entities in the training and development splits. From the perspective of F1 score, this heuristic works well, and DSpERT performs favourably on long-span entities as long as they are covered. However, the entities with extreme lengths beyond K will be theoretically irretrievable."
2081,"In this paper, we does not specifically discuss morphological problems and polysemy problems, and does not develop special strategies for both problems such as Pham et al. (2021) and Emelin et al. (2020). Besides, the simulated lexical constraint dictionary, which is extracted from the parallel sentences of the training set based on automatic word alignment, may be different from the real lexical constraint dictionary provided by users."
2082,"One limitation of our approach is that incorporating acoustic features from an SSL speech encoder, in our case WavLM, introduces extra latency overhead, as we use a standalone ASR model for firstpass. Therefore, our approach may not be appropriate for certain applications that have exceptionally low latency constraints.
Another limitation is that while multi-modal LLMs have the potential to improve ASR performance, they can be more complex and harder to interpret than text-only LLMs. This makes it more challenging to understand the model’s decision making process or debug any potential errors.
6Qualitative examples are presented in Appendix E."
2083,6 Limitations.
2084,"Our method has some limitations that we would like to explore in the future. Firstly, our method
is based on the PLMs which require large GPU resources to train and infer models. We would like to adopt knowledge distillation technology to reduce the number of model parameters while keeping the performance as much as possible. Secondly, the summary generation process still lacks enough controllability even though we incorporate various features of users and products into the saliency estimation and auxiliary inputs of the decoder. In the future, we explore aggregating the characteristics of users and products into the decoder layers to make the generation process more controllable."
2085,"The present work only points out problems of existing research and presents no final solutions. We also simplify the assumption that an automated metric validated for a specific TST task generalizes to other tasks. However, this is problematic since there is, to our knowledge, no investigation of whether a validation on one task generalizes. This concern is motivated by the fundamental differences in how different TST tasks are defined. There are several different definitions, such as datadriven TST (e.g., sentiment transfer) and linguistically motivated TST (e.g., formality transfer) (Jin et al., 2022). Also, we consider only TST papers (no text simplification) and focus on top-tier NLP and AI venues (non-workshop)."
2086,"Limitations
A2. Did you discuss any potential risks of your work? Not applicable. We review existing work in terms of text style transfer evaluation and try to point out existing problems.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 0, 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? We conducted an extensive meta-analysis of text style transfer. We surveyed 89 works summarized in
Sections 2, 3 and the Appendix"
2087,"Many challenges remain in the follow-up of our work. Here are some limitations that we intend to resolve in the future:
• We need a more robust method for compression. Although our method achieves consistent improvements in most experiments, we notice that the benefit is limited in the noisy sets, especially under a high down-sampling ratio. This drives us to develop a more robust down-sampling method for preserving meaningful information even with high compression.
• Our method compresses all the input acoustic features with the same ratio, where the ratio is determined according to the whole dataset. However, the speed of each audio is different, which results in obstacles to unified down-sampling. Ideally, each sample should be compressed with a self-adaptive ratio."
2088,"Section 7.
7 A2. Did you discuss any potential risks of your work? We propose a method for acoustic encoding, which does not have any risks."
2089,"We do not propose a solution for extremely lowresource languages, where neither unlabeled text for building language models, nor native speakers are readily available. Examples of such languages include Muscogee, with about 4500 native speakers, and 325 articles in the Muscogee language Wikipedia, and Arapaho with about 1000 speakers and no Wikipedia articles. In such cases, finding even a single expert annotator might be difficult. The development of resources in such languages, however, do not necessarily rest purely on technological factors.
On the technical side, DIRECTPROBE relies on the fact that a representation can be generated for the instance to be annotated. However, obtaining an
effective representation for structured annotations (e.g., frames, dialogue states, tables, etc) is nontrivial. While this is a problem, this is orthogonal to our contributions."
2090,"In this section, we develop a clear discussion of the limitations of this paper. Our method faces obstacles when attempting to validate it on datasets other than those previously used in this paper. For example, LRS2 (Afouras et al., 2022) is a widely used dataset in visual language recognition tasks. However, since LRS2 dataset does not provide speaker identification labels, we cannot easily classify speakers into domain-specific and domainindependent sets. Despite the enormous amount of work, re-annotating existing datasets with crowdsourcing or annotating a new real-life dataset with speaker labels is a viable solution. Besides, existing data augmentation methods cannot match the generalization requirements on visual temporal-aligned translation perfectly, which inspires researchers to develop targeted augmentation paradigms based on the study in this paper to cooperate with our meta-learning strategies."
2091,"Our work mainly focuses on cross-lingual sentencepair classification tasks. While it is directly applicable to single-sentence classification tasks (Li et al., 2020; Ye et al., 2020) but may require additional efforts to adapt our DPA framework to more complex cross-lingual tasks such as sequence tagging (Liu et al., 2021; Zhou et al., 2022, 2023; Zhang et al., 2021b) or question answering (Xu et al., 2022, 2023). Another limitation is that the proposed multilingual verbalizer in the DPA framework requires an external machine translator to produce the translated verbalizers. Finally, we limit the language set of the multilingual verbalizer to the set of target languages in a multilingual dataset. Extending this language set might give us greater improvement for cross-lingual tasks."
2092,"Section 6
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
2093,"MURMUR relies on large language models for fewshot linguistic skills like surface realization and text fusion. It is probable that smaller models do not work as well, in which case one may curate additional training data to train these modules. We
also note that our choice of logical modules is motivated by the characteristics of the task. Hence, it is conceivable that other data-to-text generation tasks might benefit from incorporating additional modules. MURMUR does not make any assumptions about the type or implementation of the modules and it should be straightforward to extend our method to other data-to-text generation tasks.
We limit our experiments to English datasets. We also adopt a simple prompting strategy for converting a reasoning path to a natural language summary by representing the path as a string. This works well in practice and OPT is typically able to resolve the module names and their arguments correctly. However, more future work is needed to understand when this fails so that better prompting methods can be developed. Despite the known limitations of standard automatic metrics like BLEU and METEOR, we use them to compare our method to previous works. While this is not ideal, we have performed comprehensive human evaluation for both tasks to further verify our claims."
2094,"Due to the nature of deep learning, our method is less explainable than path-finding-based KG completion methods (e.g., CPL), which provide a concrete reasoning path to the target entity. Composing the path with multiple queries might be an applicable strategy that is worthwhile to investigate in order to extend our work on the KG reasoning task.
For the link prediction task, we adapt the “recall and re-ranking” strategy from PKGC (Lv et al., 2022), which brings a trade-off between prediction efficiency and accuracy. We alleviate the issue by applying different hyper-parameters given different sizes of training data, which is discussed in detail in Appendix C.
As a common issue of existing KG completion models, the performance of our model also degrades when the input KG contains noisy data. The advantage of our approach in addressing this issue is that it can use both corpus-based textual information and implicit PLM knowledge to reduce noise."
2095,"While our work is consistent with the key aspects of Questions Under Discussion, we do not attempt to take into account all aspects of this broad framework. Most notably, we do not model relationship between questions (or question stacks), as mentioned in Section 2. While such relationships are potentially useful, with question stacks, the annotation task becomes much more expensive; currently, no existing dataset is available to train parsers in this fashion. We applaud the development of tools such as TreeAnno (De Kuthy et al., 2018) to aid annotation. Additionally, because questions are open-ended, they are inherently subjective, which adds substantial challenge to modeling and evaluating stacks. Constrained by DCQA’s setup, we also do not explicitly model QUD with multi-sentence answers, and leave this for future work.
The subjectivity of QUD analysis also means that there is no single “right” structure. This is in contrast to coherence structures that more rigorously define their structures and relation taxonomies (multiple analyses still exist in those structures, but to a lesser degree). Nonetheless, we
showed in Section 6 that consistency is still present despite documents being reworded and restructured during simplification.
To evaluate our parser, we developed a human evaluation scheme. As mentioned in Section 5, automatic evaluation of QUD structure contains both a generation and a question-answering component. However, human evaluation is costly; future work looking into the development of automatic evaluation measures can be extremely valuable."
2096,Section 8
2097,"The SWS benchmark have two limitations: (1) The sentences in the SWS testing set come from students’ essays, which limits the system’s ability to test its performance in other specific domains such as laws or medicine. (2) the SWS corpus is at the sentence level, but some writing suggestions can only be made after reading the entire article, which are not included in our SWS dataset."
2098,"The paper presents a dependency-aware symbolic reasoning approach for logical data-to-text generation. All technologies built upon the largescale PLM more or less inherit their potential harms (Bender et al., 2021). Besides, we acknowledge some specific limitations within our methods:
1. Data-to-text generation is essentially a one-tomany problem since there is more than one plausible and logically-consistent description given a specific table. Our approach has little control over the diversity and the logical form of the generated template. It is also possible that our approach only generates trivial or naive descriptions if trivial data dominate in the training dataset.
2. Our work mostly focuses on the named entities in the description, but logical consistency is not all about entities. The syntactic structure or other semantic information also has an influence on generation fidelity, and we leave the symbolic reasoning for more complex logical structures or formats as our future work.
3. Our table-compatible programming language is mainly designed for simple flat tables, and extra operators are necessary before it could be applied to all tables, especially hierarchical tables where its header exhibits a multi-level structure (Cheng et al., 2022).
4. Currently, it is difficult to directly integrate GPT-3 (Brown et al., 2020) or other LLMs
into SORTIE to substitute the PLM backbones. The reason is that LLM can not be used for encoding since we have no access to the dense representation in an LLM. It might be plausible to only use LLM to generate a template and use another PLM to do encoding, but we leave this exploration to our future work."
2099,"This paper proposes a denoised structure-to-text augmentation framework for event extraction (DAEE), which generates and selects additional training data iteratively through RL framework. However, we still gain the following limitations.
• The framework uses reinforcement learning to select effective samples, which is a process of iterative training and predicting the generation model, policy model, and event extraction models. The iterative training framework is complicated and time-consuming compared to the standalone event extraction model. • Even the Argument Loss decreases the number of unmatched arguments in a generated sentence, the generation model generates more fluent sentences while at the expense of the ability to ensure that all the event arguments are included completely."
2100,"We report several limitations of our proposed framework in this section.
1. Limitations due to pre-trained models: The first limitation is the reliance of our system on thirdparty hatespeech detectors which are reported to have bias towards minority groups. These models tend to overestimate the prevalence of toxicity in texts having mentions of minority or protected groups due to sampling bias, or just spurious correlations (Paz et al., 2020; Yin and Zubiaga, 2021; Waseem, 2016; Dhamala et al., 2021). Also, these models suffer from low agreement in annotations partially due to annotator identity influencing their perception of hate speech and differences in annotation task setup (Sap et al., 2019). Please note that we aim to overcome this unintended bias problem by using principles of causality but still don’t claim to have completely eliminated the problem.
2. Limitations due to training corpus: We are limited by the distributions of our training corpora in terms of what the model can learn and infer. Further, OWTC dataset used in our perplexity evaluations is a subset extracted from OPENAI-WT which contains a lot reddit and news data, where reliability and factual accuracy is a known issue (Gehman et al., 2020).
3. Limitations due to language: Our experiments are conducted experiments only on English language which could be further extended to other languages.
4. Limitations due to model evaluation: Previous studies have shown that detoxification approaches optimized for automatic toxicity metrics might not perform equally well on human evaluations (Welbl et al., 2021). A future direction of work may be to include human evaluations as part of the data.
5. Limitations due to distribution shift: There are three different datasets that are in use. The first is the dataset used to train the ATE scores. The second dataset is the set of prompts used to finetune the model. The third dataset is the dataset that is used during testing. A distribution shift between datasets may have an adverse affect on our model. For instance, there may be words which occur in the test set that are neither in the ATE training set, nor in the fine-tuning set. In case of such a distribution shift between the datasets, our model may not work as expected."
2101,Section 7
2102,"Although our method can improve the performance as well as accelerate the inference speed, it suffers from two problems: (1) the diversity of generated results is low compared with language models (LMs) due to the clamp sampling strategy, and (2) the diffusion steps of post-tuning stage should stay consistent with the steps in the training stage, and there still exists the gaps between training and inference, i.e., |T | = |K| ̸= |T ′|, To mitigate the aforementioned two issues, we can explore a better post-training or training strategy to mitigate the training-inference gaps further. In addition, we found that the diffusion model does not perform well in open-ended generation tasks, such as generating incoherent sentences. This is closely related to the drawbacks of NAR models, which have a strong conditional independence assumption. We will attempt to address this issue in the future.
Ethics Statement
It is worth noting that all the data used in this paper are publicly available, and we utilize the same evaluation scripts to make sure that all the comparisons are fair. We have replaced the people names in the corpora with special placeholders to mitigate the problematic biases (Radford et al.) issue of generation results. Although we have taken some methods to mitigate the problematic biases, such a problem cannot be solved completely. We urge the users to cautiously apply our methods in the real world and carefully check the generation results."
2103,"We only experiment with two kinds of PLMs (RoBERTaBASE and RoBERTaLARGE (appendix C.3 and appendix C.8)), leaving more diverse kinds of PLMs unexplored. While this allows us to demonstrate the effectiveness of our approach on these specific PLMs, it is important for future work to extend our problem setup to a wider range of PLMs in order to fully understand the generalizability of our findings."
2104,"CSProm-KG successfully integrates both graphbased and textual representations in the KGC task, achieving substantial performance and efficiency improvement. However, similar to other PLMbased methods, this comes at the cost of increased computational resources (v.s. graph-based KGC models). In addition, we find that CSProm-KG may occasionally collapse on small KGC benchmarks (e.g. WN18RR) under specific random seeds. This is probably due to the nature of Soft Prompts, which involve much smaller number of trainable parameters, compared to fine-tuned models. However, we never see similar phenomena when training CSProm-KG in the large KGC benchmarks (e.g., Wikidata5M). We plan to solve these issues for CSProm-KG as future work."
2105,"section 6
7 A2. Did you discuss any potential risks of your work? The potential risk of this line of work has already been discussed in previous research and our base methods (Bert)."
2106,"Our work has several limitations. We focused on fact verification, which formulates the task sentence-pair (i.e., claim-evidence) classification. Our findings may hold for certain domains where the task format is similar (e.g., natural language inference or textual entailment recognition). We did not apply beam search on input reduction, which limits us from searching multiple versions of the reduced claims having the same length. We investigated three widely used regularization methods: temperature scaling, label smoothing, and the confidence penalty. However, other subsequent methods remain unexplored."
2107,"Although form similarity is demonstrably responsible for slower translation processing, we are unable to ascertain if it is the primary reason. The work also reveals one shortcoming of alignment distributions — the measure tends to be biased towards translations with similar forms and does not always make accurate predictions about cognates. To address this limitation, future work can evaluate more elaborate models of translation that incorporate variables (e.g., form overlap, syntactic complexity, and morphological complexity) identified as relevant by previous empirical work in psycholinguistics.
Ethics Statement
We obtained all data from cited sources. Our experimental procedures and analysis do not involve human participants and are in compliance with ACL Code of Ethics.12"
2108,"We identify two sources of limitations in our work: the range of metrics we consider, and the range of models we explore in our experiments.
Our paper advocates for multidimensional leaderboards. In the interest of concision, we focused on cost and latency as well as system quality. These choices reflect a particular set of values when it comes to developing retrieval models. In §4.3, we briefly consider a wider range of metrics and highlight some of the values they encode. Even this list is not exhaustive, however. In general, we hope that our work leads to more discussion of the values that should be captured in the leaderboards in this space, and so we do not intend our choices to limit exploration here.
For our post-hoc leaderboard (Table 1), we surveyed the literature to find representative systems. We cannot claim that we have exhaustively listed all systems, and any omissions should count as limitations of our work. In particular, we note that we did not consider any re-ranking models, which would consume the top-k results from any of the retrievers we test and produce a re-arranged list. Such models would only add weight to our argument of diverse cost-quality tradeoffs, as re-ranking systems must determine which retriever to re-rank, how many passages to re-rank per query (i.e., setting k), and what hardware to use for re-ranking models, which are typically especially accelerator-intensive (i.e., require GPUs or TPUs).
For our experimental comparisons, we chose four models that we take to be representative of broad approaches in this area. However, different
choices from within the space of all possibilities might have led to different conclusions. In addition, our experimental protocols may interact with our model choices in important ways. For example, the literature on SPLADE suggests that it may be able to fit its index on machines with 8 or 16 GB of RAM, but our experiments used 32 GB of RAM.
Our hope is merely that our results help encourage the development of leaderboards that offer numerous, fine-grained comparisons from many members of the scientific community, and that these leaderboards come to reflect different values for scoring and ranking such systems as well."
2109,"We list the limitations of our work as follows. Firstly, the model architecture we use to localize the stuttering speech is simple. Future works could explore a more effective model to perform automatic stutter removal with the help of our SASE dataset. Secondly, we only test the English datasets. And other languages except for English and multilanguage stutter-oriented speech editing remain for future works. Finally, after being pre-trained on our SASE dataset, the stutter embedding in FluentSpeech could also be used to inject stutters into the reading-style speech to change its speaking style, and we leave it for future works."
2110,"DKAF model has only been tested on English data so far. At the moment, we curate new datasets by systematic modification of existing datasets. Our simulation strategy is limited as it does not capture real-world factors (e.g. COVID-19 pandemic) that have a drastic impact on restaurant availability. Finally, It would be interesting to find a real-world dataset and verify whether the proposed methods give similar performance gains on it or not."
2111,"The observed effects in this work, in principle, can only be applied to the setting of our user study (English text, English-speaking crowd-workers, colorcoded word-level saliency, and so on, as described in the paper). Therefore this study serves only as a proof of existence, for a reasonably plausible and common setting in NLP research, that laypeople can be influenced by context outside of the attributed part of the input when comprehending a feature-attribution explanation. Action taken on design and implementation of explanation technology for NLP systems in another setting, or other systems of similar nature, should either investigate the generalization of effects to the setting in practice (towards which we aim to release our full reproduction code), or take conservative action in anticipation that the effects will generalize without compromising the possibility that they will not."
2112,"HELP ME THINK is a model agnostic approach that allows users to inject facts to accomplish tasks with a variety of large language models through simple Q&A but additional experiments are needed to establish its effectiveness on new language models with different training paradigms and capabilities. The entire study is conducted only with tasks of English language. Expanding the scope of HELP ME THINK to other languages will increase the scope for non-expert users. A large scale evaluation setup is further needed to reach HELP ME THINK to non-expert users.
7See Appendix F for examples."
2113,Left blank.
2114,"In this work, we achieve a noticeable improvement in the GEC task by introducing additional context information with a NAR model. However, in order to focus on incorrect tokens, the input of the NAR is required to be constructed based on the AR output distribution. In this way, the AR and NAR model perform sequentially, which leads to much time consumption in the training stage. In the future, we will apply a layer dropout strategy to speed up model training. On the other hand, due to the limitation of computation resources, all experiments are conducted on two Nvidia TITAN V GPUs with 12GB VRAM. Therefore, we could not compare with the state-of-the-art models which are pre-trained with 100M synthetic parallel examples (Li et al., 2022). We left it as our future work."
2115,"Same tower negatives can be applied to other contrastive losses, e.g. triplet loss (Chechik et al., 2010). As we are focusing on improving the most popular method to train dual encoder models, i.e. the in-batch sampled softmax loss, we leave the application of same tower negatives to other types of contrastive loss as future work.
While SamToNe has proven to be effective to improve the training of dual encoders, its efficacy may depend on the diversity of the queries used as inputs. In dataset with a large portion of similar queries in the training set, one might need to use masking or other techniques to remove them from the negative computation. Such techniques can also improve the efficacy of SamToNe when applied to both the query and document towers, where SamToNe is currently known to hinder the performance on datasets with a low rate of unique documents, as discussed in Section 3.4.
We leave the in-depth exploration of aforementioned considerations for future works."
2116,"We list the main limitations of this work as follows: • Limited NAT Models. The conclusions in this
paper are drawn from two representative NAT models, which may be not necessarily well suited for other NAT models. The main reason is that experiments on six WMT benchmarks have cost a large number of GPU resources. We therefore appeal to future works compare more NAT models using the new benchmarks.
• Carbon Emissions. This work totally costed 40,000 GPU hours (around 8,160 kg of CO2), because 1) large numbers of experiments; and 2) scaled neural networks and training data require more GPU resources. However, we hope our empirical results can help other researchers to reduce the expense of redundant model training."
2117,"Although our model has achieved considerable improvements, as shown in Figure 3, our model tends to have a slight decrease in short impression generation, which need to be further solved in the future. In this paper, we follow previous studies and only utilize English radiology report datasets to verify the effectiveness of our proposed model, which is limited in verification in other languages. The main reason is that most publicly available radiology report datasets center on English. In addition, our model needs relatively more parameters than the models only using findings to generate impressions."
2118,Limitations564
2119,"Extension to Varied Task Formats. In this work, we limit our experiments to generating free-text explanations given a complete task sample. In future work, we aim to extend our method over more diverse settings, e.g., controllable explanation generation or synergetic generation of both task prediction and explanation. Besides, more work is needed to assess EIB’s robustness and generalization when applying it to diverse NLP domains. These domains may differ in sample type, topic, or even with different preferred explanation attributes.
More lightweight Learning Paradigm. The performance of EIB is also tied to the quality of other systems or datasets, mainly the backbone language models and automatically constructed training corpus MIXEXPL. The predictions of our method are also restricted by the capacity of the generator of EIB, where we use GPT2-small architecture as the decoding architecture. This phenomenon may be remedied if we design specific interactions with larger PLM (e.g., in-context learning) and other sources for explanation-related knowledge distillation (e.g., logical composition). For example, designing more effective prompts to induce better explanation-related knowledge from PLM to relieve the training pressure.
Diverse Combination with PLMs. While our paper focuses on the issues of explanation generation given zero-shot prompting outputs, we think EIB is easy to extend to few-shot prompting base-
lines since single-pass generation without updating also belongs to the features of conventional fewshot settings. Currently EIB still needs parameter optimization. We think future work can explore more flexible plug-and-play methods to distill sufficient and concise explanations upon large PLM.
Evaluation Quality and Consistent. Quality estimation of the natural language explanation generation is largely dependent on human evaluation due to its open-ended characteristics. Current automatic evaluation metrics are not convincing and reliable when compared to human evaluation. However, reproducing the human evaluation results across different works may be difficult. This suggests that better automatic evaluation metrics are desperately needed for free-text explanation generation. We leave improving evaluation quality to future work."
2120,"Yes, in the required limitation section."
2121,"We found that prefix tuning takes much longer to converge when compared to fine tuning, and for T5-Base, it takes around 10 days on a 48 GB GPU to complete tuning for a single setting in Table 1. Due to limitation of resources and with an aim to save energy, we did not conduct experiments with larger models such as T5-Large, T5-XL etc. We also did not perform experiments with smaller splits of the same datasets, which could have given further insights on how model performance varies when training data size is less."
2122,Limitations
2123,"This paper only considers analyzing contrastive learning in the fine-tuning stage, but we note that with isotropy being a desiderata for pre-trained language models (Ethayarajh, 2019), recent works have considered incorporating contrastive objectives in the pre-training stage (Izacard et al., 2022; Su et al., 2022). We leave analysis on this line of research for future work.
We further note that the analysis in this work focuses on theoretical properties occurred during contrastive SRL (e.g., high intra-sentence similarity), thus only focuses on semantic textual similarity (STS) data as a proof of concept. However, with the growing attention on contrastive learning, we argue that the typical STS-B is perhaps no longer sufficient for revealing the full ability of models trained with newer contrastive SRL frameworks. We call for a standard practice that the performance of contrastive SRL should be assessed on both semantic textual similarity and information retrieval tasks (e.g., Thakur et al. (2021)). We leave analysis on information retrieval tasks leveraging our analysis pipeline for future studies. For example, how high intra-sentence similarity is related to the learned attention towards tokens that enable document retrieval with better performance."
2124,"Limitation section before Reference.
7 A2. Did you discuss any potential risks of your work? This paper is a foundational research for discourse understanding, to our knowledge, there should be no potential risk."
2125,"There are several limitations to our work. First, although we choose the attribute-value dataset due to its high degree of interpretability and control, we acknowledge that its simplicity limits the impact of our findings. Though imitation
3We did not succeed in replicating results in Ren et al. (2020) (see appendix C).
by reinforcement is a data-agnostic mechanism, we have yet to explore how it behaves in more complex settings, such as using naturalistic image inputs or embodied communication. We defer to Chaabouni et al. (2022); Galke et al. (2022) for further discussion on scaling up communication settings.
A second limitation of our results is that we do not explore how imitation-based learning scales to k > 5 Experts. In particular, our hyperparameter regime handles up to around k = 5 Experts– very preliminary analyses on k ≥ 10 Experts suggest a need to also scale up hyperparameters such as agent size and communication channel capacity. When training agents to imitate, one must therefore consider feasibility of the learning problem– for example, as a function of the imitation network topology, communication channel size, agent size, etc– in order for training to converge.
Finally, although our work is inspired by imitation learning in humans, the extent to which simulations explain human linguistic phenomena is not clear. We intend for our work to only serve as a testbed to understand communication from a theoretical perspective."
2126,"The annotation scheme proposed in this work is designed to focus on non-identidy coreference, CuT, and is not able to handle some complex linguistics phenomena. That includes (not limited to) com-
plex temporal ordering, VP or NP ellipsis under conjunction and/or disjunction, event negation. As a result, during data selection process, we had to look for those linguistic features and excluded documents with them from the data set.
Specifically, to limit the scope of the research, we intentionally limited our analysis to data that:
• Is temporally linear
• Has a single terminal state
• Has a high density of object transformations referred to explicitly throughout the text
We chose to work within the cooking recipe domain because it easily satisfies criteria. However, procedural text in general satisfies these three conditions, and our current model is therefore compatible with a broader range of domains than strictly recipes. In future work, we intend to broaden the scope to include more varied domains, such as news data and narratives.
During the manual curation of 100-document subset, we did not encounter any annotation of nominal events, and therefore this work ipso facto involves only events extracted from verbs. Although event recognition is not the primary research focus of this work, being able to additionally identify different types of lexical trigger of events is indeed important when considering broader domains. We plan to integrate our framework with other lexical resources in the future, and event recognition will receive more focus."
2127,"We report the following limitations for the Text2Text-based distractor generator (the major
proposal in this study):
• The Text2Text-based generator still suffers from the concern of generating distractor same as answer or previous generated distractor. In fact, generating repeated incoherent or factual inconsistent results are commonly concerns for neural text generators (Durmus et al., 2020)(Wang et al., 2020). Although the concern is mitigated through the candidate augmentation strategy, there still are certain portions of generating the distractor of those types, as can be seen in Table 5.
• Although the CGR-based methods show their disadvantage in the evaluation, we find that CGR-based method might be a more practical one for facilitating the cloze-style MCQ preparation. The CGR-based method is able to generate ten or more candidates for educators to select, while the Text2Text generators are only capable of generating three or four distractors."
2128,Left blank.
2129,"Our brain MRI interpretations were evaluated by a single neurologist. Such annotations require deep expertise and are not easily carried out with high quality by trainees, which limited the amount of data we were able to collect. To ensure that the annotation would be as reliable as possible, we carefully thought of the dimensions in evaluating the generated interpretations and proposed a thorough annotation instruction guideline. We believe that future work can conduct more extensive studies using our annotation guidelines as a starting point. Further, the radiology reports we experiment with are from a single academic medical center, which makes the generalizability unclear. Future work is needed to evaluate the performance of our models on data from different medical centers. Finally, future work is needed to evaluate relevant and likely outputs from MRI interpretations to address different forms of interpretation bias and to expand the beam of potential likely diagnoses based on the findings.
Beyond the brain MRI interpretation experiments, our generation experiments are limited to a set of pre-trained models optimized for carrying out generation tasks in English. It is possible that multilingual models generating in languages other than English will show different properties. We are limited by the availability of resources for automatic evaluation in these settings, but a more extensive multilingual evaluation with human users could be conducted in the future."
2130,"The main limitation of SITUATIONSUPERVISION is that situation annotations can often be expensive to curate and difficult to design (though we outline some general principles for their design in §7). Furthermore, we conducted experiments on only two datasets in this paper. Future work could explore a wider genre of texts, more domains, and more languages."
2131,"In this section, we discuss the limitations of our work as follows: • As described in the paper, our proposed method
requires annotations of the latent focus; a small number of annotations (around 250 labeled samples per focus) can already bring a significant improvement (see Fig.4). Therefore when applying our approach to other domains it is necessary to prepare at least a few annotations.
• As mentioned in the error analysis section, the model is unable to generate unseen entities, such as specific drug names or laws. Further improve-
ment should be made to solve this problem for practical use."
2132,"Experiments. In this work, we propose new methods for finetuning language models. We acknowledge that similar to previous approaches, our experiments are limited to English datasets and specific supervised tasks. However, our method does not use language- or task-specific tricks and should apply to other languages and tasks.
Method. As demonstrated in Section 3, SLaSh is computationally efficient and performs comparably
to the full finetuning for small datasets. Moreover, its parameter and memory efficiency makes it an excellent private learner. However, it may underperform by a few points compared to full-finetuning larger datasets with higher intrinsic dimensionality due to using very few parameters. For example, SLaSh struggles with generative tasks such as text summarization, as generative tasks are more complex and involve making predictions over the whole vocabulary. In contrast, classification tasks have relatively fewer output labels. In our initial experiments, SLaSh reached a ROUGE-2 score of 12.93 on the XSum summarization task (Narayan et al., 2018) with pretrained BART, whereas full finetuning achieves a score of 21.94 (He et al., 2022).
The limitations of SLaSh are due to the small number of parameters it updates. Since shift is applied to only certain biases, the number of parameters can not be increased beyond a limit. However, we show that SLaSh is a more efficient and performant alternative to the methods that use a similar number of per-task parameters. Moreover, we showed that joint reparametrization improves parameter efficiency of other methods. As such, this principle can be extended to methods that are not restricted by a maximum limit on the number of parameters. For example, JR-WARP’s parameters can be naturally increased by increasing the prompt length, which should improve the results further (details in Appendix A)."
2133,Section 6
2134,"Np Decoding uses k-means clustering to reduce the number of contextualized embeddings, the performance varies by how the contextualized embeddings are clustered. As the process is relatively inconsistent, reducing the number with other methods would make the model performance more consistent. Also, as it is not trivial to add new contextualized token embeddings on top of preconstructed CE due to the clustering step, we did not perform on dynamic corpus setup where new items are added or updated.
Np Decoding is applicable to all generative retrieval models including GMR or SEAL which needs all token embeddings, however, we focused on generative retrieval models with representative output as the retrieval target in this work. Also, while it is a general approach applicable to all encoder-decoder models, we focused on applying the method to T5."
2135,"We propose TokenCluster, a sentence extraction algorithm that can automatically identify aspects in user reviews and leverage that for performing extractive summarization. Being a data-driven approach, it is susceptible to noisy data. Therefore, a limitation of TokenCluster is that the clusters of aspect-related words can be noisy and imbalanced if data is noisy. Future works can focus on more robust ways of extracting and partitioning aspect-related words. This can possibly be achieved using external data. Another limitation is that TokenCluster is computationally more expensive compared to more simple sentence extraction approaches. This is because TokenCluster clusters aspect-related words during inference, which can be restrictive when the review set is large."
2136,"Admittedly, the main limitation of this work is the selection of k nearest neighbors. Intuitively, highquality nearest neighbors can make GNN learn the representation more easily. Thus, in future work, we will focus on the process of kNN selection including that attempt more measures rather than space cosine similarity distance and more representations extracted with different strategies."
2137,"The work presented here has a few limitations: (1) The proposed model belongs to the memory-based methods for continual learning, which requires a memory that costs extra storage. In some extremely storage-sensitive cases, there may be restrictions on the usage of our model. (2) The proposed model has currently been evaluated under the RE setting. It is better to transfer it to other continual few-shot learning settings (e.g., event detection and even image classification) for a comprehensive study."
2138,"Model sizes and comparability As we have pointed out in the paper, due to computational and time constraints on the hardware we had at our disposal, we found it was unfeasible to train larger architectures. Nevertheless, we believe our comparisons between DMLM and its direct competitor, MLM, have been fair, as we have done our best
to set a level playing field between the two. Thus, while we understand that this is a significant limitation in terms of comparability to larger models, we still think the results we have obtained could pave the way for further exploration in this direction. Moreover, we have performed architecture scaling experiments to show that it is important to continue research in this direction, and test DMLM’s capabilities on larger networks, while we did not perform a similar comparison with MLM because several works have already explored how MLM scales with network size (Turc et al., 2019).
Applying DMLM only half of the time Although we acknowledge that our choice to apply DMLM to only half of the sentences can be seen as arbitrary, we argue that it is a sound choice given the nature of our objective. Indeed, we did not want our models to rely too much on the definitions provided, or they would have required them at inference time. Such a requirement is mostly unfeasible, as it would demand running a WSD pipeline before the model’s inference, and this is incompatible or unnecessary with most downstream settings. Nevertheless, we plan on training other architectures with different frequencies, so as to better assess how impactful this hyperparameter is.
Training corpus domain Our models are trained on a sense-tagged version of WikiText-103, which only contains text coming from Wikipedia, and thus is very descriptive in style. While many other works have based their pre-training corpora on Wikipedia, we do recognize that this might be a limitation, especially for downstream tasks.
Training on longer sequences In this work, we trained language models on sentences, as opposed to what is commonly done in the literature, i.e., longer sequences of text which are usually concatenated sentences. We see a limitation here in that, in its current formulation, DMLM does not support training on longer sequences as we have no way of discerning between multiple definitions appended to our input sequence. Nonetheless, while we performed WSD at the sentence level, the corpus can be brought back to full documents, which would make sequence-level training feasible with the available data, provided that an extension to DMLM that supports multiple definitions is designed. We leave such an extension to future work.
Scaling to multiple languages Our formulation of Descriptive Masked Language Modeling can be applied to, as far as we know, virtually any language. Moreover, we argue that it might be possible, in a multilingual setting, that definitions of the same sense could help in aligning the output representations of the trained models for words sharing the same sense. Nevertheless, having said this, it is worth noting that there might be two impediments to achieving multilinguality. First, in our work, we leveraged English Word Sense Disambiguation, which, despite its recent advancements, is still far from performing the task equally well on other, even high-resource, languages (cf. Pasini et al. (2021)). Second, we decided to employ definitions coming from sense inventories which, at least in English, cover a wide number of senses with meaningful descriptions, but this might not be the case for other languages, especially low-resource or endangered ones, with BabelNet (Navigli et al., 2021) being the largest resource providing textual definitions in hundreds of languages.
Reproducibility We acknowledge that, even by releasing the code and dataset on which our models are trained, it might be hard for other interested entities (e.g., groups, people, institutions) to reproduce this work, as our training runs lasted up to 8.5 days on our multi-GPU setup."
2139,"There may be some potential limitations to this work:
• Due to the maximum input length limitations and cumbersome deployments in most PLMs (i.e., BERT), we limited our input lengths with a specific selector (following previous works (Sun et al., 2019; Zhang et al., 2021b)) and searched hyperparameters in a limited range, especially in batch sizes (with a maximum batch size of 6). Theoretically, better experimental results can be reported; however, we reimplemented comparative methods and conducted all analytical experiments in the same environments with the same settings, ensuring fairness in performance comparison and problem addressing.
• Due to the characteristics of the applications in our work and the existing DG methods that are difficult to directly apply to our tasks, we only simulate the performance of plain-text models as DG benchmarks. However, textual information is inherent in UP-invariant signals for DG performance to some extent, and a comparative experiment indeed leverages the proposed method for better performance in the same OOD scenarios; therefore, it is reasonable for evaluating the performances. To further address these limitations, we will explore more DG strategies to adapt feasible DG methods applied to our personalized sentiment analysis or more complex scenarios with the external introduction of inherent domain shifts in texts such as topics (e.g., books, DVDs, electronics, and kitchen appliances).
• Last but not least, in this paper, the proposed DG method is only evaluated on personalized sentiment analysis tasks. However, more applications can be applied to our method, where domain shifts occur due to explicit knowledge injection or Fi can be augmented and exposed."
2140,"Since each Chinese character contains 1 syllable, our proposed model can control the number of syllables in the generation by the number of generated tokens. However, this method does not apply to languages with multisyllabic words (such as English). To rewrite lyrics with multisyllabic words while maintaining the same number of syllables, a special technique such as syllable-level subword tokenization may be needed. This line of work will be left to be investigated in the future."
2141,"While we have demonstrated the promising potential of utilizing the lower bound as a substitute for p(R|C,K) in knowledge-grounded generation tasks, there are several limitations that need to be acknowledged. First, the use of the language model (LM) as learning signals can introduce flaws. The model may exploit the LM’s weaknesses by generating comments with a high likelihood based on the LM but are nonsensical in reality, resembling adversarial samples. In our experiments, we observed that generating adversarial text samples, unlike vision models, proved challenging, and we did not encounter completely nonsensical comments. However, we did observe the model exploiting the flaws in the LM, indicated by certain common patterns in the generated comments. Second, there are better alternatives to a hard knowledge injection reward, such as an n-gram matching-based BLEU score used in this study. In some cases, a knowledge-grounded comment may not have any word overlaps with the knowledge instances, resulting in a n-gram-based score of 0. Ideally, an embedding-based soft knowledge reward would be more desirable for this reason. However, in our experiments, we found that the soft knowledge reward based on methods like (Kusner et al., 2015; Sellam et al., 2020) was easily exploitable, as the model learned to echo keywords from the context to achieve a high soft knowledge reward. Third, our approach primarily focuses on scenarios where well-constructed triplets are not readily available, such as when retrieving information from the Internet. However, in cases where pseudo knowledge construction is highly accurate, such as applications with more limited scopes, our approach may not outperform triplet-based approaches. Fourth, it is important to note that our method could potentially be used to generate offensive or prejudiced texts. Addressing biases in generative models is a longstanding issue, and it is not the main focus of this work. However, the ethical implications can be partially mitigated by integrating our approach
with other debiasing technologies."
2142,"Although our experiments prove the superiority of our SCPRG model, it is only applicable to document-level EAE tasks with known event triggers because both STCP and RLIG calculate the attention product of the trigger and candidate spans. However, in real-life scenarios, event triggers are not always available. In view of this problem, we have a preliminary solution and plan to improve our model in the next work. The core idea of our method is to select and integrate context and role information based on candidate arguments and target events. Based on this idea, we briefly provide two solutions for the above limitation. First, we can make the model predict the best candidate trigger words. Second, we can replace trigger words with special event tokens. In the next work, we plan to extend our model to document-level EAE tasks without trigger words and evaluate it through extensive experiments."
2143,"We acknowledge that our system has some limitations that warrants further investigation. For example, one needs to be mindful of the specific downstream applications of the proposed methods, both in terms of 1) potentially large variance in outof-distribution performance (e.g. divergent question generation applications that aim to spark children’s curiosity-driven thinking (Abdelghani et al., 2022)); and 2) of mitigating harmful/toxic contents in educational applications (Bender et al., 2021). As a result, we believe such techniques and applications are neither suitable nor safe to directly interact with children, we urge developers to use this technique in other ways, for instance, in teaching assistant application (e.g., a system that suggests examples for teachers), where the teacher can filter and modify the examples and thus making sure the content children receive is proper and safe.
We also acknowledge the prohibitively restrictive access to the GPT-3 model at the time of writing. We do believe that this constraint will relax over time, and meanwhile, hoping that our proposal can shed light on research and applications with more accessible LLMs such as GPT-J (Wang and Komatsuzaki, 2021) and BLOOM (BigScience, 2022) for future work.
While we acknowledge the many limitations
with respect to accessing GPT-3, we are not advocating against using it. On the contrary, in fact, we believe GPT-3 is still among the most cost-effective solutions especially in the context of natural language generation. The main goal of the study is thus to explore more data efficient ways of using GPT-3 to generate and evaluate questions. We strive to share our experience and insights with the community, which hopefully can be proven valuable and helpful."
2144,Section 5 on page 5.
2145,"We note that there are several limitations with such a sentiment knowledge enhanced self-supervised
learning approach. First, the preprocessing of massive videos is time-consuming and laborious. Second, the pre-training of our model has relatively large requirements on the GPU resources. Finally, we argue that there should not be too many videos without sentimental words, so as to avoid the model having a large bias and not learning any sentiment knowledge."
2146,"section 7
7 A2. Did you discuss any potential risks of your work? Our work does not have potential risks."
2147,
2148,"In this work, we studied the effects of large pretrained models in the temporal video grounding task and investigated the applicability of NLP adapters for a parameter-efficient integration. While we believe our results show the efficacy of incorporating better language models in TVG models, it is important to note that we primarily focused on proposal-free TVG models and thus have no evidence to suggest such improvement would be observed in proposal-based models.
Furthermore, as our main goal was to investigate how the chosen models’ performance varied when only changing the text encoding models, we compared state-of-the-art models using different visual features. While it would be interesting and insightful to check their performance when using the same features as our chosen models (i.e., I3D), such experiments are out of the scope of this study.
Moreover, although language adapters can be stacked before a task adapter for training on the task in a new language, we have only experimented with queries in English. It would be interesting to investigate if language adapters could be applied to TVG in different languages.
Finally, as for hardware requirements, our experiments were performed on a single 40-GB NVIDIA A100 GPU from a large cluster, and we spent about 400 USD on our experimental setup. While experiments with ExCL and TMLGA can be run on smaller GPUs with no significant increase in training time (i.e., we tested with a 16-GB NVIDIA V100 GPU), for DORi, due to the size of the input features and number of training parameters, we recommend using a GPU with at least 32GB of memory."
2149,"7 (Limitations)
A2. Did you discuss any potential risks of your work? Not applicable. Left blank.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts?"
2150,"We recognize that this work was only performed on two tasks related to story understanding, thus it is difficult to say exactly how robust it really is. However, given the capabilities of LLMs and Code-LLMs, we believe our prompting techniques or similar will prove to be useful to the story understanding community.
Our work also assumes that the CoRRPUS will be asked the same question across stories. In other words, given an example as a prompt, CoRRPUS will follow that example to generate code for the next story. We are not providing the task to CoRRPUS and having it interpret the question to figure out what it should be tracking. We simply tell it to track certain information (e.g., objects, physical features of characters) so that it can solve these tasks. Therefore, for CoRRPUS to work, the user would need to know what information is salient for
their task and prompt it to the system. Even though the Re3 dataset contains more complicated sentences than bAbI, these are still relatively simple English sentences. We do not know how CoRRPUS would perform on more complex stories or on stories in other languages.
Lastly, there is the issue of access. Due to cost, we were unable to rerun all of the GPT-3 experiments. The pricing of GPT-3 not only hinders new research, but it hinders reproducability efforts such as ours. Furthermore, as of the publication of this paper Codex has been removed from the OpenAI API, and it is as-of-yet unknown if GPT-3.5 or GPT-4 can handle code-based prompting as well. There are, however, still other code-based LLMs available, such as Github’s Copilot and Hugging Face’s Starcoder."
2151,Section 6
2152,section 2.3 and the limitations section.
2153,"Newsela Dataset One limitation to this study is our use of the Newsela dataset. Because this requires a license to access, researchers cannot fully reproduce our work without first obtaining permission from Newsela Inc. Unfortunately there is currently no other large dataset offering high quality
aligned documents for simplification under an open source license. The only other datasets so far used for document-level simplification are based on WikiLarge, which has very poor and inconsistent alignments at the document-level (Xu et al., 2015; Sun et al., 2021; Cripwell et al., 2023).
Paragraph-Level Human Evaluation In order to reduce complexity, our human evaluation was performed on paragraphs rather than full documents. As a result, there is a potential limit to the accuracy of human judgements when certain discourse phenomena are present. For example, important information may be excluded from a specific output paragraph (therefore prompting a low adequacy rating), but this could actually be present in a different part of the true simplified document.
Monolinguality This study focused entirely on simplification for English-language documents. Reproducing the proposed systems for use on other languages would require dedicated datasets of similar scale, along with sentence/paragraph alignments and operation labels (which likely do not currently exist). Further, the nature of simplification in other languages may differ quite a lot from English with respect to the types of operations that are performed, potentially reducing the suitability of the proposed framework.
Generalised Target Audience We approach this study with our definition of ""simplification"" being based on that of a generalised audience, following the standard set out by the assigned reading-levels of the Newsela dataset. Existing works often outline the intent for their systems to be used to simultaneously assist a wide array of different target users, such as those with cognitive impairments, non-native speakers, and children (Maddela et al., 2021; Garbacea et al., 2021; Sun et al., 2021). However, they rarely go into any detail about which simplification strategies work for each of these different groups or perform human evaluation with annotators from the same target demographics (Gooding, 2022). As such, we acknowledge that using our systems for a specific demographic might prove insufficient to enable their consumption of media without first making further revisions to support their precise needs."
2154,"Limited by the scale of annotated contrastive explanation corpus, our CPACE model is only fine-tuned on approximate datasets selected with some designed principles. The performance of our method can be further improved with sufficient high-quality contrastive explanation annotated datasets over more NLP tasks. Moreover, in this paper, we mainly explore the effectiveness of the CPACE model for multiple-choice commonsense questionanswering tasks, which is our goal, while previous retrieved-augmented methods cannot provide highly relevant knowledge or context for reasoning. Due to the fact that the contrastive explanation is designed to provide distinguishing information among given options [a1, a2, . . . , an] or labels, there are no given candidates or labels in generative commonsense question-answering tasks, therefore, our CPACE model cannot directly fit to other generative QA benchmark datasets. However, in our work, we provide some insights for future exploration, that is, generating question-specific distinguishing knowledge with a contrastive explanation generator can improve the performance and interpretation of current reasoning models. Meanwhile, although we validate the generalization of CPACE on other QA tasks, including QASC and OBQA, the effectiveness of our model in other NLP tasks requiring contrastive knowledge enhancement, such as open domain dialogue, needs to be further explored. In the future, following the CPACE model, we will explore a unified contrastive explanation generation framework for the generative commonsense question answering tasks via generating the chain-of-thoughts with a large generative language model-based generator, such as InstructGPT (Ouyang et al., 2022), BLOOM (Scao et al., 2022) etc., or generating topN possible candidates and ranking them with distinguishing knowledge, which is beyond the scope of this paper to explore and is also our future work."
2155,"Despite we largely improve the performance of the existing conversational search method, the mechanism of the self-supervised tasks in our SSP is simple and intuitive. Additionally, our post-training method relies on the external query reformulation dataset, which is a compromise under the scarcity of conversational search data. However, the essential contribution of this work is that we point out the significance of modeling dialogue structure (especially for topic shift), and the phenomenon of contextual semantic vanishing in conversational search for the first time. We hope future works could pay more attention to these problems and devise more complex methods to develop more
powerful conversational search systems."
2156,"Section 8
7 A2. Did you discuss any potential risks of your work? This work focus on conversational search, which has no obvious potential risks."
2157,"Under-explored multilingual generalizability. We evaluate the efficiency of our metric mainly for English. We aim to extend this work to other languages using language-agnostic PPDB as described in Section 3.2.
Restricted to lexical simplicity. We mainly examine the lexicon-level effect of the quality of the text simplification (i.e., the relative simplicity and meaning preservation of the output tokens). Other sentence-level factors that could have an effect in simplicity is not explored in this paper, such as the compositional difficulty (e.g., whether the sentence uses a inverted order) and comprehension difficulty (e.g., a sentence written with simpler words may still be hard to understand). Empirically, we observe that all the involved metrics correlate poorly with StS. We aim to conduct further research exploring the automatic metrics on sentence-level simplicity without external knowledge bases or human references."
2158,"Section 8
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
2159,"Our experiments are limited to 3 DataMUXed pretrained models (N = 2, 5, and 10) due to compute constraints. More pre-trained models with different N ’s would provide PruMUX with more options to improve throughput and would allow us to conduct a more detailed evaluation of Auto-PruMUX.
PruMUX uses CoFi as its model compression method. Experiments with other methods could improve our understanding of the interactions between model compression and data multiplexing."
2160,Section 7.
2161,"There are three points to discuss and they may inspire further investigation. First, since the length of empathetic conversations in the current benchmark dataset EMPATHETICDIALOGUES (Rashkin et al., 2019) is relatively short, the theory of selfother awareness could be explored under the circumstance of long conversations to maintain the self-awareness of chatbots for the long run. Second, for the better comprehension of self-other awareness, it is helpful to introduce more commonsense knowledge of higher quality. Finally, current automatic evaluation metrics are still not rational and proper to measure the ability of empathy. It is desirable to build better evaluation metrics for empathetic responses."
2162,Section 7
2163,"We use scientific papers as a first testbed for evaluating model robustness to layout distribution shifts. Many different layouts exist among scientific papers, and the existence of metadata databases facilitated the construction of train-test splits with layout distribution shifts. However, scientific papers are only one domain in which layout distribution shifts occur. Layouts also vary for many other visuallyrich documents, such as business forms, receipts, webpages, and newspapers. We hope our evaluation methodology engenders evaluations on a wider range of document types.
Our experiments involve a subset of the many layout-infused models proposed in recent work (e.g., Peng et al., 2022; Kim et al., 2021; Li et al., 2021). The models in our experiments were chosen because they share a similar model size and underlying architecture, facilitating comparisons between different methods of layout-infusion. We release our evaluation suite to enable more comprehensive evaluations in the future.
Performance drops occur both for layout-infused and, to a lesser extent, text-only models. The performance drops from text-only models may be due to layout information conveyed via word order and visual section boundary markers, but may also reflect shifts in text distribution. Our error analy-
ses suggest that generalization errors are driven by shifts in layout rather than content (Section 5.5). In the future, synthetic experiments (e.g., with LaTeXbased manipulations) would help to fully disentangle the effects of layout and content distribution shifts, provided that large-scale synthetic manipulations can be constrained to produce realistic layouts."
2164,"Model Capabilities We observed that current LMs struggle to generate several categories of examples. LMs struggled to generate examples related to concepts they do not understand well (e.g. cryptography and steganography). As discussed in §6, we also found that LMs struggled to generate examples with many constraints, in particular, those in the BBQ dataset (Parrish et al.,
2022). We expect these limitations to wane as LMs grow more capable with scale. Lastly, many evaluations related to LM capabilities require the dataset creator to know how to solve the evaluation. We expect that LMs will not be able to generate high-quality evaluations of this kind (e.g., to test for factual knowledge they do not yet know). Our approach is thus differentially useful for evaluating other properties of models aside from capabilities (e.g., safety-related behaviors).
Model Biases LMs learn biases from their training data (Sheng et al., 2019; Gehman et al., 2020; Brown et al., 2020), impacting the generator pg and discriminator pd. For example, generated evaluations may exhibit gender or racial biases and be lower quality for languages under-represented in the LM training data. LMs will also be systematically worse at generating evaluations for tasks that are omitted from their training data (e.g., due to copyright, licensing, or privacy issues).
Example Diversity We found limited example diversity for some kinds of evaluations (§B.5) though not all (§A). Diversity appears to depends on the kind of evaluation generated, the generation hyperparameters, and the prompt used, and thus sometimes requires e.g. hyperparameter tuning to get right. We found data visualizations to be powerful tools for understanding and debugging data diversity, such as those at evals.anthropic.com/model-written. Qualitatively, we also found that using pd to rank/filter examples limited the diversity, since pd sometimes selected for prototypical examples for testing some behavior (observed qualitatively by workers in §3.2). We are excited for future work to explore other methods that achieve similar example quality with higher diversity than our method, such as generating many examples and subsampling for diversity.
Instructions May Be Misunderstood LMs, similar to crowdworkers, may generate evaluations that are testing something different than intended, especially if the generation instructions are underspecified. For example, using the method in §3.1, we generated statements that a person who “shares beliefs with Derek Parfit” (the influential analytic philosopher) would agree or disagree with. The “disagree” statements were often ones that many people, not just Derek Parfit, would disagree with (“I support slavery” or “I believe evolution never happend”). In this case, we should have
provided more specific instructions to the LM, to have it generate examples that Derek Parfit would disagree with but that another philosopher would agree with. When feasible, we recommend briefly examining the generated data, to catch salient issues such as the above.
Sensitivity to Instructions Our approach allows the dataset developer fairly fine-grained control over the evaluation by using instructions to guide pg. However, the quality of LM outputs is sensitive to text inputs in unintuitive ways (Perez et al., 2021; Lu et al., 2022), adding hard-to-predict variance to the quality of the resulting evaluation; see Appendix §B.4 for a possible example of this effect we found. We hope that LM advances such as instruction-tuning (Wei et al., 2021; Sanh et al., 2022; Ouyang et al., 2022) mitigate this issue in the future. For now, it may be possible to use prompt sensitivity to generate more diverse datasets, by generating similar datasets with distinct prompts and combining the results, as we did in §3. Where prompt sensitivity caused issues, we found it helpful to be able to view example generated outputs in seconds, to quickly iterate and catch salient failures. For the 133 datasets in §3, we found a general instruction template that worked well; we did not do dataset-specific tuning to obtain samples rated as high-quality by human evaluators.
Hybrid Human-AI Evaluation Generation We are optimistic that hybrid human-AI dataset generation for mitigating many of the issues above, e.g., the approach in §6. For example, it is possible to generate many (possibly flawed) examples with LMs and edit or filter them manually. Hybrid approaches have succeeded in generating evaluation data (Wu et al., 2021; Ross et al., 2021), training data (Liu et al., 2022), and adversarial training data (Nie et al., 2020; Xu et al., 2021; Bartolo et al., 2020, 2021; Ziegler et al., 2022).
Text Generation Evaluations We focus on generating classification-style evaluations, but many evaluations require text generation (e.g., to evaluate text summarization). For text generation evaluations, we recommend the related approach of Perez et al. (2022); Zhang et al. (2022), who generate inputs and evaluate LM outputs using an LM-based classifier.
Potential for Misuse Our results suggests that malicious actors may be able to use LMs to
evaluate LMs’ tendencies to act in harmful ways, to exacerbate such tendencies. For example, a malicious actor may evaluate LMs for their tendency to persuade people towards their own political views, in order to influence the public’s views towards their own. Another issue is that our method is potentially useful to adversaries in finding and exploiting weaknesses in existing models (e.g., to circumvent product safety measures like safety filters3). See Perez et al. (2022); Ganguli et al. (2022) for discussion on such risks, as well as mitigating factors and interventions. Despite the above concerns, we believe it is beneficial to publish our work on LM-written evaluations. LM-written evaluations are also valuable to good actors and efforts to deploy LMs, to catch and mitigate misuse harms as well as accidental harms. In this work alone, we believe that we surfaced several potential issues that, to our knowledge, have not been found before – related to model outputs that express powerseeking tendencies, self-preservation instincts, various strong political views, and tendencies to answer in sycophantic or less accurate ways depending on the user. Overall, our results provide evidence that LMs themselves are a valuable tool for helping us evaluate LMs."
2165,"1. Our investigation in the zero-shot experiment is not exhaustive, we focused on the interplay between the three main tasks that also provide datasets of similar size: argument identification, evidence detection, and argument quality. However, there are other tasks, such as stance classification (deciding whether an argument supports or opposes a particular issue) or argument structure identification (identifying argumentative discourse units, such as claims and premises). Other tasks might be better source tasks for estimating argument quality.
2. Our experiments are based on the most popular datasets in argument mining and argument quality and may not generalize to other more specialized text domains, such as law or politics.
3. Using only English datasets limits the generalizability of the results to other languages and cultures. The ability to identify and evaluate the quality of arguments may be different in other languages and cultures, and the annotators may not be able to accurately capture these differences. This may lead to a lack of robustness and reliability of the results."
2166,"The findings of this study have to be seen in light of some limitations. Compared to CDGM (Lai et al., 2021), introducing multiple metrics to capture flexible cross-sentence event relationships would cause extra computation cost and slightly slow our training. The training parameters of different models can be seen in Table 12. We deprive our model of multiple similarity metrics and denote it as the baseline model. We can see that our model increases 7% parameters compared to CDGM and 0.009% compared to the baseline model."
2167,Section 7 limitation
2168,"This section discusses the potential limitations of our work. This paper’s analysis of model effects mainly focuses on common benchmarks for adversarial detection, which may introduce confounding factors that affect the stability of our framework. Our model’s performance on more tasks and more attack algorithms is worth further exploring. Our detection framework exploits the special properties exhibited by the adversarial sample under universal perturbation. We expect a more profound exploration of improving the connection between UAPs and adversarial samples. In Figure 2, we note that a small number (about 3%) of clean and adversarial samples do not suffer from UAP interference. It is worth conducting an analysis of them to further explore the robustness properties of the language models. We leave these problems to further work."
2169,"While the potential implications of our research are broad, we make note that there are several limitations that should be considered:
1. The scope of this study is limited to legal reasoning tasks using the COLIEE Task 4 (Entailment), which is based on the Japanese Bar exam. The results may not generalize to other legal reasoning tasks in particular common law systems.
2. COLIEE Task 4 itself depends on the COLIEE Task 3 (Retrieval), and we assumed perfect
retrieval of the relevant articles used for the premises.
3. The experiments were conducted with two versions of GPT models only, and it is unclear how other LLMs may perform with this task.
4. The study focuses on zero-shot/few-shot and fine-tuning approaches with and without explanations, as well as various prompting strategies. The explanation and the prompting strategies, however, are difficult to control. For the explanation, we rely on the explanations created by GPT-3.5 without knowledge of how reliable they may be. For the legal prompting, we show that legal strategies have a positive impact on the performance, but it is unclear how explicit mention of the strategies impacts the LM.
5. The use of clustering past training data as fewshot demonstrations is a novel approach, but it is not clear how well it would perform on other data sets (or in other domains). We do not claim that this approach would show improved results for other tasks.
6. The experiments were conducted on the most recent two years of COLIEE task data, and the results may not generalize to other years of the task. More importantly, the test data size is relatively small which is reflected by the mildly statistically significant results for 2021.
7. The experiments were carried out on the English translations only and not the Japanese original text.
8. OpenAI maintains control of GPT-3 and future models, and we cannot guarantee that the model versions used will be available to others in the exact same state."
2170,"Section 8
7 A2. Did you discuss any potential risks of your work? We don’t think there is any risk of our work."
2171,WFRE suffers common drawbacks from the existing query embedding methods. The queries that can be solved by such methods are a limited subclass of first-order queries. It is also not clear how to apply WFRE to unseen entities and relations in an inductive setting.
2172,On Page 5.
2173,"Compared to other interpretability methods, MaRC is able to create explanations that more closely resemble human rationales. Nevertheless, the similarity to human rationales is always limited by the inner workings of the respective neural network: If a network’s reasoning does not mirror human reasoning, the resulting rationales will be incomprehensible to humans.
Additionally, rationales created by MaRC are the result of a complete input optimization process. Therefore, the rationale creation usually requires hundreds of forward passes and gradient evaluations for the respective neural network, which makes the process of creating the rationale timeconsuming and therefore infeasible for many realtime applications. On modern hardware, creating a rationale for BERTbase can take two to three minutes depending on the length of the input text, while ResNet-101 and ViT-B/16 are faster at about one minute."
2174,"Dataset Utilization We have collected 137 datasets, yet we have only conducted experiments over a minority of these (∼40 datasets), leaving the remaining datasets unexplored. Since the datasets are already curated, future work should further explore these datasets in additional experiments. In this work, we do not experiment on image-text datasets for two reasons: (1) all of the image-text datasets are translated from English versions; and (2) there is no large LM available for zero-shot image-to-text generation.
Experiments We did not attempt few-shot or fully-supervised learning experiments in NusaCrowd since prior work has explored these approaches on some of the datasets (Wilie et al., 2020; Koto et al., 2020b; Cahyawijaya et al., 2021b; Winata et al., 2023). We specifically conduct our experiments on zero-shot methods to explore the generalization of zero-shot cross-lingual and zero-shot prompting approaches to extremely lowresource languages.
Task Diversity The tasks represented in NusaCrowd are skewed towards MT, sentiment, abusive text classification, and ASR. Many other tasks remain unexplored for Indonesian and regional languages. Furthermore, most ASR work come from the same authors or research groups. While these topics are prevalent among Indonesian researchers, it is also important to expand to other tasks.
Domain Diversity The datasets in NusaCrowd are primarily from the domains of social media,
news, and other general domain sources. Despite having a huge potential, narrow-domain datasets, such as clinical, biomedical, legal, financial, and educational datasets remain underrepresented for Indonesian and regional languages. Exploration of domain-specific data and use cases for Indonesian and regional languages is critical.
Language Diversity There are 700+ languages in Indonesia. However, we have only focused on a small fraction of these languages. In addition, there are also other regional languages similar to the two Sinitic languages in NusaCrowd, i.e., Hakka (Khek) and Min Nan (Teochew). More focus on under-represented languages is an interesting future direction.
Multimodality The datasets in NusaCrowd are mainly in the text modality. Exploration of speech, image, and other modalities for Indonesian and regional languages is still limited, and there are potentially exciting opportunities to capture locallyrelevant Indonesian culture in such modalities.
Utilization of Datasets There are 137 datasets contained in NusaCrowd. While we showcased three different use cases for the datasets (i.e., zeroshot NLU, zero-shot NLG, and multilingual ASR benchmarks), there is much greater potential to use the datasets in NusaCrowd. Potential areas of focus include experimenting with various approaches and analyses over multiple datasets, such as multi-task learning, continual learning, or few-shot learning."
2175,"Our work proposes a new QG framework, namely SkillQG, to frame the comprehension skill required by a question and generate the corresponding comprehension-oriented questions. The limitations are three-fold:
Firstly, we propose a new skill-based schema for the comprehension nature of questions and map the existing annotations on narrative elements of the FairytaleQA dataset to it and conduct our experiments. This kind of mapping might not reflect the required skills accurately since a narrative element can cover more than one comprehension types. Furthermore, although our proposed skill-based schema is drawn upon general text comprehension, SkillQG is only verified on the FairytaleQA dataset and lacks the analysis on generalizability. However, identifying skills and correlations with comprehension skills on new datasets can be challenging because SkillQG may struggle with the input passage with a relatively simple discourse structure, which usually does not contain complicated relations. One remedy to this issue could be collecting a new QA dataset with the annotations following our proposed schema. We regard it as our future work and deem designing a new annotation specification a promising direction.
Besides, although we boost the downstream QA performance in Section 3.4 by augmenting the original training set with generated questions, the final performance (56.9%) is also far behind the human performance (64.4%) reported by Xu et al. (2022). However, the breakdown analysis of QA performance demonstrates that SkillQG can strengthen
all of the comprehension capabilities, especially the challenging ones. As a result, generating questions that are matched with the current comprehension capabilities of the QA model and co-evolving the QA system and corresponding QG system, could be two interesting research topics.
Last but not least, our SkillQG is built on the PLMs of general domains, ignoring the domainspecific and multilingual application. The backbone PLMs are also shown a biased representation, such as race and gender (Gonen and Goldberg, 2019). Therefore, additional evaluation protocols are left for our future work."
2176,Both risks and limitation are discussed in the (unnumbered) section with the same title. This section is located after Section 5 (Conclusions).
2177,"Our work has the following limitations. First, our analysis of continuity and continuity salience only focused on the sentence level. This is limiting since actual continuous spans can be a part of tokens in a identified copied sentence. Besides, we only utilized string-based overlap for salience estimation, i.e. ROUGE. This can be limiting since semantic salience may not be captured. Furthermore, even if our method can alleviate the negative impact of high dataset extractivity, it may not fully address this issue. In the future, we plan to extend our analysis to token-based continuous spans identification and semantic based measurement for more accurate continuity quantification.
Ethics Statement
The progress in deep neural network architectures and the availability of large pre-trained language models have led to significant advancements with single document summarization. However, current state-of-the-art natural language processing (NLP) solutions still face challenges in consistently generating factual and faithful summaries without any instances of hallucination (Maynez et al., 2020). Therefore, it is imperative to acknowledge that our proposed solution, like previous approaches, is not yet suitable for deployment as it does not specifically address the issue of hallucination. To bridge this gap, future research efforts should prioritize the development of more effective evaluation measures and solutions for text summarization, aiming to ensure highly faithful summaries that accurately represent the source content and enhance the overall trustworthiness of summarization systems. Additionally, in the case of applying the proposed method to sensitive data domains such as medical patient records and legal documents, it becomes essential to incorporate privacy-preserving policies to safeguard the confidentiality of personal information (Da Silva et al., 2006). These measures are critical to instill confidence in the practical implementation of text summarization techniques."
2178,"Our findings about gender bias in the field of Language & Vision are based on two datasets, one task (Object Naming), one language (English), a mostly Western population (based on the origin of both the images and the annotators of VisualGenome and ManyNames), and one computational model. Moreover, in the third analysis, due to the characteristics of the test set of ManyNames, the sample size was small. Additionally, the bias around naming choices concerns the domain of sports only.
Regarding our most novel finding (bias in lexical choice), given the basic function of naming in language and the fact that Western Englishspeaking societies are not known to be more genderbiased than most non-Western and/or non-Englishspeaking societies, it is plausible that the identified bias extends to other L&V tasks such as image captioning, referring expression generation, or Visual Question Answering. Furthermore, given previous work on bias in our field, it is plausible that the identified bias in the model extends to other models. It is, however, not clear whether the bias will be amplified or simply reproduced. To probe whether, and to what extent, the identified biases indeed generalize, future work should tackle more tasks, languages, populations, domains, models, and data. Testing further models on the same naming data that we used is straightforward; checking for biases in some other tasks for English should be feasible at least to some extent, since some datasets provide multiple annotations per image (e.g., captions in MS-COCO). Instead, analyzing other languages and populations, such as nonbinary individuals, will in most cases require further data collection due to the scarcity of non-WEIRD data in our field.
In this study, gender was operationalized in a bi-
nary manner in order to most effectively investigate the stated hypothesis. Furthermore, there is a lack of nonbinary labels within the datasets used (5% of labels can be considered gender-neutral, i.e., ""person, human, child""), and the resources required to reflect the reality of the gender landscape currently do not exist. This indicates a separate but related issue regarding a lack of representation of nonbinary individuals within vision datasets and how to conduct ethically inclusive studies on gender (Larson, 2017). However, addressing this is beyond the scope of the present research and remains an important direction for future work. Finally, this work solely concerns the identification of biases; further work should focus on how to deal with them in terms of data collection, curation, and modeling."
2179,"Left blank.
7 A2. Did you discuss any potential risks of your work? We do not think this is relevant to our submission.
7 A3. Do the abstract and introduction summarize the paper’s main claims? The abstract does, the introduction no because it’s a short paper and we distribute the claims between the introduction and the conclusion.
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts?"
2180,"In this study, we assessed the quality of the corpora Books-Small, Medium, and Large, by training and evaluating a NER model on them but we did not include the corpus Books-Huge in our analysis. However, our results on the Books-Large corpus indicate that there is no substantial benefit to using a corpus larger than Books-Medium for training NER models. This is consistent with prior research on few-shot training on smaller corpora achieving comparable accuracy to larger, potentially noisy corpora (Blouin et al., 2021).
Given the scarcity of benchmarks for late medieval NER (Ehrmann et al., 2021, Table 3), we were unable to conduct experiments on corpora other than our own. Additionally, we utilized a NER model trained on contemporary texts as our baseline for comparison. Therefore, it is important to note that these results may not generalize to other medieval NER tasks. In the future, efforts should be made to develop more comprehensive benchmarks for late medieval NER such as our own."
2181,"In this work, we found that the implications of the geometric properties of LayerNorm affect mainly small models and are less evident for larger models.
We hypothesize that with a large hidden dimension, a Transformer model can find other solutions for computing “majority“ using gradient descent and is, therefore, less dependent on the projection component. Further, we believe that the scaling component is less useful for high dimensional models, since with higher dimensions, it is less likely to encounter a set of vectors where some of them lie within the convex hull of the others. Therefore, we encourage the community to use LayerNorm before attention layers, especially for small models that operate on long sequences.
Moreover, the projection component is clearly a linear operator that can be expressed by a linear layer before the LayerNorm, as we show in Appendix C. Nevertheless, the importance of the projection holds as we discuss in Section 3, and the benefit of using this operator explicitly in LayerNorm is shown in Section 4.1."
2182,"The first limitation of this work is that it lacks a study on how the proposed regularization methods work at other scales; although we looked in Section 5.1 at two variants based on the compute budget (615M and 1.3B), we only tested our methods on the 1.3B variant with a fixed number of experts E=64. These methods can potentially show larger improvements on larger models (larger backbone or more experts) and marginal impacts on smaller models that do not suffer from severe over-fitting. The second limitation of this work is that our methods are only validated on a single multilingual MT benchmark. Some of these techniques proved to be generalizable to a much larger benchmarks (NLLB Team et al., 2022), and we leave testing these techniques on other tasks like language modeling to future work. Another limitation of this work, and most other works on multilingual machine translation, is the evaluation metrics and how to aggregate them. We report in this paper chrF++ scores and we average across three subsets of directions and three resource levels. This makes it difficult to
highlight the impact in some challenging directions on which our methods can lead to ±3chrF++ differential in quality. We did not report other metrics for the sake of brevity, and since we are not comparing to previously published results, chrF++ is a reliable metric for comparing and contrasting our methods."
2183,"section 9
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
2184,"limitations we calibrated opt models based on wikipedia data. future work should apply calibration procedure to a wider range of datasets, to check whether our results generalize to different domains. additionally, we limited our evaluation to entropy as a measure of uncertainty and did not explore other measures. finally, we aimed at validating the calibration status of commonly used lms. future work should thoroughly evaluate the impact of the calibration status on different facets of generation quality, as text generation is one of the main usecases of large lms."
2185,"limitation of our method is the limited utilized knowledge. since our prompt tuning-based method tests on implicit discourse relation recognition (idrr) task, the elicited knowledge only comes from the dataset of this task and the model pre-training corpora. this constraint restricts the capability owing to the reporting bias (gordon and durme, 2013) in the pre-training models (plms). moreover, the relatively few training data of several second-level classes resulting from the highly skewed label distribution problem requires extensive knowledge to make the model understand data instances and the task. although we impose the prior human knowledge against the idrr task from the input template designing to the discourse connectives selection, the knowledge source still only comes from our prior knowledge and the elicited knowledge of plms. as a result, even our method obtains a valid score in all second-level classes except the cont.pragmatic cause displayed in table 6, some second-level senses, which are the same as previous studies, cannot receive a satisfactory performance (e.g., comp.concession and expa.list). the future work for this issue is to integrate more abundant knowledge and equip the model with more vital abilities. for example, grounding the arguments pair on the relevant nodes of the knowledge graph for each data instance (lin et al., 2019) or knowledge distillation from large language models to provides more contextual information and enhances the capability of the model on this task. limited predicted connectives another area for improvement is the prediction of extensive connectives. although our model includes the preselected connectives as our third layer of a designed hierarchy tree, we do not include the ground truth of connectives as our third layer. because including these extensive connectives to form many leaves will result in many paths (more than 100). this limitation may be addressed in future works by utilizing the pruning algorithms for reducing a lot of redundant nodes and leaves on each instance to enhance effectiveness and efficiency."
2186,"limitations in this section, we discuss few limitations of the proposed method and point out future directions to improve the model. first, our method needs to decompose questions into a symbolic representation, but such representations are hard for humans to comprehend, and therefore this decomposition mechanism is hard to be trained with human annotation. a promising direction is to leverage pre-trained language models such as chatgpt 2 to automate this decomposition step, leveraging chatgpt’s internal knowledge of decomposing a complex question into sub-questions. second, the execution of the zero-shot nmns is conducted in a deterministic manner, leading to high risks of error propagation if the reasoning chain gets longer. in the future, we can consider a softer way of reasoning over the image with pre-trained models. 2https://openai.com/blog/chatgpt/"
2187,"limitations while many studies show that the architectures of the deep learning models significantly influence the results, we perform experiments with several base architectures because of the constrained hardware. furthermore, there has not been a vietnamese benchmark summarization dataset, which is both sizable and of high quality. the existing summarization datasets are derived from online magazines, which usually contain misspelled words and grammatical errors. in addition, the reference summaries might not convey the main content of the corresponding articles. therefore, selecting and developing efficient summarization models for vietnamese still present numerous challenges."
2188,"limitations of group dro which have been overlooked before, and can be viewed as a cornerstone for future study in the worst-group generalization. limitations although our unsupervised frameworkq-diversity shows great superiority, when it comes to limitations, we acknowledge that (i) our empirical validations on real-world datasets just follow current benchmarks that shed light on the group shifts caused by spurious correlations. although we conduct experiments on the scenarios with noisy labels and various ood datasets, practically, apart from superficial clues, a series of contributing factors that lead to group shifts are worth further exploration. (ii) a better theoretical understanding of how the interactive training mode can guide q-diversity works in better group identification should be established, and this points out the direction for our future work."
2189,"limitations although the proposed method has shown great performance to alleviate the issues of biased learning and deficient interaction, which are common problems among electra-style pre-training models, we should realize that the proposed method still can be further improved. for example, the inherent flaw of rtd mentioned in section 3 could only be relieved rather than solved. more about mission design regarding with this issue is worth studying. besides, although the results show great performance, more efforts are required to explore the hidden impact of each course, which will help the application of the proposed model in the future."
2190,"limitations by retrieving pseudo captions from summaries, one limitation is that the most relevant sentence for a specific image may not be in the summary. however, it has a trivial impact on the overall msmo performance. if this happens, most of the time, the image will not be the salient image to select, and its caption will provide no helpful information for the text summary. in this situation, selecting a pseudo caption from summary sentences will not hinder the overall performance, though it may not be the best for the specific image. besides, even though our task setting (including the dataset and all evaluation metrics we used) strictly follows three previous works (zhu et al., 2018, 2020; zhang et al., 2021b), another possible limitation is that only one msmo benchmark is used (no other dataset exists). we believe providing more diversified datasets and investigating more about the rationale under the task setting are critical to pushing forward the multimodal summarization community, although they are out of the scope of this work."
2191,"limitations our experiments are based on (arguably) the most standard adapter architecture for adapter-based cross-lingual transfer and beyond, which also facilitates comparisons to prior work in this area. however, we again note that there are other emerging parameter-efficient modular methods, including different adapter architectures (he et al., 2022), that could be used with the same conceptual idea. we leave further and wider explorations along this direction for future work. our evaluation relies on the currently available standard multilingual benchmarks, and in particular those targeted towards low-resource languages. while the development of better models for underrepresented languages is possible mostly owing to such benchmarks, it is also inherently constrained by their quality and availability. even though our experiments have been conducted over 35 different target languages and across several different tasks, we mostly focus on generally consistent trends across multiple languages. delving deeper into finer-grained qualitative and linguistically oriented analyses over particular low-resource languages would require access to native speakers of those languages, and it is very challenging to conduct such analyses for many languages in our language sample. due to a large number of experiments across many tasks and languages, we report all our results based on a single run. averages over multiple runs conducted on a subset of languages and tasks confirm all the core findings; for simplicity, we eventually chose to report the results for all languages and tasks in the same setup. finally, training language adapters is typically computationally expensive; however, owing to the modular design of our framework with respect to language adapters, these are trained only once per language and reused across different evaluations."
2192,"limitations we should point out that the gpusq-tlm compression scheme is highly related to the nvidia gpu’s features to support gpu-friendly 2:4 fine- grained structured sparsity with various data formats. so if the gpusq-tlm compressed models are deployed on the different gpu types without such support, the deployment efficiency may not be as high as expected. for example, the last-generation v100 (nvidia, 2017b) and t4 (nvidia, 2018) gpus have no support for structured sparsity, so the deployment efficiency is lower than a100 (nvidia, 2020) gpu. we should also point out nvidia agx orin chip also support gpu-friendly 2:4 fine-grained structured sparsity as a100 gpu and mainly support edge device use scenarios like autonomous driving. so, in theory, we can also deploy the transformer-based language models on the agx orin chip. however, the large language models need to consume large on-chip memory, so they usually cannot be held by a single agx orin chip. for a100 to represent the server use scenarios, we can use multiple a100 gpus for parallel execution, but for agx orin, we usually only have one chip for the deployment device. that’s why we do not test the gpusq-tlm compressed model on the agx orin chip."
2193,"limitations this paper acknowledges three main limitations: 1) the constraints of a zero-shot setting, 2) an uncertain generalization capacity due to limited data in the target task, and 3) the longer inference time required by a large language model. given the absence of data for our task and the complexity of the target scenarios, collecting a large dataset for supervised or semi-supervised learning presents a significant challenge. as the first approach tackling this task, our framework performs the task in a zero-shot manner, but is applicable to fine-tuning if a substantial dataset becomes available. consequently, we expect that future research could further train the proposed framework using supervised learning or fine-tuning, thereby enhancing the alignment of inferred implicit intents and recommended bots with training data. this would expand our method to various learning settings and validate its generalization capacity. conversely, the gpt-j model used for recommending task-oriented bots is considerably large given academic resources, thereby slowing down inference speed. to mitigate this, our future work intends to develop a lightweight student model that accelerates the prompt inference process. such a smaller language model could not only expedite the inference process to recommend task-oriented bots but also be conveniently fine-tuned using collected data. despite these limitations, this work can be considered as the pioneering attempt to leverage commonsense knowledge to link task-oriented intents. the significant potential of this research direction is evidenced within this paper."
2194,"limitations the main limitation of this work is the usage of explicit knowledge in the knowledge graph. although using knowledge graphs is a common advantage of most current target-oriented dialogue studies, and explicit relations between entities help to effective and reliable reasoning for the recommendation, there is still a large amount of implicit knowledge in unstructured resources that cannot be extracted as explicit triplets, e.g., the multidimensional similarity between entities, but can be a further extra supplement to dialog context. in this work, we involve implicit knowledge by generating a path as a natural language sentence, but the knowledge graph is still necessary. in future work, we will explore only using unstructured knowledge for global planning."
2195,"limitations our approach has the following limitations: 1. it only considers swaps of pairs of functions at the top-level scope, which is a small set of all the quasi-invariances of the python programming language. 2. it only considers code generation in top-level functions, hence it does not evaluate class methods. 3. it relies on a syntactic substitution to generate ""correct"" gold truth outputs, which may fail if the swapped functions are called by a string expression through eval or or queried by their string names using the reflection facilities. 4. in our experiments, we can evaluate only a small number of model sizes per family, since these are the only ones available, therefore the p-values of the correlation with the loss analysis are high. 5. the independent reproducibility of the experiments on closed-source models is predicated on the continued availability of a publiclyaccessible api. at the time of writing, our experiments on the openai ""codex"" models are no longer reproducible without support from openai. items 1 and 2 can be in principle treated by considering more complex code transformations, which we leave for future work. item 3 is harder to tackle in the general case because of undecidability issues. item 4 could be addressed by reproducing our experiments on a model family that encompasses more model sizes, should it become available for public experimentation. item 5 is an unavoidable consequence of using closed-source models."
2196,"limitations although our method scn achieves state-of-theart performance in the cll-id task, there is still a performance gap between scn and the upper bound. this result is inconsistent with human behaviors because humans usually do not forget old skills when learning new skills. therefore, in future work, we hope to introduce findings from the brain science domain into the model design to overcome the problem of catastrophic forgetting."
2197,"limitations there are several limitations of this work. first, while we have observed positive correlations between ft-scores and rtt-scores and conducted experiments to predict ft-scores using rtt-scores, their relations could be complicated and non-linear. we encourage future research to investigate various rtt-score features and more complex machine learning models for better prediction models. second, we have examined the prediction models on low-resource languages in flores-101, but have not tested those very lowresource languages out of these 101 languages. we suggest auditing ft-score prediction models on a small validation dataset for any new low-resource languages in future applications. third, our assessment has been systematic and thorough, utilizing datasets such as flores-101, wmt2020-news, and wmt2020-bio. despite this, the nature of our study is constrained by the timeline of the data utilized. the wmt data we used is from 2020, opening up the possibility that more recently proposed metrics could potentially outperform the ones proposed in this work."
2198,"limitations though our proposed method exhibits superior performance, we also recognize its limitations and discuss potential solutions. our proposed method for goal-directed dialogue generation suffers from error propagation since the three stages perform in a pipeline manner. after analyzing those generated utterances with low human evaluation scores, we find that the performance of dialogue generation is prone to drop when our color model fails to plan an appropriate dialogue path. we intend to alleviate this issue by introducing some techniques in the cascaded generation, such as noisy channel models (shannon, 1948; liu et al., 2021a). in addition, other issues, such as how to make existing goal-directed dialogue systems more engaging and personalized, are worth further exploring. ethical considerations goal-directed dialogue systems can be used for creating non-obtrusive recommendations for specific products and services, introducing interesting new topics and educating users about those topics, and so forth. developing such systems requires careful consideration since it has a broad impact on applications. the intention of our work is not to force the system to reach the designated target nor force users to accept recommendations. instead, we aim to build better assistive technologies to improve the proactiveness of dialogue systems. furthermore, our experimental datasets are publicly available. they have been filtered for sensitive and private information during dataset construction. we hope to raise awareness of the potential for misuse of such systems with toxic intentions. for example, such systems may be used to pose as humans and actively manipulate users’ perceptions on specific issues or political inclinations. to mitigate these risks, we emphasize the importance of improving transparency through regulations. it is essential to inform users that they are conversing with a bot instead of a human, and regulations on target designation are crucial when deploying these systems in specific domains. it is necessary to ensure that setting a target does not violate factual accuracy, user privacy rules, or human laws."
2199,"limitations in this method, e.g., slow convergence and training instabilities. we hope that future studies in this field can alleviate the aforementioned problems and thus promote the application of prompt tuning. limitations this section disccuses the limitations of prompt tuning for the unified multimodal pretrained mod- els, and point out some directions for future work. one limitation of prompt tuning in this setup is the sensitivity to hyperparameter tuning. it is difficult to search for a suitable hyperparamter setup. the hyperparameter tuning experience in finetuning is not suitable for prompt tuning. fortunately, we find that prompt tuning for generative multimodal pretrained models is not as sensitive to hyperparameters as prompt tuning for pretrained language models. we provide details of hyperparameter setups in appendix a.1. another limitation of prompt tuning in this setup is slow convergence. though prompt tuning has noticeable advantages in training efficiency, it costs at least 40 epochs for prompt tuning to achieve the nearly best performance on some datasets (e.g., refcoco). a larger number of training epochs may incur more computation costs though prompt tuning has an advantage in training efficiency compared with finetuning. we demonstrate more details in appendix a.2. this indicates that finding a better solution for fast and stable convergence is also important besides reaching comparable or even improved performance over the conventional finetuning. despite the aforementioned limitations, prompt tuning demonstrates significantly better robustness against adversarial attack. in the future, we should pay more attention to this merit and find ways to leverage it."
2200,"limitations our model simultaneously encodes structural and temporal contexts of the tkg substructure, and uses heuristic strategies to select a portion of query-relevant facts as input texts for plms. we can achieve stunning results with these selected facts. however, this work only considers the queryrelevant one-hop neighbor facts to achieve a good performance improvement, but ignores the benefits of multi-hop neighbor facts. we leave it for future work to verify the effectiveness of multi-hop paths."
2201,"limitations of chatgpt in standard academic datasets. to our best knowledge, this is the first work that conducts an extensive evaluation of chatgpt in benchmark nlp datasets. we observe that even though chatgpt obtains impressive zero-shot performance across various tasks, it is still far from reaching human-level performance in many tasks. moreover, potential biases and ethical concerns, as well as misinformation generation risks of chatgpt are discussed. in addition, a unique capability of chatgpt has been studied. though there may have numerous other capabilities of chatgpt that go unnoticed in this paper, future work should nonetheless investigate the capability of chatgpt on more tasks. we will make all our prompts and chatgpt-generated responses publicly available. 5https://beta.openai.com/docs/models/overview"
2202,"limitations, conducting the human evaluation by the authors does not lead to any unwanted biases or ethical concerns. only the publicly available academic datasets are used that did not require any licensing. thus, no personally identifiable information has been used while evaluating chatgpt responses."
2203,"limitations in this work, we have focused on the efficiency concerns of task-agnostic domain adaptation approaches leveraging pre-trained transformer-based language models. the experiments are conducted on four tasks across 14 domains in both high- and low-resource scenarios. we only consider the methods utilizing pre-collected in-domain unlabeled text corpora for domain-adaptive pre-training. it is worth pointing out that the selected domains are strongly correlated to the selected tasks, which does not reflect the wide spectrum of domain interests. besides, the datasets are covered only in english to magnify the domain adaptation controlling factors and use cases, while multilinguality would be the next step to explore. we experimented on encoder-only ptlm based on the downstream classification tasks, where the encoder-decoder ptlm would be applicable to different tasks (e.g., natural language generation, summarization, etc.) requiring more computational resources. we hope that future research builds on top of our findings and extends the research toward more domains, more languages, more tasks, and specifically with the meta-tokenizers for efficiency concerns of domain adaptation approaches."
2204,"limitations our proposed monet is a classification-based method requiring the pre-defined ontology containing all slot-value pairs. moreover, during prediction, for each slot, its distance with all possible values is calculated, i.e., the prediction has to process 30 times, which is the number of slots in the multiwoz dataset. compared with the generation methods that only process once and do not need ontology, our method is short in training efficiency and scalability. however, most task-oriented dialogue datasets contain their knowledge base containing slot value information, so it’s acceptable to construct the ontology for random sampling. besides, the results in section 5.7 demonstrate that our method can be implemented into generation-based backbone models."
2205,"limitations persona extractor first, we need to clarify that our definition of persona is not exactly psychological, the role an individual plays in life (jung, 2013). as a result, like previous studies (e.g., persona-chat (zhang et al., 2018), pec (zhong et al., 2020)), the format of persona is flexible and variable. as stated in §4.1, there are still some issues with the model we use to infer persona information. for example, we sometimes get information that contradicts the facts. and also, there is occasionally unrelated content, as with commonsense reasoning (tu et al., 2022). furthermore, we cannot guarantee that we can infer all of the persona information that appears in the conversation because much of it is frequently obscure. and when extracting persona information, we only use what the user said previously and remove what the bot said, which results in the loss of some conversation information. the reason for this is that we have discovered that if we use the entire conversation, the model frequently has difficulty distinguishing which persona information belongs to the user and which belongs to the other party. in addition, since the code of xu et al. (2022) is not yet available, we have not compared other methods of extracting persona dynamically from the conversation. strategy-based decoding during the decoding phase, we only coarse-grained the α of each strategy because we discovered that only coarse-grained tuning produced good results, and future work may be able to further explore the deeper relationship between different strategies and persona. ethical considerations in this work, we leveraged two publicly available datasets. first, we used the persona-chat dataset, which is collected by assigning a set of fixed predefined persona sentences to workers. therefore, by participating in this dataset, workers were required not to disclose any personal information (zhang et al., 2018), which prevents issues regarding the leakage of their privacy. similarly, during the collection of the esconv dataset, participants were asked to create imaginary situations and play the role of a support seeker who is in that situation. in addition, they were instructed not to provide personal information during their conversations with the trained supporters (liu et al., 2021). regarding the persona extractor, this module is trained to infer and extract persona information solely from what the user has mentioned in the conversation rather than making assumptions about the user’s background and character, further highlighting the importance of user privacy in our research. regarding our experiments, we ensured that all workers agreed to participate in the annotation tasks. moreover, as the workers were recruited from the us, we ensured that they were paid above the minimum wage in this country for successfully completing our tasks. we acknowledge that using trained dialogue models to provide support is a sensitive subject and research on this topic should be conducted with sufficient precautions and supervision. we also acknowledge that in their current stage, such models cannot replace human supporters for this task (sabour et al., 2022a). thus, they should not be employed to replace professional counselors and intervention and interact with users that suffer from mental distress, such as depression or suicidal thoughts."
2206,"limitations there are two potential risks with our method. first, iss trades generality for efficiency by learning only task-specific representations. consequently, it may not be suitable for other tasks. secondly, our method is hardly practical for few-shot or zeroshot learning, as few or no task data are available as anchor points. these potential risks are left to future work."
2207,"limitations of the prior supsup method. through the avoidance of conflicting weight updates, exssnet not only improves performance but also eliminates forgetting, striking a delicate balance. moreover, the inclusion of the knowledge transfer (kkt) module propels the learning process, utilizing previously acquired knowledge to expedite and enhance the learning of new tasks. the efficacy of exssnet is substantiated by its superior performance in both nlp and vision domains, its particular proficiency for sparse masks, and its scalability up to a hundred tasks."
2208,"limitations this section discusses the limitations of this work for more insights on the research in this track. though ofa-ocr achieves high accuracy on multiple text recognition datasets, its costs are larger than the non-transformer baselines. in practice, it is difficult to deploy such large models. thus in our future work, we will discover how to distill or compress ofa-ocr to a light-weight model with high efficiency."
2209,"limitations the cloning model used, yourtts, is trained on the vctk dataset which consists of high-quality speech signal. it is therefore unclear whether the same accuracy would be obtained with lower quality signal which may contain some background noise. (however, it should again be noted that even if all substituted instances are identifiable in the output, the system is equivalent to a masking model.) the selection of a person ne replacement does not currently account for continuity: if the same person entity is referred to later, it may be substituted with a different entity to the previous occasion. in addition, the back-off strategy ignores aspects such as gender. to show the approach feasible, very little optimization was performed. further training and parameter optimization is likely to lead to improved performance for both asr and ner models. the approach is currently only implemented for person nes but it could be extended very simply to other types of nes. however, the degree to which other entity types require obfuscation in speech is not clear to us as mentions of organizations may well not be identifying at all."
2210,"limitations unlike the traditional multimodal contrastive loss focusing more on building the direct link between paired modalities, our proposed unis-mmc aims to leverage inter-modality relationships and potential effectiveness among modalities to create more trustworthy and complementary multimodal representations. it means that unis-mmc is not applied to all multimodal problems. it can achieve competitive performance in tasks that rely on the quantity of the joint representation, such as the multimodal classification task. it is not suitable for tasks that rely purely on correspondence between modalities, such as the cross-modal retrieval task."
2211,"limitations as pointed out by shi et al. (2020), applying ibp technologies to large-scale pre-trained bert models is challenging because of the calculation of bound propagation on the attention layer is relatively loose. since bert is currently one of the most popular architectures in nlp, there is a limitation that the proposed method combined with ibp training could not generalize to bert architectures. however, it is worth noting that the proposed method based on textcnn architectures achieves better certified robustness than the advanced baselines, safer and ciss based on bert. besides, this paper focuses on enhancing the model’s robustness to word substitutions, but not investigates the robustness to character-level or sentence-level perturbations."
2212,"limitations while we achieved preliminary results and created a preliminary projection of the factbank source and target corpus, we do not capture the full source and target nesting in our machine learning experiments. we repeat the example from section 4: mary said that john said that jane was coming to dinner, but bob said that she was not. the embedded sources for the coming event are (author → mary → john), which translates to ""according to the author according to mary according to john, did the coming event happen?"" in our experiments and machine learning architecture, we focus on the last nested source, or john in this example. in future work, we aim to link together all sources and their embedded nesting structures. we note that all experiments in this paper were performed using the flan-t5-base model. in future work on this task, we will explore different generative models such as gpt-3 or bart, which may yield stronger performing systems or more interesting results. we are especially curious about framing this task using gpt-3, especially performing tasks on few-shot or in-context learning. finally, we note that these experiments do not account for potential biases prevalent in fine-tuning large language models. we hypothesize that for some sources in text (i.e. power figures, authorities, or specific names), there may be biases towards certain labels. we will investigate these biases in future work, as an event factuality prediction system with inherent bias can have real world implications."
2213,"limitations this work presents some limitations. firstly, our focus is confined to evaluating the specificity of predictions made by pre-trained language models for entity relations. as noted in section 7, specificity can potentially be tested in a broader range of scenarios. despite this restriction, we consider this work as an initial attempt to highlight the concept of language model specificity. we believe it will stimulate further research into this crucial, yet under-explored, area. a second limitation is the scale of the models evaluated in this work. given the swift evolution of large language models concurrent with the drafting of this paper, the models we examined are comparatively small. as pointed out in the work of zheng et al. (2023), large language models may fail to answer a problem at the appropriate level of specificity. we thus encourage future investigations to delve into the specificity of these rapidly evolving, larger language models."
2214,"limitations although we demonstrated that the proposed metric peculiarity is useful for selecting candidates for few-shot cross-lingual transfer, our current work has the following limitations. lack of evaluations to argue the usefulness of peculiarity. we demonstrated that peculiarity selects candidates to efficiently enhance few-shot cross-lingual performance in several tasks and languages. in addition, peculiarity is robust for hyperparameter k. however, further verification is required to evaluate the usefulness of peculiarity. in this study, we only used xlm-r as the mmlm in the experiments, because previous works (lauscher et al., 2020; kumar et al., 2022) have demonstrated that mbert and xlm-r show the same trend and xlm-r achieves better zeroshot and few-shot cross-lingual performance. however, it is not obvious that peculiarity will work well in mbert. in addition, recently, lin et al. (2022) proposed xglm, a pre-trained multilingual causal language model, that demonstrates strong multilingual capabilities. we would like to experiment using these pre-trained multilingual models to show the usefulness of peculiarity regardless of models. we fine-tuned the mmlm using a standard training objective, predicting true labels or tags for inputs. on the contrary, zhao and schütze (2021) revealed that fine-tuning in a prompting format encourages better zero-shot and few-shot crosslingual transfer than the standard fine-tuning. it is worthwhile to examine few-shot cross-lingual transfer performance when fine-tuning the mmlm with high peculiarity examples in a prompting format because it may be possible to achieve higher accuracy in the target languages with a smaller amount of examples. we experimented using english as the source language. however, if possible, it is better to use a language that is linguistically close to the target language as the source language (pires et al., 2019; lauscher et al., 2020; chai et al., 2022). in our experiments, we did not show that peculiarity works well regardless of source languages. therefore, verifying this aspect is also a remaining challenge. definition of annotation cost. in this study, we defined annotation cost in terms of the number of candidates following previous studies (pires et al., 2019; lauscher et al., 2020; chai et al., 2022). however, a small number of candidates does not necessarily mean less work for annotators. if a candidate (sentence) length is long or hard, it is considered to take longer to understand. on the other hand, if the candidate length is short or easy, annotation time per candidate will be shorter, and the annotators can annotate more candidates in the same time. therefore, we should evaluate candidate selection methods based on total time required for annotation. in addition, aligning the cross-lingual representations between source and target languages using bilingual data is one approach to enhance accuracy for the target languages (lample and conneau, 2019; cao et al., 2020; chi et al., 2021; dou and neubig, 2021; yang et al., 2021). to align the representations, we should create bilingual data through a human or automatic translator. verification whether labeling or translating is less labor intensive and further boosting performance is one of the future goals. developing a better peculiarity-based candidate selection method. in this study, we used the bos hidden states to measure peculiarity; in other words, it measures example-level peculiarity. in classification tasks, using example-level peculiarity to select candidates is intuitive because we predict labels based on the bos hidden states. on the other hand, in the sequence-tagging tasks, we predict token tags based on hidden states of each token. in addition, we consider that it is necessary to fine-tune ms with peculiar tokens, tokens that are not covered by the source language, to ensure that the model predicts tags of these tokens correctly. therefore, we will attempt to select candidates that contain peculiar tokens by using token-level peculiarity and conduct few-shot cross-lingual transfer in the sequence-tagging tasks. we observed that peculiarity selects more redundant candidates compared to the km-based methods and argued that this aspect is the reason that peculiarity does not work in the “poor” group. we consider the possibility of other reasons for this behavior. several studies (swayamdipta et al., 2020; sorscher et al., 2022; hacohen et al., 2022) have suggested that if only a small amount of examples can be used for training, it is important to use not only hard (atypical) examples but also some easy (typical) examples for training in order to improve model performance. in terms of few-shot crosslingual transfer using peculiarity, we should finetune ms with the both highest and lowest peculiarity examples. in addition, using typical examples selected by km instead of the lowest peculiarity examples is one of the approaches. for future work, we would like to verify the effectiveness of these methods for few-shot cross-lingual transfer."
2215,"limitation, we reformulate re into multiple-choice question answering (qa) with the purpose of leveraging a task that is widely cov- ered in instruction-tuning datasets like qa, instead of re, which is barely present in these datasets. comprehensive experiments demonstrate that our qa4re framework unlocks the power of llms as zero-shot relation extractors, especially for two recent llms (text-davinci-003 and flan-t5 xxl). we also conduct thorough experiments to explore the robustness and few-shot effectiveness of our method as well as study in what llm training scenarios it is most effective. in future work, we hope to explore additional underrepresented tasks in instruction-tuning that might be challenging for llms and could be successfully aligned with more widely adopted instruction-tuning tasks like qa. additionally, we plan to continue exploring this line of work by leveraging our qa4re framework for other llms such as the opt-series (zhang et al., 2022; iyer et al., 2022) and palm (chowdhery et al., 2022), which are not included in this work due to the limited computational resources and/or access."
2216,"limitations tada makes use of the pseudo-dialectal translation systems of prior work ziems et al. (2022, 2023). we rely on them as they are validated by dialect speakers and have been shown to be predictive of performance on gold dialect data. however, they were designed as stress tests of robustness which isolates morphology and syntax. we are therefore unsure how tada performs when it faces the topical and register shifts which often are associated with naturally occurring dialects. these limitations are similar to localization issues in translated benchmarks (moradshahi et al., 2020). in this work, we evaluate tada on only encoder-only llms. increasingly, both encoderdecoder and decoder-only models are seeing widescale use due to their flexibility (wang et al., 2022). evaluating tada and developing alternate tailored task-agnostic methodologies on these alternate llm architectures is left to future work."
2217,"limitations that may suffice for current text-to-sql benchmarks with small-scale schemas. however, for large-scale schemas, modifications to the plm encoding method are necessary. if the plm were to encode the question and schema separately, the emsl would still be required."
2218,"limitations, the unk entry for ratsqlb is empty. random initialization means that model results after each training may vary slightly, so we only focus on the more salient features. although the results of ratsql and ratsqlo are similar, ratsqlo consistently outperforms ratsql in three error types when emsl is removed; this supports the view we discuss in section 2.2. more importantly, the single-word performance of ratsqlo without emsl is close to that of ratsql and ratsqlo. as discussed in appendix f.1, the representation ability on multiword of glove is worse than that of bert. the results support this view where the performance of ratsqlo and ratsql on multi-word is worse than that on single-word. when replacing the glove with bert, due to the improvement of its multi-word representation ability, the performance of ratsqlb with and without emsl are close in single and multiple words. from the right side of table 8, it can also be found that the bert brings around 5% absolute improvement on multi-word, while that on single-word is only 2%. acl 2023 responsible nlp checklist"
2219,"limitations of existing book summarization resources, such as booksum. indeed, previous datasets for full-text book summarization are, i) limited in size, and, ii) monolingual, i.e., usually covering english only. in addition, we leveraged echoes to bring to light the unsatisfying capabilities of current approaches to generalize to book summarization. finally, to mitigate this issue, we proposed a new extractivethen-abstractive baseline for book summarization, which outperforms its purely-abstractive counterpart on echo-wiki and echo-xsum, achieving results on the standard booksum test set that are comparable with the current state of the art while using a number of parameters that is only 0.1% compared to the best-performing method. we believe that echoes will foster future work on long-document summarization, especially in the multilingual and cross-lingual setting. limitations despite the multilinguality of our resource, there is still a strong bias towards the english language, as the majority of books are in english and many translations are from english. this may result in the values of english literature being reflected, and these may differ from those of other cultures; summarizing literature from different cultures and regions may not be fully accurate, as every region has had its own historical development. language models used in the experiments can inherit biases from the training data and the tools, such as the ones used for preprocessing, and have limitations that have not been fully evaluated and could impact the results of this study. this study includes the use of web data, which – while marked as public domain – may be subject to copyright laws. the data used in this study was collected for research purposes and was not intended for any other use. additionally, it is worth noting that the majority of books used in our resource are copyright-free, and therefore, old. while this allowed us to include a large number of texts in our dataset, it also means that our resource may not fully capture contemporary literature and may not be representative of current linguistic trends and cultural values."
2220,"limitations we summarize the limitations of this work as follows: (1) we conduct experiments on 7 language understanding tasks across 4 types (i.e., sentiment analysis, topic classification, natural language inference and paraphrasing). however, the effectiveness of gdfo on tasks such as sequence labeling and generation tasks has yet to be fully examined. (2) our proposed method uses a student model and a prompt generator, thereby resulting in a higher computational resource requirement in comparison to gradient-free methods. therefore, it may not be suitable for implementation on certain edge devices, but it is more appropriate for personal or enterprise users who have access to a certain degree of computational resources and have stringent requirements for the model performance. (3) we only focus on the few-shot setting in this paper. it is possible to extend our work to other scenarios such as semi-supervised learning and we will further explore it in the future research."
2221,"limitations there are two main limitations of our work: (1) our approach requires a set of previously aligned predicate pairs as training data to achieve predicate alignment between different kgs, which limits the generalization ability of our method. in our experiments, since we manually aligned a set of equivalent predicates with arguments of types person and location between the english and chinese egs, we can only perform predicate alignment and entailment graph enhancement between the <person,location> subgraphs of two egs. we will explore the semi-supervised or unsupervised predicate alignment method between different egs in our future work. (2) our current enhancement strategy introduced in section 4.2 is straightforward. it might not be robust enough when dealing with entailment graphs of poor quality. we will explore more adaptive eg enhancement methods in the future."
2222,"limitations the main concern regarding our model is the computational complexity. higher-order mfvi has a complexity of o(n3), which admits fully parallel computation and thus is fast on gpus. the complexity of structured inference of treecrf is also o(n3). however, due to the dynamic programming computation restriction, only o(n2) out of o(n3) can be computed in parallel using parallel parsing techniques (rush, 2020), slowing down the running speed. besides, differentiating through the treecrf marginals needs many gpu memories (kim et al., 2017), as automatic differentiation saves all intermediate dynamic programming items for back-propagation, which cause plenty of waste of gpu memories. in this work, since the memory problem is not too severe, we use automatic differentiation for simplicity. one solution is to manually implement the outside algorithm to mitigate the memory problem (kim et al., 2017)."
2223,"limitations for this work, we have several limitations: first, as described in section 6.6, we found that the choice of different templates and the order of generating content will both lead to performance variation. it is worthwhile to conduct a detailed investigation on this interesting problem, however, due to the limit of pages, we only experimented with limited alternative templates. second, our proposed aqe task shares some similarities with some tasks in other domains, which means that it is possible to adapt our proposed framework to other tasks, such as relation extraction and sentiment analysis. we will leave this for future research and demonstrate its effectiveness in other domains. last, subject to both the economic and time cost of dataset annotation, we only expand one existing dataset for our proposed aqe task. we will explore more possibilities for dataset construction for future work."
2224,"limitations this work is constrained by the number of grounding phenomena analyzed, which is limited by the dataset domain and their straightforward automatic computation. we only focused on lexical alignment, the use of ellipsis (fragments) and pronouns, disregarding other phenomena such as repairs (e.g. asking for confirmation or clarification) (purver et al., 2003), among others. with respect to the linguistic phenomena, we simplified the calculation of the lexical alignment by regarding only the last two turns of a conversation (the user question and the system response). in this manner, we omitted the dynamic convergence over several turns (mills and healey, 2008). it should be noted though that this was decided based on manual observation of examples, the majority of which exhibited lexical alignment in the last two turns only. this could be a limitation of the ocqa domain, and/or a bias of the topiocqa dataset. another limitation is that the form of crowdsourcing experiments we performed are mostly diagnostic of certain conditions on a given dataset, and does not reflect more organic real-use cases. an ideal setup would be to collect whole dialogues in the form of an extrinsic evaluation, which would be more costly to perform."
2225,"limitations in this paper, we provide an overview of the current state of knowledge on reasoning in large language models. reasoning is a broad concept that encompasses various forms, making it impractical to summarize all related work in a single paper. therefore, we focus on deductive reasoning, as it is the most commonly studied in the literature. other forms of reasoning such as inductive reasoning (yang et al., 2022; misra et al., 2022, inter alia) and abductive reasoning (wiegreffe et al., 2022; lampinen et al., 2022; jung et al., 2022, inter alia) may not be discussed in depth. additionally, given the rapid evolution and significance of reasoning within large language models, it is crucial to note that new contributions may have emerged in the field concurrent with the writing of this paper. an additional resource to consider is a parallel survey by qiao et al. (2022), which emphasizes reasoning via language model prompting. our coverage may not extend to papers released during or after 2023 such as evaluation on chatgpt (bang et al., 2023; zheng et al., 2023). as such, we recommend readers to check the papers that cite this survey for a more comprehensive and updated understanding of this field."
2226,"limitations we presented in this paper a real-world annotated example of seeking information in scientific publications. even if the number of instances presented here is of the same order of magnitude as what is present in benchmarks, we presented only one query and its correspondent relevance judgements, provided by one expert, due to resource constraints. as we noted above, building a corpus dedicated to the exploration of a single information need does however correspond to a real industrial use case. further, we favored the use of sentencetransformers format for all neural models for the sake of efficiency. we did not dive into providing the best-known performing models and did not consider optimizing them in our case, as overfitting to our data might induce errors in"
2227,"limitations we present some limitations of our approach, which can be investigated in the future: (1) currently, our approaches need to manually choose image for each text label, which may cause the model to be sensitive to the images selected. though the ensemble method can alleviate this problem to some extent, how to automatically map the text label into the corresponding image is an interesting research question to investigate. (2) since clip was pre-trained on noisy web-crawled data on the internet, our approaches are limited by pre-training data distribution of clip. therefore, a potential future direction is to further pre-train clip on more general downstream task datasets."
2228,"limitations the experiments were performed only on japanese and english bilingual dialogue collected from a limited number of native speakers. although the methods proposed in this work can work on any language pair, drawing"
2229,"limitations this paper focuses on the style transfer of easy language for german. due to their word inflections and high average word length, languages like german are harder to learn for language models (mielke et al., 2019). therefore, the proposed approach may work even better on easier-to-model languages, but we did not test any other language. in addition, the style transfer of simplified language uses the same vocabulary as the original language and only reduces its diversity. our approach has yet to be evaluated on other styles, for example, ones that introduce new words. when evaluating the influence of fine-tunung on the grammaticality of the model outputs, we found that even the original models were not perfect and produced grammatical errors. one possible reason is relying on gpt2-based models that are relatively small and, thus, perform worse than state-of-theart language models like palm (chowdhery et al., 2022). in addition, the german base models are often already fine-tuned versions of english models, and thus, may already suffer from catastrophic forgetting due to fine-tuning."
2230,"limitations we outline two limitations of our work from user behavior sampling and knowledge population aspects. due to huge-volume user behavior data produced every day in the e-commerce platform, it is crucial to efficiently sample significant behaviors that can indicate strong intentions and avoid random co-purchasing or clicking etc. though in this work we adopt the criteria of selecting nodes whose degree are more than five in the co-buy graph, it is still coarse-grained and more advanced methods remain to be explored in order to sample representative co-buy pairs for intention generation. some potential solutions are to aggregate frequent co-buy category pairs and then sample product pairs within selected category pairs. moreover, our proposed framework can be generalized to other types of abundant user behaviors such as search-click and search-buy, which requires to design corresponding prompts. we leave these designs to future work. for open text generation from llms, it becomes common practices to label high-quality data for finetuning to improve the quality and controllability of generation such as lamda (thoppilan et al., 2022), instructgpt (ouyang et al., 2022), and chatgpt6. however, computation cost is the major bottleneck to use annotated data as human feedback for language model finetuning with billions of parameters, like opt-30b in our work. hence we adopt a trade-off strategy to populate human judgements by training effective classifiers and conducting inferences over all the generation candidates. with impressive generation performance of chatgpt, we expect efficient methods to directly optimize llms with human feedback in more scalable way like reinforcement learning (rlhf), and enable llms to generate more typical intention knowledge with less annotation efforts."
2231,"limitations: the duration task focused only on data with a positive happiness label, but it would be interesting to see whether the framework generalizes to a complete dataset and more sophisticated problem definitions. the need for annotations limits the generalizability of our approach, but the bert-psyam framework is effective even with labels generated through semi-supervised methods and other metadata. ethical considerations: the models are intended for aggregate- and group-level inferences, and not individual or message-level inferences. despite our cross-domain validation efforts, we caution that relying exclusively on ai-inferred relationships between emotion, self-efficacy, and selfdetermination may lead to inaccurate measure- ments. finally, models trained in a specific sociocultural setting may nevertheless violate the social conventions in specific settings, such as in the workplace, and cultural conventions of individualism and collectivism in social life (diener et al., 2009). acknowledgments: we thank niyati chhaya, chaitanya aggarwal, and gerard yeo for feedback on early versions of this work. this work was supported by an nus ctic grant and a nanyang presidential postdoctoral fellowship."
2232,"limitation this work has two main limitations: (1) the performance of the model largely depends on the performance of the annotation model. if the annotation model is too simple, it may cause the performance of the dst model to decline. on the contrary, it will increase the complexity of the overall model and prolong the reasoning time. (2) even for the labeling model with good performance, the tagging values may also interfere with the dst model. for details, please refer to the analysis experiment."
2233,"limitations although htf has achieved promising performance on removing spurious correlations, we identify the following limitations. firstly, although htf encounters the smallest performance decrease among compared methods under multiple semantic interventions, the interventions still cause a performance drop. therefore, more approaches can be explored to further improve the generalization ability of htf, such as increasing the scale of the backbone model or applying more informative hypothetical examples. secondly, the experiments are only conducted in the financial domain due to limited datasets with sufficient annotation of hypothetical examples. since hypothetical examples are more economic to obtain than counterfactual examples, we believe that more datasets with hypothetical examples will be proposed in the future and thus htf can be applied in more domains. thirdly, we are unable to compare the effectiveness of hypothetical and counterfactual examples because tat-qa does not contain both types, and constructing all counterfactual examples is impractical for us due to cost constraints. note that we do not conclude any effectiveness relationship between hypothetical and counterfactual examples in the paper."
2234,"limitations in the evaluation datasets, as well as (for the scholarbert models) small model sizes relative to the large pretraining corpus. we make the scholarbert models available on huggingface (https://huggingface.co/ globuslabs). while we cannot share the full public resource dataset, we have provided a sample of open-access articles from the dataset (https://github.com/tuhz/ publicresourcedatasetsample) in both the original pdf and extracted txt formats to illustrate the quality of the pdf-to-text preprocessing. limitations our 12 labeled test datasets are from just five domains (plus two multi-disciplinary); five of the 12 are from biomedicine. this imbalance, which reflects the varied adoption of nlp methods across domains, means that our evaluation dataset is necessarily limited. our largest model, with 770m parameters, may not be sufficiently large to demonstrate scaling laws for language models. we also aim to extend our experiments to tasks other than ner, relation extraction, and text classification, such as question-answering and textual entailment in scientific domains."
2235,"limitations our work is limited in several ways. we use human judgements on our case study data to demonstrate a preference of vrm-e versus spsm. however, additional case studies in other domains such as education, healthcare, legal studies etc. are necessary in order to gather empirical evidence that preference for vrm-e generalizes. threats to validity. there are several threats to interpreting our case study estimates as causal. like any causal study with observational data, our case study relies on untestable causal identification assumptions such as no unmeasured confounding. other unmeasured confounding likely does exist. for example, our document embeddings do not necessarily measure the “quality” of the manuscripts or the “novelty” of the ideas, both of which could affect reviewers’ scores. regarding estimation, by allowing for matching with replacement, appendix c.1.1 shows that several manuscripts are reused with high frequency. this will introduce bias within our model as noted in stuart (2010). additionally, our choice of b satisfies overlap but at the expense of very similar semantic matches between manuscripts. this could explain why there was only a moderate amount of agreement between the human judges as many matches are less semantically similar than we would prefer."
2236,"limitation our method has a few limitations which may inspire future work. first, the prompt templates are manually designed, although we’ve introduced the rules and intuitions used in our implementation. second, the proposed method may have low scalability to long text. because we add the prompt at the end of the context, the prompt would be truncated if the context itself exceeds the maximum acceptable token length of the model."
2237,"limitations the proposed synthesis framework has been targeted at text-to-sql task, which may not generalize to other tasks that require large amount of synthetic data without major modification. for instance, other popular tasks involving converting natural language questions to some sort of logic forms are in natural very similar to text-to-sql, yet all techniques relying on the ""key"" property in the database might no longer be applicable. on the other hand, the template based synthesis method currently relies on templates extracted from the real data. by incorporating some carefully designed grammar (e.g. pcfg), we may be able to further enrich the template set."
2238,"limitations although our work shows that our cpll model can learn from crowd-annotated ner data well, there are at least two limitations. first, we set the hyperparameter α manually. it would be better if we could design a strategy to learn a alpha adaptive value for each sample atomically. second, though we mainly experiment on ner tasks, our model can be applied to all sequence labeling tasks, such as part-of-speech tagging (pos), chinese word segmentation, and so on. we would like to explore it in further work."
2239,"limitations the schema information and grammar design are domain specific. we have tested our approach mainly on our own dataset, though we believe the similar approach can be applied to other tasks, as long as the meaning representation involves functions and arguments. also, we have not explored all approaches to obtain the highest possible accuracy on this dataset, because our main goal is to show the effectiveness of the proposed approach, which we believe is clearly demonstrated by the current result. at this stage, the difference between in-distribution and out-of-distribution accuracies remain very large, and there is a large room for further improvements. we hope by releasing this dataset we can help promote research in related areas."
2240,"limitations the method introduced in this paper applies to a specific type of sentiment analysis task, where the item to be analysed is a review, the author of the review and the product/service being reviewed are known and uniquely identified, and the author (user) and product information is available for all reviews in the training set. while our approach is expected to perform well on other languages beyond english, the experimental results do not necessarily support that since our evaluation is only carried out on english data."
2241,"limitations although we have shown the potential of performing automatic emotion cause extraction (ece) on social media without human annotation, there are still several limitations in our work. firstly, our work only considers the ece task in chinese microblogs. it might be interesting to investigate the effectiveness of our framework in social media platforms in other languages. secondly, we only focus on extracting the emotion cause expressed in the current post. however, according to cheng et al. (2017), 37% of the emotion causes exist in the original or historical posts in a conversation thread. hence, it would be interest- ing to extend our work to more complex microblog structures in the future."
2242,"limitations the limitations of our work can be summarized in three points. first, as mentioned in section 5, although a direct consideration of prompt information is helpful for related trait-scoring tasks, it may not be for irrelevant traits. therefore, selectively applying each method depending on which traits are to score might further improve the model. second, although the use of pre-engineered features, such as our topic-coherence feature, has the advantage of interpretability (uto et al., 2020), it requires additional engineering steps, as in other aes studies using hand-crafted features (amorim et al., 2018; dascalu et al., 2017; nguyen and litman, 2018; ridley et al., 2021). finally, despite the large improvements observed on the specific datasets asap and asasp++, the model has not experimented on other datasets. feedback prize dataset8 is well-designed for scoring english-written argumentative writings with multiple trait labels, but the prompts are not defined; thus, it does not fit for cross-prompt aes. essay-br dataset (marinho et al., 2022) contains essays on multiple prompts with labeled multiple trait scores. thus, in future work, our proposed methods can be extended to multilingual cases of aes using the dataset."
2243,"limitations and should acknowledge and/or try to mitigate them to the extent possible. our work strictly follows the task definition and evaluation protocols (§ 5 and 6) of the original esc paper (liu et al., 2021), where the support is provided through social interactions (e.g., between peers or friends) rather than professional counseling. as mentioned in (liu et al., 2021), further efforts are still needed to probe the ethical extent to which dialogue models can or should provide support. these protocols should also not be used directly in fields other than the esc task (i.e., peer emotional support in daily life) that require the guidance of professional researchers, such as psychological counseling. we also ethically conducted the human evaluation. we transparently communicated with the participants of our study intent and explicitly informed them of the disclaimers before they participated. we paid the participants at the hourly wage above $10/hour, going well beyond the local labor compensation standard. we acknowledge that the results of human evaluation could be affected by the participants’ demographic and geographic characteristics. this work has obtained study approval from the institutional review board (irb)."
2244,"limitations the most evident limitation of this research is that is has only been demonstrated on english corefernce. using a lemma-based heuristic requires using a lemmatization algorithm in the preprocessing phase and for more morphologically complex languages, especially low-resourced ones, lemmatization technology is less well-developed and may not be a usable part of our pipeline. application to more morphologically-rich languages is among our planned research directions. in addition, all our experiments are performed on the gold standard mentions from ecb+ and gvc, meaning that coreference resolution is effectively independent of mention detection, and therefore we have no evidence how our method would fare in a pipeline where the two are coupled. a further limitation is that training of the crossencoders still requires intensive usage of gpu hardware (the gpu used for training longformer is particularly high-end)."
2245,"limitations in this study, we provide empirical evidence of the impact of domain gap in keyphrase tasks, and we propose effective methods to alleviate it. however, we acknowledge that this study is limited in the following aspects: (1) as the first study discussing domain adapation and few-shot results, there is few studies to refer to as fair baselines. nevertheless, we attempt to show the improvements of the proposed methods over base models by extensive experiments. (2) the pretrained keyphrase generation model can be used off-the-shelf, but the multi-stage adaptation pipeline might increase the engineering complexity in practice. (3) we have only explored three strategies for domain adaptation, and they all require generating hard pseudo labels in different ways. soft-labeling (liang et al., 2020) and knowledge distillation (zhou et al., 2021) methods are worth investigating. (4) we train a model with wikipedia annotation to predict pseudo keyphrases, and it would be interesting to see if we can use large language models (e.g. gpt-3 (brown et al., 2020)) to zero-shot predict phrases."
2246,"limitations our work is the first attempt to explore how evidential deep learning can be used to improve the reliability of current ner models. despite the improved performance and robustness, our work has limitations that may guide our future work. first, we propose a simple method to treat hard samples (such as outliers) in the dataset as oov/ood samples, enabling the model to detect oov/ood data with minimal cost. however, there is still a certain gap between these hard samples and the real oov/ood data. oov/ood detection performance can still be improved by further incorporating more real oov/ood samples, for example, real ood data from other domains, well-designed adversarial examples, generated oov samples by data augmentation techniques, etc. second, we evaluate the versatility of e-ner by applying it to mainstream ner paradigms. however, there are still other paradigms, such as hypergraph-based methods (lu and roth, 2015) and the w2ner (li et al., 2022) approach in recent work, that could be evaluated in the future."
2247,"limitations firstly, from the technical perspective, we have advocated the advantages of our proposed listwise loss for the mrhp task in terms of generalization capacity. nevertheless, there are other various listwise discrimination functions that may prove beneficial for the mrhp model training, for example neuralndcg (pobrotyn and białobrzeski, 2021), listmle (xia et al., 2008), etc. moreover, despite the novelty of our proposed gradient-boosted tree in partitioning product reviews into helpful and unhelpful groups, our method does not employ prior contrastive representation learning, whose objective is also to segregate helpful and unhelpful input reviews. the contrastive technique might discriminate reviews of distinctive helpfulness features to bring further performance gain to multimodal review helpfulness prediction. at the moment, we leave the exploration of different listwise discrimination functions and contrastive learning as our prospective future research direction. secondly, our study can be extended to other problems which involve ranking operations. for instance, in recommendation, there is a need to rank the items according to their appropriateness to present to the customers in a rational order. our gradient-boosted decision tree could divide items into corresponding partitions in order for us to recommend products to the customer from the highly appropriate partition to the less appropriate one. therefore, we will discover the applicability of our proposed architecture in such promising problem domain in our future work."
2248,"limitations. first, our method requires additional entity resources, which may be difficult to obtain for certain language pairs. with the development of multilingual entity datasets like paranames (sälevä and lignos, 2022), we are optimistic such resources will be more accessible in the near future. second, as demonstrated in section 5.4, extracting translation candidates increases inference time. due to space limitation, more limitations are discussed in appendix a.6. 15evaluated on a p40 gpu with batch size of 1, other experimental settings same as section 4"
2249,"limitations our machine translation experiments revealed that optimal generalization performance is obtained with small interval values. however, r-dangle with small intervals still runs much slower than an equivalent transformer model. despite our modifications, large r-dangle models with small intervals on large datasets remain computationally expensive. in this paper, we only explored a sim- ple periodic re-encoding strategy. however, more complex and flexible ways of re-encoding could be used to further improve computational efficiency. for instance, we could adopt a dynamic strategy which learns when re-encoding is necessary."
2250,"limitations in this work, we proposed sstuning for zero-shot text classification tasks. during inference, we may need to design verbalizers even though we can use templates like ""this text is about [label name]"". for simplicity and fair comparison, we only refer to previous works for such designs, which may be sub-optimal. as shown in table 4, using the verbalizers ""terrible."" and ""great."" work better than ""it’s terrible."" and ""it’s great."" for the sst-2 and imda tasks that we reported in the main results. if the labeled validation set is provided, the model may perform better by choosing verbalizers based on the validation set. due to limited computation resources, we only tuned the model with 5.12 million samples, which is only a small portion of the available samples. we believe that tuning the model on a larger dataset help improve the performance. even though the computational cost will also increase, it is worth it since no more training is needed at the inference phase. in addition, we did not do extensive hyperparameter searches except for the learning rate, which may further improve the performance. in our experiment, we only tested the method with discriminative models like roberta and albert. its performance with generative models is not known. it is non-trivial to test on such models since generative models can do both natural language understanding tasks and natural language generation tasks. we leave this as future work."
2251,"limitations the limitation of our study is that we only evaluated our model on a limited set of spoken language transcripts. we believe that additional attention should be given to features specific to ad patients, such as pauses and filler words in speech. furthermore, the lack of diversity in the data may also adversely affect the model’s performance on unseen samples. our model would benefit from further testing on a wider range of data, including different languages and different modalities, to see if it is capable of generalizing to other domains in the future."
2252,"limitation to this work, which is related to time complexity and speed performance. since every instance is transformed into multiple qa instances, it may take a relatively long time to process a document. 4qualitative analysis is provided in appendix c. limitations there are two primary limitations of the system presented in this work. first, each set of questions we use for training the qa model is designed specifically for the dataset we trained our model on. while we provide a set of questions for each of the two common trc datasets, we believe that training the model on other datasets may require rewrite of the questions. second, as mentioned in the previous section, every trc instance is converted into multiple qa instances which we then process individually. this may increase the overall inference time and pose a practical limitation which needs to be carefully considered."
2253,"limitations • we are unable to test the proposed model’s performance on other datasets due to the unavailability of public multi-reference headline generation datasets. • our dataset is created over a period of 6 months and contains around 3000 examples. although there are several commonly used benchmark datasets with a similar number of examples: e.g., r4c reading comprehension dataset (6.4k examples) (inoue et al., 2020), fire-lid (3357 examples), iiithner (3084 examples) datasets in gluecos benchmark (khanuja et al., 2020), wnli (634 examples), rte (2500 examples) and mrpc (3700 examples) datasets in glue benchmark (wang et al., 2018), nope corpus (around 2.7k examples) (parrish et al., 2021), we believe that it will be better to have a larger dataset for this challenging task. we plan to create a larger version of the dataset in future work."
2254,"limitations we reflect on the limitations of our model below: 1. our experiments are based on large everyday household datasets (i.e. coin and crosstask). our language model is pretrained with web data, which helps it handle such householdrelated procedures well. however, when applied to other more specialized domains like medical procedures, language models might suffer from the domain gap and impact overall model performance. 2. the language model has excellent planning ability given the ground truth start and goal steps. however, it is still hard for the language model to generate very long sequences of steps. when the planning horizon t increases, the performance of our model drops quickly just as other methods do. 3. in real-world applications (i.e planning task for robots), a good model should be able to dynamically adjust the plan given external feedback. for example, when the execution of one step fails, the model will need to re-plan as soon as possible. our model does not possess such an ability so far, since our planning approach is offline. we leave this direction for future research."
2255,"limitations there are several limitations that can be considered for future improvements: (1) in multimodal alignment and fusion, we only consider a single image for each sample, whereas multiple images can be available. a more flexible visual encoding architecture that can digest an indefinite number of input images can improve the visual information coverage; 2) the empirical results in this work focus on three attribute extraction datasets (i.e., item form, color, and pattern) that can clearly benefit from visual perspectives, while there are also various attribute types that rely more on the textual input. different traits of attributes may influence the preferred modalities during the modeling, which is out of scope for this work but serves as a natural extension of this study; 3) currently there is no specific design to improve the efficiency based on the visual question answering architecture. it can be not scalable as the number of attributes increases. there could be a dual-use regarding the attentionpruning mechanism, which can be a potential risk of this work that could arise and harm the result. the attention-pruning mechanism encourages the model to focus on the task-relevant foreground on the given image selected with category supervision, which can improve the prediction precision given the input image is visually rich and contains noisy context background. while for some types of images, such as infographics, there may be helpful text information on the images or intentionally attached by providers. these additional texts may be overlooked by the attention-pruning mechanism, resulting in potential information losses. a possible mitigation strategy is to add an ocr component along with the visual encoder to extract potential text information from given images."
2256,"limitations the main target of this paper is to utilize structural knowledge for cross-lingual comprehension. we present a new pre-training task named scp in the hope of bridging the misalignment of structural words in the parallel corpus. more generally, we expect the proposed method can facilitate the research of cross-lingual understanding. admittedly, the main limitation of this work is that we rely on off-the-shelf tools to extract and align words in different languages, which would result in some mistakes at some situations. for example, giza++ only achieves 80%-85% accuracy in aligning the corresponding words in another language. currently, no tech can achieve this goal in 100% accuracy. as a result, some bias data in pre-training calls for further research and consideration when utilizing this work to build xplms."
2257,"limitations our work has the following limitations: first, we cannot generalize our"
2258,"limitation scaling through the inference corpus. the size of the reference corpus is an additional dimension for model scale in nonparametric models. in this paper, we scale the corpus up to nearly 1b tokens, which is still smaller than the training data of very large language models (brown et al., 2020; rae et al., 2021). we think future work can scale it fur- ther using tools such as distributed faiss (johnson et al., 2019) or scann (guo et al., 2020). significant memory usage. using npm saves gpu compute and memory compared to using models with more parameters. however, npm requires more ram and disk memory due to embeddings of a reference corpus. for instance, the largest corpus in our experiments (full english wikipedia) requires 70gb of ram and 1.4tb of disk memory. future work can build more efficient npm as done in prior work in nearest neighbor search (jegou et al., 2010; norouzi et al., 2012; ge et al., 2014; izacard et al., 2020; yamada et al., 2021). exploration of larger vocabulary. large vocabulary is known to lead performance gains (conneau et al., 2020) but is bounded in memory costs. previous work explored more efficient softmax approximations (morin and bengio, 2005; chen et al., 2016; grave et al., 2017). our nonparametric training offers an alternative by removing the softmax over the vocabulary. with the roberta architecture, increasing the vocab size by 2x makes the baseline training 50% more memory expensive, but does not increase the memory in training npm. however, this paper does not include more systematic evaluation on the effect of large vocabulary. future work can explore training npm with a significantly larger vocabulary to further boost performance. extension for generation. our paper evaluates npm only on prediction tasks. it is currently nontrivial to use npm for generation, since it is the encoder-only model. future work can explore autoregressive generation as done in patel et al. (2022) or use npm for editing (schick et al., 2022; gao et al., 2022). extension to few-shot learning and fine-tuning. our paper focuses on zero-shot evaluation only. future work can extend npm to a few-shot learning setup. in fact, fine-tuning npm is significantly easier than fine-tuning larger models such as t5, opt and gpt-3 which we compare npm with, and can be explored in future work. better cross-lingual transfer. our work explored cross-lingual transfer in a limited setup where the model is trained on monolingual data. we think future work can train multilingual npm, and explore more comprehensive cross-lingual evaluation. in fact, nonparametric training may alleviate the burden of collecting large-scale multilingual corpora since it makes the model less sensitive to the language coverage in the training data, and may lead to significantly better cross-lingual transfer, as we demonstrate in the entity translation task. limitation in speed. we find that search makes inference considerably slower than the counterpart without search. we think that (1) search can significantly be faster with better engineering (we use the default hyperparameters of the faiss index with no tuning) or better index, and (2) the speed of npm is still on par with the speed of significantly larger parametric models that npm outperforms (see table 4). moreover, while not explored in this work, there has been work that improves inference speed (he et al., 2021; alon et al., 2022) that can be applied to npm. we leave improving inference speed to future work."
2259,"limitation of the moraldirection is that it is induced on individual words, and thus longer sentences are a significant challenge for the models. still, we were able to test them on parallel subtitles data, which contains slightly longer, but predominantly still short, sentences. problems that showed up repeatedly in this experiment were an over-reliance on key lexical items and a failure to understand compositional phrases, particularly negation. additionally, typical problems of pmlms, such as disambiguation problems across multiple languages, were noticeable within xlm-r. non-english languages appeared more affected by such issues, despite the fact that all our target languages are relatively high resource. (3) moral foundations questionnaire. our experiments with the mfq reinforce the"
2260,"limitations in this work we present a novel method based on representation augmentation to solve cross-lingual transfer learning for event detection (ed). although our experiments demonstrate the effectiveness of the proposed method, there are still some limitations that can be improved in future work. first, our current method only leverages sentencelevel context in input document to perform ed over different languages. this might not be optimal as document-level context has been shown to be helpful for ed (pouran ben veyseh et al., 2021b) that can be explored in future research to improve our cross-lingual models. second, the evaluation for our model is limited to only three popular languages (english, chinese, and arabic) that are supported by existing pre-trained language models, unlabeled data, and text processing tools. as such, it is unclear whether the method can be adapted to many other languages with limited access to such resources (e.g., low-resource languages). we believe this is an important direction that can be investigated in future work to advance our understanding for ed models. finally, our method requires joint training with a retrieval model (based on multilingual pre-trained language models) that can impose additional computational costs (as shown in table 3). reducing necessary computational costs for our model is an important direction to make it more accessible for different applications and domains."
2261,"limitations to better enlighten the follow-up research, we conclude the limitations of our method as follows: 1) although the method we proposed can help improve the quality of generated labels, there is still room for further improvement; 2) because our detection is not perfect, it will lead to inaccurate labels of some samples. we look forward to better methods to improve detection in the future; 3) this work has verified that the extended labels can effectively help the performance of models and proposed a method of label extension, but has not tried other extension methods or whether it is helpful to extend more labels. 4) this work focuses on solving open environment intent prediction with different generative models, without exploring other types of models."
2262,"limitations our approaches mainly leverage a fixed feature extractor together with a set of individually trained classifiers to mitigate catastrophic forgetting whereas a tunable feature extractor may also be helpful and complement the individually trained classifiers, so a future direction is to design advanced strategies to efficiently tune the feature extractor in combination with our proposed ice based classifiers. in addition, we mainly investigate the classifier drift and demonstrate the effectiveness of our solutions under the class-incremental continual learning setting. another future direction is to explore similar ideas under other continual learning settings, e.g., task-incremental learning, online learning, or the setting where new sessions also contain annotations for old classes."
2263,"limitations in each contribution of this work, we can isolate several potential limitations. in creating c-xnli, the mt model for the formation of the train set was chosen based on the results from a single dataset. additionally, an assumption that the model is plagued with typical issues that affect mt models was investigated on a small dataset. although we are skeptical of the mt model’s performance and perform qe scoring of the small dataset by a group of annotators and analysis to ascertain its performance, we are only comparing croatian machinetranslation results to results from a single language (german), assuming that results would hold for other high-resource languages. also, for some mt evaluations, we use a single metric (bleu) known to have many problems but only generally considered to correlate with human judgment. our hyperparameter optimizations are of limited scope. all hyperparameters are fixed, except the learning rate with four possible values we search over. furthermore, we only used three seeds. we could not perfectly reproduce the results outlined in the paper of our baseline xlm-r base model, partly due to a lack of elucidation in the original 4cc by-nc 4.0 5https://github.com/lobadic/c-xnli paper and partly due to limited hyperparameter optimizations. finally, we do not elucidate further and experiment with the discovered correlation between the models’ performance and the overlap in datasets, and we leave it for future work."
2264,"limitations, resulting in a reduced impact on the environment. we finetuned nine models on program and function translation tasks and due to the smaller size of the training data, all jobs took a total of 1–2 days on rtx 2080 ti gpus. a total of 100 hours of training in a single rtx 2080 ti gpu results in approximately 7.5kg of carbon emission into the environment.10 sensitive information avatar composed of parallel programs and functions that do not have any natural language (nl) comments or docstring. we remove them to get rid of any personally identifiable information or offensive content. however, there could still be such content in the form of string as we do not manually check each example."
2265,"limitations our study revealed considerable differences in transferability and other measures we considered across different datasets. nonetheless, the study focused on the differences in transferability arising from the choice of the models and the al methods rather than the dataset. to eliminate confounding due to datasets, we grouped the results by datasets and analyzed each group separately. despite this, the scope of our results is limited by the fact that all datasets used are in english and possibly contain their own biases. even though we showed that it could still be useful to transfer actively acquired datasets between transformer-based plms, it is important to keep in mind that actively acquired datasets are not representative of the original data distribution due to the sampling bias introduced by active learning."
2266,"limitations regarding our work, a few limitations should be mentioned. during the annotation process, conflicts between annotations occurred. all conflicts were discussed with a senior researcher and resolved in this way. thus, we reached the best possible agreements, but still some agreements are lower than others (see table 2). for example, legal claims and premises have a relatively large room for interpretation. perfect results can only be expected by over-anchoring the annotators and weakening the guideline, which we have consciously avoided in our research. the comparison of the iaa with other works in the field of nlp from legal science is not possible, because the works either do not examine the components of the appraisal style or identify the components of the judgment style without the indication of the iaas (urchs et al., 2020). compared to works that also annotated premises (kripp. α = 51.08%) and claims (kripp. α = 55.49%) in business pitches (wambsganss and niklaus, 2022), our work provides comparable results with respect to the agreement of the krippendorff α (premise = 55.89%, legal claim = 45.02%). further work shows similar results α = 44.1% (park and cardie, 2018). all in all, we can assume that both our components and our mounted relationships, achieve comparable or better results than comparable works (e.g., park and cardie (2018)). although our model shows accurate values between 78% and 92% for predicting the components of the appraisal style, the values for determining legal claims and premises are lower (62% and 78%) compared to the other values. however, they display reasonable values when compared to previous nlp studies. we can only compare our work to other related work in another domain because values for detecting legal claims and premises are not available in the nlp literature. for instance wambsganss and niklaus (2022), present an accuracy of 54.12% for their long short-term memory (lstm) model which detects claims and premises. with the mentioned model the authors shows positive outcomes in supporting students’ argumentative skills. our models show similar or higher precision in comparison to the works of poudyal et al. (2020) or wambsganss and niklaus (2022) (see table 7 in the appendix a) and our post-test results also show significant learning outcomes (see table 11 in the appendix a). although we can show a significant learning output, it must be noted that this is only short term. as a result, we intend to carry out additional field experiments in the future to establish the system’s effectiveness over a more extended period and demonstrate long-term success. as a third possible limitation, our models are limited to applying the appraisal style in german only. in the future, further efforts have to be made to investigate the transfer-ability or adaptation of our models to other countries with other legal systems and other languages. however, we assume that this is possible in principle, since some countries such as china now use the appraisal style in law teaching (man, 2022) and countries such as the u.s. use at least similar approaches such as learning with case studies using the irac formula (metzler, 2002). nevertheless, some adaptation of the models is needed, since the language and the legal form in each country have their own specificities."
2267,"limitations we build a new benchmark for syllogistic reasoning. the limitations are mainly in the experiments part: (1) due to the limited human resources, our test set is quite small, which may not support training large models directly. (2) we evaluate all models by comparing their predictions with the groundtruth"
2268,"limitations more work is needed to uncover the causes of the inconsistent performance across randomly initialized models in experiment 1. although the bias toward forward function application implemented in experiment 2 was effective in our experiments, it is unlikely to work as a general-purpose method, since languages vary in their branching characteristics and in the contexts in which they apply forward and backward function application."
2269,"limitations in our current experiments, prompt-based methods are primarily storage-efficient or parameterefficient solutions. since these methods all require backpropagation to the bottom layer, the training time of prompt-based methods are closely resembles that of traditional fine-tuning approach."
2270,"limitations considering that english is the most widely spoken language, we select it as the high-resource monolingual language in this study. while across is a general summarization framework not limited to a certain target language, it deserves an in-depth exploration of how across works on other highresource languages. additionally, we employ mt5 as our backbone because it supports most languages in crosssum. the performance of across after replacing mt5 with other models, such as mbart(liu et al., 2020), flan-t5(chung et al., 2022), will be investigated in the future. ethical consideration controversial generation content. our model is less likely to generate controversial content(e.g., discrimination, criticism, and antagonism) since the model is trained on a dataset from the bbc news domain. data in the news domain is often scrutinized before being published, and thus the model is not likely to generate controversial data. desensitization of user data. we use the amazon mechanical turk crowdsourcing platform to evaluate three artificial indicators (i.e., fluency, informativeness, and conciseness). for investigators, all sensitive user data is desensitized by the platform. therefore, we also do not have access to sensitive user information."
2271,"limitations in this paper, we propose the fine-purifying approach to purify fine-tuned pre-trained language models (plms) by detecting poisonous dimensions and mitigating backdoors or bias contained in these poisonous dimensions. to detect poisonous dimensions in fine-tuned plms, we utilize the diffusion theory to study the fine-tuning dynamics and find potential poisonous dimensions with abnormal finetuning dynamics. however, the validity of our approach relies on assumptions that (1) backdoors or biases are injected during the fine-tuning process of plms; and (2) the fine-tuning process can be modeled as a diffusion process. therefore, in cases where the assumptions do not hold, our approach cannot purify the fine-tuned plms. for example, (1) backdoors or biases are contained in the initial plm weights rather than being injected during the fine-tuning process; or (2) the fine-tuning process involves non-gradient optimization, such as zero-order optimization or genetic optimization, and thus cannot be modeled as a diffusion process."
2272,"limitations we only consider 14 languages and 21 categories, whereas wikipedia has pages in more than 300 languages and 200 broad categories. increasing the scale and diversity will further improve method generalization. our proposed method relies on the good multilingual translation of key and value from table pairs. although we use key, value, and category together for better context, enhancement in table translation (minhas et al., 2022) will benefit our approach. because our rule-based system requires manual intervention, it has automation limits. upgrading to completely automated methods based on a large language model may be advantageous. we are only considering updates for semi-structured tables. however, updating other page elements, such as images and article text, could also be considered. although a direct expansion of our method to a multi-modal setting is complex (suzuki et al., 2012)."
2273,"limitations while our research and empirical results support specific evaluation metrics for the task of clinical note generation according to a given evaluation criteria, more results, including testing on additional datasets are needed to further validate these findings. our manual annotations followed clear and structured guidelines, but could still contain some level of annotator bias and have an average pearson inter-annotator-agreement of 0.67 (tables 7 and 8)."
2274,"limitation one limitation of styleap is that one extra inference is needed for retrieval. it is mainly due to the monolingual retrieval accuracy is higher than that of crosslingual retrieval (refer to section 6.1). in the future, we will try stronger multilingual model to mitigate this effect."
2275,"limitations although our method achieves state-of-the-art performance consistently on the four benchmark datasets, it suffers from the following limitations: • no optimization for the verbalizer. the verbalizer we use in the prompting stage is just a simple 1-to-1 mapping, this simple design does not fully exploit the capabilities of mlm. • no explicit modeling of the relationship information between nested entities. we consider that in some other scenarios, the relationship information between nested entities is not very significant. consequently, explicitly modeling the relationship may introduce new biases. so we just utilize the potential information. but in practice, it is worth exploring how to model such a relationship from a novel perspective."
2276,"limitations this paper introduces the problem of few-shot novel product title generation to efficiently and accurately generate informative and appealing titles for novel products with limited labeled data. however, the training of our proposed model relies on the paired image-attribute-title data, which may not be easily obtained simultaneously in the real world. therefore, our model may not work well when high-quality image data or textual profile is missing. the limitations could be alleviated using techniques such as knowledge distillation or self-training. besides, the writing styles of the generated titles are highly correlated with the training data. hence, it requires specific and appropriate treatment by experienced practitioners, when deploying new products online."
2277,"limitations despite the strong performance on the presented datasets, our approach is limited in its ability to update knowledge state and adapt to new domains. a major feature of retrieve-then-read is the ability to swap in new documents when new information is learned, such as temporally more recent documents, or adding in documents from a new domain to quickly adapt to a new downstream task. our approach relies on a large language model to contain all this knowledge and adding new knowledge would likely require some retraining. in addition, large generation models still suffer from hallucination errors, resulting in incorrect predictions. when tasked with generating 10 urls, llm-url may only generate 6 or 7 which link to valid documents. finally, our approach involves very large language models, slow web requests, and document processing which may make it cumbersome to use in practice."
2278,"limitation first, there are studies (wu et al., 2021) claiming visual information only serves as regularization. in our ablation study, we find the adversarial setting of fusion-based approach outperforms the plain transformer. combined with observations from previous studies, we suggest that fusion-based architectures may apply some images information as regularization terms, yet the further quantitative analysis is needed to confirm this phenomenon. second, though our testset is carefully selected to ensure the textual ambiguity without image data, we encounter difficulties in designing a suitable metric for quantifying the degree to which the models are able to resolve the ambiguity. specifically, we find that conventional metrics, such as wordlevel entity translation accuracy, exhibit significant fluctuations and do not effectively quantify the extent to which the model effectively resolves ambiguity. we discuss this metric in more details in the appendix, and offer a glossary of ambiguous words used in the test set. we acknowledge that the evaluation of multimodal ambiguity remains an open problem and an area for future research. in addition, there are some details regarding the dataset that we need to clarify: the dataset is collected after covid-19, so some commodities will be associated with the pandemic. we collect data by category in order to cover various products to reduce the impact of the epidemic on product types."
2279,"limitations described in prior work. gpt-3 achieved near perfect performance on this new test set. we then investigated the task of noun compound conceptualization (ncc). ncc evaluates the capacity of plms to interpret the meaning of new ncs. we showed that gpt-3 still performs reasonably well, but its success can largely be attributed to copying definitions or parts of definitions from its training corpus."
2280,"limitations we see several limitations regarding our work. first, we focus on documents in the english language only, neglecting many caribbean newspapers and islands with other official languages. while some of our methods can be easily extended to non-english material (e.g. weat analysis), methods that rely on the pre-trained english model f-coref (i.e. pmi, lexicon-based analysis) can not. on the same note, f-coref and spacy were developed and trained using modern corpora, and their capabilities when applied to the noisy historical newspapers dataset, are noticeably lower compared to modern texts. contributing to this issue is the unique, sometimes archaic language in which the newspapers were written. while we validate f-coref performance on a random sample (§5.2), this is a significant limitation of our work. similarly, increased attention is required to adapt the keyword sets used by our methods to historical settings. moreover, our historical newspaper dataset is inherently imbalanced and skewed. as can be seen in tab 2 and fig 8, there is an over-representation of a handful of specific islands and time periods. while it is likely that in different regions and periods, less source material survived to modern times, part of the imbalance (e.g. the prevalence of the us virgin islands) can also be attributed to current research funding and policies.12 compounding this further, minority groups are traditionally under-represented in news sources. this introduces noise and imbalance into our results, which rely on a large amount of textual material referring to each attribute on the gender/race plane that we analyse. relating to that, our keyword-based method of classifying entities into groups corresponding to the gender and race axes is limited. while we devise a specialised keyword set targeting the attributes female, male and non-white, we classify an entity into the white group if it was not classified as non-white. this discrepancy is likely to introduce noise into our evaluation, as can also be observed in tab 7. this tendency may be intensified by the nlp systems that we use, as many tend to perform worse on gender- and race-minority groups (field et al., 2021). finally, in this work, we explore intersectional bias only along the race and gender axes. thus, we neglect the effects of other confounding factors (e.g. societal position, occupation) that affect asymmetries in language. ethical considerations studying historical texts from the era of colonisation and slavery poses ethical issues to historians and computer scientists alike since vulnerable groups still suffer the consequences of this history in the present. indeed, racist and sexist language is not only a historical artefact of bygone days but has a real impact on people’s lives (alim et al., 2020). we note that the newspapers we consider for this analysis were written foremost by the european 12the danish government has recently funded a campaign for the digitisation of historical newspapers published in the danish colonies; https://stcroixsource.com/20 17/03/01/. oppressors. moreover, only a limited number of affluent people (white males) could afford to place advertisements in those newspapers (which constitute a large portion of the raw material). this skews our study toward language used by privileged individuals and their perceptions. this work aims to investigate racial and gender biases, as well as their intersection. both race and gender are considered social constructs and can encompass a range of perspectives, including one’s reflected, observed, or self-perceived identity. in this paper, we classify entities as observed by the author of an article and infer their gender and race based on the pronouns and descriptors used in relation to this entity. we follow this approach in an absence of explicit demographic information. however, we warn that this method poses a risk of misclassification. although the people referred to in the newspapers are no longer among the living, we should be considerate when conducting studies addressing vulnerable groups. finally, we use the mutually exclusive white and non-white race categories as well as male and female gender categories. we acknowledge that these groupings do not fully capture the nuanced nature of bias. this decision was made due to limited data discussing minorities in our corpus. while gender identities beyond the binary are unlikely to be found in the historical newspapers from the 18th-19th century, future work will aim to explore a wider range of racial identities."
2281,"limitations our work is limited by several factors. first, our findings are supported only by experiments on a single nlp task (neural text simplification). we selected this task because it offered an intriguing sandbox for studying varying experimental conditions, ranging from differences in random seeds to modifications in compile-time and run-time environments and dependency versions. comparing the multifaceted outcomes arising from these experiments facilitated greater quantified estimations of the degree of reproducibility for the selected nts systems. however, the dimensions of variation that we explored in this work are common to many nlp tasks; none are unique only to text simplification. because of this, we believe that our findings would generalise broadly across nlp tasks. we used a single data set, the same as in the original paper by nisioi et al. (2017), to foster controlled study of our other experimental variables. the data set comprises aligned sentences between english wikipedia and simple english wikipedia. thus, it is unclear whether our findings would be similar if the study was conducted using data from other languages, including those with richer morphology such as czech or arabic. finally, although we conducted a robust set of experiments for the selected models across two research groups, our experiments are limited to a small set of nts models due to the extensive set of conditions tested for each model. although these models vary in their architecture, we do not know if other nts models may be more or less stable across experimental conditions. taken together, the limitations accompanying our findings suggest compelling avenues for future research."
2282,"limitations the main limitation of the proposed framework is its dependency on a reasonable amount of real implicit hate instances to be used as the prompting input material. obtaining implicit and subtle messages from social media is undoubtedly a challenging and time consuming task. more importantly, another limitation lies in the fact that the proposed framework does not rely on an automatic metric to determine if the generated messages are actually implicit. therefore, a human-in-the-loop step for validating the obtained newly generated instances is still required. additionally, there has been mounting pressure to obtain debiased plms, which might lead to the generation of less challenging examples."
2283,"limitations the main downside of ssmt (compared to presegmentation models like bpe and ulm) is its computational complexity. our architecture (figure 1) introduces additional computation in 2 way. firstly, the decoder conditions on the characterlevel history of the target sentence, so it has to process more tokens than a standard subword decoder. secondly, the dynamic programming algorithm (equation 5) requires more computations than standard mt models training on pre-segmented datasets. in practice, ssmt takes an order of magnitude (10×) longer to train than models training on a pre-segmented dataset. dynamic decoding also adds computational complexity to testing, although this is less of an issue since test set sizes usually permit run times within a few hours. it would depend on the practitioner to decide whether the performance boosts obtained by ssmt justify the longer training and decoding times. however, since ssmt is particularly strong for data scarce translation, the computational complexity might be less of an issue. for translation directions like english to swati, training times are quite short for all models (less than a day for ssmt on subpartitions of the a100 gpu), so the increased training times are manageable."
2284,"limitations even though our proposed methodology, tgtss, was able to significantly reduce model instability, there is still a gap in performance with the gold standard ensembling techniques. more work needs to be done to bridge this gap. in our empirical analysis, we used two open source datasets, massive and clinc150. both these datasets are small and may not represent the complexity in real world production datasets which may contain substantially large noise. in our proposed methodology, we train a pair of models successively, a teacher and a student, which is significantly better than ensembling in terms of computational cost. however, this setup may still be challenging in many sophisticated real world production nlu systems. more work needs to be done to reduce the computational complexity of training and inference for these systems."
2285,"limitations we analyze the limitations of this study from the following perspectives: • the aste task extracts the sentiment triplets from a review, while the aspect sentiment quad prediction (asqp) task adds an aspect category based on the triplets and provides more comprehensive information. defining the aspect category for each domain is also hard work. future work can take the aspect category into consideration. • all the models are evaluated by f1 score, in which only exact matching can be considered correct. this metric can not differentiate between partially matching and completely mismatching and is not the best choice for a challenging dataset like dmaste. future work can include some partially matching metrics for this task. • there is no specifically designed method for cross-domain aste. but we analyze the challenges of this task in detail. we are planning to design a new method for cross-domain aste based on the analysis results."
2286,"limitations there exist so many different transformer models and efficiency methods that it is extremely difficult to conduct exhaustive experiments for all of them. although our experiments demonstrate nice properties for efficiency operators, the observations are restricted to our experimental setup. considering the huge space of all combinations of transformer models, efficiency methods, and datasets, our experiments provide understanding for an important but small subspace, and it is possible that the"
2287,"limitations of scaling up data and model sizes, we hope that it will pave the way for the arabic nlp community to focus on problems that are beyond the reach of plm scaling."
2288,"limitations our study is limited in scope, studying only classification and extractive qa tasks in english; the trends we highlight in this work might not generalize to different tasks or other languages. we also acknowledge that we only use bert-based models for our analysis, so it is uncertain whether these findings are applicable to other models. in addition, the overlap we describe in this paper is defined by semantic similarity rather than literal overlap between sentences and phrases. we are not claiming that this overlap is good or bad, rather we show that when the overlap is large, it is more difficult to evaluate model generalization. we note that there are multiple confounding factors in our results. first, while we highlight the role of dataset collection method in our analysis, the naturalness of data collection method is negatively correlated with task difficulty (i.e., the more natural datasets we study are also the least difficult). as a result, differences in performance can be attributed to task difficulty as well as data col- lection method. second, our study is limited in scope of similarity metrics (only cosine similarity) and embeddings used to compute similarity. using different embedding or metric can change the results."
2289,"limitations the main drawback of the work is in its evaluation, which was performed on datasets which were not manually annotated for the task, but adapted to it in various means. while we believe these evaluation sets do provide a strong indication regarding task performance, evaluating on bespoke data explicitly annotated for the task is usually preferable. another limitation is language specificity: the work currently focuses on english, without considering other languages, which are also left for future work."
2290,"limitations the numbers in this survey are limited to papers published in the acl anthology and isca proceedings. however, we also included papers as related work from other resources if they are publicly available and accessible. in addition, the category in the survey does not include the code-switching type (i.e., intra-sentential, inter-sentential, etc.) since some papers do not provide such information."
2291,"limitations one limitation of this work is that while our approach alleviates the requirement of persona description during inference, it still requires persona description for the training corpus. a viable solution is to transfer the pre-trained persona detection models to other datasets without persona description in train set. however, the success of this approach may depend on the degree of similarity between the target dataset and the personachat dataset."
2292,"limitations in this section, we enumerate a few limitations of our work: • we believe that the need to train transformer architectures on gpu is an obstacle to the use of this pipeline, which is destined not to be used in an academic environment but by legal practitioners. • because of the specificity of each jurisdiction, generalizing to other countries may not be possible on all labels with the exact same models (for example in extracting the names of tribunals). • the manual annotation process is a weakness: while it results in gold-standard annotations, it is very time-consuming. we do acknowledge that the amount of training data presented in this work is low and that collecting more annotations in the future would improve the quality of the results. we think it would be interesting to look at self-supervised methods, weak supervision, and annotation generation. the need for labeled data also prevents easy replication of the pipeline to new data sets, which would also require manually annotating. • more precisely on the extracted categories, some categories lack precision and would require additional processing steps to achieve satisfactory results. for example, the category person sometimes refers to the claimant or their family, but sometimes refers to the name of the judge."
2293,"limitations in this paper, we indicated that the vanishing gradient problem, caused by layer normalizations, makes the training of deep post-ln transformers unstable. we proposed the b2t connection to mitigate this vanishing gradient problem. however, the proposed b2t connection does not perfectly prevent the vanishing gradient, as shown in figure 3. therefore, the vanishing gradient might harm the training in extremely deep transformers even if our b2t connection is used. in addition, this study depends on empirical observations. in particular, we provided little theoretical justification of the reason for post-ln outperforming pre-ln when training succeeds. however, as discussed in appendix c, the method with a theoretical justification often collapses in some situations. because the behavior of deep transformers in various situations is not fully understood, we believe that it is important to provide empirical findings for our research field to progress. although appendix c includes a comparison between our b2t connection and the latest method, deepnet (wang et al., 2022), we could not investigate the behavior of all methods in the 100l-100l configuration because of our limited computational budgets. however, we are confident that we conducted sufficient experiments to verify our contributions."
2294,"limitations this paper does not utilize any major linguistic theories of code-switching, such as (belazi et al., 1994; myers-scotton, 1997; poplack, 2013). our approach to generating code-switched texts replaces words with their synonyms in target languages, looked up in a bilingual lexicon. furthermore, we do not make any special efforts to resolve word sense or part-of-speech ambiguity. to this end, the resulting sentences may appear implausible and incoherent."
2295,"limitations although the proposed ckdst distills the knowledge of mt more comprehensively and efficiently from encoder representations and prediction logits, and obtains significant improvements over previous methods, it still has limitations: (1) the batch size is not very large, limited by the memory capacity of the used hardware and the extremely long sequence length of speech inputs, which leads to a small number of negative samples used in ccrd and does not fully exploit the ability of contrastive learning. in future work, we attempt to expand the negative sample size using a mechanism like memory bank (he et al., 2020). (2) as we distill knowledge from mt to st, the performance of the pretrained mt model has an impact on our framework."
2296,"limitations as the contributions of this work include a framework and preliminary experimentation, there are a number of constraints that we leave to future work. firstly, we considered only one family of response distributions. we chose normal distributions because their behavior is well-understood and they are easy to work with. however, the structural similarities between normal distributions and the best performing metrics—namely, absolute error— suggests that, more generally, the best test metrics for nhst may vary depending on the underlying response distributions. therefore, we recommend that use of our framework should potentially vary depending on the dataset being considered, and might have other distributions commonly found in model and gold standard items and responses, such as exponential or multinomial distributions. similarly, we only considered p-value estimators that are based on bootstrap sampling. implementation of our framework in future use would benefit from matching the estimator to the test metric. for instance, permutation tests are the most common way to estimate p-values for spearman correlation, and analytical tests such as student’s or macnemar’s, which are commonly used even when the underlying assumptions on which they are based are not likely to hold (as, we expect is the case here). as such, the sampling method could change based on which metric is best for the task/data."
2297,"limitations we expect that our cross-lingual models have learnt some coreference knowledge on the target languages and we conduct experiments on some languages in zero-shot settings. however, we do not get consistent and significant improvements compared to monolingual models. this should be further investigated which potentially helps languages with few or no coreference annotations. compared to monolingual models, our cross-lingual model improves the source-side coreference resolution but it requires almost two times gpu memory during training. thus, this model architecture imposes restrictions on using larger pretrained models given limited resources."
2298,"limitations besides the technical challenges discussed in section 4.4-4.5, limitations of this work also include the issue of data imbalances that some attributes may have imbalance distributions. for example, we may find significantly more profiles with the country of citizenship as united states than any other countries, which may have a negative impact on generalization, especially when the distributions of training and inference diverge. similarly, the distributional variances discussed in section 4.5 indicate that the prediction results for non-celebrity distributions should be carefully adjudicated. the degraded performances on low-resource attributes also indicate that the prediction results may be unreliable when performing inference on attributes without enough training data. in this paper, we assume that the attributes are already given. however, many wikidata attributes are not applicable to everyone. for example, attributes such as “position played on team” may be specific to athletes. therefore, it is also important to investigate how to automatically detect applicable attributes for certain users. in this work, we use at most 100 recent tweets and aggressively create training and inference examples between each attribute and those tweets. since we use sliding window on the collected tweets, involving more tweets in training or inference may significantly increase the time cost."
2299,"limitations our study has three limitations: • as reported in section 5.1, nli tasks significantly benefit from core while other tasks marginally do, or there is no benefit at all. we suppose that such a difference comes from characteristics of a task. however, we have not yet thoroughly explored which characteristics of a task attribute the performance gain. to solve this problem, we need a novel deep learning interpretation method to probe latent contexts of lm, or a thorough analysis on relationship between the pretraining objective and downstream tasks and how prompting bridges two distinct phases. we leave these research questions as our future work. • core does not work for cases where the train dataset has too long sequence texts. core requires multiple examples to be concatenated, so developers cannot benefit from core if a majority of concatenated examples from their dataset exceed the maximum sequence length of an lm. • we have not yet analyzed whether core is applicable to natural language generation (nlg) tasks. nlg is undoubtedly an important pillar in natural language processing research, along with nlu, with many interesting applications. we believe that the concept of context attuning and context filtering can be of help to major challenges in nlg, for example, controlled nlg. we plan to explore core on nlg tasks after this submission."
2300,"limitations our work is constrained into multi-choice question answering system and limited to common sense reasoning tasks, lacking more exploration in other reasoning tasks, e.g. arithmetic reasoning (cobbe et al., 2021; chen et al., 2021), conversational reasoning (chen et al., 2022) and symbolic reasoning (wei et al., 2022). we plan to leave these directions as future work."
2301,"limitations there are two limitations of this work. (1) the used corpus of argumentative microtexts contains only fully argumentative texts of moderate complexity. real-world argument texts do not always consist only of argumentative statements. however, the method could potentially be used on other argumentation annotation corpora as well; one of the main reasons for choosing the corpus was to have a parallel full version in a second language. another reason is the ability to match the edu and adu segmentations directly. (2) although the amount of training data is artificially doubled, it may not be enough to train models on the proportionally increased noise. we hope to investigate these directions in the future."
2302,"limitations our constructed dataset, fine, has limitations in terms of entity category balance. some categories have a lower number of online passages and less user attention, resulting in an unbalanced distribution of entities across categories. we aimed to simulate a real-world situation by sampling passages based on their click rates. but this may have contributed to the imbalance. additionally, our proposed method, softfine, is specifically designed for fine-grained chinese named entity recognition with hierarchical categories, and thus has its own limitations. the model is kept simple in structure, with most efforts focused on developing supervision methods, which result in more hyperparameters and require a grid search to find the optimal hyperparameters. this can be resource-intensive. however, it has fast inference speed that is comparable to the bert baseline in real-world applications. to address these limitations, future research could explore methods to automatically balance the loss ratio and dynamically score relevance between flattened hierarchical labels."
2303,"limitations of current synonym-based textual attack models, and stress the importance of context (both textual as well as multi-modal) to generate semantically coherent and grammatically fluent adversarial attacks, which are likely remain undetected. while the observed effects of visually-grounded interpretations in our human evaluation were relatively small, we do believe that it is an important future direction. for example, we expect improved results by using synonym substitution methods based on visuallygrounded word embeddings."
2304,"limitations as we mainly focus on conceptual knowledge captured in so-called tbox (terminological) axioms, the abox (assertional) axioms are not considered. abox axioms can capture situations for specific individuals (e.g., health status of a person) which could cause privacy issue and we would not expect lms to capture such knowledge. hence, dealing with abox axioms could require additional engineering for data preprocessing. ethical considerations in this work, we construct new datasets for the proposed subsumption inference (si) task from publicly available ontologies: schema.org, doid, foodon, and go, with their download links specified in section 4. the bimnli dataset is constructed from the existing open-source mnli dataset. we have confirmed that there is no privacy or license issue in all these datasets."
2305,"limitations this work is mainly dedicated to the curation of a new multilingual dataset for indic languages, many of which are low-resource languages. during data collection, we face several limitations that can potentially result in ethical concerns. some of the important ones are mentioned below: • our dataset contains only those articles written by dailyhunt’s partner publishers. this has the potential to result in a bias towards a particular narrative or ideology that can affect the representativeness and diversity of the dataset. • another limitation is the languages represented in vārta. out of 22 languages with official status in india, our dataset has only 13. there are 122 major languages spoken by at least 10,000 people and 159 other languages which are extremely low-resourced.14 none of these languages are represented in our dataset. • we do not perform any kind of debiasing on vārta. this means that societal and cultural biases may exist in the dataset, which can adversely affect the fairness and inclusivity of the models trained on it. 14https://en.wikipedia.org/wiki/ languages_of_india"
2306,"limitations the annotation of attribute categorization and subjective preferences may vary from person to person, influencing preference disambiguation results in the real world. we have tried to reduce bias by choosing categorization concepts and subjective preferences agreed upon by more than three annotators. besides, owing to time and funds constraints, we only manually paraphrase dialog flow in english. for this reason, the agent built on sure can just communicate in english. to overcome this limitation, we plan to annotate sure in multilanguage in the next stage."
2307,"limitations while our work covers a large number of languages, it is focused on a specific source and style of summaries. our experiments focus exclusively on the xlsum dataset (hasan et al., 2021) which is based on bbc articles where the opening sentence serves as a summary. it would be interesting to explore our methods on additional datasets and text generation tasks, e.g., where the summaries are longer, or there are multiple input documents."
2308,"limitations we acknowledge the following limitations of our work. limitations of newsdialogues. first, we only collect 1k human-to-human conversations with 14.6k utterances due to the high cost of the annotation process (section 4.2). this brings difficulties for the learning of news grounded dialogue generation. second, each conversation in newsdialogues is grounded on one news article, which may have limited knowledge for real-world applications. we leave the multi-article grounded setting for future work. third, as mentioned in section 4.1, the image information in the news article is neglected in this version, which requires further exploration. limitations of experiments. large language models (llm) have shown great few-shot learning ability and generation capacity on various tasks, e.g., gpt-3 (brown et al., 2020), opt-175b (zhang et al., 2022) and bloom-176b (scao et al., 2022) etc. it is important to investigate the performance of llm on newsdialogues, while this has been neglected in this work due to the limited computational resources. in addition, it is also valuable to investigate the performance of chatgpt4 on newsdialogues, and we leave this for our future work."
2309,"limitations although our tart-full model shows the effectiveness of instruction-tuning for retrieval, on some datasets tart-dual shows large performance degradation from its non-instruction-following counterpart. we hypothesize that a smaller model size (i.e., 110 million parameters) and limited interactions between query and document embeddings are the main factors. we conduct primarily experiments training larger dual-encoder models such as sgpt (muennighoff, 2022) on berri but still observe some notable performance drop on some datasets, which indicate only scaling up encoders may not significantly improve instructionfollowing retrieval systems. future work can study the better approach to train larger-scale dualencoder models as well as explore modeling architectures that enable rich interactions but are still more efficient than the cross-encoder, such as colbert-v2 (santhanam et al., 2022). retrieval tasks are excluded in prior work on instruction-following of llms. this work is the first to explore instruction tuning in the area of retrieval, and we annotate more than 100 instructions for approximately 40 tasks, and we demonstrate the effectiveness of the dataset scale in retrieval. yet, recent work (wang et al., 2022b; chung et al., 2022) show that scaling up the number of the training datasets improves llms’ ability to adapt to new task via instructions, and the current dataset scale might not be optimal. we open-source our instruction data and call for community efforts to collect more retrieval tasks and human-written instructions as in instruction-following for lms (wang et al., 2022b; bach et al., 2022), to investigate whether further increasing the number of the datasets lead to improvements. ethical considerations although instruction-tuning using many datasets enable better zero-shot transfer, tart does not always retrieve documents that perfectly align with users’ expectations. applying tart to safetycritical domains requires extra attention. berri includes approximately 40 tasks covering diverse domains. although the data has been automatically filtered, and we have examined the data, there may still be harmful or privacy-sensitive contents. we will release all of the data and preprocessing scripts for follow-up work to inspect those dataset issues and the effects of those data."
2310,"limitations those of our findings that are based on information obtained from authors are necessarily limited in that they do not reflect information that might have been obtained from authors who did not respond. moreover, we selected our initial set of papers via search with key phrase “human evaluation.” while this phrase is very commonly used to refer to non-automatic forms of evaluation, there is a chance that we may have missed papers because they used a different term."
2311,"limitations our work is predicated on hypothetical models of human cognition. these models are still under development by cognitive scientists and need to be validated in more realistic domains. our method assumes access to a simulation of the environment, which may be costly to construct in some domains. in general, instruction generation agents pose substantial risk to humans. previous studies have shown that humans can become overly reliant on ai instructions and commit disastrous mistakes (robinette et al., 2016). it is thus important for practitioners to comprehend the constraints of our experimental setting. our experiments take place in a coarse simulator of real-world indoor environments, which restricts the action and perception of the human listeners. due to the expensive cost and the large number of agent variants, our human evaluation remains limited in terms of population scale and diversity, and the comprehensiveness of the questionnaires. as each instruction is only evaluated by a single human, we have not investigated the variance of the interpretation of the same instruction among different humans. in addition, human evaluators may “guess” a path even if a part of the instruction is misleading or impossible to follow. hence, the path-similarity metrics may not reflect faithfully the quality of the instructions. nevertheless, results shown in table 4 of §a.5 indicates that instructions generated by our agents are almost as easy to interpret as those generated by humans. but again, these results are still subject to the constraints of our annotator population. to deploy our method, practitioners should carefully re-evaluate its safety and effectiveness in conditions that closely emulate the deployment conditions."
2312,"limitations we discuss the limitations of our work: • while the three-step masking is shown to be beneficial, masking (base+ott) followed by unmasking may introduce several redundant computations as a token masked in the first two steps might get unmasked in step 3. • when compared against the docogen baseline (calderon et al., 2022), iterative masking steps increase the time complexity of the domain obfuscation which leads to masking latency. moreover, as the domain classifier is a critical part of the domain obfuscation, the approach has extra memory footprints. • the proposed masking approach introduces two extra hyperparameters τ2 and τ3 on top of the hyperparameters introduced by docogen. while we identify them as a fixed scalar value working for all kinds of input, we posit that one can propose dynamic input or source domain adaptive thresholding. currently, we classify it as a limitation of the proposed work."
2313,"limitations in this paper. in this section, we talk about the prominent advantage and significant limitations of our method."
2314,"limitations in this work, we study the approach of using machine-translated text to train language models on a single target language (and language pair). our"
2315,"limitations in this paper, we propose the evaluation model umse which can be used to evaluate the summary quality in three typical scenarios. however, in the summarization task, different annotators have different writing styles, and there might exist more than one good summary for one document. moreover, there can be summaries that concentrate on different aspects of a document (e.g., describing the location and room of a hotel). in the future, we aim to incorporate more scenarios (e.g., multireferences and multi-aspects) into our unified evaluation method."
2316,"limitations the limitations of our approach exist mainly in two aspects. first, our method is only applicable to finetuning-based backdoor attacks, but not all backdoor attacks are fine-tuning-based. second, although our method can eliminate backdoors well, the computational cost of our method is much higher than that of standard fine-tuning, and needs to be improved in the future."
2317,"limitation, currently, we mainly evaluate mocl under the single-source cross-domain setting. we plan to further extend it to multi-source cross-domain settings. moreover, the interaction between named entity recognition and relation extraction can be considered to improve performance in the future. limitations we propose a sequence-level contrastive learningbased model-agnostic framework mocl to enhance entity type classification in cross-domain named entity recognition (ner). in the future, we would like to combine the different granularities of contrastive learning (i.e., token-level and sequencelevel) to learn generalized representation for further improving the capability of mocl. in addition, due to the hierarchical structure of entity types between the source domain and the target domain, it would also be beneficial to adopt non-euclidean space to represent words for better learning the relative hierarchical relationship between entities."
2318,"limitations our proposed work is dedicated to considering the noise in ds-ner, and our noise-specific analyses are all based on this task. therefore, if it were not for ds-ner task, our model would not necessarily be robust compared to other task-specific methods. also, our approach is based entirely on previous experimental settings in ds-ner, so we do not consider how to reduce noise from the distant supervision process, e.g., designing models to help the annotation process rather than learning to reduce noise from the distantly-supervised text. designing models to help the distant supervision process could be a direction for future study."
2319,"limitations we consider as profanities words that have highly offensive or vulgar connotations. we acknowledge that readers may have different sensibilities with respect to profanities. obscene words depend on different factors, such as culture, social or religious background, and more (hovy and yang, 2021). consequently, some words may be disturbing for a number of people, and should be obfuscated, while other readers may not have any issue with reading them. moreover, we should consider that there is typically a hierarchy of offense, whereby some words are more severe than others; for example, f*ck is often socially accepted while the n-word usually is not (sap et al., 2019)."
2320,"limitations we only experimented with one and three teacher models. training more teacher models and using them to predict on large datasets such as solid (rosenthal et al., 2021) would require more computing resources. furthermore, we did not train the teacher models on the augmented dataset for the same reason following recent research in kd (gajbhiye et al., 2021; sun et al., 2019). we only conducted the experiments in english. the non-availability of large-scale offensive language identification datasets such as solid (rosenthal et al., 2021) in languages other than english can be a challenge when expanding this kd research beyond english."
2321,"limitations although we sidestep the challenge of selecting a specific prompt template for experimentation by opting for widely-used templates from previously published works, it is worth noting that numerous effective prompt templates are available, and the experimental results obtained using these templates would also provide valuable insights into testing our proposed method. furthermore, while our method yields improvements, it is important to acknowledge that errors may exist in the rule-based automatic annotation of generic responses, which could potentially propagate to the learning of the diagonal parameter w ."
2322,"limitations limitation of model scale the benchmark only included the evaluation of moderate-size language models and did not experiment on large language models. we justify our reasons in section 4.6 and appendix e and include an evaluation of chatgpt in appendix e, showing that even human feedback fine-tuned large language models is far from perfect on xws-tc. however, we acknowledge that the current state of extremely weak supervision would be better understood and assessed if complete evaluations on state-of-the-art large language models, such as instruct-gpt (ouyang et al., 2022), palm (chowdhery et al., 2022), and chatgpt exist. while we lack the computational resources to perform such an evaluation, we hope this work can stimulate interest in xws-tc and complete the study. limitation of text classification another limitation is the scope of text classification. while prompt and seed methods have shown strong performances on text classification, this performance does not extend to other general classification tasks, such as natural language inference/entailment (zhao et al., 2022)."
2323,"limitations although the ability of in-context learning has been found for different architectures (e.g., transformer and lstm), we consider only transformer-based in-context learning in this paper because transformer is the current mainstream architecture of nlp. however, as for in-context learning itself, figuring out how it works for other architectures is also a meaningful problem, which we encourage to study in the future. as for the dual form we point out between transformer attention and gradient descent, we consider a relaxed form of linear attention for qualitative analysis. although the experimental results support our understanding well, the mechanism of standard transformer attention without approximation may be more complex and should be studied more clearly in the future. as for empirical experiments, our analysis needs to record a large number of intermediate results (e.g., attention output representations, and attention weights to query tokens and demonstration tokens) for thousands of validation examples. considering the storage space and computational cost of analysis, we only analyze gpt models with up to 2.7b parameters and leave larger models such as gpt 13b for future work. in addition, for the clarity of the problem definition and the convenience of experiments, our analysis is based on only classification tasks. although classification is a representative application of in-context learning, other tasks like multiple choice and open-ended generation are not considered in this paper and could be investigated in the future."
2324,"limitations increase in the number of dialogue turns the dsd dataset has a higher average number of turns compared to the sgd dataset. (20.44→21.21) this is a limitation in terms of completing a task with fewer dialogue turns, one of the objectives of the tod system (liu et al., 2018; tiwari et al., 2021). this is because dsd was created by extracting and augmenting the target turns of sgd. however, assuming that the tod agent trained with the dsd is applied to real-world scenarios, we expect that the agent will play a role in reducing the number of user rejections by expanding the range of choices to users through compare-based disambiguation."
2325,"limitations on training efficiency and inference efficiency. first, since metaretriever will first retrieve taskspecific knowledge and then make predictions in the inference phase, such a retrieve-then-extract manner will take longer inference time than nonretrieve methods unavoidably. we conduct experiments on the test set of conll04 dataset to compare overall inference time of metaretriever with uie. all hyper-parameters are set to be the same for a fair comparison. experimental results are shown in table 9 and we can find that metaretriever cost nearly twice time as uie to make predictions. as metaretriever works in a retrieve-than-extract manner, such a time cost is reasonable. second, our proposed meta-pretraining algorithm is based on bi-level optimization. in the pretraining phase, it needs to calculate high-order gradients to optimize parameters and calculating high-order gradient requires more time. therefore, it takes longer time to pretrain metaretriever. to illustrate the time cost, we conduct experiments on 10k instances to compare the pretraining time of metaretriever with simpleretriever which is pretrained without meta-pretraining algorithm. all hyper-parameters are set to be the same for a fair comparison. table 10 gives the results. from it, we can obverse that compared with simpleretriever, metaretriever takes about 1/4 longer time than simpleretriever. finally, we spent 56 hours to pretrain metaretriever on filtered pretraining corpus (6.9m instances). acl 2023 responsible nlp checklist"
2326,"limitations our work has several potential limitations. first, given the limited computational budget, we only validate our self-evolution learning on the large and base sizes. it will make our work more convincing if scaling the experiments up to the larger model size and training corpus. on the other hand, besides the improved commonsense knowledge learning ability, we believe that there are still other abilities, e.g., mathematical word problems, of plms that can be improved by our method, which are not fully explored in this work."
2327,"limitations and risks to avoid any misuse of queryform. although our proposed queryweb pre-training approach can effectively achieve knowledge transfer from publicly available webpages to form-like documents, it inevitably carries the bias and fairness problems (mehrabi et al., 2021) to the downstream task. therefore, in real-world applications, we should have more strict rules to filter and clean up the webpages, and thoroughly check the bias and fairness issues of the pre-trained model. limitations in addition to the bias and fairness concerns that we discussed in the ethical and broader impact section, we discuss the possible limitations of our method in this section. as a query-based dee framework, queryform may be prone to specific prompting based adversarial attacks (xu et al., 2022), which may further pose potential security concerns for safety-critical documents. thus, it is important to test the robustness of queryform against adversarial attacks and design defense schemes to further strengthen our method in the future. our work focuses on the closed-world setting that source documents include entities contained in the target documents, following (xu et al., 2021), without further investigating the possible openworld (shu et al., 2018) setting with unseen test entities. however, as a query-based framework that makes conditional prediction with no pre-defined set of entities, queryform actually supports the prediction of unseen entities at test time and we would like to leave it as an interesting future research direction."
2328,"limitations this section does not count toward the page limit. as illustrated in § 5.2 and shown in table 3, the coherence of the rewrite generated by edircs is not as good as that generated by purely autoregressive rewriters (e.g, t5qr). this may affect the performance of edircs when using dense retrievers. possible solutions include using an additional token reordering model (chowdhury et al., 2021) to improve the rewrite coherence or injecting the coherence signals (hao et al., 2021) or token positions information (mallinson et al., 2022) into the learning of edircs in an end-to-end way. another concern is that the effect of our text editing-based model may be limited for a few long-tail cases where many expected rewrite tokens are not in the input session. how to better deal with the search dialogues whose search intents are too implicit or vague to be accurately expressed by inferring from the dialogue context alone is a valuable direction for further improvements of our model."
2329,"limitations in this section, we discuss some of the known limitations of our set-up, data and models. to handle unknown words in the test sets, we replace them by a special unk token which is also used to mask some tokens in the training set. the unk token provides little information regarding the actual input and tapir might be unable to fully utilise the token to refine its interpretation of the past output. this has a direct influence in the incremental metrics, as the model can exploit this property by using unk token as a cue to emit the revise action. this strategy also introduces the extra hyperparameter of what proportion of tokens to mask. we put effort into achieving a diverse selection of datasets in various tasks, but our analysis is limited to english. we are reporting results on the datasets for which the non-incremental versions of the model could achieve a performance high enough to allow a meaningful evaluation of their incremental performance. tuning is still required to extend the analysis to other datasets. related to these two issues, we decided to use tokens as the incremental unit for processing. we follow the tokenization given by the sequence labelling datasets we use. extending the analysis for other languages requires thus a good tokenizer, and annotated data, which may not exist. we may also inherit limitations from the datasets that we use. although we do not include an in-depth analysis of the datasets, as our focus is on the model and not on solving the tasks themselves, they are widely used by the community and details are available in their corresponding publications. the method we propose to retrieve the action sequences depends on the chosen model, and the grounding of the action sequences in the actual prefix outputs have a direct influence in training the controller. therefore, the decisions made by tapir rely on the quality of the underlying generated action sequences. in order to ensure that the internal representations of the action generator lt do not depend on right context, we had to restrict ourselves to a single layer variation of this model when generating the sequence of actions. it is possible that with more layers its behaviour would be different, but that would invalidate the assumptions needed for an incremental processor. when it comes to the tapir architecture, the attention scores for the controller are computed independently of temporal order and we do not explicitly model relation between cache elements. the limited cache size also means that some past information has to be discarded to accommodate incoming inputs. although we have made efforts to incorporate them through the summary vector, this might be not ideal due to information bottleneck."
2330,"limitations of our work are inference speed and decoding strategy. the efficiency of semiautoregressive inference is lower than that of nonautoregressive algorithms, so currently it cannot be applied to scenarios with high frequency requests. furthermore, this paper only introduces greedy search as a decoding strategy. overcoming the challenge of introducing many complex decoding strategies such as beam/tree search (liu et al., 2020; ma et al., 2021) belongs to our future work."
2331,"limitations the current amr-tst is based on the style rewriting algorithm to rewrite the stylistic nodes of amr graphs from source style to target style. however, this method relies on style opposites features contained in the general natural language corpus. the advantage of such a method is that it does not need complex decoder retraining processes for different datasets, which maximizes the use of generic natural language knowledge and reduces training costs. however, this also leads to a limitation that the current amr-tst is applicable to text style transfer tasks with significant style polarity, such as sentiment features. for other text style transfer tasks like political and gender transfer, our current style rewriting algorithm cannot precisely rewrite the implicit style words in these tasks. to address this limitation, our future work will improve the style rewriting algorithm by finely identifying implicit style words and exploring their correlations, enabling the revised algorithm can be embedded in the current amr-tst framework that focuses on the node-level stylistic features. ethical statement this paper honors the ethical code set out in the acl code of"
2332,"limitations we discuss some limitations of this work for future research efforts. the range of the domains could be more comprehensive to cover social media and law. the experiments can potentially cover more models. as we mention in sec. 8, there are more comparable retrievers and qa readers. it would be useful in the future to benchmark more models on robustqa. finally, due to the complexity of the raw ir data, it is costly to collect our datasets. this is manifested by not only the monetary costs, but also the human efforts to create guidelines, to coach annotators, and to manually audit and validate annotations. in the future, it could be beneficial to leverage large language models with context learning to assist human labors."
2333,"limitations in this work, we propose a novel method for eae that introduces learnable soft prompts to capture specific-example context and relevant documents for prompt customization and enrichment. although experiment results have demonstrated the benefits of the proposed model, there are several limitations that can be addressed for further improvement in future work. first, similar to previous eae studies (du and cardie, 2020; li et al., 2021; ma et al., 2022), our eae model assumes golden event triggers for event types that might not be available for real-world applications. as such, future work can develop more comprehensive research and models to accommodate predicted event triggers while still maintaining competitive performance for eae. second, to aggregate relevant document representations for soft prompt computation, our eae method leverage an event type mentioning graph that capture documents, event types, and their occurrence in training data. on the one hand, the graph does not involve argument roles that are directly related to eae and might provide richer information/context to obtain representation aggregation to augment soft prompts. on the other hand, our method only explores graph attention networks to perform representation aggregation while many other variants of graph neural networks have not been considered, e.g., deep graph convolutional networks (chen et al., 2020). future work can explore richer graphs and graph neural networks to learn better representations for soft prompts for eae. third, despite the introduction of soft prompts with important benefits, our method still needs to rely on discrete prompts to explicitly specify event types and argument roles. although our experiments demonstrate better stability of the proposed method with different discrete prompt variants, adapting our method to new languages will still require some prompt development effort to achieve optimal performance. finally, in contrast to the interpretability of discrete prompts, soft prompts are less explainable, which can be addressed in future work to make the proposed method more accessible to various users."
2334,"limitations of simulated al settings, we propose guidelines to improve trustworthiness and robustness in al research. transparency our first recommendation is a call for transparency, which essentially means to report everything (dodge et al., 2019). every detail of the experimental setup, the implementation and the results, would be extremely helpful to properly evaluate the soundness of the experiments. we urge al researchers to make use of the appendix (or other means such as more detailed technical reports) to communicate interesting (or not) findings and problems, so that all details (§3) are accessible. thorough experimental settings we aim to incentivize researchers to thoughtfully consider ethical and practical aspects in their experimental settings. it is crucial to compare a wide range of algorithms, striving for generalizable results and findings across datasets, tasks, and domains. moreover, we endorse research endeavors that aim to simulate more realistic settings for al, such as exploration of al across multiple domains (longpre et al., 2022; snijders et al., 2023). additionally, we advocate for investigations into active learning techniques for languages beyond english, as the prevailing body of research predominantly focuses on english datasets (bender, 2011). evaluation protocol we strongly encourage researchers to prioritize the establishment of fair comparisons among different methods and to provide extensive presentation of results, including the consideration of variance across random seeds, in order to ensure robustness and reliability of findings. generally, we argue that there is room for improvement of the active learning evaluation framework and we should explore approaches from other fields that promote more rigorous experimental and evaluation frameworks (artetxe et al., 2020). analysis we place additional emphasis on the requirement of conducting comprehensive analysis of al results. it is imperative to delve into the nuances of how different al algorithms diverge and the extent of similarity (or dissimilarity) among the actively acquired datasets. it is incumbent upon al research papers to extend beyond the results section and include an extensive analysis component, which provides deeper insights and understanding, as in ein-dor et al. (2020); yuan et al. (2020); margatina et al. (2021); zhou et al. (2021); snijders et al. (2023), among others. if we aim to unveil why an al algorithm fails to outperform another (or the random baseline), we need to understand which data it selected in the first place, and why. reproducibility reproducing al experiments can be challenging due to the complex nature of a typical al experiment, involving multiple rounds of model training and evaluation, which can be computationally demanding. however, we strongly advocate for practitioners and researchers to prioritize the release of their code and provide comprehensive instructions for future researchers aiming to build upon their work. by making code and associated resources available, the research community can foster transparency, facilitate replication, and enable further advancements in al methodologies. efficiency finally, we propose the release of actively acquired datasets generated by different al algorithms, which would greatly contribute to datacentric research and interpretability aspects of al. particularly when employing al with large-scale models, it becomes crucial to establish the actively acquired data from other studies as baselines, rather than re-running the entire process from the beginning. such an approach would not only enhance transparency, but also promote efficiency and ecofriendly practices within the research community."
2335,"limitations within the al research community, with the intention of illuminating obscure experimental design choices. furthermore, we delve into a thorough exploration of the limitations associated with simulation in al, engaging in a critical"
2336,"limitations in this paper, we focus on masked language models, which have been shown very effective and are widely used. one limitation of the present study is not investigating another representative category of language models, the generative pre-trained models (e.g., gpt2/3 ( radford et al. (2019); brown et al. (2020))), we leave it for future work."
2337,"limitation regarding positive predictive power, there is always a risk with research on social biases that it can give practitioners a false sense of security. it is absolutely possible to evaluate on our corpus and get no bias, and still end up causing harm to racial or gender demographics, since they do not cover all biases or all domains. this should be kept in mind whenever applying this research."
2338,"limitations the deep neural networks in rho uses feature extraction and vectorization to represent the texts. the model only detects the statistical regularities and quantitative relationships among the variables but can not see qualitative relationships, such as causality, hierarchy, and other abstractions (tsimenidis, 2020). although we leverage the response re-ranking technique, which improves the explainability of rho, the neural networks are undoubtedly still “black boxes” to humans. therefore, the faithfulness of generated responses can not be fully guaranteed. ethical considerations in addition to the hallucination problem, another critical challenge, the offensive language, is also introduced with the evolutionary progress toward building reliable dialogue systems. the data-driven models are susceptible to delivering offensive responses while mimicking human conversations (xu et al., 2020b). it has been shown that racial and gender biases are encoded in the plms (blodgett et al., 2020), and these biases are present in the training corpus. since rho leverages plms and the training corpus, it is possible to generate offensive languages. we suggest that in real-world dialogue systems, it is necessary to employ some postprocessing steps to alleviate this problem when it is deployed online."
2339,"limitations there are mainly two limitations in this study. first, we still do not consider components other than the bias parameters in the prediction head. for example, the weight parameters of the prediction head, i.e., γ and w fc, can also affect a model’s prediction. second, our findings do not cover the transformer language models other than bert (base and large) and gpt-2 (small, medium, large, and xl). consistent findings were obtained for the two main architectures (i.e., encoder-based masked, and decoder-based causal language models) and for various model sizes, although future research is needed to show whether the findings can be generalized to roberta (liu et al., 2019), open pre-trained transformer language models (opt, zhang et al., 2022), and other variants. considering transformer encoder-decoder models, such as neural machine translation models and t5 (raffel et al., 2020), would also be an interesting future direction."
2340,"limitations we see two main limitations in our study. primarily, given the clear trend of larger generative models producing higher quality answers, an obvious question is to investigate whether this continues to be the case indefinitely, or whether it saturates after a critical amount of parameters. despite this, due to hardware restrictions, we were unable to experiment with models larger than bart-large. additionally, considering that the field of ambiguous qa inherently requires complementary pieces of evidence, there is no doubt that diversification methods are bound to yield better results in terms of disambiguation quality. in this work, however, we limited ourselves to using a typical neural retriever, shifting our focus toward the factuality and the fluency of the generated answers."
2341,"limitations of this approach might be its high computational cost to explore with ‘gpt3-scale’ lan- guage models (brown et al., 2020), and we expect that this can be addressed through offline reinforcement learning (fujimoto et al., 2019) techniques in future research. limitations large language models over the gpt-3 have made significant progress in natural language generation, but applying the criticcontrol method, and exploring through these large language models are computationally too expensive. to address this, offline reinforcement learning (fujimoto et al., 2019) may be a promising option to minimize training costs. criticcontrol also has inference speed degradation because additional inference costs are needed like other controlled text generation methods (dathathri et al., 2019; yang and klein, 2021). the potential solution may be to use the action-value predicting critic (yue et al., 2020), which would allow for real-time control of various attributes without affecting the inference speed of the language model. recently, the impact of instruction models (chung et al., 2022; ouyang et al., 2022) on text generation has recently been highlighted in academic research. these models, which allow for control over the generated text via input manipulation, have become widely accessible on various attributes without extra computational costs. future works will investigate the synergistic potential between the ‘inputside’ control of instruction-based models and the ‘output-side’ control of criticcontrol. ethical statement we acknowledge that our reward-driven text generation system may lead to generating harmful or misleading content when used with undesired reward models. however, controlled text generation methods have the potential to address these ethical issues present in large-scale pretrained language models, for example, through the detoxification of language. therefore, we emphasize the proper use of reward models to pursue the public good and believe that it is important to continue research in this area as these techniques can offer significant benefits."
2342,"limitations while this dataset is unique and pioneering, its size is limited, and it involves specific patients. to enhance the generalizability of the findings, a larger dataset may be required. similarly, although our framework is innovative, we anticipate the development of more comprehensive and informative annotation protocols in the future. for instance, we observed a higher frequency of the ""intervention in- formation"" category within cognitive engagement, likely because the intervention predominantly follows a q (nurse) & a (patient) format. we hope that the coding scheme established in this study can aid future research in refining this category with finer granularity, based on specific intervention theories. ethical considerations we used the nlm scrubber offered by nih to produce hipaa-compliant deidentified health information for scientific use, including dates, and places. two independent annotators evaluated the nlm-scrubber on the dataset to make sure no events or other people in patients’ posts can allow patients to be traceable. the de-identified version of our data will be shared with researchers upon request who have completed an ethical review from their institution and a data request application form from us. since the domain of our dataset is specific, the models trained on our dataset may exhibit subtle biases on out-of-domain data. further, pre-trained models that we use in our work have been shown to exhibit biases (li et al., 2021). we hope future researchers could use these models with caution regarding the biases that these pre-trained models have. the long-term goal of our work is to aid healthcare providers to quickly identify poorly engaged patients to allocate their energy and resources to provide in-time support. models trained on our data should not be deployed in the real world without human supervision because, despite the potential of transformer models, they cannot be relied on completely in sensitive medical scenarios."
2343,"limitations it is highly desirable to test our model on more datasets. however, there are very few multi-class, publicly available datasets that include information about annotator assignments. often this information is, unfortunately, either discarded or withheld. without annotator assignments, it is difficult to run experiments related to label distribution learning driven by annotator-item modeling. we hope that this paper encourages more researchers to collect and share more datasets that retain information about annotator-item matchings. datasets: we understand that the disagreement between the annotators could arise due to the subjectivity/ambiguity of the content to be annotated, nature of the study, or even worker reliability (aroyo and welty, 2013; inel et al., 2014). these observations cannot be solely utilized to disregard a dataset, since it is not a limitation of the dataset but the nature of the problem domain of annotator disagreement. ethical considerations all statistical methods are double-edged swords. used maliciously, these methods could be used to misrepresent social values and opinions. moreover, while these methods would be more informative with demographic information on the annotators, this conflicts with the privacy of the annotators, a group of workers who are often treated unfairly (gray and suri, 2019)."
2344,"limitations first, most articles are crawled from the us and uk presses. this means the crawled data is englishonly and regionally biased, limiting the scope and the diversity of issues. extending our work to other languages and more regionally-diverse presses will be helpful for reducing such bias in our dataset. second, we suspect that there will be a nontrivial annotation bias in our dataset. we are concerned with the fact that all of our in-house annotators share the same cultural background and similar personal interest (given that the annotators volunteered to partcipate in this turking task). furthermore, given that claimdiff-w is aiming to catch the subtle differences in the nuances of these professional news articles, it is very challenging for different annotators to have a common view, especially compared to claimdiff-s (which also explains why claimdiff-w human performance is much lower than that of claimdiff-s). third, since claimdiff is a sentence-level comparison task, it currently does not give information about the surrounding context of each sentence. this means inter-sentence dependency such as coreference often cannot be resolved. one way to work around this is to give an access to the full articles for each claim pair, but we have refrained from it in this work for simplicity (though we believe it will be interesting to see if the performance can be improved with such access). fourth, the size of claimdiff is relatively small compared to other fact verficiation datasets. this is mainly because its annotation process is quite challenging and requires a substantial amount of time. future work includes expanding the size of claimdiff when additional budget is available."
2345,"limitations in this section, we will summarize several limitations of our work. the first one is we only apply the proposed method to the natural language understanding (nlu) tasks. it is uncertain how to extend our approach to the natural language generation (nlg) tasks and whether it can bring considerable improvement. then, the training process of gan is sensitive to hyper-parameters, leading to us not simply using the default setting when extending to other tasks. finally, this paper does not include a theoretical explanation and proof of how our method works, which we will further study in future work."
2346,"limitations we propose a set prediction network for the extractive summarization task, which has worked well on some datasets but still has some limitations. firstly, due to the use of pre-train bert in the document encoder, our method is inadequate for long text summarization tasks. in general, the text length of a long document is much longer, so the model needs to be more capable to capture the dependency. next, we will extend the method to long document summarization tasks. secondly, the queries in the decoder are initialized with a normal distribution. if we can initialize the queries with the prior knowledge, our method may be able to find the set of sentences of the summary more accurately, which is another direction we need to focus on in the future."
2347,"limitations ground truth and data cleaning although we conduct basic cleaning by selecting the ground truth place object that has appeared the most often for a given user, this is only a heuristic and does not guarantee that the selected ground truth matches the description in the user location string, which introduces noise in the twitter-pug dataset. future work is needed to develop more accurate methods that identify the ground truth from a set of geotagged user tweets. also, the current ground truth format does not account for alternative names in geolocation. a future direction is training the seq2seq model to generate multiple formal location names from a single user location string. alternative names in gazetteers such as geonames could be used as a source of this ground truth. in figure 4, we identified several types of noise in twitter user profile locations. we did not conduct extensive data cleaning of fictional, joke, or non-existent locations. though we attempted to filter these places automatically, we found little change in model performance. a more detailed study of the effects of data cleaning would be beneficial. model size due to resource constraints, we only experiment with the mt5-small model. in a smallscale preliminary study, we found mt5 outperforms byt5 (xue et al., 2022) on our task of geolocation name transduction. it would be interesting to also test how larger (e.g. mt5-large) or other types of pretrained language models (e.g. fully autoregressive models) performs on this task. also, how much data is actually needed to train the model. coverage v.s. accuracy trade-off another limitation of the geo-seq2seq approach is that the model always produces a candidate location even when the input only contains a fictional location or does not contain a location at all. a potential solution for this is thresholding the model based on a log-probability threshold, and only producing a candidate location when the probability of a beam is high enough. such thresholding method could serve to trade off coverage and accuracy. a related issue is the accuracy at each granularity (i.e., country, admin, and city). the model performs significantly better at lower granularity, specifically at the country level (see table 2). this is important for end-users to acknowledge if this tool is used for higher-stakes analysis such as natu- ral disaster relief, versus such as studying vaccine opinions in different parts of the world. performance across demographics finally, as shown in section 8, our model has a wide range of performance with respect to f1 across countries, and a smaller discrepancy of accuracy across language. the strong multilingual performance is most likely from the original mt5 pre-training. however, there is still room for improvement. to address the discrepancy in performance across countries, a strategy is to stratify the data by country, similar to how multilingual pre-trained encoders are trained with exponential sampling based on language balance (xue et al., 2021). ethical considerations the main ethical consideration for a tool like geoseq2seq is privacy. we respect user privacy in the creation of geo-seq2seq as well as in collecting the data to build twitter-pug by only using immediately available data provided by users. as discussed in section 3, the training data is built from user profile location strings paired with a user’s most frequently tagged twitter place. once trained, geo-seq2seq only needs the user profile location to run inference. also, due to the structured nature of the output string and easy integration with carmen, researchers can easily choose at which granularity to aggregate their data, whether the city, admin (state/province), or country level. further, the use case of our model is only meant to support researchers studying location-specific demographics. the content will be studied in aggregate, as according to twitter policy."
2348,"limitations one of the limitations of our study is that the performance of knn search is highly dependent on the domain of the datastore used. as shown in section 6.4, knn search, like standard lm, does not work well for contexts and numerals for out-ofdomain data. this dependence can be reduced by increasing the size of the datastore and introducing passages from various domains; however, this strategy may bolster another limitation, as discussed hereafter. the second limitation is that knn-lm requires more memory usage for the datastore and higher latency for search during inference compared with standard lms. although the search process itself can be executed swiftly by leveraging efficient similarity search libraries like faiss (johnson et al., 2017), as the size of the datastore expands, the time required to obtain their representation vectors is expected to increase. the third limitation pertains to the lack of language variety in the utilized datasets. while we deliberately selected datasets from different domains for our experiments, they shared a common language, namely english. consequently, it is expected that knn-lm will exhibit similar effectiveness in languages with linguistic structures similar to english. however, conducting experiments on non-english datasets is necessary to provide evidence for the language-independent impact of knn-lm. this aspect will be addressed in future research endeavors."
2349,"limitations although honestbait shows promising results for generating attractive but faithful headlines, there are still some limitations: (1) honestbait is a monolingual model that only supports chinese. it requires three pre-trained scorers. also, as the fr labels are specifically difficult to obtain, it is not easy to implement in other languages. (2) running the whole framework with a batch size of 16 takes around 22 gb gpu memory, mostly because we must load all pre-trained models into the gpu. this can be alleviated by using a distilled pre-trained model. (3) on average, honestbait generates more faithful headlines than other baselines, but it still occasionally produces false information or unwanted results. this work is only for academic purposes and is not ready for production."
2350,"limitations in this paper, we simplify intention identification into a sentence classification task, i.e., exploiting a specific procedural event in an event process to predict the intention of the whole event process. a more realistic way to model this task is to enter the entire event process rather than a single event. we will go into more detail about this type of task in future work."
2351,"limitations our approach is proposed based on the intuition that false negative samples should have high similarities with the positive samples that have the same gold entity type, and they also have low similarities with the positive samples of different entity types. however, our proposed approach does not guarantee the selected negatives are true negatives. furthermore, when the negative samples are hard false negative samples, they are likely to have high similarities with other positive samples as well. however, such hard false negative samples are not prevalent in the datasets. another limitation is that there is still a large performance gap between the distantly supervised datasets and the human-annotated datasets, as mentioned in section 4."
2352,"limitations we observe two main legal limitations for this project. first, it has limited practical use for non-lawyers seeking legal help, also called selfrepresented litigants. in fact, any system providing legal citations, both precedent or statutory provision, to an untrained lawyer will be of very little use, and even harmful. it is hard to imagine in what context this might be used by non-lawyers considering that they might not be able to translate facts into a legal problem. that being said, many direct-to-public legal applications have emerged recently, and many of these applications do provide insightful legal information along with the legal sources (morrison, 2019; dahan and liang, 2020). while these applications have raised concerns as to their legality, notably with the issue of unauthorized practice of law, many regulators including in canada, the united states and europe have cautiously supported the development of ai-power technology for the general public. second, several lawyers (especially appellate) have surprisingly expressed concerns regarding “the googlization of legal databases"" (vaidhyanathan, 2011). while they recognize the advantages of intuitive ai non-boolean research, they claim that these algorithms are not superior when it comes to locating a more obscure appellate case law, to help win a case. it has even been argued that boolean logic remains faster and more efficient because it does not lead to missed case. according to this view, while the “googlized"" legal database may quickly locate important caselaw especially if decided by a higher court, it can miss less obvious cases (mart et al., 2019). in our work, this challenge translates to the long-tail problem for legal citations, and our use of embedding distance encourages matching based on semantic similarity. in other words, we only look for the most obvious citations, which correlates to higher performance on easier (procedural) citations and lower performance on harder (non-procedural) citations. our work does not sufficiently address this problem, so we encourage more proficient information retrieval or prototype discovery methods in the future. from a deep learning perspective, the main limitation is due to the use of k-means clustering in our implementation of the system. there were several points of instability noted during the training process, which we theorize is largely due to the initializations of the k-means clustering algo- rithm. when the prototypes are initialized, the corresponding terms in the loss function have a strong influence on the cross-entropy loss, which leads to model collapse. even when the prototypes are initialized properly, the loss function overfits to the prototypes after several updates but does not provide improvement in the classification performance, which is why we choose the best model by validation macro f1 instead of validation loss."
2353,"limitations. this paper takes an entirely different approach by zeroing in on a particular task, which has been augmented with a specific semantic evaluation (embodiment ratings of actions), to highlight how difficult tasks, such as figurative language interpretation, benefit not only from model size but from specific embodied semantics. figurative language is difficult for lms because its interpretation is often not conveyed directly by the conventional meaning of its words. human nlu is embodied and grounded by physical interaction with the environment (di paolo et al., 2018). consequently, it could be expected that lms struggle when the interpretation of figurative language depends on a more embodied action. yet, the opposite has been shown as more embodied concepts are more lexicalised and larger lms can interpret them better in figurative language. hence, our study provides valuable insight that raises the question of whether this effect is limited to figurative language or translates to other nlu tasks for lms."
2354,"limitations our method has three major limitations. first, the auxiliary data corpus with label information might be rare. recall that the corpus we used in this paper is the training set of different benchmarks. however, large-scale labeled data as the auxiliary data source might be infeasible in practice, hence it may limit the model deployment in real-world scenarios. second, our method is trained and evaluated on english datasets. additional data processing as well as annotation is necessary for other linguistic settings. third, external unlabeled data with the same domain as the aste datasets are needed for the pre-training of the retriever. in our experiment, we choose two external datasets in the restaurant and electronics domains. if our method is applied to other fields, we need to find additional external data in the corresponding domain for pre-training."
2355,"limitations a limitation of this work is that it is only evaluated on synthesized datasets of cartoons with limited characters and scenes. in the real world application, there might be many different scenes/characters, posing new challenges to the proposed approach. another limitation is the requirement of supervised training data and resources. despite the number of trainable parameters of our approach (850m) is less than ar-ldm (∼1.5b), the model still needs many story-level training data and computing resources."
2356,"limitation as our method does not focus on dealing with unanswerable questions, our method may not show a great advantage over other methods when there are a lot of unanswerable questions. how to improve the recognition of this type of question, avoid overrating further modeling on them, and therefore give more accurate graph modeling on answerable questions will be left to our future work. besides, our speaker modeling prefers questions focusing on speakers, and it may show limited improvement if a dataset contains few speaker-related questions. however, speakers are key roles in dialogues, and therefore, questions about speakers naturally appear frequently in drc. the power of our key utterance extraction method to other qa fields remains unknown. it can be future work to extend it to other reading comprehension tasks like narrativeqa (kociský et al., 2018). our method does not involve additional knowledge, such as speakers’ co-reference and relations (liu et al., 2020), discourse structures of dialogues (li et al., 2021; ma et al., 2021), and decoupled bidirectional information in dialogues (li et al., 2022). these types of knowledge, which are orthogonal to our work, are key components of dialogues. therefore, making full use of the additional knowledge in dialogues with our graph modeling can be an interesting direction to explore."
2357,"limitations several limitations of codeexecutor, such as its application to only python, the lack of faithfulness in the results produced, and the maximum length limit for trace generation, point toward interesting directions for future work. programming language one limitation of our current model is that it is currently only applied to python, which limits its use and effectiveness in executing programs written in other programming languages. this highlights the need for future work to expand the model’s applicability to other languages. faithfulness the result may not be faithful enough when handling difficult examples, such as those with complex logic, long loops, or many branches. for example, we observe that in two complicated programs that both contain the assignment “alpha = list(’abcdefg’)”, our model correctly predicts the value of “alpha” in one case but incorrectly in the other. the lack of faithfulness needs to be studied for further research on code execution. generation window size we limit the length of generated trace to 1024 tokens. it can be a limitation for programs with long execution traces, particularly those with loops. improving the ability of transformers to handle longer sequences (tay et al., 2021, 2022) would likely be beneficial for the code execution task. ethical statement the work is conducted in compliance with ethical principles. the datasets introduced in this paper only used publicly available data. the annotation in human evaluation was conducted by two authors of the paper, and thus there are no associated concerns, e.g. regarding compensation. therefore, there are no potential risks associated with the research."
2358,"limitations our current design and experimental studies are limited on lms in the generic domain, and are not yet been studied in specific domains such as extracting healthcare knowledge from relevant neural models. we leave the exciting work of harvesting knowledge from various kinds of neural networks across applications and domains in the future work. ethical considerations in this work, the harvested knowledge is automatically generated by lms. we would like to note that the language models could possibly generate unethical knowledge tuples, same with the risks of other applications using language models for generation. we hope that the knowledge extraction study could offer techniques to better interpret and understand the language models, and in turn foster the future research of language model"
2359,"limitations our work addresses the sequential task of modeling temporal user data through the use of path signatures as a tool for providing low-dimensional trajectories. although in our work we inject a post-level timestamp in the final representations, the path signature element is agnostic of time and rather only makes use of the sequence order. it therefore potentially hinders the model’s ability to efficiently model long timelines (unlike ours) with significant and highly irregular lags between posts. we plan to address this in future work. additionally, we understand that by employing truncated path signatures in the model, we loose information that can potentially provide additional signal through the compression that happens both in dimensionality reduction and in the signature itself. we have evaluated our model on a longitudinal mental health task. while the proposed architecture is in principle task agnostic we have not yet evaluated it on other longitudinal tasks on social media."
2360,"limitations our model introduces additional parameters in the question-guided vision bias module, compared with other methods. moreover it is also worth exploring whether the question-guided vision bias module can improve number type questions in other ood data sets."
2361,"limitations in this section, we discuss the limitations of tae. first, as our method depends on event structure information which is obtained through automatic parser, if the parser is not good enough, then it will impact the performance. second, since we focus on leveraging structural information, we restrict the experiments on text-based event explanation. future work will explore multi-modal event detection explanations and evaluate models on other nlp tasks."
2362,"limitations the findings of this study have to be seen in light of some limitations. (1) it is non-trivial to extend our model for generation tasks. since the main focus of this work is to improve both effectiveness and efficiency of the dual-encoders, text-decoder is not considered in model design. in the future, autoregressive mechanisms will be consider to applied in model architecture so that the model can be directly used for generation tasks like image captioning. (2) there may be disadvantages of the model in region-level vl tasks such as object detection. the reason is that these tasks require images in high resolution and fine-grained annotations of bounding boxes, which are non-trivial in generic vlp settings. to solve this problem, exploring different levels of granularity between image-text pairs is a promising direction and will be considered as the future work."
2363,"limitations we have demonstrated that cockatiel is capable of generating meaningful explanations that align with human concepts, and that they tend to explain rather faithfully the model. the concepts extracted of nmf are abstract and we interpret them using part 3 of the method. however, for the interpretation, we rely on our own understanding of the concept linked to the examples of words or clauses associated with the concept. this part therefore requires human supervision and will not be identical depending on who is looking. one way to add some objectivity to this concept labeling task would be to leverage topic modeling models to find a common theme to each concept. in addition, τ1 and τ2 were chosen empirically to allow for an adequate concept complexity/human understandability trade-off in our examples. we recognize that this choice might not be optimal in every situation, as more complex concept may be advantageous in some cases, and more easily understandable ones, in others. we surmise that this choice might also depend on the amount of concepts and on the model’s expressivity. finally, we have studied the meaningfulness and fidelity of our generated concepts, but ideally, the simulatability should also be tested. this property measures the explanation’s capacity to help humans predict the model’s behavior, and has recently caught the attention of the xai community (fel et al., 2021b; shen and huang, 2020; nguyen, 2018; hase and bansal, 2020). we leave this analysis for future works."
2364,"limitation our paper presents a pilot exploration of investigating a new setting in code-switched text synthesis — we allow the target language pair selection not limited to those for which we already have training data. although we have shown the strength of gloss qualitatively and quantitatively, our experimental setting is still confined due to the dataset restriction — all the input text is in english. it would be an even harder challenge if the source languages are more diverse and we leave such exploration for future work. additionally, due to the computational restriction, in gloss, we only explore mbart50-mmt and an augment-mmt as our pmmtm. from the experimental results, we do observe the benefit of having a more stable pmmtm in gloss. we anticipate the models’ performance can be further improved by leveraging more stronger pmmtm, and the exploration is left for the future. broader impacts our proposed models are based on a model that is pre-trained on a large scale of multilingual machine translation data. it is known that the machine translation model could capture the bias reflecting the training data (wang et al., 2022). therefore, our models can potentially generate code-switched text containing offensive or biased content. we suggest that for deploying our model in any real-world applications, careful examination of the potential bias is an essential step."
2365,"limitations although our model is more efficient than previous models trained using the mlm objective and the standard transformer architecture, we notice that the models runs around 30% slower. this is due to the disentangled attention mechanism, which is more computationally expensive than the standard attention mechanism. we also note that at the time of writing, the debertav3 tensorflow 2 implementation available on huggingface’s transformers library (wolf et al., 2020) experiences heavy slowdowns with tpu backends. our attempts to solve this issue were unsuccessful, and we were unable to train our model on tpus."
2366,"limitations in this section, we discuss some limitations and potential risks of our work. (1) our codeprompt focused on program and language generation tasks, so it is difficult to directly apply our method to program and language understanding tasks. (2) we designed an input-dependent prompt template with fixed backbone words (language, function name, keywords) for a simple and efficient template. a more effective template can be crafted. (3) we applied only codet5, the most state-of-the-art model, as the basis of the framework of our codeprompt."
2367,"limitations first, in this work, we assume that clustering in the encoder and the decoder is only related to the source and target languages, respectively. actually, both parameters in the encoder and the decoder are influenced by source and target languages simultaneously. therefore, our assumption may lead to a performance drop. in future work, we plan to explore more complicated clustering strategies. moreover, our adapter-families method depends on prior linguistic knowledge. its actual effectiveness can be affected by the distribution of language families/groups in clients. our methods mainly apply to comparably uniform language distribution. in addition, the effectiveness of our methods on other plms needs to be verified. however, it is easy to transfer our methods to other models so it will not be a challenging problem."
2368,"limitations we identify the following limitations of plat and strategies to overcome such drawbacks: • performance of the final classifier is dependent on the black-box source weak labeler. we believe this limitation can be worked around in a real-word setting by ensembling source models to vote on a likely weak label for practical accuracy gains. • best-performing source models might differ for different tasks. the dataless nature of ews prevents precursory accuracy evaluations while choosing the source weak labeler model. however, quality of candidate weak labelers can be gauged indirectly. users can examine confidence distributions of weak labels (as in figure 1 and figure 2) as an indicator of pseudo-label ""naturalness"". they can also perform difficulty analysis (as shown in figure 5(a)) that does not require any labeled data. in a real-world scenario, ensemble weak labelers will be used, eliminating the need to choose a single best source model."
2369,"limitation of existing visual document classification models by modeling a document as a graph and learning its embeddings using a graph attention network. by defining two types of edges (β skeleton and paragraph-based), we leverage the benefit of layout information while minimizing the effects of the errors from ocr reading order. thus, effectively embracing coarse and fine-grained layout information, gvdoc generalizes better for different layouts. while most visual document classifiers tend to perform well on in-distribution data, they fail or struggle on outof-distribution data; our model does not drop its performance on ood data. through experiments, we demonstrate the generalization of our model on out-of-distribution data."
2370,"limitations our model is currently suitable for generating ordinary tables with attribute names and records, but it may struggle with more complex table formats that involve merged cells. to improve the flexibility of our model, we plan to investigate more versatile forms of table representation. another limitation of our model is that our model training involves longer training time, compared with seq2seq baselines. this may be due to the inherent instability of target assignments. in the future, we will explore refining the model training by reducing the target assignment instability. the existing datasets for this task is relatively simple, and in the future we will conduct experiments on more complex datasets that require reasoning, such as webnlg (gardent et al., 2017)."
2371,"limitations the limitations of our r2anker includes i) performance bottleneck: as verified in our experiments, the performance of multi-adversarial ranker training depends more on types of the comprising retrievers than their performance. since the number of the types is very limited, there is a performance bottleneck of our method. and ii) compromised adversary: due to computation overheads, the adversarial process is compromised in our training framework in terms of real-time retriever updating. this would negatively affect the performance of the framework."
2372,"limitations to train ecola, we need to provide structured knowledge with aligned unstructured textual data to the model. thus, we should either manually pair quadruples with event descriptions or use some matching algorithm to automatically build the pairs. the former requires human labeling effort and is hard to apply on large-scale datasets, while the latter would introduce noise into the dataset. thus, ecola is currently tailored for domain adaptation and enhances pre-trained models with domain knowledge. there is still work to be done to let models be jointly trained on large-scale structured and unstructured data."
2373,"limitations our study is limited to the adaptation of mlms to new languages. while we believe that our proposed approach could also be applied more broadly (e.g., autoregressive models instead of mlms, or adapting to new downstream tasks instead of new languages), further experiments are necessary to empirically verify this. in addition, we observe a considerable variance across languages (§5.4), the reasons for which are not entirely clear. ideally, we would have a broader set of languages to better study this, as our language set is limited and skewed towards the indo-european family. finally, we average results over 5 finetuning runs, but computational restrictions prevented us from also averaging over multiple pretraining runs. as discussed in §a.5, we observed a non-negligible variance over pretraining runs in a preliminary experiment, but a more systematic exploration is necessary to better understand its impact."
2374,"limitations the main limitation of our work is that we can not use a unified model to complete the zero-shot entity and relationship extraction tasks. specifically, our method trains two models, dsp-zsner and dspzsrc, to extract the entities in the text first and then classify the relation of each pair of entities. this method needs to train and store two models, which is troublesome to maintain in practical applications. in addition, although our method has dramatically improved the inference speed of the previous prompt method, the method still affects the reasoning speed of the model. in the followup works, we will be committed to solving this problem."
2375,"limitations in this work, we mainly identify robust overfitting for prlms using pgd attacks instead of textual adversarial attacks. the reasons are two folds. first, we aim to check the learning curves during at. second, the results of textual adversarial attacks may not be generalizable since they integrate different strategies. in practice, however, it is more inclined to use some textual adversarial attack methods (e.g., textfooler, textbugger (li et al., 2019)) to evaluate the robustness of nlp models. as we have clarified in section 5.3.2, there exists an adversarial generalization gap when the model defends against pgd-based gradient attacks and textual adversarial attacks. while it is difficult to check their robust loss and accuracy curves during at, it is necessary and promising to explore robust overfitting under textual adversarial attacks and provide helpful insights for promoting the adversarial robustness of prlms."
2376,"limitations it’s important to note that, in this paper, we focus on bias resulting from underlying distribution of training data. bias that may result from pretraining of transformer models (li et al., 2021) is not within the scope of this paper. although we conduct a case study of finegrained hatespeech detection task, a collective effort from the research community is required to better quantify bias mitigation of our approach across multiple tasks and different types of bias. another limitation of our work is that our proposed algorithm requires dynamic adjustment of clusters. for very large datasets, this may be computationally expensive."
2377,"limitations our work depends mainly on parallel data. although tasks focusing on language abilities can leverage machine translation to obtain parallel data (hu et al., 2020), it is much harder for tasks about knowledge and facts. using parallel data to train cross-lingual model editors is like doing full supervision, while we need to leverage weakly labeled data to mitigate data scarcity. on the other hand, whether monolingual or crosslingual, model editing still struggles with the continual learning problem. in the real world, knowledge constantly emerges and fades, disabling the stop of learning. however, most studies, including our work, focus on a single or a batch of inputs. thus, an effective solution of continuously updating a series of inputs is necessary before model editing becomes a practical technic. note that our work focuses on the editor’s generalized cross-lingual editing ability. we expect the editor to perform the editing honestly. this target potentially offers the possibility to modify model behavior maliciously. though editing may not soon become a practical technic, the potential risk does exist."
2378,"limitations a large part of the dataset that we used in our experiments are semantic annotations for relatively short sentences (as the examples show). so we don’t know really know how our multilingual pre-trained language-meaning modelling for drs parsing and drs-to-text generation will work on longer sentences. in our experiments, we converted meaning representation in the sequence notation and modelled them with natural language texts in a seq2seq manner and masked tokens in the drs sequence randomly. perhaps a more natural way is to model drss as graph structures and let training objectives directly utilize any structural information from drs. a graph structure would also eliminate the explicit order of concepts that is present in the sequence notation. although we say that the drss are languageneutral, the concepts in the vocabulary are based on the english wordnet. as a result it might be the case that non-english words do not have a direct correspondence to an appropriate synset, but the number of such cases is likely very small. the only (trivial) language dependence in drss are literal occurrences of proper names in cases where they differ across languages (e.g., ""london"", ""londen"", or ""londra""). one way to remedy this is to add alternative spellings to the meaning representation to make it completely interlingual."
2379,"limitations to better understand the limitations of our proposed mtmsg, we also perform a qualitative error analysis of the incorrectly generated samples. we randomly select 100 incorrectly generated descriptions and find that our model might incorrectly generate those samples mainly due to the misunderstanding of the necessary intent information from the images and ocr tokens. the statistical results reveal that 37% of the incorrectly generated descriptions are caused because the main part of the sarcasm might lie in the images (eg. figure 5 (a)), while the other 63% error cases are attributed to the failure of our model in capturing the intent information directly from the ocr tokens (eg. figure 5 (b)). specifically, in figure 5 (a), if we want to generate better descriptions, we need to capture the fine-grained visual attribute feature happy from the image; in figure 5 (b), we need to understand the intent information from the ocr tokens that ban guns can make us feel safe when we have lunch in the restaurant. therefore, to address the above issues in the future, we will further explore the fine-grained key information in the images to help guide the msg. besides, we will explore a language interpreter to further understand the key information contained in the ocr tokens."
2380,"limitations for easier comparison with previous work, we only focus on text classification tasks, while st can also be applied to a variety of nlp tasks, such as language generation, conversational systems and commonsense reasoning (kedzie and mckeown, 2019; he et al., 2020; shi et al., 2022a,b; hendriksen et al., 2022). we also assume that the datasets are roughly balanced. however, real-world datasets are usually class-imbalanced (li et al., 2011), which might impact the performance of tapt and st. while this is out of the scope of this paper, we believe that this is an interesting avenue for future work. additionally, different labelled and unlabelled sizes may impact the performance of st approaches in the domain shift setting. however, this doesn’t alter our"
2381,"limitations a limitation of rata is always assuming the answer is included in the retrieval corpus, which is not always true. when the corpus does not contain the correct answer, the desired behavior is to inform the user that the answer cannot be obtained, but rata will provide a poorly supported answer. this also encourages rata to learn spurious correlations when the retrieved tables coincidentally contain the same value, but does not really support the answer. this problem is especially serious when the answer is very generic (for example, numbers like “0”) and same values by coincidence are common. this is related to the answerable question issue (rajpurkar et al., 2018) or evidentiality issue (lee et al., 2021; asai et al., 2022) for question answering. course location review date votes beaver island state park grand island, ny ? 0 1 joseph davis state park lewiston, ny 12/1/2009 1 0 black diamond dgc south wales, ny 12/1/2009 1 1 query: jonnieoh's dgcoursereview profile - disc golf course review gold answer: 12/1/2009 bart output: 12/1/2009; 12/2/2009; 12/1/2010; … rata output: retrieved table: jchoate7's dgcoursereview profile - disc golf course review for cell-filling on webtables, bart outperforms rata often by either copying values from other rows of the query table or producing values similar to those in other rows. however, as shown in figure 5, rata’s retrieval is often not helpful. usually, the information required to fill the query table is not repeated in the corpus, so the retrieved table cannot support the query. as a result, rata is simply retrieving some similar table, and selecting similar values in the tables."
2382,"limitations among the six ambiguous and unanswerable problem categories in table 1, our counterfactual example generation approach can not cover the calculation unanswerable and out-of-scope examples generation. the reason is that our approach focuses on the table transformation ways while generating the calculation unanswerable and out-of-scope examples requires conditional nl modification techniques. we leave this as our future work."
2383,"limitations we have discussed the implications of our research in section 6. in this section, we further discuss the threats to validity of our study. • threats to internal validity: the main internal threat to the validity of our research comes from (rq3) where we present a qualitative study on the variation of aliases. we are unable to cover all cases in the qualitative study. for example, the entity of d030342 (disease) in table 3 has 778 unique aliases. it is impossible to show all aliases to readers. to help mitigate this threat, we try to show as many examples as possible in a limited space. • threats to external validity: the main threat to external validity arises from the potential bias in the selection of experimental datasets, attacking target models and off-theshelf ner and entity linking tools. to mitigate this threat, we experiment with multiple datasets, models and tools. for experimental datasets, we choose the three most popular docre datasets (i.e., docred, cdr, and gda). we believe that these three datasets are broadly representative in this research community. for attacking target models, we choose three typical models ranging from non-contextualized sequence-based to graphbased, and to contextualized transformers models. for off-the-shelf ner/linking tools, we comprehensively investigate five state-ofthe-art ner taggers and two entity linkers. ethical considerations as our goal of this study is to challenge current problem setups of docre, we heavily rely upon existing well-known datasets, models and nlp tools. we only claim that our findings may hold on similar datasets or domains. we acknowledge the risk of generalizability of our findings on other privacysensitive datasets or specific domains. in general, we suggest that practitioners repeat all experiments following our procedures when using other corpora."
2384,"limitations in this paper, we focus on the end-to-end accuracy and passage retrieval accuracy for opendomain qa. we have also experimented on the beir benchmark (thakur et al., 2021) to evaluate our method in the zero-shot document retrieval task. overall, we obtained 48.1% macro-averaged ndcg@10 compared to 47.8% by the re-ranking method. for some tasks, tour obtains significant improvements with a pre-trained document retriever (hofstätter et al., 2021). for example, tour improves the baseline retriever by 11.6% and 23.8% ndcg@10 on bioasq and treccovid, respectively, while also outperforming the re-ranker by 2.1% and 2.4% ndcg@10. we plan to better understand why tour performs better specifically on these tasks and further improve it. tour also requires a set of validation examples for hyperparameter selection. while we only used in-domain validation examples for tour, which were also adopted when training re-rankers, we observed some performance variances depending on the hyperparameters. we hope to tackle this issue with better optimization in the future."
2385,"limitations first, as indicated in table 2, different tokens are not equally vulnerable to privacy attacks. as such, assigning every token with the same output size k and privacy parameter ϵ might not be an ideal choice. an improved method would be to adaptively allocate privacy costs across tokens so that all of them are adequately protected. second, we adopt two simple strategies to decide whether a token is sensitive: assuming all tokens are sensitive or based on a pre-defined stopword list. however, the prior might be over-protective, but the latter can lead to privacy leakage since stopwords might help infer other sanitized tokens. therefore, a more flexible and practical way to decide the sensitivity of tokens is required."
2386,"limitations while our study shows that easyproject can effectively translate the source sentences with special markers inserted to the target languages, using the google translation and nllb model, it is unclear whether all translation models can work well when special markers are inserted. to generalize this approach to future mt systems, we design a simple and computationally efficient approach to improve the robustness of mt systems in handling special markers. however, the translation quality for the marker-inserted text still falls behind the original text. we leave the work of further optimizing the translation quality as future work."
2387,"limitations in our proposed k-htc method, incorporating the knowledge graph requires the concept recognition and pre-training process, as we introduced in section 3.2. this process may consume additional time compared with other htc methods, but it can be done in advance and does not need to be repeated, making it suitable for both research and industrial settings. besides, due to the errors of concept recognition algorithms, this process may introduce some noisy information in reality. this will interfere with the use of knowledge. in future work, we will attempt to utilize entity linking algorithms (wang et al., 2023) to further guarantee the quality of recognized knowledge. another limitation is that we utilize the label name in the khla module. it may not be available for some datasets with only label ids. in response to this, we can select high-frequency keywords from documents in each category, which play the same role as the label name."
2388,"limitations first, our taxonomy of multi-answer mrc instances only considers whether we know the exact number of answers from the questions. in some cases, one might have an imprecise estimate of answer numbers from the question. for example, for the question who are barcelona’s active players?, one might estimate that there are dozens of active players for this football club. yet, these estimations are sometimes subjective and difficult to quantify. therefore, this instance is classified as passage-dependent according to our current taxonomy. we will consider refining our taxonomy to deal with these cases in the future. second, we did not conduct many experiments with pre-trained models larger than the large-size ones due to limited computational budgets. generation models of larger sizes show great potential with more parameters and larger pre-training corpora. we encourage more efforts to deal with multi-answer mrc with much larger models, such as gpt-3.5."
2389,"limitations one major limitation of this study is its input modality. specifically, our model is limited to textual inputs and ignores other modalities (e.g., vision and audio). open and domain incremental lifelong learning across modalities is more realistic and challenging. fortunately, we can obtain robust features of different modalities via multi-modal pre-training models (xu et al., 2021; huo et al., 2021). for future work, we will try to tackle multimodal tasks in an open (including out of distribution data (lang et al., 2022, 2023a,b)) and domain incremental lifelong learning scenario with better approaches."
2390,"limitations due to the maximum input length constraint of both the clip text encoder and the text-to-image model, we are unable to process long texts. we are interested in exploring alternative prompt configurations to circumvent this limitation. our methodology is readily extendable to these settings, making it an intriguing area of study."
2391,"limitations as a limitation of the modeling assumption, dsntm assumes that the number of topics is constant over time; however, this assumption is inappropriate for some time-series documents, such as scientific papers. as the number of scientific papers is increasing annually, increasing the number of topics over time would be appropriate for modeling the time-series evolution of academic literature. we used the abstracts of the papers as text, and the attention was computed using textual information. however, citations mainly appear in the body text when a paper cites other papers. therefore, there might be a discrepancy between the attention among topics and the citation relation among papers because the attention cannot not consider information in the body text. in future work, it would be desirable to evaluate our model using a corpus containing the body text of the papers. generally, topic models sometimes infer the incorrect information about topics, such as the frequent words appearing in topics, the topic proportion in each document, and the dependencies among topics. it would be the potential risk to induce the misunderstanding of users."
2392,"limitations in our experiments, we use t5-base and t5-large models as the target model since they are widelyused, representative pre-trained seq2seq models and use comet-atomic2020 as the commonsense knowledge source. however, there are other pretrained seq2seq models such as bart, and neural commonsense models such as comet that we did not experiment with. moreover, we only experimented with 10 million randomly sampled sentences from the english wiki and bookcorpus datasets. it would be interesting to investigate whether continually pre-training with a larger scale dataset can further improve the performance. ethical considerations our work focuses on improving the commonsense reasoning ability of pre-trained language models. it probably does not introduce extra ethical concerns. however, in commonsense knowledge extraction, the neural commonsense knowledge model may generate unexpected (e.g., biased) commonsense inferences, and training with these inferences may lead to additional bias in the pre-trained model. nevertheless, all pre-trained language models contain bias and should be examined."
2393,"limitations the proposed method has limitations in its dependence on the accuracy and performance of the probe classifier as noted in (belinkov, 2022), and may be limited in scenarios where the dataset is small or lacks sufficient information about the protected attribute. additionally, this approach increases inference time due to the use of a sequential debiasing classifiers. in future work, we aim to find a single probe that eliminates non-linear leakage. finally, the proposed method aims to eliminate information about a protected attribute in neural representations. while it may align with fairness metrics such as demographic parity, it is not specifically designed to ensure them. ethical considerations ethical considerations are of utmost importance in this work. it is essential to exercise caution and consider the ethical implications when using this method, as it has the potential to be applied in situations where fair and unbiased decision-making is critical. it is important to thoroughly evaluate the effectiveness of the method in the specific context in which it will be used, and to carefully consider the data, fairness metrics, and overall application before deploying it. it is worth noting that our method is limited by the fact that gender is a nonbinary concept and that it does not address all forms of bias, and further research is necessary to identify and address these biases. additionally, it is important to consider the potential risk of inadvertently increasing bias through reversing the direction of the debiasing operation in the algorithm. it is crucial to be mindful of the potential impact of this method and to approach its use with caution and care."
2394,"limitations by analyzing the error cases, we find that almost all the existing work (including our let) cannot handle the disorder problem of words well, primarily when the error occurs far from the correct location. for example, there is a correct sentence: ’on my way to school today, i bought a very tasty apple.’. if the erroneous form is as follows: ’on my way to school apple today, i bought a very tasty.’, it is hard for the model to understand that the right thing to do is to put apple back at the end of the sentence."
2395,"limitations of the retriever-reader pipeline approach to odconvqa, which is prone to error propagation from the retriever, unable to run both sub-modules in parallel, and demanding effort to manage these two submodules, due to its decomposed structure. to address such issues, we formulated the odconvqa task as a dense phrase retrieval problem, which makes it possible to directly retrieve the answer based on its representational similarity to the current conversational context. furthermore, to model the conversational dependency between the current and its previous turns, we force their representations to be similar with contrastive learning, which leads to retrieving more related phrases to the conversational history as well as the current question. we validated our proposed pro-convqa on odconvqa benchmark datasets, showing its efficacy in effectiveness and efficiency. limitations as shown in table 3, the contrastive learning strategy to model the conversational dependencies between the current and previous conversational turns is a key element in our phrase retrieval-based od- 1https://github.com/mcgill-nlp/topiocqa convqa task. however, when the current conversational topic is significantly shifted from the previous topic as the user may suddenly come up with new ideas, our contrastive learning strategy might be less effective. this is because modeling the conversational dependency is, in this case, no longer necessary. while we believe such situations are less frequent, one may further tackle this scenario of significant topic switching, for example, with history filtering, which we leave as future work."
2396,"limitations our method requires access to a large set of labeled examples for the memory bank—ideally with some relevance to the evaluation tasks. this limits the languages and tasks that are optimal for this method: there does not exist a large variety of training examples for low-resource language varieties, nor for certain much more specific tasks—as in, for example, industry applications with domainspecific customer data. and while multilingual models could leverage cross-lingual transfer, it is unclear how well this model would generalize into low-resource languages when (for example) using multilingual bart. when using the full demonstration memory, meta-training does not run on a 16gb gpu using our current implementation. while this does exclude more common gpus, our approach could still run quickly on a 32gb gpu in a few hours, thus costing far less than pre-training a language model of comparable few-shot performance from scratch."
2397,"limitations as shown in the ablation studies, using the boundary shift loss without the base model for scope prediction leads to a huge negative impact on the performance. that is, bsl strongly relies on the assumption that the proposed candidate spans are, to some extent, being an accurate estimation of the target spans. the experiment of using bsl solely could be seen as an extreme case, that no candidate spans are proposed at all. for our task, bsl could benefit from the strong base model. for the case of noisy datasets or on a more challenging task, where a base model could not generalize to a reasonably good coarse span proposal, the benefit of bsl might be limited."
2398,"limitations we used only the pre-trained bart-large model when training each model within the qag framework. we assume that comparative experiments using several sequence-to-sequence language models will be good future works. also, we only used six interrogative words, and did not consider ‘whose’ and ‘whom’ in the process. we considered these as originating from ‘who’, but generating eight interrogative words including ‘whose’ and ‘whom’ would be a good approach. at last, in order to create a robust ranker, it is best to have a dataset that contains positive and negative samples. since the manual data generation process required a timeconsuming process, we utilize in-context negative samples as an alternative. if there is a dataset for the ranker learning purpose, much better performance can be achieved."
2399,"limitations test set size. one of the main limitations of our work is relatively smaller test set sizes. this stems from the way our perturbation experiments are set up - we can only use existing test sentences which already end with specific punctuation in order to measure the effect of deleting them, or start with sentences which do not have sentence final punctuation in order to measure the effect of inserting them. in general, a majority of the official test sets have sentences ending in full stops; this results in having a smaller test set to work with. this is also the same issue that presumably gives rise to sensitivity issues in the trained models. however, given that our focus has been on each particular punctuation, instead of merging them all together, we find that our test sets are larger than the ones used in previous work for each punctuation. combined with the fact that we ensure to perform significance testing and manual analysis, we believe our results are reliable. appendix a.1 includes details and a"
2400,"limitations though achieving promising results in the experiments, our work still has the following limitations. • as shown in table 2 and table 3. the proposed gaussian embedding may have a calibration problem leading to performing badly on fine-grained similarity tasks measured by spearman’s correlation. • the proposed method assumes that all relations are symmetric and adopts a symmetric similarity measurement. however, not all the relations are symmetric. and the ability to deal with unsymmetric relations with unsymmetric measurement is one important advantage of density embeddings which point embeddings do not have. • the proposed mrpes dataset should be improved in terms of quantity and quality. the number of test samples should be increased to over a thousand to get more statistically robust results. the types of unseen relations should be also increased to have a more comprehensive investigation of the ability to generalize on relations. the negative samples should be elaborately designed to provide the anchor event with different negative samples under different relations."
2401,"limitations we acknowledge that the methodology used to build dlama-v1 still has limitations related to the information within its relation triples. while directly querying wikidata as a dynamic source of facts provides the flexibility needed to acquire data that is relevant to different cultures (as opposed to using the static t-rex dump of triples), the diversity of the triples that are compiled depends on the availability of a diverse set of facts on wikidata in the first place. for instance, the smaller number of relation triples related to arab countries for the predicates (p136 - genre), (p190 - sister city), and (p449 - original network) in dlama-v1 (arabwest) demonstrates the difficulty of querying the exact number of facts for both cultures despite using exactly the same queries with the only difference being limiting the region to which the triples belong. another limitation is the inability to enumerate valid and fine-grained subclasses of objects for specific subjects, if these fine-grained objects are not on wikidata. steps #3 and #5 of dlama explained in §4.1 ensure that a possible and more general object is still valid for a specific subject. however, inferring a more specified object from a generic one is impossible. for example, the fact that someone speaks “american english"" implies that they speak english as well, but knowing that someone speaks “english"" is not enough to speculate about their dialect (i.e.: “american english"", “british english"", etc.). while the triples within dlama are sampled by picking the ones whose subjects have the largest wikipedia articles’ sizes, the infeasibility of manually reviewing the large number of diverse facts within dlama-v1 makes it hard to claim that the facts are free of inaccuracies or missing information. more broadly, dlama supports relations predicates that are already part of mlama to fairly compare the results on dlama to those previously reported on mlama. moreover, we make sure that the subjects and the objects of the relation triples are available in the different languages of interest. having these constraints might imply that some culturally relevant facts might have been dropped out of dlama-v1 (e.g., predicates that are not part of mlama, or triples having missing labels in one of the languages of interest). lastly, we used mlama’s probing setup in which the models rank a predefined set of objects for each prompt. their prediction is correct if the top-ranked object is one of the valid labels for the corresponding relation triple used to populate the prompt. therefore, a model’s performance is expected to be higher than that achieved by a generative setup in which the model is asked to generate the most probable completions for the masked tokens."
2402,"limitations we showed that our model is efficient in handling conditional qa on long documents with hierarchical reasoning framework. however, our discourse graphs for each document section are constructed based on the prediction of the pretrained discourse parser. there is promising improvement for our approach by use of more efficient discourse parsers."
2403,"limitations & ethical and societal considerations we consider the following limitations and societal considerations of our work. machine-generated data our analysis is based on gpt-3 generated data. though not perfectly aligned with real-world scenarios, as demonstrated in park et al. (2022), such analysis can provide insights into the nature of social interactions. however, this could induce specific biases, such as skewing towards interpretations of words aligned with gpt-3.5’s training domains and potentially overlooking more specialized domains or minority speech (bender et al., 2021; bommasani et al., 2021). the pervasive issue of bias in offensive language detection and in llms more generally requires exercising extra caution. we deliberately generate multiple contexts for every statement as an indirect means of managing the biases. nevertheless, it is a compelling direction for future research to investigate the nature of biases latent in distilled contexts for harmful speech and further investigate their potential impact. for example, it would be valuable to collect human-annotated data on cobra to compare with the machine-generated data. however, we must also recognize that humans are not immune to biases (sap et al., 2019b, 2022), and therefore, such investigations should be carefully designed. limited contextual variables although cobracorpus has rich contexts, capturing the full context of statements is challenging. future work should explore incorporating more quantitative features (e.g., the number of followers of the speaker) to supplement contextual variables such as social role and power dynamics. in this work, we focus on the immediate context of a toxic statement. however, we recognize that the context of a toxic statement can be much longer. we have observed significant effects even in relatively brief contexts, indicating the potential for improved performance when more extended contexts are present. we believe that future research could explore the influence of richer contexts by including other modalities (e.g., images, videos, etc.). limited identity descriptions our work focused on distilling the most salient identity charac- teristics that could affect the implications of toxicity of statements. this often resulted in generic identity labels such as “a white person” or “a black woman” being generated without social roles. this risks essentialism, i.e., the assumption that all members of a demographic group have inherent qualities and experiences, which can be harmful and perpetuate stereotypical thinking (chen and ratliff, 2018; mandalaywala et al., 2018; kurzwelly et al., 2020). future work should explore incorporating more specific identity descriptions that circumvent the risk of essentializing groups. english only we only look at a us-centric perspective in our investigation. obviously, online hate and abuse is manifested in many languages (arango monnar et al., 2022), so we hope future work will adapt our frames to different languages and different cultures. subjectivity in offensiveness not everyone agrees that things are offensive, or has the same interpretation of offensiveness (depending on their own background and beliefs; sap et al., 2022). our in-context prompts and qualification likely make both our machine-generated explanations and human annotations prescriptive (röttger et al., 2021), in contrast to a more descriptive approach where we would examine different interpretations. we leave that up for future work. dual use we aim to combat the negative effects and harms of discriminatory language on already marginalized people (sap et al., 2019b; davidson et al., 2019). it is possible however that our frames, dataset, and models could be used to perpetuate harm against those very people. we do not endorse the use of our data for those purposes. risk of suppressing speech our frames, dataset, and models are built with content moderation in mind, as online spaces are increasingly riddled with hate and abuse and content moderators are struggling to sift through all of the content. we hope future work will examine frameworks for using our frames to help content moderators. we do not endorse the use of our system to suppress speech without human oversight and encourage practitioners to take non-censorship-oriented approaches to content moderation (e.g., counterspeech (tekiroğlu et al., 2022)). harms of exposing workers to toxic content the verification process of cobracorpus and cobracorpus-cf is performed by human annotators. exposure to such offensive content can be harmful to the annotators (liu et al., 2016). we mitigated these by designing minimum annotation workload, paying workers above minimum wage ($7-12), and providing them with crisis management resources. our annotation work is also supervised by an institutional review board (irb)."
2404,"limitations our model achieves outstanding performance in relation to chinese spelling correction. however, it has several potential limitations: (i) errors of missing and redundant characters cannot be corrected by our model. the ptcspell model only focuses on spelling errors, and requires that the input text has no grammatical or semantic errors. (ii) the error-correcting language is targeted at chinese. the pre-trained model based on similar pinyin cannot adapt to other languages, while pretrained model based on similar character shape can adapt to other languages well, because the pinyin input method is unique to chinese, but character error due to a similar shape is a common problem in many languages. nevertheless, we put forward the idea of matching the pre-trained model with error correction tasks, which is suitable for all languages."
2405,"limitations nonfacts generates grammatically correct nonfactual summaries. however, in practice, summaries can be non-grammatical, noisy, and nonsensical. this can limit the generalization of our performance in such cases. additionally, hypothesis-only results show that a considerable number of samples are identified correctly without their context document. the reason can be the memorized knowledge in pre-trained classifiers or surface features and semantic plausibility. broader impact our model has no direct environmental impacts, fairness or privacy considerations. however, it is important to note that it must not be used as a factchecking tool as there is a potential risk that false statements may be labelled as true. our classifier evaluates the factuality of a summary based on a context document, and if the document is misleading, the summary can be factual based on misleading information. additionally, nonfacts generates nonfactual summaries, which might have potential risks if misused for generating massive nonfactual summaries (claims). addressing such risks is an open issue in the field and is not specific to our work."
2406,"limitations the findings of this work are limited and dependent on the presented experiments. the image dataset may be biased since the gender, ethnicity, and age were estimated by the dex algorithm (rothe et al., 2015) and checked by the authors. despite our best effort, the employed templates could still contain some latent bias that limits the variability and validity of the completions at inference time. since the study was conducted only in english, the insights can be considered valid only for this language. ethical statement one main concern with bias in vl is the potential harm it can cause to marginalized communities. biased vl models can perpetuate and amplify existing societal inequalities and injustices. this can result in discrimination against certain groups of people, such as racial and gender minorities, people with disabilities, and more. in particular, we are concerned about the use of vl in areas such as content moderation, hiring decisions, and criminal justice. biased models used in these contexts can have serious consequences, such as wrongful censorship or discrimination against certain job applicants. while we acknowledge that the specific harms we fear may not always be likely to occur, we believe it is important to prioritize ethical considerations and strive for the highest possible standards of fairness and inclusivity in vl research and applications. this work contains harmful language and stereotyped statements, which are only intended as examples to showcase the possible negative connotations of the analyzed models and experiments. every social, ethical, religious, or political statement or association is to be interpreted within the purpose of the experiment and condemned otherwise. we are aware of our approach’s shortcomings in terms of the binary consideration of our gender analysis. this is due to data and linguistic limitations rather than a value judgment."
2407,"limitations there are several limitations in our paper. first, we have not validated the advantages of our proposed peer in model scales larger than base model, due to the constraint in our computation resource. we plan to experiment the peer in larger scale models when more computation resource is available. second, in order to filter out potential noise from the relative weak generator, our current rank label retrieving scheme uses a strict condition t = 3, which leads to the fact that a significant proportion of tokens have rank label −1 and essentially are involved only in the original rtd task. please refer to the details in appendix a.3. we intend to design some label retrieving scheme which applies a softer criterion so that more tokens can be fully or partially involved in the complete tqr task. finally, our peer currently does not have the ability of automatically searching for an optimal value of hyperparameter δ, which we also plan to design in the future."
2408,"limitations by applying mutual learning, introducing distance polarization regularizer and utilizing cyclical annealing schedule, ml-lmcl achieves significant improvement on three benchmark datasets. nevertheless, we summarize two limitations for further"
2409,"limitations the current dialogue system still has some limitations. for example, although the current crg model can make the output contain the key concept words in the knowledge path, due to the large scale of the pre-training model, the output semantics of the current method are still not very interpretable and controllable. a feasible way is to explore new fine-tuning methods to approach high-level semantic style control. in addition, our current dialogue system lacks human qualities such as empathy, factual correctness judgment, and moral common sense representation. a key breakthrough is to explore a goal-oriented dialogue dataset with richer dimensions."
2410,"limitations we only use pre-trained video transformers off-theshelf to encode the videos, while more nuanced and specific utilization of other models can be explored to further improve the performance. there are also valuable egocentric videos and demographic statistics along with the ego4d dataset that we have not yet incorporated in our approach. due to the difficulty and cost of collecting videos with transcriptions and voting outcome annotations, the total number of games is insufficient to train a deep neural network for voting outcome deduction, though data augmentation techniques can be explored to mitigate this limitation."
2411,"limitations this work has a narrow focus: small-scale analysis, translation between one language pair (french and english), examining terminology around two realworld public figures (whose forms of address are both highly prescribed and publicly documented),8 in a specific newsworthy event (the accession to the throne of a new king after over 70 years of data and translation about a queen). first, the scale of the analysis is quite small, so it does not examine in detail questions of frequency of errors, distributions of errors, or statistical significance. while this work raises issues that may be relevant for consideration across other language pairs, the relevance of the specific linguistic conventions discussed here will vary across language pairs, and certainly do not cover the full range of asymmetries in linguistically encoded information (see, e.g., mager et al. (2018)). due to the prescribed forms of address of the two monarchs in question, this work only examined translations related to a small subset of terms (e.g., “his”/“her”, reine/roi) and does not examine performance on terms used related to other individuals or to other third person singular pronouns or forms of address that could be used by a monarch. the specific circumstances (a 70 year reign of a sovereign of a country with an official bilingualism policy and this particular set of linguistic features) means that we may not expect these results to generalize to other potentially comparable scenarios. lastly, we cannot examine the training data used for the public models, so we can only draw"
2412,"limitations on the adoption of ask an expert pertains to certain domains where the system must be deployed locally to uphold privacy concerns, such as mental health systems aiming to safeguard patient data. in such instances, relying on external api services becomes less feasible. however, it is not always necessary to utilize all the knowledge of large expert models. and for specific domain use cases, such as mental health, it is unlikely that the full size of the model is indispensable. given the effectiveness of our approach, in future work we would like to explore the extent to which the expert model can be distilled (sanh et al., 2019; schick and schütze, 2021c) into models which are able to run locally on consumer-grade hardware."
2413,"limitations in our experiment, we use only abstract text as the input text for literature review generation however, in writing literature reviews, a writer reads the full text of each cited paper and even other papers related to the research area. therefore, the input data are insufficient to write a complete literature review. as only 70% of the cited papers have access to the body text in our scireviewgen, a dataset containing full-text information is required for further research. in human-written literature reviews, the chapters complement each other and are not redundant. however, as our qfid and baseline models generate each chapter independently, they cannot consider the relationships between chapters. furthermore, the relations between each cited paper are considered in the actual literature review writing process (e.g., which paper is the first on the topic and which is the following). however, these relationships are not considered in the models. in future research, a literature review generation model that can consider the relations between chapters and cited papers by using additional information, such as the contents of other chapters, citation networks, and citation sentences, should be investigated. as mentioned in section 5.2, the generated text contains incorrect information to a certain extent. therefore, we cannot publish it without human revision. currently, the model can be utilized as a writing assistance tool, not as a complete literature review generation model."
2414,"limitation in our current study. for instance, we did not explore sampling techniques other than random sampling; while recent works (yuan et al., 2020; paul et al., 2021; guo et al., 2022) have shown promising directions in data sampling that outperforms random sampling. another interesting direction is to explore the model architecture’s influence on generalizability, and thus the learning curve, which we left for future works."
2415,"limitations during the configuration search stage, because this is a multi-objective optimization problem involving performance and efficiency, we use the evolutionary algorithm to search here. designing a robust and efficient optimization objective is not simple and it will affect the convergence of search results. 1https://huggingface.co/bert-base-uncased, https://huggingface.co/bert-base-chinese limited by hardware, and in order to speed up the search, we use a small subset of the validation set to search retention configuration, which is bound to have a certain impact on the overall search results. ethical statement in this paper, we propose ase, an algorithm to accelerate multi-turn response selection by prograssively selecting and eliminating unimportant tokens. the training corpora including the ubuntu corpus, the douban corpus and the e-commerce corpus used for evaluating our framework are publicly available and don’t pose privacy issues. the algorithm that we propose does not introduce ethical or social bias."
2416,"limitations although our proposed framework beats several baseline methods for medical dialogue generation, there is still room for progress. we exploit an entity flow and a dialogue act flow to improve dialogue understanding and guide response generation. however, our summarized dialogue acts are limited in the types and granularity of functions they denote. we can manually annotate more medical-related dialogue acts in our future research following the soap notes. besides, more medical knowledge with different formats, such as medical articles and medical examination reports, can be incorporated. finally, it is crucial to recognize the potential risks associated with system utilization and the possibility of patient privacy leakage. a collaborative approach involving both dialogue systems and medical professionals should be considered. this will ensure that responses are endorsed by physicians and stringently overseen by reliable authorities."
2417,"limitations as the computation complexity of knn is o(n2), when the size of a dataset gets really big, speed becomes one of the limitations of our method. multithreads and multi-processes can greatly boost the speed. lempel-ziv jaccard distance (lzjd) (raff and nicholas, 2017), a more efficient version of ncd can also be explored to alleviate the inefficiency problem. in addition, as our purpose is to highlight the trade-off between the simplicity of a model and its performance, we focus on the vanilla version of dnns, which is already complex enough compared with our method, without add-ons like pretrained embeddings (pennington et al., 2014). this means we do not exhaust all the techniques one can use to improve dnns, and neither do we exhaust all the text classification methods in the literature. furthermore, our work only covers traditional compressors. as traditional compressors are only able to capture the orthographic similarity, they may not be sufficient for harder classification tasks like emotional classification. fortunately, the ability to compress redundant semantic information may be made possible by neural compressors built on latent variable models (townsend et al., 2018)."
2418,limitations a limitation of this work is that the dataset has not yet been thoroughly vetted by native speakers of the languages contained in the dataset. we acknowledge the importance of working with native speakers and manually reviewing datasets in greater detail as argued for by kreutzer et al. (2022) and lignos et al. (2022). we hope to do more manual review of lr-sum and other summarization datasets in the near future.
2419,"limitations the paper focuses on the vqa task only, we will extend our method to other multimodal tasks in future works, e.g., referring expression comprehension (rec). moreover, although our ddg outperforms most of the state-of-the-art methods, the performance is still a long way from humans."
2420,"limitations we raise a new challenge task in our medical dataset ccs. comparing with existing datasets, ccs requires text-to-sql models to generalize to different databases with the similar structure in the same domain. to tackle this problem, we provide a baseline method named syntactic role prediction, which is an auxiliary task and can be combined with the main task in a multitasking way. our experiments prove that srp can help improve the cross-schema generalization ability of models. however, the improvement is not that large. how to generalize models across different databases sharing the similar structure is still a challenge issue. we expect that future works can solve this difficult problem."
2421,"limitations while we already manage to outperform the baseline, the pre-training data quantity is relatively small (∼20k instances). given the computational cost of training 30 models—six train sets, over five random seeds each—and testing them within inand cross- domain setups, we break the inspection of the optimal pre-training data amount at 24k instances. however we do not exclude that more pre-training instances would be even more beneficial for improving even more over the baseline. related to computation cost constrains, we test our syntax pre-training approach over one set of ud labels only (nsubj, obj, obl, nmod, appos). different sets could be investigated, e.g. including acl and compound, which present a lower, but still considerable amount of instances (see figure 3). finally, while approaching re by assuming that the gold entities are given is a common area of research, we leave for future work the inspection of the proposed method over end-to-end re."
2422,"limitations comparing downstream performance of pretraining objectives with large-scale models is prohibitively expensive. because of this, we employ scaled-down models that closely resemble the architectures and training procedures of popular plms. in doing so, we assume that our findings are transferable to some larger publicly available models. as noted by hazarika et al. (2022), ctxaug offers an interesting alternative to prompting generative lms that are significantly smaller than those that typically exhibit few- and zero-shot capabilities (brown et al., 2020). while we provide support for both hazarika et al. (2022)’s claim and our assumption in preliminary and supplementary experiments with select plms (see section 1 and appendix b), these experiments are still performed on models of up to 140m parameters. therefore, we stop short of concluding that our findings generalise to llms, which dwarf these models in comparison. additionally, the number and types of target attributes that a user may want to control for in various downstream text generation tasks are potentially endless. however, our study focuses on only two possible target attributes, namely, inquisitiveness and positive sentiment, for the task of conversational dialogue modelling. in this way, our work partially serves as a re-implementation and reproduction study, confirming the main findings from hazarika et al. (2022), but also highlighting limitations."
2423,"limitations this paper examines the anisotropy and outlier phenomenon only for a few, relatively similar, models. the isotropy-increasing transformations are nondeterministic and have to be calculated post-hoc based on some set of embedded data, which may not be practical for applications where inference is done on individual or small batches of examples. since we specifically consider sentence representations, we first average over word embeddings before calculating the mean and standard deviation for outlier analysis. this in effect reduces the sample size and leads to a smaller standard deviation, making our analysis more sensitive to even slight outlier dimensions. another reason to work with relatively small datasets is to make computing the transformations simple and fast, but this may limit the ability of these transformations to generalise."
2424,"limitations we enumerate some limitations to our work. while we did create the largest union dataset to date, it is still of moderate size. as shown by our learning curves (app. g), the amount of training data we created seemed sufficient to saturate the learning of the models with which we experimented, but it might still be found insufficient for training other models. our annotation protocol might have influenced the compression rates of the unions, as we instructed workers to annotate sentence unions by first choosing a base sentence and then highlighting the other sentence. additionally, while the highlighting facilitates the annotation process, it cannot directly be used for analyses of the dataset since it is uni-directional. the dataset includes only input with exactly two sentences and it might be desirable for future works to also be able to train systems that take more than two sentences as input. our dataset is also domain specific, in that all the sentences are taken from news sources. this might result in challenging cross-domain generalization. this dataset is limited to the english language. while the suggested annotation protocol seemingly fits other languages, the step in which words are highlighted might prove problematic for morphologically rich languages, in which a single word includes many pieces of information. a segmentation of the text before annotation might be required."
2425,"limitations in our work, we use only one solution from the llm to distill information into the student model, and according to wang et al. (2022), multiple subquestion-solution pairs can be sampled, and using majority voting, all pairs leading to the most frequent answer can be used to distill knowledge into the student models. also, due to computational budget, we used a single prompt to compare the cot and socratic cot and using more prompts (up to 8) might lead to a fairer comparison and better results (wei et al., 2022b). we leave these experiments for the future. ethical considerations although this work improves the reasoning capabilities of smaller models, the models are still not powerful enough to be used in sensitive settings such as education. we plan to release our code and model checkpoints, but the models must be used carefully by users, as many generative models, including ours, are prone to hallucination."
2426,"limitations while hrdsattack is, to the best of our knowledge, the first dataset on extracting attacks on human rights defenders, there are some limitations. for one, while being the first corpus of its kind, our dataset is english-only. second, the number of documents is limited. while the sample size of hrdsattack (500) is on par with some of the other ee datasets, such as ace05 (599), we do see more samples being beneficial to subsequent model training and supporting other future studies. in addition, despite the effort to balance the class labels in the event attributes, some of the labels still remain imbalanced, such as perpetrator type."
2427,"limitations the approach of language model integration for neural machine translation is analyzed and compared to the de-facto standard method of backtranslation. due to constrained resources, this work has several limitations. we focus on translation of text in a single domain, namely news-articles. different domains might exhibit different behaviour. for the back-translation experiments, we use beam search to create the synthetic data, other methods like sampling were not considered. when combining the synthetic and real parallel data, there are additional methods like tagging and block-wise batching, which we did not utilize in this work. finally, we compare against the most commonly used lm fusion approach, i.e. shallow fusion. there exist other lm fusion techniques which might exhibit different behaviour when used in combination with ilm neutralization."
2428,"limitations a limitation of the proposed method is that our gazetteer is constructed only by dataset annotations. and it affects the gazetteer coverage in unseen cases. following previous work, such as lin et al. (2019) and fetahu et al. (2022), we will construct a larger gazetteer using external resources such as wikipedia or knowledge bases. as mentioned in section 3, we will leave this for future work. another limitation is that the gazetteer contains many spans that are associated with multiple entity types. taking the running examples in section 3.1 for example, the span ""london"" has type locationgpe in most cases, while it is sometimes labeled as type art-music. however, in our current design, given a named entity, there is no way to explicitly distinguish between different types. in future work, we will consider the context of named entity when fixing errors."
2429,"limitations in experiments we only take plms into account because of their prevalence, hence the transferability to non-pretrained models is still unknown. however, due to the generality of plms, this can be a minor point in practical scenarios. moreover, although we successfully transfer adversarial attack methods in cv to nlp using a unified framework, we only instantiate the framework with the pgd attack as an example. it would be interesting to transfer more attack methods in cv and conduct a comprehensive analysis of what methods can benefit nlp, aiming to have a deeper understanding of plms. ethical consideration in this section, we discuss the potential broader impact and ethical considerations of our paper. intended use. in this paper, we design a general framework to adapt existing gradient-based methods in cv to nlp, and further, propose a decisionbased textual attack method with impressive performance. our motivations are twofold. first, we attempt to introduce adversarial attack methods of cv to nlp, since image attack methods have been well-explored and proved to be effective, therefore helping these two fields better share research resources hence accelerating the research process on both sides. second, we hope to find insights into the interpretability and robustness of current blackbox dnns from our study. potential risk. there is a possibility that our attack methods may be used maliciously to launch adversarial attacks against off-the-shelf commercial systems. however, studies on adversarial attacks are still necessary since it is important for the research community to understand these powerful attack models before defending against these attacks. energy saving. we will public the settings of hyper-parameters of our method, to prevent people from conducting unnecessary tuning and help researchers quickly reproduce our results. we will also release the checkpoints including all victim models to avoid repeated energy costs."
2430,"limitations in this section, we discuss the limitation of the proposed model. currently, the hidden layer of the two neural networks for drift and diffusion are shared between all entities and relations. this might cause over-fitting on relations that show simple structures if the neural networks are set to be very deep. on the other hand, if the neural networks are set to be shallow it might negatively influence modeling complex relations as the direction of trajectories will be limited. one possible solution is to cluster relations based on complexities and use separate neural networks for each cluster depending on the complexity of the corresponding relation. this requires, however, prior knowledge about the structure of different relations which we leave as future work."
2431,"limitations in this paper, we leverage amr to the gec model as external knowledge, and achieve a high f-score on single model. however, we do not use r2l reranking, model ensemble and other methods to ensemble single model and compare them with state-of-the-art ensemble models. our aim is to provide a strong baseline for incorporating amr in gec, so it is easy to generalize amr-gec to ensemble models."
2432,"limitations there are three main limitations of our approach: (1) our model cannot handle negation operation. enabling bidag to support negation operation is a direction for future work. (2) the modeling for query representation and logical operators is too simple. improving bidag by more ingenious modeling for query representation and logical operators is also a direction for future work. (3) the training process cannot be parallelized well, which is a common drawback of qe models, as qe models have to predict node representations one by one."
2433,"limitations due to time constraints, we were unable to implement some unreleased models as baselines for the proposed tasks. we did not conduct simile interpretation/generation on msd-ch in this paper since we could not automatically annotate the shared property in chinese data like the ""as...as"" mode in english. we are currently working on this annotation and plan to release the chinese simile interpretation/generation results on the data link. the coarse/fine version data we introduced in this paper can still be used for enlarging the msd data. we will study to utilize them for more simile data and richer language phenomena."
2434,"limitations despite achieving superior performance, our proposed method requires manual selection for hyperparameters to decide the number of tasks, i.e., the number of virtual relations v and the number of synthetic tasks nv for each virtual relation. in future work, we target developing the method with automatic adjustment to add/remove virtual rela- tions and the corresponding tasks according to the status of the few-shot learner with the training going on by curriculum learning. besides, although we adopt a task selector to adaptively select beneficial tasks, it is still inevitable to bring noisy tasks in the meta-training stage. we will explore the strategy to achieve better denoising."
2435,"limitations although our model has made some progress, it still has some limitations. first of all, sgt uses the tag type to represent the connection order of glcs fragments when forming a complete utterance, and the average statistics on the three datasets we used show that more than 99% of the complete utterance can be composed with less than three glcs fragments. that will lead to situations that need to combine multiple glcss (e.g., more than 3) to form a complete utterance, which cannot be fully trained or fall into unbalanced tag categories. second, like other tagging-based models, the fragments that make up the complete utterance must exist in history utterances or connection words, which does not work well for situations where it is necessary to combine context information and introduce new words to express their complete utterance."
2436,"limitations main limitation 1 the experiments of this paper are only done in 14 languages that use the latin alphabet, and with a high share of indo-european languages, with up to 4 germanic languages. this is due to two reasons: (i) the scarcity of xpos and feats annotations in treebanks from other language families, and (ii) the research team involved in this work did not have access to proficient speakers of languages that use other alphabets. hence, although we created a reasonable diverse sample of treebanks, this is not representative of all human languages. main limitation 2 although we follow previous work to automatically generate perturbations at character-level, and these are inspired in psycholinguistic studies, they might not be coherent with the type of mistakes that a human will make. in this work, generating human errors is not feasible due to the amount of languages involved, and the economic costs of such manual labour. still, we think the proposed perturbations serve the main purpose: to study how morphological tags can help parsers when these face lexical errors, while the used method builds on top of most of previous work on adversarial attacks at character-level."
2437,"limitations while we aim for our hegel crowdsourcing methodology to be applicable to other languages, and in particular low-resource languages, the ui design and our analyses require knowledge of the intended language, as well as familiarity with the regions where it is spoken. moreover, as our methodology relies on people’s familiarity with the places, it limits the cities chosen for the task and the participants that could take part, restricting the demographics of the participants accordingly. in addition, relying on people’s memory of the environment causes many of the descriptions to be too vague for humans to geolocate, thus, many of the descriptions were disqualified during the validation process as they could not have been resolved. the relatively low percentage of place descriptions that were successfully validated, raises the costs of collecting such a dataset."
2438,"limitations in this section, we discuss the limitations of this work. first, on the problems side, it’s non-trivial to consider the order of all kinds of grounding knowledge, but we have only explored persona-chat. we hope to apply our method to more grounded generation tasks such as knowledge-grounded and document-grounded dialogue in the future. second, on the methods side, our framework is trainingbased, but we hope more lightweight techniques could be developed to improve the model’s robustness even without training the model."
2439,"limitations. but any llm distillation will show a similar trend. existing kd methods are highly customized to the specifics of the teacher model, and require additional pretraining, fine-tuning, or data augmentation. our approach is much simpler and agnostic to both architecture and task. we ran our experiments on an rtx3090 gpu with 24gb ram which cost only $0.11 an hour, which is considerably cheap compared to other approaches that include teacher fine-tuning. we showed that our method is particularly effective on small datasets, and competitive with other kd methods which are much more computationally intensive and tailored to the teacher. a possible reason could be since the fine-tuning of bert on small datasets like mrpc, stsb, or rte can be unstable (zhang et al., 2020), eliminating it makes the kd more robust and improves the results. all other methods such as tinybert (jiao et al., 2020) or patientkd (sun et al., 2019b) use fine-tuned teachers. distilbert (sanh et al., 2019) does not use a fine-tuned teacher, but it is only limited to students with a hidden state of 784 due to the cosine loss it uses and lacks generalization across architectures."
2440,"limitations we train the task-optimized adapters based on the pre-trained weights of dialogue lm. therefore, if applied to other dialogue tasks such as chit-chat and conversational qa system, the performance could be lower than that shown in our research. and we need future works to clarify the reason why the performance was better on the multiwoz 2.2 dataset, which is expected that our model does not overfit to the confused labels. our model inferences in on the end-to-end manner, but trains like modular system for each task. end-to-end learning is currently under study. we could adapt the multitask end-to-end learning to our method, which may lead to the better performance. also, we could analyze the inner working of task-optimized adapters applying xai technologies."
2441,"limitations of image generation but also provides the data necessary to improve visual metaphor generation in the future. we plan to further examine the effect of prompt phrasing on the quality of the generated visual metaphors,and how that effect differs across different models. limitations while the results of human-ai collaboration for visual metaphor generation are very promising, such a procedure might be time-consuming but at the same time necessary for maintaining quality. we want to acknowledge that both our llm and bestforming diffusion models are released through a paid api and are not open-sourced. while our best-performing system uses chain of thought prompting, there are several other prompting or task decomposition techniques that we did not perform an extensive comparison with.last but not least, there is still enough room for potential improvement in generating visual metaphors which can be achieved by designing better prompts or by improving the compositional generalization of diffusion models. we also recognize the inherent limitation of an english-only basis for our visual metaphors and hope in the future to expand to other languages for source material."
2442,"limitations to address the low-resource data in the training of tau-dr, we apply two heuristics, dataset enrichment and generation from different checkpoints. despite being effective, they require additional computational time that might be challenging in applications with low-computational resources. a possible approach to reduce the computational time might be to average the checkpoints. we believe that this might lead to competitive results, with a significant reduction in computational time, since checkpoint averaging proved to be an effective approach in low-resource settings. another limitation is when the original dataset is in a highlyspecialized domain that might contain domainspecific phrases that were most likely not included in the pre-training data of the language model. the results obtained by existing data augmentation approaches will most likely exhibit only marginal improvement."
2443,"limitations we showed that our proposed method can greatly improve the performance of parameter efficient tuning on diverse nlu tasks and three different pre- trained models (i.e., roberta-large, debertalarge and gpt2-large). however, we acknowledge the following limitations: (a) the more super-sized pretrained models with tens of billions of or more parameters were not studied due to limited computation resources. (b) other tasks in natural language processing, like the text generation tasks, were also not considered. but our framework can be easily transferred to other backbone architectures and different types of tasks. it would be of interest to investigate if the superiority of our method holds for other backbone models and types of tasks. and we will explore it in future work."
2444,"limitations apart from the issues mentioned in § 5.4, another limitation of trenc is that it does not integrate any pre-training process such as bert, which is effective in increasing the language understanding ability and adopted by previous works focusing on token-level classification tasks (wang et al., 2022; deng et al., 2022). two factors lead to this decision. first, we use dom nodes instead of tokens as the classification object and focus on relations between nodes rather than tokens. as the node text sequence is a composition of an arbitrary number of tokens, adopting the conventional masked language modeling (mlm) training objective (devlin et al., 2019) seems impractical since there is no direct mapping from an embedding vector, one-hot encoded or not, to a sentence. the second reason is simply that we do not possess the corpus or computation resources for model pre-training. in fact, we expect a properly designed pre-training scheme to bring better node semantics representation and si-pt relation modeling. it is an interesting topic and deserves further study."
2445,"limitations we see two main limitations of this work. the first one concerns the diversity of the language models and datasets used. bert, distilbert and roberta have similar architecture, and sst2, imdb and rotten tomatoes are datasets designed to evaluate the sentiment of english text. it would therefore be interesting to validate the robustness of our results on more diverse languages, tasks and language models. in this short paper, we decided for brevity to follow the experiment design of sanyal and ren (2021), while being aware of its inherent limitations. the second limitation of this work concerns the time complexity of sig. as it needs to compute explanations for each word individually, this method can become very computationally expensive when applied on large text data. to alleviate this issue, we first made it possible to compute gradients in parallel, using an internal batch size similar to how captum (kokhlikyan et al., 2020) implemented the integrated gradients method. secondly, as discussed in 3.2, it is possible to reduce the number of interpolated points, which makes the computation faster while retaining better performance than the original ig. in this work, we ran our experiments on a machine with 16 cpus, and one nvidia tesla t4 gpu. with this setting, computing sig on sst2 and rotten tomatoes takes around one hour for each model. on the larger imdb, computing sig, on 2000 randomly sampled inputs, takes around 5 days for bert and roberta, and 2 days for distilbert."
2446,"limitations and future work one limitation of our method is that sampling requires sampling both a conditional and a unconditional model, which results in slower inference times. on the other hand, progressive distillation (meng et al., 2022) provides an attractive solution to this problem. another limitation is that ho and salimans (2021) show that the diversity of generative models is degraded as w increases. ideally we would be able to have a model that improves upon the fluency as well as the model diversity. as for future work, we will leverage advanced large language models as the base architecture for training diffusion models to compete with high performance auto-regressive models. additionally, we will investigate modifications to diffusion models that are inherent to discrete data."
2447,"limitations as mentioned in sec. 2, the current selection of fundamental reasoning skills for language models is limited by the availability of well-defined tasks and clear definitions of those tasks, as well as the availability of sufficient training data. as a result, some skills may overlap or may not be fundamental enough. for example, simple qa skill may overlap with ner skill to some extent. in the future, it would be worthwhile to explore self-supervised training tasks that can inject more fundamental abilities into language models. additionally, the selection and combination of fundamental reasoning skills can be further explored. for example, the inclusion of numerical reasoning ability to solve mathematical problems. additionally, methods for skill-centric pre-training corpus construction can also be explored to improve the effectiveness of these skills."
2448,"limitations the primary constraints encountered in our research result from our dependence on a single dataset for experimentation and computing resource limitations. despite these, we postulate that our ranking-based methodology can be utilized for any summarization task that necessitates robust correspondence with a specific structure within the input. to validate this hypothesis, further experimentation is required to assess the generalizability of our technique to alternative datasets and domains. in addition, our limited computational resources prevented us from experimenting with other long document encoder-decoder models such as bigbird and longt5 (michalopoulos et al., 2022; guo et al., 2022) as well as using higher beam widths during decoding. furthermore, the cost and complexity of procuring expert evaluators within the legal domain resulted in using automatic metrics alone. ethical considerations the usage of the generated summary results from legal opinions remains important. abstractive summarization models have been found to contain hallucinated artifacts that do not come from the source texts (kryscinski et al., 2019; zhao et al., 2020; kryscinski et al., 2020). while our model incorporated the argument structure of the source article, the generation results may still carry certain levels of non-factual information and need to be utilized with extra care. similarly, as mentioned in the prior line of works using canlii (elaraby and litman, 2022; zhong and litman, 2022), canlii has taken measures to limit the disclosure of defendants’ identities (such as blocking search indexing). abstractive approaches may cause user information leakage. thus using the dataset needs to be cautious to avoid impacting those efforts."
2449,"limitations we would like to acknowledge the following limitations of this work. our study setup only takes advantage of supervised data in the form of triples of <speech, transcriptions, translations>. this is because we first and foremost want to investigate the effectiveness of pseudo-labeling in the most extreme case. however, the setup can be extended to be able to also rely on asr-only (<speech, transcription>) and st-only (<speech, translation>) pairs. we leave incorporating asr and st data as a future work as well as incorporating external language and machine translation models. we identified two sources of domain mismatch: input length ranges and vocabulary mismatch. however, the solutions that we investigate directly target the length mismatch, without explicitly addressing the vocabulary mismatch. the latter is indeed more challenging to address, especially without incurring additional supervision. in fact, circling back to the previous item as a future direction, incorporating supervision in the form of asr or st can expand the vocabulary set, also addressing vocabulary mismatch."
2450,"limitations the limiation of this work is that we only consider one type of prefixes/suffixes, i.e., negative prefixes/suffixes. in our future work, we would like to work on other types of prefix/suffix sense detection tasks, such as prefix/suffix sense detection on occupation. for instance, in english, there are many suffixes such as -or, -er, and -ee, which mean a person with a certain occupation."
2451,"limitations in this study, we propose differentiable segmentation to learn how to segment speech from the underlying translation model, and verify its effectiveness on simultaneous speech translation. however, since it can be jointly trained with the underlying task (sequence-to-sequence task), differentiable segmentation is not limited to the simulst task, but can be generalized to more streaming/online tasks, such as streaming automatic speech recognition (streaming asr), simultaneous machine translation (simt), real-time text-to-speech synthesis (real-time tts), online tagging and streaming parsing. given that there may be some task-specific differences between various tasks, this work only focuses on the differentiable segmentation in the simulst task, and we leave the study of how to apply differentiable segmentation to other streaming tasks into our future work."
2452,"limitations so far jgr has only been evaluated on the domains of summarization, conversational summarization, question generation, and dialog. it should be evaluated on a wider range of benchmarks, such as machine translation and code generation. and we have not explored jgr’s performance with extralarge language models such as gpt-3. we will evaluate jgr on the above points in the future. because the generator of jgr samples candidates using auto regressive sampling, it may occupy relatively longer computational time and larger memory then the conventional mle training. though the performance of jgr is satisfactory, we still want to improve its computational costs. we will try non-auto regressive sampling and other improvements such as parameter sharing in the future."
2453,"limitations in this section, we discuss some limitations of our work. we conduct preliminary experiments to verify the influence of task transfer and vocabulary expansion on language learning in complex forms, and to explore the effectiveness of our proposed architecture, symbolic mapping, and we assume that language was formed through simple interactions in the early stage. therefore, additional experiments involving more complex games or other input forms like real images have not been studied and are left for future work. besides, more advanced language properties and syntax are temporarily not studied in this work. as for task transfer, we verify the effectiveness of a two-stage curriculum starting from referential games, while more advanced curriculum are left for future work, where more cognitive science findings should be involved."
2454,"limitations first of all, our study is limited to languages that use the latin script. still, the 4 languages are from different language families and are typologically diverse. secondly, the low-resource scenario is simulated. as mentioned in 3, in order to carry out the experiments the languages involved were required to have enough monolingual data to train lms, as well as available evaluation datasets for nlu tasks. the source of the pre-training corpora for swahili and finnish (cc100) is not completely comparable with the corpora used for basque and spanish (75% news, 25% wikipedia), due to the unavailability of a large curated corpus for swahili, and the lack of big news corpora for finnish with an open license that allowed us to share freely the pre-training data. 16https://github.com/orai-nlp/low-scaling-laws our study is limited to 3 language model sizes and 3 pre-training corpora sizes. including other model sizes like a bert-large or a model between 51m and 16m (where there is a big gap in results), and adding more pre-training corpora sizes (let’s say 625m and 1m words) were out of the scope of this work. in addition, we use the default hyperparameters that are commonly used for bert-base (bert124m ) for the pre-training and fine-tuning of the bert51m and bert16m models without any hyperparameter tuning."
2455,"limitations this paper proposes a pre-trained language model with prompts for temporal knowledge graph completion. however, there are some limitations in our method: 1) our prompts in the temporal knowledge graphs, especially the time-prompts, are built manually. it needs to be reconstructed manually for different knowledge graphs. we are exploring a way to build prompts in temporal knowledge graphs automatically. 2) our model uses a random sampling method, which suffers from the problem of few high-quality training samples and high sam- ple noise. for future work, a more effective way to sample is worth exploring."
2456,"limitations although the proposed method provides interpretations for continuous prompts with both faithfulness and plausibility, it can still only be used as an approximation to find the most likely combination, since the process of combining discrete prompts to continuous prompts is irreversible. moreover, the output layer of plms tends to degenerate and occupy an anisotropic cone in the vector space (wang et al., 2020; li et al., 2020b), which significantly increases the difficulty of finding the correct interpretations. we encourage future research to take the magnitude of token vectors and the tokens in their neighborhoods into consideration for a more robust interpretation. due to space and time constraints, we only perform detailed experiments on p-tuning and the bidirectional language models like bert and roberta, which ignored numerous sota works such as prefix tuning (li and liang, 2021), prompt tuning (lester et al., 2021) for continuous prompts and gpt (radford et al., 2019), t5 (raffel et al., 2020) for plms. we encourage future research to conduct experiments on more prompt methods and plms to investigate the generalizability of our method. ethical statement we propose a novel view to interpret continuous prompts, which have been considered ""black boxes"", as combinations of human-understandable discrete tokens. since the method itself is unbiased and faithful, and all experiments are conducted on publicly available datasets, we believe that our work does not create any potential ethical risk. further, we discover shortcuts latent in continuous prompts, implying that systematic biases or discrimination may also exist in continuous prompts. these biases may originate from training datasets which are exploited by continuous prompts as a shortcut to the acquisition of true labels, or even originate from artificially implanted backdoors. we hope this work will provide the possibility to detect these potential biases in continuous prompts. our created artifacts are intended to provide researchers or users with a tool for understanding decision-making and detecting possible unexpected shortcuts of continuous prompts, while at the same time offering the feasibility of cross-model transfer without extra training signals on target plms. they are compatible with the original access conditions. all use of existing artifacts is consistent with their intended use in this paper."
2457,"limitations all our experiments are performed using the bertsmall language model due to the computational requirements of generating and testing models considering many configurations of adversarial training and attack methods. although using larger language models might have provided different performance measurements, our findings that compare input- and embedding-space adversarial training methods are expected to remain unchanged. another limitation of our work is the semantic gap between attacks in input and embedding space needs further research. specifically, how do perturbations in the embedding space get translated in the input space? finally, other forms of robustness techniques, besides adversarial training, in the context of large language models require examination."
2458,"limitations gdmm establishes a compelling starting point for dmel research. in spite of this, the proposed approach has several shortcomings. first, gdmm currently generates entity name within the entity candidate set, however, we saw how retrieval errors limit entity linking performance. thus, how to work collectively with the retrieval system to diminish errors takes appropriate action. second, how to handle large tables still remains under-explored. it is infeasible to represent a huge database with the table flattening technique. in practice, it is possible to filter out less likely candidates to compress the search space, but a more promising approach is to represent the table more efficiently. gdmm also enables studies on more diversemodal tasks. new tasks can be easily framed based on the proposed architecture, such as visual question answering, grounded generation, and diversemodal commonsense reasoning. we believe that with more follow-up work on diverse tasks, this approach will turn out to be a more comprehensive generative diverse-modal framework."
2459,"limitations one limitation in this work is the metrics employed in the automatic evaluation. the metrics mainly focus on the quality of generated response and the accuracy of emotion recognition, while automatic evaluation lacks a comprehensive method to evaluate empathy. another limitation comes from the utilization of the dataset designed for open-domain dialogue system, so that the generated response from the proposed framework is not task-oriented. in the future, we will build empathetic dialogue generation datasets with diverse and task-oriented response, and develop metrics to evaluate the understanding of the speaker’s situation."
2460,"limitations the loss objective of the proposed siwcon regularization is computed on augmented data, which increases the time required for the model to complete training. we evaluate siwcon on classification tasks, but it may be applied to various other tasks, such as reading comprehension and textual entailment. more evaluations are expected to be done in future works. the proposed siwcon regularization is effective in defending against word-level adversarial attacks, as the basic elements of the augmentation methods are words. however, similar regularization techniques can also be applied to characters and sentences, and we leave evaluating the effectiveness of such variants in future works."
2461,"limitations resulting from the use of gpt-3 discussed above, there are more general ethical questions surrounding the use of gpt-3 and similar models, for example the high energy usage and resulting carbon emissions, and societal questions around the oligopoly on state-of-the-art language models that is currently in the hands of a handful of large us-based companies. the second consideration relates to the task that we introduce: while we see perspective transfer models as a valuable tool for studying how language ‘frames’ (social) reality that could also have practical applications, for example in journalism, we strongly believe that any such applications must be approached with extreme care. the models that we introduce are scientific analysis tools that could be used to suggest alternative viewpoints on an event, but we believe that generations should not be seen as necessarily reflecting a ‘true’ or ‘better’ perspective, and should not used in a prescriptive way (i.e. used to tell someone how to write). we believe that the authors (journalists or others) of any text ultimately bear exclusive responsibility for the views, perspectives and (implicit) values expressed in it, and should be careful in making use of texts (re-)written by computers, such as the ones produced by our proposed models. finally, we are aware that our task domain (femicide/gender-based violence) is a societally and emotionally loaded topic, and that the texts contained in our dataset and produced by our models might be disturbing. in particular, in some cases, models may produce graphic descriptions of violence and/or produce questionable moral judgements (e.g., we have occasionally seen statements such as “the perpetrator of this horrible crime does not have the right to live” spontaneously produced by some of the models), and potential users of applications of the model should be aware of this. for the purposes of this paper, the only people external to the research team who have been extensively exposed to model outputs were the annotators in our human evaluation study. in the introduction page of our online questionnaire, annotators were warned about the sensitive nature of the topic and advised that they could stop their participation at any time if they felt uncomfortable and could contact the authors with any questions. prior to running the online questionnaire we have requested and obtained ethical approval by the ethical review committee of our research institution. author contributions authors g.m. and h.l. share first co-authorship (marked with ‘*’). g.m. had primary responsibility for data collection and preparation, setting up the gpt-3 experiments and running the human evaluation survey. h.l. had primary responsibility for the mbart experiments and the automatic evaluation. b.m. annotated data (pair alignment) and contributed to prompt engineering and the design of the evaluation questionnaire. m.n. coordinated and supervised the overall project."
2462,"limitations explored above, we identify several potential risks with this paper. some may be offended by the images we include. we tried to mitigate this risk by including a warning in the abstract and not including images featuring genitalia. however we appreciate these images may contribute to the sexualisation and objectification of non-cisgender people, particularly if taken out of context. though we did not set out to generate offensive images (this would be counter to the models’ intended use, for example as specified by dayma et al. (2021)11), images from the full data set could 11https://huggingface.co/dalle-mini/ dalle-mega similarly offend and even be weaponised. they might accompany transphobic messages online. a data set of cisgender and non-cisgender images labeled by photorealism and presence of a clear face could feasibly be used to finetune a model to identify non-cisgender people (a concern raised by the community). as such, we make our image data set available only upon request; it is intended to measure the harm done to non-cisgender people, not contribute to it."
2463,"limitations our comix approach assumes availability of parallel bilingual (embedded and matrix language) corpora and mature tools for pos tagging and phonetic transcription for both the embedded and matrix languages which does not hold true for every language. but these assumptions are reasonable for a large number of languages as shown in appendix 7. second, our current choice of guiding function for attention fdkga and mixing probability pmix are based on limited knowledge of the linguistic structure specific to english and indic languages, and might need to be adapted for other language families. additionally, as discussed in section 4, due to multiple variations in code-mixed generation, current automated metrics that compare system generated text with reference text do not provide a true reflection of a system’s ability to generate code-mixed text. lastly, as with large language models, our comix models are also vulnerable to biases inherent in the training corpus."
2464,"limitations, future directions and"
2465,"limitation section (section 9), smarter ways of multilingual s2st architectures are preferred in the future to reduce the cost of energy from current quadratic to linear number of models. generally, s2st circumvents traditional cascaded systems which concatenate asr, mt and tts with high latency and high requirements of datasets. there are 3,000 around languages in the world who do not have their own writing systems or textual vocabularies. through our duplex s2st models, we hope to be friendly to these languages so that more and more languages can be covered."
2466,"limitations in this section, we illustrate the limitations of our method, which could be summarized into the following two aspects. firstly, since the cumbersome data annotation leads to few publicly available datasets of idrr tasks, we only conduct experiments on english corpora including pdtb 2.0 and pdtb 3.0. in the future, we plan to comprehensively evaluate our model on more datasets and datasets in other languages. secondly, considering that instances of pdtb are contained in paragraphs of the wall street journal articles, our approach ignores wider paragraphlevel contexts beyond the two discourse arguments. as shown in (dai and huang, 2018), positioning discourse arguments in their wider context of a paragraph may further benefit implicit discourse relation recognition. it is worth exploring how to effectively build wider-context-informed discourse relation representations and capture the overall discourse structure from the paragraph level."
2467,"limitations although the proposed prequant achieves promising results especially in reducing the storage and computational resources, we discuss some limitations of our work in this section. in our experiments, we observe that the performance of prequant is highly correlated with the data size. when fine-tuning with very limited data, prequant may not meet expectation to preserve the performance of plms. moreover, our model performance also depends on the number of parameters (i.e. outliers) restored in the fine-tuning stage. this hyperparameter controls the trade-off between model performance and parameter efficiency. the optimal choice of the hyper-parameter for different tasks requires further investigation. additional"
2468,"limitations while we strongly argue against empathetic conversational systems, there may be use cases – such as psychotherapy or educational chatbots – where validating a user’s emotions is, if not required, helpful in terms of their goal. in addition, while a lot of the work on empathetic responses we have discussed is intentional, generative models like chatgpt produce relatively uncontrolled responses that may well be unintentionally empathetic. as with toxic outputs, care should be taken to prevent these models from validating users’ emotions that cannot be understood."
2469,"limitations one of the limitations of this work is that we restrict ourselves to examining datasets for supervised learning that contain relatively short instances of text. this likely facilitated the reweighting of data that we wished to perform as an intervention to produce the reweighted data that we study, as the short length of each text effectively capped the number of different lexical features that could cooccur in the same instance. the results we present here might not be representative of lexical feature bias in data with much longer units of text. also, the fact that the datasets that we used are all in english means that our lexical features were premised on simple whitespace tokenization with punctuation removal; for other languages with a larger variety of reasonable tokenization schemes at varying levels of granularity, the distribution of lexical features, and the resulting"
2470,"limitations while the results of our experiments seem sufficient to validate the concept and our general approach to bilingual distillation, we have not carried out a detailed systematic analysis of alternative implementations of the various aspects of our methods, such as different student model initializations, distillation objectives and hyperparameter settings. furthermore, our bistil models are likely undertrained due to limited computational resources. consequently, we do not claim our specific implementation of bilingual distillation to be optimal or even close to optimal. areas that warrant further investigation toward realizing the full potential of this approach include the use of hidden dimension reduction, which yielded impressive speed gains for minilmv2 in our experiments, and other innovations in distillation such as progressive knowledge transfer (mukherjee et al., 2021). with the exception of improved efficiency, our bistil models inherit the limitations of the mmts from which they are distilled; notably, there is a discrepancy between the performance on high- and low-resource languages resulting from the distribution of data used during mmt pretraining. in this work, we have only considered english as the source language; some target languages may benefit from other transfer sources. future work may also consider the use of multi-source transfer, which would entail distilling with more than two languages. here the challenge would be optimizing the balance of model capacity allocated to source languages versus the target language."
2471,"limitations aside from the reverse models used in backtranslation (which we did analyze in section 4), we only studied translation of language pairs into english. using data augmentation techniques like back-translation where english is not the target language, or is neither the source or target language is certainly worthy of study, but was out of scope in the present work. we did however, include many source languages that are typologically different from english (see table 8 in the appendix). in order to study the effectiveness of bt in a large number of languages we relied on extant multilingual datasets, namely flores101 and tico19. the direction of human translation when building these datasets was from english into another language. we did not run repeated trials on our experiments. many models required training for a couple of gpu-weeks on v100s, and additional trials would have added significant computational expense. we believe the trends we have identified are sufficiently clear and supported by the statistical analysis in section 4."
2472,"limitations though our proposed method outperforms current state-of-the-art methods, there are still many challenges we should overcome in future research. first, for colloquial expression which confuses current dependency tree parser, we should come up with new solutions. second, emotional prediction of tweet posts describing current issues needs external knowledge, which is absent in existing research."
2473,limitations our experiments demonstrate that it is possible to analyze company executive and analyst language during earnings calls and use it to predict future earnings surprises with reasonable accuracy that is well above random chance. we acknowledge that the dataset contains events that result in significant (in magnitude) earnings surprises so the performance numbers do not directly translate to a live trading setting in which many events do not result in material surprises. we also note that predicting future earnings surprises is correlated with but not equivalent to predicting future stock returns so more work must be done to translate our results into an actual trading strategy that is out of the scope of this paper.
2474,"limitations while we carried out our research in four language pairs (in both directions), we recognise that these are mainly european languages and each pair is from or into english. the choice of language pairs was limited by the data and evaluation tools we had access to, however as our methods are languageindependent, this research could be expanded to other pairs in the future. another limitation is that the work was conducted in one domain (tv subtitles) and it remains for future work to investigate whether similar benefits can be achieved in other domains, though the findings within language modelling with cue in novotney et al. (2022) who used a different domain suggest so."
2475,"limitations as we discussed in section 5.1, further work is needed to investigate whether the negative effect of full contextualization beyond static + positional embeddings at the input layer is an idiosyncrasy of the embedding transfer procedure, or if this is a true effect. in future work, an experimental setup that is closer to the training setup, such as masking only the recon token instead of all tokens and transferring the source could be adopted, in order to reduce the noise potentially introduced by the distributional change in the inputs. regardless, we believe that findings regarding the information content of the representation at the input layer (static + positional embeddings) are novel and meaningful, and the quantification method we propose for comparing two representations in terms of their predictive utility is a generalizable methodological contribution. we furthermore note that our attempts to conduct evaluation on newer masked language models were made challenging due to several technical issues in the library (e.g., masked language modeling being unavailable in deberta (he et al., 2021): https://github.com/huggingface/ transformers/pull/18674)."
2476,"limitations and advantages of both methodologies for the two scenarios. according to our results, the supervised models show good id performance at the price of a significant drop in ood performance. in contrast, unsupervised zero-shot systems excel in ood settings but do not outperform supervised models in id contexts. a reasonable compromise between the two methodologies is the nli-fine-tuned method, which improves odd results compared to supervised systems and achieves good performance compared to the zero-shot approach in an id setting. in a situation where limited training data are available, the fine-tuned nli system has the advantage of achieving a good trade-off between id and ood performance, with less training data than supervised models. using a zero-shot nli-based system is preferable in situations where the final data distribution is unknown. furthermore, it requires less implementation time and no training dataset. our experimental analysis is not without limitations. we focused on emotive content (emotion classification and hate speech detection), therefore our results might not be extendable to other domains. emotions have a certain degree of subjectivity that can affect the annotation process by making data annotator-dependent. in other fields, this might not be the case. moreover, our analysis is limited to ten datasets that we believe are representative of the work in this field. however, many other datasets are available, especially in the hate speech domain, and a wider evaluation might lead to a more definitive"
2477,"limitations although we carefully designed open-wikitable for complex open-domain table qa, there are some limitations since it is based on the existing datasets. first, ambiguous or erroneous samples from the original wikisql or wikitablequestions dataset may still lie in our training and validation set. as we mentioned in section 3.2, most of the equivocal samples were attributed to the ambiguity of the original question and excluded from the test set, but not removed. second, unlike semantic coverage of the questions is extended by decontextualization and paraphrasing, the coverage of the question remains in that the answer and logic to derive the answer in each question is the same. still, openwikitable demonstrates the potential for further research on open-domain qa over the table."
2478,"limitations there are two main limitations to the present work. first and foremost is the computational cost associated with the present experiments. we present here results and analyzed gleaned over 10 runs, 7 pretraining regimens, 8 rl gradient propagation variants and 2 data sampling approaches, for a total of 1120 models. while training any one of our models is cheap (less than 3 hours on a single a100 nvidia gpu), the total number of models may pose a challenge for future replication studies and comes at an environmental cost. this also prevented us from selecting optimal batch size, learning rate, and so on for specific setups—as described in appendix a, we set these values globally prior to running experiments. this may affect results and impact"
2479,"limitation should be further considered in the future. besides, restricted by our knowledge and efforts, the main experiments cannot cover all common tasks and all pre-trained models in nlp. relatively, the token- and sequence-level classifications have demonstrated attractive experimental performance on most nlp tasks. next, we also plan to extend w2cspace to more nlp tasks and find its more specific value."
2480,"limitations while fixed input prarameterization (fip) enables performing prompt-dependent tasks efficiently, there are limitations that need to be addressed in future work. in particular, the current fip methods cause task performance degradation. moreover, the computational cost needed for the injection of prompts and the storage required to store the parameters of every injected model have not been extensively considered. for example, when considering previous conversation history as the prompt to be injected in a long-term conversation setting, fast injection may also be a requirement for real-world application. updating or adding a relatively small number of parameters (hu et al., 2021; wang et al., 2021) may be a potential avenue for addressing the problems."
2481,"limitations while ice does not require fine-tuning on large amounts of data, it requires querying a powerful llm at inference time (we use gpt-3 for our experiments which has 175 billion parameters). this can be a pay-per-use model or an open-source model such as bloom. this makes a downstream system that uses ice reliant on an external dependency, which carries the risk of the external dependency failing. 3summeval annotations are all based on the source, and the src-to-hyp version of bartscore performs best across dimensions for this benchmark. we use this version for all dimensions, leading to identical scores. we format bartscore results unlike rouge-l because in theory bartscores can differ across dimensions for an arbitrary benchmark. relatedly, in this paper, we are limited due to monetary constraints in a variety of experiments we perform. for instance, we restrict ourselves to text summarization and use samples of benchmark meta-evaluation suites during some of our experiments. we leave the investigation of using ice for other dimensions and downstream tasks for future work."
2482,"limitations this paper mainly focuses on the query-focused meeting summarization(qfms) task. besides, we have explored the performance of the rankergenerator framework on the long-input summarization task. but the results do not show a significant improvement. although qmsum dataset is also faced with the long-input challenge, the qfms task only summarizes specific parts of the original text, so it can take these parts as the input. while the goal of the long-input summarization task is to generate an overall summary, which needs to have a global view on the original text. so we think the extract-then-generate framework is unsuitable for the long-input summarization task. the previous work summn (zhang et al., 2022) is more suitable for the long-input summarization task. in addition, the multi-stage approach has a performance disadvantage over the end-to-end approach. however, the computational complexity of the multi-stage approach is much lower than that of the end-to-end approach. the multi-stage approach can balance experimental performance and computational complexity. so it is worthy of exploration as well as the end-to-end approach."
2483,"limitations differences in experimental setup may make it difficult to accurately and fairly compare published results. for example, to prevent data leakage, we report validation performance at the end of training and do not perform early stopping. this is in contrast to most other papers which report peak validation performance. results reported for other methods are reproduced in the same learning environment as our method unless explicitly stated otherwise. this takes into account recent work demonstrating problems with fairly and accurately evaluating pet methods that use early stopping improperly (chen et al., 2022). although many pruning criteria exist in the literature, in this paper we only consider one pruning criterion. although not presented in this paper, experiments we conducted with various formulations of magnitude pruning did not produce better results. although prompt tuning is a popular pet method, we do not perform nas for prompt tuning to determine the most efficient positions for inserting prompt tokens into the input. pruning may or may not prove to be a successful strategy for this problem. other nas strategies exist in the literature besides pruning, such as evolutionary, reinforcement learning, and darts (liu et al., 2018). however, our pruning method seems to give a good trade-off between validation performance and computational expense."
2484,"limitations in this work, we focused on problems posed in the hand-crafted humaneval dataset (chen et al., 2021). a potential pitfall of a curated dataset such as humaneval is that the results may not generalize to real-world scenarios where developers often deal with more complex problems and code bases (e.g, code with multiple dependencies across multiple files). to address this limitation, we originally explored the use of datasets mined from github. however, our experiments indicated memorization issues (e.g., verbatim generation of solutions to niche problem), potentially due to the sample code already being included in the model training set(lee et al., 2021). in practice, high quality code deduplication required to avoid this specific limitation is challenging. work by allamanis (2019) find that the impact of duplicate code can be severe, sometimes inflating model performance scores by up to 100%. furthermore, in our early pilot tests, functions extracted in the wild were found to contain insufficient context (e.g. absence of docstring) for even expert human annotators and isolating functional tests is challenging without heavy curation. further research is therefore needed to understand how our findings might generalize to a wider variety of deployment settings as well as research on designing diverse evaluation datasets. in addition, future work may also explore the impact of problem difficulty on the observed results in our study."
2485,"limitations one drawback of the experiments presented here is the reliance on a constructed language. while we have tried to design a language that is as representative of natural language as possible, there may be additional statistical effects that are not taken into account. for example, it is unlikely that one language would capture all 29 phenomena presented here and that the process would be triggered enough times to produce a large enough corpus. how these findings extended to existing language corpora is an open question for future studies."
2486,"limitations while dims makes the cross-entropy based family competitive with alignment based variants, it still falls behind one some cases. moreover, dims can improve the performance of models trained on raw data, but the best performance is still achieved when dims is applied on distilled datasets. therefore, dims still depends on an auto-regressive model for the best translation quality."
2487,"limitations our method overcomes degeneracy in instructional videos under the assumption of the existence of textual instructional scripts describing the exact instructions of instructional videos. thus, our method is applicable to instructional videos having such recipe documents. however, we note that similar documents exist for various types of instructions other than cooking, such as topics in other datasets (alayrac et al., 2017), e.g., how to jump start a car, or change a tire."
2488,"limitations due to the lack of data, our dataset is not comprehensive since it only consists of tang poems. our model may not perform well on unseen data in other forms. we plan to update the dataset in the future continuously. our evaluation metrics and generation results for the machine translation tasks are not certified by experts in classical chinese, so the results and"
2489,"limitations our approach relies on the pre-trained language model performance. although using a graph neural network with the user-product graph helps improve the performance in sentiment analysis, the pre-trained language model still plays an important role in the task. if the pre-trained language model cannot obtain good results, it will affect the performance as discussed on the imdb dataset. furthermore, the graph density could affect the performance of gnnlm-gnns, as discussed in the experimental results. since gnnlm is built on top of gnnlm-gnns, gnnlm is also affected by the sparsity problem. as already reported, the density of the user-product graph on the imdb, yelp-2013, and yelp-2014 datasets are 0.06, 0.05, and 0.02, respectively. the greater the value is, the denser the graph is. comparing gnnlm with gnnlm-lm, we found that the improvements we could obtain on the imdb, yelp-2013, and yelp2014 datasets are 6.1, 5.0, and 4.8, respectively. the trend of improvement conforms with the density of the graph. therefore, if the user-product graph is very sparse, it would greatly affect the performance of gnnlm."
2490,"limitations on the unknows. such efforts will lead to more accurate and reliable responses from llms, which will have a positive impact on their applications in diverse fields. limitations • generalization of reference sentences. at present, we have selected sentences with uncertain meanings exclusively from the gpt-3 and instructgpt series, potentially overlooking uncertainty present in responses generated by other llms. however, it is not feasible to catalog all sentences with uncertain meanings exhaustively. as a direction for future research, we propose to concentrate on the automated acquisition of more accurate reference sentences to address this concern. • limitations of input forms: our examination was confined to three unique input forms: direct, instruction, and icl. there is burgeoning research aimed at bridging the gap between models and human-like methods of reasoning and problem-solving, including but not limited to approaches like reflexion (shinn et al., 2023), tot (yao et al., 2023), mot (li and qiu, 2023). future endeavors will integrate additional cognitive and decision-making methods to delve deeper into the self-knowledge exhibited by these llms."
2491,"limitations our work has two limitations. first, we update the cluster centroids at each step during training, which requires a large mini-batch to maintain clustering accuracy and consumes more gpu memory. second, our method still may not identify false negatives accurately, as we use the training model for coarse-grained clustering rather than a well-trained model. we leave the improvement of memory consumption and further improving false negative discrimination for the future."
2492,"limitations this paper proposes five novel contrastive losses for multi-label text classification tasks. however, our method has the following limitations: 1. we only selected the multi-label emotion classification task and multi-label news classification as the representative of the multi-label text classification tasks. 2. we only conduct experiments on the single modal of text, and have not extended to multimodal tasks. 3. our method chooses the spanemo model as the backbone, lacking attempts to more models."
2493,"limitations of the existing methods, thus improving the accuracy of the kb-rec task. in future work, we will explore more fine-grained information in expressions and combine it with knowledge and visual content. limitations to better understand the limitations of the proposed method, we conducted an error analysis by randomly selecting 100 incorrect predictions and categorizing their error types. the results revealed that 32% of errors were caused by grounding issues, specifically, an inability to distinguish between multiple objects of the same category, despite having knowledge category of the referent object. the results indicate that there is a need for improvement in the ability to discriminate visual objects, especially for object categories with long-tailed distributions. additionally, the results show that 20% of errors are due to imprecise object detection, particularly for small objects. this highlights the need for optimization of the visual encoder and loss function. moreover, 14% of errors are attributed to incorrect knowledge retrieval. to address this, incorporating more fine-grained information in expressions for retrieval should be considered as a future research direction. furthermore, 34% of incorrect predictions can be attributed to issues with the ground-truth annotations, which may negatively impact the model’s learning process."
2494,"limitations of the existing ea method, the lack of interaction and heterogeneous embedding spaces, we propose a unified textual entailment framework for entity alignment called tea. we transform the origin relational triples and attribute triples of an entity into textual sequences and model the ea task as a bi-directional textual entailment task between the sequences of cross-kg entities. we propose two kinds of plm-based aligners to capture the fine-grained correlation between entities with two kinds of sequences in a unified manner. the entailment probability is used for measuring entity similarity and ranking the entity candidates. experiment results on five cross-lingual datasets show that tea outperforms existing ea methods and enables the mutual enhancement between the heterogeneous information. limitations despite that tea achieves some gains for ea, tea still has the following limitations: first, tea has a higher computation cost than the embedding-based ea methods in the re-ranking phase, since tea process entity-pair input for modeling the interaction between them. for reducing time costs, we adopt the confidence-aware reranking strategy to reduce the number of re-ranking samples and candidates. however, the inference time cost is still higher than the embedding-based methods. in addition, the candidate selection may be limited in some corner cases if the ground truth entity is not ranked in the top |c| similar entities calculated by entity embeddings. we will further explore efficient approaches which could cover the corner cases. second, the alignment of relational information of tea requires the entity names to construct sequences. however, the entity names are not always available in some ea datasets, such as the wikidata kg in openea benchmark (sun et al., 2020). in that case, tea can use the attribute sequences without entity names for entity alignment. though tea w/o t r can achieve competitive performance as shown in table 3, it still limits the application of tea. we will further explore plm-based ap- proaches to align the relational information without the requirement of entity names."
2495,"limitations our evaluations rely on the masked language modelling task as it was a convenient task to conduct our experiments and following up on similar related works. to apply it to models trained differently, e.g., models of the gpt class, one needs to develop comparable appropriateness measures, which is a general desiderata of the field. we evaluated the models through prompts. we used a fixed set of prompts, and others could produce better results for each of the tasks at stake. even if one could find some working prompts, this may not be ambitious enough, however. it would show that the relevant information is present in the system, say about hyponym-hypernym pairs. but the tests we are proposing rely on the idea that models should work well consistently, across tasks, and across prompts. with our zero-shot prompting method, we tested the pre-trained models. one could imagine ways to fine-tune these models to our requirements. our goal here was to first make visible that the groundless training might have been sufficient to encode a consistent semantics, and yet that it did not. in future work, we also hope to develop quantitative measures of success and consistency (starting from the statistical models in appendix d), consistency measures which compare parallel performance over more tasks at the same time."
2496,"limitation, which improves the trigger detection rate. (cf. fig. 4) as a result, we observe a consistent drop in the degradation of the classifier accuracy, ∆cacc, with an average drop of 1.45%, particularly on the sst-2 datasetfrom 7.39% to 1.77%. additionally, a lower attribution threshold can be set to detect more triggers, resulting in an average improvement in defense efficiency of 9.61%. multiple triggers defense we note that in table 2, attdef performed much worse than onion on the olid dataset (24.37% vs. 63.5%). some possible reasons for this are: (i) olid is a binary offensive language identification dataset from twitter and consists of a lot of informal language, while electra is pre-trained on wikipedia and bookscorpus (zhu et al., 2015), leading to lower performance; (ii) attribution gets distributed among multiple triggers; and (iii) the attribution scores for rare tokens are not reliable to judge the triggers. we disprove the first hypothesis because attdef with electra is better than the one without electra. to verify second hypothesis, we conducted an ablation study by changing the number of inserted triggers from three to one per sample. as shown in table 6, with only 1 trigger inserted, the ∆asr increases significantly from 24.37% to 60.73%, though it is still worse than baseline 69.03%. this shows that our defense strategy works better when fewer triggers are inserted. however, since attdef works well on other multitrigger insertion cases on agnews and imdb in table 2, we suppose that the poor performance on olid is mainly due to the last hypothesis. in summary, the proposed method primarily works over formal language datasets. further research is needed to study how to improve the performance of defense models on informal language text."
2497,"limitations there are several limitations of the proposed methods. (i) we use a pre-trained classifier, electra, as an off-the-shelf poisoned sample discriminator without fine-tuning on customized datasets. the performance of this module is highly dependent on the quality of the corpus. (ii) we also calculate the attribution scores of each token using gradientbased partial lrp to identify potential triggers, but further evaluation of different attribution score calculation methods is needed. (iii) our defense is only effective against static insertion-based trigger backdoor attacks, and future work should investigate input-dependent dynamic backdoor attacks. (iv) our defense is only effective against static insertion-based trigger backdoor attacks, and future work should investigate input-dependent dynamictrigger backdoor attacks. ethical consideration in this paper, we present a defense mechanism to counter the impact of backdoor attacks. our code and datasets will be publicly available. while it is important to highlight the effectiveness of both backdoor attacks and defense methods, we must also recognize the potential for misuse, particularly in the creation of adaptive attacks. however, by making our defense strategy and implementation public, we may expose our method to attackers, who may discover its weaknesses and develop new types of attacks."
2498,"limitation of onion to launch a strong defense, as shown in table 2. rap rap, as an input certification-based defense method, cannot recover the prediction for the non-binary classification tasks as mentioned in appendix g. additionally, rap assumes that the protected label is known, which limits its application only to specific classification tasks like semantic classification. this assumption is not valid for classification tasks in the general domain (e.g., topic classification on agnews dataset). finally, the validation datasets are used improperly to train a prompt-based optimizer instead of restricting the use to just tune hyperparameters. acl 2023 responsible nlp checklist"
2499,"limitations a major limitation of activeaed is that it is significantly more compute-intensive than other scoringbased aed methods such as aum or dm. this is inherent to the proposed method because the ensemble requires training of multiple models and, after receiving human feedback, the full ensemble has to be re-trained. also, the ensembling of activeaed requires more training runs than trainingdynamics-based aed methods. however, most model-based methods require a cross-validation scheme (klie et al., 2022). the ensembling component of activeaed is more data-efficient than these approaches, because it makes use of the training dynamics captured during cross-validation instead of discarding them. a second limitation of this work is that while we chose baselines that performed strongly in klie et al. (2022), they represent only a fraction of the scoring-based aed methods described in the literature. finally, our evaluation is limited to a single language model and it would be interesting to investigate how activeaed interacts with larger language models than distilroberta."
2500,"limitations our method of identifying important words requires a dataset for a semantic task (in our case nli or pi), which limits its applicability. this requirement also prevents us from generalizing our observations too broadly: we tested our method only on one high-resource language where both dependency parsers and nli / pi datasets are available. our analysis also lacks the comparison to other indicators of word significance."
2501,"limitations since the propsegment dataset feature entailment labels for all propositions in a document, the label distribution are naturally imbalanced, which would potentially pose challenge for modeling. we observe low presence of contradiction examples in our dataset construction process, which could be a limiting factor for the utility of the dataset. unlike previous nli datasets (bowman et al., 2015; williams et al., 2018), we speculate that reference determinacy, i.e. whether the hypothesis and premise refer to the same scenario at the same time, cannot be certainly guaranteed and safely assumed in our case, which in part leads to low presence of contradictions during annotation. we offer a detailed"
2502,"limitations we note that the datasets used in this work solely represent social media interactions from reddit, which is known to have a demographic bias toward young, white, american males3. furthermore, systematic, spurious differences between diagnosed and control users can prevent trained models from generalizing to other data. future research on other social media and datasets is needed to determine to what extent the presented findings are generalizable to broader populations."
2503,"limitations the proposed approach focuses more on logical reasoning on explanations for zero-shot classification. the semantic structures in explanations, such as inter-entity relations and event argument relations, are less touched (although the pre-trained language encoders such as bert provides semantic matching ability to some extent). within the range of logical reasoning, our focus are more on first-order logic, while leaving the"
2504,"limitations our work overcomes visual noise data that limit extraction performance, incorporating multi-modal knowledge of different levels. empirical experiments demonstrate that our method avoids noise data misleading the mmre model. however, there are still some limitations of our approach can be summarized as follows: • due to the limitation of the existing mmre datasets, we only experiment on two modalities to explore the influence of image features. we will study more modalities in future work. • our method neglects the multiple relations for an input, which may not consider the multiple semantics of entities. we leave the multiple relation extraction method for future work."
2505,"limitations compared to the empirical risk minimization scheme, the introduced self-regularization scheme incurs certain overhead because each mini-batch of data will go through two models. for bertbase scale pre-trained language models, the additional memory overhead is about 27% and the additional training time overhead is about 30%. nevertheless, once pruned, the sparsified model can enjoy considerable efficiency gains in terms of storage and inference time. therefore, this is a trade-off that future practitioners might need to consider."
2506,"limitations we inject the medical knowledge graph into local texts for entity span representations enhancement. however, unlike most joint extraction methods, the proposed model is hard to be trained in a parallel way. therefore, it is time-consuming to obtain a well-trained model. we would like to optimize the architecture of the model in the future. moreover, our model is adapted to chinese medical texts where a token usually means a character (not a word). hence, there will be errors when aligning the entities from the knowledge graph with mentions from local texts. word segmentation task will be considered in our future work."
2507,"limitations we observe that although coaug outperforms baselines on multiple datasets, it is still prone to errors that emerge from the bootstrapping process. specifically, our framework utilizes models to augment weak labels to the training set, and if the proposals are extremely noisy, training on noisy examples in future iterations will further exacerbate the ability of the framework to identify entities with high precision. incorporating constraints to preserve the quality of the pseudo-labeled data (shrivastava et al., 2012) is an exciting direction for future work in low-resource named-entity recognition."
2508,limitations our work has a number of important limitations that affect the
2509,"limitations our approach is heavily dependent on the quality of the pre-processed orthographic-phonemic transcription data as it provides the ground-truth for unsupervised alignment objectives. generating phonemic transcriptions and aligning them correctly with orthographic representations can be costly. despite our significant efforts, the alignment is still far from perfect optimality. secondly, our approach might not be effective in improving performance on randomly chosen language pairs. as our framework aims to exploit phonemic similarities of languages with different orthographic representations, the methods are only effective in cross-lingual transfer between lexically similar languages in terms of phonology such as cjkv languages. languages that do not fall into this category might observer little to no performance gains with our proposed framework."
2510,"limitations the experimental results suggest the possibility of our mtl-based qe approach being biased towards the word-level qe task, as the jointly trained qe models show better performance improvements for the word-level qe task as compared to the sentencelevel qe task. further, we also observe that our approach does not work well for language pairs with english as a source language (en-de and en-mr). the qualitative analysis of the english-marathi mtl-based qe model shows that the model performs poorly when inputs are in the passive voice. our multi-pair setting experiments use all seven language pairs. we do not consider properties like the similarity between the languages, translation directions, etc., to group the language pairs. so it may be possible to achieve comparable performance using a subset of languages. we choose the nash-mtl approach for mtl-based experiments because it has been compared with around ten other mtl techniques and it has been shown that the nash-mtl approach outperforms them on different combinations of the tasks. in the current work, we have not experimentally analyzed how the nash-mtl approach gives better improvements than the ls-mtl approach."
2511,"limitations the major limitations of our work are three-fold: (1) in the empirical experiments, we only train and evaluate models on english datasets. as the analyzed pitfalls are essentially language-independent, we believe the empirical"
2512,"limitations while the proposed method performs well on four benchmarks, we discuss some of its limitations. on one hand, as discussed in §5.5, our method is not accurate enough to predict sentences at the end of the documents. there may be some better strategies to construct training samples so that the model can better take into account each part of the documents and make more accurate predictions. on the other hand, our model is not pre-trained with more diverse domains and larger scale data. our datasets are limited to two types, i.e., paper abstracts and short stories, both of which have comparatively obvious order characteristics. in addition, we do not use some larger scale datasets, such as nsf abstracts and arxiv abstracts, because of computation and time constraints. with more diverse and larger data, the performance of our model should be further improved."
2513,"limitations our proposed multimodal grammatical error correction (gec) model is based on a seq2seq generative framework, which utilizes different encoders to extract information from each modality, and then fuses them to provide input to an autoregressive decoder. however, in this work, we did not explore the use of a sequence tagging framework, which may be a consideration for future research, as it has the advantage of faster decoding speed. additionally, this study focuses on the use of audio representations of the source-side of gec data, rather than the target-side, to construct multimodal gec data. our further analysis concludes that our proposed multimodal gec model has limitations in correcting certain minor error types (e.g., adv, conj, punct, and word order) when compared to text-only gec models."
2514,"limitations the limitation of our method comes from the requirement to identify simple words in simple texts in section 2.1. the deepblueai we have used is a deep model, meaning it takes much time when inference. in our experiment, it takes 362.78 seconds to identify simple words from 10,000 sentences with an average length of 8.12. we expect that there will be methods with higher identification accuracy and higher inference speed in the future. due to page limitations, we have placed the related work in appendix a and the ablation experiments in appendix c. due to time constraints, we do not perform a human evaluation of the output of llms. we hope to conduct a more comprehensive evaluation of the performance of llms in the future."
2515,"limitations as we only used english framenet as the dataset for our experiment, it is unclear how well our method would work with other languages or corpora. however, because the method is neither language- nor corpus-specific, fine-tuning may lead to better results with other datasets. also, the method relies on a semantic frame knowledge resource, and annotation will thus be required if it is applied to languages without such resources. this study only considers core frame elements and does not show results for non-core frame elements."
2516,"limitations several main limitations exist in our study in its current form. first, our reported results only simulated experimental participants by manipulating the temperature hyperparameter. we compared this approach with natural language prompting for experiment 1, but that prompting did not increase ""participant"" diversity, so it was abandoned. moreover, approaches for simulating psycholinguistic experimental ""participants"" could go far beyond what was tried here; our prompting method was relatively limited, and more detailed prompting could be included in future experimental simulations. second, making a direct comparison with actual psycholinguistic experiments might not be the only method to investigate llms’ discourse capacity. a comprehensive list of discourse probing tasks might play a similar role despite a different way (koto et al., 2021). third, this study is strictly behavioral: limited by both computational resources and obscure mechanisms of in-context learning, we do not dive into models’ internal representations in our analyses."
2517,"limitations this work focused on experiments on english datasets and did not explore other languages. while we expect that our assumptions hold across languages and the proposed methods and metrics can be applied without any further modifications to other languages this has not been explicitly verified. additionally, we ensured to experiment with counterfactual editors that are representative of the main counterfactual editing methodologies, however we did not exhaustively cover all publicly available editors as our main goal was to demonstrate that our proposed method is widely applicable rather than to exhaustively compare editors."
2518,"limitations this study suffers from four limitations. the first limitation is that even though we annotated 850 dialogues manually, which includes almost 200,000 dependencies, there is still room for improvement in the total number of labeled dialogues. the second one is that our parsing method of inter-edu in the inter-utterance situation is simplistic and straightforward, and it can not cover certain difficult labels. it is desirable to propose a more elegant and comprehensive approach. the third is somewhat analogous to the second. in the future, we should propose an end-to-end method that replaces the current approach, which consists of several processing steps. the last one is about our pseudo-labeled data selection method. it could be interesting to investigate the iterative process."
2519,"limitations to facilitate future research, we analyze the difficulties and possible solutions in this new area. (1) as we present extensive empirical results and address the weakness of clip on vocabulary expansion, its theoretical risk on open tasks is urged to be investigated. (2) the current evaluation protocol is an approximation of the real open world. an evolving benchmark could facilitate future research. (3) for various visual categories, their degree of abstraction, the ease of describing them in natural language, and their density in the data distribution can also influence the extensibility and stability of models, which are worth studying."
2520,"limitations in section 6.4, we already discussed the limitations and challenges of the model proposed (e.g., the model has access to less contextual information from the conversation history, errors can propagate more easily as it does not re-predict the entire cumulative state at each step, and mistakes could only be fixed by explicit delete or update operation). in the following, we concentrate on the limitations that refer to the scope of this work. languages. we experimented with a limited number of languages (english) and datasets (multiwoz 2.1, 2.2 and 2.4). we do not have experimental evidence that our method can work for other languages, including languages with a richer morphology. still, our system has been built without any language-specific constraints or resources, other than the t5 checkpoints and the manually annotated training set. our method can be applied to any other language (without modification) for which these resources are available, or by applying crosslingual techniques and resources (e.g., multilingual language models, translation/projection of the training set) to transfer to other languages zero-shot. in those cases, the expected quality is lower, but the efficiency advantage of diable remains. models. we experimented with two models (t5v1.1 base and large). this is due to the restriction on our computational budget to be both economically- and environmentally-friendly, which made it infeasible to conduct thorough experiments using larger-scale language models. however, we re-emphasise that diable allows one to easily plug and play arbitrary language models and the efficiency advantage of diable remains. diversity in the evaluation dataset. we experimented with three different versions of the multi- woz dataset (2.1, 2.2, and 2.4). although this is the current benchmark for dst accepted by the community, and we followed the standard evaluation methodology and metrics, we are aware that the results presented might not be directly generalisable to other datasets or real-world scenarios with a considerable data shift with respect to multiwoz. additionally, multiwoz has a certain level of noise and this can have an impact on the evaluation and the generalisation capabilities of the model trained."
2521,"limitations in this section, we describe the limitation of our proposed method in terms of data augmentation and the way to combine contrastive learning with cycle adversarial training. first, our data augmentation strategy relies on the reconstruction ability of cycle adversarial training. we believe that more data augmentation strategies for topic distribution will be studied. second, with the symmetrical structure of cycle adversarial training, it is worth exploring how to optimize the encoder e and generator g through contrastive learning simultaneously. we can extend the proposed framework to a conjugated structure in future work. moreover, although it has been explored that contrastive learning and cycle adversarial training working synchronously performs better, we believe that more sophisticated training strategies will be designed to further improve the performance of topic modeling."
2522,"limitations since our contrastive framework requires the probability mass of various summaries given a source document, there is an extremely large consumption of gpu memory even if the batch size is small, which limits the scale of contrastive data and suppresses the potential of our method. meanwhile, due to limited sample points (i.e., probability mass), our bootstrap re-sampling procedure is susceptible to outliers and cannot fully take advantage of this algorithm. in addition, like most abstractive summarization systems, our model does not attach importance to controllable text generation (hu et al., 2017; prabhumoye et al., 2020; he et al., 2020), which means that the generated text might contain redundant and incorrect information. ethical considerations while there is limited risk associated with our work, similar to existing abstractive summarization systems, there is no guarantee that the generated summaries are factually consistent and free from hallucination (maynez et al., 2020; kang and hashimoto, 2020). therefore caution is imperative when our system is applied to practical projects."
2523,"limitations our work has two limitations that may impact the generalization ability of our proposed framework. firstly, in the clause generation section (sec. 4.3), we deduce logic clauses involving a fixed number of atoms, represented by ⌊5k×β⌋, rather than variable length for each iteration of gcn. while this approach has demonstrated superior performance on the multimodal misinformation detection and sarcasm detection tasks, it may harm the generalization of our framework to more complex multimodal misinformation tasks, such as the detection of fake news that involves various modalities, including social networks, text, user responses, images and videos, as discussed in (zhou and zafarani, 2021; alam et al., 2022). secondly, in our work, the incorporation of logic into the neural network relies on the use of product t-norm to differentiate logic operators (i.e., ∧ and ∨). however, as shown in the ablation study (sec. 5.4), product t-norm may lead to vanishing gradients with the increase of logic atoms during the training stage, which may limit the ability of our proposed framework to handle more sophisticated scenarios. we plan to address these limitations in future research."
2524,limitations all our experiments and the
2525,"limitations while we do include multilingual datasets in our experiments, our error analysis is limited to languages of the indo-european family, specifically english, french, and spanish, as these are the languages covered by our datasets which we can confidently analyze. in addition, it is possible to question some of the assumptions made in our theory, which should be kept in mind when considering our work. for example, we assume that, for each content word token in a discourse, there exists a single concept which that word is intended by the sender to express, regardless of whether it appears unambiguous to the receiver. however, unlike in mathematics, theoretical assumptions may not always hold in practice; for example, puns often exploit multiple meanings of a word for humorous effect. while such cases are not frequently considered in lexical semantics, we can expect exceptions to almost any assumption or"
2526,"limitations and future work we acknowledge the limitations of lm-toast in few-shot calibration training. from our pilot experiments in sec. 3.1, we observe that a significant amount of data points are needed for the calibration task training. in lm-toast, we effectively utilize the whole training set for the calibration task. some learning paradigms assume only a small number of annotated samples at first, which limits the effectiveness of lm-toast. for example, in active learning (zhang et al., 2022; schröder et al., 2022), only a very small number of samples are available at the beginning most of the time, and models need to rely on them to find informative unlabeled samples to annotate. the vanilla confidence scores can be effectively utilized in this setting. however, lm-toast may not learn the calibration task well given very limited samples, resulting in poor performance in searching informative samples. we plan to investigate the calibration task in the few-shot setting and bring out the potential in lm-toast to make it suitable for more tasks."
2527,"limitations intapt adopts a prompt tuning method which utilizes the inherent information from pre-trained models that already shows good asr performance on l1 english speakers. therefore, in order to apply our method we need a pre-trained model that already has good performance on a specific task which might not be available for other langauges. also, our method might potentially need sufficiently large pre-trained model size in order for prompt to utilize the internal information of the model."
2528,"limitations since the proposed method scales probability distribution, not shift, the proposed idea does not change an output when coupled with a greedy decoding strategy. greedy decoding simply takes argmax of probability distribution at each time step, and hence, the output with or without the local temperature scaling will not be changed within greedy. therefore, the proposed idea is required to be utilized with a beam search, or a variant of it."
2529,"limitations in our experiments, the most significant limitation is the lack of computational resources. experimental results in this paper and previous work(saha et al., 2022) have shown that a larger scale of models could lead to higher structural and semantic accuracy of explanation graphs in this task. constrained by computational resources, bart-large is the largest model on which we can perform the complete process of experiments. we believe that graph generation would be better if sufficient resources were available to perform synthetic data based pre-training on a larger model. in addition, since the evaluation metrics for graph generation tasks are incomplete yet, we can only evaluate a few samples manually outside of the metrics of the dataset, which is more subjective. with more evaluation methods with standardized processes proposed, the results of the experiment will be evaluated more objectively."
2530,"limitations we think the limitations of our work are three-fold. (1) as discussed in section 2.1, we employ existing cgec models to select sentences for annotation when building the media and thesis domains of nasgec. although this reduces annotation costs, it inevitably introduces biases into our dataset. for instance, the proportion of complex syntax- or semantic-related errors may be lower than that in reality, since existing cgec models fail to identify them. note that although we manage to mitigate such biases by voting with multiple models, this issue still exists. future work should explore how to automatically mine erroneous sentences from a low error-density domain with minimal biases. (2) the current size of our dataset is relatively small. we will continuously collect more data from more diverse domains. compared with other domains, thesis has a much smaller data size (1.5k), as authorized papers are hard to obtain. in the future, we plan to cooperate with universities and thus accumulate more authorized data to enrich this domain. (3) based on our multi-domain nasgec, we have reported and analyzed cross-domain performance preliminarily. however, besides fine-tuning with small-scale data in the target domain, many other potentially helpful domain adaptation techniques can be tried. we believe cross-domain gec is a valuable research topic and encourage future work to study it with nasgec."
2531,"limitation one limitation of this paper is that we do not validate fedpetuning on large scale models, (e.g., t5 (raffel et al., 2019), llama (touvron et al., 2023), vicuna (chiang et al., 2023), etc). although the parameter efficiency of petuning is more impressive on larger pre-trained models, we have to consider the limited computational resources of clients in fl, making it challenging to deploy such a large-scale model. in addition, with the increasing size of modern pre-trained models, the community needs to design fl-friendly petuning methods. in this sense, our work can serve as a benchmark and guide for future exploration of petuning in fl."
2532,"limitations there are two limitations of the current mixpave model. first, although mixprompt can achieve comparable extraction performance with full finetuning, how to identify the optimal combination of the two prompts is challenging and remains unanswered. we conduct grid search in our experiments to empirically find the best prompts length. in future, we plan to investigate a systematic solution for identifying the optimal or a suboptimal combination. second, our model learns attribute-specific prompts for a new attribute. we plan to explore a parametric network that could guide the learning of attribute-agnostic prompts."
2533,"limitations apart from the effective attack performance against multi-exit bert, we acknowledge that our work has several limitations. firstly, we only evaluate our slowbert on the glue benchmark (wang et al., 2018a), which demonstrate the effectiveness on alphabetic languages such as english. however, for logograms (e.g., chinese), it requires to design language-specific method to generate the corresponding substitution set to achieve the attack goal. secondly, despite we present a new security threat against multi-exit bert, potential defenses should be analyzed such as adversarial training (geng et al., 2021). for the above mentioned limitations, we leave them as the future works of our proposed slowbert."
2534,"limitations as with other prompting methods, preadd’s performance may vary depending on the exact wording of the prompt, and may require manual prompt design to achieve the best possible performance. additionally, compared to more basic forms of prompting, preadd requires accessing the base language model’s output logits at each step of decoding, which can be inconvenient with certain apis such as the openai gpt3 api (although preadd is technically runnable through the gpt3 api, it will be less computationally efficient). with respect to the actual performance of preadd, a rare but nontrivial failure mode is setting a high α parameter. in particular, setting α to have a magnitude of above 2.5 tends to result in degenerate continuations. this is due to how the output logit distribution shift induced by preadd may significantly increase the logits of “nonsensical” tokens. the issue seems to appear predominantly in positive control applications of preadd (e.g. our sentiment control task), wherein the logit distributions “spike” more and have higher entropy. however, logit distribution truncation methods (e.g. top-k and/or nucleus sampling) can be used in preadd to alleviate text quality decay by eliminating nonsensical tokens prior to applying the control. in this work, we evaluate toxicity using perspectiveapi as a convenient automatic metric, but we acknowledge that perspectiveapi is not a perfect measure of toxicity. for example, it may be biased against african-american english, and may fail to capture certain types of harmful outputs (mozafari et al., 2020; elsherief et al., 2021). overoptimization against perspectiveapi could lead to unexpected side effects or biases in model outputs (jacobs and wallach, 2021; xu et al., 2021). additionally, although controlled generation methods like preadd may reduce the toxicity of generated continuations in the presence of highly toxic prompts, they may still struggle to explicitly counter the original toxic language in the input. for our gender bias reduction task, we have focused only on occupations as provided in the winobias dataset. there are of course innumerable other types of bias which are important to mitigate, ranging from gender bias in facets of language other than occupations, to other types of bias such as those based on race or age; blodgett et al. (2020) provide a more complete"
2535,"limitations the limitations of our work are as follows: (1) as we construct eventoa for a new task by manually annotating, the data size can be further extended. (2) our study is limited to english sources, and we hope work can pay attention to event ontologies in other languages. building datasets for multilingual event ontology alignment would have a positive impact on applications in other languages beyond english."
2536,"limitations as a preliminary study, our proposed classifier decomposition framework focuses on the first training stage of cre models with a lack of explorations on stage 2. besides, more experiments can be conducted by combining our framework with previous leading cre models, which we leave for future research. in addition, our work only focuses on strategies with the ffn layer. as the bert encoder is the main component of cre models, we call for more attentions to the research of improving encoder representations in the first training stage."
2537,"limitations we used the asap data set to evaluate the performance of the proposed method. although the dataset is well known and widely used, it has two major limitations. at first, the data size is small. even with pre-training the model with a decently large data set (e.g., wikipedia), the interpretation of experimental results are limited by the data size. the second limitation is an inherited bias in the data set. since the asap data set is labeled by human raters, the data set is biased by personal preferences. at last, the proposed approach requires a reasonably large pre-processing to extract all the additional features which hinders a scalability. additionally, our work is limited to only measure creativity in expression but not in content."
2538,"limitations there are two limitations: (1) although mt r include three types of reasoning types (deductive, inductive ,and defeasible reasoning), we only focus on relation reasoning task. for other tasks, it is also necessary to construct more datasets with the fusion of multiple reasoning types. (2) our primary focus remains monotonic reasoning, however, the combined reach of deduction and induction is only the tip of the iceberg of human reasoning (choi, 2022). this also inspires us to focus on more nonmonotonic reasoning and more logical combinations."
2539,"limitations the proposed dataset is annotated for verb metaphors in particular. however, other lexical units including adjectives and adverbs should also be studied in order to truly understand the role of metaphors in news. it is important to examine the diversity of the generated ideas when performing metaphor generation. in this study, we proposed a simple approach to cluster words using wordnet. however, the metric is far from perfect and can be improved. for the task of candidate generation, we performed word masking to generate metaphorical and literal substitutes as we were curious about the ability of llms to generate relevant metaphorical mappings while preserving the underlying semantic idea. direct substitution of metaphorical candidates resulted in syntactically incoherent sentences in a few cases. it may be better to paraphrase the sentence after selecting the metaphorical mapping (ottolina and pavlopoulos, 2022). ethical concerns and broader impact we created the dataset from a publicly available news headlines dataset. this ensures that data is free from (a) anonymity concerns, (b) obscenities and (c) any stereotyping or bias. as the task is cognitively intensive, we only assigned at most 150 headlines to each annotator. all annotators were duly acknowledged and appreciated by nvidia ai technology center for their contribution. the original dataset of news headlines is the under apache license 2.0. we are thus permitted to modify and redistribute it. generating metaphors carries concerns due to the implicit potential to craft misleading text. the usage of metaphors has been shown to resonate emotionally with readers (citron and goldberg, 2014). this should not be a concern with our data as we only release generated candidates that preserve the underlying semantic meaning of the source headline."
2540,"limitations sample separation based on model predictions can only eliminate part of the noise, and it costs extra time in training. moreover, although our dynamic gce loss based on prediction entropy works well in distantly-supervised ner, eq. 4 is determined mainly because it has superior experiment results and it lacks theoretical proof."
2541,"limitations as described in section 4.4.3, the proposed model requires precise prediction on the target domain at each dialogue turn to utilize the relevant slot information within the service schema. this possesses a difficulty to apply the proposed model to some new domains, especially when the domains share high similarity with seen domains. to reduce this difficulty, as a future improvement, one potential approach is to process on a hierarchical structure of slots within schemas, instead of individual slots. in this way, the model does not need to rely on the domain, but only on a group of similar slots. the model can then perform matching of a chosen group of slots with available slots within a schema and composes responses based on those slots."
2542,"limitations the most important limitation of our work lies in the size of the havqa dataset. however, substantial further funding would be needed to resolve this. for the baseline multimodal experiments, we did not use the image directly but resorted to extracting textual tags and including them in the text-only translation input. a tighter fusion technique may give better performance. 14https://github.com/shantipriyap/hausavqa/ tree/main"
2543,"limitations by manual analysis, we found that claim-dissector suffers from overconfidence in blocks with at least 1 relevant evidence. then it seeks to select more relevant evidences inside, even when they are not. we believe this is connected to how irrelevant negatives are mined in fever — they originate only from blocks without relevant evidences. on real data, the system often struggles to recognize what facts are refuting, and what are irrelevant (especially when applied out-of-domain). we demonstrate this in a case study on downstream application, where we replaced retrieval on wikipedia with news-media in test-time. we tried to verify the claim ""weapons are being smuggled into estonia"". our system discovered article with facts about ""weapons being smuggled into somalia"", and used it as a main refuting evidence to predict refute veracity. lastly, cd is trained with evidence from wikipedia, and do not considers other factors important for relevance assessment in practice, such as credibility of source, its reliability, or its narrative. this is the area of active research, as human fact-checkers also need to deal with lies (uscinski and butler, 2013)."
2544,"limitations there may be some possible limitations in this study: 1. discourse schema. it should be acknowledged that web documents are heterogeneous themselves and a unified framework to accommodate all web documents may be infeasible. in this article, instead of pre-define the domain/type of web documents we target at, we adopt a problem-motivated research paradigm where we ground ourselves to two characteristics (multiple topics and multiple hierarchies) during discourse schema design. although we simplify the discourse schema to promote its universality, due to the free-style and domain diversity of web document data, it still has a limited scope of usage, mainly on general news report with multiple topics. for future studies, label sets could be revised in order to better account for the semantic functions in web documents of specific domains, where fine-grained labels and domain-specific labels can be considered. 2. task setting. in this paper, we only consider parsing the discourse from a list of document logical blocks already pre-processed in advance while do not contain a complete pipeline from input html source code to the final output discourse structure in the task setting. in the future, the gap between html elements and document logical blocks should be automatically closed in order to apply to downstream application scenarios. 3. data bottleneck. the annotated data volume in this paper is not big enough due to the expensive labour overhead, which may introduce noise into experiments and distort the performance and analysis. also, the domain diversity and multilingualism of dataset could be questioned since we collect data from single platform in chinese. in the future, such data bottleneck can be remedy by more dedicated manual annotation efforts, the help of weak supervision techniques, as well as developing data-efficient models."
2545,"limitations we identify a few limitations of this work. first, our approach is applicable to language models pretrained with tables, which may not always be included in all language models (especially small ones). second, our approach’s limited improvement in commonsense reasoning tasks suggests that its effectiveness may depend on the specific task and the level of structured reasoning required."
2546,"limitations this study has potential limitations. firstly, we only test our method on one dataset. we plan to apply our model to more datasets in future versions. secondly, ablation experiments are not sufficient. we will conduct comprehensive ablation experiments to demonstrate the contribution of different components."
2547,"limitations in this paper, we only focus on whether or not there is a causal relationship between the given events, does not discriminate the specific cause/effect event. in addition, we only conduct research on sentence-level eci, whereas document-level eci often present more challenges. these are the focus of our future research."
2548,"limitations our experiments are mainly on traditional datasets and do not fully demonstrate the effectiveness of the method in end-to-end scenarios. in order to solve the low-rank bottleneck problem, this paper proposes a method of svd weight transfer, but this method is limited to matrix factorization and does not apply this method to more general lowrank factorization, such as tensor factorization. we leave it as future work."
2549,"limitations deplot’s strength is highly dependent on the accuracy of plot-to-text(table) conversion. to obtain effective plot-to-text conversion, large amounts of diverse and in-domain plot-table parallel data are usually needed. it is unknown to which extent deplot can work for out-of-domain (ood) plotto-text conversion. we investigated this in section §6.2 but in the future a wider range of web charts can be used to gain a deeper understanding into deplot’s robustness for ood plots. beyond, deplot does not work for visual language that does not have a clear latent textual representation such as textbook figures where the visual illustrations are created using specialized software and do not have clear structured representations. another limitation of the current deplot approach is that we ignore any layout information such as orientation and color of the visual elements/objects. in future work, we can incorporate such attributional information by including them in the decoding target."
2550,"limitations our method utilizes the amr annotations as additional training signals to alleviate the data scarcity problem in the event extraction task. in this problem setup, generally speaking, amr annotations are more expensive than event extraction annotations. nonetheless, in reality, the amr dataset is much bigger than any existing event extraction dataset, and amr parsers usually have higher performance than event extraction models. leveraging existing resources to improve event extraction without requiring additional cost is a feasible and practical direction. our work has demonstrated the effectiveness of leveraging the feedback from amr to improve event argument extraction. however, it’s still under-explored what additional information and tasks can be leveraged as feedback to improve trigger detection. we did not have quantitative results for the alignment between amr and event graphs. the authors randomly sampled 50 event graphs from ace05-e and found 41 are aligned with their amr graphs based on human judgment. in future work, more systematic studies should be conducted to evaluate the alignment. there is a large gap between the validation and testing datasets in terms of label distribution on ace05-e and ace05-e+. we observe that performance improvement on the validation set sometimes leads to performance decreasing on the test set. both the validation and test dataset miss certain labels for event trigger types and argument role types. the annotations in the training set, validation set, and test set are scarce and highly unbalanced, which causes the low performance on trained models. we argue that a large-scale more balanced benchmark dataset in the event extraction domain can lead to more solid"
2551,"limitations it is important to note that the study is based on a limited set of examples and although it is enough to give a signal if a system is struggling or not in faux pas tests, the number of stories is not sufficient for statistically significant ranking between systems. ethical statement the study’s scope did not include the representation of harm toward specific populations. the narratives were evaluated by a clinical psychologist to ensure that they did not contain offensive content. however, it is important to acknowledge the potential value of further research on the representation of harm in relation to culturally sensitive and socially controversial topics."
2552,"limitations our experiments focus on the t5-base and t5-large models as these are widely used, representative pretrained seq2seq models. however, there are other pre-trained seq2seq models such as bart that we did not experiment with. it would also be interesting to experiment with pre-trained models with more layers such as t5-3b and t5-11b. we have not conducted these experiments due to resource constraints."
2553,"limitations this resource paper proposes a new dataset and experiments with a baseline model only. we do not focus on creating new models and architectures. in the future, we plan to create models that perform much better on the isltranslate dataset. moreover, the dataset has only 31k video-sentence pairs, and we plan to extend this to enable more reliable data-driven model development. in the future, we would also like to incorporate isl linguistic knowledge in data-driven models. ethical concerns we create a dataset from publicly available resources without violating copyright. we are not aware of any ethical concerns regarding our dataset. moreover, the dataset involves people of indian origin and is created mainly for indian sign language translation. the annotator involved in the dataset validation is a hard-of-hearing person and an isl instructor, and they performed the validation voluntarily."
2554,"limitations our study used t5-base (220m) due to the capacity of our computational resources (tesla v100 32gb). thus, it is unclear whether our method is also effective for larger models, such as t5-xl/xxl. lester et al. (2021) argues that continuous prompts are particularly effective for large t5 models. following their results, our instruction embedder is also expected to be effective for larger models. as shown in figure 3, instruction optimization is slightly unstable to converge. some studies tackled the unstable convergence of bilevel optimization by l2-normalization, early stopping (zela et al., 2019), or perturbation of hyperparameters (chen and hsieh, 2020). these methods might be effective in stabilizing the instruction optimization."
2555,"limitations, structured models (jie et al., 2022; shao et al., 2022) have exhibited the capacity to outshine large language model prompting on mathqa. additionally, the recent emergence of instructionfollowing models (ouyang et al., 2022; wang et al., 2022a), exemplified by alpaca (taori et al., 2023), has prompted our interest in equipping large language models with mathematical reasoning capacities (wang and lu, 2023) while maintaining the integrity of their underlying language understanding capabilities. limitations the methods we have employed for prompting and fine-tuning have yielded noticeable improvements, yet certain limitations persist within practical applications. to achieve optimal performance, we continue to rely on prompting using large language models, which prove to be costly for the research community. furthermore, retrieval efficiency may present a challenge when dealing with extensive training sets, as identifying the top m exemplars for each example becomes increasingly time-consuming. consequently, devising a more efficient algorithm to expedite the retrieval process represents a potential area for future exploration. despite the potential for performance improvement by sampling 40 reasoning paths for each question as presented by wang et al. (2022a); fu et al. (2022), we were unable to incorporate this approach due to budget constraints. additionally, although training data has proven beneficial, the gains for smaller models are insufficient to surpass the performance of large language models. this observation may indicate the necessity for a fundamentally different model design or a superior pre-trained model (e.g., galactica (taylor et al., 2022) or code-t5 (wang et al., 2023)) as a more effective basis for fine-tuning."
2556,"limitations model architecture due to our computational resource constraints, we only used the bert base architecture. we cannot confirm whether our results and observations are transferable to any other transformer-based architectures, especially for larger ones. randomness we did not run pretraining for multiple times with different random seeds due to the limited computational resources and research budgets, though we fine-tuned models five times each with different random seeds in any downstream tasks. this might affect the overall results shown in the paper. languages other than english it is uncertain whether any results and"
2557,"limitations the limitations of this work mainly lie in two aspects: (i) the synthesis quality is determined by the performance of existing data2text approaches, while data2text generation is still a difficult task that waiting for deeper exploration. the common errors in generation are included in sec. 4.2.3. (ii) we adopt a plm as the decoder in data2text generation in order to generate fluent utterances. however, as stated in (ribeiro et al., 2021), plms tend to pay more attention to sentence fluency than to the graph structures of inputs, which may cause the loss of some critical information."
2558,"limitations there are several limitations to account for in the presented work. first, the large gpu requirements for the execution and replication of the presented experiments. second, the lack of empirical results beyond english-based text, and how morphologi- cally and syntactically more complex corpora may affect the presented evidence. third, our evaluation section compares gp-ts performance to the common hyperparameter grid-search alternative, yet we acknowledge that other bayesian optimization techniques used in the machine learning community may provide suitable and competitive alternatives to explore. in addition, we have not run any hyperparameter tuning beyond mlm dynamic masking, which might improve all studied algorithms’ performance. finally, our"
2559,limitations and future work for improvement.
2560,"limitations the natural idea to improve robustness is to add adversarial examples to the training set and retrain the model. however, generating adversarial examples for a large training set can be very time-consuming. thus, it would be interesting to explore more efficient methods that implicitly involved adversarial examples in the training process, e.g., (yang et al., 2022)."
2561,"limitations fuzzers are designed to reach deep and complex control flow in large software. many programs for current ai for code datasets do not have complex control flow. as a result, afl++ can quickly cover all program branches before generating many inputs for us to feed to the model. we plan to try data-flow coverage as a more accurate coverage metric in the future. afl++ uses branch coverage to track fuzzing progress. although it works well on c/c++ programs, it may be ineffective on languages with exceptions, which are implicit control flow. for example, afl++ cannot distinguish different exceptions thrown in the same block, which sometimes leads to low coverage in python programs. to overcome this issue, one possible way is to change from branch coverage to line coverage. although our current implementation requires a fuzzer, our approach can also work on tasks with only functions or code snippets as long as we can acquire adequate input/output pairs of the functions or code snippets, which may have some engineering challenges but is not infeasible. for example, in recent years, the software engineering community has proposed various ways to fuzz bare functions (serebryany, 2016; ispoglou et al., 2020). ethical consideration our method exploits fuzzing test cases for program understanding. improved semantic understanding of programs facilitates various tasks, e.g., code generation and code completion, which might further be used to patch vulnerabilities or fix defects of softwares and systems. nevertheless, considerable effort has to be further devoted to apply the method to these applications, for which we encourage to take special care in advance. in addition, a number of crashes and hangs have been observed on programs in the adopted datasets, since fuzz testing is utilized. we do not demonstrate test cases that lead to these crashes and hangs to avoid misuse of this information."
2562,limitation of dhr by achieving better zero-shot performance without relying on document structure information. we also study the accuracy-storage-latency land- scape. we believe these findings are critical to the real-world adoption of hhr to odqa systems.
2563,"limitations we recognize the following main limitations of the present study. although the approach we devised is not bounded to a specific model architecture and language, our study fouced only on one neural language model and a limited set of languages and this may limit the generalization of our results. moreover, we are aware that discourse coherence is a multifactorial phenomenon that can only be partially covered by the devised methodology and dataset."
2564,"limitations in this study, we primarily focus on the examination and experimentation of the dsi-qg model, with plans to expand our research to include more recent models that utilize differentiable search indexing, such as the nci model. while our approach has demonstrated effective improvements in dsi retrieval outcomes, and both tct-colbert and our proposed dsi-qg-multi performed well in our empirical analysis concerning relevance ordering, we cannot dismiss the possibility that these favorable results may be attributed to the extraction of insights from a specific bert reranker model that shares similarities or correlations with the one used to define the desired ranking. despite showing improvement over dsi-qg, our model remains slightly less effective than stateof-the-art dense retrieval methods such as tctcolbert v2. our approach offers advantages over dense retrieval models such as reduced storage and maintenance overhead, as dsi models do not require additional index structures for online use. though index structures are utilized during the training phase of dsi-qg-multi, the generated index structures are temporary in nature. furthermore, due to the limitation of model memory, current research on dsi only experiments on a subset of the entire ms marco dataset or small dataset such as nq. therefore, an important future direction is to develop more efficient architectures to deal with the issue of memory bottleneck, for example, by using the current popular large language models (llm) or constructing aggregation structures for storing all information in hierarchical pieces."
2565,"limitations data unbalancing is one of the limitations of our work. in the future, we would like to address this problem by data oversampling or undersampling. for our experiments, we have used all samples from the iu x-ray dataset. but from the mimic-cxr dataset, we have used only 44578 reports out of 227827 reports. our results for the mimic-cxr dataset might differ when we use the whole dataset. to evaluate the generated pathological descriptions, we consider the pathological descriptions that we extract from original reports as ground truth. to evaluate the generated full reports, we generate templated reports by replacing ground truth pathological description in normal report template and consider it as ground truth. so it considers the abnormalities from the original reports and the normal sentences from the normal report template. automatic generation of chest x-ray reports will make it easier for radiologists to diagnose and write reports. our model achieved comparable performance with state-of-the-art models on chest x-ray report generation. in realistic scenarios, it is still a long way from being used clinically."
2566,"limitations the datasets utilized in this research contain documents and summaries in english (xsum and cnn/dm datasets) and thus mainly represent the culture of the english-speaking populace. gender, age, political or other biases may also exist in the dataset, and models trained on these datasets may propagate these biases. our experiments and analyses are based on the assumption that training data contains artifacts that lead to factual errors in summarization models. also, it is evident from the results, that the effectiveness of our proposed models is relatively higher for the noisier xsum dataset. so, our analytical results and improvement from a model may have limited implications on a perfect dataset that does not exhibit any learnable artifacts. we relied on automated metrics, such as rouge and entity recall for measuring information relevance, and entity precision, question answeringbased metrics and dependency arc entailment accuracy for information correctness. these metrics are error-prone. exclusively for a subset of models, that perform the best according to automated metrics, we use human annotations for additional evaluations."
2567,"limitations in this first effort, opinesum was demonstrated for the english language. since the wealth of review information on many websites span several languages, extending our work to other languages is a key area for future work. there are two language specific components—the proposition identification rules, and the textual entailment model. for the latter, there are multilingual resources such as mt5 models (xue et al., 2021) and multilingual entailment datasets (conneau et al., 2018) which are good starting points. the proposition rules are much more language specific. very recent work has introduced a corpus and learned model for proposition identification (chen et al., 2022), and future research in languages other than english could strength this component. a second noteworthy point is the scalability of the silver data creation. as described in section 3.3, we perform a quadratic number of entailment queries per item. in this work, this was of the order of a few billion. we used an apache beam pipeline to scale our computation using a lot of parallel computation on cpus. readers must be aware of this computation when applying such an approach for their work. however, note that the processing only needs to be performed once for training data creation. future work on more efficient transformer models such as khattab and zaharia (2020), will help vastly improve these types of computations."
2568,"limitations this work contributes a debias benchmark mmsd2.0 for building reliable multi-modal sarcasm detection system. while appealing, mmsd2.0 is built on the available mmsd benchmark. in the future, we can consider annotating more data to break through the scale and diversity of the original mmsd."
2569,"limitations our work still exist some limitations. first, we choose an entity typing system on the base of wikidata tags, however, the granularity of the typing system remains to be discussed. a system with too many types would introduce noise to long-tail types, while insufficient types would weaken the disambiguation ability of type similarity. thus, building a type system with adequate granularity remains a challenge. second, we combine the entity typing task with plm-based semantic encoders, which require a fixed type system and further finetuning. integrating the entity typing task into the pretraining process may enhance the transferability of the model and remove the dependency on a fixed type system. potential risks. our proposed dataset nel centers on ambiguous entities, whose type distribution may not remain the same with other datasets. a potential risk is that the model trained on nel may experience under-exposure of other entity types, which would damage their transferability and lead to undesired outputs on other datasets."
2570,"limitations our paper has some limitations. first, we do not give detailed instructions about techniques that we recommended (e.g., factor analysis, synthetic data generation). we rely on our readers’ autonomy to acquire the necessary information (that is specific to their research projects) by further reading our recommended references. second, we only survey tpc papers included in the acl anthology, despite other tpc papers existing outside this venue. while this means that the challenges we identified might be specific to these papers, we believe they are still a good representation of the tpc research done in nlp. lastly, we limit our"
2571,"limitations although simple achieves great performance with size reduction and generation speedup on various generative language tasks, it is interesting to explore combining the stage of mask learning during the pre-training. then, one pre-trained model can be applied to downstream tasks with any required sparsity with a stage of fine-tuning. in addition, the pruning of the larger generative pre-trained language models during the fine-tuning is also worth trying. in the future, we would like to investigate the generation ability of the compressed models with more pre-trained data and larger models."
2572,"limitations in this work, we present pgra to retrieve taskspecific context evidence to support nki tasks. however, our work has some limitations. firstly, we have not experimented with our pgra on sentence-pair tasks, such as mrpc (dolan and brockett, 2005), in which the model needs to infer the relationship between two sentences. retrieving two sentences from an external datastore is non-trivial as there are hardly sentence pairs in the wikipedia datastore. a larger corpus with more diverse data sources may help in this case. secondly, we restrict our pgra to classification tasks but not generation tasks. similar to sentence-pair tasks, retrieving sentences that may help the model generate text is more complex. for example, data related to both the source and the target may help in machine translation (khandelwal et al., 2021). we will research this question in the future. last but not least, we have not extensively tested the performance of our method on ki tasks, except for some preliminary analysis in appendix f. this restricts the generality of our methods. solving ki tasks depends on knowledge in the passage-level external datastore while matching such information needs possibly more specialized prompts for our method. thus, it is for our future work."
2573,"limitations while our ndd metric has demonstrated its effectiveness in measuring the semantic distance between overlapped sentences, there are still some limitations to consider. firstly, the calculation efficiency of ndd may become a bottleneck when dealing with large amounts of data. the mask-andpredict strategy requires the generation of a large number of predictions for each word in the lcs, which can be computationally expensive. therefore, for large-scale applications, more efficient algorithms or hardware acceleration may be necessary to speed up the calculation of ndd. secondly, our method currently cannot selectively compress certain parts of the text. the mask-and-predict strategy compresses the entire overlapped segment, which may not always be desirable. for example, in some cases, it may be more desirable to compress only the less relevant portion of the text while retaining the most informative content. while ndd has an advantage over supervised compressors in controlling compression ratio, it still cannot control the compression orders. future research may investigate techniques to allow for more fine-grained control over the compression process. overall, while ndd shows great promise in improving the evaluation of semantic similarity and text compression, further research is needed to address these limitations and improve the compression rate controlling ability and versatility of the method."
2574,"limitations although recontriever narrows the gap between bm25 and unsupervised dense retrievers, it still lags behind bm25 when acting as a generalpurpose retriever. this issue may make recontriever not directly usable when facing a new domain, thus limiting its practicality. also, as recontriever is initialized from the language model bertbase, there may exist social biases (zhao et al., 2017) in recontriever and thus have the risk of offending people from under-represented groups."
2575,"limitations all experiments are conducted on data containing exclusively english language. consequently, the results may differ in particular on morphology-rich languages and/or non-inflectional languages. similarly, all presented techniques expect languages to use latin characters. therefore, our method first needs to be adapted in order to be used on languages using different characters, such as cyrillic languages, korean or persian. using the bert-classifier in conjunction with al is resource intensive. loading the ""bert-baseduncased"" model from huggingface along with one of the datasets with a batch size of 24 requires around 22gb of gpu memory. we train for a maximum of 15 epochs, requiring up to 1 gpu hour, depending on the size of the dataset and the length of individual inputs. as such, a single al experiment requires approximately 20 gpu hours to complete. in addition, computing the al strategies requires up to 2 gpu hours for the alps strategy and up to 1 gpu hour for the dal strategy, depending on the dataset. the subword strategy is calculated only once and used up to 1 gpu hour. in total, our ral experiments take around 60 to 100 gpu hours to complete. it is important to note that the rl itself is not expensive and does not require gpu, therefore ral can easily be adapted to scenarios with low computational resources by employing a different classification model as well as using al strategies that do not rely on large pre-trained language models."
2576,"limitations selecting a representative set of examples to label becomes essential when working with limited labeled data. in this work, we use uniform sampling for our results, which might not be the best approach. we discuss these limitations in more detail in the appendix. while we evaluate our model on a multi-label dataset (dstc2) and show improvement over standard baselines, the effectiveness of our approach on such problems needs more investigation. ethical considerations while our algorithm is primarily a tool for improving classifier performance in label-scarce settings and uses publicly available, anonymized datasets, we acknowledge the potential ethical implications it may carry. despite the neutral nature of our tool, it could unintentionally propagate or amplify biases favoring certain styles of communication. if used in real-time settings or without proper checks in place, it could inadvertently alter the natural dynamics of the classroom as teachers or students might modify their behavior based on how they believe the classifier categorizes their utterances. like any other, we recognize that our tool could make mistakes or be misused by over-relying on quantitative aspects over qualitative aspects of instruction. therefore, real-world application requires continuous vigilance and open dialogue with practitioners and stakeholders to ensure its use benefits teaching and learning."
2577,"limitations in our paper, we provide a variety of experiments and"
2578,"limitations multijugate dual learning improves the model’s performance in tod tasks in low-resource scenarios, but the introduction of the dual training objects increases the required graphics memory and training steps. in addition, the rephrasing mechanism necessitates an additional paraphraser to rewrite the training samples; hence, the number of training samples increases according to the number of paraphrases. despite this, we find that the higher training cost associated with multijugate dual learning is preferable to employing a large quantity of dialogue data for further pre-training or manually labeling data. considered from a different angle, the scenario described above presents possibilities for future research, such as the development of higher-quality rephrasing algorithms to filter the augmented text. in the meantime, multijugate dual learning is a learning objective between structured and unstructured texts. therefore it may be extended to any task involving heterogeneous data, such as generative information extraction, and data-to-set generation."
2579,"limitations in the augmentation-driven self-training, we implement the data augmentation with random masking for simplicity, since augmentation is not the focus of this work. and wang and henao (2021) has explored more fine-grained data augmentation strategies, which may further improve performance."
2580,"limitations our dqgf still exists some limitations. while our generated data improves performance in diverse questions settings, there is still some noise in the generated data that affects the performance of original single question. in the following, we will give the limitations of our dqgf on its three components. the diversity of the question depends on the diversity of the equations. our equation generator is based on heuristic rules, resulting that the generated equations are very simple. in the future, we will try a model based equations generator to generate more diverse equations. in the question generator, it can only recognise equations with the operator ""+-*/"" due to the limited operator set in our training dataset unbiasedmwp. in the future we will expand the operators so that the generation model can recognise more operators and be more universal. filtering strategy is also important. using the answers of expert model as a criterion for evaluation still exists bias and leads to the noisy data. in fact, we have tried to generate more diverse equations but all are filtered by the current data filter. we will look for better filtering strategies in the future."
2581,"limitation of dft++ is the computational overhead caused by generative plms. additionally, our current approach includes all utterances generated by the plm, even those that might lack contextual relevance or contain noise. these issues are left for future exploration."
2582,"limitations to inspire future work, we conclude some limitations of our work as follows: • while our method achieves promising performance on sentence embedding related tasks like sts, the performance on other natural language processing tasks are still need to investigate. • the ai feedback in our experiments comes from gpt-3, which requires a fee to use. • we do not explore the effect of different task description prompts on the quality of generated sample pairs, which may influence the performance of claif. • in clhaif, we only use the ai feedback for positive sample pairs. how to utilize ai feedback for negative sample pairs remains to be studied."
2583,"limitations the training process of mars needs to rely on manually annotated belief states and action states as semantic states to explicitly model the relationship between dialog context and semantic state representations through contrastive learning methods. we propose mars in the research community and hope it can be better applied to real-world scenarios in the industry. however, the annotated data is expensive, which makes our methods have some limitations in the landing process of real scenarios. in the future, to better apply our proposed mars to real-world scenarios, we will introduce semi-supervised methods to reduce the dependence on annotated dialog corpus."
2584,"limitations we reused data collected by previous work in the literature. collecting news articles is susceptible to various sampling biases, related to the sources collected, the topics covered, and the time span of the collection, which influences what appears in the articles. in addition, labels given to articles are actually the political orientation of their source in the case of the allsides and politics datasets, which is obviously likely to induce errors. they rely on expertise provided respectively by the allsides11 and ad fontes12 websites. the exact methods are undisclosed, but such labeling has necessarily a subjective aspect, oversimplifying predefined political categories, and can evolve in time. this affects classification reliability when applied to different sources, different times, different topics. this is on top of any specific elements related to the language (english) and cultural background of the sources (predominantly u.s.-based sources). this study is not intended to provide an accurate tool for predicting the political orientation of a text, but to provide analyses of the linguistic expression of bias, as seen through a supervised model. ethical considerations studying the political orientation of various media is already the objective of various institutions (allsides, ad fontes, media bias/fact check). it depends on many factors, and a reliable automatic identification is still out of reach of current models, as can be seen from existing experimental results, and some of the limitations underlined above. these models should thus not be used for something other than research purposes, or supporting human analysis. this is one of the reasons why we develop an explainable approach to bias predic- 11https://www.allsides.com/media-bias/ media-bias-rating-methods 12https://adfontesmedia.com/ how-ad-fontes-ranks-news-sources/ tion, but these also have their own limitations, and shouldn’t be used either as a strong indication of bias in one way or another without careful human examination."
2585,"limitations our dataset construction method has certain limitations. one important limitation is that it is difficult to get the distribution of the required commonsense knowledge types. this can be addressed in future work with human designed commonsense knowledge schema and human annotation. one potential risk of our work is that the text games may be limited by the time of writing, thus raise fairness considerations. however, our dataset construction strategy is not limited to these specific games, better sampling games can help to reduce such biases."
2586,"limitations in this section, we faithfully discuss the current limitations and potential avenues for future research. first of all, in the analysis, we observed that giving heavy weight to the soft loss at initial training epochs improves the convergence speed. yet, continuing training with such heavy weight to the soft loss could hinder the further performance improvement of the student. therefore, adjusting soft loss weights depending on the training phase from a larger value to a small value (e.g., using the time function) would be helpful for both convergence speed and improving the model’s quality. secondly, it has been demonstrated in the visual recognition domain that adjusting the temperature of distillation loss for poorly performed teachers can improve the student model quality due to the regularization effect. following them, increasing the temperature to smooth the soft labels from poorly performed teachers, such as 1-layer or 2- layer teachers, would help improve the quality of distillation via the regularization effect."
2587,"limitations in this work, we propose a detector that aims to detect adversarial samples via sharpness of input loss landscape for model. however, the computational cost of the sharpness is high because it requires at most k-step gradient descents. moreover, in this work, we mainly considered word-level adversarial sample detection as often studied in previous work, while character-level and sentence-level adversarial samples are not studied. these two problems will be explored in our future work."
2588,"limitations the proposed attack is specific to textual data while many membership inference attacks are universally applicable to all modalities as they mainly rely on loss values obtained from models, our proposed method for generating neighbours is specific to textual data. while standard augmentations such as rotations could be used to apply our method for visual data, this is not straightforward such as the transfer of other attacks to different modalities. implementation of baseline attacks as the performance of membership inference attacks depend on the training procedure of the attacked model as well as its degree of overfitting, it is not possible to simply compare attack performance metrics from other papers to ours. instead, we had to reimplement existing attacks to compare them to our approach. while we followed the authors’ descriptions in their papers as closely as possible, we cannot guarantee that their attacks were perfectly implemented and the comparison to our method is therefore 100% fair. ethical considerations membership inference attacks can be used by malicious actors to compromise the privacy of individuals whose data has been used to train models. however, studying and expanding our knowledge of such attacks is crucial in order to build a better understanding for threat models and to build better defense mechanisms that take into account the tools available to malicious actors. due to the importance of this aspect, we have extensively highlighted existing work studying how to defend against mias in section 6. as we are aware of the potential risks that arise from membership inference attacks, we will not freely publicize our code, but instead give access for research projects upon request. with regards to the data we used, we do not see any issues as all datasets are publicly available and have been used for a long time in nlp research or data science competitons."
2589,"limitations first, inference efficiency is one of the main limitations of this work. the bart model takes about 14 minutes to complete the inference on our dataset, while our utged needs 92 minutes. the reason for the slow inference is that utged requires heavy computation to update the gradient to the encoder’s states and decoder’s states (as shown in eq.7~eq.9). future work may consider how to advance model efficiency further. second, the lack of multimodal content in the published tweets would result in another limitation. the images contained in the published tweets are ignored in this work. however, due to the complicated relationships between images and texts in a multimodal tweet, images might provide complementary content and complete the meanings of the message (vempala and preotiuc-pietro, 2019). therefore, future studies might explore selfintroduction generation using multimodal tweets (images and text) to indicate personal interests."
2590,"limitations in this paper, we conducted an experiment on code summarization using two benchmark datasets, the java dataset (hu et al., 2018b) and the python dataset (wan et al., 2018). blocsum may need to be tested for its generalizability to other program languages. we chose two program languages (java and python) that were easily parsed to map the block position of code and ast. we believe that since other programming languages have similar syntactic structures, blocsum should be able to achieve similar performance on them as well."
2591,"limitations our experiments are conducted based on the t5base pre-trained language model. due to the computational resource constraints, we did not conduct experiments on other similar plms, such as bart, and t5 model with larger scale, such as t5-large and t5-3b. although we believe our"
2592,"limitations this work follows in line with those studies (poon and domingos, 2009; goldwasser et al., 2011; titov and klementiev, 2011) where unsupervised semantic parsing relies on the dependency parse trees of texts. although it enables us to leverage advanced syntactic parsers and to disentangle the complexity in syntactic analysis from that in semantic parsing, the errors made in the dependency parse trees created for input texts could propagate to semantic parsing. in the future, we would like to explore the feasibility of jointly performing syntactic and semantic parsing in a completely unsupervised fashion. even though an improved mh merge-split sampler was proposed in this study to speed up the mixing and convergence of markov chains by leveraging pre-trained distributed representations, the computational effort required to fit the model can still be substantial, especially for a large body of texts. we plan to improve computational efficiency beyond that offered by this study by starting with good initialization and updating the state space in a distributed and parallel manner."
2593,"limitations & risks limitations: in this work we present a novel method to address data scarcity issue for relation extraction (re). although our experiments demonstrate the effectiveness of the proposed method, there are still some limitations that can be improved in future work. first, similar to previous work (dos santos et al., 2015; veyseh et al., 2019), the current method assumes golden entity mentions to perform re that might not be the case in different applications. it is thus helpful to explore the method in a more realistic setting where entity mentions are predicted, e.g., using joint inference models to simultaneously extract entity mentions and relations in an end-to-end fashion. second, our method is currently evaluated only for sentence-level re (i.e., entity mentions are in the same sentences). future work can further explore our method for documentlevel re to allow entity mentions to appear in different sentences to better demonstrate its advantage. finally, our method requires the generative gpt-2 model for data generation. to perform well, gpt-2 needs to be trained on large unlabeled datasets that might not be readily available for low-resource languages. as such, it is important to further evaluate our method on low-resource languages to better reveal its effectiveness. risks: in this work, we employ gpt-2 to generate new training samples for the task of re. although gpt-2 is publicly available and the datasets employed in this work to fine-tune gpt-2 for re are also publicly available, a generative language model might produce biased sentences, insulting texts or reveal private information. as such, it is necessary to take further measures before publicly releasing the automatically generated labeled data. to this end, we inspect the data employed for finetuning to exclude any offensive text and identity information. the generated data will also be inspected for purpose before publicly releasing the data."
2594,"limitations one of the advantages of the fusion-in-decoder approach is that it uses the off-the-shelf t5 architecture with publicly available checkpoints. the proposed fido modifications strongly improve performance and inference speed for retrieval-augmented question-answering, but require pre-training from scratch. it is in general preferable to have a small number of checkpoints that can be fine-tuned for any application. for example, it may not be feasible to train different giant language models for use in the retrieval-augmented setting. instead, the architectures for such large models may need to be a compromise for different use cases."
2595,"limitation, our primary contributions include: • counterfact+, a dynamic specificity benchmark, which adapts to the model edit under test, and is more sensitive than the existing benchmark • neighborhood kl divergence (nkl), a specificity metric based on the full probability distribution as a complement to the currently used metrics which focus only on the tokens directly implicated in the model edit. limitations the main limitation of the approach we took for improving model editing benchmarks is that it is ultimately based on manual inspection of test cases to understand the failure modes of model editing methods. this approach is not scalable and has a significant cost in terms of time and effort. as far as the specific benchmark we propose is concerned, more research is needed to assess its effectiveness for more complex scenarios such as dialogue and multi-turn conversations. we also have not investigated the application of our benchmark to scenarios in which multiple model edits are performed simultaneously. furthermore, we do not evaluate other types of model edits, such as parameter pruning, and transfer learning. future work should focus on developing methods that measure and quantify the effects of model edits on long-term aspects of language models, such as their ability to capture discourse structure and fluency of generated text. this could include corpus-level analysis and dynamic approaches like red-teaming or dynamic benchmarking to uncover subtle adverse effects."
2596,"limitations even though santa shows strong effectiveness on learning the representation of structured data, it heavily depends on the alignment signals between structured and unstructured data. such alignment relations can be witnessed everywhere, but the quality of constructed pairs of structured and unstructured data directly determines the effectiveness of santa. besides, we use the product bullet points and code descriptions as the unstructured data in our experiments, which is designed for specific tasks and limits the model’s generalization ability. on the other hand, santa mainly focuses on evaluating the structured data understanding ability through text data representation and matching. it is still unclear whether santa outperforms baseline models in all downstream tasks, such as code summarization and code generation."
2597,"limitations although our model has shown superior performance, there are still a few limitations that could be improved in future work. • we create few-shot datasets from the perspective of the combination of sentiment categories without considering the distribution of aspect items, such as the number of aspects in each sample. it may affect the performance of the model on the task of extracting aspects. we should create more efficient datasets for mabsa in the few-shot setting. • as we put more emphasis on the performance of the main task, the performance of the subtask of predicting the number of aspect terms in each example may suffer. we will further improve the accuracy of the subtask in future work. • we roughly exploit initial image features and do not perform alignment between text and image modalities. we plan to accomplish the alignment of multiple modalities further to improve the performance of mabsa in future work."
2598,"limitations we would like to acknowledge three categories of limitation that we recognize in this work: • we evaluate the effectiveness of priming in a setting where the tasks used during the priming stage and the fine-tuning stage offer no additional disparity besides being different in language, i.e., they are all ner tasks coming from the same domain. while this degree of variation is consistent with the application of meta-learning in other modalities, e.g., vision (finn et al., 2017), whether or not the gains we report here remain at the same strength when we introduce diverse tasks during priming and fine-tuning still needs to be tested. examples of such diversity include strong domain shift or using one task, e.g., pos, for priming and another, e.g., ner, during fine-tuning. • it’s not clear how the size of the pre-trained model affects the necessity of priming. priming might consistently result in gains, or its benefits might fade away with larger plms encoding stronger language capabilities. this also needs to be evaluated. • finally, our work does not implement higherorder gradient calculation and does not evaluate and discuss the potential additional gains that might come as a result. that opportunity can be further explored as well."
2599,"limitations let us begin with the obvious limitation: axomiyaberta only works on assamese. in addition, since assamese comprises a number of dialects and we trained on internet-sourced data, we have no clear evidence regarding which dialects axomiyaberta is most suited to or if it performs as well on non-standard dialects. axomiyaberta did not perform all that well on wikipedia title selection, compared to other transformer-based models. our best result is on par with xlm-r and close to indicbert-base, but well below mbert performance. we hypothesize that the amount of wikipedia training data in mbert is a cause of this, but we find that phonological attention makes a big difference in axomiyaberta’s performance (increasing accuracy from 26% to 59%). nonetheless, the reasons behind this subpar performance, and whether axomiyaberta can be improved for this task without, say, overfitting to wikipedia, need further investigation."
2600,"limitations there are several limitations to this paper. first, due to time and space constraints, we are unable to experiment with other interesting model compression techniques such as neural architecture search and quantization. we also have to select only a small subset of baseline text-to-sql models to represent the performances on each of the datasets. we are also aware of the existence of ryansql (choi et al., 2021), a sketch-based model for the spider dataset. however, we are not able to reproduce the baseline results to the best of our efforts and have to exclude them from our analysis. therefore, it is important to be aware of these potential limitations and biases when using our results for real-world deployments."
2601,limitation of hyhtm is that it is parametric and therefore requires empirical analysis to find the optimal number of topics at each level. we plan to investigate this shortcoming in the future.
2602,"limitations we propose and construct korc as a new benchmark dataset for deep text understanding. the limitations are two folds. first, in the benchmark design, korc do not take more complicated knowledge into consideration, including literal knowledge and qualifier knowledge. we leave extending korc to these knowledge in future work. second, in the dataset construction, we examine automatic name anonymization and question generation strategy, and present korc-l. korc-l relies on large language models. rather than medium-scaled language models that can be maintained by a single machine, gpt-3 is used via its online apis. although the service of gpt-3 is currently available, we still need to find a substitution for better reproducibility. besides, although llm saves human effort, the execution of llms potentially consumes more energy power. it would be better if we can preserve the high question generation quality and propose a small model to proceed data annotation."
2603,"limitations although the ihlda shows better performance than existing models in multiple experiments, there are three limitations that we did not fully address in this paper. first, the gibbs sampling is slower than other approaches such as autoencoding variational bayes (kingma and welling, 2014), which limits data scalability. we can incorporate the literature on distributed algorithms for topic modeling (newman et al., 2009; yu et al., 2015; karras et al., 2022) and variational inference (wang and blei, 2009; wang et al., 2011; bryant and sudderth, 2012; hughes et al., 2015) in future research. second, crowdsourced evaluation limits a corpus choice because we should not expect workers to have any prior knowledge (ying et al., 2022). our crowdsourced evaluation only used bbc news, the most accessible documents among the three corpora. future research can thoroughly validate the performance of the crowdsourced workers and trained coders. existing literature (buhrmester et al., 2016; kees et al., 2017) found that mturk had a comparable quality against traditional survey panels, but they did not use mturk for evaluating outputs from a machine learning model. third, an estimated hierarchical structure does not necessarily match the semantic hierarchy human readers expect. this mismatch is not surprising because unsupervised models do not directly incorporate information about a tree structure. existing papers improved the interpretability of flat topic models by providing topic-specific sets of keywords (jagarlamudi et al., 2012; harandizadeh et al., 2022) and labels (mcauliffe and blei, 2007; ramage et al., 2009), which is a future direction for a hierarchical topic model."
2604,"limitations currently, we build a vocabulary from the original one used in mbart-50, and only conduct downstream experiments across 6 languages (english, german, french, indonesian, japanese and chinese). although we could involve more languages, it would require a larger cuda memory that might go beyond our device capacity. hence, we merely select the above languages that have sufficient overlap with our pre-training datasets. in addition, for fair comparisons, we only use the strictly-aligned multilingual multi-modal dataset provided in (zhou et al., 2021), which is augmented through machine translation. it is unclear how the quality of strictly-aligned dataset would affect model performance. meanwhile, the length of texts in our weakly-aligned multilingual multi-modal dataset is generally very long. as a result, we truncate textual inputs before feeding them into the encoder, possibly bringing information loss to some extent."
2605,"limitations our method regen is a general framework for zero-shot text classification. in this work, we aim to first bring in simple and intuitive way to justify the power of unsupervised dense retrieval for zeroshot learning. effective as it is, there is still much room for improvements, including designing better objectives for pretraining rθ as well as better strategies for removing noisy training data (lang et al., 2022; xu et al., 2023). how to improve these components is an important line of future work. besides, our experiment results are all based on bertbase sized models. although regen performs on par with or better than previous dataset generation methods using giant nlg models, it remains unknown to us how the benefit of regen scales with more parameters for both rθ and cϕ. also, we point out that this work focuses on zero-shot text classification with task-specific verbalizers and unlabeled generic corpus, thus it can be nontrivial to adapt our framework to other tasks such as natural language inference (nli) as well as low-resource tasks where even the unlabeled generic corpus can be hard to collect. extending regen to these settings will reduce the annotation burden under more challenging scenarios."
2606,"limitations there are three limitations. first, our model requires the retrieval of relevant structured and unstructured knowledge from different knowledge sources, which can be time-consuming. using cosine similarity over question and fact embeddings can be a bottleneck for the model performance. second, our model focuses on rich background knowledge but might ignore some inferential knowledge, which can be acquired from other sources such as atomic. third, our model might not be applicable to low resources languages where knowledge graphs are not available."
2607,"limitations the major limitation is that we test our method only on a binaural speech dataset, in which there is a person moving slowly while speaking. because this person moves slowly, the doppler effect is not so obvious. we will try to find or collect a sound dataset of a source moving at high speed, such as a running man, flying objects, or vehicles, and further, analyze the experimental phenomena at different speeds of the moving source."
2608,"limitations while the results have shown the effectiveness of our framework in ie without using any additional resources, we did not explore the potential enhancement by utilizing existing resources in the easy-tohard learning process. on one hand, we can build the easy stage with the help of existing data of simpler tasks. on the other hand, the data of harder tasks can be used for the hard stage. to enhance the e2h framework via effectively using existing resources is an interesting and promising direction. another limitation is that we did not extensively explore the possible skill sets for each task. exploring more approaches to obtain the skill sets is also open for future research. we plan to investigate these possibilities in our future work."
2609,"limitations although our geometric embedding approach can handle a complete set of basic fol operators (existential quantification, conjunction, disjunction and negation), the modeling of negation operator cannot narrow down the predicted answers to relevant topics of atomic queries. for example, one can expect the answers of this negation question/query “list argentina players who are not lionel messi in world cup 2022?” to be any teammates of lionel messi (i.e. 2in query structure). however, the current model is designed to return all elements in the entire entity set except for lionel messi, which have redundant objects (e.g. trees, music, houses). this is a common limitation not only in geometric-based models but in others using fuzzy sets representation. this is due to the fact that the modeling of negation operator is assumed to be the complement set of a questionable entity w.r.t. the entire entity set. our hypothesis is that the expected answers should be narrowed into the complement set w.r.t. a sub-topic of relevant entity set. in addition, when apertures of two sector-cones are obtuse angles, the current calculation of partial intersection cannot correctly model the conjunction operator. this special case is inevitable in a system using geometric representation that is closed under negation and conjunction, but not for disjunction (see appendix a.2 for further details)."
2610,"limitations this work requires that news videos are organized into different events and each event has more than one candidate video. the debunking rectification module relies on the existence of labeled debunking videos, and the graph aggregation module relies on existing fake news detectors to provide the initial features for each video. the textual length in videos is limited due to that the debunking inference module is based on a pre-trained bert model with limited sequence length."
2611,"limitations: despite these successes, our current work is still limited in the following ways, which we leave to future work: • our current model is based on pretrained gpt2 (radford et al., 2019), and therefore its generation ability is limited that of gpt-2. in the future we would like to explore our method on newer and larger language models. • human labels are currently provided at the sentence level, either a rating of the whole sentence or providing a new sample sentence. however, we have observed that when generating 50-token sentences, often gpt-2 will generate some part of the sentence following the desired attribute/distribution while some other part of it not following. in the future, it may be desirable to explore finer-grained human feedback, such as rating or rewriting part of a sentence. • our experiments are performed on low quantities of data to demonstrate that our method works under a few-shot setting. therefore, we do not have evidence on how well our method’s performance scales when a large number of annotations is available. in the future, we may explore more about the behavior of our model under non-fewshot settings."
2612,"limitations while the proposed method has demonstrated superior performance and high efficiency, there are several limitations that warrant further investigation: (1) in few-shot settings where the number of training examples is limited, the performance of our method and other baselines drops significantly. future work should focus on uncovering essential features of the task in few-shot scenarios and generating embeddings of higher quality. (2) the storage consumption has been reduced to a small amount, however, the number of neurons is still relatively large compared to that of heads and therefore becomes a bottleneck for further decreasing storage requirements. as discussed in sec 5.2, one possible solution is reducing the number of layers used to generate the embedding. future work could also include assigning intermediate neurons into groups to make the embedding coarser in granularity, thus reducing storage requirements."
2613,"limitations although our proposed method achieves the stateof-art performance, it still has a few limitations. firstly, we only consider the dependency between aspect and opinion in the target text yet ignoring the order influence in the input text, which may bring more improvements. secondly, there are three label types for aste, including aspect, opinion, and sentiment. currently, we only utilize the aspect and opinion markers in the marker-oriented sequence labeling module. we believe that the specific design for the sentiment marker can further improve the performance, which can be a future direction."
2614,"limitations in this paper, we mainly focus on evaluating our approach on two english-centric corpora, iwslt17 and pc32. future research could consider more multilingual machine translation benchmarks with different number of languages and training samples and conduct experiments on more challenging training scenarios such as chain configurations where we have multiple bridge languages and different zero-shot distances."
2615,"limitations we state the limitations of this paper from the following three aspects: 1) regarding linguistics-aware data construction, we only perform seed-guided pattern enrichment for four reaction roles (product, yield, temperature, and time, see table 4) due to the lack of sufficient reliable patterns for other roles. incorporating more advanced pattern mining methods (li et al., 2018; chen et al., 2022) may alleviate this issue and discover more reliable linguistic cues, which we leave for future work. 2) as in the previous work, we adopt a fixed reaction scheme to extract structured chemical reaction information. however, there are always new informative roles in the text (jiao et al., 2022), such as experimental procedures (vaucher et al., 2021), so how to predict both roles and arguments without being limited to a fixed scheme could be a meaningful research topic. 3) reactie is capable of detecting chemical reactions within scientific literature by predicting if a given passage contains a product. however, accurate text segmentation of a paper remains an unresolved and crucial issue. incomplete segmentation may result in the failure to fully extract reaction roles, while excessively long segmentation may negatively impact the model performance. therefore, integrating a text segmentation module into the existing two-step pipeline may be the next stage in the chemical reaction extraction task."
2616,"limitations first, as ear largely relies on gar generators, the performance of the method is closely tied to the quality of the generator used. we have attempted to use large language models such as t0-3b without fine-tuning as a replacement for the gar generator during testing, but the performance becomes worse. the main reason is that the quality of query expansions generated by t0-3b is too diverse, which makes ear has a higher chance to select from terrible expansions. in contrast, the output quality of gar is more stable. we may need a more complex mechanism that can exclude terrible query expansion if we want to directly use the query expansions generated by t0-3b during inference. second, ear has demonstrated a strong generalization ability to out-of-domain data, but the method may still face challenges when transferring to other languages without any supervised qa data, which gar and ear are trained on. although challenging, we are still trying to train the ear system without supervised qa data."
2617,"limitations our model is evaluated in standard english datasets for classification. as we stated earlier we plan to investigate the cross lingual setting in the next step. the iterative nature of self-training imposes a high cost on the experiments. this has led to a few common practices. most existing studies (including all the studies that we used as baselines) employ one underlying classifier to carry out the experiments–i.e., bert or rnns. this practice albeit limiting, is justified by the argument that if an algorithm does not make any assumption about the underlying structure of the classifier, then one can safely select the best available classifier and use it in the experiments. we used bert in our experiments. another limitation is that, which is again stemmed from the high cost of self-training, one is typically forced to select a few sample sizes as labeled sets to carry out the experiments–e.g., 100 or 300. this is in contrast to similar research areas, such as active learning, when one can usually afford to report a learning curve to illustrate the performance with a few training examples all the way to using the full labeled dataset. given that we have 10 baselines, we reported the performance with 300 and 500 labeled examples."
2618,"limitations in this work, we focus on small and medium size models (up to 134m parameters), while recent work in large language models (llms) targets models with billions of parameters(brown et al., 2020; chowdhery et al., 2022). it is unclear how well the performance improvement from the examined network architecture would translate to other model sizes or baseline architectures, e.g., gpt models. further on, it is unclear how these findings may translate to other application domains and datasets, or impact other nlp tasks, such as document retrieval/ranking. we will investigate these directions in future work."
2619,"limitations we use the density matrix to represent modal features, and one of the advantages is that the matrix contains more information. however, the requirements for memory and large gpu resources also in- crease. based on the best hyper-parameter setting, the shape of a pure state is 16×100×100, while the shape of a density matrix is 16×100×100×100. at the same time, the matrix will also increase the calculation and time cost. in future work, we will explore how to reduce the computational expense, and it is an idea to build the sparse density matrix."
2620,"limitations of our work as follows: • we only validate the effectiveness of the setmatching strategy for generative models on the coqe task. • we observe that the scale of the coqe datasets is quite small and has caused the model’s overfitting problem. in the future, we will conduct further research from the following perspectives: • explore further application of the setmatching strategy in multiple research directions, such as information extraction, sentiment analysis, etc. • utilize unsupervised data to better help the models mine comparative opinion information. • design data augmentation methods to relieve the data sparsity problem."
2621,"limitations as mentioned above, the current study is limited to the question of whether (and when) conditioning turn-taking prediction on the response improves the performance. it does not yet show how the model could be incorporated in a spoken dialogue system. moreover, this study focuses only on written conversations without incorporating spoken dialogues. thus, the interpretations can be limited to dialogues that are relatively ‘formal’ without hesitations, repetitions, etc. note also that we only analyse lexical cues to turn-taking (just like with turngpt), and leave out other modalities for future work."
2622,"limitations of each, and discussing some future directions. 19this is also related to warstadt et al.’s (2020) results, who show that better pre-trained models are less prone to rely on superficial (and potentially spurious) features for predictions."
2623,"limitations we consider only lexical bias based on the cooccurrence between a token and a certain label in data bias for identifying shortcut tokens, while nlu tasks involve various types of data bias, e.g., overlap bias, position bias. although our method can mitigate llm-based task-specific models’s reliance on shortcut tokens, it can only identify a limited set of bias in the data. therefore, in the future we would like to incorporate more data biases to identify shortcut tokens and discourage llms from exploiting them."
2624,"limitations although our proposed aligner has surpassed the existing lm-based alignment extraction methods in most of the datasets, it could not make any improvement for the en-fr language pair, as shown in table 1. this suggests that our proposed method might be only beneficial for more distant languages. on the other hand, for similar languages, it not only cannot add any information to the similarity matrix, but also its estimation for the alignment probabilities might add noise to the alignment extraction method. thus, investigating ways to more effectively estimate the alignment probabilities of source and target tokens might be helpful in future work. another limitation of our method, as well as other lm-based aligners, is that they first extract subword-level alignments, and then heuristically map them to word-level. by observing the aligner outputs, we realize that many errors occur when the pre-trained lm can not efficiently split words into meaningful subwords. this happens more often for low-resource languages or far languages from english (like persian or hindi). thus, achieving better subword tokenization in pre-trained lms or applicable methods to convert subword-level representations into word-level could help improve the quality of lm-based aligners."
2625,"limitation by working in two directions: leveraging the intrinsic characteristic of frameset resources, including semantics-based clusters and cross-predicate role semantics, and tighter integration of other semantics-based tasks, such as word sense disambiguation, into srl. we hope our work will be a stepping stone for innovative research on high-performance srl systems for non-verbal predicate-argument structures, a problem that still needs extensive investigation. for this reason, we release our software and datasets at https://github.com/sapienzanlp/ exploring-srl. 3https:/"
2626,"limitations part of our analyses and experiments is based on our parallel-semlink dataset, which provides parallel annotations for propbank, framenet, verbnet, and verbatlas. we take the opportunity to remark that this is a constrained setting, as these resources cannot be mapped 1-to-1 without losing information. as such, this setting may not provide the full picture of how these resources compare against each other. however, we also believe that a setting like this can at least provide an intuitive idea of the role of a linguistic resource in crossinventory generalization. creating novel benchmarks that can better compare the role of different linguistic resources is certainly a direction for future work that may provide novel insights into verbal and non-verbal srl. another limitation of our work is the small size of challenge-srl. even though challenge-srl contains only about 300 sentences, it features almost 2000 predicate-argument pairs, and this is a number that is sufficient to show the inability of a current state-of-the-art system to generalize across predicate types. we acknowledge that a larger benchmark may have provided further insights. however, we also note that, in our case, increasing the number of annotations would hardly have brought us to a different"
2627,"limitations of monolingual factuality metrics. moreover, our exploration of the automatic factuality evaluation in cross-lingual settings illustrates its challenging nature. limitations the scenarios we studied are limited to chinese to english and english to chinese. for other languages, the factual characteristics may be different. the genre of the source documents we study is news or blog post. for other genres, such as dialogue, our"
2628,"limitations our work sheds light on understanding the training dynamics of cross-lingual transfer learning of multilingual lms. in our work, we selected to use english as the source of cross-lingual transfer following previous work (vu et al., 2022). we acknowledge that using other languages as the source language can provide benefits depending on the task (lin et al., 2019; turc et al., 2021). our work does not focus on choosing source language to maximize downstream performance but instead focuses on the difference between classification tasks and generation tasks in cross-lingual transfer. secondly, we acknowledge that some of the datasets (yang et al., 2019; chen et al., 2022) used in our work are created by machine translation and human annotation. previous studies have pointed out that translationese in datasets affects cross-lingual transfer performance (artetxe et al., 2020a; artetxe et al., 2020c). we believe that translationese in datasets also have impact on xlrs. we leave the study of how dataset features (size, quality, translationese) affect cross-lingual transfer for future work."
2629,"limitations the introduced dataset has a moderate scale, as it is currently designed for fine-tuning instead of large model pretraining. our proposed collection scheme can be futher applied to enlarge the dataset. moreover, as we focus on english, the data source has multiple language versions written by experts. hence, extending causaldialogue to multilingual is straightforward. with reward labeling, the dataset can be more intuitively used for offline rl. meanwhile, the dataset includes personality descriptions that can be used for personalized dialogue generation, even though is not the focus in this paper. finally, training a generative model on dialogue domain can require various computational costs, depending on the aspects such as lengths of input and output texts and number of model parameters, as well as special designs to prevent misuses."
2630,"limitations although lclr shows great potential for unifying the slu decoding process, existing slu models experiment on a set of predefined labels (closed domain), and our lclr can handle the case of missing predefined labels in the train. it is interesting to try to perform lclr on a more challenging task of detecting out-of-domain (ood) detection where unseen intents/slots are not available."
2631,"limitations our study has limitations in two aspects. first, multilingual transformers support a wide range of task types, and it is challenging to study our research question on all types of end tasks. we conduct experiments on two common types of end tasks, i.e., text classification and question answering. we leave the study on other types of end tasks in further work. second, under pmid, we only consider the situation that the end-task models are obtained by finetuning public pretrained models. the cross-lingual transfer of black-box end-task models is also an interesting research topic to study. besides, plugin-x reassembles the modules from publicly-available models rather than training from scratch, so it can naturally inherit the risks from those models."
2632,"limitation since there is currently no standard evaluation metric for evaluating post-hoc explanations, we use aopc(k) as the quantitative evaluation metric, which is widely used in the research field. however, because different modification strategies might lead to different evaluation results, aopc(k) is not strictly faithful for evaluation attribution explanations (ju et al., 2022), thus, we evaluate with two modification strategies del and pad and we didn’t introduce new strategies to get attribution scores, which avoid the risk of unfair comparisons due to customized modification strategies mentioned in ju et al. (2022). even so, there is a risk of unfair comparisons because the aopc(k) tends to give higher scores to erasure-based explanation methods such as loo. we don’t conduct human evaluation because we believe human evaluation needs a very large scale to guarantee objective and stable, of which we can afford the cost. thus, we post visualizations of all explanations in our experiment to demonstrate the effectiveness of our approach (https://github.com/juyiming/ he_examples)."
2633,"limitations our proposed degm for event skeleton generation still contains some limitations: • it only considers the problem of event skeleton generation, a subtask of temporal complex event schema induction. it is promising to explore the whole task, which includes entities and entity-event relations. • perspective from errors found that our model suffers from a tendency to generate correct duplicate substructures."
2634,"limitation of the popular softmax layer is its global word embeddings. the problem would become more serious when there are more tokens whose meanings are locally defined (e.g., names in the booksum dataset). our methods would be more useful in those circumstances and might alleviate some biases described in shwartz et al. (2020) and ladhak et al. (2023). moreover, the meaning of tokens are also locally defined in many other applications such as variables in code or math problems, the new terminologies in a scientific paper, or the products in a sequential recommendation problem. we believe that our methods could become an efficient alternative of reranker (cobbe et al., 2021; welleck et al., 2022) and create impacts in those areas. finally, our results show that when there are some uncertainties in the next word (e.g., could be king or woman), existing lms could have some difficulties of copying the words from the context and our methods alleviate the problem. thus, our methods should also be able to improve the lexically controllable language generation models that put the desired keywords into the context such as goldfarb-tarrant et al. (2019) and lu et al. (2021)."
2635,"limitation our primary focus is on the ood robustness of text classification tasks. however, there are other nlp tasks that the community should not ignore. glue-x currently does not include language generation tasks such as machine translation, summarization, and dialogue. moreover, extending the current glue-x to more real-world datasets from different domains is of great importance. we aim to make glue-x a continuously maintained project."
2636,"limitations masking accuracy of classifiers reported in this work is for masking both the training and the test data for reasons explained in section 5.1. each result is for a different type of mask and hence for a different test set. our results cannot be used to gauge performance for unmasked or differently masked inputs that can be expected in applications. rationale evaluation with roar to evaluate rationales, roar uses the performance of a model trained and tested with input masked according to the rationales. the reported numbers therefore do not only reflect the quality of the rationales but also the difficulty of the task, the size of the training data and the performance of the machine learning method. furthermore, the measurements are influenced by randomness in training as the masking of training data changes the path of the optimisation process.13 the values have to be seen relative to baseline performance of full and rrand and in comparison to different types of rationales. domains the experiments are restricted to the two domains of the dataset, namely restaurant and laptop reviews, with just 28 aspect entity types and 14 attribute labels. we encoded these in a shared vocabulary with the review sentences.14 we did not explore alternatives such as using reserved embedding table entries to encode the domain and aspect categories (so that tuning these embedding table entries does not affect the embedding of the tokens of the review sentence) or using more natural question templates, e. g. adding function words where appropriate and lower-casing the categories. performance differences may change for other domains, number of aspect categories and the ratio of the training size of smallest domain and largest domain (in our work 4:5). task the absa task (see section 1) assumes that the aspect category is already marked in the input and labelled with entity type and aspect category. number of test scores on first sight, the high number of test scores could be a concern as testing many models on test data can lead to overfitting to test data. however, only the result for the full setting is a vanilla test set result. all remaining results are testing on derived (masked) test sets matching the masking applied to the training data. therefore, these results do not leak performance information for building better classifiers on the test data. using the test set here is convenient as the data set does not come with a validation set and the validation set we held out from the training data for selecting the training epoch is very small. language experiments are for english only due to availability of sea data. various factors may cause different patterns for other languages, e. g. (a) bert subword units, (b) evaluation excluding function words vs. languages that use mostly morphology instead, (c) freer word order may result in annotators producing more discontinuous ses. 13our reporting of averages over twelve runs, and in some cases 24 runs, compensates for the latter effect as each run shuffles the order of the training data and uses a new random initialisation of the classification head. 14for the aspect entity type and attribute label, sharing is reduced by using capital letters."
2637,"limitations in this work, we limit our experiments to the most commonly used document-level system architec- 6the precision and recall are roughly the same for the f1 scores reported in table 4. ture and training criterion. other approaches exist, which might exhibit a different behavior in decoding. two out of the three document-level translation tasks we use in this work are low resource with less than 500k sentence-pairs as training data. we chose these tasks due to computational limitations and to be better comparable to other works, but higher resource scenarios are more realistic for actual applications. we limit the analysis of pronoun translation to the english-german language pair. also, there are other aspects of documentlevel nmt, like consistent translation of entities, which we did not consider in our analysis."
2638,"limitations we discuss limitations and ethical consideration of our work. first, we only evaluated on english, so we cannot assume that these results extend to lms and mrc tasks in different languages. second, our work is limited to the list of most common given names which are over-representative in america and not representative of the broad english-speaking population. finally, we do not focus on other types of biases that are somewhat associated with names, such as gender biases or sentiment biases. we expect these limitations to be addressed in future work."
2639,"limitations future work must look into the generalizability of the results presented here in other domains of language use, and other languages. while we present the utterances as constituting natural speech by one speaker (the congressperson who sent the tweet), it is likely most congresspeople employ social media teams that help in crafting the language of some of their tweets. however, we believe for the sake of interpersonal group membership, the relationship between the speaker(or speakers) and their target(s) would not be affected. techniques like inlp extract information that is linearly extractable. while we’ve shown that it is possible to extract and manipulate language information using such simple linear techniques, more complex methods like those proposed by ravfogel et al. (2022) might be able to manipulate more non-linearly encoded properties. the alterrep procedure, as can be seen in our results and in ravfogel et al. (2021), is sensitive to parameters like α and the number of inlp iterations. picking these parameters is tricky and we have done it in a manner that preserves information in the language model. it is possible that a different set of settings not explored here could lead to different results."
2640,"limitations our work is limited as it has not explored the effectiveness of our implicit memory transformer in other tasks outside of simulst, such as asr. we have also not explored the impact of our implicit memory left context on alternative blockprocessing-based transformer models. furthermore, extensive ablation studies could help showcase the potential of the implicit memory transformer."
2641,"limitations our experiments are conducted on transformerbased models with the same multi-modality fea- tures. considering the importance of entity information in textual context and specific regions of images, it is also important to investigate whether the performance of the model promotes with different methods of extracting multi-modality features. we use the bpe technique to encode all entities in input articles which may separate the whole entity word into several sub-word tokens and may affect the impact of vision features. there are still a lot of entities of captions that don’t appear in articles from datasets on our experiments."
2642,"limitations although the work is aimed at better understanding bert’s internal representations, there is no transparent way to know on the basis of what features of the training data some particular sentences are found to be similar. for task 2, the representations may have been affected in unexpected ways by the process of creating averaged sentence embeddings. there is no way to fully exclude the effect of lexical context and thus get a representation of the meaning of a construction without noise in unsupervised transformer models, which may affect the extent to which we can accurately probe for a construction. in task 1, the set of potential words that could be predicted is limited by the bert tokenizer’s vocabulary. some limitations are caused by our choice to compare to stefanowitsch & gries’s results. we used the relatively small corpus that they used and we have demonstrated the methods for a limited set of two english constructions. we also limited the analysis of the results to the top 20 most strongly associated collexemes, as they did. using a larger corpus would probably yield more than 35 instances of the x-waiting-to-happen construction that s&g found and that our reproduction yielded. we also did not experiment with ungrammatical or perturbated input as such results cannot be compared to the original corpus study, which only uses natural language data. the scope of our study was also limited by to the construction-specific data collection, preprocessing and manual annotation required. for modern web-scale corpora, task 2 would require significant gpu resources. as the way in which bert is trained clearly differs in many ways from how humans acquire language, also according to the construction grammar framework, this bert-based work does not warrant any claims about how human language works besides extremely broad ones and findings are limited to"
2643,"limitations; benchmarking cd with an existing standardized measure yields no simple answer to the question whether we are now talking about children’s actual tom. that does not make standardized tests uninformative, but contextualizes their merit: if we agree that tom (and language) are social competences, we should also test them in social contexts, not to claim superiority over but rather complement work done in controlled settings. our classroom context has as advantage regarding tom, that children feel more motivated to do a fun task, engage with narratives as natural finding place for mental state content, have freedom to explore the (social) scenario they want, and that their language has a social goal: immersing the audience in their narratives as possible worlds. this social context may stimulate children more to challenge their language skills. to entice their audience, children may leverage their vocabulary skills to refer to rare settings, uncommon objects, unorthodox characters, and special social situations which is not possible in standardized language tests like the peabody picture vocabulary test (dunn and dunn, 1997). additionally, children may also recycle complex linguistic structures and plots from prior exposure to narratives in their own narratives, to entice their audience. thus, the influence of the social context could result in more complex language use than one would expect based on age, which makes the direct relation between age and language competence in narratives less obvious. overall, our results support the link between more complex language and tom. that said, not all tom-related content requires complex language. explicating character thought could linguistically also be represented without complement, e.g. with free direct thought (‘was she angry with him?’) (leech and short, 2007; van duijn et al., 2022); moreover, the words used in this thought are not complex, nor is the syntax. this example serves to illustrate the point that in our approach, our classifier makes no assumptions at the outset about the linguistic complexity of tom-related content."
2644,"limitations we discuss a few limitations of our work. one limitation of self-improved is its complexity in usage. the data augmentation process involves generating predictions for the entire training dataset with a large beam size, resulting in a time complexity of o(nk), where n is the train dataset size and k is the beam size. additionally, the fine-tuning step to derive θimproved also adds a significant amount of computational complexity. this limitation is discussed in section 6 to weigh the performance benefits of our method against the computational sacrifices. another limitation is that self-improved has only been applied to encoder-decoder models in this work. however, it is also applicable to other types of auto-regressive models, including encoderonly models, which are commonly used for tasks such as code completion (radford et al., 2019; lu et al., 2021; guo et al., 2022). a few models can be named are gpt models (radford et al., 2019; brown et al., 2020), codex (chen et al., 2021), codegen (nijkamp et al., 2022), etc. further research into these applications is left for future work."
2645,"limitations a fraction of 157 essays was too long to fit into our transformer model so that arguments later in the text have not been identified at all. as gold standard information about the writing prompt for a specific essay was not released with the dataset, we had to rely on automatically assigned prompt information with an estimated average accuracy of 0.97 according to ding et al. (2022). in a realistic class-room setting, however, the information about the writing prompt would be readily available, thus we probably underestimated the effect of adding prompt information. we tested our models on the persuade dataset of english high-school writings only, thus we cannot be sure whether results transfer to other educational contexts and languages. we will address this further in future work, where we aim at using essays in german and from efl contexts as well. since our model was trained on a limited amount of data, it may have the potential risk of discouraging students to write innovative, but effective arguments. as discussed in automatic essay scoring approaches, computers may be able to analyze writing for the presence or absence of certain words or structures (in our case arguments), but they cannot really understand or appreciate a writer’s message in the same sense that human readers can (powers et al., 2002)."
2646,"limitations the main limitation of this work is our use of automatic metrics rather than human evaluation. first, the score distribution produced by a metric is not guaranteed to be similar to one produced by human annotators, which could influence results. secondly, the metrics we examined do not incorporate context. motivated by evidence that document-level (or contextual) information is becoming necessary to distinguish between human translations and high quality machine translation (läubli et al., 2018; toral et al., 2018), recent wmt evaluations have incorporated context. since the human annotations are influenced by the context in which they appear and the automatic metrics are not (i.e., given an identical segment in two different contexts, the automatic metric will score them identically while a human annotator may not), additional study may be necessary to answer questions such as whether additional preceding source context should be displayed to annotators (as suggested in castilho et al. (2020)), to determine how much additional time reading this context would take (which may influence the annotation budget), or to determine whether human annotator behavior may differ based on where in a document the snippet comes from. we also do not directly address issues such as the best interfaces for human annotation; a problem that is mostly orthogonal to the question of what data should be annotated. in this work, we also follow the approach in the wmt metrics shared task of treating the scores assigned to systems (in our case by automatic metrics rather than human annotators) as full rankings of systems, rather than as clusters of systems. in practice, this may mean that statistically insignificant differences between systems are considered on par with statistically significant ones when we examine reorderings that occur based on different sampling procedures. while this is a major concern in human annotation (where there is also an effort to handle annotator variation, a separate source of instability), it is less of a concern in this setting where the annotation is guaranteed to be consistent. one additional limitation to our proposed future work of using metrics as a pre-sampling approach is that they may not perform equally well across all languages. see appendix a for the list of language pairs on which these experiments were performed."
2647,"limitation to better understand the limitations of the proposed model, we carry out an analysis of the errors made by pigeon. specifically, we randomly select 100 instances that are incorrectly predicted by pigeon and summarize the primary types of error. the first error category is boundary prediction error. since we modeled the ape task as a sequence labeling, our model may only recognize a part of an argument. thus, multiple consecutive arguments may be identified as a single argument. the second type of error is caused by the absence of semantically similar words in the argument pairs. in this case, the proposed probing graphs cannot model the relations between argument pairs. third, another error category occurs when semantically similar words are also presented in non-matching argument pairs. the argument relation may be misled by these words. it suggests that certain relation modeling method needs to be devised in the future so as to better infer argument relation. for example, we may leverage the high-level topic information over argument pairs to guide the learning of relation-specific features. in addition, the proposed probing approach may be computationally expensive and we can alleviate this problem by saving the similarity of all word pairs for one time for the entire dataset. we will address this issue in future work."
2648,"limitations despite the strong performance of diffusum, its design still has the following limitations. first, diffusum is only designed for extractive summarization, and the diffusion generation module only generates sentence embeddings instead of tokenlevel information. thus, it is not applicable to the abstractive summarization setting. moreover, diffusum is only tested on single document summarization datasets. how to adapt diffusum for multi-document and long document summarization scenarios need further investigation. in addition, our generative model involves multiple steps of noise injection and denoising, compared to discriminator-based extractive systems."
2649,"limitations this work has some limitations regarding the architecture and the data used: our model assumes that masked modeling could be used as rehearsal and anticipation tasks; however, other approaches could also be effective. we use transformer layers, so our model scalability is tied to the scalability of the transformer model. also, our approach relies on obtaining additional annotation from pre-trained models for the masking process, so we are limited to the misprediction of those models. we only tested our model with the english language; further exploration of other languages would be valuable to validate the language-independent functionality of the model."
2650,"limitations of our method are as follows: (1) despite the better performances our method deflate achieves on multiple ave experiments, its mechanism of using attribute as queries needs to construct the same number of sequences as attributes for a target product, which requires more time for training and evaluating when there are particularly many attributes to consider. (2) the low f1-micro scores of deflate and all other leading methods for ave on our desire dataset emphasizes the demand of further researches for the information extraction of the e-commerce products with implicit attribute values. and we would explore strategies such as incorporating external knowledge (structured or unstructured) to further enhance the ability of our method on the ave task in future works."
2651,"limitations for now, the latent of our skill is sampled from a distribution, whose flexibility is not fully investigated. we intend to exploit more flexible skills and goal discovery, or direct generation via state or action. additionally, the sparse reward in text-based games is also a burning challenge, which hinders the efficient exploration of agents. our abstracted action, skill, to some extent eases off this issue, but is not enough. we will dive into this more and design a fancy solution later."
2652,"limitations although our single-site interchange interventions provide causal evidence that particular sub-circuits are necessary for a particular downstream behavior, this technique has known limitations addressed by recent distributed alignment search (das) approaches (geiger et al., 2023). first, it will overcount certain “synergies:” when a single effect is jointly produced by the conjunction of multiple heads acting in concert, we will identify all heads as making distinct contributions to the circuit. second, it will under-count “redundancies:” if there are multiple heads that are individual sufficient to produce the effect, then no single head will be detected as strictly necessary. ideally, rather than single-site interventions, we would explore all combinations of different heads to find minimal spanning sets that are both necessary and sufficient, but this procedure becomes intractable given the number of heads, requiring more sophisticated optimization-based approaches to find promising sets (e.g. csordás et al., 2021; de cao et al., 2022)."
2653,"limitations due to the lack of research in this area, there is only one direct related paper to our work, which serves as the main baseline in our experiments. we hope we can compare our method with more related works to verify its effectiveness in the future. also, the domain adaptation problem not only exits in the machine translation filed, but also various generation and understanding nlp tasks, where we should evaluate our method on if we are not limited by time and resource."
2654,"limitations the most notable limitation of our work is the lack of external context. consideration of external contexts that may be relevant for the classification task in our current models, such as the profile bio, user gender, post history, current and past political scenarios of the concerned region, and so on, might prove beneficial for the results in this field. our research now focuses majorly on only six types of social biases rather than all conceivable degrees of prejudice. we also focused on utilising hindi, english, korean, and italian in our study, and the hindi dataset is primarily from the indian context. the limited scope of concern can be further explored with our presented experiments to prove to be fruitful for a wider range of audiences by covering datasets bias annotations pertaining to other low-resource languages. we show the effectiveness of few-shot transfer learning using language models with relatively fewer parameters as compared to recent state-of-the-art language models."
2655,"limitations our model improves topic interpretability of ntm with seed words, but we believe there are still limitations to be explored in the future works. for the methodology part, large-scale pre-trained language models could be considered to provide more context information when incorporating seed words. for the experiment part, only single label dataset are used for extracting seed words, and more explorations on multi-label datasets should be conducted."
2656,"limitations although our proposed curriculum can be applied to any multimodal architecture, curriculum aware loss requires modifications for use with dual encoder architectures that don’t use cross-modal attention. additionally, we use an off-the-shelf partof-speech tagger to divide the data into different phases. as such, the correctness of this division is dependent on the quality of tagger. a poor tagger can negatively impact the curriculum design. moreover, our approach doesn’t apply to possible image-captions dataset which contain only short captions, containing possibly only one noun."
2657,"limitations 1. error accumulation: incorrect parsing results will affect the reasoner. in our experiments on geometry3k dataset, incorrect parsing results lead to a substantial 21.0% performance drop. 2. the reasoner relies on manually predefined theorems, which limits its adaptability. 3. the random exploration in the rl training process leads to uncertainty in the rate of convergence."
2658,"limitations our work is the first study of generative asqp task from the view of what not to generate. despite the state-of-the-art performance and template-agnostic effectiveness, our work still has limitations that may guide the direction of future work. firstly, implicit information is still challenging for uaul. failed cases in error analysis §3.3.6 demonstrate that tough cases require in-depth semantic understanding. though uaul achieves wide improvements in the generation paradigm, it struggles to deal with implicit cases. secondly, in this work, we only design tokenlevel marginalized unlikelihood learning. since aspect sentiment quadruplets contain four types of information, considering span-level and whole sequence-level negative sample learning may attain further gains. thirdly, uaul increases the training time, as shown in table 8. we optimize the implementation by parallel computation. meanwhile, mc dropout is only adopted in the last dropout layer. the training time is still significantly enlarged. nevertheless, our method does not require additional human labor, which has obvious advantages in real applications."
2659,"limitations in this paper, we only evaluated our method on a limited number of nlu tasks and datasets. it is possible that our method may not generalize well to other tasks or domains that require different types of prompting knowledge or cloze-driven prompts. a promising direction for future work is to investigate how the prompt design and the learning objective influence the performance and robustness of plms on few-shot nlu tasks."
2660,"limitations an obvious limitation is that our work relies on the typology features of languages. some extremely rare languages might lack typology studies (its features are missing values in the wals database). our approach is limited for these languages. another non-critical limitation is that the technical contribution of our work is limited. after detailed analyses of position vectors, our methods for generating position vectors are not that complex, but we believe that an effective method is not neccessarily complex, and designing experiments to reveal key properties of position features and their connection with linguistic knowledge could still make solid contributes to nlp community."
2661,"limitations coverage of experiments there is room for further exploration in terms of model architectures, data, and evaluation settings in our study. experiments in more diverse settings would enhance the generality of the"
2662,"limitations our research presents an initial step toward a knowledge injection framework for msa and still has some limitations to be tackled in the future. firstly, we can learn more disentangled representations by carefully selecting contrastive pairs for further improvement. secondly, it will be interest- ing if we extend our method with multiple external sources that come from different knowledge domains."
2663,"limitations of a study. researchers and developers are encouraged to make justified accurate claims about their achievements. in addition, as the community grows and new practices are introduced, including all necessary practices in a single study is expected to become infeasible. nonetheless, our framework provides a comprehensive reference to collect the necessary evidence for validating nlu evaluation. “why is the"
2664,"limitations despite the strong performance of the proposed attenwalker. there is still large room for improving efficiency. for example, the time cost of our method is still high. since we need to search for all transformer layers and heads to find potentially re- lated spans, the dataset construction could be quite time-consuming. therefore, an algorithm could be designed in the future to pre-select proper layers and heads for attention-based graph walking, which would save much time in dataset construction."
2665,"limitations despite the remarkable improvement on complicated information extraction, there are still some limits of our method. first, due to the multi-round argument extraction modeling, we discard the parallelism in element extraction. furthermore, the mdp process interacting with the dqn further increases the computational load of the extraction process. so compare to other methods, our framework is relative slow at the inference stage. second, though our framework can be easily adapted to different extraction task with different schema, we still need an extra module helping identifying the relations (event types) in the instance beforehand. because of the difference task definition and modeling (extraction task and classification task), although recognizing them potentially implies order decision making, they are beyond the scope of this paper."
2666,"limitations as with many other model-based metrics, rise is best suited for evaluating offline due to the expensive nature of inferring with a large model. it is not as well suited as other metrics like rouge or bleu for evaluating during training or finetuning. we leave the exploration of using rise for evaluation-in-the-loop kind of training for summarization models future work. additionally, as with other model-based metrics, it is possible that the models may have seen some of the data during pretraining as is in the eval datasets. we do not think it would be too significant, as the pretraining task (for t5/mt5 for example) is rather different than a summarization task and, more importantly, it does not include the gold reference. thus the model would not be able to make such a connection easily despite having seen the data. we chose to work with the t5-family of models due to the ease-of-use for others to implement and improve upon our ideas. we would expect our ideas to work just as well with other models, such as bart, mbart, longformer, etc. following recent works, we have studied the evaluation based on the summeval benchmark (fabbri et al., 2021). in the future, we may want to build other benchmarks that covers more domains and languages to compare different methods."
2667,"limitations dataset noise as the audios are obtained from the videos on youtube, the quality of the videos will have an impact on the quality of the final transcript. for example, inferior recording equipment may affect the quality of the sound, although we have done noise removal to keep the quality, the presence of background noise will cause some losses in the transcribing process. relationship between transcripts and scenes in this work we get the transcript of shiba inu dogs, and we also find that the dataset covers a variety of activities and scenes. there may be an interesting relationship between the dog vocal units and the environment including the scene and activity. however, we did not quantitatively analyze the relationship. considerably more work will need to be done to discover semantic information in dog barks. phoneme labeling accuracy in section 2.6 we cluster the syllables and assign phonetic symbols to them. then in section 4.2.1 we evaluate the result by mos. it can be seen in figure 10 that the accuracy score is not very high, which can be improved in our future work."
2668,"limitations constructing semantic graphs, in general, requires multiple additional tools, which inevitably introduce errors. in this paper, we worked to reduce potential errors. we used the sota amr parser and coreference resolution model and adopted mechanisms to reduce error propagation to our final graphs. however, we have not measured errors involving topic segmentation and amr parsing due to the expensive human annotations required. it will be helpful to investigate how these errors can impact system performance when they are combined with encoder-decoder llms and if the amr encoder is robust to small errors in amr graphs. we leave these investigations as future work. finetuning llms for long dialogues requires many gpu hours and energy. therefore, our hyperparameter search was limited to 3 different values for learning rates, 3 for warmup steps, and 2 for batchsize. a more extensive search may bring additional improvement to our model."
2669,"limitations in this paper, we use a pre-trained bart to observe the changes in the model intrinsic features by varying the fine-tuning objectives and datasets. however, our methods can be made significantly more generalizable when the range of summarization models (e.g., pegasus (zhang et al., 2020)) and datasets (e.g., frank benchmark (pagnoni et al., 2021)) are broadened. additionally, we can try using other evaluation methods, such as factcc (kryscinski et al., 2020), reported to have a high correlation with human judgment (pagnoni et al., 2021) to interpret the model intrinsic behavior during the shuffle tests. we leave the research on methods to optimize the inductive bias during the fine-tuning process to improve the factual consistency as future work. our experimental results can also be integrated with those of previous studies focusing on hallucinations in datasets (kryscinski et al., 2020; wan and bansal, 2022)."
2670,"limitations efficientvlm is applied on x-vlm. however, there are also many recent fully transformer vlms achieving comparable or better performance. therefore, applying our distilling then pruning framework on other state-of-the-art vlms can be interesting. also, we do not apply quantization or matrix decomposition, which are prevalent model compression techniques."
2671,"limitations of such an approach in section 8. an additional question arises of whether one dataset may have multiple documents associated with one individual. there are several ways to go about dealing with this. one standard approach in differential privacy is to linearly scale the ε parameter. thus, if there are k documents associated with a given individual, then a privacy budget of kε is accounted in total (dwork and roth, 2013). another option would be to simply append all texts associated with one individual into a single ‘document’, rewriting this using just a single ε privacy budget."
2672,"limitations of the privatized text rewriting approach as a whole. future research directions include utilizing large-scale pre-training to potentially reach a better privacy/utility trade-off, as well as investigating domain specific text rewriting for relaxing the strict requirements of the ldp approach."
2673,limitations of the ‘classical’ gaussian mechanism.
2674,"limitations due to the limitation of time and resources, in this work, we select a relatively small number of clusters during the clustering process, which results in coarse-grained clustering. fine-grained clustering can provide a better latent concept but will also lead to increased computational resources and time consumption. we will attempt to trade-off between the cost and the granularity of clustering in future work to further explore the impact of the latent concept on cskg completion. besides, our node clustering module is not integrated in an end-toend manner in our work; we will consider using the topic neural network to construct an end-to-end model."
2675,"limitations the limitations can be illustrated from the perspective of task development: defi originally evolved from sentence-level work, as is clearly evident from the large number of sentence-related annotations retained in dlef corpus. our work benefits from these abundant annotations and achieves huge performance improvements. currently, there is a trend to gradually move towards end-to-end practice in event factuality identification. for example, the studies based on the dlef-v2 (qian et al., 2022a,b) and eb-dlef (zhang et al., 2022a) corpora have attempted to use less annotation information. although these efforts do not achieve competitive performance for the time being, it is an exciting research direction because it allows models to be more easily applied directly to realistic scenarios. the limitation of our work lies in the fact that it runs counter to the end-to-end concept, so we need more other work (e.g, event extraction and sefi models) to apply the model to the real world, which makes our work less applicable."
2676,"limitations we evaluate hybrank on natural questions, ms marco and trec 2019/2020 datasets, which focus on english open-domain question answering. although none of the components in hybrank are specifically designed for english, the verification of hybrank on other languages is limited. otherwise, there are more general information retrieval tasks involving diversity or broader coverage in the returned results. considering the possibility of lacking collaborative property, whether hybrank can generalize to these high-coverage retrieval tasks is still inconclusive. as transformer encoder architecture is adopted in the sequence interaction and aggregation, the computation cost would be unacceptable when the length of passage list or number of anchors is too large. this is also the reason why we only conduct experiments with anchor numbers no more than 100. besides, hybrank only uses similarities computed by off-the-shelf retrievers as input features, and thus lacks sufficient interaction between raw inputs. the performance of hybrank may be limited by the capability of upstream retrievers. how to incorporate the interaction of raw inputs into hybrank while avoiding massive computation cost is still an open problem for further investigation."
2677,"limitations of existing inversion attacks and propose a generative embedding inversion attack to better recover original text sequences given their sentence embeddings. then we show that lm-based sentence embedding models are potentially vulnerable to our proposed attack. we conduct extensive experiments to demonstrate the inability of previous embedding inversion attacks and disclose the privacy risks from our attack. the defenses against the embedding inversion attack are not well studied yet, even though it is much more malicious than the attribute inference attack. for future work, we call for more attention to effective defenses to address the embedding inversion attack with minor costs. limitations from the adversary’s perspective, our attacker model’s main limitation is the incapability of recovering exact domain-specific tokens. during our experiments, we evaluate attacking results on the personachat and qnli datasets. the personachat dataset collects daily conversations between speakers with almost no expert knowledge. the qnli includes question-answer pairs from wikipedia with far more domain-specific named entities than the personachat dataset. by comparing the attacking evaluations in table 1, 3 and 10, all attacks on the personachat dataset are more successful than attacks on the qnli dataset. for instance, in table 1, f1 scores on pc are 0.1∼0.2 larger than on qnli on average. in addition, qnli 2 of figure 5 shows that geia fails to recover the exact location “fresno” 7 out of 10 times. though most inverted results are similar to “what was the population of the city in 2010?” it is hard to capture the exact city name “fresno”. ethical considerations we declare that all authors of this paper acknowledge the acm code of"
2678,"limitations although our qaar achieves better performance on inductive relation prediction, it still suffers from some limitations. first, for a give query we extract a k-hop subgraph without using any sampling method, which will require large gpu memory when the extracted subgraph is large. second, our qaar does not leverage logical rules to enhance the performance which has shown useful in previous methods (lin et al., 2022). we believe that our method can be further improved by incorporating logical rules. we will leave these opening issues in the future work."
2679,"limitations although the ckcl performs satisfactorily in erc, there are still some limitations. because ckcl primarily concentrates on the effect of modeling context and external knowledge on the prediction results, when met some tasks that do not rely on context and external knowledge, pseudo labels can not be annotated, which causes the paralysis of the ckcl. in addition, when the class distribution of the sample is not uneven, the improvement of emotion scl will be weakened."
2680,limitations of single-modal speech.
2681,"limitations the proposed clkd is technically applicable to other nlp tasks, but we discuss the effectiveness of the approach for question answering systems, specifically for answer sentence selection (as2) tasks. in this study, we put our focus on as2 tasks as the research community has not well discussed or proposed multilingual as2 tasks/datasets. we also find that using only english teacher models is another major limitation of this study. however, choices of teacher models in the proposed clkd are not limited to english models. it would be interesting to discuss the generalizability of the proposed clkd beyond as2 tasks, but we note that such"
2682,"limitations due to time and budget constraints, this work remains limited in a number of important ways. the relatively small size of our corpus and its specificity to covid-19 necessitates the development of systems with richer inductive biases and the ability to effectively transfer knowledge from related corpora like fever. additionally, due to the large size of cord-19, the database of scientific literature from which we draw the abstracts in check-covid, it is difficult to evaluate abstract retrieval components like vespa with our annotations. there are likely many abstracts in cord-19 in addition to the ones we’ve selected that contain evidence relevant to any given claim, precluding both measurements of abstract precision and the evaluation of notenoughinfo in the full fact-checking pipeline setting. finally, as the claims in checkcovid are drawn from western, english language news sources and annotators, they are likely unrepresentative of the full range of covid-related (mis)information in need of fact-checking online. 6experimental details can be found in the appendix c."
2683,"limitations even though our work improves early exit performance effectively, some limitations are still listed below: • our approach focuses on making the intermediate representations of early exit models capable of general linguistic representation learning and task-specific representation extraction. therefore, we did not fully use the model’s high-level representation and fuse representations of previous layers, which may restrict the performance of our method. for future work, we would like to strengthen the information interaction between layers to make full use of the previous layers’ predictions, thus optimizing the representation for the internal classifiers. • although our early exit method has achieved better performance, we have lost some inference speed due to the introduction of additional adapter modules. in the future, we will try more efficient adapter-based tuning. • in recent years, the parameter size of generative pre-trained models has been continuously increasing, leading to remarkable performance on various nlp tasks. there is an urgent need to develop inference acceleration methods for generative pre-trained models. unfortunately, our method is limited to discriminative pre-trained models. our future work will investigate early exit strategies for generative pre-trained models."
2684,"limitations size. a limiting factor for some low-resource applications might be the size of a saved factorizer file. we only have to store the subword vocabulary, which however takes substantially more space than bpe as it needs to be stored as dawg trie to keep the tokenization speed similar to bpe. for example, the saved english factorizer takes about 115mb of space while the english bpe with 32k subwords takes just about 1mb of space. we believe that the space requirements are negligable compared to the size of large language models, but they can a limiting factor in some edge cases. the parameter-efficiency of factorizer follows the basic nature of factorized representations: a sequence of 3 bytes can represent more than 16m values (2563). that’s how we can embed millions of subwords with a negligible parameter count. on the other hand, when we store a vocabulary with millions of subwords on disc, it necessarily requires more space than a bpe vocabulary with 10 000s of subwords. glue performance. while our main interest and focus has been on morpho-syntactic downstream tasks, it is reasonable to ask what is the performance of factorizer-based language models on other tasks, such as natural language understanding. we utilize the pretrained english language models and finetune them on 8 glue tasks (wang et al., 2018). the results in appendix c indicate that in this setting, our method is comparable to bpe but not better on average, even though both approaches stay within a standard deviation from each other. we hope that these results can be improved in future work."
2685,"limitations there are two main limitations to our study. the first is that the stimuli used were limited to those provided by urbach and kutas’s (2010) study. this is because, as stated, we wanted to be able to compare the patterns in the language models’ predictions to the patterns in the human n400 response. thus, we do not look at logical quantifiers like kalouli et al. (2022), or any others that have previously been studied (in, e.g., pezzelle et al., 2018; talmor et al., 2020). the other (and perhaps more important) limitation is in the models we were able to use. crucially, we were not able to access models larger than gpt3 175b such as palm 540b (chowdhery et al., 2022). this is important because recent work has shown that some inverse scaling patterns become u-shaped (i.e., as language model size increases, performance degrades and then improves again) with such larger models (wei et al., 2022)."
2686,"limitations although our primary effort in this work was to extract as much parallel corpora as possible, the improvement in the performance has been found to be only marginal. the labse and qe-based filtering experiments involve a hyper-parameter called ""threshold quality score."" to achieve optimal results, we conduct experiments with different values of this hyper-parameter. the proposed few-shot transfer learning technique requires a small amount of data that needs to be annotated by multiple annotators."
2687,"limitations the main limitation of our work is that 100 clauses is a small test test, but this was necessary to keep our human evaluation experiments manageable. we furthermore believe this was sufficient to be able to draw meaningful"
2688,"limitations limitations on the evaluated language models and obtained results: the presented model architecture utilises various pre-trained language or image models. the main limitation of the experimental evaluation is not using other language models. due to the limited budget and processing power, we have included the language models that have been shown to perform better based on the previous work. another limitation is that we excluded language models that exceeded the 80 gb memory of an nvidia a100 gpu. our experiments led to different results for the gpt-3 compared to yang et al. (2022). it can be explained by using different methods for converting images to textual representations and slightly varying prompting structures. limitations on the used image models: the limitation concerning the pre-trained image models is that we selected a handful of methods based on their success for related tasks. including other pretrained models would increase the parameter space and thus increase the budget for the study. limitations on the selected datasets: all datasets are multimodal tasks where the underlying text is only in english. the choice of the dataset is related to the fact that there are limited multimodal datasets in other languages. the evaluation metric for the ok-vqa dataset requires the output to match exactly one of the expected answers. it counts as a wrong answer even if a slight change in the answer or another paraphrase is given as an output, e.g. “race” vs “racing”. we applied the same evaluation criterion and left this improvement as future work."
2689,"limitations our proposed metric is mainly designed for a turnlevel evaluation of dialogue systems. we recognize that our metric may not generalize to other evaluation scenarios directly, such as dialogue-level evaluation or human-chatbot interactive setups. as shown in section 5.5, the easiest way to extend our metric to a multi-turn dialogue evaluation is by evaluating every turn in a dialogue individually, and then aggregating their scores. however, as the dialogue-level evaluation is not considered during the development process of our metric, it is not clear whether such a simple extension would be applicable without a decrease in performance. nevertheless, as turn-level evaluation is a fundamental component to build a holistic evaluation framework for a dialogue, we believe that it is an important task to investigate better evaluation metrics for individual responses."
2690,"limitations there are two limitations to this work. (1) the total number of known and unknown intents are predefined, requiring an extension in real-world scenarios; (2) the frame knowledge is predefined and, therefore, inflexible to address complex intents. in addition, some user queries have no frame in framenet matching. there are additional computation costs for frame knowledge learning. the model fine-tunes two bert(bert-base-uncased, 340m) models in the training stage and runs sentence-bert in the evaluation stage. the pre-training stage of our model lasts about 10 minutes, and clustering runs for 90 minutes on clinc with a 75% known intents ratio, both using a single nvidia tesla v100 gpu(32 gb of memory)."
2691,"limitations in this paper, we studied paragraph-level qag models, which limits their input up to around 500 tokens, and the same approach cannot be easily applied to longer documents. also, the answer is an entity or a phrase consisting of a few tokens and the question requires one-hop reasoning, so our models are not able for use in generating longer answers or multi-hop questions. as far as the languages are concerned, the models studies here are english only and to adapt squadshifts qa evaluation in other languages, we need qa datasets to train and evaluate the qag model in those languages. the focus on this paper was on evaluating the quality of generated question-answer pairs. as such, we do not attempt to achieve the best qa model possible, but rather use question answering as an extrinsic evaluation. this extrinsic evaluation could be further enhanced with an intrinsic manual evaluation that we did not perform in this paper. finally, given computational constraints, our qa evaluation is based on a single model only. again, the goal here was not to achieve the best qa performance, but we acknowledge than using different models could lead to different results."
2692,"limitations the representation dimension (default is 32) is important but limited by our gpu resources. with the support of large gpu, a large dimension (e.g., 512) may achieve better performance. we also attempt to expand the inductive setting (relational patterns are same in the source and target kgs) to the independent setting (the source kg is independent from the target kg), but experimental performance is not good. that is, if the two kgs are irrelevant, it may be impossible to transfer information."
2693,"limitations we discuss the limitations of pluglm as follows: (1) despite the strong performance achieved by our approach with dpm, it results in a reduced inference efficiency at the same time due to the mips search. for example, pluglm is about two times slower than pure transformer-based models in glue. this would be more crucial when the external memory is much larger. potential solutions to this issue include (1) constructing the memory using a coarser granularity (borgeaud et al., 2022); (2) compressing dpm by semantic clustering as in tay et al. (2022) or knowledge summarization as in xu et al. (2022). (2) in this paper, we choose wikipedia for dpm construction and pluglm pre-training. while wikipedia is the most commonly used data source for language model pre-training (devlin et al., 2019; liu et al., 2019), there are also many other types of knowledge not covered in wikipedia, and how to integrate different types of knowledge (e.g., factual, commonsense, syntactic and semantic knowledge) into our framework remains under-explored. (3) although this paper proposes a general architecture that is applicable to plms of all kinds and sizes including bidirectional (devlin et al., 2019; liu et al., 2019; yang et al., 2019), unidirectional (radford et al., 2018, 2019; brown et al., 2020) and encoder-decoder-based plm (lewis et al., 2020b; raffel et al., 2020; song et al., 2019), we only experiment with bidirectional models in moderate size. in particular, we believe this architectural design would be greatly beneficial for llm (smith et al., 2022; chowdhery et al., 2022; ouyang et al., 2022) for the following reasons: (1) the parameters of llm could not be easily updated once the pre-training is done due to the unaffordable training cost. (2) the additional latency cost by mips retrieval is negligible compared with that of the whole llm."
2694,"limitations of multilingual models as weak learners and showed that larger models with richer pre-training objectives, in the form of instruction fine-tuning, yield more meaningful representations. post-processing refinement logic improves lowresource relation classification, as evidenced by the consistent outperformance of discoflan+ref over the baseline model. it addresses issues of mismatches caused by generation problems, leading to enhanced classification accuracy. our findings highlight the potential of augmenting weak label learners with distributional logic to improve model classification. discoflan showcases instruction finetuning for multilingual discourse relation classification for the disrpt 2023 shared task and provides valuable insights for future research in this area. we recognize the potential of larger models to improve prediction quality; however, due to constraints in terms of resources and time, we were unable to test the performance of flan-t5-large in our study. furthermore, we acknowledge that further advancements in decoding strategies and imporved prompts have the potential to enhance label representations and generation. in our future work, we intend to explore these topics to enhance our current models. limitations while using the majority label solves the problem of handing out-of-vocabulary labels during fine-tuning, we acknowledge that label refinement method relies on the majority label. this makes a strong assumption about our dataset bias, namely, that the majority label outnumbers the rest of the labels significantly to impact accuracy. hence, this method may not be applicable to well-balanced datasets. we also note that simply predicting the majority label is simple method of label prediction which does not generalized to new unseen datasets. improving label prediction by enriching datasets manually or automatically might make the task more representative of natural data. using larger models can improve model prediction however due to time and machine constraints we leave the evaluation using flant5-large for future work."
2695,"First, our work essentially relies upon a generative language model to understand the relationships between the sentiment elements in contrast to discriminative/extractive models which make structured predictions by design. As a result, our model is susceptible to usual anomalies suffered by generative models e.g., malformed outputs. We recover the quadruples from the model’s output sequence using regular expression based matching with fixed templates, as a result, an end-user will never receive any irrelevant text generated by the model. However, the accuracy will still be impacted in such cases nevertheless. Second, input sequences in user-generated content can be arbitrarily long and that might result in increased decoding time because of the underlying generative model. Last but not the least, all the instruction templates we provide in this work are designed solely for English. It would be interesting to explore systematic ways to be more language inclusive for instruction tuning based ABSA."
2696,"Due to the high-impact nature of the solved task, we review the ethical considerations made during this research project. Additionally, we outline further steps to ensure safety and transparency beyond publication, as well as recommendations for buildup work.
First, let us focus on the presence of biases in the data. We put extensive procedures in place even at the very start of the project. By inviting media researchers into our core team, we wanted to minimize misunderstandings and mistakes that scientists from the field of computational linguistics could easily make when assembling the methodology for the task of trustworthiness assessment due to their limited knowledge of the current literature and theory in the area of journalism. Prior to the data annotation, we invited scholars in media studies and journalists from the industry to a series of workshops, where we asked them to submit feedback and discuss the methodology. Based on the assembled comments, we kept updating it until a general consensus was reached. In terms of the annotation process itself, multiple safeguards have been employed to prevent annotators’ bias towards specific sources or authors (that may affect the classification).
Second, let us shift towards the ethics of using any technology built around this data in the wild. We want to stress that anyone using this dataset for the purposes of creating a trustworthiness classification system should provide transparent information to the users that this process is automatic and hence faulty to a certain extent. We must note that it still needs to be determined how models trained on this data generalize for future articles (i.e., news about topics and events they have not encountered in the training set) and news sources not included in the training set. A study into these should be conducted prior to making this technology available
unrestrictedly to the public. Despite bearing these safety questions in mind is crucial, such systems can eventually be great assistive tools for people reading news stories online. The potential benefits of such technology should support initiatives to safeguard it first and establish public and academic trust."
2697,"Given the subjective nature of our proposed task, this work does have some limitations and challenges. Firstly, the notion of harm or potential
to do harm is seldom an objective factor and is also difficult to measure or quantify. Our experiments on inter-annotator agreement use a small dataset, so this study could be expanded with collaboration with social science researchers to better qualify how people perceive the agenda in different articles. Our work is also grounded in the United States, so it may have limited applications to the news in other countries (discussed more in Section 8). Secondly, our data and framework can be used to build and train a system to perform posthoc detection of harmful agendas in news articles. However, in a real-world system, this identification would likely need to happen on the fly, so as to make readers aware of these agendas as they are exposed to the articles. Finally, another aspect that we have not addressed in this study is the effect that a platform or community may have on the perceived harm in an article. For example, on dedicated social media channels hosting discussions on alternate theories and contentious topics (such as the efficacy of COVID-19 vaccines), a junk science article with dubious claims may not be as “harmful"" as opposed to the same article being posted on an open forum where readers may perceive it as scientific fact, thereby making the article more “harmful"". The context in which news articles are disseminated may have a profound impact on this perceived harm and this may be an interesting direction for future exploration."
2698,"Our study has several limitations that should be acknowledged. First, the study was conducted on a relatively small test dataset. Future work is needed to assess whether our results are generalizable to larger datasets. Second, while we employed a rigorous methodology for evaluating ChatGPT’s performance, we have not measured other safety criteria, such as biases or privacy issues in using
14https://open-assistant.io/ 15https://lmsys.org/blog/
2023-03-30-vicuna/
this model. Third, our study focused only on the initial step of suicide risk assessment and did not explore the use of ChatGPT in ongoing monitoring or intervention. Fourth, we are unsure if the UMD dataset has been used in the training of ChatGPT in any capacity since the specifics of the training data of ChatGPT are not disclosed to the public. Future work should focus on creating new datasets to assess the performance of ChatGPT on fully unknown test sets.
It is important to note that despite these limitations, our work represents an important first step in understanding the potential for ChatGPT in suicide risk assessment. Future research should aim to address these limitations and explore the feasibility, safety and effectiveness of ChatGPT in broader clinical settings."
2699,"In this paper, we focus on lexical transformations between source domain and target domain to reduce the domain shift between them. To do this, we identify unique lexical features in the target domain and place them in the source domain so that the transformed domain is distributionally similar to the target domain. But there are also semantic differences between the two domains in terms of content, domain-specific jargon, and other nuances. This work does not take into account those transformations. Also, we use Twitter as the target domain for our work. While the general principles of our work are applicable to any source-target domain pairs, the transformations discussed in this work cater broadly to social media text, and specifically to Twitter data. The generalizability to other target domains has not been tested in this paper and remains a topic of further investigation.
In this paper, we work with a POS tagging dataset. POS tagging is a token level task where we classify each token as belonging to a certain category. We feel that because POS tagging is dependent on each token in the sentence, domain transfer affects this task most adversely. Sequence classification tasks like sentiment analysis that only require a high level representation of the entire sentence to make classification decisions might witness different levels of improvement. The current method needs to be tested for other task types, in-
cluding sequence classification tasks like sentiment analysis, or generative tasks like question answering and text summarization. This was beyond the scope of a short paper."
2700,"We only analyze four high-resource languages in this study, our analysis could have benefited from more languages, especially low-resource ones. Additionally, we only analyze Japanese and English Pride/Shame as a known cultural difference; analyzing other differences could provide stronger results. We perform a small user study, and our work could have benefited from a larger-scale study with more annotators and completions analyzed.
We recognize the added complexity of investigating Pride embeddings from a culture where explicit expressions of Pride are discouraged; we note this may be a contributing factor to our results indicating that LMs do not reflect the culturally appropriate nuances of Shame and Pride.
Additionally, we acknowledge that the experiments outlined in this paper are specific to investigating cultural awareness from the lens of emotion. These experiments are not easily applicable to measuring cultural awareness from different perspectives; therefore, results may not be generalizable.
At a higher level, we equate language with culture. Psychologists have observed higher cultural similarities within languages than between them (Stulz and Williamson, 2003), however, we recognize there are variations within the populations that speak each language. For example, Spanish is spoken by people in Spain, Mexico, and other countries, each having a unique and varied culture."
2701,"Some limitations of the present study should be considered. First and foremost, the inherent statistical biases of Chat-GPT and GPT-3.5 might skew the data distribution, which is difficult to control without knowing what data the models are trained on. It is therefore also certainly possible that the generative models have already been trained on the Vaccinpraat dataset. Additionally, we found that
the text examples in the prompts also affect the distribution. Moreover, these statistical biases might lead to repetitive sentence structures in the data.
Second, the generated data contains false information about vaccines and COVID. One should therefore act with caution when interpreting the synthetic data and should only consult fact-checked sources for information about COVID vaccines. Moreover, while the messages are believable enough as a reader to be vaccine-hesitant, the messages are more ""neutral"" in nature than the goldstandard data. This is especially apparent in the Chat-GPT dataset, which was to be expected because of the guardrails imposed on the model. This distribution shift could explain the degraded performance compared to the back-translated data in the in-platform. Future work could explore tuning the prompts further to minimize this distribution shift.
Third, since we only focused on the vaccine hesitancy monitoring task, more research should be conducted with the presented method for more multi-label tasks. However, this method could only work effectively for datasets with a relatively small number of labels, as the descriptions need to fit in the prompt. However, the promising results from the conducted experiments and analyses should stimulate further exploration for other multi-label text classification tasks."
2702,"In this paper, we only explore binary sentiment classification, as it is enables cross-lingual experiments to be somewhat comparable. However, this is a simplified task, which should be taken into account when interpreting the results. Our multilingual datasets also come from various domains and, although we try to control for this in English, this does lead to some effect in the results. Finally, for emotion detection, we only experiment in English.
We also chose only a few representative methods for each approach (few-shot, prompting, rulebased, etc). This was a necessary simplification given the large number of available models, and care was given to choose truly representative methods for each approach. However, some relevant methods may not be represented here.
Finally, we only report the results for a single run for the supervised models, rather than the average of 5-10 runs as is common. We compensate by averaging over results on several datasets and across several methods."
2703,"The primary limitation for this work is the difficulty of telling the difference between mistakes made by automatic systems and wrongly assigned importances from the attribution techniques. Additionally, our work currently only relies on a single pre-trained model that was fine-tuned on the currently only available data set for Dutch irony detection. The patterns in computational modeling we described only apply to this particular system and data set and may very well differ when training a different model on data that was collected
in a different way, where the data set may rely on different patterns and biases. Finally, despite the good agreement scores (a Cohen’s kappa of 0.84) for binary irony classification, this remains a complex task where annotators can be uncertain about the label. In the end, the annotators for any irony or sarcasm detection task can only make assumptions about what the author of a text intended to convey. For our setup, both the annotators and the automated system predict whether a text is ironic without considering the corresponding context. In a realistic setting, most social media texts are reactions to previous comments or external events that can be essential in order to recognize the irony. This means that the model predictions can differ from the annotated label but still be a plausible interpretation."
2704,"Our methodology provides a novel approach to predicting emotional reactions to social posts. While our proposed methodology has shown promising results, further research is necessary to address some limitations and expand the scope of our findings.
Firstly, our model’s reliance on only textual content and metadata may limit prediction accuracy, as other factors such as user demographics or multimedia content may also impact emotional reactions. Future work could explore the inclusion of additional features. Secondly, this study focuses on predicting emotional reactions to posts, rather than the reasons behind them. While our method provides textual explanations, they may not fully capture the complexity of user emotional responses.
Lastly, our approach was evaluated on Meta posts, but may not generalize well to other social networks or domains. Further evaluations on different datasets are needed to confirm the applicability of our methodology across different contexts."
2705,"A large limitation of this work is the ubiquity of English. With the exception of the AfriBERTa (which has seen Naija), the remaining PLMs in this study all included English in the pretraining data. As a result, it is difficult to disentangle the benefits of including relevant languages in the pretraining data, from the general benefits of including English in the pretraining data, for processing code-mixed text. To this effect, future work in examining the capacity of PLMs for code-mixed language would benefit from examining commonly code-mixed language pairs, that do not involve English (e.g. TurkishGerman).
In a similar vein, our work is limited in that we did not try other non-English monolingual PLMs. For the Indic languages, this is because monolingual Indic PLMs typically use the Devanagari
script, but the datasets in this paper are constrained to using the Latin script. For Naija, we likewise did not experiment with monolingual models for the other relevant Nigerian languages; to our knowledge, most publicly available PLMs for Hausa, Yoruba, and Igbo seem to be created through continued pretraining with monolingual data over existing multilingual PLMs. Thus, experimenting with these models still does not strictly control for English and other languages.
Beyond PLMs, another limitation of this work pertains to the error analysis, which hinges upon currently available LID technologies. As explored in detail by Caswell et al. (2020), most LID technologies operate on a document level, and thus intra-utterance LID is still an open problem. For code-mixed language, the lack of robust LID puts limits us to coarser-grained analysis of the data (e.g. partitioning samples by mostly-English or mostlyHindi). Ideally, a finer-grained partition of the data could be useful in determining the extent to which a PLM’s knowledge of English enables performance on downstream tasks."
2706,"While our dataset shows promise based on the models we train with it, at about 1000 annotated examples in the training partition, it is relatively small. Before working on increasing the size of the dataset, we need to work on improving the guidelines provided to Mechanical Turk workers and finding more robust ways of excluding bad faith annotators.
The dataset also currently misses some information that could be useful, e.g., polarity, beliefs involving out-of-sentence coreference resolution, as well as believer and belief span annotations. We plan to address all of these in future work."
2707,"Currently our approach relies on translation to analyze multilingual tweets. Future work would include using multilingual pre-trained models like XLM-RoBERTa and the use of non-English training data to build a language agnostic emotion model ensemble.
We carry out our in-domain optimization on a small validation dataset that was annotated by a different set of raters than the one used for the test dataset, which results in a performance drop in the few-shot mode. Ideally, the availability of a high quality validation dataset would boost the zero-shot performance and further adapt the label mappings to the target domain. We also aim to carry out in house annotations by experts to release a publicly available dataset annotated with emotions in the political domain which would pave the way for further analysis in this domain."
2708,"The current study was based on primarily American respondents, though approximately half had lived in China for at least a year (including 75% of WeChat users). As a result, our data does not fully reflect the wide range of users or non-users of WeChat. We encountered correlations between our participants’ familiarity with WeChat, the individual WeChat emoji, Chinese culture, and the Chinese language. WeChat use and emoji familiarity had the strongest impacts on emoji interpretation differences, but further work is needed to control for the impacts of these other factors.
Additionally, we limited our analysis to emoji in isolation, as our goal was to assess whether the most basic interpretation of the emoji still relied on experience with the emoji. Emoji are sometimes used by themselves without text, so these results will apply to some real-world usage. But of course, in their general use, emoji tend to appear in richer conversational contexts and are accompanied by other linguistic information. As such, it is not certain that the observed user/non-user sentiment differences will persist for emoji used in conjunction with texts. That said, the assessment of these emoji in isolation can serve as a baseline for future research examining how their sentiment differs in real-world conversations, especially in cases of sarcasm, hyperbole, or irony."
2709,"As this is a position paper, I mainly provide thoughts here, and do not include any experiments or actions myself.
Although the goal of this paper is to combat biases in AER, it is limited to discussing the dominance of English. Other biases, like the bias towards social media texts, or the tendency to ignore neurodiversity and conditions like alexithymia and autism spectrum disorder, are not addressed in this paper.
The counting study I performed to demonstrate that the number of papers dealing with other languages than English does not increase – contrary to the number of languages that are addressed, which does show an upward trend – is only based on papers presented at the Workshop on Computational Approaches to Subjectivity and Sentiment Analysis. Maybe other patterns could be discovered when analyzing the papers of other venues."
2710,"The sample we collected has limitations shared by most datasets focusing on naturalistic behavior surrounding some event. First, because the starting point in this sample was the quit messages, and
not every person posts each week, there are necessarily more target posts than other posts. Quit weeks and weeks immediately surrounding target posts were also more verbose than other weeks and, because work-related concerns were salient at the time, likely included more comments and posts about work or career planning.
Second, the Reddit sample we analyzed is heterogeneous. In most respects, that is a benefit of these data—the conversations covered diverse topics and took place in groups with varying social norms, cohesiveness, and cultures. In that way, these messages are more naturalistic than language from controlled experiments or narrowly focused social media research. Yet there are better options than simply averaging over these differences. For example, emotional expressions are both inflated and suppressed by forum norms regarding emotional self-disclosure (see Balani and De Choudhury, 2015), and the same terms take on different affective meanings across communities (Hamilton et al., 2016). Future research on these or similar data may benefit from clustering forums into psychologically meaningful groups or developing sentiment lexicons tailored to each forum.
Finally, as with any analysis of self-labeled data on social media, we are taking people at their word, accepting the likelihood that some of the messages about quitting in our sample were exaggerated or fabricated (Coppersmith et al., 2015). Despite efforts to stringently filter out hypothetical, satirical, fictional, remembered, or otherwise non-literal references to recent quitting, there are also no doubt some remaining false positives."
2711,"Our work can be considered to have the following possible limitations:
1. The dataset we introduce and use to perform analysis contains 2000 tweets sampled from a specific time frame over a single social media platform. However, we aim to extend this work by collecting more political data across various social media platforms and using it to model aggressive behavior. Please do note that these tweets have been manually filtered from a larger set of 10,000 tweets while manually labelling them and ensuring that they are relevant to the political domain.
2. The number of user handles that we scrape tweets from for this study is around 110. This number might not be reflective of a large political space considering the plethora of politically active personalities in India. However, it is noteworthy that each of these 110 user handles has a minimum of 100, 000 followers on Twitter, on the basis of which we consider them to be influential on a social media platform."
2712,"While dealing with the MLEC task, we find that most outputs from our method have only one category. Therefore, our method still has room for improvement in handling MLEC problem."
2713,"Train Dataset for MuRIL-SIMCSE: While we try to minimize the hateful samples in this dataset by removing all the toxic/hateful samples of the respective datasets used to form this dataset, there could be samples containing certain biases like gender bias and racial bias. Also the dataset contains the respective languages written in the Roman script, so the results might not be transferable to the respective native scripts of the languages.
MURIL-SIMCSE: The model was trained on a single Tesla P100 GPU for 9 hrs. We could have trained further and on more data, but we could not due to resource and economic constraints."
2714,"limitations of the approach employed. in future work, we plan to explore the reasons for this variability across datasets further. additionally, we acknowledge the need to address the issue of biased models, such as the ones trained on mafiascum, onlinede, and diplomacy, which tends to favor truthful labels owing to the label imbalance in these datasets, resulting in an f1 score of 0. to overcome this challenge, we could employ techniques like oversampling to rectify the class imbalance and improve the reliability and effectiveness of our approach. the goal of our future work is to create robust deception detection models that work reliably across corpora and domains. this includes understanding differences in the concept as it represents itself in these data and understanding differences in linguistic realization. our unidecor dataset serves as a valuable resource for future research enabling standardized data comparison, transfer learning, and domain adaptation experiments."
2715,"limitations there is no dataset currently available specific for fine-grained emotional paraphrasing. for our study, we have to utilize publicly available paraphrase datasets, google paws, mrpc, and quora and augment their text pairs with emotions labels. these datasets may not be best suited for study- ing this new task. therefore, new datasets that are particularly developed for fine-grained emotional paraphrasing are needed. furthermore, it is also desirable to evaluate the proposed methods in alternative application scenarios other than lowering sentiment intensity. when using goemotions as our fine-grained emotion classifier, we selected the emotion with the dominant confidence score above the threshold of 0.5. as the authors of goemotions have pointed out, there is still much room to improve on the classification accuracy of goemotions. although the confidence score threshold of 0.5 worked well in our experiments, how to set this threshold still requires more studies. similarly we utilized nltk’s vader scores to place emotions into high, low, and neutral intensity groups. the vader score thresholds for this grouping were selected empirically. further studies are needed for setting the thresholds or developing better ways for intensity grouping. in the evaluation of our fine-grained emotional paraphrasing models, we utilized two sets of metrics for emotion transition and paraphrasing respectively. it is desirable to jointly evaluate these two aspects, which we believe would be best done by well-designed human studies in future work."
2716,"limitations while working with the data of the iemocap data set, we realised that some of the examples are of very low quality which can negatively affect the performance of the models. this, however, does only apply to a small part of the data set. additionally, it is not possible to correctly asses the influence of the gender on the performance. the same applies to a possible influence of the way, the data is generated: scripted or improvised. in some models we use selu activation function, which is still not widely used, therefore, it is possible that there are problems that are not that well known. in general, there are limitations based on the data set. it only contains scripted and improvised recordings, by actors, which might not be representative of naturally occurring emotions. also, as the data set is recorded in english, any generalizations outside this language are not possible."
2717,"limitations the dataset used in this work is in italian and the plms are pre-trained for the italian language. the performance of the models and the results may be influenced by language-specific properties. to reduce the ecs sparsity and, therefore, better modelling the inter-dependency between ec and valence prediction tasks, particularly in the experiments ec→ val., a larger dataset with more narratives per narrator is needed."
2718,"limitations of each method. our findings indicate that bert yields the best performance in terms of sentiment classification accuracy. in combination with shap, it offers a global view of feature importance, which helps detecting spurious features and bias. we think that end users will profit from xai methods which allow to get an aggregated view of feature importance for a particular topic category, or based on a specific time frame. however, due to the high dimension of our data, local explanations are overall not very plausible, regardless of the underlying ml model and explainability method. moreover, the bert model is less faithful than cnn and lstm, due to high complexity of the model. for our use case, the computational time for generating explanations with lime, ig or saliency would be acceptable in a real-time application, except for shap which suffers from a long computational time. in future work, we seek to identify the training data points responsible for model misclassifications and find training instances that show bias through influence functions (koh and liang, 2017), and investigate the impact of the pretrained embeddings and model."
2719,"limitation our proposed method demonstrates stable perplexity even as the style transfer weight changes, but it yields a lower bleu score compared to other controllable style transfer models. we hypothesize that the lower bleu score may be attributed to the fact that the bleu score calculation is based on just one human-written transferred sentence option per source sentence. this lower score could be a result of our model generating diverse sentences that do not necessarily overlap with the provided human-written references."
2720,"limitations in prior research (salminen et al., 2022), several challenges have been identified in this field, such as noisy or low-quality data, semantic ambiguity, absence of standards, social desirability bias, and the requirement for human intervention. our study aimed to tackle the challenge of detecting pain points and devised various strategies for managing noisy real-world reviews. nonetheless, to fully unlock the potential of the painsight, additional research is necessary to explore the wide range of emotional polarities beyond the generic ‘negative’ sentiment. furthermore, customer reviews often show mixed sentiments, which calls for addressing semantic ambiguity. lastly, the performance of painsight assessment was constrained to five product categories, highlighting the need for a comprehensive, high-quality benchmark encompassing diverse domains and performance evaluations across distinct categories."
2721,"limitations the current work focuses on al with pre-trained language models based on lowest prediction confidence. in spite of the effectiveness of the strategy shown both in these experiments and in previous work (schröder et al., 2022; ein-dor et al., 2020), neural models are often not calibrated well (yuan et al., 2020; park and caragea, 2022), which implies that the output of the softmax function could be a suboptimal metric for measuring prediction confidence, i.e. informativeness, for a given training sample. future work on this topic should therefore investigate whether other metrics work better for al with pre-trained language models and whether these metrics also benefit from unsupervised task adaptation. additionally, experiments could only be conducted on a limited amount of tasks and datasets. future work should shed new light on the usefulness of the proposed approach in different settings."
2722,"limitations: chatgpt struggles to explain sequences that do not fit into the learned patterns. further, it will not indicate when something is not funny or that it lacks a valid explanation. instead, it comes up with a fictional but convincingsounding explanation, which is a known issue with chatgpt. joke detection. we identified three main characteristics that generated jokes had in common, i.e., structure, wordplay, and topic. the presence of a single joke-characteristic, e.g., the question-answer template, is not sufficient for a sample to be wrongly classified as a joke. the fact that chatgpt was not misled by such surface characteristics shows that there is indeed a certain understanding of humorous elements of jokes. with more joke characteristics, a sample is more likely to be classified as a joke. although chatgpt’s jokes are not newly generated, this does not necessarily take away from the system’s capabilities. even we humans do not invent new jokes on the fly but mostly tell previously heard and memorized puns. however, whether an artificial agent is able to understand what it learned is an exceptionally tough question and partly rather philosophical than technical. in the present experiments, all prompts were posted in an empty, refreshed chat to avoid uncontrolled priming. but, clearly, context plays an important role in the perception of humor. chatgpt is able to capture contextual information and adjust its responses accordingly to the preceding course of conversation. this is an intriguing capacity, which we would like to include in future investigations."
2723,"limitations the present study comes with two major limitations. first, humor is highly subjective, and a valid and reliable evaluation is hard. things can be perceived as funny for very different reasons - even for being particularly not funny, such as antijokes. thus, when chatgpt generates an odd joke about ml, one could even argue that chatgpt has a sense of humor that is just different from ours. also, humor is diverse in reality. the present investigation focuses on one specific form of humor, namely standalone jokes. there are more manifestations to consider, which would require a much more complex experimental setup. second, we cannot confidently trace back the outcome of the system or map it to specific input data. this is challenging for large data-driven models in general, but especially in this case, where we neither have access to the model itself nor to any training data or to the exemplary samples from rlhf. this prompt-based investigation creates a good intuition for the opportunities and limitations of chatgpt. however, our observations and"
2724,"limitations as the evaluation methodology for prompt engineering is still under development in the nlp community. we arbitrarily decided on the size of the development set. our study focused on zero-shot setting with the purpose of evaluating the latent understanding on causal claims within chatgpt. further exploration could be conducted to investigate the impact of few-shot settings by carefully selecting examples based on recent progress in few-shots prompting methods (lu et al., 2022; liu et al., 2022). we conclude that chatgpt has a promising but still limited ability in understanding causal language in science writing. cots improved prompt performance, but finding the optimal prompt is difficult with inconsistent results and the lack of effective methods to establish cause-effect between prompts and outcomes. following instruction is an important prerequisite for using chatgpt as a text classification tool, to avoid high labor cost for post-processing its answers. however, chatgpt provides a new, simulation-style approach for designing and evaluating human annotation guidelines."
2725,"limitations the big 5 personality trait model measures the fundamental dimensions of human on a continuous scale. this real valued representation preserves more information and is more descriptive of interindividual differences. while we acknowledge that the binary classification of big 5 traits fails the purpose of the model, it is a necessary simplification to understand the ability of llms to perform personality assessment. our investigation shows potential to improve the practical utility of llms in personality estimation. despite the strong results from existing works in support of in-context learning and larger message history for better performance, we were limited by the significant multiplicative cost these experiments entailed, as the gpt-3 api is billed based on token usage. further, since each user’s post history is typically long, it is infeasible to experiment with all in-context learning options due to gpt-3’s context window size limitation. this is worthy of exploration, to understand the sample efficiency of gpt-3 and the impact of post history on its performance."
2726,"limitations a limitation of this work is that the poems written by adults are by experienced writers who are often known for their poetry. these poems may therefore not be representative of poems written by adults in general, and could affect the patterns and trends in emotion words we see. as future work we would like to expand the collection of poems written by adults to include those written by novices as well."
2727,"limitations we considered the binary stances examples topics mainly i.e. for/against, support/refute, or agree/disagree. the proposed methodology lever- ages the contrastive learning framework which is conditioned to work with two stance labels examples to identify whether the author of the text is in favor of or against the topic of"
2728,"limitations of such a study and we discuss them in the next section. we release any data, code, and models produced during this study (including any raw data, but keeping user handles anonymous) publicly for further research by the community. we license this release under cc-by-nc-sa 4.0. in the future, we aim to collect more data from multiple social media platforms and release it to model aggressive behavior. we plan to perform similar experiments on a large dataset while benchmarking and comparing our current models’ performance. we also plan to investigate online or active learning for the same. finally, we also aim to expand on the theoretical underpinnings of sublime aggression and offense by attempting to identify these within other more tangential domains, viz., comedy."
2729,limitations the test dataset size makes it difficult to draw meaningful
2730,"limitations mentioned in subsection 4.3 because the model being fine-tuned sees all the training set examples and at inference you pay only for the tokens in the one example to be classified. also, as this experiment showed, fine-tuning is a very powerful text classification technique when it is used with gpt models."
2731,"limitations while the multilingual models employed in this study are capable of processing a range of languages, their performance is restricted when it comes to code-mixed sentences that feature a combination of roman urdu and english. this limitation suggests that the models may yield comparable results when dealing with similar language pairs. additionally, the effectiveness of utilizing chatgpt’s api to translate code-mixed sen- tences into english has not been conclusively established, and thus, it remains uncertain whether this approach represents the optimal solution."
2732,"limitations the limitations of this work can be concluded into three points: 1) the data in the test set is relatively small, so it cannot more accurately reflect the effectiveness of the method proposed in this paper. we believe that tuning the model on a larger dataset can help improve the performance of the model. 2) due to device performance limitations,we did not experiment with larger models. in our experiment,we only tested the method with models like xlm-roberta, mbert and bert. its performance with larger models is not known. 3) we did not perform an extensive hyperparameter search, which might further improve the model’s performance."
2733,"limitations in addition to the data-based limitations listed in section 6, we faced the following technology-based limitations: • the results are greatly dependent on the prompt design. it is not uncommon to spend a lot of efforts on coming up with the right prompt. each use case may need a different prompt. • overall, chatgpt provides a stable output especially if one asks for a specific output format. but there is still an element of volatility when one or few responses contain extraneous text, or the categories are outside of the predefined list. this is due to the conversational nature of the model, and such cases had to be processed as exceptions. • chatgpt ""remembers"" the past conversations, but this memory is limited to the context window size which is only 4096 tokens. this makes it challenging to work with large datasets which have to be split into pieces to be processed independently. • one has to remember that one must pay for the use of the chatgpt api - very long prompts used multiple times or too many examples for few-shot learning may be discouraged for cost savings purposes."
2734,"This resource paper describes the dataset in detail, providing strong baselines and first initial crossdomain experiments. It does not aim to provide an extensive set of experiments on cross-domain argumentative zoning yet.
The entire dataset is only singly-annotated. The agreement study was performed on complete documents and hence has only limited data for several labels. Due to the limited funding of the project, we could double-annotate the entire dataset.
Finally, we only test one model class (BERTbased transformers). A potential next step is to test a bigger variety of models and embeddings. Because AZ labels are interdependent within a document, especially document-level models or CRFbased models are promising methods to try. We have also tested only one method (multi-label random oversampling) to deal with the strong class imbalance in the dataset. We have not yet tested further such methods (Henning et al., 2023) or data augmentation methods."
2735,"Multilingual language models such as XLM-R (Conneau et al., 2020) and GigaBERT (Lan et al., 2020) are typically pre-trained on large amounts of unlabeled text crawled from the Web. Since these models are optimized to capture the statistical properties of the training data, they tend to pick up on and amplify social stereotypes present in the data (Kurita et al., 2019). Since our coreference resolution models use such pre-trained language models, they may also exhibit social biases present on the Web. Identifying and mitigating social biases in neural models is an active area of research (Zhao et al., 2018; Sheng et al., 2021; Gupta et al., 2022). In the future, we plan to work on removing social biases from coreference resolution models.
Furthermore, while our proposed methods are highly effective, the performance of our best ensembles is still far from perfect. On OntoNotes Arabic, our best system only achieves an F1 score of 66.72%. Such performance may not be acceptable for some downstream tasks (e.g., information extraction from critical clinical notes).
Finally, even though Wikipedia is available in more than 300 languages, there are still very few Wikipedia pages for some very rare languages. Our proposed methods are likely to be less effective for such rare languages."
2736,"The encoder of the LCM which we have utilized for our experiments is a basic deep neural network. Replacing it with more robust and effective architectures could help achieve better performance. Furthermore, instead of using pre-trained GloVe embeddings for the encoder, using IDDR-specific
embeddings could have been a more efficient approach. Lastly, our models have been trained and evaluated on PDTB 2.0, instead of the latest PDTB 3.0, which includes also intra-sentential implicit relations and has a more systematic sense hierarchy."
2737,"limitations in spite of the strong performance of changes, its design still has the following limitations. first, changes only extracts the sentence-sectiondocument hierarchical structure of academic papers. we believe the model performance could be further improved by incorporating document hierarchy of different granularity like dependency parsing trees and rhetorical structure theory trees. we leave this for future work. in addition, we only focus on single academic paper summarization in this work. academic papers generally contain a large amount of domain knowledge, thus introducing domain knowledge from peer papers or citation networks should further boost model performance."
2738,"limitations although our model improves upon state-of-the-art methods of bam by incorporating entity coreference and co-occurrence information, there are still some limitations to our model. first, it is not easy to apply our model to other domains where no coreference resolution tool is available. second, the number of nodes and edges of the generated heterogeneous graph will become enormous if the documents are long and many entities are extracted, which requires more gpu resources."
2739,limitations the corpus presented in this paper is relatively small and so the
2740,"limitations although the new architecture works well on pdtb-like structured data, we are often challenged with texts without clear paragraph structure. this would make it either necessary to pre-process texts and split sentences into semantically closed paragraphs such that our proposed model takes advantage of the surrounding context, or develop a new sentence-based model which was not successful in previous work. limiting the model to predict only continuous alternative lexicalizations does not highly affect results on the pdtb, but might have a more considerable impact on other text genres, e.g. speeches and debates. this would require the use of a more complex signal encoding as mentioned in section 3.1."
2741,"limitations. more long term, we plan to extend our discourseaware approach towards controllable generation with given entities. specifically, instead of using the learned gr scores, the model could generate summaries with desired entities provided by human users. limitation in our method, we employ an existing ner tool (spacy) to label the entities in both the source documents and the summaries, and the performance of the ner tool may have an influence on the results of the model. thus a good in-domain ner tool may be required when the work is extended to some specific domains, e.g. medical text. in addition, we use pegasus(zhang et al., 2020) as our base model in all the experiments on different datasets, as it has delivered top performance on multiple summarization datasets. we follow the original paper on the length limits of all the datasets, however, the length of the source documents in both scientific paper datasets are much longer than the length limit (3k/6k v.s. 1024), which leaves the room for further improvement with sparse attention techniques applied (xiao et al., 2022; guo et al., 2022)."
2742,"limitations we have carried out all analyses according to our best abilities. nevertheless, it should be noted that rst structures and qud structures were annotated by distinct researchers. while all annotations have been double-checked by at least one other expert for plausibility, in many cases there are alternative analyses of the texts which may also be applicable (as is usually the case for discourse structure). since we do not have direct access to the discourse creators and their goals, this limitation is unavoidable in corpus studies."
2743,"limitations in our current project, we have not taken into account the temporal information that treats the historical behavior of users as a sequence of actions. thus, the model may not capture how user behaviors change over time. to ensure full support to users in need, we recommend that future work should address this limitation by considering users’ historical behaviors as a sequence of actions. moreover, although our pre-trained models achieved significant results without fine-tuning discourse embeddings, we suggest that fine-tuning these models can enhance performance by capturing the nuances of the datasets’ distribution and contexts. furthermore, conducting a detailed comparison of additional open-source large language models (llms) would provide more comprehensive insights into their performance. additionally, in addition to analyzing the efficiency of different models, it is crucial to evaluate the cost associated with implementing these models. therefore, future work should consider both fine-tuning and evaluating additional llms, while also taking into account the costs of utilizing these models."
2744,"limitations the proposed model with sentence-aware encoder aims to efficiently incorporate external knowledge and bag-of-words for topic modeling, which means that in this work we are mainly interested in how documents should be encoded for topic inference. however, the decoder of topic models can also be coupled with word embeddings through factorization, such as embedded topic models (dieng et al., 2020). it is worth exploring how hierarchical semantic embeddings can be employed for topic modeling with our model. in this paper, we do not conduct any fine-tuning for the pre-trained language model. our approach reveals how the frozen pre-trained language model can be effectively used to improve the performance of the topic model with limited computational overhead, given that the parameter size of the pretrained language model is much larger than that of the topic model. moreover, fine-tuning pre-trained language models for topic modeling as an unsupervised learning task (mueller and dredze, 2021) is challenging."
2745,"limitations this work is about document-level nmt, we focus specifically on methods that improve the model performance for long input sequences. due to constrained resources, this work has several limitations. to be able to train all methods including the inefficient baseline approach, we have to limit the context size to 1000 tokens. while we do a comparison to existing approaches, other approaches have been proposed to improve the performance of systems with long context information, which we do not compare against. we run experiments on three different tasks, but two of them are low resource and two of them translate into german, which was necessary because we only had access to german language experts for preparing the evaluation."
2746,"limitations the current work is limited by the size of the dataset and the nature of spontaneous conversation. while the discourse relations proposed as part of this work were selected to be general and build on categories from the literature, the list is not exhaustive and it is likely that these relations may be culturally, linguistically, and situationally specific. future work in this area should validate the generality of the discourse relation system used in this work. the selection of edus and cdus for annotation is also non-exhaustive; additional segments could be included in future work. annotation quality is also a practical limitation. annotation for discourse relations typically results in low-agreement data, even among expert annotators (e.g., discogem; scholman et al., 2022). even though our research questions focus on this disagreement as a positive, other researchers may require greater numbers of annotations in order to obtain a gold label."
2747,"limitations our proposed summarization model is pretrained exclusively on news datasets, however, our experiments and analysis were conducted on biographical narratives. we only studied english summarization and our processes and in particular relevance findings are likely not entirely applicable to long multi-lingual documents. moreover, single-domain trained models may propagate inductive biases rooted in the data they were pretrained on. this was evidenced in finetuning on our target dataset as the model demonstrated a moderate degree of transferability in adapting the newswire domain to our biographical discourse genre. our work studies generated summaries for long narrative text. while our taxonomy appears generalizable to other domains, investigating summarization quality of large-scale datasets, such as scientific articles, patent documents, government reports or meeting discourses was confined to the scope of baseline performance comparison."
2748,"Our transcription guidelines occupy a certain position in the continuum between completely preserving the authenticity of learner handwriting and completely ig-
noring it. This position is motivated by our aim of capturing mainly orthographic features, which comes at the expense of other (e.g. readability, comprehension, and cohesiveness) features of the text.
In the course of this study, we only applied the guidelines to German texts. While we are quite certain that they generalize to other alphabetic languages (especially closely related ones), it cannot be ruled out that we missed some language-specific phenomena. However, these could be mitigated by augmenting the guidelines accordingly. Our guidelines are not directly applicable to other, e.g. logographic, writing systems."
2749,"As mentioned in Chapter 2, since data is annotated by the author in person, it may include some human error in labeling. However, this can be resolved through a review of the labels and the publication of an amended version. In addition, the sentence data is obtained from the Universal Dependencies (UD) Japanese-GSD data set, this is just one of eight UD Japanese corpora and other ones could be used to expand the data set. Also, this research treats the single kanji homographs as the target, and the work could undoubtedly be improved by expanding the research to kanji combinations. As discussed in Chapter 1, Japanese is one of the languages that lack word boundaries. Therefore, the first interesting point will be that when a kanji homograph is in a kanji combination or phrase, which kanji homographs will tie together to make a word to create a word boundary with other kanji homographs. Then the second point is how the new kanji combination can affect the pronunciation selection of those kanji homographs.
An anonymous reviewer suggests that we compare against the kanji disambiguation system embedded in MeCab. However, we leave this
comparison for future work.
Finally, due to time constraints, this research extracts n-gram features for the target kanji homographs. There will be other features that help analyze the context to improve the model performance."
2750,"As PE remains largely undeciphered, our results can only be evaluated on the small subset of numerals which we have manually disambiguated. As noted in the paper, these may be easier than the rest of the corpus, meaning our evaluation can only give an upper bound on model performance.
We use a feature-based classifier to give interpretable results which can more easily be shared and discussed with non-technical experts in Assyriology. A limitation of this approach is that model performance depends on the choice of input features, and features which are effective can sometimes seem arbitrary. We attempt to justify the features used by our model by explaining in Table 2 which aspects of the script each is intended to capture.
Lastly, PE numerals are just one aspect of a complex and multifarious decipherment problem. Our results alone cannot paint a complete picture of this script, and must be interpreted in relation to results from outside of computer science."
2751,"limitations in this work, we transliterate all maltese words in the same manner. given the hybrid nature of maltese, it might be optimal to handle words which do not have an arabic origin in a different way. similarly, we do not treat named-entities any differently. moreover, we assume that the maltese text is written using the standard orthographic rules. in turn, the system might produce spurious transliterations for cases with spelling errors. this issue also exists when the text is in raw form, but may be further exacerbated with transliteration. the character mappings could be expanded to handle dropped maltese diacritics, such as writing c instead of ċ, but there are other cases where silent letters such as gh̄ are dropped altogether, making the problem non-trivial."
2752,"limitations our work is focused on just a single case study of language identification of romanized text. as detailed in section 2, distinguishing romanized hindi and urdu is a good candidate for a case study for several reasons, but it would be beneficial to extend this work to other language situations. another limitation was our choice to focus on already existing pre-trained models, rather than directly controlling the training data that is input to each model. this means some of the"
2753,"limitations there remains a problem we have yet to address: the abbreviation of loanwords in japanese. japanese often abbreviates multi-word expressions after transliterating them into katakana. for example, スマートホン <su-ma-a-to-ho-n> ‘smart phone’ becomes スマホ <su-ma-ho>. for our method, we manually extended these abbreviated words to their full forms, but automating this process would be preferable due to the prevalence of these words in japanese. however, back- transliterating them presents challenges as they deviate further from their original english forms. we designed our method to specifically focus on back-transliterating of content words, unlike many other studies that focused on the name entities data. this is because the loanwords of content words are prevalent in japanese. however, names are also challenging as they are in other languages. previous studies have suggested that a more sophisticated method may be necessary for backtransliterating names."
2754,limitations of this research and directions for future research.
2755,"limitations our work focuses on the problem of spelling variation in japanese. the japanese writing system is the most complex of any modern writing system (to find anything of comparable complexity, one would have to go back to cuneiform akkadian or hittite) and presents a unique range of issues that impact speech and language technology, one of which is the spelling variation discussed in this paper. nonetheless, as also noted in section 6, we believe that the approach here should be applicable, perhaps with less dramatic results, to other cases where spelling variation occurs. this may be particularly an issue in lan- guages that do not have a standardized writing system—e.g. colloquial arabic dialects—and where a large amount of spelling variation is often observed. however we have not evaluated the approach on this sort of data. our evaluation system is not open-sourced due to the propriety lexical resources, text normalizer and kana/kanji translators. the text normalizer could probably be replaced with, e.g., the open-source mecab (kudo, 2006) system, though we expect that performance would be degraded. similarly our lexical resources could potentially be replaced with publicly available japanese dictionaries such as jmdict (breen, 2004), but again performance would probably suffer. note in particular that unlike cjki’s japanese orthographic dictionary, jmdict entries have not been carefully curated to indicate which spellings are interchangeable, and which are, rather, words with the same reading but distinct meanings. an informal manual evaluation we performed on potential spelling variant pairs that were extracted from jmdict entries nominally representing the same word sense, revealed that about 92% were valid variant spellings, but that the rest were either wrong, or at least unclear."
2756,"limitations proto-elamite is undeciphered, which means that our results on this script cannot be compared to any known ground truth. we attempt to ground our results by situating them relative to current assyriological scholarship instead. writing systems exhibit considerable variation in terms of the number of characters used, the visual complexity of those characters, and the degree to which they represent phonetic information. although we try to cover a range of alphabetic and non-alphabetic scripts in our evaluations, we cannot cover all possible cases, and focus on those which have some similarity to the proto-elamite script which is the main concern of our work."
2757,"limitations the work described here is part of an ongoing project, and our results, while promising, should be viewed as preliminary. we only report results for the writing systems of two languages, which is a major limitation for a study focusing on typology and cross-writing system variation; past studies in this vein (e.g., marjou (2019); sproat and gutkin (2021); rosati (2022)) have rightly considered a wider range of languages. while the systems we consider (including their “phonographized” versions) provide good points of comparison, the results would be strengthened by considering a wider range of writing systems (which the authors intend to do). finally, it should be noted that the morphological parsing done on the data used in this study may be imperfect, despite the first author’s best efforts. limitations in modern understanding of sumerian result in some cases that should perhaps be viewed with some caution. similarly, for japanese, the treatment of all jukugo words as bimorphemic may or may not accurately reflect how such words should be analyzed in modern japanese. it’s also possible that some non-jukugo two-kanji words were accidentally categorized and parsed as if they were jukugo. certain non-jukugo compounds may have also escaped detection. given the in-progress nature of this research, code and (cleaned) datasets have not yet been made publicly available, but it is the authors’ intention that these resources will be released in the future."
2758,"We would like to discuss the following limitations and ethical considerations:
In this paper, we investigated the cross-domain extraction performance based on a multi-source corpus. Our working assumption is that this corpus represents enough variety to support such a claim. However, we point out that the corpus is biased towards English scientific and patent language, as well as the chemical / material science subject domain. Further, we remark that the subjects distribution itself is biased towards the BM and MSP datasets as the the more varied MeasEval dataset only contains few examples for each of its 10 subjects. Consequently, a balanced corpus should have a more even distribution of both subject domains and language domains by increasing the size of the currently underrepresented domains and ideally including data from more than only the English language.
Further, despite having substantial IAA scores for the re-annotation of the MSP corpus, we often perceived the task as difficult and ambiguous and felt the limitations of only having two contextual entities, instead of the three as proposed by Harper et al. (2021). Yet, the low IAA score (0.334) for the excluded Qualifier entity suggests that including it may not have eased the task. Hence, it may be valuable to further the study of how the measurement extraction problem can be modelled to resolve some of the ambiguities for context extraction.
Finally, while we tried to stay as closely to the original annotation guidelines as proposed by Harper et al. (2021) as possible (with the exception
of the two cases explicated in Appendix C, there is a high likelihood of annotation drift. The reannotators of the MSP corpus were not involved in the original MeasEval annotation procedure and it is possible that the interpretation of the annotation guidelines was slightly different at places than the authors have originally intended. Our adaption of the annotation guidelines can be found at the end of this paper (Appendix J)"
2759,"Our proposed method has certain limitations and ethical considerations that merit discussion. The effectiveness of our approach heavily relies on the rule-based labeler. However, it is important to acknowledge that the labeler may not capture unseen patterns or variations, potentially limiting improvements in various evaluation metrics. Moreover, we were unable to conduct a comprehensive human evaluation of the rule-based labeler in this study due to resource constraints. Therefore, future work should include a detailed evaluation to assess its performance and address any potential limitations.
Collaboration with three radiologists at Kyoto University is a critical aspect of our work. The regular expressions designed in the rule-based labelers were validated through mutual confirmation by computer scientists and radiologists. However, it is essential to note that the radiologists involved in the collaboration primarily work in a Japanese hospital setting. This may introduce potential biases or patterns that are specific to the local context. Therefore, it is necessary to cross-check the performance of the rule-based labeler with radiologists
from different regions and healthcare systems to ensure broader applicability and minimize any potential bias.
Regarding the datasets used in our study, we exclusively utilized publicly available datasets that are properly anonymized and de-identified, addressing privacy concerns. However, it is crucial to emphasize that if datasets containing comparison exams become available in the future, additional precautions must be taken to ensure that no personally identifiable information is inadvertently disclosed or used in a manner that could identify individual patients.
By acknowledging these limitations and ethical considerations, we aim to encourage future research and discussions in the field, driving advancements in radiology report generation while prioritizing patient privacy, accuracy, and fairness. These considerations will contribute to the development of robust and ethically sound approaches in radiology report generation."
2760,"All of the examples mentioned in this section are shown in Table 6 in the appendix.
WER has the main advantage of being simple and consistent. Unlike sentiment or text embeddings, there are not multiple models. The main limitation of WER is that, because it is not based on any understanding or model of the language, there are severe errors that have a relatively low WER, and vice versa, there are non-severe errors that have a high WER such as a multivitamin vs. a multi vitamin.
Sentiment has strong limitations due to the fact that these algorithms are designed to only measure how positive or negative a text is. Sentiment proved to be sensitive to misses in disfluencies like um or uh. This is highlighted in the example, uhm it started last night vs and it started last night,
where there was a strong difference in sentiment of 1.707. This can be an advantage or a limitation depending on the scenario. Many ASR systems overlook disfluencies, but, for example in human robot interactions, spoken dialogue systems, or in the prediction of dementia status, disfluencies can be vital to understanding and performance (Baumann et al., 2017; Clark and Tree, 2002; Farzana et al., 2022; Lopez-de Ipiña et al., 2017; Mueller et al., 2018).
There is the also a limitation on the accuracy of the model. In the examples any previous surgeries vs. any previous surgery or uh i smoke about a pack a day vs. uh smoke about a pack of day, there is a high difference in sentiment yet the only difference is in missing the pronoun i or the plural of surgery, which should not affect sentiment greatly.
Despite these limitations, sentiment is able to catch some severe errors where the WER is relatively low. In the example where crystal meth becomes crystal mud or where chest pain becoming chatting the WER is 0.125 and 0.333 respectively, but the difference in sentiment is very high at 1.858 and 1.889 respectively.
Text embeddings are limited by the performance of the model, like sentiment, yet capture more than just polarity of a given text. Knowing that many of these models are trained in a self-supervised manor using the context in the training text, we can see how the embeddings in the example of my parents and our friends would be similar. Both of these phrases could occur in with similar surrounding text; they have the same grammatical structure (a possessive adjective followed by a noun) and
parents and friends are both human relationships. Another limitation on these models is the amount of text they can handle. Anything above the model’s limit gets truncated, and consequently, loses the meaning of truncated text. Although utterances are commonly short in ASR training data, the character limitation on these models could affect performance on longer utterances.
Despite these limitations, text embeddings were able to capture well the differences in meaning. Text embeddings were able to give a high score to the examples where crystal meth becomes for sunlight and where chest pain becomes testing when WER and sentiment scores were relatively low. Text embeddings were also able to give low ratings for different writings of the word okay and numbers (ok vs. okay, or uh thirty eight degrees vs. 38 degrees) when WER were high."
2761,"One limitation of our study is the small size of the test set, which may impact the generalizability of our results. Additionally, we restrained our work on clinical entity extraction; in future work, we would investigate more in several tasks using the E3C temporality layer to cover a task of Name Entity Recognition and Relation Extraction tasks.
Finally, the E3C guidelines have been designed for clinical entity extraction and entity-linking via UMLS entities. After the first step of manual annotation, some spans of the entities have been modified to fit as close as possible to the semantical concepts found in UMLS (Magnini et al., 2020). For instance, clinical entities could be split into separate disorder concepts, and the extent of a disorder candidate could be reduced to fit with a concept. These biases could induce additional difficulties in finding the correct span for a given model."
2762,"In our work, we manually annotated small portions of corpora. Such limited size is justified by the time-consuming task of temporal annotations and the requirement of expertise for toxicity event annotations. Although our temporal representation seems to perform well with other clinical reports containing information about a different type of cancer from that on which it was trained (e.g. lung cancer vs. colon cancer), such results must be validated on clinical reports containing information about additional cancer types. Additional experiments are also needed to validate the generalizability of our event-independent representation, such as evaluating it on other hospital or data warehouse clinical reports with various structures and evaluating it on other extraction tasks with different event definitions."
2763,"The methodology of the constructed corpus is based on NER, making SSI a corpus integrating RE with NER. We aimed to enrich our experiments with multi-task learning so as to measure the impact of NER on the extraction of relations. Unfortunately, after conducting statistical analysis on SSI, it showed that, due to its limited size, we did not have enough instances of the same pair of entities participating in one sentence. As this statement
would negatively bias our multi-task model, we refrained from conducting these experiments, before proceeding to annotate further instances and add them to our corpus in further work. In terms of annotation, only one annotator has been invested in the current task, as it is a preliminary work including a new dataset for relation extraction, with new entities and relation types. In a future work, multiple annotators performing the annotation task and metrics for the corpus quality will be presented."
2764,"One limitation of this study is a potential lack of reproducibility for some experiments due to the employment of the proprietary GPT 3.5 LLM. In the future, this could be addressed by deploying and versioning open-source LLMs with similar capacities locally. We acknowledge a potential bias in our dataset, as only a quarter of the full corpus was manually reviewed based on existing annotations, i.e., three-quarters of GGPONC 2.0 were excluded from our analysis so far. Therefore, the matched dataset of controls will likely overrepresent the prevalence of ECCNPs, while not adequately representing the full spectrum of hard negative cases. However, we considered a manual review of the full corpus as unnecessary due to the very low number of ECCNPs expected in the remaining documents. Further, we note that also other kinds of coordination ellipses (omitted adjectives or verbs) besides ECCNPs may be relevant for downstream information extraction tasks. Although preliminary annotations for these exist in GGPONC 2.0, we excluded them from our analysis, as annotations were not comprehensive, and their resolution is often more ambiguous than elliptical compounds."
2765,"Since the training datasets of ChatGPT are unknown, some data used for evaluation may or may not exist during the training phase of ChatGPT Also, a new version called the GPT-4 model has been released that may ensure higher accuracy. Nonetheless, GPT-4 is very costly to use, around 60x more expensive than ChatGPT. Meanwhile, even using the paid ChatGPT Plus2 subscription, it is available for just a limited use (allows evaluation of only 25 samples in 3 hours). Another limitation of this research is that the results mentioned in this paper for ChatGPT may not be reproducible, as ChatGPT may generate different responses for the same input prompt. Although the experimental results may change over time, this work will still give a concrete direction for future research using ChatGPT like large language models in the biomedical domain."
2766,"Although our findings suggest that biomedical language model pre-training is quite robust to suboptimal tokenization, we note that our work has a few potential limitations that should be explored further. The use of a biomedically relevant subset of the SIGMORPHON Shared Task dataset for evaluating biomedical term tokenization is a straight-forward and reasonable strategy, however, it is important to highlight that the resource was not created for this purpose and might not be perfectly aligned with ideal biomedical tokenization. Additionally, we would like to point out that even though our BioVocabBERT tokenizer outperforms other equivalent tokenizers like PubMedBERT’s, it severly underperforms the best possible segmentation accuracy (48.5 vs 74.1 for our fine-tuned CANINE model). It is therefore possible, although unexpected, that a tokenizer which performs biomedical tokenization at even higher levels could lead to sudden improvements in the pre-training process. Finally, we note that the effects of the BioVocabBERT’s much larger vocabulary size, almost three times larger than PubMedBERT’s, on the pre-training process were not explored in depth. Nevertheless, given that some previous work (Feng et al., 2022) argues that larger vocabularies lead to slight improvements in downstream tasks, our main conclusions are likely to hold."
2767,"The MIMIC-III corpus includes a discharge summary for each admission. However, it is limited
to patient’s time in the intensive care unit (ICU), meaning that the patient’s history for any time after transfer from the department is lost. Given most patients progress to lower severity departments as they recover from intensive care, a large cross section of the patient’s notes are missing from our analysis. In Section 3.2 we discuss the statistics that justify this conclusion."
2768,"Retrieval augmentation adds complexity to natural language generation requiring a separate retrieval module before the text generation step can begin. Additionally, retrieval augmentation possibly introduces more input text than the original input which is problematic for many neural network architectures with limited input space especially in the case of summarizing entire scientific papers. Finally, retrieval augmentation itself could introduce factual or relevancy errors if the retrieved documents are irrelevant or incorrect and they end up being used in the generated summary."
2769,"The data augmentation method mentioned in the paper requires the augmentation dataset to be of a similar structure to the task dataset, which is not the case for MeQ-SUM. As a result, the data augmentation experiments do not provide significant improvement in performance."
2770,"The limitations of this work are mainly that there is a small amount of data available for inference to test the models. ROUGE-L is used as an assessment metric and n-gram overlap metrics are notably not optimal for abstractive summarization assessment (Zhang* et al., 2020; Deutsch, 2022)."
2771,"There are a few limitations pertaining to the training data we used, some of which are listed below.
1. Our domain adaptation of LLMs was performed on English reports only; therefore, it may not work out of the box in a multilingual setting.
2. The paper utilizes the MIMIC-IV dataset for DAPT training, which might include overlapping data from MIMIC-III and MIMICCXR. Consequently, there is a potential risk of information leak in this method.
3. There is a data imbalance concerning imaging modalities and anatomies covered by our training data. For example, regions such as extremities, neck, spine, and shoulder are underrepresented in the dataset, and report summarization related to those regions needs thorough evaluation.
4. A study is needed to examine the diversity of patients represented in the data and how it impacts the model’s performance for underrepresented communities.
5. Different radiologists and radiology departments have distinct preferences and styles for writing reports. Moreover, clinical referrals occasionally dictate the extent to which certain details are documented in the report. No study has been conducted on the consistency, uncertainty, or information richness of the report.
Aside from the training data, the model’s space and time throughputs may render them unsuitable for on-premise and/or at-the-edge applications. This aspect presents an opportunity for further research on how to best quantize and deploy RadBloomz (and similar LLMs) within the clinical workflow to enhance efficiency for radiologists. Additionally, the paper utilizes the MIMIC-IV dataset for DAPT training, which could contain overlapping data from MIMIC-III and MIMICCXR. Consequently, there is a potential risk of information leak in this method."
2772,"In this work, we used GPT-3.5 to generate additional training examples and showed that it helps improve the relevance (ROUGE and BERTScore) of the generated summary. However, we did not explicitly analyse the quality of generated examples to check whether or not they are faithful and factually correct which could lead to the same problem in generated summaries. To obtain better training examples, we could use faithful or factuality metrics to assess generated training examples and then use the post-editing method or human evaluation
to remove unfaithful content, which we leave for future research."
2773,"limitations we only investigate representative methods of three widely-used el paradigms. however, there are more el methods and paradigms we may not cover, and we leave them as future works. furthermore, more auxiliary information in the biomedical domain can be introduced to address the nil issue we identify in this work. for example, a hierarchical structure exists for concepts in kbs in the biomedical domain. therefore, nil may be solved by linking them to hypernym concepts in the partial kbs (ruas and couto, 2022). we consider the hierarchical mapping between nils and in-kb concepts as a potential solution for performance degradation in partial kb inference. users can obtain different entity-linking results based on their own kbs which have the potential risk of missing important clinical information from the texts."
2774,"limitations this work has certain limitations in terms of the scope of the experiments and what can be reliably inferred from them. our dataset contains notes that are predominantly from anglophone countries. however, there are less than 20% of the rows that originate from non-english speaking regions. they might contain words in other languages (e.g. italian), and although the disease names are usually rendered similarly as english, our models are pretrained on english and their ability to process other languages is therefore limited. another issue is the relatively short length of these notes. while some notes span a few sentences, most are very short and no more than 4− 5 tokens in length. this hampers the ability of a contextualised model to derive meaning from the context around each word and limits the power of attention-based architectures that are well-suited for larger contexts. in this preliminary study we only targeted one condition and looked at binary classification. the natural step towards a more inclusive experiment would be to consider other conditions and also use multi-class classification setups where a more finegrained scheme is used to classify a condition. creating a sizable multi-class and multi-label corpus is a labour-intensive endeavor that requires more time and effort and would be a goal for a future work. we did have access to multi-class annotations for our current training set, however, one major issue is that the cancer-positive cases are a small percentage of the entire rows and among the cancer types themselves, there are types that occur only once or twice and the rest belong to more frequent classes. this would make it harder for the model to learn infrequent classes. we plan to augment the annotations over time to be able to conduct experiments in scenarios beyond binary classification and cancer alone. the issue of negation was further complicated in this work by a few cases where the note had been classified as cancer positive because the doctor had identified a history of this condition in the patient but had ruled out or downplayed the possibility of cancer at the present time. distinguishing a current co-morbidity of cancer from a past history of cancer would introduce further complexity and this work does not attempt to address that."
2775,"limitations of wer, and 3) incorporating severity into the training of an asr system increased performance, lowering the overall severity and wer significantly. in future work, we will experiment with different architectures, data, and methods for using semantics in the training of asr systems. limitations aside from the limitations of these proposed measures mentioned in section 6.5, we acknowledge other limitations here. the"
2776,"limitations one important aspect of achieving optimal performance when using llms is the design of a highquality prompt. in this study, we consider both zero-shot and few-shot learning scenarios, which assume no or very limited task-specific data. however, iteratively refining the prompt over time to obtain the best-performing prompt may break the zero-shot or few-shot scenario. moreover, the final prompt used in this study is specifically designed to guide the crowd workers in the annotation process of the coda-19 dataset, with frequently asked questions (faqs) refined over time to address workers’ confusion. in a real-world scenario, users would not have access to such helpful faqs when working on a new task. therefore, the performance of llms may be lower in practice. also, llms are susceptible to a data leakage problem due to their training with internet data. for example, chatgpt is known to have been trained on internet data prior to september 2021. considering that the coda-19 dataset was released in july 2020, with its train, validation, and test sets made publicly available, there is a possibility that some closed models have seen the exact test instances, leading to an unfair comparison. since the training data are not disclosed for the closed models, the impact of this exposure on the models’ performance remains unknown."
2777,"limitations there are also several potential limitations. first, the severity of the identification of depression is subjective, which inevitably leads to annotation bias, and we cannot verify the actual diagnosis. thus, our model is not intended to be used as a psychiatric diagnosis tool but an estimate of depression level for users, which can be utilized to direct intervention and treatment for non-clinical use. second, the datasets used are collected from a single social media platform (reddit) and are imbalanced. to train a more robust and effective model, future works should collect more precise data from other social media and increase collaboration with clinicians to ensure the quality of the data. despite these limitations, we believe that our work will facilitate depression severity detection. in future work, we would like to explore other fusion strategies to better integrate various information and assess the performance of chatgpt (yang et al., 2023a) on the depression severity detection task."
2778,"limitations our experiments only used the abstracts of the articles and we did not consider the full text. actually, the full text might provide valuable information for the decision about the similarity, while making it harder at the same time, since the text is much longer. however, full text is usually not available for many of the articles in pubmed, which would result in even shorter datasets. we did not train any model based on the available datasets, as carried out by (mysore et al., 2022). indeed, our goal was to evaluate the dataset based on general-purpose text similarity algorithms that had not been fine-tuned for a particular dataset. for the annotation of the facets, we simply asked the annotators to select sentences that were part of the research goal. however, it might not have been carried exactly as in the annotation of the smafira-c dataset."
2779,"limitations our work follows a pipeline approach, where the optimization of event trigger detection and argument identification are done separately. this causes the event trigger detection errors to be passed to the argument identification step. in the future we plan to do the joint optimization of both steps to reduce internally transmitted errors."
2780,"limitations the result of dictionary match and autoner based on our implementation is not comparable with 5https://www.amed.go.jp/en/ the results shown in the original paper (shang et al., 2018). because the performance of distantlysupervised ner is severely dependent on the domain dictionary used, we can not simply compare the performance of the methods if common domain dictionaries are not used."
2781,"limitations in the design of this study offer directions for further work to improve performance. since the redditmh dataset uses posts from general mental health subreddits, the binary-choice qa model was not exposed to finer-grained data from illness-specific subreddits, which could lead to better assessments. also, the multiple-choice qa model forces an answer from the list of five diseases even if there are no distinct language markers of any disease. future work will combine binary and multi-choice qa models to filter at-risk individuals. two-stage qa models will serve as effective screening tools to ultimately provide care to individuals who need it the most."
2782,"limitations although our models performed well in all downstream tasks, the models also have some limitations. one of the limitations is the lack of varied biomedical corpus. hence, we plan to work on integrating clinical documents e.g. ehr data, specifically physician notes, to make the model more robust to various kind of biomedical documents. the models can also be enlarged by using continual learning strategy from well-known french pre-trained language models. camembert (martin et al., 2020) can be used as a base model and the training can be continued using our biomedical corpus, like biobert (lee et al., 2020) and others did. moreover, our models used 512 sequence of tokens and more longer sequence lengths can be used as seen in the long language models like bigbird (zaheer et al., 2020). we are currently working on a new version of alibert with more data and a greater diversity of corpora that include text from ehr and medical notes in our corpora. finally, we also plan to train alibert to generate biomedical texts for different purposes. a reasonable amount of computational resources was used to conduct this study, since approximately 20,160 hours of gpu computation were used to create the three pre-trained models presented above. the total environmental cost according to green algorithm (lannelongue et al., 2021)12 is equivalent to 1.45 mwh or 71.11 kg co2e. this computational cost and environmental impact should be taken into consideration when training such a model."
2783,"limitations the authors want to use the opportunity given by this column to highlight the fact that the definitions generated by this procedure do not all meet the standards required for presentation to users, or for reasoning-required scenarios, due to their imperfect quality. we release this dataset for building retreival-based systems, and evaluate large biomedical language models on the definition-generation task (and eventually for low-rank finetuning of existing language models). in addition to the imperfect quality of the generated definitions and the presence of hurtful definitions in the dataset, it might also be useful to consider the bias induced by the choice of snomedct as our source of knowledge. while extensive, snomedct does not cover all possible relationships between concepts, and by biasing the output towards relationships present in snomedct, we might perpetuate existing biases in the data. another limitation is that we only evaluate the generated definitions on three metrics, but more could be relevant depending on the application. finally, our rating of what is considered acceptable insight was biased towards what could possibly be condensed in short definitions (49 words on average), but longer definitions might sometimes be required to express the full range of nuance required by biomedical concepts. it is however difficult to estimate the value of omitted information."
2784,"limitation the model has achieved a significant performance boost. however, the trade-off is the computational time. due to using two forward passes, the model requires more time to generate the results compared to the other models."
2785,"limitations while interdapt can be used as a strategy to reduce the negative impact of weak labeling in realworld use cases, it is difficult to understand the magnitude of performance improvements that can be achieved using interdapt as these improvements are highly dependent on how noisy the target sub-domain datasets are and how robust the target entity labels are. despite potentially reducing data annotation costs, interdapt still has data requirements that are domain-specific to some extent. while such data can be obtained from publicly available datasets, those tend to be less noisy than real-world data. as this work does not explore the impact of the amount and nature of noise in data, it is unclear at this time how this framework would perform when cleaner datasets are used in prior stages of training."
2786,"limitations, conducting this evaluation does not lead to any unwanted biases. only the publicly available academic datasets are used that did not require any licensing. thus, no personally identifiable information has been used."
2787,"limitations we have applied our neighborhood knowledge graphs to the pubmedbert and pubmedbert+bran models and show the effectiveness of the graphs on the pubmedbert model. we have not deeply investigated how our approach cooperates with other enhancements, and the performance is lower than the state-of-the-art model (wang et al., 2022)."
2788,"limitations this paper shows that nar generation can be achieved from publicly available plms parameters. however, there is still room to be validated for additional pre-training. the performed pre-training in this study is small, and further investigation of whether the training steps and the size of the corpus are sufficient, and whether the self-training tasks other than permutation language modeling are effective."
2789,"limitations in this study, we used entity type markers to enable a qa model to output multiple answers for re for multiple entity pairs that share an entity in a sentence. however, we have not evaluated other qa models that can output multiple answers. in addition, since entity type markers are incorporated before and after entities, it is difficult to apply the method to nested entity pairs. qa-based re methods are not computationally efficient since they require multiple qa processes to extract the relation for an entity pair. furthermore, the question templates in qa-based re affect the performance of re, but in this study, only one template set was used for evaluation, and there is room for improvement. direct comparison with existing sota (weber et al., 2022) is not performed because of the unavailability of the original drugprot test data set. in addition, the sota model is an ensemble of 10 pretrained language models, and we cannot directly compare the performance in a fair setting."
2790,"limitations our proposed model has three limitations. firstly, because of the large size of the literature graph in this study, representation learning is performed on the literature graph using transe, but in fact, there are relations in the literature graph that cannot be represented by transe as shown in section 5.1. to overcome this issue, more expressive methods such as rotate (sun et al., 2019) and gcn (kipf and welling, 2017) could be investigated. these methods are expected to be able to represent complex relationships associated with multiple types of external information. secondly, our model adds a representation of the literature graph to the target text, so longer sentences require truncation of textual information. thirdly, we have not analyzed the results in detail and how the proposed method positively and negatively impacted document classification. we leave these limitations for future work."
2791,"limitations we propose welt as an approach to address the class imbalance problem. we have only evaluated welt for bioner tasks, although we hypothesize that it can be adapted to any application / domain that has skewed dataset. we further want to point out that our method was only evaluated on english datasets. yet, we argue that it can be applied to other languages as well. finally, we have not assessed welt’s performance on larger training datasets (i.e., more than 30,683 training examples)."
2792,"limitations such as time and resource constraints. however, incorporating the expertise of medical professionals during the development process may provide valuable insights into the clinical implications of the generated summaries, resulting in the creation of more clinically relevant and useful models. given the effectiveness of incorporating human feedback in various nlp tasks (ouyang et al., 2022), we recommend future research to explore the performance of involving medical experts in the development and evaluation through a human-in-the-loop approach."
2793,"limitations there are a few limitations in our work: first, the rules developed are totally based on frequency filtering and not further checked by medical experts, we are not sure whether there are any hidden template or patterns in the clincal note summary. second, due to time limitation, we did not conduct second time pre-training for the language models, t5 was originally trained from generic text of which the genre is quite different from the clinical domain. 765 training examples may not be enough for the model to learn. third, we would like to give a full test of the more recent large language model (e.g. chatgpt9), but we cannot fine tune it with the open free api."
2794,"limitations during the manual labeling process of the notes, we searched for keywords to select for the namedentity-recognition and compared them to those appearing on the summary. for the system to work effectively, the list of problems and diagnoses should appear in its entirety in the original text columns. however, this was not the case. most of the examples had summaries with additional diagnoses and problems with words that could not be extracted from the original text. in addition to these cases, having an inconsistent use of acronyms leads to different versions of the same term in the original text and the summary. finally, when comparing both lists of tokens, the items found in both segments had a distinct order of appearance, having examples score low for only counting one of them as a match in rouge-l, where the list of words is the same but in a different order."
2795,"limitations the proposed model is computationally demanding. recent work on parameter-efficient fine-tuning methods, such as lora (hu et al., 2022), suggests that they can significantly reduce the number of trainable parameters at a minimal performance cost, which may help further democratise the development of domain- and task-specific models. in addition, as we continued to pretrain, to obtain the pulsar models, their tokenizer was inherited from corresponding flan-t5 model. thus it does not contain domain-specific terminology, which may be a limitation in terms of representation density (i.e. frequent clinical terms may be split in multiple rare sub-tokens)."
2796,"limitations the pre-training methodology used in this work applies a masking process at the sentence level that requires scoring the relevance of each sentence within the text. therefore, this implies additional computational costs, limiting the scalability of our approach. due to time restrictions, the appearance of hallucinations in the generated radiology reports by our models has not been measured. it would be necessary to quantify this aspect because of the criticality of the domain of use in future works."
2797,"limitations the limitations of the presented system derive directly from the pre-trained models they are based 3https://huggingface.co/chizhikchi/ sci-five-radsum23 on. flan-t5 was pre-trained on a large-scale text dataset and was not assessed for existing biases. as a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data (raffel et al., 2020). thus, it is not recommended to utilize such systems in any application without first conducting a thorough evaluation of the safety and fairness concerns that are specific to the particular application in question."
2798,"limitations though our systems achieve promising results in solving the summarization task for long documents, we believe that we can gain more improvement with the following further considerations. the current explicit key information selection strategy is somehow heuristics. we can alternatively try extractive summarization methods. also, this lay summarization is interesting which helps nonexpert readers can understand scientific articles. however, specific strategies focusing on this aspect such as using non-expert vocabulary, or mapping to general knowledge, are yet applied. some minor parameters such as the sequence lengths (9k, 12k), or tuning the sota brio model also need to be investigated more deeply."
2799,"limitations our model demonstrates commendable performance in terms of readability and relevancy, but it falls short in the factuality metric, this is one of the potential areas for improvement. given more time, one of the directions that we might have explored, is the factuality based re-ranking which considers factuality as metric for comparison, instead of rouge scores, or considering both scores giving then certain weights. we have made substantial efforts to improve efficiency and reduce memory requirements, but large language models still impose significant demands on time and computational resources, which remains a limitation of our current work. additionally, the constraint of a token threshold set at 512 posed challenges in our work. these limitations highlight areas for future research and development."
2800,"limitations this study is our first exploration of this research topic. we use pre-trained primera, pegasus, and bart-longformer models and fine-tune them for technical abstract and lay summary generation. novelty in the techniques is the main limitation. we downloaded all pre-trained from hugginface, which may be inappropriate for this summarization task. how to determine an excellent pre-trained model doesn’t include in this study. in addition, we only fine-tuned all pre-trained models over the task-given datasets without collecting other related summarization data to enhance the model performance."
2801,"limitations this study is constrained by limited input token length due to hardware memory limitations and lengthy training times. even with the lsg attention mechanism’s efficiency, this inadequacy persists in both subtasks. longer token length could improve summary relevance and factuality. particularly in the second subtask, where the abstract is absent, this poses a challenge in generating a summary from sections with high information density."
2802,"limitations the performance of the model heavily relies on the quality and relevance of the keywords and headings provided in the dataset. if the dataset lacks rigor during the column labeling stage or if the information in these fields is inadequate, it can lead to a decrease in the overall effectiveness of the model. moreover, incorporating additional prompt tokens may introduce length constraints, potentially resulting in article truncation and the loss of crucial information."
2803,"limitations our best results were obtained using a few-shot prompt of the text-davinci-003 model from openai. while the technical barriers to this method are very low due to the ease of use of the model api, the cost of querying the api can become prohibitively expensive and this limited our own experiments with the model10. this cost would rise significantly more if the text-davinci-003 was finetuned to perform these lay summaries. if this is a concern, then using the open-sourced biogpt models may be beneficial. it should be noted that performing the fine-tuning process is itself expensive and requires access to high-end gpus. practitioners should investigate parameter-efficient finetuning techniques (hu et al., 2021) if access to these gpus is an issue."
2804,"limitations despite our efforts to enhance the efficiency and minimize the memory cost of our models, large language models still demand considerable time and memory resources, which remains a limitation of our work. given sufficient time and computational resources, we could explore the possibility of increasing the batch sizes and running additional epochs to further optimize the model’s performance. while our final system excelled in relevance and factuality aspects, it was relatively poor on readability, which represents a potential area for improvement. to improve the robustness of the model, given additional time, we would use a combination of machine learning and list-based approaches to identify arcane words and technical terms and substitute them with their easy-tounderstand synonyms. with these procedures, we believe that we can make our summaries more readable."
2805,limitations we can improve our model in the future by using the keywords given in the dataset to enhance the quality of the summaries generated by using keyword embedding in addition to the fine-tuned model.
2806,"limitations there are some limitations to our approach. firstly, we utilized the bart model, which has a maximum sequence length of 1024 characters. therefore, we had to divide the articles into sections and process them one by one, which could result in less coherence in the final summary. additionally, we worked on google colab, which has a time and memory limit, and we encountered occasional connection issues, which could slow down our progress."
2807,"Testing Necessary, But Not Sufficient Conditions For Tutoring With LLMs. In this paper, we test the abilities of LLMs to perform the functions present in Intelligent tutoring systems, namely generating explanations and corrections. There are also other desirable properties, like the ability to answer direct questions from a student or the ability to present content engagingly, which are beyond the scope of this paper. Indeed, those properties are some of the areas where LLMs probably excel relative to traditional ITS. We have only explored a necessary condition – are models able to reliably teach – not a sufficient set of conditions for the evaluation of tutoring using an LLM.
Focusing On Mathematics. In this paper, we focus on tutoring in rudimentary mathematics. While this is useful – it is a necessary condition for a useful tutoring system, especially because arithmetic skills are used in almost all domains of learning – there are many other domains to which we might want to apply tutoring. LLMs may have greater or lesser aptitude in these domains than in arithmetic. Evaluation at the level of gradeschool mathematics tells us that these models are still error prone, but does not necessarily tell us how close they are to usefulness in tutoring other subjects (either more advanced mathematics or orthogonal subjects like history or writing).
Generalizing Text vs Code Results. We aim to examine the differences in ability of code scratchpads and text scratchpads for the purposes of tutoring. While this paper provides evidence in that direction, we only compare two GPT-3 models: text-davinci-002 and code-davinci-002. The amount of manual effort required to evaluate explanations and correction limited the number of comparisons we could conduct, as did the limited number of highly performant code/text generating models."
2808,"In this work, we explored question generation for computer science textbooks. We have not yet explored a broader range of course subjects, and it may be that the prevalence of computer science knowledge on the Internet, including through forums like Stack Exchange, makes QG easier for this discipline than for others. Furthermore, we examine a relatively narrow range of question types. Other questions –like multiple choice questions, or compare and contrast questions– will require deeper exploration and substantial adaptation of the methodology that we proposed."
2809,"While our study provides valuable insights into the use of prompt-based methods with GPT-3 for GEC tasks and its controllability, several limitations should be acknowledged.
Focus on GPT-3: This study exclusively examines GPT-3 as the language model for GEC tasks. While GPT-3 has shown remarkable performance in various NLP tasks, other pre-trained language models, such as GPT-4, may offer different results. A broader investigation that includes other language models would provide a more comprehensive understanding of the applicability of promptbased methods in GEC tasks.
Limited evaluation metrics: The evaluation of GPT-3’s performance and controllability in our experiments mainly relies on quantitative metrics, such as edit distance and task scores. These metrics may not fully capture the nuances of grammatical error correction or the model’s ability to adapt to different learning scenarios. Additional qualitative analysis, along with more diverse evaluation metrics, could provide a richer understanding of the model’s performance and controllability.
Variability in examples: While our study highlights the importance of example selection in fewshot settings, we do not thoroughly explore the impact of example quality or diversity. The effect
of using different types of examples or a more diverse set of examples remains to be investigated, which could further inform the design of effective example sets for prompt-based GEC tasks. By addressing these limitations in future research, we can further advance our understanding of the performance and controllability of prompt-based methods with GPT-3 and other language models in GEC tasks and beyond.
Potential fine-tuning on test data: There is a possibility that GPT-3 has been fine-tuned (instruction tuning) on the test data we are using, which might explain the higher evaluation scores compared to previous research. As this information has not been disclosed, we are unable to verify it at this time. This point should be taken into consideration when interpreting our results."
2810,"Our experiments were built on perfect sentence alignments in the original and revised essay drafts, thus the performance could be lower in the real end-to-end Automated Writing Evaluation (AWE) system. In addition, our corpus is small due to expensive annotation processes, which makes it challenging to train or finetune large language models. Also, we only focus on revisions in argumentative writing, specifically, we focus on the evidence and reasoning revisions, however other revisions like claim revisions are not used. Furthermore, the revised drafts were done after providing feedback on the original drafts, which means the revised student essays are likely to follow the instructions in the feedback but we did not use this information for revision quality predictions, which will be used in our future work.
Our proposed Argumentative Contexts (ACs) are generated by ChatGPT which is not free for the whole community. Also, ChatGPT-generated ACs have small randomness, which is also the reason we did 3-seed runs in the experiments. In addition, the ACs are essay-level context which means different revisions in the same essay use the same context. It could be tailored to have sentence-level ACs where each sentence-level revision has slightly different revision purposes, but it would cost more
time and money. Moreover, our proposed zero-shotCoT prompts perform better than Single prompts by small margins in specific cases, which indicates that Chat-GPT is limited to conducting CoT extraction and summarization to handle complex wording and sentence structure. Therefore, we might need to redesign the prompts in our future work."
2811,"While our work provides a useful starting point for understanding student feedback, there are limitations to our work. Addressing these limitations will be an important area for future research.
Comments may not reflect real student feedback. The comments in our dataset are from users who have chosen to post publicly on YouTube. Addi-
tionally, the comments may include features specific to this online education setting. Thus, the comments may reflect real student comments from these courses.
There is a selection bias in lecture sources. SIGHT includes lectures that may be drawn from the most successful offerings of that course. The instructional quality may not be representative of typical instruction. Thus, inferences drawn about the instruction should be interpreted with caution, as they might not generalize to other lecture settings.
We analyze only English comments. We analyze only English comments because the lecture content is given in English and the authors are most comfortable with English. As a result, our rubric may not capture the types of feedback from nonEnglish students watching lectures taught in English. In the future, the rubric and analysis should be adapted to account for the multilingual feedback setting.
We annotate a small subsample of the data To assess the validity of the automatic labels, we conduct a diagnostic study on a small, randomly selected subset of the dataset, comprising approximately 2% of the comments. Our work aims to establish a preliminary evaluation of the humanmodel agreement and model annotations, and further validation of the automatic labels is necessary. Future work can focus on acquiring such gold-standard annotations to enhance the quality and reliability of the automatic labels."
2812,"Given that the WLAs are calculated as the sum of all the TLAs representing one single word, it is possible that there could be an underlying preference for longer words in the framework. However, multiple tokens in a word could also have conflicting attributions, so it is not entirely clear how this affects the framework. Given the results of this implementation, it could be reasonable to try and calculate the WLAs as the mean of all TLAs instead.
Furthermore, it is reasonable to discuss the consequences of the preprocessing steps being carried out in the experiment. Although such preprocessing steps might increase the IAA measured between the human rationales and model rationales, it is reasonable to question what these preprocessing steps actually result in and their possible value in realworld applications. In cases where the use case is to identify and highlight certain important words, such preprocessing steps might bring a considerable amount of value. However, if the end goal is to represent the model’s attention as precisely as possible, these preprocessing steps might skew the representation of the model’s attention. Consequently, one could argue that there exists a trade-off between usable model explanations, which can be used as an assisting or guiding tool for the human
expert, and explanations that are fair representations of the model’s inner workings. In the case of ASAG, explanations such as the ones created by the presented framework could likely be used as an assisting tool in helping human expert graders find important words or sentences. Given such a framework, the speed of grading could likely be increased without removing the trust of having a human grader making the end decision.
Lastly, it is worth noting that the use of top k sentences should only be seen as a means of calculating IAA. However, in a real-world inference setting, the number of relevant sentences might be dependent on the task as well as the subject. In the case of assisting a human expert in grading, the number of top k sentences might be a parameter controlled by the human expert in order to showcase only the most relevant sentences marked by the model annotations, where the number of relevant sentences might be dependent on the length of the student answer as well as the complexity of the given question."
2813,"Our user study tests students on only two short lecture videos which are pre-recorded and carefully edited. Future work should test the efficacy of the summaries under a wider range of conditions: prerecorded videos versus live lectures, lectures and summaries of different lengths, and a wider range of topics and disciplines.
Overall, our experiments compare three different conditions. Adding other conditions might have shed light on the relative value of automatic summaries. For instance, if we limit the time available for participants to prepare before taking the quiz, and at the same time track the amount of time spent on summaries and/or videos, then that could give better insights into how students would utilize the two sources differently with limited time constraints. Finally, we could also contrast the usefulness of summaries versus transcripts."
2814,"For future work, we plan to update the annotated corpus and the scope of annotation to paragraphs and/or whole documents. In this study, we opted for the minimal context approach for practical reasons, such as budget, time constraints, and copy rights of
the source corpora. The minimal context approach allowed the annotation sample to represent as many writers as possible for better generalization with relatively small sample sizes. However, future research should use longer units of analysis to enhance the quality of manual annotation. Despite this limitation, the results of the current study indicated that the current approach is a promising direction for further research on automated analyses of rhetorical features."
2815,"Our experimental results should be interpreted with the following limitations in mind. First, our experiments involved relatively small datasets in English only. The performance of the model should also be evaluated on other languages and larger datasets. Second, the improvement observed in our best models depends on both the efficacy of the linguistic features and on the strength of the neural model itself. As neural models continue to improve and effective linguistic features are identified, the best methods for combining may also need to be updated."
2816,"The current work has a number of limitations to consider.
(A) The paper’s experimental design was limited to a single language because we are not aware of any other learner corpora with multiple answers provided to the same exercises.
(B) The described approach to assessing the correctness of learner answers is limited by its design. First, the number of GEC hypotheses to check depends on the GEC model’s performance and, potentially, on the language. Second, if a word was not corrected, it can be a false negative error instead of a correct answer. Third, the GEC model can suggest corrections (valid and not valid) even to a correct answer depending on the data it was trained on. (C) Our approach focuses only on grammatical errors and it does not take into account semantic or pragmatic errors.
(D) Due to limited resources, we were unable to involve more people with prior annotation experience in the re-annotation of the RULEC test set, as well as in the manual verification of hypotheses generated by the GEC model. We acknowledge that the annotation performed by our annotators may not be entirely error-free: the annotators were free to work at their own pace and therefore could potentially rush and make errors themselves. Hence, we do not claim that the re-annotated RULEC test set does not include any inconsistency anymore. We believe that the existing datasets should be thoroughly checked, given the small amount of learner data available for languages other than English, before utilizing them to train and evaluate new models. (E) Considering the practical use of our GEC model as a component of a CALL system, we find that it can potentially be used in a limited context, i.e., for checking answers provided to cloze and multiple-choice exercises, only for best-performing error types. As for alternative correct answers, even for best-performing categories of answers, a human teacher should verify proposed corrections. We have to underline that learner errors in RULECGEC (and especially in any synthetic dataset) can significantly differ from errors made by learners with various backgrounds, native languages, and proficiency levels. We also find that low recall of state-of-the-art GEC models impedes their usage in language learning settings. At the moment, learner answers should be verified by a human teacher."
2817,"Unmeasured covariates. A range of factors may affect instructional outcomes, only a subset of which could be measured with this data. Making strong claims about the link between discourse moves and instructional outcomes requires experimental validation. For example, the quality of the math task that the students are working may affect the discourse as well as learning outcomes. We can isolate the effect of discourse moves by randomly assigning teachers to learning opportunities that help them improve their use of these moves,
and examining downstream impacts of these new talk moves on student outcomes. (Demszky et al., 2021a) has taken a similar approach successfully in an informal teaching context, but such a study is yet to be done in a K-12 context.
Generalizability. Although the NCTE transcript dataset is the largest available dataset of U.S. classroom transcripts, it only captures a tiny fraction of U.S. classrooms and hence there are limitations to its representativeness. The data represents mostly white female teachers working in mid-size to large districts, so it would be valuable to collect new data from other types of districts and a more diverse teacher population. The fact that the data was collected a decade ago may pose limitations to its ongoing relevance; during the period under study (2010-2013), many schools were transitioning toward Common Core-aligned instruction in mathematics but yet lacked high-quality curriculum materials for doing so. That said, research in education reform has long attested to the fact that teaching practices have remained relatively constant over time (Cuban, 1993; Cohen and Mehta, 2017) and that there are strong socio-cultural pressures that maintain the instructional status quo (Cohen, 1988). In general, it is important to carefully validate measures built on the NCTE data on a new domain to ensure that it is representative of the target population.
Limitations of linked data. Education research has attested the limitations of standardized assessments in capturing student learning and reasoning (Sussman and Wilson, 2019). Student questionnaires in the NCTE data can provide an alternative perspective on students’ experiences and mathematics outcomes but these responses have a lot of missing values, and hence it may not provide robust estimates. Furthermore, understanding equity in instruction is a high priority for our research team and for the field more generally. However, studying equity within this data is challenging, since student speakers are not linked to administrative files containing student background and achievement variables. That said, such speaker-level demographic data is rarely available in instructional contexts, for important ethical reasons, and thus this limitation may encourage researchers to develop measures of instructional equity that leverage classroom-level, instead of speaker-level demographic information."
2818,"Future work on automated question generation for learning contexts could benefit from a number of potential research paths. In this paper, we tested three different architectures - but there are many more to be considered including those that incorporate knowledge graphs which have been shown to improve the richness and semantic correctness of generated questions (Bi et al., 2020). There is also room to explore different prompt strategies including a fill-in-the-blank approach which may be more appropriate fo the TQA data. For the attribute models, we used the single task objective of question generation, but it would be worthwhile to explore jointly generating the question attribute and the question itself. Additionally, document level Abstract Meaning Representations with resolved coreferences has been shown to improve
the quality of knowledge based question generation (Kapanipathi et al., 2021). We also recognize that we focused on different context for the input, but not on the wide variety of generation strategies available for this task. On top of the variety of model architectures, we would like to evaluate a greater set of corpora that include additional topics such as history and economics. Reading comprehension is critical to these fields as well and there is limited, if any, research on question generation for these topics.
Additionally, in future work we will conduct evaluation with expert annotators to incorporate into more complex models. Ideally, we will have educators and students assess the output of our models for factual correctness, relevance, and fluency of the questions generated. This output can then be used to train an instruction fine-tuned model. In order to make a solution that is viable for the classroom, it is critical to think beyond the automated metrics and get real teacher feedback. This preliminary research demonstrates the potential for expanding automated question generation to multiple classroom subjects and the value of incorporating discourse information into different model architectures to produce high quality questions."
2819,"As noted in the evaluation section, our system does not perform perfectly in multiple-choice question generation, particularly when it comes to generating distracting options, even with the powerful ChatGPT. In the next step, we can adopt an opensource framework of LLMs and fine-tune a domainspecific model using the extensive educational materials provided by middle school teachers. This way, the question generation ability may be improved, and we will not need to rely on the OpenAI API.
On the other hand, although extensive evaluations have been conducted, they only involve a small fraction of teachers and students in a preinterview setting. Once our system is widely deployed, a larger amount of user feedback will be collected and analyzed to monitor its effectiveness."
2820,"This section discusses the limitations related to the evaluation process and potential ethical considerations associated with the use of ChatGPT or similar language models in educational settings.
Human evaluation Our evaluation is conducted with a limited sample size of two teachers. Future work should aim to include a larger and diverse sample of teachers to capture a wider range of perspectives. This would help tease apart the potential teacher biases from generalizable claims about the feedback quality.
Ethical considerations The use of language models like ChatGPT in educational contexts war-
rants careful examination. For example, because the model relies on transcribed speech and is trained on primarily English, it might misinterpret the transcriptions of teachers or students who do not speak English fluently. Additionally, deploying language models in education settings raises concerns regarding privacy and data security. For example, the raw classroom transcripts should not be directly fed into the model to provide feedback as it may contain personally identifiable information about students. Guardrails should be set to prevent classroom data from being sent directly to external companies."
2821,"Our analyses were based around one metric of uniform DIF, z. The benefits of z are that it is commonly used in practice, it is highly interpretable with well-established effect sizes, and it is easy to aggregate across items and focal groups. One of the drawbacks, however, is that it does not capture non-uniform DIF, and it is not ideal in terms of statistical power (Woods et al., 2013).
Consistent with other analyses of DIF, our study struggles to identify sources of DIF (Zumbo, 2007).
Although it is outside the scope of this study, a finegrained analysis of examinees’ language, especially based on L1, could provide insight. Additionally, it could be beneficial to explore the possibility of modifying BERT using debiasing techniques (Sun et al., 2019). These techniques could potentially reveal sources of DIF and reduce DIF. Follow-up analyses along these lines of inquiry may be found in Kwako (2023)."
2822,"This paper presents a dataset of expert-curated Socratic conversations where instructors assist novice programmers in fixing buggy solutions to simple computational problems. The dataset serves as a benchmark for evaluating the Socratic debugging capabilities of LMs. While GPT-4 outperforms GPT-3.5, its precision, and recall remain below human expert levels (70.0), highlighting the need for further research. We find that GPT-family language models may generate repetitive and irrelevant Socratic utterances that could mislead learners. The utterances may also appear too early in the conversation, causing confusion, and can be overly direct, potentially diminishing learning outcomes. Study limitations include: The automatic metrics are limited in capturing the correctness, helpfulness, and relevance of a Socratic utterance, and the benchmark dataset may not represent all common novice misconceptions. Moreover, the manual evaluation is limited to 5 dialogues and could be expanded, but this process is highly time-consuming."
2823,"It is essential to address the ethical limitations observed our fine-tuned OPT model, ranked 4th in the competition. The model card provided by Meta AI highlighted that the training data used for their model consisted of unfiltered internet content, leading to the presence of significant biases within the model. These ethical considerations raise concerns regarding fairness, inclusivity, and potential biases in the responses generated by the model. Further research and development in addressing these limitations are imperative to ensure the responsible and unbiased deployment of chatbot models."
2824,"limitations of large language models as tutoring systems, we hope our work will prevent the premature use of these technologies."
2825,limitation of the presented evaluations. future work will therefore need to determine whether the errors that learners make in half-open exercises are also good distractors for sc exercises or whether learners instantly perceive them as incorrect when contrasted against the correct option. it is also yet unclear to what extent the most frequent misconceptions differ between and within learners over extended periods of time.
2826,"limitations this study reports early results and was based on a limited cohort of students from one master’s course. the generalizability of these results is therefore subject to certain limitations. follow on studies will test the system with larger samples and different disciplines to add weight to any significance of the results. notwithstanding the relatively limited sample, this work offers valuable insights into how a video-based student support platform that uses knowledge tracing improved graduate students’ perception of their learning experience. as the students finish their course, we will collect additional quantitative data in the form of final student grades. these will be compared between students who used the system and those who did not. we will also compare the final grades of students who used the system between the course in which they used it and other courses in which they did not. it also remains unclear what influence the course content has on the students’ experience. to examine this element further, the system should be tested on multiple courses imparted by different lecturers, and in varying subject fields. there may be an inherent positive opinion of aipowered technologies by students at an ai university. to test that hypothesis, the system should be provided to students at other universities in subject areas not related to ai. several questions, however, remain to be answered. further research should be undertaken to test several hypotheses, for instance, whether perceived ease of use (peou) and perceived usefulness (pu) would predict the attitude towards usage (atu) of orbits."
2827,"limitation of this study is that we used only the squad dataset in our experiments. the squad dataset has often been criticized because it is overly dependent on the similarity of question/answer sentences rather than on human-type reasoning, meaning it requires only superficial read- ing skills. thus, examining the effectiveness of our proposed method by applying it to various other datasets will be an important future task. furthermore, in the human evaluation experiment presented in section 6.4, we examined only 60 question–answer pairs generated through the proposed model from ten randomly selected reading passages. the relatively small scale of the experiment is due to the high workload required for people to carefully evaluate the various properties of a large number of questions. however, in the future, we aim to conduct a larger-scale human evaluation in order to increase the reliability of the experimental results. although the present study used only five qa systems, the use of a larger number of qa systems with different characteristics is expected to improve the accuracy of question-difficulty estimation and provide difficulty estimates with finer granularity. therefore, examining the effects of increasing the number and variability of qa systems will be another future direction of this research. we also need to confirm in greater detail whether qa systems can be substituted for human learners. a comparison between irt-based question difficulties calibrated from the responses of qa systems as well as human learners might be a plausible approach. another future goal is to develop a method of transforming the scale of the irt-based difficulty, estimated based on qa systems, into a scale appropriate for a population of target learners. such a scaling adjustment is expected to be achievable by using equating, which is a well-established technique in irt. furthermore, our qg method is easily extended to adaptive qg systems based on the framework of computerized adaptive testing, as mentioned in section 5.3. developing and evaluating such an adaptive system using our qg method will also be our focus in future work."
2828,"limitations as noted, this work is limited in that it does not address neopronouns. we speculate that the augmentation techniques deployed in this work may extend to these pronouns as well, we recognize that they do not have the same linguistic reality as he/she/they pronouns. neopronouns may be similar to singular they in being relatively infrequent in a naturalistic corpus, but they are also different in that they don’t overlap with a frequent morphologically-identical paradigm like plural they. additionally, the singular they augmentation technique we propose is specific to english and distributional facts about english pronouns. for one, english singular they morphologically overlaps with a plural pronoun, which is the primary motivation for using coreference information to identify contexts where they would have a primarily singular interpretation. this is often not the case for other languages, as in swedish where the gender-neutral hen is functionally similar to singular they but morphologically and distributionally dissimilar in that it does not overlap with a plural pronoun (gustafsson sendén et al., 2015)."
2829,"limitations accessibility we have tried to develop readalong studio web app with accessibility in mind, using accessible colour contrasts, ensuring buttons have aria-labels, and ensuring that the website is legible when zoomed-in to 200%, among other considerations. using google pagespeed insights, our website scores 89 for accessibility, but we recognize that there are still improvements to be made; specifically, we would like to perform an audit of the website with respect to web content accessibility guidelines (wcag). inexact transcription readalong studio will work best if the transcription is exact; that is, if there are as few discrepancies between the text and audio as possible. if extraneous text exists (such as page numbers, chapter titles, or translations), or if the audio includes un-transcribed speech (such as false starts), these errors will accumulate and can result in poor alignments. the extent to which these discrepancies affect the final result depends on the length of the recording to be aligned. in practice, we have found that readalong studio is able to recover from minor transcription errors when the speech data to be aligned are around 5 minutes or less in length. we have successfully aligned much longer (up to 40 minute) files, but “your mileage may vary” depending on the exactness of the transcription, the language’s orthography, and the type of data used. singing several teachers have successfully aligned songs with the corresponding text using readalong studio. for such an alignment to be successful, however, it is necessary that the sung words be vocalized clearly, and not be drowned out by the accompanying music (if any). extended legato singing (e.g., where one syllable is extended across multiple notes) can also cause poor alignments, since the speech-trained acoustic model does not expect single syllables to correspond to multiple intensity peaks in this way. language support the software works with most languages out-of-the-box. as mentioned in §2.1, readalong studio comes with support for 39 languages built-in, and handles other languages with a rough, best-guess g2p based on unicode table information. at several international workshops (§3), we found that it worked reasonably well with every language brought by workshop participants, even those with unique alphabets like western armenian or korean. however, not every language will work equally well. it will typically work well in languages with systematic orthographies that use letters in cross-linguistically common ways. we anticipate difficulty with orthographies that use familiar letters in cross-linguistically unusual ways, such as “font-encodings” (pine and turin, 2018), abjads that leave out many vowels, and languages like japanese where the pronunciation of logographs is highly variable and determined by context. just like a human could not simply guess the missing vowels in written hebrew without knowing hebrew, the software will not be able to do this either. additionally, the software is limited to languages which are both written and spoken—we do not support signed languages since the aligner requires audio to align with text, and the tool is fundamentally inapplicable to unwritten languages. the interface itself is currently only translated in english, french, and spanish, limiting potential users who do not speak one of those languages. numbers and symbols while readalong studio can do rough zero-shot g2p for most alphabetic and syllabic writing systems, it is not capable of general text normalization—while it can guess that “t” might be pronounced [t] in an unfamiliar language, it simply has no basis to guess any particular pronunciation for “634”, as this task is not only language-dependent but highly variable within any given language (bigi, 2011). therefore, all input must be “spelled out” for alignment to succeed. if the input contains numbers or symbols, readalong studio web app will prompt the user with a warning that it found uninterpretable symbols.18"
2830,"limitations of the knn system with regards to making new corrections. limitations the three base models used for the experiments were trained with different settings. as a result, it is challenging to understand the exact source of discrepancies between the results. additionally, each of the three models used different subword tokenizations, resulting in variable datastore sizes. although we have some hypotheses about why knn affects gec differently from mt, more experiments need to be conducted to confirm them."
2831,"limitations our work has several limitations. in terms of method generalization, the proposed method depends on multi-lingual neural machine translation models to generate trans-lingual definitions, and hence limits its application scope to those languages rarely supported by translation models. moreover, our findings are based on three languages, but different families of languages may exhibit distinct phenomenon that even challenges our"
2832,limitations and alternative approaches. such conceptual and theoretical
2833,"limitations, future research could explore ways to combine the strengths of humans and language models to produce even more accurate and informative explanations. therefore, chatgpt has the potential to assist teachers and other professionals in the creation of high-quality assessment items through a well-designed prompt, which can help ensure that items have a single correct answer, independent options, non-overlapping options, and plausible options. furthermore, chatgpt ability to classify items based on ecd principles is promising, but further research is needed. for example, the evidences could be provided to language models and ask them to classify each item in one of them. also, they could be asked to create the options and the respective explanations based on some kind of guidelines such as the one cited."
2834,"limitations that should be addressed in future work. the small sample size we collected makes it difficult to assess the generalizability of our findings. this also prevented us from running any analysis of internal structure or differential item functioning (dif) using methods from factor analysis or item response theory, as these models require large sample sizes (min and aryadoust, 2021). as items generated by gpt-3 should contain no dif and have similar factor structures as items written by humans, these are important analyses to explore in future work. we also did not examine the diversity of the generated items, in other words, how thoroughly the model explored the construct space. it is a well-known problem in psychometrics that having too many similarly worded items can inflate the reliability and reduce the validity of a measure (clark and watson, 1995), and our results may have been susceptible to this. a related problem is ensuring that the distribution of labels in the generated items remains balanced, and while we took steps to account for this, we did find that the distribution of gpt-3 items was somewhat unbalanced. for example, there were far fewer neutral items than either entailment or contradiction. improving the prompt design to account for diversity and other psychometric properties simultaneously is a fruitful direction for future work. our experiment with gpt-4, while disappointing, was also quite limited and should be expanded upon. we deliberately kept the prompt design as similar as possible between the two models, to avoid possible confounds. making effective use of the system query and changing the structure of the prompts to suit a conversational style could lead to much better results, however. finally, although we believe nli is a good task to use for initial experimentation, we also acknowledge that it is significantly different from the tasks of interest in education (e.g., question answering), and future work should explore our approach on tasks with stronger educational applications. llms have the potential to greatly ease the burden of scale development, and transform educational and psychological measurement. our results contribute to the growing field of llm-based automated item generation, and demonstrate the potential these methods have for generating valid and reliable items at a scale that would have previously been impossible. further research, combining our approach with more advanced prompting strategies, or zero-shot parameter estimation, could conceivably lead to a system that generates high-quality items in a fully autonomous fashion, which would transform the practice of writing and validating test items. limitations we emphasize that our research is exploratory and the generated items we produced should not be used for making critical evaluations of cognitive skillsets in either humans or llms. as discussed in section 5, our small sample size makes it difficult to draw broad"
2835,"limitations our task setting and baseline system requires that the input is already segmented into words including mwes. the mwe identification step in the construction process of our dataset involved timeconsuming manual annotation. building a highquality system that fully automates the process is an issue for future work. our dataset can be used to evaluate such a japanese mwe identification system. additionally, as shown in section 5, our baseline model performed relatively poorly in the higher complexity tiers. this is an effect of the dense annotation setting; it results in uneven distributions of complexity as shown in figure 2, where easy words greatly outnumber difficult words. one possible solution would be creating another lcp dataset using sparse annotation, where target words are selected using frequency bands so that the words are distributed across a wide range of frequency (shardlow et al., 2022). our data could provide insights as to what kind of words should be targeted by sparse annotation for such a dataset."
2836,"limitations. for example, none of the models (not even the multilingual one) give a special treatment to cognates (sets of words in one of the two languages that have been inherited in direct descent from the other one) which are normally considered easier to acquire. another concern has been the lack of transparency in the classifier’s predictions, a direct consequence of the dense representations we favoured over the more interpretable linguistic features. finally, a contextual classifier may predict different levels for occurrences of the same lexeme in different contexts. those limitations underline the need for human validation of the output of such systems."
2837,"limitations the limitations of the findings in experiments 1 and 2 have to do with the relatively small scale of the study. we experimented with two books and, while the findings were broadly consistent, it could be that results would not generalize to other books. experiment 2 was conducted with a specific group of readers in a specific context of implementation; studies with additional groups of readers are needed to evaluate generalization of the findings. another limitation of our experiments is that the dynamic model of lexical experience is evaluated only as an aggregate index per passage and not as a predictor for specific words or types of words. in particular, the model predicts a slight increase in surprisal of function words if their density in the story is generally lower than in the background corpus. this assumption may or may not be correct; further experimentation is necessary to evaluate the surprisal model in more detail. we thank a bea reviewer for pointing out this limitation."
2838,"limitations we identify two limitations of the current work and make suggestions for future directions. first, while our proposed method is language-agnostic in principle, our evaluation is limited to our french benchmark dataset. expanding our approach to encompass other languages would bring new and interesting challenges for further investigation. second, despite topic diversity within our exercise documents (e.g., the first example in fig. 1 consists of independent sentences, while the second is a coherent text centered around the same topic.), it would be interesting to quantify the degree of topical bias introduced during our training process and its impact on our binary task evaluation. for future work, we first aim to adapt seq2seq models for our task particularly text-to-text models such as t5 (raffel et al., 2020). there is also potential to explore different prompting strategies for large language models (llms), when generating gap-filling grammar exercises. for instance, the utilization of chainof-thought prompting (wei et al., 2022), which involves generating intermediate steps before producing the final response, could be explored for generating grammar exercises. additionally, an interesting future study would involve investigating the number of example demonstrations that llms require in order to accurately mimic example gap exercises."
2839,"limitations of manual classroom observation and provide scalable, automated feedback on instructional practice. while our results reveal that chatgpt has room for improvement in generating insightful and novel feedback for teaching, our proposed tasks and evaluation process provide a foundation for future research to address the challenges of teacher coaching using nlp. our work underscores the challenge and importance of generating helpful feedback for teacher coaching. moving forward, we propose several directions for further research, such as improved prompting methods and reinforcement learning with feedback from coaches. ultimately, we envision a future where generative ai can play a crucial role in supporting effective teacher education and professional development, leading to improved outcomes for students."
2840,"limitations by building this task-specific dialogue system for kids, we aim to increase the overall quality of basic math education and learning at-home experiences for younger children. in our previous school deployments, the overall cost of the whole school/classroom setup, including the wall/ceilingmounted projector, 3d/rgb-d cameras, lidar sensor, wireless lavalier microphones, servers, etc., can be considered as a limitation for public schools and disadvantaged populations. when we shifted our focus to home learning usages after the covid19 pandemic, we simplified the overall setup for 1:1 learning with a pc laptop with a built-in camera, a depth camera on a tripod, a lapel mic, and a playmat with cubes and sticks. however, even this minimal instrumentation suitable for home setup can be a limitation for kids with lower socioeconomic status. moreover, the dataset size of our initial home deployment data collected from 12 kids in 12 sessions is relatively small, with around 12 hours of audio data manually transcribed and annotated. collecting multimodal data at authentic homes of individual kids within our target age group (e.g., 5- to-8 years old) and labor-intensive labeling process is challenging and costly. to overcome these data scarcity limitations and develop dialogue systems for kids with such small-data regimes, we had to rely on transfer learning approaches as much as possible. however, the dataset sizes affect the generalizability of our explorations, the reliability of some results, and ultimately the robustness of our multimodal dialogue system for deployments with kids in the real world."
2841,"limitations include: the automatic metrics are limited in capturing the correctness, helpfulness, and relevance of a socratic utterance, and the benchmark dataset may not represent all common novice misconceptions. moreover, the manual evaluation is limited to 5 dialogues and could be expanded, but this process is highly time-consuming."
2842,"limitations in generating pedagogically sound responses consistently. we then fine-tuned gpt-2 and dialogpt on the tscc dataset and evaluated their performance using bertscore and dialogrpt metrics. we also proposed an approach using rl to optimize directly for pedagogical values. we hypothesized that several dataset characteristics (e.g., dialog completeness, sampling) pose challenges to achieving superior performance with fine-tuning. to this end, we recommend the extension of the dataset to include longer prompts with extended context. finally, we also draw attention to the need for more domain-specific metrics (in both evaluation and reward-based training) in enabling the generation of accurate, context-aware, and factually correct teacher responses."
2843,"limitations a limitation of our approach is that it relies heavily on the quality and relevance of the prompts used. the prompts were engineered based on observations made in the training data and this approach may not work if the prompts are not representative of the corpus. finally, our approach may not be suitable for all types of teacher-student dialogues and may require modifications for different contexts or domains. one possible concern with the techniques mentioned in this paper is the limited reproducibility of openai’s language models, such as gpt-3.5turbo. the weights of these models are proprietary and not publicly accessible, which makes it challenging to replicate the findings of earlier research or expand on them. ethical considerations ai-generated teacher utterances may contain bias, which may become apparent particularly in exercises or chit-chat. in this project, we took steps to avoid profanity in the ai-generated responses, but similar protection against bias should be put into place. additionally, human evaluators should be used to assess the quality of the ai-generated responses and to identify any potential biases. we recognize that language models like gpt3.5-turbo are trained on large datasets that reflect the biases and prejudices present in society. as there is always a risk of perpetuating these biases when using generative ai for dialogue systems it is important to evaluate the ai-generated responses for potential biases and to take steps to correct them."
2844,"limitations, it is challenging to organize a shared task during which any possible number of submissions can in principle be evaluated with adequately remunerated human evaluations. what is more, data is very important in the context of real-world applications and shared tasks. although the corpus used in this shared task is a valuable resource in our domain, some particularities of this corpus and the data sampling method also had an undeniable impact on the results. therefore, in future editions of this shared task we should rethink some of the current potential limitations, such as the fact that the dialogues had to be limited to 100 tokens, resulting in partial conversations; the fact that some dialogues, if extracted from the data randomly might have led to data leakage; and the fact that the dialogues did not always follow strictly role-alternating format, with some teacher turns being preceded by previous teacher utterances, rather than a student utterances. in summary, the field of education has already been significantly changed by llms, whose capabilities keep improving constantly. we hope that this shared task will serve to help the scientific community better understand the current capabilities of llms in educational contexts. having learned from this shared task and going forward, we hope to make its future iterations even more informative."
2845,limitations that should be addressed in the future.
2846,"limitation in relying solely on automatic evaluation metrics (as detailed in section 5.1) prompt engineering to adapt language and tone in tutoring systems our experiments reveal an intriguing finding where manipulating the prompt influences the tone and language of the generated response, presenting an opportunity for tutoring systems to potentially adapt to the students’ learning styles and/or teaching goals. further research should delve into teaching instruction methods, potentially exploring the pedagogy of constructivist learning (graesser et al., 2005) or engaging students in ill-structured exercises for productive failure (kapur, 2008) using llms of this nature. gpt-3’s robust handling of errors and noncanonical form of language during the data preparation phase, a manual inspection of the data revealed the presence of grammatical and spelling errors in some utterances. additionally, since the dataset originated from chatroom text-based conversations, there were instances where mathematical symbols were used instead of natural language, such as this example utterance output teacher: but e.g. pleased with their visit = good idea. it is worth noting that we did not employ any nlp processing toolkit to correct these errors or non-canonical forms in the dialogue utterances. however, despite this, the gpt-3 model could still generate appropriate responses effectively. llms’ potential in multilingual settings in the context of l2 acquisition, the dialogue nature in caines et al. (2020) provides valuable opportunities for tutors to adapt to students’ native languages. code-switching strategies as such have been found to enhance teaching, including the explanation of concepts (köppe and meisel, 1995), and leveraging ai tutoring systems can facilitate this process. llms possess multilingual capabilities that enable them to address language barriers, accommodate low-resource languages, and exhibit promising performance even on unseen languages (yong et al., 2022). to enhance accessibility, the development and adoption of open-source multilingual models, such as bloom (scao et al., 2022), should be encouraged, thereby facilitating the utilization of llms in educational applications across diverse linguistic contexts."
2847,"When a dictionary for a low-resource language lacks a word, but has several related ones in terms of synonymy or semantic similarity, it is a definite benefit to be able to provide those to the dictionary user instead of merely saying, “No results found.” However, there are some potential drawbacks here: for example, this could increase the rate at which words acquire connotations by analogy with English. ‘Locomotive’ and ‘train’ are closely related concepts in English; but that does not necessarily hold for every language, and there is some risk in implying that it does.
Language instructors will be all too familiar with students using tools like Google Translate to do their homework for them instead of doing the hard work of learning the language. On a larger scale, Google Translate itself was formerly available as a free service that software developers could use to do automated machine translation in bulk; this was abruptly discontinued in 2011. Industry rumour7 held that the bulk service was being used to generate so much of the parallel text appearing on the internet—parallel text needed to train machine translation models—that those models could no longer improve sufficiently if they continued to inadvertently be fed primarily their own outputs. This highlights the possible risk that applying machine learning tools like word embeddings can end up distorting language. To this end, we believe that the use of word embeddings to provide analogous words to dictionary users is beneficial, but does not and cannot replace actual lexicography.
7https://kv-emptypages.blogspot.com/2011/06/analysisof-shutdown-announcements-of.html"
2848,"limitations one limitation of this work is the lack of a more comprehensive study of langid methods, which could impact slightly the results. another limitation is the number of non-bil languages, which can be increased to more than 1,000 languages with the datasets proposed in (brown, 2014). furthermore, the use of wikipedia data limits the search of samples, since all pages are supposedly written in portuguese. so, relying on a broader set can bring a more realistic estimate on the in-the-wild search for data. in addition, a major limitation of this work is the lack of inspection of the results with native speakers. we are already engaging with one mbya guarani community, but it is quite difficult to extend such engagement to other communities."
2849,"limitations the fst does not yet have an extensive coverage of the lushootseed vocabulary, so it does not work on all domains of text. also, writing an fst takes a lot of time and requires special knowledge of the language. the neural model is limited to nouns only, but it can work on out-of-vocabulary words unlike the fst, however, we have only tested its accuracy using the words that are known to the fst, which means that words that follow very different inflection patterns will, most likely, not be analyzed correctly. furthermore, the neural model was not trained on derivational morphology, which means that word derivations might also result in erroneous predictions."
2850,"limitations although improving the performance of our embedding methods is desirable, the most apparent limitation of our work is not the overall quality of representations produced, but rather the range of words our methodologies can be applied to. currently, our methodology can only be used to construct word embeddings for dictionary headwords. this represents a considerable limitation, as wolastoqey and mi’kmaq are both polysyntheic languages, in which speakers often build new words by creatively combining roots. as this is the case, no dictionary is expected to contain definitions for all word-forms of these languages. because of this, future work is required to extend our approach to construct embeddings for words that do not appear in a bilingual dictionary."
2851,"limitations this work focuses on using computational tools to determine dialect based on a small quantity of writings of a spoken language, using a writing system that was adapted recently, rather than one that evolved alongside the language for thousands of years. this limitation in orthography leads to differences in character usage, frequently between dialects (which is helpful for this problem), but there is also variation also within dialects depend- ing on the author. for example, different writers will use different methods of transcribing a nasal sound; eastern tends to use nh for nasal sounds in the middle of a word, although some writers will use a capital n. southwestern anishinaabemowin tends to use ny, ns or nz for these same nasal sounds within words. as noted by valentine (2001b), there are variations in language within dialects, including age-stratified language proficiency, where older speakers tend to be more fluent than younger ones, largely due to differences in opportunities to learn the language. these differences might be detected and interpreted as dialect differences if the diversity in writers is not comparable between the two sets of texts being compared. additionally, an individual’s word choice may change depending on their gender or occupation (valentine, 2001b), and having only a small sample of writings does not allow us to capture these differences well. to test how much our model is learning author preferences over dialects, using some writings from authors not included in the training data would provide some insight. future work could do a better job of tracking authorship between cross validation folds as well as sourcing from a wider set of writers. only small quantities of text were used for each dialect, which was limiting in terms of methods that could be used. the methods utilized in this paper could be easily applied to minority languages that do not have large quantities of written text available, of course, with permission from and in collaboration with indigenous language keepers. our future work could involve the collection of larger quantities of text, which would allow the use of a wider range of language analysis. ethical statement some of the texts used for our samples were transcribed aadizookaanan, a type of traditional story highly revered by ojibwe. these particular stories are not to be spoken out loud during non-winter months without snow on the ground. there are particular spiritual reasons for this, and unfortunate things can happen to individuals telling or hearing these stories when there is no nearby snow. therefore, we will not write out the stories here and we strongly encourage citation followers to heed precaution. for more information, bring your tobacco and questions to a trusted anishinaabe knowledge keeper. indigenous peoples have experienced a long his- tory of colonialism, including by well-meaning researchers. please remember that indigenous peoples must maintain sovereignty over their languages, traditional stories, and other knowledge. all research involving indigenous knowledge, including that for the development of generative ai, should be done ethically in reciprocal relationships with indigenous peoples. the research should also meet their needs and wants, as described by the given indigenous peoples (smith, 2021)."
2852,"limitations in this work we explored transfer learning approaches only by using pretrained word embeddings. transfer learning should be explored further. some of the segmentation methods have their own hyperparameters which are usually obtained for high-resourced languages and might be suboptimal in our case. these hyperparameters should be systematically explored. finally, token-free pretrained models fine-tuned on our data should be investigated. it is costly and difficult to acquire human translations, due to the limited number of speakers and exclusive lrl communities; moreover, the fact that we are not wayúunaiki speakers limited our qualitative assessment."
2853,"limitations. – mbart50 - is a multilingual sequenceto-sequence model pre-trained using the multilingual denoising pre-training objective (tang et al., 2020). hyper-parameters - for the transformer approach we tokenized the source and target parallel sentences into subword tokens using byte pair encoding (bpe) (gage, 1994). the bpe representation was chosen in order to remove vocabulary overlap during dataset combinations. for other approaches we applied the tokenizer of each model, table 3 shows hyper-parameters used in our baseline experiments."
2854,"limitation indicates that the transformer model’s performance is highly dependent on the availability of extensive parallel corpora for effective machine translation. on the other hand, the transfer learning approach showed more promising results for low-resource indigenous languages. we observed that models pretrained on high-resource languages, such as spanish, and fine-tuned on the indigenous languages improved translation quality. however, even with transfer learning, the performance was not satisfactory, and there were errors that persisted across all three approaches. the general error that all three approaches failed to address adequately was the translation of domain-specific and culturally specific terms in mazatec and mixtec. these languages have unique vocabulary and cultural nuances that require a deeper understanding and context to ensure accurate translation. the limited availability of domain-specific parallel corpora for these languages hampered the models’ ability to capture and translate such terms effectively."
2855,"limitations this paper’s aim is to give an introduction to researchers, students, of interested community indigenous community members to the topic of machine translation for indigenous languages of the americas. therefore, this paper is not an in-depth survey of the literature on indigenous languages nor a more technical survey of low-resource machine translation. we would point the reader to more specific surveys on these aspects (haddow et al., 2022; mager et al., 2018b). ethical statement we could not find any specific ethical issue for this paper or potential danger. nevertheless, we want to point to the reader that working with indigenous languages (in this case, mt) implies a set of ethical questions that are important to handle. for a deeper understanding of the matter, we suggest specialized literature to the reader (mager et al., 2023; bird, 2020; schwartz, 2022)."
2856,"limitations one major limitation of the current methodology is its predication on the existence of a recorded lexicon in some form that can be ported into an online format. this approach may not be ideal as a framework for the development of a new dictionary, as its express goal is to increase accessibility to existing resources and facilitate the expansion of those resources. additionally, the development of a bespoke, dependency-free web-application for showcasing existing resources is likely low on the list of viable strategies for a community-led reclamation effort, especially if there are no community members already familiar with web-development. any replication of the work described here is more suited for a group with a set of existing resources, access to developers, and a need to quickly get those resources into the hands of community members. the other primary limitation of this tool and its development methodology in its current form is the potential lack of accessibility in the types of communities for which it is intended. despite the increase in access to internet-capable smart devices in communities such as that on st. lawrence island, in many such places, the availability of reliable wireless internet access remains relatively low. users are often forced to use expensive cellular data plans to conduct any amount of web-browsing. while the ultimate vision for this dictionary is for it to be packaged in a downloadable, offline format, replications of the tool in its current live-web implementation may leave many people unable to use it with the frequency that they would like. though this is certainly a limiting factor in the effectiveness of this tool, the deployment of an offline version of the dictionary remains a preeminent goal of the project. akuzipik dictionary website https://bhunt6.github.io/ akuzipigestun-sangaawa akuzipik dictionary code https://github.com/bhunt6/ akuzipigestun-sangaawa"
2857,"Since hypothesis sentences were created and labeled by medical experts, the size of our current dataset is small. In particular, the number of examples of contradiction is small because the hypothesis sentences were created based on captions to efficiently construct our dataset. However, we can increase the number of examples of contradiction by rewriting phrases in the hypothesis sentences. The claim of this study is that we can relatively efficiently create a VTE dataset in the medical domain from the existing image caption dataset, and can empirically demonstrate the challenges of current vision-and-language models on the VTE dataset. Although increasing the data size is an important next step, it is beyond the scope of this paper."
2858,"In our study, we tested the chatbot on the Hindi database, which humans heavily annotated. Thus, when the database size is enormous, the scalabil-
ity of the annotation approach is a critical question. Since the questions and answers could be possible in different languages, it will require considerable effort to translate them and, at the same time, preserve their context. In our study, we observed the success ratio of the developed chatbot to be 70% for Hindi queries. However, it is not indicative of its performance in different natural languages. For a given user query (q), the performance of our best approach for the FAQ-retrieval system is highly dependent on the number of different relevant questions (Q) existing in our ASHAFAQ database for the given q. Considering the large number of user queries that can be asked in the healthcare field, the small size of our ASHAFAQ database is a significant reason behind the instances where our method fails to suggest relevant questions (Q) to the user. Moreover, our work does not analyze the quality of answers present in the ASHA-FAQ database. Hence, a user study would be required to analyze the questions’ diversity and the answers’ quality in our ASHA-FAQ database."
2859,"The limitation of this work was the use of ROUGE-L as the evaluation metric. Given the many acronyms and synonyms in medical writing, ROUGE-L, based on the longest common sequence, does not capture the many nuances in its score. Researchers have shown concerns for the ROUGE score and have developed metrics for summarization that are more semantically aware of the ground truth (Akter et al., 2022), but their usability is yet to be validated.
Training large language models from scratch uses a considerable amount of carbon footprint. (Patterson et al., 2021) Fine-tuning large language models for downstream tasks is one way to reduce carbon footprint but still needs to be cost-effective. As the AI community progresses in this field, developing a cost-effective and carbon-friendly solution is needed. The NLP field is moving towards prompt-based methods with larger LLMs (Lester et al., 2021), so the next step for this research is to experiment with soft prompting approaches to address low resource settings and leverage prompt tuning in LLMs for the problem summarization task."
2860,"A limitation in this study is we only used opensource models. We were unable to evaluate ChatGPT, for example, because the data use agreements under which these datasets are made available forbid sending the data to outside APIs. Other models are frequently being released and we did not exhaustively test all publicly available language models. However, the focus of the paper is not to find the best LLMs but instead providing insights into using LLMs to improve transferability."
2861,"There are a few limitations of this study: First, the patient sample size for the validation cohorts was limited to 954 patients from a local hospital. As annotation in the medical setting is expensive and time consuming, we only get patient level labels and cannot pay the effort for document level annotations. The size and diversity of the data sample could be improved by collecting clinical notes for patients from other hospitals in different age groups and of similar clinical complexity. We did not perform cross label check for the sampled patients, as there is a large number of uncertain patients, among those patients there are still ones who suffer from dementia but not diagnosed. Second, more statistical models can bee developed. At the moment we only tried a keyword based model and a deep neural models. Traditional statistical models like Logistic Regression with biomedical concept features can also be considered. Furthermore, our study would have benefited from more model interpretability and human error analysis on the classifier predictions. We have plans to extend our current work with the above mentioned directions."
2862,"We finetuned pre-trained NLI models for TDG parsing. Both data sets we used were in English. To apply this model to other languages and to get the best results, pre-trained NLI models or NLI data sets might be required for the new language. Templates to verbalize the temporal relations in the new language are also required.
The clinical data set (i.e. THYME) we used in this work only contains EHRs from one institution: Mayo Clinic. Clinicians from different hospitals can have different writing style or use different templates when writing the notes. Future work should test the TDG representation and parsers on EHRs from other institutions, and EHRs of different patient populations."
2863,"The experiments in this paper were performed using OpenAI’s GPT-3 API. While running locally does not require a large amount of computational resources, the server-side service cannot be easily replicated and requires a large amount of computational resources. Additionally, given the inherently restrictive nature of medical text, we can only evaluate our approach on a small corpus of English-language dialogues taken from the dataset of a single company’s medical service, which we cannot release due to privacy concerns. Finally, given summarization is a challenging task to evaluate, we rely on a small number of expert human annotators and automatic metrics. However, additional annotations may be helpful and it may also help to study and report labeler agreement when reporting human preferences."
2864,"The methods described in this paper do not leverage any external medical knowledge, a technique that has been shown to be effective by other studies (Joshi et al., 2020; Michalopoulos et al., 2022). And like other methods based on large language models, in theory, our models are also prone to hallucinations and omission of key-clinical concepts. We plan to explore constrained beam search4 as a mitigation strategy for addressing these challenges in the future.
Although the impact of the Task C model as a data augmentation tool is undoubtedly positive (Section 5), qualitative error analysis of patientdoctor conversations produced by the model showed that the output contained a small number of dialogue turns, and each individual turn was too long, packed with information. Producing conversations with a more natural flow should yield an even better boost on downstream tasks, and we leave exploring such methods to future experimentation. We also recognize that N-pass summarization for Task B with higher values of N should be able to cover the entirety of the input conversations in the Task B datasets, albeit with diminishing returns as N increases. We hope to evaluate them in future iterations of similar shared tasks."
2865,"Data. A challenge we faced was that MIMICIII was unevenly distributed across the races (e.g. ethnicities) for the patients represented. We had significantly more White and Black patients than any other race of people and even still many more White than Black patients. Therefore we continue to express the need for more representative, inclusive, and balanced datasets. Further, the dataset did include ethnic breakdowns, but due to the lack of patients present in those ethnic groups we could
not include Caribbean or Middle Eastern patients as well as many other subgroups in our analysis. We would like to use a more comprehensive dataset in the future, potentially from a facility that consistently services marginalized and privileged communities. If we had more time, we would like to partner with a medical facility that regularly serves marginalized and non-marginalized groups, steadily, to develop a dataset which captures more features that could reveal some bias and ensure they are more descriptive (i.e. has_insurance) to get higher quality data. Better Feature Selection and Using More Demographic Features. To ensure the quality of the aforementioned data, we will perform a causal analysis to identify the specific features that cause testimonial injustice. We anticipate that variables such as age and education level of patients need be included, as these factors have been shown to affect how patients are treated, particularly in the medical field (Dunsch et al., 2018; DeVoe et al., 2009). Fairness Metrics. Existing and popular, fairness metrics cannot be generalized to fit in settings where intersectionality must be considered. Another challenge we faced was having a lack of good baselines to use when analyzing intersectional differences. Intersectionality is highly unexplored, in the future we would like to develop our own metric which can be more beneficial in detecting intersectional disparate treatment between individuals. Additional Analysis. We plan to conduct additional analysis to understand if specific physicians treat similar patients similarly based on the intersection of their demographic features. Further, we plan to perform statistical significance testing on differences in how patients were treated based on the intersection of their demographic features and the occurrences of specific physicians’ use of testimonial unjust terms to other patients."
2866,"One drawback of our method is that given a maximum span length, we always miss longer spans. For example, the BC2GM and JNLPBA NER
datasets contain lengthy spans so we do not do as well on those tasks. Another drawback of our method is that it requires embedding the full corpus. One of our methods for making this tractable introduces another limitation - span filtering based on token types may discard spans that are useful to the user. Additionally, although we demonstrate that our method can be robust to training time (A.1), we have not explored principled methods for selecting the model checkpoint in supervised KRISS-Search, as the user does not label a validation set. Methods for making the process more rigorous should be explored, especially for out of distribution tasks."
2867,"We only explored one-shot prompting strategies with GPT-4. More examples (few-shot) may improve performance. We prompted GPT-4 with only a single randomly selected sample that included all of the annotated event types. Our post-processing included simple rules to process the generated output and may be improved. The quality of the sample and the selection method may influence performance. We explored two prompting styles. Future work could explore more prompting methods such as question & answering and chain-of-thought (Wei et al., 2023) and fine-tuning non-proprietary LLMs."
2868,"Some limitations of this work are listed below:
• The architecture and configuration for the custom PLM are the same as bsc-bio-ehr-es. Another architecture and configuration could obtain better results.
• The textual data come from just one provider. Using data from several providers could help with generalization.
• The custom PLM has not been compared with other PLMs in language tasks such as named entity recognition or question answering. This
comparison can help to understand if the custom PLM can outperform available PLMs in other types of tasks.
Ethics Statement
The ethical considerations of this work are related to the data that we used and the models we built. The data was extracted from administrative and clinical records from an insurance and health provider that specialized in labor accidents. Within this data, it is possible to find personal and sensitive information such as personal and company names, addresses, health information, pre-existing conditions, and diagnoses, among others. An anonymization process was not carried out since the model will be used for internal purposes and will not be released. As a process of memorization can occur in the PLM, we believe it is best to keep the model private because privacy attacks can extract personal and sensitive information.
We did not test the models for any bias under any protected field. Therefore, the trained models could benefit certain patients or accidents over others in the insurance decision. If a biased model is deployed in this provider’s systems, it could harm patients with their insurance coverage decisions."
2869,"Although our method delivers optimal results, it doesn’t comply with data protection regulations
like HIPAA, even though Azure offers a HIPAAcompliant option. From a privacy standpoint, deploying a local model such as LED might be preferable, but our findings indicate that further work is needed for this method to achieve satisfactory performance. Regardless, when creating automated conversation-generation systems, healthcare providers and developers must ensure that the entire system—including text-to-dialogue, data transmission and storage, and model inference—complies with privacy and security standards to maintain trust and avoid privacy breaches in clinical environments. Hence, developing an automated conversation generation system from clinical note entails several ethical considerations.
Firstly, obtaining informed consent is vital: patients should be informed about their recordings, and data ownership must be emphasized. Equitable access is also crucial; the system must accommodate patients from various backgrounds, including those with disabilities, limited technical literacy, or language barriers. Lastly, continuous evaluations are required to ensure that the system’s performance does not deteriorate and adversely affect the quality of care."
2870,"Considering the critical nature of the domain of the task, it is of paramount importance to ensure stability in the results expected from the model. Despite setting the temperature (T) as 0 for all decodings in our experiments, we observe the variance in the generated summaries across runs. Table 4 contains the results for three runs for Task A and Task B. The in-context examples for each sample and other parameters have been kept constant across these runs to identify the degree of stochasticity. Further, in-context learning has shown to be susceptible to changes in order of in-context examples (Lu et al., 2021), as well as the template of the examples (Shin et al., 2020). A more reliable process to generate the summaries along with identification of the optimal examples (template, order) is thus required. Additionally, due to the context limit of the GPT-4 model, evaluating the impact of natural language instructions in addition to the examples could not be performed."
2871,"limitations some potential limitations of this work include a relatively small sample size, which may limit the generalizability of the results. hindi is spoken differently across india, hence the translations made by the three translators may not be representative. study did not examine the potential impact of regional dialects or variations in hindi, on the accuracy of the diagnosis. finally, the study focused solely on the use of speech narratives and did not explore the other types of data, e.g., imaging or genetic data, which could be important for the diagnosis of ad."
2872,"limitations 1) on plm capability for transferring to new language, in this work, we used metaai’s wmt21 multilingual pre-trained language models as our test-base for the knowledge transfer into an external language fine-tuning and translation. this new-language ability is much dependent on the mplms we used, such as wmt21fb (tran et al., 2021) as a huge size model, a conditional generation from meta-ai’s massive m2m-100 model (fan et al., 2021). if we try to fine-tune a bilingual model on an external language that the plm did not see, it will not be that good because for smaller-sized models such fine-tuning would be too much of a change, and the model will lose generalisation which leads to problems. for huge multilingual plm models, the 250k of fine-tuning data is a small set of numbers, and that’s why the model does not lose generalisation and captures new data well without losing linguistic knowledge of other languages that it was trained on. 2) on the impact of language families, the mmplm wmt21fb we deployed has both alphabetic languages and cjk (chinese, japanese, koran) character languages, as well as slavic language (russian). this might make it easier to transfer to a new language, e.g. alphabetic language. however, in situations when the mplms did not include any of the language scripts that belong to the language family of the target one, it can be much harder for it to transfer to the new target language. this needs further investigation. one possible extension of this work is using the dynamic vocabulary method proposed by lakew et al. (2018)."
2873,"limitations. we employed human evaluation for assessing factual consistency, and this analysis has been conducted over a larger set of automatic metrics to provide a more comprehensive picture. furthermore, we demonstrate that further optimizing the model using reinforcement learn- ing (rl) with the metric as a reward can result in significant improvements in factual consistency. our contributions include a simple yet effective approach for two medical summarization tasks, validation of several automatic evaluation metrics for their correlation with expert-assessed factualness, and the identification of the best-correlating metric to guide generation models toward enhanced summary correctness. this work lays the foundation for the development of more robust clinical trial summarization systems, facilitating the efficient dissemination of medical knowledge to practitioners and researchers."
2874,"limitations our work does not cover the full range of domainagnostic pretraining objectives, including denoising objectives such as electra (clark et al. (2020)), or contrastive objectives, such as simcse (gao et al., 2021; rethmeier and augenstein, 2023). this paper focused on comparing the masked language modeling (mlm) objective with specially designed dialog-aware objectives. it is our expectation that, given the empirical findings of this project, task-agnostic general objectives like electra, or simcse, will also outperform dialog-aware methods. in addition, due to the lack of task-related datasets, the set of corpora used during our experiments is limited."
2875,"limitations to consider. firstly, minimal prompt engineering was used, and context could have been provided in the form of few-shot or chainof-thought examples, which have been shown to increase accuracy (wang et al., 2022; ye et al., 2023). strategies like self-consistency decoding (huang et al., 2022) and retrieval augmentation are also promising for healthcare where varying factual content of responses from each model even to the same prompt poses a clinical risk. additionally, we did not compare the llm responses to those of human experts, which is an important comparison for appropriateness and safety."
2876,limitations we did not have the chance to explore latest large models. the obvious future work would be on applying public instruct based models if the hardware capacity is enough or private instruct based models if the budget allows. more future work could be on preprocessing of the dialogues. intuitive postprocessing approaches could also be explored.
2877,"limitations our findings are limited to medication entities, the only semantic class that is annotated in all available corpora. moreover, we had to exclude bronco150 for long-span experiments due to a mismatch of entity definitions. although the label alignment decisions are somewhat subjective, they are made based on a thorough inspection of definitions and samples. the differences in annotation quality and biases may be playing an uncertain role in the models. however, making statements on the impact of the annotation quality is challenging, since each work followed a different annotation protocol and reports different measures of annotator agreement. this is another area where harmonization efforts might be warranted for future research. furthermore, exploring different hyperparameter configurations lied out of scope for our work, but could have a substantial impact. mainly, the results from the transformers comparison (table 6) could shed different"
2878,"limitation of this work is the small speech database used in this paper. we are actively building a detailed annotated large speech and language database from hundreds of patients with post-stroke aphasia, with the aim of training and developing asr for pathological speech. we expect that such work will promote greater confidence in the use of ai and specifically nlp for healthcare intervention."
2879,"limitations this method for designing and annotating clinical text for a specific clinical use case can be beneficial for researchers needing to annotate a corpus. however, there are some limitations. first, the experiences are based on a specific clinical case and focus on the qualitative aspects. details of certain parts of the design and annotation process will likely need to be adjusted based on resources available to other researchers. this can include the data selected for annotation, the number of annotators available, and the annotators’ level of expertise. for instance, the use case in the design process is based on using 8 annotators to annotate 100 synthetic ae notes over 5 sessions. second, expertise and additional time are required to generate synthetic notes for annotation. finally, future work is still needed to replicate the described design and annotation process on other forms of clinical text and problems. ethical considerations to protect patient privacy when designing and annotating clinical text, synthetic ae notes were manually generated and verified by a nurse to ensure the data is anonymized. additionally, the annotation guideline includes the sensitivity category to allow annotators to label potential information in the synthetic notes that could identify a patient. this process was described to provide an example for researchers who need to annotate sensitive data. the norwegian regional committees for medical and health research"
2880,"limitations biodlm adopts dlms as backbone models. compared to plms with other training objectives, dlms may miss language modeling benefits and squeeze representation space. besides, our benchmarks can be extended to more biomedical discriminative tasks, such as relation extraction, document classification, and entity disambiguation. we consider extending our exploration to more dlms and biomedical tasks as valuable future works."
2881,"limitations due to strict data protection regulations and a high annotation workload in the clinical domain, obtaining more diverse target tasks to validate our approach is a challenge. in this work, we focused on only two use cases in german clinical applications and need to extend our experiments to english or other non-english languages in the field. in addition, we need to conduct more experiments in future work in order to achieve a better balance between the amount of training data required for the source and target tasks."
2882,"limitations while the proposed methods offer promising results in improving the performance of knowledge graph-based language representation models, there are some limitations to this work that should be noted. firstly, the experiments were conducted on a limited number of datasets, and the results may not be generalized to other datasets or domains. therefore, further experiments are needed to validate the effectiveness of the proposed methods on a broader range of datasets and nlp tasks. secondly, the proposed methods require additional computation and may increase the complexity of the models. therefore, it is important to consider the trade-off between performance improvement and computational cost when applying these methods in real-world applications. lastly, while the proposed methods address some of the limitations of existing knowledge graph-based language representation models, they still may not capture all the contextual relationships and nuances of natural language, leading to potential semantic errors and reduced accuracy. therefore, it is essential to continue exploring new approaches and techniques to further improve the performance of nlp models."
2883,"limitations this paper is limited to the study of clinical ner models using an encoder-only architecture. the use of generative models with a zero-shot learning approach (hu et al., 2023) is another promising approach for low-resource languages that could be compared with clt and translation-based approaches in a future work. however such methods require a careful prompt selection strategy and cannot be directly compared to supervised models. this paper is also limited to cross-lingual transfer to french and german. ideally, this work could have included experiments with other target lan- guages and also other source languages than english, as yarmohammadi et al. (2021) do in their general-domain comparison of strategies for crosslingual transfer. however evaluation datasets are lacking for that purpose in the clinical domain. similarly, more general"
2884,"limitations the work presented in this paper is subject to a number of limitations which will be addressed in future work. firstly, we evaluate umls-kgi-bert on a very narrow range of tasks limited to token classification - a broader range of information extraction and reasoning tasks would be necessary for a more complete picture of the utility of our pretraining methods. in addition, we only train models for mid-to-high-resource languages; to properly validate the applicability of this approach, in particular the lessening of the need to rely on large training corpora, it will be necessary to train and evaluate such models in more low-resource settings. table 6: macro-f1 scores for the ablation experiments. dataset base model kg tasks cas-pos cas-sg quaero-medline essai-pos drbert-4gb - 90.84 62.20 66.66 94.69 ep+lp 91.59 64.85 66.08 94.62 ep+tc 90.86 62.11 66.75 94.88 tc+lp 92.01 65.98 66.89 94.41 all 92.04 66.22 67.15 94.50 ncbi-disease biored-ner jnlpba04 pubmedbert - 93.53 83.35 81.13 ep+lp 93.24 82.40 81.25 ep+tc 93.37 83.09 82.66 tc+lp 94.13 83.38 84.30 all 94.11 83.45 84.36 pharmaconer meddocan bioroberta-es - 81.11 91.84 ep+lp 81.12 91.86 ep+tc 82.40 91.80 tc+lp 83.22 91.71 all 83.46 91.77"
2885,"limitations evaluation of generated text is difficult evaluating automatically generated text, including clinical notes, is generally hard due to the inherently subjective nature of many aspects of output quality. automatic evaluation metrics such as rouge and bertscore are imperfect (deutsch et al., 2022) and may not correlate with aspects of expert judgment. however, they are frequently used to evaluate model-generated clinical notes and do correlate with certain aspects of quality (moramarco et al., 2022). to further validate our findings, we also conducted a human evaluation with three expert physicians (§4.3). as noted previously (savkov et al., 2022), even human evaluation of clinical notes is far from perfect; inter-annotator agreement is generally low, likely because physicians have differing opinions on the importance of each patient statement and whether it should be included in a consultation note. we also found low interannotator agreement in our human evaluation and speculate this is partially due to differences in specialties among the physicians. physicians 1 and 3, both from family medicine, had high agreement with each other but low agreement with physician 2 (cardiac surgery, see table 3). investigating better automatic metrics and best practices for evaluating clinical notes (and generated text more broadly) is an active field of research. we hope to integrate novel and performant metrics in future work. data privacy while our gpt-4 based solution achieves the best performance, it is not compliant with data protection regulations such as hipaa; although azure does advertise a hipaa-compliant option.13 from a privacy perspective, locally deploying a model such as led may be preferred; however, our results suggest that more work is needed for this approach to reach acceptable performance (see table 3). in either case, when implementing automated clinical note-generation systems, healthcare providers and developers should ensure that the whole system — including text-tospeech, data transmission & storage, and model inference — adheres to privacy and security requirements to maintain trust and prevent privacy violations in the clinical setting."
2886,"limitations the use of machine learning models in clinical decision-making requires an understanding of the reasoning behind model predictions. our study focuses on improving the performance of smaller models using context and subtask blocks. while the subtask state labels provide some interpretability, we have not explored its impact on trust among medical professionals. in addition, the relative benefit of the different multi-pass strategies and different types of context appear to depend on the degree of domain mismatch, which should be further explored in future work."
2887,"limitations in the experiments conducted by the team, several limitations were observed. firstly, the handling of the semi-structured structure was not accomplished perfectly. when generating the entire content at once, many sections did not appear, and when creating sections separately, the content of the notes was repetitive. additionally, addressing the clinical domain properly was not achieved."
2888,"limitations in this work, we adopt three gcns to exploit the inter-code relations under different levels. however, this may bring extra training difficulty and the risk of over-parameterization to the model. besides, during the preprocessing stage, we adopt a wordlevel tokenizer and cbow to obtain their embeddings for mimic-iii texts and code descriptions. however, this might not be enough to represent the words since medical documents have some special characteristics, but we do not take them into consideration. we tried in our work with other pretraining strategies, such as clinicalbert (alsentzer et al., 2019), bert (devlin et al., 2019), biobert (lee et al., 2020) and biowordvec (zhang et al., 2019). we added as well the bpe tokenizer (sennrich et al., 2016) in order to capture the meaningful medical sub-word units. however, the results are all far from satisfactory."
2889,"limitations and boundary conditions of our models in order to make more accurate, informed, and measured claims about their clinical outcomes. for each patient transcript, we generate 9 versions of the original text that amplify types of language errors in both semantic and syntactic categories. table 2 compares the aphasia and dysarthria models’ performance on all categories of errorinfused transcripts, with f1 as the evaluation metric."
2890,"limitations due to clinical and financial constraints, both the patient and the mturk sample sizes of our study are still relatively small. this means that we cannot afford to set aside patient data as a hold-out test set, and have to use the validation set for model evaluation. as we work towards enrolling more patients and recruiting more healthy volunteers to 429 improve model generalizability, we hope to expand the scope of our pipeline beyond english to serve non-native speaker patients. one major limitation of the cookie theft picture description task is its lack of equitable assessment for an increasingly diverse patient population. steinberg et al. (2022) identify gender as a particularly fraught aspect of the picture’s expected response, as the rubrics of the initial nihss were established from a male-only corpus. although there is no alternative picture or stroke patient corpus available to our study, we try to ensure the equity of our models by maintaining a gender balance in our patient set, with 136 female patients and 132 male patients. on our patient-only evaluation set, our aphasia model performs significantly better on female patients (auc=0.909) compared to male patients (auc=0.702), while the dysarthria model exhibits better performance on male patients (auc=0.778) than female patients (auc=0.719). at present, we are unable to draw any definitive"
2891,"limitations in this work, we studied only the evaluation of textual medical report generation from both automatic and human evaluation perspectives, but we did not study the evaluation of data-to-text medical report generation, which has its own specificities."
2892,limitations the paper does not cover all types of possible methods and models for the generation of clinical notes. the challenge datasets are also limited in terms of size and medical specialities. further experiments and evaluations are needed to validate the best performing methods on other datasets and scenarios.
2893,"limitations generating clinical notes or summaries of clinical conversations using nlp technology is a rapidly developing field with great potential. however, there are several limitations to this technology that must be considered. firstly, nlp models rely on high-quality data to achieve accurate results. in the medical field, obtaining such data can be challenging due to privacy concerns and regulations. secondly, the complex and technical nature of medical language poses a challenge to nlp models, which may struggle to understand and interpret medical terminology and abbreviations accurately. additionally, clinical conversations often involve sensitive information that requires careful handling, making it important to ensure the security and privacy of generated clinical notes. this field is considered a safety critical area, where high precision is expected, therefore, the use of nlp models in such clinical settings must be performed with caution and under medical professionals’ supervision to ensure the generated notes’ accuracy and reliability."
2894,"limitations identified in this study. for classification, we will experiment with model configurations and explore alternative machine learning algorithms. for summarization, we will refine prompt strategies, incorporate domain-specific knowledge, and investigate various fine-tuning techniques. lastly, conducting user studies with medical professionals will provide valuable feedback to assess the utility and accuracy of our generated summaries in real-world clinical settings and further refine our approach."
2895,"limitation of subtask a one significant limitation of our transformer-based model is that it does not directly consider length during its generation process. this often results in the production of overly verbose summaries. in contrast, models like openai’s gpt-3 and gpt-4 have much longer input and output limitations, allowing them to handle more extensive text samples more effectively. it is worth noting that our model was trained on the samsum dataset, which has longer texts compared to subtask a. consequently, our model struggles to adapt to the shorter length requirements of subtask a. moreover, the training dataset for subtask a is relatively small, which further complicates the model’s adaptation. future exploration should look at how to constrain the conciseness of generated summaries, which may involve reconsidering the generation method chosen or examining other techniques to promote brevity. developing methods to better control the length of generated summaries is essential to improve their relevance, coherence, and usability in real-world applications. length limitation of subtask b for subtask b, it is challenging to achieve reasonable results using fine-tuned models. in reality, this task is more representative of real-world scenarios, where inputs and outputs are considerably long, and the output is expected to maintain a specific structure and format. thus, we see the main advantage of using contextual examples lies in their ability to guide the structure, style, and length of the desired output. we believe that openai’s llm is wellsuited for similar real-life scenarios, provided that it is given an appropriate context. in such cases, its performance will significantly surpass that of fine-tuned transformer-based models. factual inconsistentcy while our study did not specifically investigate the following issues, we noted several factual errors that occur in summaries. a previous study has shown that llms also exhibit a noticeable occurrence of attribute errors and misinterpretation errors (tang et al., 2023a). prompting we find that prompt template and demonstration example selection both have a substantial impact on results. using more prompt examples for demonstration improves significantly. we acknowledge that we did not explore different selection strategies, such as semscore, lmscore, and tlength, which involve using top-ranked examples. these strategies have been shown to potentially improve the quality of the generated summaries by selecting more effective prompt examples. while our current approach did not incorporate these strategies, we recognize that exploring and incorporating better prompt examples could potentially yield improved results. this is an area that warrants further investigation and experimentation to enhance the performance of our models in future iterations of the study. data privacy both gpt-3 and gpt-4 are not local models; we utilize openai’s api to run these models, which actually violates data protection laws such as hipaa. ensuring data privacy during fine-tuning or testing is of paramount importance. we have not taken this aspect into consideration."
