,Text
0,"The primary limitations of the current study pertain to the selection of prompts and evaluation metrics. The experimental cost of requesting API responses from OpenAI to assess ChatGPT’s text generation abilities imposes significant constraints on our choice of datasets. Therefore, we have to limit our experimentation to only two related controllable text generation datasets. While we have evaluated ChatGPT’s performance at both the document and sentence levels, we cannot extrapolate that ChatGPT has similar performance for other text generation datasets. Additionally, the experimental cost prohibits us from conducting traversal experiments on the selection of hyperparameters. We relied on the default configuration recommended by OpenAI, and we maintain consistency in all hyperparameters to ensure the fairness of the experiments.
Secondly, although we have studied the impact of prompt engineering on ChatGPT, the selection of prompts is mainly affected by human understanding, and the number of potential prompts is infinite. Hence, we cannot guarantee whether other prompts that we did not select will yield the same conclusions as our experiment. Furthermore, ChatGPT is subject to continuous updates and iterations, which may lead to improved performance, making it difficult to predict if future versions of ChatGPT will have similar results to our experiments.
Finally, to select appropriate evaluation metrics, we have included both domain-related evaluation metrics (such as reading difficulty and text formality) and domain-independent evaluation indicators (such as fact-checking and hallucination detection). However, we acknowledge that the automatic met-
rics may sometimes not capture all aspects of the intended construct correctly."
1,"This study has the following limitations: • We fixed the vocabulary size of each subword
tokenizer to 30k. Using a different size might yield different results than those in our paper, though the effect of varying the vocabulary size for a subword tokenizer seemed to be small if the size is sufficiently large (e.g., over 16k or more) (Toraman et al., 2022). • We have used the BERT architecture for our comparison, while there are other commonly used model architectures such as T5 (Raffel et al., 2020) and GPT-3. The investigation with these architectures is our future work. • To investigate the impact of tokenizers on the downstream performance of PLMs in scriptio continua languages, we have taken Japanese as a case study. Other scriptio continua languages will be addressed in the future."
2,"In this section, we discuss two limitations of this study. The first limitation is that aspect and temporal commonsense are outside the scope of our dataset. Here, temporal commonsense refers to knowledge regarding events and the appropriate
duration of those events. For example, the event “I washed my face for three years” is unnatural in terms of temporal commonsense, but this study did not consider such unnaturalness.
The second limitation is that the proposed method is currently applicable only to Japanese. In this study, we used a Japanese case frame dictionary to generate natural sentences. However, other languages such as English do not have resources equivalent to such a dictionary. Therefore, to apply our method to additional languages, we must first prepare a case frame dictionary for each language."
3,"While our study provides valuable insights, it is important to keep in mind its limitations. Firstly, it was confined to text classification and did not include other NLP problems such as Named Entity Recognition (NER) (Wang et al., 2022b), Question Answering (QA) (Pandya and Bhatt, 2021), etc. Expanding this research to a wider range of tasks would provide a better understanding of the methods’ performance in diverse data scenarios. Additionally, the inclusion of a task shift can be valuable, where the model is trained on a single task but OOD data come from a totally different prediction problems.
Secondly, we conducted our experiments using only RoBERTa model. We chose a widely used language model for text classification, but there are several other architectures worth testing, especially large language models (LLMs) (Zhao et al., 2023) that now becoming extremely popular. A more comprehensive evaluation of the models and methods could provide more insights into whether the development of transformer-based methods contributes to better detection of OOD data.
Finally, due to restricted computational time, we did not perform a hyperparameter search for either model or methods. We just used recommend values from the original publications. This may have affected the obtained results, and it is certainly an aspect worth investigating in the future."
4,"The MECO dataset (Kuperman et al., 2022) is recorded at different labs following the same strict protocol. Nevertheless, location and experimenter effects may be confounding factors for the NLIR task. The CELER data (Berzak et al., 2022), used by (Berzak et al., 2017), seems to all be recorded at the same lab. Since we confirm their hypothesis, we do not see this as a fatal flaw in our study. There is no other available dataset that would allow us to replicate their finding."
5,"There are several limitations to the experiments conducted in this project that should be acknowledged:
• Selection of the best pre-trained language model (PLM) for prompt-based learning: The evaluation method used to compare the performance of BERT, GPT-2, and T5 in the context of manual templates and manual verbalizers may not be entirely accurate. The performance of these models did not show significant differences, making it difficult to determine the best model for prompt-based learning. Furthermore, other domain-specific PLMs, such as Bio-BERT, which may be better suited for handling clinical data, were not considered in this project.
• Limited exploration of templates: The experiments utilized a limited number of templates, particularly for soft and mixed templates. These templates were primarily based on prompts derived from manual templates. Further experimentation is needed to explore different patterns, such as varying the position and length of soft token sequences or using soft tokens in mixed templates to replace manual tokens (e.g., ""Question:"").
• Comparison with frozen PLMs: The experiments did not include a comparison between fine-tuned and frozen PLMs, as done in Taylor’s study (Taylor et al., 2022). This comparison could provide valuable insights into the performance trade-offs between these two approaches.
• Addressing the effects of imbalanced datasets, several strategies have gained popularity. 1) Re-sampling techniques, for example, Monte Carlo Simulation Analysis, can be used to balance class distribution by oversampling the minority class, undersampling the majority class, or the combination of these two (Gladkoff et al., 2021). 2) Data augmentation techniques, such as the use of Generative Adversarial Networks (GANs), can generate new examples for the minority class by applying transformations to existing data. 3) Furthermore, machine learning approaches like bagging and bootstrapping can reduce variances
by implementing a ""voting system"" that enables models to make better decisions.
• Finally, it would be advantageous to develop a post-processing step that generates a table displaying all treatments along with their corresponding temporal information. This would create an end-to-end system that physicians could use as a practical tool.
Future research should address these limitations by exploring a broader range of PLMs, templates, and experimental setups to provide a more comprehensive understanding of the performance characteristics of prompt-based learning methods in the clinical domain. Application to some more powerful computational resources will also extend this work."
6,"The main limitation of our work is the focus on the specific domain and the dataset that is not yet publicly available. However, we should note that the dataset can be requested for further research and replication studies and it will be released in the future.We believe that testing adapters with different settings in the emergency response domain is a valuable contribution but we are also aware of the fact that the dataset used in our experiments is not large or exhaustive enough to cover all the variety of topics relevant for the emergency response. For example, our data cover cases of explosions, leakages of hazardous materials and building collapse but do not include any dialogues for open field rescue operations or car accidents.
Another issue that is worth mentioning is the fact that all recordings were collected during the training sessions and not the actual missions. Hence, the responders might be under less pressure than in a real life-threatening situation and their communication might be more of a textbook case. However, all simulations had a realistic setting that includes several operators, robots and points of interest (objects or locations) and we believe that the recorded communication is representative for the domain in question."
7,"We explore early, very simple structured event representations. Recent works in visual–linguistic semantic representations which use richer representations comprising predicate–argument structures and event types and argument roles, the general graph-based approaches, as well as scene graphs, are left for future work. Furthermore, the wikiHow articles may reflect the bias of their human authors."
8,"While we demonstrate the efficacy of TSDSHAPLEY empirically, the current work is limited in terms of theoretical analysis. For example, while we have good empirical performance with a linear SVM, additional analysis could determine if there are optimal ways to select an alternative simple model architecture for the source classifier depending on the target classifier or dataset. Additionally, while we found a strong correlation between number of sampling chains and performance when the subset size was > 2% of the training data size, the lower subset size threshold to observe this correlation may be dataset dependent, which additional analysis could address."
9,"This work used the moral foundations dictionaries to measure the moral content of text produced by GPT-3. While studies have demonstrated correspondence between results from the dictionaries and human labels of moral foundational content (Mutlu et al., 2020; Graham et al., 2009), dictionarybased analysis is limited in its ability to detect nuanced moral expressions. Dictionary-based analysis could be complemented with machine-learning approaches (Garten et al., 2016; Johnson and Goldwasser, 2018; Pavan et al., 2020; Roy et al., 2022) as well as human evaluation. This study attempted to control for variations in the prompt phrasing by averaging results over several prompt styles (Tables 2 and 3). These prompt variations were chosen by the author. A more principled selection procedure could result in a more diverse set of prompts. The human studies that this study refers to (Graham et al., 2009; Frimer, 2020) were performed on populations from the United States. The precise political connotations of the terms “liberal” and “conserva-
tive” differ across demographics. Future work may explore how language model output varies when additional demographic information is provided, or when multilingual models are used. Documentation for the datasets used herein indicates that the crowd workers leaned politically left, and morally towards the Care/Harm and Fairness/Cheating foundations (Forbes et al., 2020; Hendrycks et al., 2021; Fraser et al., 2022). However, bias in the marginal foundation distribution does not hinder the present analysis, since the present experiments experiments focus primarily on the difference in foundation use resulting from varying political identity. The analysis in Section 2.1 relies more heavily on the marginal foundation distribution; a foundationallybalanced dataset was constructed for this experiment. This study used GPT-3 (Brown et al., 2020), GPT-3.5 (OpenAI, 2022), and OPT (Zhang et al., 2022). Other pre-trained language model families of similar scale and architecture include BLOOM8, which I was unable to test due to compute budget, and LLaMA (Touvron et al., 2023), which was released after the experiments for this work concluded. While the OPT model weights are available for download, GPT-3 and GPT-3.5 model weights are not; this may present barriers to future work that attempts to connect the moral mimicry phenomenon to properties of the model. On the other hand, the hardware required to run openly-available models may be a barrier to experimentation that is not a concern for models hosted via an API.
Criticisms of Moral Foundations Theory include disagreements about whether a pluralist theory of morality is parsimonious (Suhler and Churchland, 2011; Dobolyi, 2016); Ch. 6 of (Haidt, 2013), disagreements about the number and character of the
8https://bigscience.huggingface.co/blog/bloom
foundations (Yalçındağ et al., 2019; Harper and Rhodes, 2021), disagreements about stability of the foundations across cultures (Davis et al., 2016), and criticisms suggesting bias in the Moral Foundations Questionnaire (Dobolyi, 2016). Moral foundations theory was used in this study because it provides established methods to measure moral content in text, and because MFT-based analyses have identified relationships between political affiliation and moral biases, offering a way to compare LLM and human behavior. The methods presented here may be applicable to other theories of morality; this is left for future work.
Work that aims to elicit normative moral or ethical judgement from non-human systems has received criticism. Authors have argued that nonhuman systems lack the autonomy and communicative intent to be moral agents (Talat et al., 2022; Bender and Koller, 2020). Criticisms have also been raised about the quality and appropriateness of data used to train such systems. Notably, crowdsourced or repurposed data often reflects a priori opinions of individuals who may not be informed about the topics they are asked to judge, and who may not have had the opportunity for discourse or reflection before responding (Talat et al., 2022; Etienne, 2021). Some have argued that systems that aggregate moral judgements from descriptive datasets cannot help but be seen as normative, since their reproduction of the popular or average view tends to be implicitly identified with a sense of correctness (Talat et al., 2022). Finally, several authors argue that the use of non-human systems that produce apparent or intended normative judgements sets a dangerous precedent by short-circuiting the discursive process by which moral and ethical progress is made, and by obscuring accountability should such a system cause harm (Talat et al., 2022; Etienne, 2021).
The present study investigates the apparent moral rationalizations produced by prompted LLMs. This study does not intend to produce a system for normative judgement, and I would discourage a normative use or interpretation of the methods and results presented here. The recent sea change in natural language processing towards general-purpose LLMs prompted into specific behaviors enables end users to produce a range of outputs of convincing quality, including apparent normative moral or ethical judgements. Anticipating how these systems will impact end users and society requires studying model behaviors under a variety of prompting
inputs. The present study was conducted with this goal in mind, under the belief that the benefit of understanding the moral mimicry phenomenon outweighs the risk of normative interpretation."
10,"limitations this study stems from a novel idea for chinese historical phonology studies. as few direct predecessors could offer hindsight, there are quite a few limitations to this study that may be addressed with further work. 1. while the initial-final-tone decomposition is convenient in this context, it also limits the transferrability of the proposed tool to languages outside of the sinosphere. this calls for further exploration of more generalizeable approaches to phonological representation learning. 2. polyphonic characters were not fully utilized in the study, and their alignment perreading and tokenization into separate identifiers should be considered in future work. 3. finally, making full use of the dataset is crucial, and the stochastic train-test split used in this study may leave out important hints. alternative sampling strategies, such as crossvalidation or bootstrapping, could enhance the robustness of the results."
11,"limitations the current work has several limitations that warrant further investigation. firstly, due to time constraints, we did not conduct experiments using the proposed framework on few-shot settings or a more challenging multi-label classification task. secondly, our ablation study in section 4.4 showed that the framework with the weight assignment resulted in only a marginal improvement in performance, suggesting that simcse may not be the most effective method for addressing prediction bias. therefore, future work will explore alternative modeling approaches for bias reduction. thirdly, in section 4.5, we noticed that several irrelevant words are also generated as keywords with the language prompt, which may negatively impact the final representation. to address this issue, a better solution, such as keyword filtering, should be considered to improve the current framework. lastly, we treated each word as a single atomic entity in the kg embedding space, regardless of its possible different senses or meanings. a more careful treatment of word meanings is necessary to handle the problem of polysemy. the race is on: second private team sets launch date for human spaceflight (space.com) space.com - toronto, canada -- a second\\team of rocketeers competing for the #36;10 million ansari x prize, a contest for\\privately funded suborbital space flight, has officially announced the first\\launch date for its manned rocket."
12,"limitations this papers focuses on a variety of videoqa - event-level videoqa, we only incorporate event information from the question (textual) side as we think that parsing video frames is inaccurate and could introduce unexpected errors, we should also explore how to inject event-level information from visual side in the future with more competitive visual parsing models. our experiments are only conducted on one dataset due to resource constraint, we should also conduct experiments on more datasets to verify the effectiveness of our approach."
13,"limitations we used xlm-r for the baseline model to train with our dataset in our experiments because we wanted to make experimental settings as close as the previous study of codebert but for multilingual data. since codebert is based on roberta, we chose xlm-r, which is also roberta-based and already trained with multilingual data."
14,"limitations in this work, we confirm the effectiveness of the proposed method only on the english-german translation tasks using the multi30k dataset, the most commonly used dataset in the mnmt reserach area. it is not clear whether the proposed method is effective for translation for language pairs other than english and german or translation when a larger training dataset is used (e.g., when using an existing data augmentation method for mnmt). we will leave these verification experiments for future work. the proposed method has improved translation performance of mt, but the performance is not perfect and translation results could include translation errors. accordingly, there still remains a possibility that translation results by the proposed method could convey incorrect information. the proposed method requires an additional process for transforming images, compared with conventional mnmt models. the experiment, including model training and testing, on the proposed model mnmt(conv.) took about 20 hours longer than that on the baseline mnmt model mnmt(orig.) when using rtx3090 gpu × 1."
15,"limitations the main limitation of our study comes from the extra parameters caused by confidence calculation, in which two separate self-attention operations and biaffine transformation are performed. incremental parameters results in a more time-consuming training process, and a higher hardware demand for storage. to address this issue, we plan to combine parameters from different attentional transformations into shared weight matrices in our future work to reduce the model size."
16,"limitations the dialogue generated for the player exhibits a higher degree of repetition and has a tendency to- wards looping. this limitation exists as we did not focus on generating player dialogue as that is a different problem of its own. to account for this limitation, both the self-diagnosis and the turing quest only evaluate the npc’s dialogue. currently, the maximum context window for the dialogue history portion is limited by the max tokens of a given model minus the tokens required for the npc header. despite being a rare occurrence, it is possible that the dialogue history becomes so long that the model may not be able to generate any responses as there is no more remaining space. we did not experience this problem; however, a workaround would be to discard the oldest dialogue history entry as needed. this approach however may cause the npc to lose out on information that it would otherwise be able to leverage in dialogue."
17,"limitations we acknowledge three limitations in our experiments. first, in our second experiment, we fixed the window size for each type of note to be n/2. a more comprehensive investigation could also search for the optimal window size for each note type. second, although we explored one fragmented window configuration p = both, we did not explore other fragmented window configurations due to resource constraints. lastly, we did not investigate more types of clinical notes (e.g., physician notes and ecg notes) because mimic-iii has limited examples for other note types. we expect it to be resolved in future works with mimic-iv’s publication (johnson et al., 2023)."
18,"limitations that may influence the interpretation of our results and call for future works. first, we did not conduct a hyperparameter search for the regularization strength of corrloss. second, since f1 score decreases are substantial and universal across all experiments on mimic-iii-50, we did not run experiments multiple times with different seeds. third, we did not provide a rigorous explanation of what caused our empirical findings. future works can investigate the plausible hypothesis that the trade-off between the dependency information and the learning complexity causes these findings. besides these limitations, future works can also investigate more scenarios and methods of incorporating the correlation prior."
19,"limitations the morpho-syntactic parameters used in this study are just a fraction of various other linguistic parameters that have been proposed in theoretical syntax (e.g., roberts 2019). a set of optimal language parameters for language clustering may vary depending on the target task. it remains to be seen whether and how various parameters in theoretical linguistics could improve different nlp tasks. for example, cross-lingual transfer learning may be performed more effectively by carefully tailoring the linguistic parameters to a particular task, like what we have done for ner. related to the above point, one limitation of our approach would be the fact that some languages have not yet been investigated well in theoretical linguistics, particularly some underdocumented or endangered languages. even as for welldocumented languages in theoretical linguistics, some parameters still remain controversial, such as the so-called np/dp parameter (e.g., bošković 2012). thus, our approach proceeds in tandem with the advancement of theoretical linguistics."
20,"limitations we built buzzer quiz answering systems. however, they do not take into account the time required to respond, and these systems do not have the ability to generate real-time responses, which is essential in actual buzzer quizzes. additionally, the experiments in this study were conducted only in japanese, and it remains unclear whether similar results would be obtained in other languages. particularly, english has a significantly different sentence structure compared to japanese, hence further investigation is necessary to confirm whether appropriate results can be achieved."
21,"limitations although our research led to improvements in the translation of subject pronouns, object pronouns, and possessive adjectives and pronouns, these improvements did not cover non-binary-associated pronouns, such as they/them/theirs, xe/xem/xyr and ze/hir/hirs. the large underrepresentation of non-binary genders in textual and visual data contributes to propagating the misrepresentation of non-binary people by language models. in this paper, we were unable to work against this issue, thus we hope to contribute to a fairer representation of these disadvantaged groups in the future."
22,"limitation although our leco framework is shown to be effective in improving the multi-exit bert training, it still has certain limitations that need to be addressed in the future: (a) mha exits and our learned exits indeed introduce new parameters and additional flops. we would like to explore more parameter-efficient methods to improve multi-exit bert training in future works. (b) in this work, we demonstrate our framework’s performance on sentence classification or pair classification tasks. in future works, we would like to extend our work to broader tasks such as sequence labeling, relation extraction, and text generation. we would like to explore this aspect in the future."
23,"limitations while this research provides valuable insights into using the gan-bert model for authorship attribution, there are also a few limitations to note. we only focused on a limited number of authors from the late 19th century, which may include shortcomings towards model generalisability. future research should consider using the whole dataset of long 19th-century novelists to address this limitation. due to the copyright issues explained in section 4.6 and section 7, we do not release the whole dataset, instead, we release scripts to reproduce the datasets. furthermore, incorporating a rich feature set and comparing performance among different models would be another interesting research direction."
24,"limitations our studies focus on the differences in how-to guides written for specific audiences only in one language, namely english. a major limitation is therefore that we do not consider other languages. the perspectives provided by the data source we rely on, wikihow, allow us to identify specific phenomena and peculiarities. yet, contemplating only one data source lets us generalize only to a limited extent. for example, the audiences considered in this work depended on the target groups portrayed in the data. they are neither exhaustive nor representative of the diversity of humankind, especially of marginalized social groups. therefore, a wider variety of data sources will be needed to test generalizations. finally, a further limitation of our studies concerns intersectionality. while it seems possible that guides can be tuned by contemplating one specific attribute of the audience at a time, this does not hold with regard to the actual attributes of the readers. such attributes are per se coexistent, and consequently, they are not separable."
