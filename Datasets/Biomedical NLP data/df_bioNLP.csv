,Text
0,"We would like to discuss the following limitations and ethical considerations:
In this paper, we investigated the cross-domain extraction performance based on a multi-source corpus. Our working assumption is that this corpus represents enough variety to support such a claim. However, we point out that the corpus is biased towards English scientific and patent language, as well as the chemical / material science subject domain. Further, we remark that the subjects distribution itself is biased towards the BM and MSP datasets as the the more varied MeasEval dataset only contains few examples for each of its 10 subjects. Consequently, a balanced corpus should have a more even distribution of both subject domains and language domains by increasing the size of the currently underrepresented domains and ideally including data from more than only the English language.
Further, despite having substantial IAA scores for the re-annotation of the MSP corpus, we often perceived the task as difficult and ambiguous and felt the limitations of only having two contextual entities, instead of the three as proposed by Harper et al. (2021). Yet, the low IAA score (0.334) for the excluded Qualifier entity suggests that including it may not have eased the task. Hence, it may be valuable to further the study of how the measurement extraction problem can be modelled to resolve some of the ambiguities for context extraction.
Finally, while we tried to stay as closely to the original annotation guidelines as proposed by Harper et al. (2021) as possible (with the exception
of the two cases explicated in Appendix C, there is a high likelihood of annotation drift. The reannotators of the MSP corpus were not involved in the original MeasEval annotation procedure and it is possible that the interpretation of the annotation guidelines was slightly different at places than the authors have originally intended. Our adaption of the annotation guidelines can be found at the end of this paper (Appendix J)"
1,"Our proposed method has certain limitations and ethical considerations that merit discussion. The effectiveness of our approach heavily relies on the rule-based labeler. However, it is important to acknowledge that the labeler may not capture unseen patterns or variations, potentially limiting improvements in various evaluation metrics. Moreover, we were unable to conduct a comprehensive human evaluation of the rule-based labeler in this study due to resource constraints. Therefore, future work should include a detailed evaluation to assess its performance and address any potential limitations.
Collaboration with three radiologists at Kyoto University is a critical aspect of our work. The regular expressions designed in the rule-based labelers were validated through mutual confirmation by computer scientists and radiologists. However, it is essential to note that the radiologists involved in the collaboration primarily work in a Japanese hospital setting. This may introduce potential biases or patterns that are specific to the local context. Therefore, it is necessary to cross-check the performance of the rule-based labeler with radiologists
from different regions and healthcare systems to ensure broader applicability and minimize any potential bias.
Regarding the datasets used in our study, we exclusively utilized publicly available datasets that are properly anonymized and de-identified, addressing privacy concerns. However, it is crucial to emphasize that if datasets containing comparison exams become available in the future, additional precautions must be taken to ensure that no personally identifiable information is inadvertently disclosed or used in a manner that could identify individual patients.
By acknowledging these limitations and ethical considerations, we aim to encourage future research and discussions in the field, driving advancements in radiology report generation while prioritizing patient privacy, accuracy, and fairness. These considerations will contribute to the development of robust and ethically sound approaches in radiology report generation."
2,"All of the examples mentioned in this section are shown in Table 6 in the appendix.
WER has the main advantage of being simple and consistent. Unlike sentiment or text embeddings, there are not multiple models. The main limitation of WER is that, because it is not based on any understanding or model of the language, there are severe errors that have a relatively low WER, and vice versa, there are non-severe errors that have a high WER such as a multivitamin vs. a multi vitamin.
Sentiment has strong limitations due to the fact that these algorithms are designed to only measure how positive or negative a text is. Sentiment proved to be sensitive to misses in disfluencies like um or uh. This is highlighted in the example, uhm it started last night vs and it started last night,
where there was a strong difference in sentiment of 1.707. This can be an advantage or a limitation depending on the scenario. Many ASR systems overlook disfluencies, but, for example in human robot interactions, spoken dialogue systems, or in the prediction of dementia status, disfluencies can be vital to understanding and performance (Baumann et al., 2017; Clark and Tree, 2002; Farzana et al., 2022; Lopez-de Ipiña et al., 2017; Mueller et al., 2018).
There is the also a limitation on the accuracy of the model. In the examples any previous surgeries vs. any previous surgery or uh i smoke about a pack a day vs. uh smoke about a pack of day, there is a high difference in sentiment yet the only difference is in missing the pronoun i or the plural of surgery, which should not affect sentiment greatly.
Despite these limitations, sentiment is able to catch some severe errors where the WER is relatively low. In the example where crystal meth becomes crystal mud or where chest pain becoming chatting the WER is 0.125 and 0.333 respectively, but the difference in sentiment is very high at 1.858 and 1.889 respectively.
Text embeddings are limited by the performance of the model, like sentiment, yet capture more than just polarity of a given text. Knowing that many of these models are trained in a self-supervised manor using the context in the training text, we can see how the embeddings in the example of my parents and our friends would be similar. Both of these phrases could occur in with similar surrounding text; they have the same grammatical structure (a possessive adjective followed by a noun) and
parents and friends are both human relationships. Another limitation on these models is the amount of text they can handle. Anything above the model’s limit gets truncated, and consequently, loses the meaning of truncated text. Although utterances are commonly short in ASR training data, the character limitation on these models could affect performance on longer utterances.
Despite these limitations, text embeddings were able to capture well the differences in meaning. Text embeddings were able to give a high score to the examples where crystal meth becomes for sunlight and where chest pain becomes testing when WER and sentiment scores were relatively low. Text embeddings were also able to give low ratings for different writings of the word okay and numbers (ok vs. okay, or uh thirty eight degrees vs. 38 degrees) when WER were high."
3,"One limitation of our study is the small size of the test set, which may impact the generalizability of our results. Additionally, we restrained our work on clinical entity extraction; in future work, we would investigate more in several tasks using the E3C temporality layer to cover a task of Name Entity Recognition and Relation Extraction tasks.
Finally, the E3C guidelines have been designed for clinical entity extraction and entity-linking via UMLS entities. After the first step of manual annotation, some spans of the entities have been modified to fit as close as possible to the semantical concepts found in UMLS (Magnini et al., 2020). For instance, clinical entities could be split into separate disorder concepts, and the extent of a disorder candidate could be reduced to fit with a concept. These biases could induce additional difficulties in finding the correct span for a given model."
4,"In our work, we manually annotated small portions of corpora. Such limited size is justified by the time-consuming task of temporal annotations and the requirement of expertise for toxicity event annotations. Although our temporal representation seems to perform well with other clinical reports containing information about a different type of cancer from that on which it was trained (e.g. lung cancer vs. colon cancer), such results must be validated on clinical reports containing information about additional cancer types. Additional experiments are also needed to validate the generalizability of our event-independent representation, such as evaluating it on other hospital or data warehouse clinical reports with various structures and evaluating it on other extraction tasks with different event definitions."
5,"The methodology of the constructed corpus is based on NER, making SSI a corpus integrating RE with NER. We aimed to enrich our experiments with multi-task learning so as to measure the impact of NER on the extraction of relations. Unfortunately, after conducting statistical analysis on SSI, it showed that, due to its limited size, we did not have enough instances of the same pair of entities participating in one sentence. As this statement
would negatively bias our multi-task model, we refrained from conducting these experiments, before proceeding to annotate further instances and add them to our corpus in further work. In terms of annotation, only one annotator has been invested in the current task, as it is a preliminary work including a new dataset for relation extraction, with new entities and relation types. In a future work, multiple annotators performing the annotation task and metrics for the corpus quality will be presented."
6,"One limitation of this study is a potential lack of reproducibility for some experiments due to the employment of the proprietary GPT 3.5 LLM. In the future, this could be addressed by deploying and versioning open-source LLMs with similar capacities locally. We acknowledge a potential bias in our dataset, as only a quarter of the full corpus was manually reviewed based on existing annotations, i.e., three-quarters of GGPONC 2.0 were excluded from our analysis so far. Therefore, the matched dataset of controls will likely overrepresent the prevalence of ECCNPs, while not adequately representing the full spectrum of hard negative cases. However, we considered a manual review of the full corpus as unnecessary due to the very low number of ECCNPs expected in the remaining documents. Further, we note that also other kinds of coordination ellipses (omitted adjectives or verbs) besides ECCNPs may be relevant for downstream information extraction tasks. Although preliminary annotations for these exist in GGPONC 2.0, we excluded them from our analysis, as annotations were not comprehensive, and their resolution is often more ambiguous than elliptical compounds."
7,"Since the training datasets of ChatGPT are unknown, some data used for evaluation may or may not exist during the training phase of ChatGPT Also, a new version called the GPT-4 model has been released that may ensure higher accuracy. Nonetheless, GPT-4 is very costly to use, around 60x more expensive than ChatGPT. Meanwhile, even using the paid ChatGPT Plus2 subscription, it is available for just a limited use (allows evaluation of only 25 samples in 3 hours). Another limitation of this research is that the results mentioned in this paper for ChatGPT may not be reproducible, as ChatGPT may generate different responses for the same input prompt. Although the experimental results may change over time, this work will still give a concrete direction for future research using ChatGPT like large language models in the biomedical domain."
8,"Although our findings suggest that biomedical language model pre-training is quite robust to suboptimal tokenization, we note that our work has a few potential limitations that should be explored further. The use of a biomedically relevant subset of the SIGMORPHON Shared Task dataset for evaluating biomedical term tokenization is a straight-forward and reasonable strategy, however, it is important to highlight that the resource was not created for this purpose and might not be perfectly aligned with ideal biomedical tokenization. Additionally, we would like to point out that even though our BioVocabBERT tokenizer outperforms other equivalent tokenizers like PubMedBERT’s, it severly underperforms the best possible segmentation accuracy (48.5 vs 74.1 for our fine-tuned CANINE model). It is therefore possible, although unexpected, that a tokenizer which performs biomedical tokenization at even higher levels could lead to sudden improvements in the pre-training process. Finally, we note that the effects of the BioVocabBERT’s much larger vocabulary size, almost three times larger than PubMedBERT’s, on the pre-training process were not explored in depth. Nevertheless, given that some previous work (Feng et al., 2022) argues that larger vocabularies lead to slight improvements in downstream tasks, our main conclusions are likely to hold."
9,"The MIMIC-III corpus includes a discharge summary for each admission. However, it is limited
to patient’s time in the intensive care unit (ICU), meaning that the patient’s history for any time after transfer from the department is lost. Given most patients progress to lower severity departments as they recover from intensive care, a large cross section of the patient’s notes are missing from our analysis. In Section 3.2 we discuss the statistics that justify this conclusion."
10,"Retrieval augmentation adds complexity to natural language generation requiring a separate retrieval module before the text generation step can begin. Additionally, retrieval augmentation possibly introduces more input text than the original input which is problematic for many neural network architectures with limited input space especially in the case of summarizing entire scientific papers. Finally, retrieval augmentation itself could introduce factual or relevancy errors if the retrieved documents are irrelevant or incorrect and they end up being used in the generated summary."
11,"The data augmentation method mentioned in the paper requires the augmentation dataset to be of a similar structure to the task dataset, which is not the case for MeQ-SUM. As a result, the data augmentation experiments do not provide significant improvement in performance."
12,"The limitations of this work are mainly that there is a small amount of data available for inference to test the models. ROUGE-L is used as an assessment metric and n-gram overlap metrics are notably not optimal for abstractive summarization assessment (Zhang* et al., 2020; Deutsch, 2022)."
13,"There are a few limitations pertaining to the training data we used, some of which are listed below.
1. Our domain adaptation of LLMs was performed on English reports only; therefore, it may not work out of the box in a multilingual setting.
2. The paper utilizes the MIMIC-IV dataset for DAPT training, which might include overlapping data from MIMIC-III and MIMICCXR. Consequently, there is a potential risk of information leak in this method.
3. There is a data imbalance concerning imaging modalities and anatomies covered by our training data. For example, regions such as extremities, neck, spine, and shoulder are underrepresented in the dataset, and report summarization related to those regions needs thorough evaluation.
4. A study is needed to examine the diversity of patients represented in the data and how it impacts the model’s performance for underrepresented communities.
5. Different radiologists and radiology departments have distinct preferences and styles for writing reports. Moreover, clinical referrals occasionally dictate the extent to which certain details are documented in the report. No study has been conducted on the consistency, uncertainty, or information richness of the report.
Aside from the training data, the model’s space and time throughputs may render them unsuitable for on-premise and/or at-the-edge applications. This aspect presents an opportunity for further research on how to best quantize and deploy RadBloomz (and similar LLMs) within the clinical workflow to enhance efficiency for radiologists. Additionally, the paper utilizes the MIMIC-IV dataset for DAPT training, which could contain overlapping data from MIMIC-III and MIMICCXR. Consequently, there is a potential risk of information leak in this method."
14,"In this work, we used GPT-3.5 to generate additional training examples and showed that it helps improve the relevance (ROUGE and BERTScore) of the generated summary. However, we did not explicitly analyse the quality of generated examples to check whether or not they are faithful and factually correct which could lead to the same problem in generated summaries. To obtain better training examples, we could use faithful or factuality metrics to assess generated training examples and then use the post-editing method or human evaluation
to remove unfaithful content, which we leave for future research."
15,"limitations we only investigate representative methods of three widely-used el paradigms. however, there are more el methods and paradigms we may not cover, and we leave them as future works. furthermore, more auxiliary information in the biomedical domain can be introduced to address the nil issue we identify in this work. for example, a hierarchical structure exists for concepts in kbs in the biomedical domain. therefore, nil may be solved by linking them to hypernym concepts in the partial kbs (ruas and couto, 2022). we consider the hierarchical mapping between nils and in-kb concepts as a potential solution for performance degradation in partial kb inference. users can obtain different entity-linking results based on their own kbs which have the potential risk of missing important clinical information from the texts."
16,"limitations this work has certain limitations in terms of the scope of the experiments and what can be reliably inferred from them. our dataset contains notes that are predominantly from anglophone countries. however, there are less than 20% of the rows that originate from non-english speaking regions. they might contain words in other languages (e.g. italian), and although the disease names are usually rendered similarly as english, our models are pretrained on english and their ability to process other languages is therefore limited. another issue is the relatively short length of these notes. while some notes span a few sentences, most are very short and no more than 4− 5 tokens in length. this hampers the ability of a contextualised model to derive meaning from the context around each word and limits the power of attention-based architectures that are well-suited for larger contexts. in this preliminary study we only targeted one condition and looked at binary classification. the natural step towards a more inclusive experiment would be to consider other conditions and also use multi-class classification setups where a more finegrained scheme is used to classify a condition. creating a sizable multi-class and multi-label corpus is a labour-intensive endeavor that requires more time and effort and would be a goal for a future work. we did have access to multi-class annotations for our current training set, however, one major issue is that the cancer-positive cases are a small percentage of the entire rows and among the cancer types themselves, there are types that occur only once or twice and the rest belong to more frequent classes. this would make it harder for the model to learn infrequent classes. we plan to augment the annotations over time to be able to conduct experiments in scenarios beyond binary classification and cancer alone. the issue of negation was further complicated in this work by a few cases where the note had been classified as cancer positive because the doctor had identified a history of this condition in the patient but had ruled out or downplayed the possibility of cancer at the present time. distinguishing a current co-morbidity of cancer from a past history of cancer would introduce further complexity and this work does not attempt to address that."
17,"limitations of wer, and 3) incorporating severity into the training of an asr system increased performance, lowering the overall severity and wer significantly. in future work, we will experiment with different architectures, data, and methods for using semantics in the training of asr systems. limitations aside from the limitations of these proposed measures mentioned in section 6.5, we acknowledge other limitations here. the"
18,"limitations one important aspect of achieving optimal performance when using llms is the design of a highquality prompt. in this study, we consider both zero-shot and few-shot learning scenarios, which assume no or very limited task-specific data. however, iteratively refining the prompt over time to obtain the best-performing prompt may break the zero-shot or few-shot scenario. moreover, the final prompt used in this study is specifically designed to guide the crowd workers in the annotation process of the coda-19 dataset, with frequently asked questions (faqs) refined over time to address workers’ confusion. in a real-world scenario, users would not have access to such helpful faqs when working on a new task. therefore, the performance of llms may be lower in practice. also, llms are susceptible to a data leakage problem due to their training with internet data. for example, chatgpt is known to have been trained on internet data prior to september 2021. considering that the coda-19 dataset was released in july 2020, with its train, validation, and test sets made publicly available, there is a possibility that some closed models have seen the exact test instances, leading to an unfair comparison. since the training data are not disclosed for the closed models, the impact of this exposure on the models’ performance remains unknown."
19,"limitations there are also several potential limitations. first, the severity of the identification of depression is subjective, which inevitably leads to annotation bias, and we cannot verify the actual diagnosis. thus, our model is not intended to be used as a psychiatric diagnosis tool but an estimate of depression level for users, which can be utilized to direct intervention and treatment for non-clinical use. second, the datasets used are collected from a single social media platform (reddit) and are imbalanced. to train a more robust and effective model, future works should collect more precise data from other social media and increase collaboration with clinicians to ensure the quality of the data. despite these limitations, we believe that our work will facilitate depression severity detection. in future work, we would like to explore other fusion strategies to better integrate various information and assess the performance of chatgpt (yang et al., 2023a) on the depression severity detection task."
20,"limitations our experiments only used the abstracts of the articles and we did not consider the full text. actually, the full text might provide valuable information for the decision about the similarity, while making it harder at the same time, since the text is much longer. however, full text is usually not available for many of the articles in pubmed, which would result in even shorter datasets. we did not train any model based on the available datasets, as carried out by (mysore et al., 2022). indeed, our goal was to evaluate the dataset based on general-purpose text similarity algorithms that had not been fine-tuned for a particular dataset. for the annotation of the facets, we simply asked the annotators to select sentences that were part of the research goal. however, it might not have been carried exactly as in the annotation of the smafira-c dataset."
21,"limitations our work follows a pipeline approach, where the optimization of event trigger detection and argument identification are done separately. this causes the event trigger detection errors to be passed to the argument identification step. in the future we plan to do the joint optimization of both steps to reduce internally transmitted errors."
22,"limitations the result of dictionary match and autoner based on our implementation is not comparable with 5https://www.amed.go.jp/en/ the results shown in the original paper (shang et al., 2018). because the performance of distantlysupervised ner is severely dependent on the domain dictionary used, we can not simply compare the performance of the methods if common domain dictionaries are not used."
23,"limitations in the design of this study offer directions for further work to improve performance. since the redditmh dataset uses posts from general mental health subreddits, the binary-choice qa model was not exposed to finer-grained data from illness-specific subreddits, which could lead to better assessments. also, the multiple-choice qa model forces an answer from the list of five diseases even if there are no distinct language markers of any disease. future work will combine binary and multi-choice qa models to filter at-risk individuals. two-stage qa models will serve as effective screening tools to ultimately provide care to individuals who need it the most."
24,"limitations although our models performed well in all downstream tasks, the models also have some limitations. one of the limitations is the lack of varied biomedical corpus. hence, we plan to work on integrating clinical documents e.g. ehr data, specifically physician notes, to make the model more robust to various kind of biomedical documents. the models can also be enlarged by using continual learning strategy from well-known french pre-trained language models. camembert (martin et al., 2020) can be used as a base model and the training can be continued using our biomedical corpus, like biobert (lee et al., 2020) and others did. moreover, our models used 512 sequence of tokens and more longer sequence lengths can be used as seen in the long language models like bigbird (zaheer et al., 2020). we are currently working on a new version of alibert with more data and a greater diversity of corpora that include text from ehr and medical notes in our corpora. finally, we also plan to train alibert to generate biomedical texts for different purposes. a reasonable amount of computational resources was used to conduct this study, since approximately 20,160 hours of gpu computation were used to create the three pre-trained models presented above. the total environmental cost according to green algorithm (lannelongue et al., 2021)12 is equivalent to 1.45 mwh or 71.11 kg co2e. this computational cost and environmental impact should be taken into consideration when training such a model."
25,"limitations the authors want to use the opportunity given by this column to highlight the fact that the definitions generated by this procedure do not all meet the standards required for presentation to users, or for reasoning-required scenarios, due to their imperfect quality. we release this dataset for building retreival-based systems, and evaluate large biomedical language models on the definition-generation task (and eventually for low-rank finetuning of existing language models). in addition to the imperfect quality of the generated definitions and the presence of hurtful definitions in the dataset, it might also be useful to consider the bias induced by the choice of snomedct as our source of knowledge. while extensive, snomedct does not cover all possible relationships between concepts, and by biasing the output towards relationships present in snomedct, we might perpetuate existing biases in the data. another limitation is that we only evaluate the generated definitions on three metrics, but more could be relevant depending on the application. finally, our rating of what is considered acceptable insight was biased towards what could possibly be condensed in short definitions (49 words on average), but longer definitions might sometimes be required to express the full range of nuance required by biomedical concepts. it is however difficult to estimate the value of omitted information."
26,"limitation the model has achieved a significant performance boost. however, the trade-off is the computational time. due to using two forward passes, the model requires more time to generate the results compared to the other models."
27,"limitations while interdapt can be used as a strategy to reduce the negative impact of weak labeling in realworld use cases, it is difficult to understand the magnitude of performance improvements that can be achieved using interdapt as these improvements are highly dependent on how noisy the target sub-domain datasets are and how robust the target entity labels are. despite potentially reducing data annotation costs, interdapt still has data requirements that are domain-specific to some extent. while such data can be obtained from publicly available datasets, those tend to be less noisy than real-world data. as this work does not explore the impact of the amount and nature of noise in data, it is unclear at this time how this framework would perform when cleaner datasets are used in prior stages of training."
28,"limitations, conducting this evaluation does not lead to any unwanted biases. only the publicly available academic datasets are used that did not require any licensing. thus, no personally identifiable information has been used."
29,"limitations we have applied our neighborhood knowledge graphs to the pubmedbert and pubmedbert+bran models and show the effectiveness of the graphs on the pubmedbert model. we have not deeply investigated how our approach cooperates with other enhancements, and the performance is lower than the state-of-the-art model (wang et al., 2022)."
30,"limitations this paper shows that nar generation can be achieved from publicly available plms parameters. however, there is still room to be validated for additional pre-training. the performed pre-training in this study is small, and further investigation of whether the training steps and the size of the corpus are sufficient, and whether the self-training tasks other than permutation language modeling are effective."
31,"limitations in this study, we used entity type markers to enable a qa model to output multiple answers for re for multiple entity pairs that share an entity in a sentence. however, we have not evaluated other qa models that can output multiple answers. in addition, since entity type markers are incorporated before and after entities, it is difficult to apply the method to nested entity pairs. qa-based re methods are not computationally efficient since they require multiple qa processes to extract the relation for an entity pair. furthermore, the question templates in qa-based re affect the performance of re, but in this study, only one template set was used for evaluation, and there is room for improvement. direct comparison with existing sota (weber et al., 2022) is not performed because of the unavailability of the original drugprot test data set. in addition, the sota model is an ensemble of 10 pretrained language models, and we cannot directly compare the performance in a fair setting."
32,"limitations our proposed model has three limitations. firstly, because of the large size of the literature graph in this study, representation learning is performed on the literature graph using transe, but in fact, there are relations in the literature graph that cannot be represented by transe as shown in section 5.1. to overcome this issue, more expressive methods such as rotate (sun et al., 2019) and gcn (kipf and welling, 2017) could be investigated. these methods are expected to be able to represent complex relationships associated with multiple types of external information. secondly, our model adds a representation of the literature graph to the target text, so longer sentences require truncation of textual information. thirdly, we have not analyzed the results in detail and how the proposed method positively and negatively impacted document classification. we leave these limitations for future work."
33,"limitations we propose welt as an approach to address the class imbalance problem. we have only evaluated welt for bioner tasks, although we hypothesize that it can be adapted to any application / domain that has skewed dataset. we further want to point out that our method was only evaluated on english datasets. yet, we argue that it can be applied to other languages as well. finally, we have not assessed welt’s performance on larger training datasets (i.e., more than 30,683 training examples)."
34,"limitations such as time and resource constraints. however, incorporating the expertise of medical professionals during the development process may provide valuable insights into the clinical implications of the generated summaries, resulting in the creation of more clinically relevant and useful models. given the effectiveness of incorporating human feedback in various nlp tasks (ouyang et al., 2022), we recommend future research to explore the performance of involving medical experts in the development and evaluation through a human-in-the-loop approach."
35,"limitations there are a few limitations in our work: first, the rules developed are totally based on frequency filtering and not further checked by medical experts, we are not sure whether there are any hidden template or patterns in the clincal note summary. second, due to time limitation, we did not conduct second time pre-training for the language models, t5 was originally trained from generic text of which the genre is quite different from the clinical domain. 765 training examples may not be enough for the model to learn. third, we would like to give a full test of the more recent large language model (e.g. chatgpt9), but we cannot fine tune it with the open free api."
36,"limitations during the manual labeling process of the notes, we searched for keywords to select for the namedentity-recognition and compared them to those appearing on the summary. for the system to work effectively, the list of problems and diagnoses should appear in its entirety in the original text columns. however, this was not the case. most of the examples had summaries with additional diagnoses and problems with words that could not be extracted from the original text. in addition to these cases, having an inconsistent use of acronyms leads to different versions of the same term in the original text and the summary. finally, when comparing both lists of tokens, the items found in both segments had a distinct order of appearance, having examples score low for only counting one of them as a match in rouge-l, where the list of words is the same but in a different order."
37,"limitations the proposed model is computationally demanding. recent work on parameter-efficient fine-tuning methods, such as lora (hu et al., 2022), suggests that they can significantly reduce the number of trainable parameters at a minimal performance cost, which may help further democratise the development of domain- and task-specific models. in addition, as we continued to pretrain, to obtain the pulsar models, their tokenizer was inherited from corresponding flan-t5 model. thus it does not contain domain-specific terminology, which may be a limitation in terms of representation density (i.e. frequent clinical terms may be split in multiple rare sub-tokens)."
38,"limitations the pre-training methodology used in this work applies a masking process at the sentence level that requires scoring the relevance of each sentence within the text. therefore, this implies additional computational costs, limiting the scalability of our approach. due to time restrictions, the appearance of hallucinations in the generated radiology reports by our models has not been measured. it would be necessary to quantify this aspect because of the criticality of the domain of use in future works."
39,"limitations the limitations of the presented system derive directly from the pre-trained models they are based 3https://huggingface.co/chizhikchi/ sci-five-radsum23 on. flan-t5 was pre-trained on a large-scale text dataset and was not assessed for existing biases. as a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data (raffel et al., 2020). thus, it is not recommended to utilize such systems in any application without first conducting a thorough evaluation of the safety and fairness concerns that are specific to the particular application in question."
40,"limitations though our systems achieve promising results in solving the summarization task for long documents, we believe that we can gain more improvement with the following further considerations. the current explicit key information selection strategy is somehow heuristics. we can alternatively try extractive summarization methods. also, this lay summarization is interesting which helps nonexpert readers can understand scientific articles. however, specific strategies focusing on this aspect such as using non-expert vocabulary, or mapping to general knowledge, are yet applied. some minor parameters such as the sequence lengths (9k, 12k), or tuning the sota brio model also need to be investigated more deeply."
41,"limitations our model demonstrates commendable performance in terms of readability and relevancy, but it falls short in the factuality metric, this is one of the potential areas for improvement. given more time, one of the directions that we might have explored, is the factuality based re-ranking which considers factuality as metric for comparison, instead of rouge scores, or considering both scores giving then certain weights. we have made substantial efforts to improve efficiency and reduce memory requirements, but large language models still impose significant demands on time and computational resources, which remains a limitation of our current work. additionally, the constraint of a token threshold set at 512 posed challenges in our work. these limitations highlight areas for future research and development."
42,"limitations this study is our first exploration of this research topic. we use pre-trained primera, pegasus, and bart-longformer models and fine-tune them for technical abstract and lay summary generation. novelty in the techniques is the main limitation. we downloaded all pre-trained from hugginface, which may be inappropriate for this summarization task. how to determine an excellent pre-trained model doesn’t include in this study. in addition, we only fine-tuned all pre-trained models over the task-given datasets without collecting other related summarization data to enhance the model performance."
43,"limitations this study is constrained by limited input token length due to hardware memory limitations and lengthy training times. even with the lsg attention mechanism’s efficiency, this inadequacy persists in both subtasks. longer token length could improve summary relevance and factuality. particularly in the second subtask, where the abstract is absent, this poses a challenge in generating a summary from sections with high information density."
44,"limitations the performance of the model heavily relies on the quality and relevance of the keywords and headings provided in the dataset. if the dataset lacks rigor during the column labeling stage or if the information in these fields is inadequate, it can lead to a decrease in the overall effectiveness of the model. moreover, incorporating additional prompt tokens may introduce length constraints, potentially resulting in article truncation and the loss of crucial information."
45,"limitations our best results were obtained using a few-shot prompt of the text-davinci-003 model from openai. while the technical barriers to this method are very low due to the ease of use of the model api, the cost of querying the api can become prohibitively expensive and this limited our own experiments with the model10. this cost would rise significantly more if the text-davinci-003 was finetuned to perform these lay summaries. if this is a concern, then using the open-sourced biogpt models may be beneficial. it should be noted that performing the fine-tuning process is itself expensive and requires access to high-end gpus. practitioners should investigate parameter-efficient finetuning techniques (hu et al., 2021) if access to these gpus is an issue."
46,"limitations despite our efforts to enhance the efficiency and minimize the memory cost of our models, large language models still demand considerable time and memory resources, which remains a limitation of our work. given sufficient time and computational resources, we could explore the possibility of increasing the batch sizes and running additional epochs to further optimize the model’s performance. while our final system excelled in relevance and factuality aspects, it was relatively poor on readability, which represents a potential area for improvement. to improve the robustness of the model, given additional time, we would use a combination of machine learning and list-based approaches to identify arcane words and technical terms and substitute them with their easy-tounderstand synonyms. with these procedures, we believe that we can make our summaries more readable."
47,limitations we can improve our model in the future by using the keywords given in the dataset to enhance the quality of the summaries generated by using keyword embedding in addition to the fine-tuned model.
48,"limitations there are some limitations to our approach. firstly, we utilized the bart model, which has a maximum sequence length of 1024 characters. therefore, we had to divide the articles into sections and process them one by one, which could result in less coherence in the final summary. additionally, we worked on google colab, which has a time and memory limit, and we encountered occasional connection issues, which could slow down our progress."
