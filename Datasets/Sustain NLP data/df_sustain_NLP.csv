,Text
0,"While our approach is effective at compressing models, it is not the most efficient. In order to discover the most optimal compression approaches and evaluate their performance performed hundreds of experiments. As a result, scaling our approach to every novel language understanding language model is not tractable. Another limitation of our work is we did not track the complete compute utilization of our entire experimentation process but we can provide some estimates. Experiments in pruning during fine-tuning leveraged a
single V100 16 GB GPU and took approximately 14 hours per experiment. The pre-training of structurally pruned models with knowledge distillation required 4 A100 40GB GPUs for approximately 72 hours. Pruning during pre-training with Knowledge distillation required approximately 100 hours on the same setup. Task-specific fine-tuning happened on a single V100 16GB GPU and depending on the size of the task was anywhere from a few minutes to 20 hours. Based on all of our experiments we estimate 400 V100 hours of pruning during fine-tuning, roughly 16,000 A100 hours10 for pretraining, and assuming an average of 10 V100 hours per sparse transfer run, a total of 4000 V100 hours for sparse-transfer and sparse-transfer with quantization."
1,"While much of our work has focused on showcasing the broad usability of compressed language models, they are not without fault. While our experiments focus on the compression of RoBERTa, the size of its training dataset makes complete exploration of the ability of pruning during pretraining somewhat limited. The work in the paper shows the ability to compress RoBERTa on a smaller pretraining dataset but does not contrast it with the impact of compression on the full dataset.
A second limitation of our work is the high computational demand required for creating public domain sparse language models. Despite amortizing the cost of compression to a few pretraining training regimes, the reduction of other language models like ALBERT (Lan et al., 2019) or XLMR (Conneau et al., 2019) require completely new training, pruning, and transfer experiments."
2,"While our work makes a broad study on how to improve model efficiency our scope is limited. Our work is limited to the usage of BERT-base and it is not clear how our compression approaches scale to more varied architectures like the sequence-tosequence models used by DocT5 (Lee et al., 2022) or more optimized models like RoBERTa (Liu et al., 2019) or compressed models like MiniLM (Wang et al., 2020)."
3,"Due to the limited resources, we could not experiment with this approach for larger language models such as roberta-large and bert-large. It would be interesting to investigate the performance of ADEPT with larger LMs.
In addition, we only evaluate the ADEPT approach on seven downstream tasks. It would also be interesting to test it on more broad natural language processing tasks, such as information extraction, natural language generation, question answering, and so on.
Broader Impact
As discussed earlier, fine-tuning large pre-trained models for downstream tasks can be really expensive. The ADEPT approach can help AI practitioners to assess the abilities of LM without using a lot of resources. This approach of prompt tuning can also help smaller end users to take advantage of harnessing the power of LM with the minimal resources they have. It can be used by social scientists, Non-profit organizations, etc. to create a positive impact in society in spite of limited computing resources."
4,"We present KG-Flex, an end-to-end model that can access new relations at test-time without retraining. Our current KG-Flex model does not perform entity resolution, and so we rely on resolved entities provided by the datasets. However, resolved entities may not always be available, so tools such as automatic entity recognition may be necessary. While it is possible for end-to-end models to jointly learn to resolve entities in questions before relation following (Saffari et al., 2021), we consider this outside the scope of this focused work.
Additionally, KG-Flex is limited in the kinds of reasoning it can do over a Knowledge Graph. Currently, KG-Flex only performs relation following, so it cannot handle questions which require complex reasoning like counts, comparatives, min/max, etc, such as “Who is the tallest NBA player?” We hope to address this in future work.
Further, we test KG-Flex on popular datasets representing possible real human questions. However, we do not deeply investigate the semantic properties of these questions. Notably, McKenna and Steedman (2022) show that searching for similar relations in embedding space (as done in KGFlex) may work better for paraphrastic inference, and only in certain cases for directional inference where semantic precision matters, e.g. DEFEAT entails PLAY, but PLAY does not entail DEFEAT. We
leave deeper investigation of KG-Flex semantics and edge cases to future work."
5,"Our proposed method MANER for improving NER is best suited for low-resource settings. As discussed in Section 3.3, we measured the effectiveness of MANER in situations where more training data is available and found that MANER boosts F1 performance over the SNER baseline until about 400 training examples, and then both methods perform similarly. The result demonstrated that MANER is best suited for extreme low-resource languages and rapid prototyping because it is easy and cost-effective to obtain very few human annotations to achieve significant performance improvements.
We base the experiments in this paper on a widely adopted model, XLM-RoBERTa, pretrained on multiple languages. It is possible that the empirical conclusions we draw from the observations do not generalize to other pre-trained models."
6,
7,"The limitations of this work are as follows:
• English. Our work builds autocomplete models for English language only.
• Accuracy-memory tradeoff only. Our work primarily focuses on deploying models on lower-end edge platforms where memory, as opposed to latency, is the major bottleneck. Hence, our methods may not improve the accuracy-latency tradeoff, which is a focus for future work.
• WikiText-103 dataset Our work explores only WikiText-103 dataset for creating broad prompts. In the future, we will study
6We provide a qualitative analysis of the baseline and proposed character models in the Appendix A.10.
other datasets (e.g., 1 Billion Word Language Model benchmark (Chelba et al., 2013)) that explore the full range of low-frequency prompt patterns, which can arise in real-world situations.
• Transformer-XL architecture Our work studies only Transformer-XL architecture to build word based and character based autocomplete models. In the future, we will study other popular architectures (e.g., GPT-2 (Radford et al., 2018)) to see the generalizability of proposed techniques."
8,"Limited Experimental Scope Our study’s experimental scope was limited to testing distilled student models against a single teacher model. A more comprehensive evaluation would involve multiple teacher models of varying sizes, fine-tuning tasks, and datasets. Additionally, in our experiment with DistilBERT-based student models, incorporating more checkpoints would enable a more thorough comparison across different factors.
Unexplored Embedding Size Variations We kept the embedding size (768) consistent across student models to maintain variable consistency. Future research could investigate student models with different embedding sizes to determine if the observed trends hold true across models of varying widths.
Lack of Error Analysis A common distillation limitation, as noted by Hooker et al. (2020), is the considerable performance decline for certain data subsets. In our study, we couldn’t conduct a thorough error analysis due to the lack of appropriate tools for comparing individual data points in retrieval tasks."
9,"limitations as described in section 1, the purpose of this study is to relax the existing parameter sharing strategy which shares the parameters of one layer with all layers (dehghani et al., 2019; dabre and fujita, 2019; lan et al., 2020). experimental results indicate that the proposed simple parameter sharing strategies can be a better alternative to the existing method. as many studies on neural methods, this study also depend on empirical observations. in other words, this study lacks theoretical justifications for proposed parameter sharing strategies. we conducted experiments on various situations. we mainly focused on sequence-to-sequence tasks and trained each model from scratch. our conducted experiments indicated the efficiency of the proposed strategies but we did not conduct experiments on the pre-training and then fine-tuning configuration such as comparison with bert (devlin et al., 2019) due to the limitation of our computational budgets. thus, it is difficult to claim that the proposed strategies are also more efficient in such configuration. in addition, we have to investigate the effectiveness in a more realistic situation. for example, we will investigate the performance of the combination of our proposed method, which is the parameter efficient way for internal layers, and a parameter efficient embedding such as takase and kobayashi (2020). through experiments in various configurations, it is difficult to conclude which strategy is the best. experimental results imply that the best strategy depends on the task and transformer architecture (post-ln or pre-ln). such phenomena are reported in previous studies (press et al., 2020; gulati et al., 2020). in fact, the architecture explored by press et al. (2020) is better in the language modeling task but ineffective in the machine translation task. since it is intractable to investigate a tremendous amount of possible parameter assignment way due to the limitation of computational budgets, there might be a superior way to three simple strategies proposed in this paper. however, we emphasize that all our proposed strategies are more efficient than the universal configuration. because the purpose of our experiments is not to detect the best parameter sharing strategy but to indicate that our proposed parameter sharing strategies are more efficient than the universal configuration, we consider that our conducted experiments are sufficient to verify our claims."
10,"limitations this study is limited to classification tasks with an encoder architecture, short texts (e.g. utterances), datasets with at least several thousand examples, and a relatively low amount of mislabeled data. in theory, we could apply our method to longer texts, but our takeaways might not directly apply. for similar reasons, our"
11,"limitations this work has several limitations. first, we only experiment on english datasets. it would be interesting to explore whether the general patterns hold for non-english languages with different structural properties. moreover, we only explore incorporating hard constraints for decoding with local models at testing time. exploring more applications of structural constraints, such as learning with constraints, or incorporating other types of constraints, such as soft ones, would be promising future directions. finally, we only explore three simple sentence-level structured prediction tasks, while extentions can be made to more complex tasks with larger output space, such as text generation or document-level information extraction, where constraints may play more interesting roles."
12,"limitations the two videoqa datasets used in experiments are associated with relatively short videos. therefore it would be better if more experiments could be conducted on videoqa datasets with long videos to verify the effectiveness of our approach on a wider range of videoqa tasks. although the proposed approach in this paper can also be used in other video-language tasks, our experiments focuses on a specific video-language task - videoqa. experiments on more video-language tasks are needed to show that our approach are also effective in other video-language tasks."
13,"limitations despite the promising results of our iml pipeline for image captioning, our work has some limitations. firstly, the experiments were conducted on a domain-specific dataset, vizwiz, and may not generalize to other datasets or domains. secondly, our approach may not be suitable for scenarios where user feedback is sparse or unreliable, as the effectiveness of iml heavily depends on the quality and quantity of the feedback. thirdly, our use of episodic memory to retain knowledge from previously seen clusters may not scale well to smaller datasets and other methods may be required. lastly, our approach does not address the challenge of bias in the data, which can lead to biased models. ethical statement as of now, we do not see ethical concerns with the study presented in this paper. we used a dataset that is publicly available. the study is currently not applied to human subjects with personal data; in this case, the use of user feedback in the training process could potentially introduce biases if the feedback is not diverse or representative of the population. lastly, our approach may be used to develop image captioning models that generate harmful or inappropriate content, such as captions that perpetuate harmful stereotypes or stigmatize certain groups of people."
14,"limitations one limitation of our study is that, due to computational constraints, we use what are now considered as relatively “small-sized” models and corpora, exclusively focusing on the english language and generic domains such as wikipedia articles and books. the generalizability of our findings to larger corpora, other languages, or specific domains such as medical texts warrants further investigation."
