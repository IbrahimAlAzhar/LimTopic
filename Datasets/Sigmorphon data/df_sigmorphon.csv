,Text
0,"One of the limitations of our work is the unavailability of context data and unavailability of phrasebased annotations for all languages except Hindi. The unavailability of phrase-based annotations prevents the usage of universal tags because markings that are present on a single word in highly agglutinative languages like Marathi or Tamil get expressed on 2–3 words in isolating or fusional languages like Hindi or Bengali (where markings are present on post-positions). The benefits of using phrase level morphology over token level morphology have been discussed in Goldman and Tsarfaty
(2021). For example, the word ‘sochega’ in Hindi will have MSD tags: future tense and male gender, while in English, it would take two words, ‘he will think’ to express the same amount of morphological information. The presence of contextual data can also help to disambiguate MSD tags. The other limitation of our work is the mismatch between the languages for which pretrained models (especially encoder-decoder models) are available and the languages for which we have the annotated data. For example, UniMorph dataset contains annotated examples for Assamese and Sanskrit, but we do not have multilingual pretrained encoder-decoder models for these languages."
1,"While this methodology does not require the large amount of text data utilized by more advanced models, it does depend on access to grapheme/phoneme alignment information for all input words. This does limit the usefulness of the model for languages with little linguisticallytagged data available, though the success of the small Deri & Knight corpus does indicate that the model architecture can be made to function effectively with a limited amount of annotated data."
2,"While the WFST model didn’t perform very well overall, our sense is that it is worth pursuing further. Specifically, there are several moves worth exploring.
First, we should move to a compiled system so that we can test the ""individual variable"" models more thoroughly.
Second, we should try models where we set the variable weights by training, rather than naively in advance.
Third, in an individual variable setting, it would be promising to weight the variables by locality. Specifically, do mismatched variables have more effect when they are closer to where the changes happen? Similarly, we might adjust the granularity of the variables as a function of position, with
single-character variables sometimes and variable spans in other cases.
Fourth, We had individual WFSTs for each lemma, but with a compiled system it makes sense to put them all together into a single WFST."
3,"Due to time constraints, it was not possible to perform a satisfactory grid search on the large combinations of training hyperparameters, preprocessing techniques, and stem approaches. It is possible that a more optimal system is possible, but we were unable to find it."
4,"limitations the results of this work may be limited in reliability and replicability due to some hard-to-avoid aspects of the low-resource setting. our numerical results have low statistical power, as illustrated by the wide bleurt confidence intervals in tables 4 and 6. without a large test set, most differences are not statistically significant at the accepted level. they should be treated as trends which can motivate further investigation rather than solid"
5,"limitations the method proposed in this paper has some limitations: (1) this model learns not only the features of words, word formation, and context but also the relationships between labels. when there is too much noise, it can disturb the relationship between labels. (2) the model needs more training time than other models to learn pretrained label relationships."
6,"limitations one of the limitations of our dataset is the lower number of wordforms belonging to a closeclass partofspeech as we chiefly focus on nouns, verbs (transitive and intransitive) and ad jectives. on the other hand, we only include in flectional morphology without paradigms of word formation. furthermore, we only address the mor phology of the standard variety of central kurdish, i.e. that of sulaymaniyah. we plan to extend our work to include other varieties of central kurdish along with derivational morphology. given that central kurdish lacks a treebank, it will be com pelling to bridge central kurdish morphology and syntax as well. another limitation of the current work is due to the unimorph schema. using the lgspec tag is not recommended for features that are found across languages but for those that are limited to specific languages (sylakglassman, 2016, p.30). given that some of the features of central kurdish, such as izafe and pronominal copula, are also found in other closelyrelated languages, we believe that the current schema should be extended to use specific tags for such features or a better schema, akin to guriel et al. (2022)’s hierarchical model, is needed for languages with rich morphology like kurdish."
7,"limitations the speech recognition used was the wav2vec 2.0 model. some of the errors may have been influenced by the fine tuning of the final layers; this could lead to errors being corrected by the language model. furthermore, wav2vec 2.0 produces character output which we transformed to phonemes using a grapheme-to-phoneme tool; this will lead to some loss in the variation. these limitations can be overcome to some extent by using a wav2vec 2.0 phoneme model which we plan for our next experiments. we have only worked on french to date, even though we believe that the method is applicable to other languages. finally, the experiments were done only on timit. while this is a balanced dataset, use of other datasets will likely lead to better insights."
8,"limitations and ethical considerations. first, the system’s performance is constrained by the use of automated systems to produce pseudo-parallel data. errors in translation and morpheme labeling on the high-resource side propagate to the output and cause mistakes in target side labeling. we have not yet performed the extensive error analyses needed to understand how much error propagation might be affecting the system. second, we have not yet tested the system in an actual documentation project. when working on nlp with endangered and/or indigenous languages in mind, there is a clear risk of perpetuating existing oppression (bird, 2020; schwartz, 2022). we hope to avoid some of these harms by using data from a wide range of non-threatened languages first, waiting to involve language community members and documentary linguists until we have a system with good enough results that we expect it could actually be helpful in real world contexts. we have already developed collaborations with several speakers of endangered languages and linguists working on documentation projects, and we look forward to continuing this work with their guidance and involvement."
9,"limitations a limitation of this study is the fact that the concreteness ratings of brysbaert et al. (2014) are curated solely from self-identified u.s. residents. and the affectiveness ratings of warriner et al. (2013) are solely curated in english. as such, there is a risk of an anglocentric bias in the created dataset. nonetheless, the goal of this study is to explore the potential of leveraging colexifications to bootstrap cross-lingual datasets in as many languages as possible, including a lot of low-resource languages."
10,"limitations a major limitation of the current work is the absence of gold alignments for evaluating the different methods. gold alignments would also enable us to provide more reliable estimates of the prevalence of the evaluated phenomena in the three datasets. we are not aware of any other similar corpora that come with gold character alignments. the work of wieling et al. (2009) uses word lists, not entire sentences. furthermore, our work currently only covers european languages in latin script. some of the presented techniques also assume identical writing systems in the transcribed and normalized layers. our setup may therefore not generalize well to the dialectal variation and writing systems present in other parts of the world. for example, the v-c proportion cannot be easily determined in scripts that do not specify all vowels. although there is an extensive amount of research in particular on arabic and japanese dialects and their normalization (e.g., abe et al., 2018; eryani et al., 2020), we currently limit our experiments to data written in latin script."
11,"limitations the small amount of training data provided in this shared task poses a challenge for models that need large amounts of data to reliably learn linguistic patterns. while we did not employ any data augmentation techniques, we suggest future work to train the models on all the possible feature combinations (weighted with the probabilities as the experimental data) for the stems in the two corpora. owning to the lack of time and computing resources, we did not fully optimize our transformer models and we did not fully utilize and explore i) speaker-specific information, especially for the transformer models, ii) token frequency information in the corpora, as we assumed extension of morphological patterns is based on type, not token, frequency (bybee, 2001; pierrehumbert, 2001). furthermore, we did not experiment with training models with either high-frequency, low-frequency and pseudoword items. it is possible that some speakers’ high/low/pseudoword items would be better served as part of the training set. the ldl model in section 5.1.3 was not able to evaluate the experimental items due to the unattested triphones. this shortcoming can be mitigated by using phonological features (tang and baer-henney, 2023)."
