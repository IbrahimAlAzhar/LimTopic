,Text
0,"Though our ultimate goal is to build a versatile QA system that can handle all types of questions, our benchmark mainly focuses on extractive questions – those can be explicitly answered by copying from a document in the knowledge source. We start from extractive QA because they cover a wide range of real-world questions and are easier to be automatically evaluated.
Although we addressed the issue of long-form QA evaluation with human evaluation and a range of automatic evaluation metrics, there is still much room for improvements in terms of evaluation of long-form text — human evaluation can be expensive and non-reproducible while current automatic metrics are not without faults. We encourage future work exploring various evaluation strategies of long-form QA.
Furthermore, all questions are in English and possibly collected from English-speaking users. We also use the English Wikipedia as our knowledge source. Thus, our models and dataset may under-represent the non-English speakers."
1,"In this paper, we present a comprehensive framework for measuring the quality of a dialogue system dedicated to activities of daily living assessments. We have created a new high-quality dataset of human-written questions and answers with corresponding profile information. We are currently working on expanding the dataset by adding more profiles and removing any factual inconsistencies resulting from human error. Although more complex models showed better query classification performances, we need to consider the trade-offs between model size and generation time in the deployment environment to ensure a smooth user experience. We also identify areas where LLM performance can be augmented by a knowledge base filled with human written natural language facts, and that this augmentation need not come at a penalty to sensibleness, specificity, or the realistic quality of conversation. General conclusions based on our initial work here may not be possible given the limited number of evaluators and small amount of evaluated dialogues, and this is a major limitation of our contribution. Future work is needed to develop a more robust and replicatable evaluation framework, especially to perform evaluations of long and complex conversations like the type that assessors perform in the field. Such an evaluation will need to include larger numbers of human raters to improve the statistical power of the surveys. Recent automatic evaluations may also help improve development efforts, as a sufficiently powerful LLM such as GPT-4 may be able to monitor the chatbot for regressions in its ability to speak fluently, sensibly or specifically. This assessment, known informally as the ""Vicuna Assessment""(Chiang et al., 2023), cannot give an evaluation of the chatbot’s fit-for-purpose, but could be used to compare short conversations from several versions of the same chatbot. This could free up
more human resources to evaluate the knowledgegroundedness and fit-for-purpose of future versions. In addition, given more computing budget and more time to engineer prompts, larger language models beyond LLaMA 7B could be further studied or fine-tuned while experimenting with fine-tuning datasets and process. There are also many thresholds and parameters that could be further tested in the development of the knowledge-grounding system, wherein similarity measures inform the system’s decision to answer using a generative model versus responding with language directly from the knowledge base."
2,"The limitations of this study are primarily due to budget and credit constraints. Consequently, our query rewriting observations are based on a sample size of 2000, leading to limited generalizability of findings. Another limitation of the limited resources was the limited context size of ChatGPT and the relatively long nature of the questions in our dataset. Hence, we could not test prompting ChatGPT with in-context examples for better query rewriting performance. Hence one potential future work is testing the performance of query rewriting using in-context examples. Finally, ChatGPT architecture is not open source, preventing us from testing advanced prompting methods. Hence another future work would be to test query rewriting using open source models and with advanced finetuning methods like Prefix Tuning (Li and Liang, 2021) and Prompt Tuning (Lester et al., 2021).
The study is also subject to the risk of ""hallucinations"" in ChatGPT’s responses, which may lead to imprecision in query rewriting. The study suggests further investigation into these issues to improve the accuracy and reliability of the results. We recommend further investigation into these limitations and any potential societal biases present in our dataset to enhance the reliability and performance of query rewriting."
3,"The main limitation of our work is its focus on English datasets. While this was due to their popularity and extensive usage (and our limited language skills), it overlooks datasets like DuConv (Wu et al., 2019) and NaturalConv (Wang et al., 2021) (both Chinese) which employ more explicit annotation instructions regarding dialog ‘path’ and topic transitions. Studying the way these restrictions affect conversational attributes, is necessary for a more comprehensive understanding of the problem.
Another limitation is the lack of an empirical investigation on how/if these artefacts and biases affect the final objective of KGD modeling, i.e. response generation. This of course is not easy in the absence of a less biased dataset, but synthetic datasets –which have become much better in quality and flexibility thanks to large language models– can probably provide reliable estimations, which we plan to explore in future studies."
4,"limitations this work has potential limitations: • we found that on the figure 3 and 4, the entailment of the methods after applying multiple position embedding (red lines) are sometimes lower than origin methods(blue lines). this is not meet our expectations since we don’t want our method to decrease performance. in our opinion, we think the reason might be the embedding method has never been seen before during the pretraining of models, which requires the model’s additional efforts to adapt the embedding, thus hurts the performance.. we leave it as future work to be improved. • we also found that the multiple position embedding does not work very well to alleviate the order effect in the lm loss-only settings4. we have discussed this in previous sections. since lm loss only does not help the model distinguish which parts in the input sequence are knowledge set and thus treat them the same as history. the multiple position embedding will not be trained finely to help the model distinguish. we also left this as a future work to be improved."
5,"limitation of our contribution. future work is needed to develop a more robust and replicatable evaluation framework, especially to perform evaluations of long and complex conversations like the type that assessors perform in the field. such an evaluation will need to include larger numbers of human raters to improve the statistical power of the surveys. recent automatic evaluations may also help improve development efforts, as a sufficiently powerful llm such as gpt-4 may be able to monitor the chatbot for regressions in its ability to speak fluently, sensibly or specifically. this assessment, known informally as the ""vicuna assessment""(chiang et al., 2023), cannot give an evaluation of the chatbot’s fit-for-purpose, but could be used to compare short conversations from several versions of the same chatbot. this could free up more human resources to evaluate the knowledgegroundedness and fit-for-purpose of future versions. in addition, given more computing budget and more time to engineer prompts, larger language models beyond llama 7b could be further studied or fine-tuned while experimenting with fine-tuning datasets and process. there are also many thresholds and parameters that could be further tested in the development of the knowledge-grounding system, wherein similarity measures inform the system’s decision to answer using a generative model versus responding with language directly from the knowledge base."
6,"limitations while our proposed method demonstrates promising results and outperforms several state-of-the-art techniques, it is important to acknowledge certain limitations. • dependence on pre-trained llms: our method relies heavily on the pre-trained llm’s quality and the knowledge it has captured. as a result, any biases, inaccuracies, or limitations present in the llm may directly impact the performance of our evaluation metric. • lack of diversity in the dataset: the fed dataset, which we use for evaluation, is primarily derived from conversations with the meena and mitsuku chatbots. consequently, it is possible that our evaluation might not have better correlation with human ratings for other dialogue systems or more diverse conversational contexts. • adaptability to new evaluation dimensions: our method currently focuses on eight turnlevel metrics. extending the method to incorporate additional or novel evaluation dimensions might require further investigation and calibration. • computational cost: the current implementation of our approach is around twice as slow as the baseline nll-based method due to multiple times of the inferences of the language model. the efficiency of the implementation can be improved in the future by re-using the log-likelihood of the dialogue history. • subjectivity in human judgments: our evaluation metric’s correlation with human judgments serves as a key performance indicator. however, human judgments are inherently subjective, which could lead to inconsistencies or discrepancies in the evaluation results. despite these limitations, our proposed method presents a significant step forward in dialogue evaluation, offering a model-agnostic, unreferenced, and training-free approach that captures the human and the system interaction. future work could address these limitations and explore additional dimensions of evaluation, further refining the method and its applicability across a broader range of dialogue systems and text evaluation systems."
7,"limitations of this study are primarily due to budget and credit constraints. consequently, our query rewriting observations are based on a sample size of 2000, leading to limited generalizability of findings. another limitation of the limited resources was the limited context size of chatgpt and the relatively long nature of the questions in our dataset. hence, we could not test prompting chatgpt with in-context examples for better query rewriting performance. hence one potential future work is testing the performance of query rewriting using in-context examples. finally, chatgpt architecture is not open source, preventing us from testing advanced prompting methods. hence another future work would be to test query rewriting using open source models and with advanced finetuning methods like prefix tuning (li and liang, 2021) and prompt tuning (lester et al., 2021). the study is also subject to the risk of ""hallucinations"" in chatgpt’s responses, which may lead to imprecision in query rewriting. the study suggests further investigation into these issues to improve the accuracy and reliability of the results. we recommend further investigation into these limitations and any potential societal biases present in our dataset to enhance the reliability and performance of query rewriting."
