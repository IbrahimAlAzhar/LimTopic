,Text
0,"Testing Necessary, But Not Sufficient Conditions For Tutoring With LLMs. In this paper, we test the abilities of LLMs to perform the functions present in Intelligent tutoring systems, namely generating explanations and corrections. There are also other desirable properties, like the ability to answer direct questions from a student or the ability to present content engagingly, which are beyond the scope of this paper. Indeed, those properties are some of the areas where LLMs probably excel relative to traditional ITS. We have only explored a necessary condition – are models able to reliably teach – not a sufficient set of conditions for the evaluation of tutoring using an LLM.
Focusing On Mathematics. In this paper, we focus on tutoring in rudimentary mathematics. While this is useful – it is a necessary condition for a useful tutoring system, especially because arithmetic skills are used in almost all domains of learning – there are many other domains to which we might want to apply tutoring. LLMs may have greater or lesser aptitude in these domains than in arithmetic. Evaluation at the level of gradeschool mathematics tells us that these models are still error prone, but does not necessarily tell us how close they are to usefulness in tutoring other subjects (either more advanced mathematics or orthogonal subjects like history or writing).
Generalizing Text vs Code Results. We aim to examine the differences in ability of code scratchpads and text scratchpads for the purposes of tutoring. While this paper provides evidence in that direction, we only compare two GPT-3 models: text-davinci-002 and code-davinci-002. The amount of manual effort required to evaluate explanations and correction limited the number of comparisons we could conduct, as did the limited number of highly performant code/text generating models."
1,"In this work, we explored question generation for computer science textbooks. We have not yet explored a broader range of course subjects, and it may be that the prevalence of computer science knowledge on the Internet, including through forums like Stack Exchange, makes QG easier for this discipline than for others. Furthermore, we examine a relatively narrow range of question types. Other questions –like multiple choice questions, or compare and contrast questions– will require deeper exploration and substantial adaptation of the methodology that we proposed."
2,"While our study provides valuable insights into the use of prompt-based methods with GPT-3 for GEC tasks and its controllability, several limitations should be acknowledged.
Focus on GPT-3: This study exclusively examines GPT-3 as the language model for GEC tasks. While GPT-3 has shown remarkable performance in various NLP tasks, other pre-trained language models, such as GPT-4, may offer different results. A broader investigation that includes other language models would provide a more comprehensive understanding of the applicability of promptbased methods in GEC tasks.
Limited evaluation metrics: The evaluation of GPT-3’s performance and controllability in our experiments mainly relies on quantitative metrics, such as edit distance and task scores. These metrics may not fully capture the nuances of grammatical error correction or the model’s ability to adapt to different learning scenarios. Additional qualitative analysis, along with more diverse evaluation metrics, could provide a richer understanding of the model’s performance and controllability.
Variability in examples: While our study highlights the importance of example selection in fewshot settings, we do not thoroughly explore the impact of example quality or diversity. The effect
of using different types of examples or a more diverse set of examples remains to be investigated, which could further inform the design of effective example sets for prompt-based GEC tasks. By addressing these limitations in future research, we can further advance our understanding of the performance and controllability of prompt-based methods with GPT-3 and other language models in GEC tasks and beyond.
Potential fine-tuning on test data: There is a possibility that GPT-3 has been fine-tuned (instruction tuning) on the test data we are using, which might explain the higher evaluation scores compared to previous research. As this information has not been disclosed, we are unable to verify it at this time. This point should be taken into consideration when interpreting our results."
3,"Our experiments were built on perfect sentence alignments in the original and revised essay drafts, thus the performance could be lower in the real end-to-end Automated Writing Evaluation (AWE) system. In addition, our corpus is small due to expensive annotation processes, which makes it challenging to train or finetune large language models. Also, we only focus on revisions in argumentative writing, specifically, we focus on the evidence and reasoning revisions, however other revisions like claim revisions are not used. Furthermore, the revised drafts were done after providing feedback on the original drafts, which means the revised student essays are likely to follow the instructions in the feedback but we did not use this information for revision quality predictions, which will be used in our future work.
Our proposed Argumentative Contexts (ACs) are generated by ChatGPT which is not free for the whole community. Also, ChatGPT-generated ACs have small randomness, which is also the reason we did 3-seed runs in the experiments. In addition, the ACs are essay-level context which means different revisions in the same essay use the same context. It could be tailored to have sentence-level ACs where each sentence-level revision has slightly different revision purposes, but it would cost more
time and money. Moreover, our proposed zero-shotCoT prompts perform better than Single prompts by small margins in specific cases, which indicates that Chat-GPT is limited to conducting CoT extraction and summarization to handle complex wording and sentence structure. Therefore, we might need to redesign the prompts in our future work."
4,"While our work provides a useful starting point for understanding student feedback, there are limitations to our work. Addressing these limitations will be an important area for future research.
Comments may not reflect real student feedback. The comments in our dataset are from users who have chosen to post publicly on YouTube. Addi-
tionally, the comments may include features specific to this online education setting. Thus, the comments may reflect real student comments from these courses.
There is a selection bias in lecture sources. SIGHT includes lectures that may be drawn from the most successful offerings of that course. The instructional quality may not be representative of typical instruction. Thus, inferences drawn about the instruction should be interpreted with caution, as they might not generalize to other lecture settings.
We analyze only English comments. We analyze only English comments because the lecture content is given in English and the authors are most comfortable with English. As a result, our rubric may not capture the types of feedback from nonEnglish students watching lectures taught in English. In the future, the rubric and analysis should be adapted to account for the multilingual feedback setting.
We annotate a small subsample of the data To assess the validity of the automatic labels, we conduct a diagnostic study on a small, randomly selected subset of the dataset, comprising approximately 2% of the comments. Our work aims to establish a preliminary evaluation of the humanmodel agreement and model annotations, and further validation of the automatic labels is necessary. Future work can focus on acquiring such gold-standard annotations to enhance the quality and reliability of the automatic labels."
5,"Given that the WLAs are calculated as the sum of all the TLAs representing one single word, it is possible that there could be an underlying preference for longer words in the framework. However, multiple tokens in a word could also have conflicting attributions, so it is not entirely clear how this affects the framework. Given the results of this implementation, it could be reasonable to try and calculate the WLAs as the mean of all TLAs instead.
Furthermore, it is reasonable to discuss the consequences of the preprocessing steps being carried out in the experiment. Although such preprocessing steps might increase the IAA measured between the human rationales and model rationales, it is reasonable to question what these preprocessing steps actually result in and their possible value in realworld applications. In cases where the use case is to identify and highlight certain important words, such preprocessing steps might bring a considerable amount of value. However, if the end goal is to represent the model’s attention as precisely as possible, these preprocessing steps might skew the representation of the model’s attention. Consequently, one could argue that there exists a trade-off between usable model explanations, which can be used as an assisting or guiding tool for the human
expert, and explanations that are fair representations of the model’s inner workings. In the case of ASAG, explanations such as the ones created by the presented framework could likely be used as an assisting tool in helping human expert graders find important words or sentences. Given such a framework, the speed of grading could likely be increased without removing the trust of having a human grader making the end decision.
Lastly, it is worth noting that the use of top k sentences should only be seen as a means of calculating IAA. However, in a real-world inference setting, the number of relevant sentences might be dependent on the task as well as the subject. In the case of assisting a human expert in grading, the number of top k sentences might be a parameter controlled by the human expert in order to showcase only the most relevant sentences marked by the model annotations, where the number of relevant sentences might be dependent on the length of the student answer as well as the complexity of the given question."
6,"Our user study tests students on only two short lecture videos which are pre-recorded and carefully edited. Future work should test the efficacy of the summaries under a wider range of conditions: prerecorded videos versus live lectures, lectures and summaries of different lengths, and a wider range of topics and disciplines.
Overall, our experiments compare three different conditions. Adding other conditions might have shed light on the relative value of automatic summaries. For instance, if we limit the time available for participants to prepare before taking the quiz, and at the same time track the amount of time spent on summaries and/or videos, then that could give better insights into how students would utilize the two sources differently with limited time constraints. Finally, we could also contrast the usefulness of summaries versus transcripts."
7,"For future work, we plan to update the annotated corpus and the scope of annotation to paragraphs and/or whole documents. In this study, we opted for the minimal context approach for practical reasons, such as budget, time constraints, and copy rights of
the source corpora. The minimal context approach allowed the annotation sample to represent as many writers as possible for better generalization with relatively small sample sizes. However, future research should use longer units of analysis to enhance the quality of manual annotation. Despite this limitation, the results of the current study indicated that the current approach is a promising direction for further research on automated analyses of rhetorical features."
8,"Our experimental results should be interpreted with the following limitations in mind. First, our experiments involved relatively small datasets in English only. The performance of the model should also be evaluated on other languages and larger datasets. Second, the improvement observed in our best models depends on both the efficacy of the linguistic features and on the strength of the neural model itself. As neural models continue to improve and effective linguistic features are identified, the best methods for combining may also need to be updated."
9,"The current work has a number of limitations to consider.
(A) The paper’s experimental design was limited to a single language because we are not aware of any other learner corpora with multiple answers provided to the same exercises.
(B) The described approach to assessing the correctness of learner answers is limited by its design. First, the number of GEC hypotheses to check depends on the GEC model’s performance and, potentially, on the language. Second, if a word was not corrected, it can be a false negative error instead of a correct answer. Third, the GEC model can suggest corrections (valid and not valid) even to a correct answer depending on the data it was trained on. (C) Our approach focuses only on grammatical errors and it does not take into account semantic or pragmatic errors.
(D) Due to limited resources, we were unable to involve more people with prior annotation experience in the re-annotation of the RULEC test set, as well as in the manual verification of hypotheses generated by the GEC model. We acknowledge that the annotation performed by our annotators may not be entirely error-free: the annotators were free to work at their own pace and therefore could potentially rush and make errors themselves. Hence, we do not claim that the re-annotated RULEC test set does not include any inconsistency anymore. We believe that the existing datasets should be thoroughly checked, given the small amount of learner data available for languages other than English, before utilizing them to train and evaluate new models. (E) Considering the practical use of our GEC model as a component of a CALL system, we find that it can potentially be used in a limited context, i.e., for checking answers provided to cloze and multiple-choice exercises, only for best-performing error types. As for alternative correct answers, even for best-performing categories of answers, a human teacher should verify proposed corrections. We have to underline that learner errors in RULECGEC (and especially in any synthetic dataset) can significantly differ from errors made by learners with various backgrounds, native languages, and proficiency levels. We also find that low recall of state-of-the-art GEC models impedes their usage in language learning settings. At the moment, learner answers should be verified by a human teacher."
10,"Unmeasured covariates. A range of factors may affect instructional outcomes, only a subset of which could be measured with this data. Making strong claims about the link between discourse moves and instructional outcomes requires experimental validation. For example, the quality of the math task that the students are working may affect the discourse as well as learning outcomes. We can isolate the effect of discourse moves by randomly assigning teachers to learning opportunities that help them improve their use of these moves,
and examining downstream impacts of these new talk moves on student outcomes. (Demszky et al., 2021a) has taken a similar approach successfully in an informal teaching context, but such a study is yet to be done in a K-12 context.
Generalizability. Although the NCTE transcript dataset is the largest available dataset of U.S. classroom transcripts, it only captures a tiny fraction of U.S. classrooms and hence there are limitations to its representativeness. The data represents mostly white female teachers working in mid-size to large districts, so it would be valuable to collect new data from other types of districts and a more diverse teacher population. The fact that the data was collected a decade ago may pose limitations to its ongoing relevance; during the period under study (2010-2013), many schools were transitioning toward Common Core-aligned instruction in mathematics but yet lacked high-quality curriculum materials for doing so. That said, research in education reform has long attested to the fact that teaching practices have remained relatively constant over time (Cuban, 1993; Cohen and Mehta, 2017) and that there are strong socio-cultural pressures that maintain the instructional status quo (Cohen, 1988). In general, it is important to carefully validate measures built on the NCTE data on a new domain to ensure that it is representative of the target population.
Limitations of linked data. Education research has attested the limitations of standardized assessments in capturing student learning and reasoning (Sussman and Wilson, 2019). Student questionnaires in the NCTE data can provide an alternative perspective on students’ experiences and mathematics outcomes but these responses have a lot of missing values, and hence it may not provide robust estimates. Furthermore, understanding equity in instruction is a high priority for our research team and for the field more generally. However, studying equity within this data is challenging, since student speakers are not linked to administrative files containing student background and achievement variables. That said, such speaker-level demographic data is rarely available in instructional contexts, for important ethical reasons, and thus this limitation may encourage researchers to develop measures of instructional equity that leverage classroom-level, instead of speaker-level demographic information."
11,"Future work on automated question generation for learning contexts could benefit from a number of potential research paths. In this paper, we tested three different architectures - but there are many more to be considered including those that incorporate knowledge graphs which have been shown to improve the richness and semantic correctness of generated questions (Bi et al., 2020). There is also room to explore different prompt strategies including a fill-in-the-blank approach which may be more appropriate fo the TQA data. For the attribute models, we used the single task objective of question generation, but it would be worthwhile to explore jointly generating the question attribute and the question itself. Additionally, document level Abstract Meaning Representations with resolved coreferences has been shown to improve
the quality of knowledge based question generation (Kapanipathi et al., 2021). We also recognize that we focused on different context for the input, but not on the wide variety of generation strategies available for this task. On top of the variety of model architectures, we would like to evaluate a greater set of corpora that include additional topics such as history and economics. Reading comprehension is critical to these fields as well and there is limited, if any, research on question generation for these topics.
Additionally, in future work we will conduct evaluation with expert annotators to incorporate into more complex models. Ideally, we will have educators and students assess the output of our models for factual correctness, relevance, and fluency of the questions generated. This output can then be used to train an instruction fine-tuned model. In order to make a solution that is viable for the classroom, it is critical to think beyond the automated metrics and get real teacher feedback. This preliminary research demonstrates the potential for expanding automated question generation to multiple classroom subjects and the value of incorporating discourse information into different model architectures to produce high quality questions."
12,"As noted in the evaluation section, our system does not perform perfectly in multiple-choice question generation, particularly when it comes to generating distracting options, even with the powerful ChatGPT. In the next step, we can adopt an opensource framework of LLMs and fine-tune a domainspecific model using the extensive educational materials provided by middle school teachers. This way, the question generation ability may be improved, and we will not need to rely on the OpenAI API.
On the other hand, although extensive evaluations have been conducted, they only involve a small fraction of teachers and students in a preinterview setting. Once our system is widely deployed, a larger amount of user feedback will be collected and analyzed to monitor its effectiveness."
13,"This section discusses the limitations related to the evaluation process and potential ethical considerations associated with the use of ChatGPT or similar language models in educational settings.
Human evaluation Our evaluation is conducted with a limited sample size of two teachers. Future work should aim to include a larger and diverse sample of teachers to capture a wider range of perspectives. This would help tease apart the potential teacher biases from generalizable claims about the feedback quality.
Ethical considerations The use of language models like ChatGPT in educational contexts war-
rants careful examination. For example, because the model relies on transcribed speech and is trained on primarily English, it might misinterpret the transcriptions of teachers or students who do not speak English fluently. Additionally, deploying language models in education settings raises concerns regarding privacy and data security. For example, the raw classroom transcripts should not be directly fed into the model to provide feedback as it may contain personally identifiable information about students. Guardrails should be set to prevent classroom data from being sent directly to external companies."
14,"Our analyses were based around one metric of uniform DIF, z. The benefits of z are that it is commonly used in practice, it is highly interpretable with well-established effect sizes, and it is easy to aggregate across items and focal groups. One of the drawbacks, however, is that it does not capture non-uniform DIF, and it is not ideal in terms of statistical power (Woods et al., 2013).
Consistent with other analyses of DIF, our study struggles to identify sources of DIF (Zumbo, 2007).
Although it is outside the scope of this study, a finegrained analysis of examinees’ language, especially based on L1, could provide insight. Additionally, it could be beneficial to explore the possibility of modifying BERT using debiasing techniques (Sun et al., 2019). These techniques could potentially reveal sources of DIF and reduce DIF. Follow-up analyses along these lines of inquiry may be found in Kwako (2023)."
15,"This paper presents a dataset of expert-curated Socratic conversations where instructors assist novice programmers in fixing buggy solutions to simple computational problems. The dataset serves as a benchmark for evaluating the Socratic debugging capabilities of LMs. While GPT-4 outperforms GPT-3.5, its precision, and recall remain below human expert levels (70.0), highlighting the need for further research. We find that GPT-family language models may generate repetitive and irrelevant Socratic utterances that could mislead learners. The utterances may also appear too early in the conversation, causing confusion, and can be overly direct, potentially diminishing learning outcomes. Study limitations include: The automatic metrics are limited in capturing the correctness, helpfulness, and relevance of a Socratic utterance, and the benchmark dataset may not represent all common novice misconceptions. Moreover, the manual evaluation is limited to 5 dialogues and could be expanded, but this process is highly time-consuming."
16,"It is essential to address the ethical limitations observed our fine-tuned OPT model, ranked 4th in the competition. The model card provided by Meta AI highlighted that the training data used for their model consisted of unfiltered internet content, leading to the presence of significant biases within the model. These ethical considerations raise concerns regarding fairness, inclusivity, and potential biases in the responses generated by the model. Further research and development in addressing these limitations are imperative to ensure the responsible and unbiased deployment of chatbot models."
17,"limitations of large language models as tutoring systems, we hope our work will prevent the premature use of these technologies."
18,limitation of the presented evaluations. future work will therefore need to determine whether the errors that learners make in half-open exercises are also good distractors for sc exercises or whether learners instantly perceive them as incorrect when contrasted against the correct option. it is also yet unclear to what extent the most frequent misconceptions differ between and within learners over extended periods of time.
19,"limitations this study reports early results and was based on a limited cohort of students from one master’s course. the generalizability of these results is therefore subject to certain limitations. follow on studies will test the system with larger samples and different disciplines to add weight to any significance of the results. notwithstanding the relatively limited sample, this work offers valuable insights into how a video-based student support platform that uses knowledge tracing improved graduate students’ perception of their learning experience. as the students finish their course, we will collect additional quantitative data in the form of final student grades. these will be compared between students who used the system and those who did not. we will also compare the final grades of students who used the system between the course in which they used it and other courses in which they did not. it also remains unclear what influence the course content has on the students’ experience. to examine this element further, the system should be tested on multiple courses imparted by different lecturers, and in varying subject fields. there may be an inherent positive opinion of aipowered technologies by students at an ai university. to test that hypothesis, the system should be provided to students at other universities in subject areas not related to ai. several questions, however, remain to be answered. further research should be undertaken to test several hypotheses, for instance, whether perceived ease of use (peou) and perceived usefulness (pu) would predict the attitude towards usage (atu) of orbits."
20,"limitation of this study is that we used only the squad dataset in our experiments. the squad dataset has often been criticized because it is overly dependent on the similarity of question/answer sentences rather than on human-type reasoning, meaning it requires only superficial read- ing skills. thus, examining the effectiveness of our proposed method by applying it to various other datasets will be an important future task. furthermore, in the human evaluation experiment presented in section 6.4, we examined only 60 question–answer pairs generated through the proposed model from ten randomly selected reading passages. the relatively small scale of the experiment is due to the high workload required for people to carefully evaluate the various properties of a large number of questions. however, in the future, we aim to conduct a larger-scale human evaluation in order to increase the reliability of the experimental results. although the present study used only five qa systems, the use of a larger number of qa systems with different characteristics is expected to improve the accuracy of question-difficulty estimation and provide difficulty estimates with finer granularity. therefore, examining the effects of increasing the number and variability of qa systems will be another future direction of this research. we also need to confirm in greater detail whether qa systems can be substituted for human learners. a comparison between irt-based question difficulties calibrated from the responses of qa systems as well as human learners might be a plausible approach. another future goal is to develop a method of transforming the scale of the irt-based difficulty, estimated based on qa systems, into a scale appropriate for a population of target learners. such a scaling adjustment is expected to be achievable by using equating, which is a well-established technique in irt. furthermore, our qg method is easily extended to adaptive qg systems based on the framework of computerized adaptive testing, as mentioned in section 5.3. developing and evaluating such an adaptive system using our qg method will also be our focus in future work."
21,"limitations as noted, this work is limited in that it does not address neopronouns. we speculate that the augmentation techniques deployed in this work may extend to these pronouns as well, we recognize that they do not have the same linguistic reality as he/she/they pronouns. neopronouns may be similar to singular they in being relatively infrequent in a naturalistic corpus, but they are also different in that they don’t overlap with a frequent morphologically-identical paradigm like plural they. additionally, the singular they augmentation technique we propose is specific to english and distributional facts about english pronouns. for one, english singular they morphologically overlaps with a plural pronoun, which is the primary motivation for using coreference information to identify contexts where they would have a primarily singular interpretation. this is often not the case for other languages, as in swedish where the gender-neutral hen is functionally similar to singular they but morphologically and distributionally dissimilar in that it does not overlap with a plural pronoun (gustafsson sendén et al., 2015)."
22,"limitations accessibility we have tried to develop readalong studio web app with accessibility in mind, using accessible colour contrasts, ensuring buttons have aria-labels, and ensuring that the website is legible when zoomed-in to 200%, among other considerations. using google pagespeed insights, our website scores 89 for accessibility, but we recognize that there are still improvements to be made; specifically, we would like to perform an audit of the website with respect to web content accessibility guidelines (wcag). inexact transcription readalong studio will work best if the transcription is exact; that is, if there are as few discrepancies between the text and audio as possible. if extraneous text exists (such as page numbers, chapter titles, or translations), or if the audio includes un-transcribed speech (such as false starts), these errors will accumulate and can result in poor alignments. the extent to which these discrepancies affect the final result depends on the length of the recording to be aligned. in practice, we have found that readalong studio is able to recover from minor transcription errors when the speech data to be aligned are around 5 minutes or less in length. we have successfully aligned much longer (up to 40 minute) files, but “your mileage may vary” depending on the exactness of the transcription, the language’s orthography, and the type of data used. singing several teachers have successfully aligned songs with the corresponding text using readalong studio. for such an alignment to be successful, however, it is necessary that the sung words be vocalized clearly, and not be drowned out by the accompanying music (if any). extended legato singing (e.g., where one syllable is extended across multiple notes) can also cause poor alignments, since the speech-trained acoustic model does not expect single syllables to correspond to multiple intensity peaks in this way. language support the software works with most languages out-of-the-box. as mentioned in §2.1, readalong studio comes with support for 39 languages built-in, and handles other languages with a rough, best-guess g2p based on unicode table information. at several international workshops (§3), we found that it worked reasonably well with every language brought by workshop participants, even those with unique alphabets like western armenian or korean. however, not every language will work equally well. it will typically work well in languages with systematic orthographies that use letters in cross-linguistically common ways. we anticipate difficulty with orthographies that use familiar letters in cross-linguistically unusual ways, such as “font-encodings” (pine and turin, 2018), abjads that leave out many vowels, and languages like japanese where the pronunciation of logographs is highly variable and determined by context. just like a human could not simply guess the missing vowels in written hebrew without knowing hebrew, the software will not be able to do this either. additionally, the software is limited to languages which are both written and spoken—we do not support signed languages since the aligner requires audio to align with text, and the tool is fundamentally inapplicable to unwritten languages. the interface itself is currently only translated in english, french, and spanish, limiting potential users who do not speak one of those languages. numbers and symbols while readalong studio can do rough zero-shot g2p for most alphabetic and syllabic writing systems, it is not capable of general text normalization—while it can guess that “t” might be pronounced [t] in an unfamiliar language, it simply has no basis to guess any particular pronunciation for “634”, as this task is not only language-dependent but highly variable within any given language (bigi, 2011). therefore, all input must be “spelled out” for alignment to succeed. if the input contains numbers or symbols, readalong studio web app will prompt the user with a warning that it found uninterpretable symbols.18"
23,"limitations of the knn system with regards to making new corrections. limitations the three base models used for the experiments were trained with different settings. as a result, it is challenging to understand the exact source of discrepancies between the results. additionally, each of the three models used different subword tokenizations, resulting in variable datastore sizes. although we have some hypotheses about why knn affects gec differently from mt, more experiments need to be conducted to confirm them."
24,"limitations our work has several limitations. in terms of method generalization, the proposed method depends on multi-lingual neural machine translation models to generate trans-lingual definitions, and hence limits its application scope to those languages rarely supported by translation models. moreover, our findings are based on three languages, but different families of languages may exhibit distinct phenomenon that even challenges our"
25,limitations and alternative approaches. such conceptual and theoretical
26,"limitations, future research could explore ways to combine the strengths of humans and language models to produce even more accurate and informative explanations. therefore, chatgpt has the potential to assist teachers and other professionals in the creation of high-quality assessment items through a well-designed prompt, which can help ensure that items have a single correct answer, independent options, non-overlapping options, and plausible options. furthermore, chatgpt ability to classify items based on ecd principles is promising, but further research is needed. for example, the evidences could be provided to language models and ask them to classify each item in one of them. also, they could be asked to create the options and the respective explanations based on some kind of guidelines such as the one cited."
27,"limitations that should be addressed in future work. the small sample size we collected makes it difficult to assess the generalizability of our findings. this also prevented us from running any analysis of internal structure or differential item functioning (dif) using methods from factor analysis or item response theory, as these models require large sample sizes (min and aryadoust, 2021). as items generated by gpt-3 should contain no dif and have similar factor structures as items written by humans, these are important analyses to explore in future work. we also did not examine the diversity of the generated items, in other words, how thoroughly the model explored the construct space. it is a well-known problem in psychometrics that having too many similarly worded items can inflate the reliability and reduce the validity of a measure (clark and watson, 1995), and our results may have been susceptible to this. a related problem is ensuring that the distribution of labels in the generated items remains balanced, and while we took steps to account for this, we did find that the distribution of gpt-3 items was somewhat unbalanced. for example, there were far fewer neutral items than either entailment or contradiction. improving the prompt design to account for diversity and other psychometric properties simultaneously is a fruitful direction for future work. our experiment with gpt-4, while disappointing, was also quite limited and should be expanded upon. we deliberately kept the prompt design as similar as possible between the two models, to avoid possible confounds. making effective use of the system query and changing the structure of the prompts to suit a conversational style could lead to much better results, however. finally, although we believe nli is a good task to use for initial experimentation, we also acknowledge that it is significantly different from the tasks of interest in education (e.g., question answering), and future work should explore our approach on tasks with stronger educational applications. llms have the potential to greatly ease the burden of scale development, and transform educational and psychological measurement. our results contribute to the growing field of llm-based automated item generation, and demonstrate the potential these methods have for generating valid and reliable items at a scale that would have previously been impossible. further research, combining our approach with more advanced prompting strategies, or zero-shot parameter estimation, could conceivably lead to a system that generates high-quality items in a fully autonomous fashion, which would transform the practice of writing and validating test items. limitations we emphasize that our research is exploratory and the generated items we produced should not be used for making critical evaluations of cognitive skillsets in either humans or llms. as discussed in section 5, our small sample size makes it difficult to draw broad"
28,"limitations our task setting and baseline system requires that the input is already segmented into words including mwes. the mwe identification step in the construction process of our dataset involved timeconsuming manual annotation. building a highquality system that fully automates the process is an issue for future work. our dataset can be used to evaluate such a japanese mwe identification system. additionally, as shown in section 5, our baseline model performed relatively poorly in the higher complexity tiers. this is an effect of the dense annotation setting; it results in uneven distributions of complexity as shown in figure 2, where easy words greatly outnumber difficult words. one possible solution would be creating another lcp dataset using sparse annotation, where target words are selected using frequency bands so that the words are distributed across a wide range of frequency (shardlow et al., 2022). our data could provide insights as to what kind of words should be targeted by sparse annotation for such a dataset."
29,"limitations. for example, none of the models (not even the multilingual one) give a special treatment to cognates (sets of words in one of the two languages that have been inherited in direct descent from the other one) which are normally considered easier to acquire. another concern has been the lack of transparency in the classifier’s predictions, a direct consequence of the dense representations we favoured over the more interpretable linguistic features. finally, a contextual classifier may predict different levels for occurrences of the same lexeme in different contexts. those limitations underline the need for human validation of the output of such systems."
30,"limitations the limitations of the findings in experiments 1 and 2 have to do with the relatively small scale of the study. we experimented with two books and, while the findings were broadly consistent, it could be that results would not generalize to other books. experiment 2 was conducted with a specific group of readers in a specific context of implementation; studies with additional groups of readers are needed to evaluate generalization of the findings. another limitation of our experiments is that the dynamic model of lexical experience is evaluated only as an aggregate index per passage and not as a predictor for specific words or types of words. in particular, the model predicts a slight increase in surprisal of function words if their density in the story is generally lower than in the background corpus. this assumption may or may not be correct; further experimentation is necessary to evaluate the surprisal model in more detail. we thank a bea reviewer for pointing out this limitation."
31,"limitations we identify two limitations of the current work and make suggestions for future directions. first, while our proposed method is language-agnostic in principle, our evaluation is limited to our french benchmark dataset. expanding our approach to encompass other languages would bring new and interesting challenges for further investigation. second, despite topic diversity within our exercise documents (e.g., the first example in fig. 1 consists of independent sentences, while the second is a coherent text centered around the same topic.), it would be interesting to quantify the degree of topical bias introduced during our training process and its impact on our binary task evaluation. for future work, we first aim to adapt seq2seq models for our task particularly text-to-text models such as t5 (raffel et al., 2020). there is also potential to explore different prompting strategies for large language models (llms), when generating gap-filling grammar exercises. for instance, the utilization of chainof-thought prompting (wei et al., 2022), which involves generating intermediate steps before producing the final response, could be explored for generating grammar exercises. additionally, an interesting future study would involve investigating the number of example demonstrations that llms require in order to accurately mimic example gap exercises."
32,"limitations of manual classroom observation and provide scalable, automated feedback on instructional practice. while our results reveal that chatgpt has room for improvement in generating insightful and novel feedback for teaching, our proposed tasks and evaluation process provide a foundation for future research to address the challenges of teacher coaching using nlp. our work underscores the challenge and importance of generating helpful feedback for teacher coaching. moving forward, we propose several directions for further research, such as improved prompting methods and reinforcement learning with feedback from coaches. ultimately, we envision a future where generative ai can play a crucial role in supporting effective teacher education and professional development, leading to improved outcomes for students."
33,"limitations by building this task-specific dialogue system for kids, we aim to increase the overall quality of basic math education and learning at-home experiences for younger children. in our previous school deployments, the overall cost of the whole school/classroom setup, including the wall/ceilingmounted projector, 3d/rgb-d cameras, lidar sensor, wireless lavalier microphones, servers, etc., can be considered as a limitation for public schools and disadvantaged populations. when we shifted our focus to home learning usages after the covid19 pandemic, we simplified the overall setup for 1:1 learning with a pc laptop with a built-in camera, a depth camera on a tripod, a lapel mic, and a playmat with cubes and sticks. however, even this minimal instrumentation suitable for home setup can be a limitation for kids with lower socioeconomic status. moreover, the dataset size of our initial home deployment data collected from 12 kids in 12 sessions is relatively small, with around 12 hours of audio data manually transcribed and annotated. collecting multimodal data at authentic homes of individual kids within our target age group (e.g., 5- to-8 years old) and labor-intensive labeling process is challenging and costly. to overcome these data scarcity limitations and develop dialogue systems for kids with such small-data regimes, we had to rely on transfer learning approaches as much as possible. however, the dataset sizes affect the generalizability of our explorations, the reliability of some results, and ultimately the robustness of our multimodal dialogue system for deployments with kids in the real world."
34,"limitations include: the automatic metrics are limited in capturing the correctness, helpfulness, and relevance of a socratic utterance, and the benchmark dataset may not represent all common novice misconceptions. moreover, the manual evaluation is limited to 5 dialogues and could be expanded, but this process is highly time-consuming."
35,"limitations in generating pedagogically sound responses consistently. we then fine-tuned gpt-2 and dialogpt on the tscc dataset and evaluated their performance using bertscore and dialogrpt metrics. we also proposed an approach using rl to optimize directly for pedagogical values. we hypothesized that several dataset characteristics (e.g., dialog completeness, sampling) pose challenges to achieving superior performance with fine-tuning. to this end, we recommend the extension of the dataset to include longer prompts with extended context. finally, we also draw attention to the need for more domain-specific metrics (in both evaluation and reward-based training) in enabling the generation of accurate, context-aware, and factually correct teacher responses."
36,"limitations a limitation of our approach is that it relies heavily on the quality and relevance of the prompts used. the prompts were engineered based on observations made in the training data and this approach may not work if the prompts are not representative of the corpus. finally, our approach may not be suitable for all types of teacher-student dialogues and may require modifications for different contexts or domains. one possible concern with the techniques mentioned in this paper is the limited reproducibility of openai’s language models, such as gpt-3.5turbo. the weights of these models are proprietary and not publicly accessible, which makes it challenging to replicate the findings of earlier research or expand on them. ethical considerations ai-generated teacher utterances may contain bias, which may become apparent particularly in exercises or chit-chat. in this project, we took steps to avoid profanity in the ai-generated responses, but similar protection against bias should be put into place. additionally, human evaluators should be used to assess the quality of the ai-generated responses and to identify any potential biases. we recognize that language models like gpt3.5-turbo are trained on large datasets that reflect the biases and prejudices present in society. as there is always a risk of perpetuating these biases when using generative ai for dialogue systems it is important to evaluate the ai-generated responses for potential biases and to take steps to correct them."
37,"limitations, it is challenging to organize a shared task during which any possible number of submissions can in principle be evaluated with adequately remunerated human evaluations. what is more, data is very important in the context of real-world applications and shared tasks. although the corpus used in this shared task is a valuable resource in our domain, some particularities of this corpus and the data sampling method also had an undeniable impact on the results. therefore, in future editions of this shared task we should rethink some of the current potential limitations, such as the fact that the dialogues had to be limited to 100 tokens, resulting in partial conversations; the fact that some dialogues, if extracted from the data randomly might have led to data leakage; and the fact that the dialogues did not always follow strictly role-alternating format, with some teacher turns being preceded by previous teacher utterances, rather than a student utterances. in summary, the field of education has already been significantly changed by llms, whose capabilities keep improving constantly. we hope that this shared task will serve to help the scientific community better understand the current capabilities of llms in educational contexts. having learned from this shared task and going forward, we hope to make its future iterations even more informative."
38,limitations that should be addressed in the future.
39,"limitation in relying solely on automatic evaluation metrics (as detailed in section 5.1) prompt engineering to adapt language and tone in tutoring systems our experiments reveal an intriguing finding where manipulating the prompt influences the tone and language of the generated response, presenting an opportunity for tutoring systems to potentially adapt to the students’ learning styles and/or teaching goals. further research should delve into teaching instruction methods, potentially exploring the pedagogy of constructivist learning (graesser et al., 2005) or engaging students in ill-structured exercises for productive failure (kapur, 2008) using llms of this nature. gpt-3’s robust handling of errors and noncanonical form of language during the data preparation phase, a manual inspection of the data revealed the presence of grammatical and spelling errors in some utterances. additionally, since the dataset originated from chatroom text-based conversations, there were instances where mathematical symbols were used instead of natural language, such as this example utterance output teacher: but e.g. pleased with their visit = good idea. it is worth noting that we did not employ any nlp processing toolkit to correct these errors or non-canonical forms in the dialogue utterances. however, despite this, the gpt-3 model could still generate appropriate responses effectively. llms’ potential in multilingual settings in the context of l2 acquisition, the dialogue nature in caines et al. (2020) provides valuable opportunities for tutors to adapt to students’ native languages. code-switching strategies as such have been found to enhance teaching, including the explanation of concepts (köppe and meisel, 1995), and leveraging ai tutoring systems can facilitate this process. llms possess multilingual capabilities that enable them to address language barriers, accommodate low-resource languages, and exhibit promising performance even on unseen languages (yong et al., 2022). to enhance accessibility, the development and adoption of open-source multilingual models, such as bloom (scao et al., 2022), should be encouraged, thereby facilitating the utilization of llms in educational applications across diverse linguistic contexts."
