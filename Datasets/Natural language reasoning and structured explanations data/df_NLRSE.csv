,Text
0,"We present a method for question answering using a KGQA retriever and a language model reasoner. Limitations of our method include a lack of an integrated entity resolution system when training our KGQA model: we instead rely on annotated entities from the datasets. While our KGQA architecture is robust to new entities added at test time, it does require retraining when new relations are added to the KG or if a different target KG is used. Additionally, our results are based on training and evaluating on one dataset at a time; training on a mix of datasets could lead to better generalization, however this is not tested."
1,"Although we showed significant improvements in explanation generation using prompt-based few-
shot learning, our work still has some limitations. First, we experimented only on the e-SNLI dataset: although e-SNLI is a reference for the task, it would be interesting to extend the proposed methodology
to other datasets with natural language explanations (see Wiegreffe and Marasović (2021) for an extensive review).
Second, we did not attempt to automatic prompt optimization: although this may bring further minor improvements, we decided to leave optimization to a next step, as it does not change the core contribution of our work.
Third, we believe there is an intrinsic limitation in comparing our results with SOTA, as there is not a clear consensus on which metric is to be taken as the reference metric for benchmarking, along with the fact that measures sometimes disagree on scoring one system better than another. We hope that in the future this task and its evaluation will consolidate into a shared benchmark.
Finally, as for our our use of e-SNLI, we are assuming that for all sentence pairs in the dataset there is an implicit explanation of the semantic relation between the sentences. Under this assumption we always generate an explanation, even when the explanation is already explicit in one of the sentences. We think that a better capacity to detect those cases would bring relevant insight to our approach."
2,"Our approach requires access to planning information for each instructional text domain. In general, creating this information requires programming and domain knowledge to formally specify the planning constraints. However for high-value applications the effort associated with generating these planning domain definitions may be justified by their potential to help in generating more valid plan-based semantic parses. Having this knowledge is also crucial to allowing an agent or robot
to execute the resulting plan and may be naturally available in many domains as part of the execution component. In the course of developing our semantic parsing model, we discovered that Codex could generate valid planning domain definitions in a variety of output formats including the Planning Domain Definition Language (Fox and Long, 2003). This may provide a path towards automatically generating planning domain definitions for novel environments or reducing the need for human annotators. Future work could also evaluate our method in other planning domains that contain tasks beyond cooking such as VirtualHome (Puig et al., 2018) or ALFRED (Shridhar et al., 2020)."
3,"The proposed Reasoning Circuits framework intends to replace the need for thousands of annotated examples with a strong inductive bias of structured rationales. There is two issues with this approach at a conceptual level: 1. It may not always be possible to break down a multi-step reasoning problem cleanly into discrete reasoning steps, and another related issue it increasing complexity of the circuit with the complexity of the task. 2. For the design of these reasoning circuits a researcher must develop a thorough understanding of this reasoning task, so that the final circuit design broadly covers all possible types of reasoning problems expected to be solved. An under- or illdesigned reasoning circuit may cause the system to either not support a certain portion of problems or produce non-sensical outputs.
Essentially, there is trade off between a tighter control over reasoning by investing in a deep understanding of the problem leading to a comprehensive reasoning circuit design and lower annotations budget, versus, less control over logic and depending on a large number of annotations which allow the model to discover this logic on its own at much higher cost of large scale annotations budget.
At the implementation and operations level one of the the key limitations our proposed system is the number of inference steps to solve the problem. The number of times model inference may be needed to solve a single example is equal the length of the longest task sequence chain in the reasoning circuit. One possible solution for this could be by training the model to solve the entire problem by generating all the steps of reasoning and the target string in a single inference step and could massively reduce inference time and costs."
4,"In this section, we faithfully discuss the current limitations and potential avenues for future research.
First of all, the generation performance of our knowledge-augmentation framework largely depends on the efficacy of retrievers. In other words, if the retriever fails to retrieve the relevant facts to the input question, the prompted LLM, conditioned on the irrelevant facts, is likely to generate the incorrect answer (See Figure 3). Similarly, if the retriever is not designed to retrieve the facts in 2-hop neighborhoods of the question entities, LLMs are less likely to generate the answer requiring 2-hop knowledge. Note that, for the Mintaka dataset (Sen et al., 2022), the number of answerable questions with 1-hop facts is only 40% of total samples. However, when we include 2-hop triples, the number of answerable questions becomes 62%, which suggests the necessity of 2-hop retrievals, which is yet challenging (See Table 2). Thus, future work may improve the retrieval scheme itself to provide more accurate facts including multi-hops to the LLM, or may develop the mechanism to prevent the LLM from being misled by unrelated facts.
On the other hand, the evaluation metric for the generation performance of prompted LLMs may be further improved. Specifically, regarding our target KGQA tasks, the answer for the question is the entity in KGs. However, the prompted LLMs without additional training (i.e., zero-shot) tend to generate the answer as the sentence. For instance, the
label entity for the question (e.g., Where did Alex Chilton die?) in Table 4 is ""New Orleans"", however, the LLMs often generate the sentence-level output: ""Alex Chilton died on March 17, 2010 in New Orleans, Louisiana due to a myocardial infarction"". We currently evaluate the model performance by measuring whether generated tokens contain the answer entity or not; however, it would be worthwhile to develop the additional metric to compare the sentence-level output from LLMs to the word-level answer in KGs in a more effective way. Note that we also try other available metrics (See Appendix B.3), such as F1 and Exact Match (EM) scores (Rajpurkar et al., 2016), however, they largely penalize the longer sentences (e.g., EM of correct examples in Table 4 are 0), thus may not be appropriate for evaluating LM prompting schemes.
Lastly, since we focus on the improvement of knowledge injection in LM prompting, we use the labeled entities in KGQA datasets when evaluating models, following the existing KGQA evaluation setups (Cohen et al., 2020; Sen et al., 2021). However, in real-world applications where the entities in the question are mostly not provided, we first need to extract entities in the question with existing entity linking techniques; therefore, our model performance depends on the efficacy of entity linking. In particular, regarding the result with entity linking in Table 5, the portion of answerable questions from labeled entities in the dataset is 40%, however, the portion of them with entities from the entity linking model (Ayoola et al., 2022) is 22%. Therefore, since the improved entity linking performance would contribute to the performance gain of our KAPING framework, for KGQA tasks, future work may advance such the entity linking scheme."
5,"While the proposed methods are attractive due to their efficiency, explainability and not needing training data, the limitations are also manifold: The pipeline nature propagates all errors that occur. For instance, the dependency parser in use performs rather poorly on informal texts such as tweets. Further, our definition of positive and negative effect relations is quite shallow and does not always live up to the real world’s complexity. We only capture effect relations that are formulated explicitly within one sentence, and only one effect relation per sentence. Requiring the nodes to link to Wikipedia might be too restrictive while not even truly solving the problem of filtering non-sense nodes. Both the low inter-annotator-agreement in our effect graph evaluation as well as the discrepancy of the crowds’ and the expert’s annotations make it hard to assess the correctness of the extracted effect relations. And lastly, while we showcase some generated explanations, we did not properly evaluate how reliable the approach is in finding reasonable explanations. Indeed, first results suggest that this approach of generating explanations works rather inconsistently, though the ranking helps to a certain degree.
What one might consider another limitation is that we do not check the effect relations for factual correctness, which ultimately leads to contradictions and inconsistencies in the effect graph. While fact checking is a difficult and controversial task, we also purposefully decided against any form of fact or consistency checking. Each edge in the effect graph is meant to represent one effect relation exactly as it was expressed. Including critical effect relations in the graph allows for identifying, analyzing, and potentially disproving them."
6,"limitations one significant limitation to our study is that, as of march 23rd 2023, openai has deprecated access to code-davinci-0024, thus rendering our results non-replicable for any team not granted special access to these models by openai. we did not anticipate this deprecation while conducting this work and we believe this raises serious questions about the usage of api-based language models in scholarly work. another limitation is that the 12 tasks we selected may not be representative of the broader population of natural language tasks. had we conducted our experiments on a larger selection of tasks there may have been larger-scale trends that we would have been able to uncover. the largest and most pressing limitation with our work is that the models we are testing on have closed-source pre-training datasets. thus, we are unable to verify the extent to which our task datasets have been included in the training or instruction fine-tuning data. given that the training data for most of the models tested in this work cuts off in late 2021, this is a very strong possibility. our results should be viewed with this limitation strongly in mind. finally, while we experimented with different code prompts, the search space of possible prompts is very large. thus, it is very likely that there exists some prompt that outperforms our chosen prompts for each task. drawing"
7,"limitations our experimental setup excludes free-text rationales explaining the decisions of a model (wiegreffe et al., 2022; camburu et al., 2018), because their output is not based on attribution scores or highlighted spans of the input text, so we argue that they are not trivially comparable. however, there are end-to-end rationalization frameworks that can accommodate arbitrary saliency methods (jain et al., 2020; chrysostomou and aletras, 2021; ismail et al., 2021; atanasova et al., 2022; majumder et al., 2022), but require large language models that are expensive to train and perform inference with, so this is out of scope for this study. however, we also see that high-quality free-text rationales can be more easily generated with llms (wang et al., 2023; ho et al., 2023), and a comparison between them and our attribution-based explanations is an interesting avenue for future work. inferring high-quality explanations from large language models necessitates excessive amounts of compute and storage. although gpt verbalizations are most promising, we urge the research community to look into more efficient ways to achieve similar results. in the future, we will explore if training a smaller model on top of the collected rationale-augmented verbalizations is feasible. emphasizing the concerns of rogers (2023), we do not recommend the black-box model gpt-3.5 as a baseline for interpretability, because the model’s training data or internal parameters can not be accessed and the dangers of deprecation as well as the lack of reproducibility are serious con- cerns. however, we do think it has revealed great potential as a surface realization and contextualization tool for the task of saliency map verbalization. the causality problem explained in jacovi et al. (2023a) is not solved by our verbalizations, as it is an inherent problem with feature attribution and rationalization. future work includes verbalizations alongside counterfactuals, e.g. in interactive setups (feldhus et al., 2022; shen et al., 2023). although multiple models and explanationgenerating methods are available, we specifically focus on one pair for both datasets (bert and integrated gradients), because the focus of our investigation is on the quality of the representation rather than the model. finally, explicitly modelling expected highlights to mitigate misalignments as reported on in schuff et al. (2022), jacovi et al. (2023b) and prasad et al. (2021) is still unexplored."
8,"limitation of existing lm prompting schemes, which rely on the static knowledge internalized in parameters; therefore, when such knowledge are incomplete, inaccurate, and outdated, llms may generate factually incorrect answers. to tackle this challenge, we introduced a novel knowledge-augmented language model prompting (kaping) framework, which augments the knowledge for the input question from kgs directly in the input prompt of llms, with the fact retriever to inject only the relevant knowledge. the proposed framework is completely zero-shot, and versatile with any lms, without additional parameter updates and training datasets. we validated that our kaping yields huge performance gaps from the lm prompting model relying on its internal knowledge, especially with smaller lms, on the kgqa tasks. we believe our new mechanism for augmenting facts from kgs to the lm prompt will bring substantial practical impacts in generating knowledge-grounded answers."
9,"limitations concepts in this work, we extract the concepts from semi-structured explanations whose format reassures consistency and non-ambiguity of the exploited concept(s). the selection of datasets and corresponding concepts is primarily conditioned by data availability, as the semi-structured explanations are available merely for a small set of datasets. we acknowledge that our selection of concepts is not representative for a vast variance of concepts that users might expect models to learn from context in interaction. some important concepts’ features that we identify are following: (i) a number of premises or reasoning inference steps needed to map the input to output, (ii) the granularity of the reasoning steps, (iii) a type of the premises; for instance, whether the familiarity with a given concept requires a memorization of an entity property (such as “sun emits light”), or a reasoning mechanics such as analogical reasoning (“if animals can run and cat is an animal, then a cat can run”). we invite future work to identify or propose a taxonomy that would better reflect the wide variance of reasoning concepts that models are expected to comprehend in order to serve a wide scope of unseen tasks. such taxonomy can motivate a more targeted collection of concepts from explanations, or annotation of new explanations demonstrating new concepts. models we acknowledge the limitation in a variance of evaluated models given by their availability and our computational possibilities. we evaluate only two models of the gpt family due to the usage limits of openai api. outside gpt models, we do not evaluate models over 20b parameters, given the infrastructure requirements of such settings. nevertheless, we argue that the relevance of the models with constrained access, or resource requirements exceeding the limits of most organizations also remains a subject of open question. datasets one should note that the sizes of our evaluation datasets, for which we are able to extract concepts from explanations (fig. 2), are too small to compare concept sensitivity between models. the sizes of our sensitivity evaluation datasets are the following: worldtree: 2,204 samples, openbookqa: 792, glue diagnostics: 282 samples, hotpotqa: 182 samples. ethical considerations & broader impact as outlined in section 1, in-context learning recently presents a research direction of broad public interest, where the outstanding results on nlp benchmarks often do not meet the users’ expectations. it is understandable that the focus of development in in-context learning llms goes to measurable improvements on existing benchmarks, as ecologically-valid evaluations (de vries et al., 2020) on end use-cases are timely and challenging to compare to related work. nevertheless, in this highly-exposed and fastpaced direction, we identify the necessity for the emergence of fast proxy measures that can shed light on the decision-making of the llms as expected by their end users. the presented evaluation of models’ sensitivity to demonstrated reasoning concepts introduces a technical framework for quickly assessing models’ compliance with our expected functioning; however, a selection of a comprehensive set of concepts that we can agree our models should be able to learn remains a subject of open"
10,"limitations while our study provides valuable insights into the impact of finetuning on reasoning performance and the role of explanations during finetuning and prompting with respect to various reasoning skills, there are several limitations to our work. firstly, we only consider a single llm, opt, as our base model. our results may not generalize to other llms with different architectures or pretraining objectives. secondly, we only use a limited set of reasoning datasets for finetuning due to the limited availability of open-source datasets with explanations. however, it is possible that our findings may not hold for models finetuned on larger closed datasets as usually seen in real-world scenarios. thirdly, our experiments only cover a limited range of model sizes due to limitations in computational budget, therefore it is possible that our findings may not hold for much larger models. finally, we only consider finetuning using fewshot prompting conditions in our experiments, and it is possible that our findings may not hold for models finetuned without in-context exemplars. overall, while our study provides valuable insights into the impact of finetuning and explanations on reasoning performance, further research is needed to investigate these factors across a broader range of models, datasets, and finetuning strategies."
11,"limitations while this work explores the impact of the typical compositional modifiers on entailment relations, we did not consider other fine-grained information that further captures upward or downward monotonicity from the monotonicity calculus of the premise/hypothesis sentence pairs. further, the dataset that we generated is relatively small, at approximately 1,300 sentences. we also did not evaluate the dataset over t5, bart, gpt-x, and other state-of-the-art llms, which may provide more insights. we also did not conduct any evaluation for explanations and interpretation of the evaluated nli models, which could be future work. lastly, we did not include a comparison with existing datasets that were created specifically for negation modifiers and universal & existential quantifiers. we see all these issues as exciting avenues for future work."
