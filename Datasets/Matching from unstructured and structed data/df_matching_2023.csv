,Text
0,"Despite its promising results, our study has limitations. Our model primarily works with English text, limiting its applicability to other languages. Its focus on sentence-level extraction doesn’t consider document context, which could be investigated in future research. The employed training dataset is relatively small, potentially not encompassing all possible event types, thus affecting the model’s performance and generalizability. Additionally, our two-stage inference framework, while enhanced by a ranking module, is prone to error propagation. If a trigger isn’t identified in the first stage, its associated arguments cannot be extracted. Future work should address these issues for improved performance and broader applicability."
1,"Our experiments focus solely on English-language entity linking. Similar models have been trained to perform entity linking in multiple languages (De Cao et al., 2022), but we do not consider performance beyond English. The issues faced in other languages are likely to be similar, but the multilingual element of other models might lead to different results. Further, how to select keywords in the multilingual setting is unclear.
In addition, we are limited by the available annotated entity linking datasets. Given that we need a large amount of data to train these models, they are inherently reliant on Wikipedia. These entity linking datasets are skewed towards specific types of matches, including ones that are frequently exact matches. The effectiveness of this model might change when trained on a dataset with different characteristics, even with a large amount of data.
Finally, the computational resources required to train these models are large, and our final results do not reflect numerous other preliminary experiments. This restricts our ability to run multiple experiments, train models from scratch easily, and potentially leads to underfitting of our final models."
2,"In this section, we faithfully discuss the current limitations and potential avenues for future research.
First of all, the generation performance of our knowledge-augmentation framework largely depends on the efficacy of retrievers. In other words, if the retriever fails to retrieve the relevant facts to the input question, the prompted LLM, conditioned on the irrelevant facts, is likely to generate the incorrect answer (See Figure 3). Similarly, if the retriever is not designed to retrieve the facts in 2-hop neighborhoods of the question entities, LLMs are less likely to generate the answer requiring 2-hop knowledge. Note that, for the Mintaka dataset (Sen et al., 2022), the number of answerable questions with 1-hop facts is only 40% of total samples. However, when we include 2-hop triples, the number of answerable questions becomes 62%, which suggests the necessity of 2-hop retrievals, which is yet challenging (See Table 2). Thus, future work may improve the retrieval scheme itself to provide more accurate facts including multi-hops to the LLM, or may develop the mechanism to prevent the LLM from being misled by unrelated facts.
On the other hand, the evaluation metric for the generation performance of prompted LLMs may be further improved. Specifically, regarding our target KGQA tasks, the answer for the question is the entity in KGs. However, the prompted LLMs without additional training (i.e., zero-shot) tend to generate the answer as the sentence. For instance, the
label entity for the question (e.g., Where did Alex Chilton die?) in Table 4 is ""New Orleans"", however, the LLMs often generate the sentence-level output: ""Alex Chilton died on March 17, 2010 in New Orleans, Louisiana due to a myocardial infarction"". We currently evaluate the model performance by measuring whether generated tokens contain the answer entity or not; however, it would be worthwhile to develop the additional metric to compare the sentence-level output from LLMs to the word-level answer in KGs in a more effective way. Note that we also try other available metrics (See Appendix B.3), such as F1 and Exact Match (EM) scores (Rajpurkar et al., 2016), however, they largely penalize the longer sentences (e.g., EM of correct examples in Table 4 are 0), thus may not be appropriate for evaluating LM prompting schemes.
Lastly, since we focus on the improvement of knowledge injection in LM prompting, we use the labeled entities in KGQA datasets when evaluating models, following the existing KGQA evaluation setups (Cohen et al., 2020; Sen et al., 2021). However, in real-world applications where the entities in the question are mostly not provided, we first need to extract entities in the question with existing entity linking techniques; therefore, our model performance depends on the efficacy of entity linking. In particular, regarding the result with entity linking in Table 5, the portion of answerable questions from labeled entities in the dataset is 40%, however, the portion of them with entities from the entity linking model (Ayoola et al., 2022) is 22%. Therefore, since the improved entity linking performance would contribute to the performance gain of our KAPING framework, for KGQA tasks, future work may advance such the entity linking scheme."
3,"Although our dataset presents a significant advancement over previous benchmarks, it is still limited in that it only contains entities already known to Wikidata. One could argue that the very long tail is what is even beyond Wikidata.
In the second stage, our method harnesses an LM pre-trained for entity disambiguation. Therefore, our methodology, in its current form, cannot predict objects that are not already known to that LM and its underlying KB."
4,"limitations while there is a lot of work on creating and making available large pre-trained language models for a range of languages, there is to our knowledge not that many knowledge graphs for other languages than english — especially general knowledge ones, like conceptnet. this is a major limitation, as it restricts research to one single language and the structured representation of knowledge found in the culture associated with that specific group of language users. creating commonsense kgs from unstructured text is a costly process that requires financial resources for annotation as well as available corpora to extract the graph from."
5,"limitations of existing methods. our model uses a pretrained language model module and graph neural network module to jointly represent event graphs. in addition, we make the event embedding space geometrically meaningful by imposing two constraints on event embeddings: event temporal order should be in accordance with event embedding norm, and event temporal relations should only exist between events whose embeddings are close enough. experiments demonstrate that our method significantly outperforms baselines by generating accurate and globally consistent temporal event graphs. in the future, we aim to incorporate external background knowledge and commonsense knowledge into our framework. we also plan to make use of the generated temporal event graphs in downstream tasks, such as future event prediction and question answering. limitations in the current design setting, our proposed model is only able to classify temporal relations between event pairs into one of three classes: before, after, and no relation. our model should be more practically useful if we can extend it to predict more relation types in addition to temporal relations, such as parent-child and causecaused_by relations. we believe that our model is able to make such extension without too much modification. in addition, as mentioned in the previous section, our model does not make use of any external knowledge, e.g., commonsense knowledge of event temporal relations. our framework should be more powerful to deal with domain-specific articles if utilizing such knowledge in the framework. ethical considerations we acknowledge that our work is aligned with the acl code of the"
6,"limitations domain shift: in the current implementation, our prompting model relies on the availability of a training set. this assumption may not hold in cases where the relations to be discovered exhibit a significant domain shift from the training set. to address this limitation, future work should explore fully unsupervised prompting approaches that can better adapt to new domains and mitigate the impact of domain shift. limited number of relations: in this study, our analysis is restricted to a total of 25 relations. while this allows for a focused exploration of these specific relations, it also limits the scope and potential applications of our model. to broaden the applicability and effectiveness of our approach, future work should aim to utilize wikidata more com- prehensively, incorporating a larger number of relations for more extensive and diverse analysis."
7,"limitation of existing lm prompting schemes, which rely on the static knowledge internalized in parameters; therefore, when such knowledge are incomplete, inaccurate, and outdated, llms may generate factually incorrect answers. to tackle this challenge, we introduced a novel knowledge-augmented language model prompting (kaping) framework, which augments the knowledge for the input question from kgs directly in the input prompt of llms, with the fact retriever to inject only the relevant knowledge. the proposed framework is completely zero-shot, and versatile with any lms, without additional parameter updates and training datasets. we validated that our kaping yields huge performance gaps from the lm prompting model relying on its internal knowledge, especially with smaller lms, on the kgqa tasks. we believe our new mechanism for augmenting facts from kgs to the lm prompt will bring substantial practical impacts in generating knowledge-grounded answers."
8,"limitations we focuses on resolving various mentions from different domains. although we have tested our framework on multiple datasets, it relies on a humanannotated dataset and effort should be taken to investigate how the model performs with emerging domains without human-annotated data. our model works with mentions that have been extracted from raw text. it would be more practical if the model could work with raw text directly and interact with another mention-extraction module. the performance of the model is largely affected by the surface form of the mentions, although our framework is robust to variations in the surface form, it would be more beneficial to further investigate how adversarial turbulence in the mentions could affect the behaviors of the framework."
