,Text
0,"Possible limitations include the limited PLM architecture/size (although we include additional results with RoBERTa in the Appendix D) and faithfulness evaluation metrics are not necessarily comprehensive. And we only focus on text classification tasks. As a result, we do not investigate other language classification (e.g., natural language inference and question answering) and text generation tasks. If we can intrinsically know or derive the golden faithful explanations (Bastings et al., 2022; Lindner et al., 2023), the exploration of model robustness and explainability will be alternatively investigated for revealing the internal reasoning processes. And future work could include human study (e.g., evaluation about whether explanations help users choose the more robust of different models) and improve the robustness by more diverse ways (e.g., model distillation and data augmentation).
Our findings are also in line with Tang et al. (2022) and Logic Trap 3 (Ju et al., 2022) which claims the model reasoning process is changed rather than the attribution method is unreliable. Different from this two works – output probability perturbation or changing information flow, we view our results as complementary to their conclusion via sourcing the improvement of faithfulness. Although we show the link between robustness and faithfulness empirically, future work can strengthen the conclusions by discussion on a more conceptual and theoretical level. From a theoretical perspective, one possible reason is that the gradient of the model is more aligned with the normal direction to the close decision boundaries (Wang et al., 2022). In the future, we would like to analyze the relationship between robustness and explainability from geometric dimension.
Furthermore, we do not exhaustively experiment with all possible evaluation settings of interest even with the scale of our experiments. For example, saliency guided training methods (Ismail et al., 2021) could have been used as another baseline. We hope this work inspires more future work that develops more effective strategies to make explanations reliable and investigate how our findings translate to large language models, such as GPT-3 model family2, as with the emergent capabilities of these models, fidelity to their explanations or rationale will have societal impacts on accountability of NLP systems.
2https://beta.openai.com/playground"
1,"While we have included two distinctive dataset (CNNDM and XSUM) in our experiments, more non-news datasets could be included in future studies. Other possibilities for future work include comparing the capability of RL-based reward learning and contrastive reward learning in improving the factuality of abstractive summarization models."
2,"Our investigation focuses on one aspect of commonsense reasoning restricted to one dataset. There may be numerous other factors in real-world applications. Therefore, our findings may not comprehensively capture the entirety of commonsense reasoning phenomena. Another limitation is that for the sake of simplicity and feasibility, we assumed a fixed threshold of k=200 to categorize frequent and less frequent names. However, this threshold may not be universally applicable to all contexts or datasets, and different thresholds could lead to different results."
3,"This work introduced a powerful attackability detector but also demonstrated that its success is limited to a matched setting, where the same attack method is used in both training and evaluation of the detector. A second limitation with this work is that all experiments were carried out on natural language classification tasks. It would be useful in the future to extend these experiments to sequenceto-sequence tasks to have a more comprehensive set of results."
4,"This analysis relies on comparing model predictions with human annotations. One limitation of this approach is the following: we are assuming that the human annotated labels represent a reasonable ground truth. However, it’s likely that the annotations have their own bias issues. A future area
of work is to analyze how reliable the annotations are for some of the top keywords surfaced here, especially for reclaimed speech and for Tweets with AAVE. However, because previous work has found that word choice and profanity are likely stronger contributors to bias against AAVE than linguistic features of AAVE (Harris et al., 2022), we hope that bias mitigation techniques at the keyword level can also help alleviate bias against AAVE without the use of sensitive racial or dialect classifiers. Another fruitful area of future work would be to better understand the relationship between mitigating bias at the keyword level versus the dialect level.
Our methodology is helpful for detecting the most widespread and prevalent problems. However, there may be other serious problems that do not receive the same amount of traffic that still deserve attention. Oftentimes, smaller groups of people, especially those who live at the intersection of multiple marginalized identities can suffer the worst harms from algorithmic systems (League, 2022). Thus, relying on frameworks that focus on bigger segments of the population poses the risk of missing important harms to smaller communities. In this work, we develop a list of keywords for bias evaluation by analyzing a corpus generated from all English Tweets on Twitter. However, because English Twitter is primarily composed of users from the United States and the United Kingdom, our list of keywords for evaluation is likely heavily skewed towards US-centric or Western issues. One way to mitigate this would be to repeat the analysis conducted here, but using separate corpora for each country or upsampling Tweets from countries with smaller populations of Twitter users in order to ensure we are getting appropriate coverage in other countries with smaller user bases. This would help increase coverage for minority groups in the data we use for bias evaluation. Another critical area of work would include expanding the analysis to other languages beyond English. The overemphasis of English has led to the underexposure of other languages in NLP research (Hovy and Spruit, 2016).
This work treats reclaimed uses of slurs as an important facet of the speech of marginalized communities. However, reclamation is not a ""bullet-proof"" process - some may find reappropriated uses acceptable and others may not. Additionally, reclamation may only be deemed acceptable by in-group members or in certain contexts (Rahman, 2012). Since the marginal abuse model only uses the text of a sin-
gle Tweet (and not any information about the Tweet author or conversational context), it is difficult for the model to account for such nuance. Furthermore, because this model is used to moderate all English content on Twitter, the model implicitly assumes the same utterance has the same meaning across the world, which is an extreme oversimplification. In other words, the model does not account for local variations in language use. Reclamation can also backfire, for example the Hong Kong media’s mocking of the reclaimed use of ""tongzhi"" (literally meaning ’comrade’) by the gay and lesbian community (Zimman, 2017; Wong, 2005). This example serves to illustrate the essentially contested nature of reclaimed speech and how language ideologies shift over time. With respect to automatic content governance, shifting language ideologies indicate the importance of 1) meaningfully engaging and consulting with affected communities on models used for content governance, 2) the utility of regular audits and model refreshes to account for change in language use over time, and 3) additional user controls to better accommodate for multiple definitions of harmful content. Lastly, there are inherent limitations to fixing socio-technical problems through purely technical means (Ball-Burack et al., 2021). We hope that our analysis provides an interesting case study of some of the challenges associated with automatic content governance in industry and sparks further discussion."
5,"Our main limitation comes from dataset size. This was limited because we used human evaluation to label model responses as truthful or untruthful. That is, we have manually confirmed GPT-judge labels on Davinci responses, and extrapolated the system to Ada, Babbage, and Curie. Frankly, the limitations caused by the small size of the dataset were quite evident because the truthfulness detector was often biased towards producing one label (either 1 or 0). We attempted to solve this problem using lower regularization parameters, but this often produced models with lower performances. An ideal solution to this problem would be training the truthfulness detector on a large set of training instances, which is also our future direction."
6,"Our study covered three types of widely used single attention with different mechanisms of assigning attention weights. However, we did not cover some compound attention mechanisms such as dual attention mechanism [8] and Co-attention [27], which
might contain different patterns affecting the fairness of the models. Also, Transformer [24], the cornerstone of PLMs, should be considered in this study as it is composed of multiple self-attention modules, and the intersection impact of multiple attention mechanisms can be studied by incorporating this model. Apart from the classifier itself, the different word representation models, which are well discussed in Naseem et al. [21], can also be brought into scope since word embedding can also affect fairness. From the dataset aspect, the quality of text can be further improved with pre-processing techniques mentioned in Naseem et al. [20] to ensure better performance and reduce the effect of the irrelevant factors. Also, since the toxicity classification tasks are not easy even for a human, there are noisy data inside the chosen datasets since we found that we disagree with some of the humanannotated labels by manual checking. Furthermore, the HSOL and Jigsaw datasets are imbalanced in terms of distributions of different classes. Therefore, modifications can be made to the loss function in the same way as focal loss [16] or dice loss [15] to mitigate the influence of data imbalance."
7,"A limitation of our conclusions is that although it is necessary to use several fairness metrics to be able to properly quantify the bias, this is not enough. These metrics must be well chosen according to the context and the task being looked at. The expertise of a person working in the field is therefore always necessary to have the most complete possible interpretation of the bias. More specifically, the different fairness metrics measure distinct properties, and the fact that they are often incompatible has been a core part of the fair ML conversation from the beginning (Barocas et al., 2017). Thus, suggesting to choose a different metric depending on the sample size may sometimes be inappropriate, since this choice may depend on the meaning of the metric in a given application. We must therefore be very careful and see the notion of robustness as additional necessary information and not as a replacement for the metric’s meaning.
We also did not reduce the bias using advanced strategies because this paper focuses more on the analysis intended for a population closer to the law than to machine learning. In this vein, it is interesting to note that more and more tools are available to reduce bias. In particular, (Sikdar et al., 2022) makes it possible to reduce the bias according to several fairness metrics, therefore remaining in our
logic of taking several metrics. The main problem raised by our article comes from the fact that fairness indices are not stable when they are calculated. We should consider them as random variables and look at their law. The first step is to look at the mean and the variance as done in this paper but having the full distribution would be more interesting. Works that compute the asymptotic law can be taken as an example like (Ji et al., 2020; Besse et al., 2022).
Ethics Statement
Natural Language Processing is gaining a considerable amount of attention these days and it is extremely important to evaluate how NLP datasets will impact the gender bias when used to train models that will be used in the real world. This work uses different experiments and fairness metrics to shed light on the shortcomings of these metrics with respect to gender bias made by ML algorithms on textual data. We believe that transmitting knowledge from research to industry on a subject like fairness is essential to make the field of ML more ethical. Hence, this work focuses on issues that most affect the industrial ML landscape and contains a clear message to them on how they should change their current practices."
8,"The DEPTH+ metric resolves the problem of botgenerated Wikipedia editions that have many botcreated articles and bot-made edits on their articles. Yet, the DEPTH+ metric does not fix the problem of automatically translated Wikipedia editions in the Wikipedia project that their articles have been largely translated by poor direct translation or shallow template-based translation. The quantifications of these automatically translated Wikipedia editions in the Wikipedia project cannot be carried out as systematically as the bot-generated Wikipedia editions, and examining each Wikipedia edition separately is the only way to accomplish such quantification. Another limitation of the DEPTH+ met-
ric is depending on the active users metric, which dynamically decreases the DEPTH+ metric values if there are no editing activities on the articles in the last thirty days. We preferred to use the total unique users who made at least one edit but do not have that figure, so we are approximating it with the already calculated active users metric by the Wikipedia project."
9,"Despite memory and GPU limitations presenting significant obstacles for our project, we were still able to create high-quality fake scientific papers. Nonetheless, we believe there is room for improvement in addressing such limitations.
Due to the complexity of parsing PDFs, we are currently limited to specific sections (abstract, introduction, conclusion) instead of complete papers. Moreover, processing entire publications would require substantial computational efforts. We believe that selecting sections dynamically at random instead of a fixed choice is worth exploring and will
be the focus of future work. Beyond DetectGPT (Mitchell et al., 2023), other zero-shot text detectors such as GPTZero7 present promising solutions worth testing on our benchmark dataset. However, at the time of writing, such solutions are not available for experiments at scale.
In future work, we aim to address these limitations by exploring dynamic section selection, improving papers’ quality, adding human-LLMs co-created samples, and investigating the potential of zero-shot text detectors like GPTZero as they become more accessible and scalable."
10,"The main concern is that debiased techniques may remove simple biased features. However, to our knowledge, most debiased techniques (Rathore et al., 2021) can only remove biases across a concept subspace (e.g., the bias direction for gender) in the embedding space. Another setup of data debiasing, e.g., He et al. (2019), requires hypothesized biases to train biased models and is limited to tasks with known hypothesized biases (e.g., lexical overlap for NLI). Also, they remove biased examples rather than identify biased symbols (e.g., label hints). However, we still expect future works to consider other complicated patterns beyond symbol insertions or word substitution."
11,"We are optimistic that the algorithmic workflow presented in this paper can be generalized to other languages. When the victim models are in languages other than Chinese and English, however, we also acknowledge the uncertainty in achieving a high attack success rate while at the same time achieving fluency in generated examples. In addition, because of the variation in linguistic structures
across different languages, further efforts are required to design language-specific transformation methods (such as the homophone and morphonym transformations for the Chinese language case in this paper)."
12,"To recruit participants who are more likely to write posts in other languages, our recruitment was restricted to bi/multilingual individuals residing in the US. Due to these criteria, most participants in both studies were Spanish speakers and writers, which is the second most common language spoken in the US. Since Spanish is a rich-resourced language, findings about the quality or accuracy
of MT may be different if we consider a more linguistically diverse participant sample. Future work could recruit global bi/multi-lingual participants to understand broader perceptions of machine translation.
Our discussion section advocates for a number of control features that are currently not available in SNS. Our results suggest that these features would make users more comfortable with MT, but this suggestion would ideally be confirmed with a controlled experiment. Our future research will be focused on designing the proposed controls. In particular, we plan to conduct an experiment on prototypes of three different translation features: one allowing the user to read but not edit the MT, one allowing the user to add a translation manually, and one allowing the user to read and edit the MT if there are any errors. Each prototype will also have an option to include/not include the translation with each post. This experiment will measure which features most improves users’ perceived control, perceived satisfaction, ease of use, and intention to use MT."
13,"limitations. our modification utilizes sense embeddings. since the senses were not mapped to an external inventory, the senses cannot be interpreted. apart from the lack of interpretability, sense embeddings are superseded by contextual embeddings derived from transformer models with sense awareness (huang et al., 2019; levine et al., 2020; scarlini et al., 2020). while sense embeddings and contextual embeddings are not mutually exclusive, it is necessary to alternate between them for the purpose of privatization and optimization."
14,"limitations through our work, we analyze various sentiment and toxicity analysis models to determine if they show an ableist viewpoint. the results depict a statically significant presence of disability bias, and we publish our method for any individual to access and use. this step is crucial in the field of nlp to mention the ramifications a given model can have on society. one limitation of this work is that we analyze models that are trained in the english language. we understand that the social concept of disability can change for various cultures and languages. the scope of this paper for now only looks into one language. ethical statement the paper provides a method to parameterize ableist bias in nlp models, but we acknowledge that this is not the sole method that can be used for identification. the work is limited only to identification in sentiment analysis and toxicity detection models. there can be other methods of identification that are rapidly being worked on which may not have been included in this process. we also understand the effects various other forms of social biases can have when viewed alongside disability bias. we, therefore, will be working on measuring the combination of social biases through a cultural lens for the future."
15,"limitations our work only preliminarily explores the field of textual adversarial attack on chinese minority languages and evaluates the robustness of the tibetan part in the first chinese minority multilingual plm. the textual adversarial attack is a major threat in the information processing of chinese minority languages. we hope our attack method, experiment results, and"
16,"limitation (see ‘limitations’ section), we would like to underline that our primary goal of this article is to highlight the ample possibility of data leakage and the impossibility of verifying the lack of data leakage with a closed model. as long as the trend of closed models and continuous training loop continues, it will become more challenging to prevent data leakage (training-test data contamination) and ensure fair evaluation of models. therefore, in order to ensure the fair evaluability of the models, we argue that the model creators should (1) pay closer attention to the training datasets and document potential data contamination, (2) create mechanisms through which the training datasets and models can be scrutinized regarding data leakage, and (3) build systems that can prevent data contamination from user inputs."
17,"limitations in this paper we attempt to understand model responses using multiple prompts, and 2 different set- tings (tokens and full text). the gpt-3 responses were too inconsistent. we attempt at explaining our findings by analyzing the full text responses, but a more thorough analysis of the full text responses would shed more light into how these models behave. this will require extensive manual analysis of each statement and prompt response. currently we do not explore every kind of full text response for each category type and prompt. more work needs to be done to systematically analyze the full text responses and connect them to the token responses and confidence scores. besides, text-davinci-003 was the best performing llm when we started experimentation. recently released chatgpt api and gpt-4 from openai, and other open source models were not analysed in this study, but one could extend our study to any class of llms to assess llm quality as well as the differences among them."
18,"limitations. we note that directing the substitution to candidates with matching grammatical categories incurs additional information leakage that is not accounted for by our modification. too remedy the unaccounted information leakage, one could recast the candidate selection through the exponential mechanism (mcsherry and talwar, 2007)."
19,"limitations of its representation of the depth of collaboration in wikipedia corpora. we also quantified the bot activities in the wikipedia project and excluded the bot-created articles and the bot-made edits on wikipedia articles. we lastly proposed the depth+ metric, defined its formal definitions, and highlighted its features, including a better representation of the depth of collaborativeness, a user-centered depth metric, and bot-free wikipedia editions after the removal of the bot-generated articles and the bot-made edits on those wikipedia editions’ articles. we hypothesize that a metric that is a better measure of authentic human collaborativeness will be a better measure of the degree to which corpora authentically represents the language and the culture of native speakers. one key aspect of our future work is to find ways to test this hypothesis. specifically, we aim to examine the performance and societal implications of training llms on unrepresentative and inorganic corpora, particularly on the bot-generated wikipedia articles. reproducibility data collection, implementation of the depth+ metric, and an expanded technical report can be found on github at https://github.com/saiedalshahrani/depthplus."
20,"limitations presenting significant obstacles for our project, we were still able to create high-quality fake scientific papers. nonetheless, we believe there is room for improvement in addressing such limitations. due to the complexity of parsing pdfs, we are currently limited to specific sections (abstract, introduction,"
21,limitations and further enhance the utility of our benchmark for the research community. we release a repository containing our benchmark dataset as well as the code used for experimental results8.
22,"limitations to our knowledge, the current study is the first effort in the ml community aiming to define pi and estimate how much of it is present in two major training corpora, c4 and the pile. however, we recognize that there are ways in which our study can be improved, and directions in which future studies can be conducted. to start with, when annotating the pi found both by using presidio and regular expressions, we observed that new forms of pi have appeared with the advent of the internet, but have yet to be considered in traditional definitions (e.g. facebook events urls), despite their potential for risk. also, given the diversity of types of pi that exist, it is unsurprising that systematically detecting them remains a challenge. as we reported section 4, we found that both presidio and regular expressions were able to detect certain types of pi, such as emails and phone numbers, relatively well, but failed on other types, such as ssns and credit cards; however, without access to ground truth annotations, measuring and characterizing false negatives is impossible. other limitations of both types of approaches is that they are language- and often country-specific, and need to be adapted to contexts of application and languages. this can quickly become complex, because the format of common types of pi such as bank account numbers varies immensely depending on its country of provenance. finally, linguistic characteristics of individual languages make it difficult for multi-lingual pi detection since features that are relevant towards pi detection in some languages are not relevant for others; more work on developing more modular and extensive pi detection tools would be an important contribution to many communities and endeavors, and it is conceivable that ml-based approaches can contribute to these efforts. broader impact statement our work endeavors to help the nlp community better understand and quantify the types and quantity of personal information contained in popular training corpora. in order to strive towards this goal, we manually annotated a subset of the personal information detected in c4, which constitutes a dataset that could be valuable to the community. however, given the quantity of high-risk personal information that this sample contains, we do not feel comfortable disseminating it. we are, however, working on methods for developing synthetic and lower-risk labeled corpora to help develop better methods for detecting pi. as large language model development is increasing dramatically, more models will be trained on these data sources, so its becoming increasingly important to quantify and characterize the personal information present in datasets as well as help practitioners develop better pi detection methods."
23,"limitations while we endeavor in this work to shed light on the impact of various pseudonymization techniques, we recognize a major limitation of our work – especially the llm-based pseudonymization approach. using closed-source llms may not be an acceptable solution for many settings since it requires sending a (potentially sensitive) text to a third-party api, which, in the absence of appropriate legal safeguards and responsible-use agreements, defeats the purpose of privacy preservation. there are some more technical limitations of the work, such as the following: • while this is a problem that affects sensitive texts in all languages, all the experiments were conducted for data in the english language only. • llms are highly sensitive to prompts, as well as the number and ordering of examples provided for few-shot learning. in this work, we experimented with a limited number of prompts for llm-ps due to api cost constraints. • for the data privacy detection experiment, the flair ner system was trained using the conll-2003 dataset, which might affect its performance for privacy protection tasks. this may also apply to gpt-3 and chatgpt models as the authors do not state specifically on which data they were trained. • we considered only a limited part of named entity types, specifically, person (per), location (loc), and organization (org), whereas it is well understood that pii encom- passes a much broader range of data types (eg. dates, phone numbers, etc.). we also do not consider sentiments associated with named entities used for substitution in the downstream task of text classification. we plan to address these in future work."
24,"limitations the models used in this work are small, compared to the large language models (llms) used in many language generation tasks today. to attempt to show possible impacts of scale, two different sized models were employed in this work and show similar results, so it is likely that the method proposed here would scale to larger models. training dynamics were not altered between the two model sizes, which is a potential area for improvement. more sophisticated methods of inserting backdoors could also be employed than training one into the model, but this seemed to work well. ethical statement this work attempts to improve the state of watermarking llms in order to demonstrate ownership. our hope is to help improve the space of responsible llm usage by helping model creators assert or demonstrate ownership of their models, although there are probably applications of watermarks that we have not considered that may be detrimental. this work does expose ways to find watermarks, which could be used by a potential adversary who had stolen a model and was attempting to use it illicitly. however, we believe that disclosure of vulnerabilities allows stronger system construction and is preferred over security by obscurity."
25,"limitation of requiring knowledge of models and training data, we extract simple patterns (e.g., label hints and answer hints) from the min-min optimization to make text unlearnable. although our experiment explores patterns for text classification and question-answering tasks, the pipeline potentially works for any nlp task. reproducibility. to ensure the effectiveness of unlearnable modifications, we slightly tuned the training hyperparameters to achieve well-trained models, such as setting maximum gradient norms and early stopping according to validation sets. we open-source codes with configuration files, which contain hyperparameters regarding model architectures (e.g., the number of layers), batching (e.g., data sampling), and training setups (e,g., learning rate). since these files are configurable in json format, future works can easily reproduce and extend the experiments."
26,"limitations although we have shown that the overall performance of imbert is superior, we mainly target insertion-based backdoor attacks. however, substitution-based attacks have been recently investigated and proven to be a practical approach in text classification (qi et al., 2021c) and machine translation (wang et al., 2021; xu et al., 2021). it is unknown whether imbert can effectively adapt to these attacks. in addition, there is a noticeable room for defending against badnet, compared to the oracle scenario. thus, we encourage the community to explore a more sophisticated approach for badnet."
27,"limitations. for authors, the editing of posts would only be possible for output languages that they can read and write in. furthermore, authors’ ability to edit the translations of their posts could result in ethical concerns, as this feature may be used to create language-specific misinformation. a solution to this problem is to present the reader with both the original machine translation and the user-edited version. alternative solutions include allowing users to disallow the automatic translation of posts on a case-by-case basis, or to filter the audience of a post based on its original language. this aligns with the fact that users’ language choices are intentional, and depend heavily on the topic of the post and the target audience [13]: many purposely write posts in a certain language as a means to target those who speak that language. overall, we see a substantial benefit in making sns users aware of how their posts are translated and shared with others, and in allowing them to remove the translation if the post was meant to be targeted to a specific audience or correcting the translation to preserve the original meaning of the post. together, these design solutions would ensure users that their information is conveyed accurately and only to their intended audiences."
28,"limitations our methodology is currently tested with only english. we conjecture that the methodology should be applicable to other languages, but may be limited by the capacity of llms in those specific languages. it is possible that value-aligned knowledge distillation may be more difficult with languages from countries and regions that do not have a complete set of human value definitions. thus, exploring the value-aligned task in different languages other than english is a promising research direction. our main experimental results are based on a 175b parameter model, which requires large gpu resources or access through an api. this may hinder other researchers from reproducing experimental results. additionally, we explored different sizes of llm including 1b and 6b models, which do not require large gpu resources, and showed they can achieve comparable results. we hope they can be possible alternative options for researchers who may not have access to 100b+ models. although sexism is a suitable case study for us to investigate the feasibility of the value alignment task as we have shown throughout this work, it is still one domain. further expansion to different domains or value-aligned classification tasks such as the detection of racism, toxicity, other than sexism, are needed."
