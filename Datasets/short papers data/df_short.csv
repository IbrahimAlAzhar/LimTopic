,Text
0,"This evaluation had two limitations. First, although the method is not language-dependent, it was evaluated on a single language, Japanese. It would be worthwhile to evaluate the method on other languages to examine the approach’s versatility. Second, the method uses dictionaries to obtain patterns. Although Japanese morphological analysis commonly uses dictionaries to perform lemmatization, it would be worthwhile to evaluate the method with only training data or dictionaries derived from text.
Below, I discuss the current limitations for word segmentation, POS tagging, and lemmatization in detail.
Word segmentation The proposed method’s accuracy of word segmentation will depend on the target language’s typological factors (Shao et al., 2018), such as the character set size, lexicon size, and average word length. Among those factors, the character set size will especially matter because the current patterns mostly comprise surface strings and are likely to suffer from data sparseness. It will thus be valuable to evaluate the method on Chinese, which has a larger character set than Japanese. It will also be important to evaluate the method on languages with different typological factors from Japanese, such as Hebrew and Finnish. The training data size will not matter if the method is used to approximate some existing resource-efficient method via structure compilation (Liang et al., 2008).
POS tagging Compared to word segmentation, POS tagging requires more complex and abstract feature sets that are tailored for the target language and POS tag set (Spoustová et al., 2009), which poses a challenge for the proposed method. The current pattern template is tailored for Japanese and the JUMAN POS tag set; hence, for other languages and POS tag sets, a pattern template will need to be designed by referring to the feature templates of existing learning-based methods for the target language and POS tag set. Because the method jointly solves word segmentation and POS tagging in a left-to-right manner, patterns cannot leverage certain abstract features from posterior contexts of the target word (e.g., the next word’s suffix). For application to other languages, it would be worthwhile to explore not only left-to-right processing but also right-to-left processing and a cascaded pipeline approach.
Lemmatization The approach here currently requires a morphological dictionary with lemmas or a fine-grained POS tag set that includes conjugation types and forms to perform lemmatization. Because lemma generation rules for other languages can be induced from lemma-annotated datasets (Straka, 2018), the method could be applied to other languages by using such lemma generation rules as the target labels for classification. Challenging target languages include morphologically rich languages such as Arabic and Czech."
1,"This paper argues that the proposed task of ellipsisdependent reasoning is a difficult challenge for GPT-3 models, which are among the most powerful current language models. The data constructed here is restricted to English, and furthermore is restricted to a single form of ellipsis, namely verb phrase ellipsis. It may well be that other forms of ellipsis may give rise to different effects, and it is also important to test the claims made here on other languages."
2,"While we have improved the asymptotic running time of a classic algorithm with regard to grammar size, the time complexity of our algorithm is still cubic in the length of the input. Our result follows the tradition of dynamic programming algorithms that trade time for space by memoizing and reusing pre-computed intermediate results. The usefulness of this trade-off in practice depends on the specifics of the grammar, and while the complexity is strictly better in terms of non-terminals, it will be most noticeable for denser grammars with many nonterminals."
3,"Limitations (unnumbered)
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
4,"One of the main limitations is that we used the standard LIWC-based analysis approach, which is purely lexical and does not take into account the context in which a word appears. Consequently, many words that have very specific senses in the context of the IETF get miscounted as occurrences of LIWC categories. This could be addressed by a more advanced method of mapping to LIWC categories that would account for context. Another limitation is that we manually generated a filtering list containing words specific to the IETF. This list might not be exhaustive enough. Also, we were limited by not conducting an exhaustive hyper-parameter search on our models. We also understand that many emails are longer than 512 tokens (the input limit of the BERT model we used) and might have not been captured completely by our BERT model. However, most of the emails do fit into this BERT sequence length limit. We did not fine tune BERT on the IETF data; this might have given better performance, although it is not clear if it would have given more insight: our main goal is not performance but analyzing/comparing characteristics of existing models. It is also worth highlighting that the data used in this work is strictly in English, and the psycholinguistic categories in LIWC are also based on English language. Hence, this study may be biased and not fully capture variations in linguistic traits that are culturally agnostic.
Ethical considerations — Participation in the IETF is bound by agreements and policies explicitly stating that mailing list discussions and Datatracker metadata will be made publicly available.7 We use only this publicly available data in our analysis. We have discussed our work with the IETF leadership to confirm that it fits their acceptable use policies. We have also made provisions to manage the data securely, and retain it only as necessary for our work.
7See both https://www.ietf.org/about/note-well/ and the IETF privacy policy available at https://www.ietf. org/privacy-statement/."
5,Section 6
6,"This paper mainly focuses on modelling basic meaning to identify metaphors, typically learning basic meanings from literal annotations of the VUA dataset. However, our analysis reveals that the literal annotations of the VUA dataset are incomplete, which means that some words in VUA have no literal instances annotated. Although we propose using contextual word embeddings as a backup in this paper, another promising solution for this issue might be using external resources such as dictionaries. Leveraging dictionaries is commonly used to assist manual metaphor detection, so it could also help our BasicMIP mechanism to generalise. We leave this for future work."
7,"We highlight three limitations of our work. The first is that xsim++ is automatically constructed. There could be noisy sentences leading to errors that are irrelevant to the quality of encoders. The second is that xsim++ applies transformations solely to English sentences. Generalizing it to non-English language pairs requires additional research. Finally, we have experimented with the two most popular multilingual encoders: LASER and LaBSE. There are other available approaches which would be interesting to also validate xsim++ against."
8,"There are some limitations in the use of GPDA.
• The label propagation procedure requires anchor matching in the light of annotation precision, which limits the unlabeled data source. However, Wikipedia is a open-domain easyto-fetch corpus with anchor links, which can somehow mitigate the issue.
• Augmented Data generated by GPDA provide more diversity. But for some datasets, simple modifications (NERDA) on the original words performs better. We are investigating a hybrid approach to apply GPDA and NERDA in the same framework."
9,Left blank.
10,"As mentioned in the main paper, one of the limitations of our Centrum model is that it tends to produce longer outputs in comparison to PRIMERA. This necessitates controlling the length of the summary by truncating to a desired length. Moreover, due to our requirement of at least three documents in a cluster for centroid computation, we are unable to utilize clusters of only two documents present in Gu et al. (2020). This constraint significantly reduces the utilizable corpus size, leading us to work with roughly 45% of the corpus size used by PRIMERA. Future research could explore the possibility of initializing Centrum with the gap sentence generation-based Pegasus (Zhang et al., 2020) single document summarization objective, potentially allowing for full utilization of the corpus size of Gu et al. (2020)."
11,Section 6
12,"Section ""Limitations"" (unnumbered, page 5)
7 A2. Did you discuss any potential risks of your work? There are no reasonable risks in our work."
13,"Limitations
7 A2. Did you discuss any potential risks of your work? We don’t see the potential of how our two techniques can be misused.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 4
3 B1. Did you cite the creators of artifacts you used? 2, 4"
14,"Limitations The model we proposed is specifically for classification, while it is possible to be extended to other NLP tasks by changing the highlevel task-specific layer. Besides, in the evaluation, we focused on English corpora. We plan to test on other languages in the future.
Potential Risks We make our code publicly available so that everyone can access our code. As the model is a classification model, it does not generate risky content. Users should also notice that the classification predictions may not be perfectly correct."
15,
16,"Unnumbered ""Limitations"" section at the end."
17,"While BOLT shows an impressive performance in imposing soft constraints and some hard constraints, it still lacks when it comes to imposing harder constraints, for e.g., keyword control with more than three keywords. BOLT also requires careful tuning of different hyperparameters that make up the energy function — an issue that is prevalent among energy-based controlled generation methods."
18,"Although multilingual, the constructed open KB is limited to the sampling of the chosen six languages. We do not know how well the system will generalize to various language families that have not been considered here. Further, even among the languages considered, the performance of even the best-performing systems, as measured through H@1 is still in the low 20’s. Therefore the models are not yet ready to be deployed for real-world applications."
19,Limitations section (after Section 6: Conclusions)
20,"Broader Impacts
Despite the promising performance of MARCO at detoxifying text, there are several limitations, ethical considerations, and broader impacts of our approach, which we list below.
First, in this work, we seek to detoxify sentences. However, toxicity itself is a subjective and sensitive concept with large potential downstream impacts caused by annotator and subsequent model biases (Sap et al., 2022). We somewhat mitigate this variation by selecting human evaluators that scored highly on a toxicity qualification task (see Appendix D), in line with a prescriptive paradigm of toxicity annotation (Rottger et al., 2022). Future work could investigate the effect of demographics on preference for different rewriting algorithms, e.g., in a more descriptive paradigm.
In addition, achieving meaningful semantic preservation in detoxification is challenging. Specifically, it is difficult to disentangle the toxic and non-toxic meanings from the input, making it challenging to generate detoxified rewrites with high preservation of only the non-toxic content; this may risk minimizing marginalized groups’ speech (Xu et al., 2021). Partially, this could be due to a lack of context incorporation (social, conversational, preceding sentences; Yerukola et al., 2023); future work should consider adapting detoxification methods in context (Cheng et al., 2020; Roy et al., 2023).
MARCO also requires finetuning two pretrained LMs, which is not computationally insignificant (Strubell et al., 2019; Schwartz et al., 2020). Future work could explore using smaller LMs to control a larger model (Liu et al., 2021), or even more lightweight approaches.
Additionally, we acknowledge that in the evaluation, we expose Turkers to toxic content, which might harm individuals, especially those with identities that the offensive content applies to (Roberts, 2017; Steiger et al., 2021). However, we pay a fair wage (US$8/h) and our work is approved by our institution’s ethics review board (IRB). See Appendix D for further details.
Another major ethical implication of our work is that, following previous work, we use the Perspective API to automatically assess toxicity, a classi-
fier which contains documented biases (e.g., demographic biases and racial biases; Dixon et al., 2018; Sap et al., 2019). Future research could consider different, more holistic views of toxicity and biases (e.g., Sap et al., 2020).
Finally, although our application in this paper is detoxification, we acknowledge that MARCO could be applied for the opposite purpose, ie., generation of toxic text from non-toxic text; this is a malicious application which we condemn. Although this issue is more prevalent for controlled generation methods (McGuffie and Newhouse, 2020), this is still a risk MARCO faces. In a similar vein, we do not endorse using the toxicity or microaggression datasets to develop models to generate more toxicity or microaggressions, as this may incur harm, especially to marginalized/vulnerable populations."
21,"Perhaps the main limitation of this work is that we only explore the approach within the context of machine translation benchmarks, although we conduct extensive experiments within this task that cover different training data scales and diverse pairs of languages, including low-resource ones. Nevertheless, we remark that the proposed approach is entirely general-purpose, and can be applied to any other language generation or even any neural classification tasks. We leave it to future work to investigate whether the same gains would apply in those settings. Furthermore, we have not yet explored how this technique would interact with other modelling choices, such as different optimizers, training objectives, or subword tokenisation algorithms. Lastly, our unigram initialisation of the bias term is currently done at the level of subword units, which do not always correspond to lexically or morphologically meaningful linguistic units. We leave the extension of this approach to more meaningful linguistic units, such as words or morphemes, to future work."
22,"Our approaches that are developed in the parameterefficient weight ensembling framework, and experiments have the following limitations. First of all, our framework cannot efficiently extract information from the parameters of the trained lightweight objects, resulting in relatively unsatisfactory performance of the approach resorting to the information from the weights, i.e., GraNd. Furthermore, the modules that we focus on in our analysis of module importance are only blocks and sub-layers of the blocks. We have not probed finer modules, in which we speculate more precise information about transferring lightweight objects across tasks is concealed. Last, all tasks in our experiments are formulated into the text-to-text format, and we have not conducted analysis on tasks in other formats."
23,"Page 5, after the Conclusion, and before the References.
7 A2. Did you discuss any potential risks of your work? Our work is only about algorithms, it is almost impossible to exist potential risks, so we didn’t discuss them in our paper."
24,"In this paper, we suggest incorporating textual and visual data from search engines for multimodal relation extraction. Despite the fact that the proposed model yields competitive results on the benchmark, it still has several limitations. Firstly, using a search engine is a feasible way to obtain related knowledge, but it also brings the issue of noisy evidence. Unrelated visual and textual evidence returned by the search engine may lead to incorrect predictions from the model. Additionally, not all the retrieved evidence is equally reliable, and sometimes sources may contradict each other. On the other hand, retrieval-augmented methods are slower than content-based counterparts, since retrieving evidence from the Internet requires extra time. Therefore, it may not satisfy some of the time-sensitive scenarios. Lastly, evidence may be presented in different forms other than texts and images. For instance, structural information such as tables, info lists, and knowledge graphs also provide important contexts for identifying semantic relations. Humans are able to extract relevant information from these heterogeneous sources for inference, while our relation extraction system can only model and reason over textual and visual evidence."
25,"In our work we faced numerous types of limitations that fall under different categories.
Data Our relatively small dataset size limits our analysis, especially with the use of language models. Furthermore, the label distribution is skewed across the different specialties (domains), which affects model performance, robustness and generalizability. The differences in distribution might be the result of how the data was collected, which was not in light of the anchor words, or due to the domain’s nature and/or the medical providers’ language of that specialty. Furthermore, the time frame that the data was sampled from might manifest certain biases that are different from other time frames. Finally, our datasets are only representative of a small number of specialties from two medical institutions. Patient populations and providers may vary greatly across medical fields and additional institutions.
Task The formulation of the labels for our task imposes limitations and challenges. Stigmatizing language is subjective and can vary between the perspective of the patient and the medical provider. As a result, we are aware that our medical experts’ annotations might impose a bias. Additionally, the negative connotations of language might be ambiguous and can change depending on a medical expert’s identity, background and specialty, which creates a bias that is hard to mitigate.
Computational Resources We only used IRBapproved servers to access the dataset and perform the experiments. Because these platforms had limited computational capacity and lacked the specifications required to build more complex neural models, we were not able to include more recent language models in our experiments that might
have yielded better performance. In the future, we hope to have access to machines that support more recent and state-of-the-art models."
26,"One potential way to improve the extractive performance of a generative system is to explicitly model the likelihood of extracts during training. Driven by this intuition, we investigate creating a mixture of extractive and abstractive candidates for contrastive learning in BRIO. Specifically, we obtain extractive candidates with beam labeling proposed in Xu and Lapata (2022b), while the abstractive ones are from the original BRIO training data. Nevertheless, as we can see, this mixing method hurts both BRIO’s extractive and abstractive performance. However, it is noteworthy that extractive summary is important in a wider context, as shown in Section 4: reference summaries
in CNN/DM are highly extractive and optimizing a model on these summaries therefore may have provided it with the task instruction needed for extractive summarization, albeit implicitly. We leave the study of a more effective extract-aware learning strategy for future study.
Furthermore, we emphasize that the conclusions drawn in this paper are based on results produced on English datasets from the news domain. Even though these datasets are established benchmark datasets for summarization it is imaginable that other domains and languages may have produced different evidence. Despite this, the results remain insightful as the results show that extractive summarization is in fact feasible with modern abstractive systems. In future research, we look forward to shedding light on the possibilities and limitations of the proposed methods in a broader context."
27,"Section 9.
7 A2. Did you discuss any potential risks of your work? Section 9."
28,"Our proposed method has the following main limitations which we believe are important directions for future work to address:
1. Gender dependency: Our approach does not account for sentences that only make sense for a single gender. For example, sentences like ""She needs to see a gynecologist"" would not be captured by our method. This is a common problem encountered by most debiasing algorithms as it is difficult to distinguish these.
2. Finite wordlist: The wordlist does not contain all gender-based words as the language con-
tinues to evolve. We believe that future works could employ better approaches that can automatically mine gender words relevant to a dataset.
3. Blunt substitution: The phrase substitution method is an improvement over direct word substitution, but there are still plenty of instances where the new sentence might be semantically incorrect. This does not have any major implication on inference as we are only doing few-shot learning, but it should not be extended to the entire dataset.
4. Binary gender: The method only focuses on the male and female gender. It does not consider non-binary or gender-neutral pronouns such as ""ze/hir."" This can be solved by using an updated wordlist, but the authors could not come across one at the time of writing.
5. Downstream analyses: While our work proposes methods that show reduced gender bias as per a set of metrics, the work in no way claims to reduce gender bias in general, especially on downstream tasks. However, we strongly believe that this technique holds potential to reduce gender bias on downstream tasks as well since we adopt a regular finetuning approach and focus mainly on better data interventions. Moreover, recent research has shown that fine-tuning-based debiasing approaches do not damage a model’s internal representations to a critical extent (Meade et al., 2022).
Overall, these limitations suggest that our approach may not be suitable for use in contexts where gender-specific or non-binary language is prevalent, and the underlying wordlist should be frequently updated."
29,"In the section ""Limitations"""
30,"The general performance of CLiCoTEA could be improved with a better MPLM than mBERT, such as XLM-R which has a larger token vocabulary and has been pre-trained on a much larger dataset. Our approach is currently not applicable to generation tasks where a multilingual text decoder is needed to generate text in unseen languages. We leave this
adaptation for future work. Unlike the statement made in Zeng et al. (2022), current multilingual VL models still do not surpass the Translate-Test baseline of the tasks from IGLUE benchmark. The performance of CLiCoTEA is promising but the best scores are still obtained when translating everything to English and using the (English-only) ALBEF model. The smallest difference in accuracy on MaRVL dataset between CLiCoTEA and ALBEF with Translate-Test is obtained in Swahili (-2%), while the gap is much larger (around -6%) for the other languages. Outperforming the Translate-Test achieved by ALBEF still remains an open challenge, especially for high-resource languages."
31,"The method to select negative examples could be improved, as randomly selecting negative examples for training might lead to identifying most of examples in the evaluation datasets as reasonable. Secondly, we did not explore using other number of candidates in the training set, we always use 2 candidate answers for each question."
32,"Section 7
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
33,"Limitations Section
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
34,"Our study has three important limitations. First, our study is small in scope. By their nature, experts are difficult to recruit and consequently the domains we can cover are limited. The small sample also suggests that the quantitative measures may not be stable in a larger or more representative sample.
Second, our observation process was somewhat artificial. We generated replies for our experts and did not to do any prompt tuning. This reflects the way the expert chose to ask the question, but does not capture the ceiling of performance that would be possible in a conversation. As the Family Medicine expert noted about our question comparing Wikipedia to ChatGPT, “for more detail one could spend more time with Wikipedia and to the organization themselves, but chat provides an immediate general summary and the opportunity to drill down further with ongoing questions and conversation.I have used chat GTP to do medical and biological research In a matter of minutes which would have taken me hours previously”. A more extensive study on information seeking behaviors would be of interest and Liu et al. (2023) is a useful step in that direction.
Third, the responses across experts are not necessarily comparable. We allowed experts to choose their own questions and provide their own interpretations of the key measures like coherence or conciseness. Comparability of scales across contexts is a long-standing problem in survey research (King and Wand, 2007) and we highlight some of the concerns around the accuracy question above. Nevertheless, we felt that asking a set of closedended questions would help to provide some aggregate judgment, adding some systematic data to the anecdotes shared in public forums. While we caution about drawing any binding conclusions from our preliminary work, we felt that given the fast-evolving nature of the technology, a quick assessment was merited. Our findings are broadly supported using different questions and methodology in Liu et al. (2023).
One important aspect that is out of scope in our analysis is differential accuracy by question asker. Latanya Sweeney’s classic study of racial discrimination in online ads (Sweeney, 2013) points to the possibility that how a question is asked or where it is asked from could result in inaccurate or harmful answers for marginalized communities (see also Noble, 2018; Benjamin, 2019). We have also focused exclusively on English language questions and answers, but differences in easily-available training data across languages can produce substantial differences in the information offered. For example, Yang and Roberts (2021) shows that embeddings trained on Baidu Baike—an online Chinese encyclopedia—encode substantially different
associations with sensitive historical events and people than Chinese Language Wikipedia (which is regularly blocked in China). There is much more to understand about the degree to which large language models can mimic expertise."
35,"HyDE has yet to be tested in a large-scale and multisite setting, which may offer more generalization challenges. Furthermore, an evaluation of notelevel classification performance was not conducted. Although we expect that HyDE would perform well under such an evaluation, this would require heuristics to aggregate multiple MCMs per note."
36,"Section 5
7 A2. Did you discuss any potential risks of your work? Section 6"
37,It’s the last section and it is unnumbered.
38,"The underlying assumption of our method is that abstract reflects the entire article, creating an unbiased summary of the paper. However, abstract does not guarantee an objective representation of the paper, can often emphasize the main findings while discarding details that the authors deem insignificant. This can lead to potential inaccuracies in paper representations, affecting the results of paper retrieval and recommendation.
Also, in this work we did not exhaust all possible training settings and evaluation strategies due to limited resources. We perform evaluation using three different standards. While we selected the most relevant evaluation tasks, it would be interesting to assess the quality of representations in other ways, such as citation graph reconstruction, predicting reader activity and other clustering-based evaluations. Additionally, with the emergence of large-scale language models, another interesting direction for future research is to investigate the relationship between model size and final performance."
39,"Our eKnowIA attack contains logical rules designed specifically for the English language. While these rules may apply or be adapted to other languages with simple morphology, there could be languages in which completely new rules may be needed. Both our attack and the KNOW method rely on knowledge bases, which may sometimes be noisy. We employed manual efforts to eliminate (a small number of) noisy triples from ConceptNet. Our attack also relies on a manual annotation to ensure that the adversarial inputs are natural (estimated to be the case 81.5% of the time). Finally, we were not able to test our methods on instances with long text, as we are not aware of datasets with NLEs for long text inputs or long NLEs."
40,"The main limitation is that the in-domain noise is hard to recognize in noisy multi-party conversations. Though our proposed RARM achieves the
best performance compared to all baselines, we find that if the content of the noise is close to the multi-party conversation’s content, the average accuracy of all methods is not high, how to improve the performance on these hard samples is worthy of further study."
41,"Whilst the system presented within this paper is capable of allowing human-in-the-loop contributions (via selecting the input keywords on which to condition the output), it is not able to produce tongue-twisters that take advantage of particular features of speech sounds such as place and manner of articulation, in order to create more advanced outputs that exploit phonetic relatedness (rather than exact matches). The same can be said of our proposed metrics, PO and Init-PO, which do not account for phonetic similarity across sounds that share manner/place of articulation (e.g. ""she sells sea shells""). Additionally, whilst commonly known tongue twisters may follow a particular format (e.g. rhyme schemes), such schemes and templates have not been enforced here. We also do not demonstrate the capabilities of these systems if they were trained on phonetic transcriptions explicitly, as we only aim to assess their performance when training on graphemes in standard orthography."
42,"In this section, we discuss some limitations of our method and future work based on the limitations.
First, the enhancement of the word-level module is not as strong as the remedy of the sentence-level module. Our word-level module solely achieves improvement compared with XBERTScore but doesn’t improve as much as the sentence-level module. The main reason is that the XBERTScore framework lacks sentence-level semantic knowledge. Besides, our word-level self-guided contrastive method doesn’t resort to external information and only consolidates the alignment already existing in the pre-trained language model. Second, ReFreeEval performs comparably with baseline models on language pairs involving German. We guess it is due to the evaluation of QE. Ma et al. (2019) mention that the evaluation results across all language pairs are unstable in “QE as a Metric” track and can’t explain yet.
In the future, we’ll further explore valuable external information on word level. And we’ll try to explore discrepancies among language pairs to optimize the results. In addition, our simple but effective data augmentation method - clause per-
mutation doesn’t rely on rules or toolkits, which is an initial attempt at modeling fluency. It could benefit from further refinement such as languagespecific knowledge, syntactic and semantic parsing to recognize clauses. We’ll conduct an in-depth investigation into further work."
43,"Section: Limitations
7 A2. Did you discuss any potential risks of your work? No potential risks"
44,Limitations
45,"The experimental environment we used for testing our agents gives artificially generated natural language text, whose distribution of vocabulary, syntax, and semantic frames is controlled and limited to what the natural language text generators can provide. While we tried to include out of vocabulary for entities in our experiments, applying the proposed approach to natural language text in the wild, such as chatbots working with human, must be faced with issues such as out-of-vocabulary entities, relations, etc. We believe, however, approaching from controlled “wildness” is an important direction of the work for interactive-text agents.
The experiments and embodiment of the method presented here also makes some assumptions on the underlying model (MDP) of the environment. These are discussed in the problem definition and methods (section 2 and 3). Perhaps the most important is the assumption that the environment can be sufficiently approximated with logical states. We also used a deterministic planner so highly stochastic environments are currently out-of-scope."
46,"One limitation of our work is that we focus on robustness of pre-trained transformer language models against word-level adversarial attacks, which is the most common setting in this area. Future work could extend this empirical study to other types of attacks (for example, character-level and sentence-level attacks) and for diverse types of architectures. Further, it will be very interesting to theoretically understand how label smoothing provides (1) the implicit robustness to text adversarial attacks and (2) mitigates over-confident predictions on the adversarially attacked examples."
47,Section 5.
48,"We find that 8 out of 14 reviewed works do not include an adequate discussion of the limitations of gloss approaches, inadvertently overstating the potential usefulness of their experiments.
In the context of sign languages, glosses are unique identifiers for individual signs. However, a linear sequence of glosses is not an adequate representation of a signed utterance, where different channels (manual and non-manual) are engaged simultaneously. Linguistically relevant cues such as non-manual movement or use of three-dimensional space may be missing (Yin et al., 2021).
The gloss transcription conventions of different corpora vary greatly, as does the level of detail (see Kopf et al. (2022) for an overview of differences and commonalities between corpora). Therefore, glosses in different corpora or across languages are not comparable. Gloss transcription is an enormously laborious process done by expert linguists.
Besides, glosses are a linguistic tool, not a writing system established in Deaf communities. Sign language users generally do not read or write glosses in their everyday lives.
Taken together, this means that gloss translation suffers from an inherent and irrecoverable information loss, that creating an abundance of translations transcribed as glosses is unrealistic, and that gloss translation systems are not immediately useful to end users."
49,"Section without number, after conclusion"
50,"We identify three limitations to our approach. First, parsing research claims and automatically classifying a sentence’s purpose (its meta-discourse) are not solved problems. It is more prudent to surface novel claims supported by original research than an author’s allusion to other research as background context. Second, the domain of biomedical scientific text is complicated by wordy prose, hedging, and long-distance anaphora. These aspects make natural language understanding challenging and present implementational challenges for tokenization, including truncating long sentences and extracting meaning from out-of-vocabulary tokens. Third, commonsense reasoning for detecting contradictions in biomedical text requires expert background knowledge and a working definition of when contexts are sufficiently aligned such that two claims are called contradictory, which may differ depending on the use case. We believe that context sensitivity and interpretability analysis of LLMs for NLI in challenging domains like this using attention mechanisms or frameworks such as
maieutic prompting (Jung et al., 2022) are particularly fruitful research directions."
51,Final required Limitations section
52,"We acknowledge the following limitations of our work:
• While the oracle selects a sentence according to the benefits it provides when performing NER, it does not consider the interactions between selected sentences. This may lead to lowered performances when the several sentences are retrieved at once.
• The retrieval heuristics considered are naive on purpose, as the focus of this work is not performance. Stronger retrieval heuristics may achieve better results than presented in this article.
• The studied documents only consist in the first chapter of a set of novels. Using complete novel would increase the number of possible information to retrieve for the presented global heuristics."
53,"Our analysis of the behavior of SPRL focused on intrinsic task scores. Higher SPRL scores suggest a better system. In practice, we do not yet understand how these scores affect downstream uses of SPRL labels. Furthermore, SPRL datasets are relatively small and are English only. As we are limited to the labels in the existing datasets, we are uncertain about how our results would generalize to larger datasets, new domains, and other languages."
54,"Limitations (unnumbered, page 5)"
55,"While the initial results are promising, the accuracy of our method remains lower than human VQA accuracy and models finetuned on the VQA datasets, which suggests that there may still be substantial progress that must be made before few-shot VQA methods with code synthesis are useful for practical real world applications. Also, further work is needed on extending the framework to additional primitives, as the results in Appendix F show that doing so does not always lead to improvements over the baseline method. Another limitation of our approach is that it relies on large capable LMs, which may be restricted in use due to compute requirements or cost (e.g. via available APIs). We also focus in this work on benchmarking VQA capabilities with English as the primary language – future work may extend this to other languages via multilingual LMs."
56,Section 6
57,"The first clear limitation of our approach is its textbased nature. This prevents important audio information, typically silences in speech patterns, from being exploited to generate subtitle breaks. A more complete system could be devised though, for instance by associating our text-based approach with the information provided by a forced alignment toolkit, whenever audio information is available. A simple method along these lines could be the following: 1. Apply our MLM-based segmentation but only generating a unique segmentation tag SEG; 2. Insert EOB markers wherever the
7Examples of segmented subtitles can be found in Appendix A.
silence between two aligned words is above a specified threshold; 3. Traverse the text sequentially and replace SEG with EOL if there exists a previous marker of type EOB, otherwise replace with EOB. We left this use of our method in combination with audio information for future research, as audio alignment for subtitles typically involves additional factors such as non-literal transcriptions.
Additionally, our method is limited in its adaptability to specific segmentation guidelines, which may be company-specific. The main adaptable parameters of our methods are the minimum and maximum parameters of the segmentation window, and the set of predefined punctuation marks over which masking is computed, neither of which could fully model idiosyncratic segmentation guidelines. However, in our experience at least, segmentation in real professional data tends to display varying degrees of consistency with respect to guidelines, and natural linguistic breaks seem to be the dominant factor for subtitle segmentation. A specific evaluation would be needed on data from varied professional datasets to determine the extent to which our method might deviate from specific guidelines.
Finally, other aspects of subtitling, such as the recommendation in some guidelines for subtitles to appear in a pyramidal view, i.e. with the first line shorter than the second line, have not been taken into consideration in this work. Our aim was to evaluate our core LM-based approach without additional variables that can vary across guidelines and may also have led to results that are more difficult to interpret overall. Our approach could nonetheless be easily augmented with constraints on relative line lengths within subtitles, by incrementing the scores of segmentation candidates that respect this surface-level constraint."
58,"Section 5
7 A2. Did you discuss any potential risks of your work? Our work introduces no additional risks on top of the risk associated with the underlying technologies."
59,In the Limitation section after Paper’s conclusion.
60,"The datasets in this paper systematically control lexical cues and world knowledge between critical conditions, allowing us to tease apart the effects of statistical heuristics versus reasoning about causal relations. However, the manipulation brings unnaturalness to sentences when scaling up into large-scale synthetic datasets, and constrains the level of linguistic complexity. As we have seen in Exp3, the small-scale dataset has more complex combinations of conflicting lexical triggers than the large-scale dataset, causing language models to behave differently across datasets. Though we further address the effects of conflicting lexical cues in Appendix A.4, it will be valuable to carry out additional investigation of effects of sentence naturalness, and to consider designing large-scale datasets using naturally-occurring data.
The study raises and leaves open a number of interesting questions: How exactly might counterfactual reasoning benefit from world knowledge? To what extent does GPT-3’s stronger performance reflect robust logical and counterfactual reasoning? While we lay out some possible explanations in the Conclusion and investigate the role of other linguistic and non-linguistic factors in the above experiments and in the Appendix, we leave additional systematic analysis for future work.
Finally, the experiments use English, in which counterfactual conditionals have distinct and systematic linguistic markers relative to other types of conditionals. It would be interesting to investigate other languages in which counterfactual conditionals are not marked linguistically, and require world knowledge to disambiguate. For example, a Chinese conditional could be ambiguous between “if it had rained today” and “if it rains today”."
61,Limitations (after the conclusion)
62,"Our work comes with some limitations. It is uncertain whether our results in two specific settings, multiple choice and extractive QA, would extend to more general settings for NLI, although the use of contradictions for factual consistency by Laban et al. (2022) suggests that they could. Additionally, 3-class NLI is not sufficient to capture all the natural language relations that might be needed to verify an answer. As such more challenging datasets in other settings and more granular NLI settings should be attempted.
Another limitation involves answer ranking and the associated computational cost. The main reason we did not test answer ranking in extractive QA is that we did not generate diverse outputs, but another reason is that such a procedure grows prohibitively expensive as the domain becomes more open. In a fully open domain, ranking would require a quadratic evaluation for each context passage against each reformulated answer candidate (Schuster et al., 2022). Future work should look at comparison approaches that amortize this cost, such as NLI-based dense passage retrieval (Reimers and Gurevych, 2019)."
63,"The last section of the paper.
7 A2. Did you discuss any potential risks of your work? Left blank."
64,in separate limitations section at end
65,"Limitations section at the end of the paper
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
66,"Some of the summarization datasets annotated for faithfulness are relatively small, which makes score estimates uncertain. Furthermore, many datasets contain only output from a limited number of generation systems, which makes it hard to properly account for potential biases towards certain generation systems that may confound scores (see Pagnoni et al. (2021)). These concerns are, however, alleviated to some extent since we study trends across many independently created datasets, which makes it less likely for a single bias to persist in all of them. Furthermore the availability of generation and thus annotated faithfulness data limits our experiments to English. Finally, it remains
unclear whether our results would still provide advantages when applied to larger models such as T5-11B, whose parameter count makes experimentation infeasible on the hardware available to us."
67,"At the time of writing this work, ChatGPT is only available as a proprietary free research preview via a web interface. This is limiting in several ways. (1) Parts of our analysis are qualitative, as quantification is challenging due to limited accessability of the investigated model. (2) Some details about the investigated model are not yet disclosed. This is true for the model design as well as for the data used to train ChatGPT. MultiWOZ is a freely available and widely used dataset, therefore no guarantee can be given that ChatGPT has not been exposed to at least some meta details regarding this dataset. (3) Given the nature of the free research preview, exact reproducibility is not guaranteed, as the model may change any time. However, it is expected that any future version of ChatGPT retains its general abilities and behaviors.
Model-as-a-service. Building a general purpose model such as ChatGPT is extremely costly and an option only for few. However, once it exists, it may be utilized for a multitude of purposes. As a model, ChatGPT does not need to be built for DST in order to be useful for DST. With capable enough general purpose models, fine-tuning towards specific tasks may be avoided. Fine-tuning is challenging for multiple reasons such as the need for adequate data, computational costs, risk of over-fitting and catastrophic forgetting, among others.
Just like its sibling model, ChatGPT will become available as model-as-a-service. The advantage of this is that a massive LM such as this is usable independent of the user’s hardware. But this advantage comes with the disadvantage that it will in all probability remain proprietary. In consequence, it will likely not be possible to ever run, adapt, train or modify ChatGPT on local machines.
ChatGPT as model-as-a-service is likely to remain a black box to customers and researchers, even if just in parts. The model may change any time. In fact, a model update during our experimental evaluation prompted us to re-process a few of our test dialogues. This property impedes backward compatibility and the ability to trust in familiar behavior.
A general purpose model may show too general behavior and converse about more than what is required or requested. This also poses vulnerabilities for adversarial attacks. To this end, models such as ChatGPT have been trained with human feedback to better handle malicious intent and abusive
behaviors. A model-as-a-service is a gated resource. As such, its indefinite availability cannot be guaranteed. Further, recurring costs for access may be too high for certain downstream tasks. As a hosted service, latency might become a bottleneck or hindrance for its use as a component in complex applications."
68,"Limits of Prompt-based Generation. This work specifically proposes improvements to the controllable generation portion of mixed-initiative dialogue systems. However, dialogue policy planning is still an important problem to consider. In order to evaluate generation improvements, we hold dialogue policies fixed — in the static evaluation, we condition on ground truth dialogue intents, and in the interactive evaluation, we follow the same dialogue intents prescribed by the RAP system. To this end, a mixed-initiative dialogue system cannot consist solely of a generation module powered by prompting. There needs to be a set of rules or models that govern how a system can regain control of a conversation; the generation module is just a means of enacting these rules. As discussed in Section 7, prompting is a great option if there is already a pre-existing policy planner.
Due to these limitations, we did not conduct an interactive evaluation in the ESC setting. Emotional support conversations are highly personal, as circumstances vary across individuals. It would have required having study participants pretend to require support regarding a fixed scenario, or for
participants to disclose their personal issues, which can raise other ethical concerns. Moreover, dialogue policy planning is not straightforward for emotional support, due to this highly variable nature. Effective support strategy planning requires expert knowledge.
In Section 7, we also discussed that prompting may be appropriate for developing systems for novel tasks in low-resource settings. However, deploying prompt-based systems may be less useful for the purpose of setting new benchmarks on existing leaderboards with a plethora of data. Such setting already have plenty of well-annotated conversations and simple fine-tuned models can often achieve strong performance.
Guardrails. Proper guardrails should be put inplace prior to productionization of any dialogue system, prompt-driven or not. While we witness strong overall response quality both in terms of human evaluation and automatic metrics, language models can generate contradictions. System builders may consider employing guardrails for dialogue consistency (e.g. Jin et al. (2022)) and coherence (e.g. Ye et al. (2021)), among others.
As with any training set, InstructGPT and other LLMs have been trained on finite amounts of data. InstructGPT has not been trained on data after 2021. This is also true of training corpora such as P4G or ESC; these corpora were published in 2019 and 2021, respectively. Particularly in any sensitive environments, guardrails should be put in-place for factual correctness (e.g. Santhanam et al. (2021); Wang et al. (2020)). RAP attempted to remedy this by incorporating retrieval for factual questions, which we also embedded into our prompting approach, but this knowledge base is also finite. In Section C we discuss one such example (Table A5). A possible solution is internet retrieval (Komeili et al., 2022), but search engines can also yield misinformation, which leads to hallucination.
Computational Cost of Language Models. LLMs are computationally expensive, and in the case of models such as InstructGPT, they are not open source. However, in this study, we did not have access to equally powerful open-source models such as OPT 175B, nor the appropriate hardware to load such a model (loading OPT 175B requires 350 GB of GPU memory). We performed initial experiments with much smaller models which fit our hardware constraints such as GPT-
J 6B, but there was much higher variance in performance. This is supported by the fact that many reasoning capabilities do not seem possible with models smaller than 175B parameters (Wei et al., 2022b,a). Given our limited budget for human evaluation, we opted to use the best performing LLM we had access to, InstructGPT.
Prompt Optimality It is possible that we do not use an “optimal” set of prompts as we did not mine prompts or perform soft prompting. However, prompt optimality itself is a problem in dialogue generation, because open-ended dialogue evaluation is a difficult task. Most automatic evaluation metrics do not align well with human ratings in dialogue (Yeh et al., 2021; Liu et al., 2016). This makes it suboptimal to use as a discriminator in soft prompting, for instance. Most existing work that does search for optimal prompts or tunes prompts works with tasks that have clearly defined automatic evaluation, such as sentiment analysis or table-to-text generation (van de Kar et al., 2022; Li and Liang, 2021; Lester et al., 2021). Moreover, human ratings are expensive and not scalable for systematic optimization."
69,"First, we only access limited computation resources and perform continual pre-training from BERT (Devlin et al., 2018), which is not general enough for every event-related reasoning task. Second, counterfactual reasoning makes our approach conservative in identifying causal relationships, so our
method has a higher precision. However, some potential causal relationships will be discarded. How to achieve a good trade-off between precision and coverage is a problem. In addition, the way we utilize knowledge is relatively simple, and it is very likely that we have not made full use of knowledge. Designing more complex knowledgeenhanced methods may lead to better results."
70,"After Conclusion
A2. Did you discuss any potential risks of your work? Not applicable. Left blank.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 3.1"
71,"Currently, our approach does not effectively leverage syntax tree information via GCNs, a commonly used method for incorporating syntax trees in this task. Further research is required to determine the most effective way to integrate syntax tree information into TOWE models."
72,Second to last section.
73,"While the dataset by Kumar et al. (2021) enabled us to test models for a range of often overlooked groups (e.g., non-binary or bisexual annotators), we ultimately modelled only four specific attributes (gender, age, education, sexual orientation). There are likely to be more factors that could play a role. Additionally, annotators in the Kumar et al. (2021) dataset are exclusively from the United States of America, so that results do not necessarily hold for other countries or cultures (Hovy and Yang, 2021). Specifically perceptions of harmful content online are known to vary across countries (Jiang et al., 2021).
We used only the (Kumar et al., 2021) dataset. This is mainly due to our strict criteria regarding dataset size and availability of annotator-level labels and sociodemographic information. These characteristics were a prerequisite for our experiments across different attributes with sufficient numbers of annotators. Most datasets which include annotator-level labels and sociodemographic information contain much smaller numbers of annotators and attributes. Nevertheless, with the Measuring Hate Speech Corpus there is at least one additional dataset (Sachdeva et al., 2022) with comparable characteristics that could be used in future experiments. Also, additional small-scale, more focused experiments could use datasets like Sap et al. (2022) or HS-Brexit (Akhtar et al., 2021) which was annotated by 6 annotators, each from one of two sociodemographic groups.
We do not study the aggregation of individual predictions or evaluate against majority labels, as these are not directly relevant to our investigation of sociodemographic attributes in models of annotation behaviour. Consequently, we cannot derive a conclusion about performance in those settings from our results. This is a noteworthy limitation, because part of the experiments introducing
multi-annotator models in Davani et al. (2022) compare labels aggregated from multi-annotator models against predictions from a standard classifier (directly trained on aggregated labels).
For computational reasons, our experiments use a comparatively small pre-trained language model (RoBERTa, Liu et al. 2019). Thus, results might differ with larger models."
74,"Limitations, 8"
75,"While we used multiple parsers to avoid biasing the evaluation towards one parser, all parsers used are relatively high-performing parsers—all have la-
belled F₁ scores above 0.8 on the CCGbank development and test sets. This evaluation is thus biased towards especially difficult sentences, since those will be the ones where good parsers produce errors. While we found no correlation between parser scores and judge disagreement, at least suggesting that the judgements were not a function of parse quality, poorer parsers (or good parsers on novel domains) may make different kinds of errors than those that appeared in our sample. It is unclear how F₁ and DF₁ would compare under such circumstances; understanding this better remains an open area of research.
The relatively high disagreement among judges in the second task (24% of sentence pairs) is concerning, but it should be noted that the sentence pairs were sampled from a set of disagreements between two different scoring methods. The extent to which this is a problem in practice is unclear, as judge agreement may not be as low on outputs from different parsers evaluated by the same scoring method—but it could also be lower.
Although the dependency-based evaluations discussed in this paper are standard for CCGbankbased statistical ccg parser evaluations, the reliance on extra resources (namely, the generate program and markup files from C&C) makes it somewhat unique. Because of this, the extent to which decomposed scoring, or the ideas behind it, would be useful for other evaluations scenarios (such as for other corpora, including ccg corpora from other languages) is unclear."
76,"Measurement of translation literalness is neither well studied nor well understood. We rely on a combined interpretation of multiple measurements to investigate our hypothesis and its implications. This limits the extent to which we can make strong claims, since in the absence of a highly correlated metric for translation literalness, it is hard to compare systems. We could only claim that our investigation indicates the presence of a tendency towards non-literalness in GPT translations, but a stronger result would have been preferred to further disambiguate the translation characteristics. Further, we only compare GPT translations in the standard zero-shot and few-shot settings and it is quite conceivable that more specific & verbose instructions could steer the LLMs to produce translations with different characteristics."
77,Section 5
78,"Experiments. We acknowledge the limited scope of our experiments, including only 8 (closeddomain) documents, 3 models and a single language. This is largely due to the limited availability of suitable large LMs and their high computational cost. Still, we believe that our experiments are valuable as a case study that already clearly showcases some interesting features of our methodology.
Computational cost. While we have demonstrated an efficient strategy to obtain predictions for all tokens at all possible context lengths, it still requires running the model N times for a document of length N .
For a k-fold reduction in computational cost, the technique may be modified to use a sliding window with stride k > 1 (instead of k = 1 as proposed above). See Appendix A.1 for details.
Choice of metrics. The proposed methodology allows investigating how any given metric is impacted by context, yet our study is limited to NLL loss and the proposed KL divergence metric (the latter for defining importance scores). These may not be optimal for every purpose, and other choices should be explored depending on the application. For example, to study sequences generated (sampled) from a LM, one might want to define importance scores using a metric that does depend on the generated token, e.g. its NLL loss or its ranking among all candidates. (Indeed, our web demo also supports ∆-scores defined using NLL loss values.)"
79,"Section 5
7 A2. Did you discuss any potential risks of your work? We did not identify any risks. The usual risks related to large language models are arguably not present here since we are not proposing or training a new LM, but a way to explain it."
80,Limitation Section.
81,"Yes. Section 6
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
82,"We list the limitations of our work as follows: • The Query Augmentation is designed for the En-
glish alphabet; therefore, other languages with different alphabets will require further work. • Since the training strategy relies on fine-tuning a pre-trained language model using a large passage retrieval dataset, it may not be suitable for languages with limited resources"
83,"Section 6
7 A2. Did you discuss any potential risks of your work? There is no potential risk associated with increasing the robustness of information retrieval applications to question containing misspellings."
84,"Although FPT shows better control ability, there are two points that need to be improved in the future. First, as in Gu et al. (2022), we need to select hyperparameter α to balance between the control ability and fluency in generated texts. Second, as shown in Table 5, although the time cost of FPT is lower than that of GeDi, it is higher than those of other prefix tuning-based methods and grows approximately linearly by a factor of 2 × number of attributes."
85,"Section 7
7 A2. Did you discuss any potential risks of your work? Our work is a foundational research and does not contain potential risks. Our experiments are fair."
86,"After Conclusion.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
87,"Our study illuminates the deficiencies of the MRF approach and applies statistically-motivated approaches to craft more performant probabilistic models. However, it is admittedly not clear how these insights can immediately be applied to improve downstream NLP tasks. We focused on models over pairwise tokens in order to avoid sampling
and work with exact distributions for the various approaches (MRF, HCB, AG). However this limits the generality of our approach (e.g., we cannot score full sentences). We nonetheless believe that our empirical study is interesting on its own and suggests new paths for developing efficient and faithful MLMs.
Ethics Statement
We foresee no ethical concerns with this work."
88,"Our model only focuses on utilizing text information for recommendation, which is a key limitation of this work. In real-world settings, recommender systems are usually required to handle heterogeneous information inputs. UniTRec is a pure textbased recommender modeling user history and candidate texts as inputs. However, incorporating additional side information (e.g., user profile, text topic labels, and dwell time of user behaviors) could further improve the recommendation performance and alleviate the cold start problem. Furthermore, UniTRec only models two-level relations of user behavior history. Nonetheless, incorporating more user behavior information, such as implicit and negative feedback, could further enhance the recommendation performance."
89,"In this study, we focused on the clustering task in order to assess the real impact of anisotropy on the quality of representations. The conclusion is clear regarding Euclidean and directional clustering but investigating other tasks like information retrieval and anomaly detection would further strengthen the present findings. Also, the set of post-processing methods is not limited to the ones used in this study, and it would be interesting to conduct a more comprehensive study, including more transformation functions. Finally, an important future direction is to assess the impact of anisotropy on other languages, especially on embedding models trained on a restrained corpus, which can be the case of low-resource languages."
90,"Our paper has the following limitations
1. Our class-based influence score cannot improve the performance of GC algorithm. Although class-based version of GD, IF, and TracIn outperformed the original GC, we aim to develop a stronger version of GC. From the analysis in Sec. 4, we believe that a partially normalized GC could have better performance. In partial GC, we normalize the gradient of the clean data point z′(j) only. That will remove the noise introduced by ∥∇θ̂ℓ(z′(j))∥ while retaining the valuable information about the norm of ∇θ̂ℓ(z(i))."
91,The Limitation section after conclusion and before the references
92,"section limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
93,"In this paper, we only focus on investigating and improving gender fairness of pre-trained language models and didn’t touch other fairness issues given the length of the paper. However, we would like to note that with the investigation of other fairness issues in human language more deeply conducted, if the biased words regarding other fairness issues can be more specifically concluded, GEEP can be directly applied to address other fairness problems in pre-trained large language models.
7"
94,"We include a ""Limitations"" section in the paper."
95,Section 8
96,"While our analysis suggests that IT models do not fully utilize instructions but instead learn superficial patterns from instructions, there are some limitations to our experiments. First, we only analyze a SOTA IT method on the NatInst-V2 dataset and T0 dataset. Though Wang et al. (2022) showed that their model can outperform other large models such as Instruct-GPT (Ouyang et al., 2022) and T0 (Sanh et al., 2021), we did not analyze other IT methods, such as RLHF (Reinforcement Learning from Human Feedback) in Instruct-GPT. Secondly, since our analysis is conducted in the training stage, we cannot analyze private models such as Chat-GPT. Also, we did not explore models larger than 7B parameters due to our computation resource limitation. This may miss some emergent abilities of large language models (LLMs) (Wei et al., 2022). Lastly, while we observe the models do not utilize the majority of the instructions by IT, a certain degree of instruction understanding may already exist in pre-trained LLMs, which we did not study in this work. In conclusion, our work is a concentrated analysis to illuminate the potential vulnerability of the current IT models and evaluation metrics. We encourage future works to conduct more comprehensive studies on larger models and propose more reliable IT methods and evaluation frameworks."
97,"Dataset and Experimental Limitations. The datasets and tasks we focus on are from the XGLUE benchmark (Liang et al., 2020). The structured prediction tasks, namely Named Entity Recognition (NER) and Part of Speech (PoS) Tagging, both have a limited number of training samples at 15k and 25.4k samples respectively. This is due to the difficulty in annotating on the token level, however it can still be viewed as a limitation when compared to the remaining sentence-level tasks the majority of tasks have at least 100k samples.
Methodological Limitations. Below are a list of the main methodological limitations we perceive of our work:
• Our method requires a teacher model that is already trained on the downstream task which can then be used to perform knowledge distillation. This is limiting when there are constraints on the computing resources required to produce the quantized model.
• We have focused on the problem of reducing accumulative qunatization errors which become more apparent the deeper a network is. However, this problem is intuitvely lessened when the model is shallow (e.g 3-4 layers) but perhaps wider. Hence the results may be less significant if the model is shallower than what we have experimented in this work.
• By introducing the distillation loss we require an additional regualrization term β to be optimally set, relative to the main distillation loss α. This can be viewed as a potential limitation has it introduced an additional hyperparameter to be searched to obtain best results on a given task.
• Lastly, since intermediate layer outputs of the teacher network are required for self-attention distillation, we have to perform two forward passes during training. Since standard KLD distillation only requires the output logits, it is common to store the training data teacher logits, eliminating the need to perform two forward passes at training data. However, this is not an option with self-atttention outputs as the storage required offline scales with the number of self-attention heads, number of layers and the size of the training data."
98,"Section 7
7 A2. Did you discuss any potential risks of your work? We study open-domain information extraction for researches in this area"
99,"Page 5
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
100,See the limitations section on page 5.
101,"The PhotoBook dataset has a very limited number of images (i.e., 360) and image combinations (i.e.,
14Cross-attention mechanism explained in Appendix B. 15Likely due to limited dataset size and configuration. More
analysis and exploration can be found in Appendix E.
5 per game theme), which may lead to undesirable overfitting behavior as we discuss in Appendix E. Also, since our model depends heavily on CLIP (Radford et al., 2021), it is likely to inherit CLIP’s biases and weaknesses. For example, Radford et al. (2021) mentioned that CLIP fails to perform well on abstract or more complex tasks, such as counting or understanding spatial relationships between objects. Finally, whether our listener model can be easily applied/adapted to productive real-world tasks (e.g., automated customer service with image inputs) requires further exploration."
102,"First, neither Feldermodell (Reis, 1980; Wöllstein, 2018; Höhle, 2019) nor Doppelbaum (Wöllstein, 2018) has obtained complete concurrence among linguists. Also, we limit our scope to the English– German language pair and the IT domain using the WMT 2019 training, validation, and test data sets. A broader scope would not provide confidence in the validity of conducted experiments because there are hardly any standard setups for experimental research (Chatterjee et al., 2018, 2019; Akhbardeh et al., 2021).
In addition, the conducted experiment should take into consideration the effect of randomness that is attended in the process of training artificial neural networks; different techniques, different hyperparameters, and multiple runs of optimizers (Clark et al., 2011) may present different results. However, as previous studies (Chatterjee et al., 2018, 2019, 2020; Akhbardeh et al., 2021), including the study on the baseline model (Shin et al., 2021), do not consider the effect of randomness, we also do not investigate the effect of randomness further, considering that training multiple models (Appendix A) to obtain good estimators (TER and BLEU) will cost a lot."
103,"Section 8.
7 A2. Did you discuss any potential risks of your work? With the standard setup, studies in the field of automatic postediting, including this work, do not involve potential risks."
104,"Section ""Limitations"" (5th section)"
105,Limitations
106,"All our experiments are done on the sequence labeling task, and they can be further evaluated on sentence classification tasks with classifier-based fine-tuning since the [CLS] token used for classification represents the whole sentence. We provide a causal opinion on demonstration-based learning and a simple but not systematic method to alleviate the induced bias. Our demonstration-based learning builds upon previous works (Lee et al., 2022; Zhang et al., 2022a), where BERT or RoBERTa are used instead of Large Language Models, such as InstructGPT (Ouyang et al., 2022), PaLM (Chowdhery et al., 2022), and OPT (Zhang et al., 2022b). Furthermore, our conclusions are drawn from fewshot learning settings and cannot be directly applied to zero-shot inference."
107,Section 6
108,"• We currently rely on gold annotations for attribute marking, which are not always available depending on the dataset. However, RAMP could be easily extended to unsupervised settings through LLM feature attribution (Sarti et al., 2023), i.e., extracting salient tokens driving the attribute prediction. This approach builds upon recent techniques in unsupervised language generation metrics (Fomicheva et al., 2021, 2022; Leiter et al., 2022). We leave an empirical evaluation of its effectiveness to future work.
• Besides the choice of in-context examples, prompting is also sensitive to their ordering (Lu et al., 2022) and the design of the template (Jiang et al., 2020). We refrain from tuning example orders and templates to avoid introducing too many variables.
• Multilingual LLMs perform competitive MT out of the box for languages seen during their pre-training. However, we noticed that BLOOM 175B produces better EN-IT translations than XGLM 7.5B even though IT is not listed as a training language of BLOOM. This could possibly be due to typological similarity between Italian and the Romance languages included in BLOOM training. We leave experiments of unseen languages as future work.
• Multilingual LLMs like the ones used in this paper require larger GPU resources for inference than standard bilingual MT systems.
• One test set we use (MT-GENEVAL) provides only two gender values (female and male), but we do not intend to imply that other genders do not exist."
109,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
110,Limitations
111,"This work has several key limitations. First, we have relied on evaluation against self-reported (questionnaires) assessment of anxiety. Selfreporting the degree of anxiety on a survey instrument is not entirely dependable in diagnostic accuracy. However, it has shown reliable associations with diagnoses, serving clinical assessment treatment purposes beyond diagnosis (Kroenke et al., 2001). For example, anxiety scores from selfreported surveys have been robustly associated with consequential real-world outcomes such as mortality (Kikkenborg Berg et al., 2014). Clinical evaluation of the assessments proposed in this work should be evaluated against clinical outcomes.
Furthermore, the sample may not fully reflect the language use of the general population as it is skewed towards young and female4 and only focused on English spoken by those from the U.S. and U.K., although previous work suggests this dataset contains a diverse representation of socioeconomic status (Matz et al., 2019). Additionally, we do not focus on actual utilization of discourse relations in assessing anxiety, as the scope of this work limits us to showing the viability of modeling anxiety on a continuous scale and the importance of discourse information towards modeling it. Lastly, the strong associations of theoretical discourse relations come from models that themselves are not perfect, with F1 scores ranging from 0.770 for counterfactuals to 0.868 for causal explanations, though one might expect this error to lead to underestimates of correlation with anxiety.
With NLP increasingly working towards better human-focused applications (e.g., improving mental health assessment), we are presented with increasing considerations for human privacy as a trade-off with considerations for open data sharing. In this case, the data used was shared with consent only for academic research use. Open sharing of such data violates trust with research participants (and agreements with ethical review boards). These and additional issues are discussed at length in Benton et al. (2017a). While it would be ideal to
4The self-reported user age averaged 22.6 (SD 8.2), and over half (58.1%) marked their gender as female.
release everything and preserve privacy, in this situation, we believe the fact that the unprecedented data is not universally available suggests an imperative for those with access to openly share our work as best possible within ethical guidelines. We are thus releasing aggregated anonymized features from the secondary evaluation dataset that allows one to qualitatively replicate the associations in our results while preserving the privacy of participants."
112,"We briefly mention some limitations of our work. First, we have only used a single dataset, and a single model family in our experiments. This is mainly due to the fact that the benchmark we use is the only publicly available dataset at this time to the best of our knowledge. We also solely focused on extraction metrics, but did not do a deeper analysis on the extracted sequences. A fine-grained analysis of extracted sequences could yield important insights for understanding memorization and extraction in LLMs. Similarly, we also did not analyze what our prompts converge to, and whether they yield explainable prompts at the time of converge. Such analysis can provide better insights as to why, for example, training prompts with aligned CLM performs better that the basic CLM setting. Finally, we believe the evaluation of our defense could be improved further by measuring other utility metrics (e.g., accuracy) on downstream tasks."
113,See Section 6
114,"Section Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
115,"MOSPC can improve ranking accuracy on each fine-grained MOS score segment, but at the same time, the training method based on pair comparison may impair the generalization performance. As there are unseen-systems in the BVCC validation set, the system-level results of BVCC are affected by the generalization performance degradation. We introduced the C-Mixup algorithm to enhance the generalization performance, which increased the complexity of the experiment to some extent."
116,"Our proposed system was tested on two opendomain TableQA datasets, with one of them (E2EWTQ) being relatively small compared to the other. Also, the current open-domain TableQA datasets are limited to look-up questions. They do not cover more complicated questions that involve multiple cells and complex table operations, such as SUM/MAX/MIN/SUBTRACT in some questions of WikiSQL and WikiTableQuestion. Therefore, the effectiveness of our system should be further evaluated on more complicated datasets of larger scale in the future. Another limitation lies in the token length limit of modern Transformer models. The best-achieving models typically accept up to 1024 tokens (e.g. BART, the base model of TaPEx). This limitation becomes more obvious when tables grow longer and the information being sought go beyond the limit. We believe that, with better approaches addressing this limitation, our system can achieve better performance. The solution can be either applying sampling strategies to pick the rows and columns that are most relevant to answering the question, or increasing the capacity of future Transformer models."
117,"Section 7
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
118,"While the vector representations obtained using XL-LEXEME for different languages are potentially comparable, lying on the same geometric space, the evaluation of cross-lingual semantic changes cannot be performed for lacking crosslingual LSC Detection resources. SemEval 2020 Task 1 datasets consist of small sets of target words, i.e., the number of target words for English, German, Latin, and Swedish is 37, 48, 40, and 31, respectively. The example of the Latin language highlights that XL-LEXEME can perform poorly on languages that are underrepresented in the training set of XLM-R and not covered by the WiC dataset. Generally, at the moment is not possible to state precisely how and how much XL-LEXEME
performance is affected by the language distribution in the XLM-R training set and the WiC dataset."
119,"The key limitation of our work is that, when conducting the review of approximately 60 papers (by searching through the ACL Anthology for works in computational social science since 2010), we encountered a skewed distribution of descriptive versus integrative works. In fact, it was relatively simple to find descriptive works, and that section of Table 1 could have been much longer. We also recognize that, due to the mixed nature of our field, scientific and integrative findings are not the only goal—our ‘big tent’ includes engineers as well, who value gains in performance indicators. Finally, the fact that we have few examples of papers showing a return to theory renders the possibility that our central claim is misinterpreted in a normative way as a mandate."
120,
121,"Our proposed ensemble approach for training the Transformer architecture has demonstrated promising results for the task of AMR ensembling. However, there are limitations that warrant further investigation in future research.
Our first limitation is the lack of generalization, as the approach was only evaluated on AMR parsing. Therefore, the application of an autoregressive ensembling model has not yet been tested on other Natural Language Processing tasks.
Moreover, in order to properly compare each ensemble system under the same conditions, we base all our experiments using the same underlying architecture, i.e. SPRING. There needs to be an exploration of these approaches using more recent, better performing parsers. However, this will require access to such systems.
Furthermore, the computational cost is also a limitation, as even though our proposed merger method, Assemble!, is more efficient than previous ensemblers, it is still computationally expensive, and particularly when we have to ensemble long graphs from multiple predictions. Moreover, as our Assemble! model is based on LongT5, it might be challenged when working with large datasets or when running experiments on resource-constrained systems. Therefore, we encourage the use of ensembling strategies focused on selecting the best graphs instead of merging.
Lastly, as our ensemble approach is based on Transformer, results can be difficult to interpret, as it can be challenging to understand how the generated graph has been ensembled by different predictions, leading to a lack of interpretability.
In summary, the proposed ensemble approach for training the Transformer architecture has shown promising results for the task of AMR ensembling and has the potential to be applied to other tasks, however, further research is necessary to address its limitations and improve performance."
122,Section 5
123,"The section called ""Limitations""."
124,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
125,"Though our method produces full sonnets that are more impressive than all previous approaches, it
is still not at the level of human-generated poetry. It is not clear how to achieve this level, whether it would be using massive large language models, or through our general approach, which is to bend those models around an interpretable framework that knows the rules that sonnets obey. Certainly our approach requires a lot less data – even if one used all the sonnets that have ever been written to train a language model, it is unclear that the language model would learn the very specific rules required of sonnets. However, there may be other ways to obtain these constraints that have not yet been developed."
126,The Limitations section
127,Section A.1 of the Appendix
128,Limitations
129,Section Limitation
130,Limitations
131,"The results we present must be viewed in the context of a few limitations. A limitation is that we only perform experiments in English and on one task at a time. To be more comparable to a LLM few-shot settings, other languages and a multi-task setup could be explored. Furthermore, in order to replicate the results access to none public models is required and inference must be performed on large amounts of data. Another limitation of our work is that it only explores the original CoT prompting approach, but we do not explore subsequent improvements, such a self-consistency (Wang et al., 2022)."
132,Limitations Section
133,section Limitations
134,"We are releasing ScoNe as a diagnostic tool for conducting controlled scientific experiments. This is our primary intended use, and we advise against uncritical use of ScoNe for real-world applications, as we have not audited the dataset for such purposes.
As a diagnostic tool, ScoNe’s primary limitation is its focus on English. Cross-linguistically, we find many strategies for expressing negation. The English-language strategy of using mostly adverbial modifiers for sentential negation is not the only one by any means, and we would expect to see quite different results for languages in which negation is expressed, for example, with verbal suffixes. This highlights the value of potential future efforts extending ScoNe to other languages.
By the same token, we acknowledge that many linguistic phenomena interact with negation even internal to English. ScoNe restricts to negation in the context of lexical entailment, and mostly uses “not” as the negative morpheme. This excludes a wide range of negation morphemes and negation strategies that ultimately need to be brought into the picture.
Finally, we note that there may be undesirable biases in ScoNe that could interact with biases in the models. ScoNe is in part derived from SNLI, which is known to contain gaps, social biases, and artifacts (Poliak et al., 2018; McCoy et al., 2019; Belinkov et al., 2019; Gururangan et al., 2018; Tsuchiya, 2018), and ScoNe may inherit some of these."
135,"Yes, primarily in the Limitations section."
136,"All prompting methods are trying to extract knowledge from the Large Language Models (LLMs).
Our paper compares their knowledge extraction abilities. Thus, the performance of RoBERTa-large can serve as a reference point and provide insights for other LLMs. However, it is still necessary to assess each large language model independently to understand its capabilities comprehensively.
We only tested a handful of simple manual prompt-and-verbaliser pairs which are included in Tables 3 and 4. It is entirely possible that there is a lot of room for improvement in the design of manual prompt-and verbaliser pairs, thus providing us a even stronger baseline. We have opted to use ten trigger tokens in Auto, in alignment with the experiment settings originally presented in the AutoPrompt paper (Shin et al., 2020). However, since the verbaliser domains generated under few-shot learning settings are noisy, reducing the number of trigger tokens may improve performance."
137,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
138,"We evaluated the most widely used distillation objectives including prediction layer transfer, hidden states transfer and attention transfer. However, some objectives are not included in our evaluation due to missing implementation details in their paper. For example, we only implemented the contrastive intermediate layer distillation objective proposed by Sun et al. (2020a) in task-specific setting, since code and implementation details are missing for task-agnostic setting. New objectives are increasingly appearing for model compression in the field of computer vision, such as Wasserstein contrastive representation distillation (Chen et al., 2021) and distillation with Pearson correlation (Huang et al., 2022), which can be included to have a broader scope of distillation objectives evaluation.
This work empirically studied the impact of the teacher layer choice for initialization and training objectives, however, further analysis is needed to understand why lower teacher layers are essential for initialisation, and why attention transfer behaves consistently well under various teacher layer choices in the task-specific setting, while hidden state transfer does not."
139,"A potential limitation of our experiments is the use of oracle validation labels instead of human manual annotation as in the real-world setting. However, all validation sets we used in our experiments were collected based on the manually defined seed set of entities and relations, carefully cleaned and augmented with manually labeled negative samples. Moreover, we chose this more easy-to-implement setting to make our results easily reproducible and comparable with future work.
Another limitation of experiments that use established data sets and focus on isolated aspects of knowledge-graph construction is their detachment from the real-world scenarios. Indeed, in reality knowledge graph completion is done in a much more complicated environment, that involves a variety of stakeholders and aspects, such as data verification, requirements consideration, user management and so on. Nevertheless, we do believe that our method, even if studied initially in isolation, can be useful as one component in real world knowledge graph construction."
140,"In this section, we point out several limitations in this work.
First, our in-domain evaluation experiments focus on passage retrieval for ODQA. While the dense retriever is mostly successful in ODQA, it can be also used in other types of retrieval tasks which may have different input and output format. For example, the KILT benchmark (Petroni et al., 2021) provides several knowledge-intensive tasks other than ODQA. The performance of TASER models trained on such retrieval tasks remain unknown.
Second, compared with traditional sparse vector models like TF-IDF and BM25, the cost of training is an inherent issue of dense retrievers. Although TASER significantly reduce the number of model parameters, the training cost is still high.
Third, in our experiments, we show that the learned routing does not outperform the deterministic routing. This may suggest a better architecture and/or training algorithms for learned routing is needed to fully unleash the power of MoE.
Last, as observed in §4.2, there is still a gap between TASER and BM25 in out-of-domain evaluation. Therefore, how to close this gap will remain
a critical topic for future work on dense retrievers."
141,"Section 7
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
142,"While our work shows promising results in improving the generalization capabilities of Transformers to sequences of arbitrary length, some limitations must be considered. First, our evaluation is confined to synthetic algorithmic reasoning tasks, which may not fully capture the complexity and diversity of natural language. We focused on synthetic datasets since they showed clear and somewhat surprising limitations of Transformer architec-
tures (Delétang et al., 2023). However, the generalizability of our approach to other tasks and domains remains an open question, and additional research, such as evaluation on SCAN (Lake and Baroni, 2018), CFQ (Keysers et al., 2020), COGS (Kim and Linzen, 2020), or the Long Range Arena (Tay et al., 2021), is necessary to understand its potential in real-world applications. Second, our approach introduces a new hyperparameter – the maximum sequence position L. Although our experiments in Appendix B.1 show that our method’s performance is largely unaffected by the precise value of L, practitioners may still have to tune the parameter depending on their specific problem domains. Third, we only isolate and ameliorate one failure mode of Transformer length generalization on synthetic datasets. However, there are other factors contributing to poor length generalization, such as attention becoming less peaked for longer sequences (Chiang and Cholak, 2022). Overall, we believe that our study’s limitations offer several interesting directions for future research."
143,"Regarding the Wikipedia articles used for creating our dataset Wikipedia Table and Image Generation (WikiTIG), some infoboxes may not follow the defined format and rules. This is because various users can freely edit infoboxes. Moreover, the HTML dump data published by English Wikipedia is not based on recent information.
In image generation, due to the standard settings recommended by Zhang et al. (2021); Ramesh et al. (2021); Wang et al. (2022); Wu et al. (2022a), our image generation task requires generating a cropped fixed-size square image instead of the original aspect ratio.
In addition, a table in an infobox may contain cells unrelated to image generation, and thus it may be redundant for image generation."
144,"First, unlike decoders in neural seq2seq models, which can attend to any previously generated tokens, QCFGs have a strong context-free independence assumption during generation. With this assumption, Neural QCFG cannot model some complex distributions. A potential solution is to use stronger grammars, such as RNNG (Dyer et al., 2016) and Transformer Grammars (TG; Sartran et al., 2022).
Second, we assume that both the grammars used by the source-side parser and QCFG are in CNF. Although it is convenient for discussion and implementation, CNF does not suit for modeling the structure of practical sequences. In semantic representations (e.g., Abstract Meaning Representation (Banarescu et al., 2013)), a predicate could have more than two arguments. Ideally, we should represent n-ary predicates with n-ary rules. However, for grammars in CNF, n− 1 unnatural binary rules are required to represent n-ary predicates. In natural language, we will face semantically meaningless spans due to CNF, which is discussed in Sec 4.2.
Third, although using decomposition improves the speed and the memory requirement, our lowrank models still cost much more computation resources than neural seq2seq models for two main reasons. (1) A large amount of nonterminal symbols increase the memory cost significantly. (2) Because finding the most probable string t2 from pθ(t2|t1) is NP-hard (Sima’an, 1996; Lyngsø and Pedersen, 2002), we follow Kim (2021) to use a decoding strategy with heavy sampling. For real data, we may need to sample hundreds or thousands of sequences and then rank them, which can be much slower than the decoding of neural seq2seq models."
145,"In this work, we focus on creating the EnglishFrench tense corpus. These two languages are among the most frequently and widely used languages in the world. In addition, they have several similarities in tenses, which are pretty helpful for research on tense consistency through machine translation. Thanks to the distinctive tense struc-
tures, the study of these two languages makes it possible to examine many common tense issues, but there are also some tense issues in other languages that are not covered by this study. For example, the implicit tense expressions in Chinese are difficult to correspond to the explicit tense expressions in English (Jun, 2020). Hence, our next step will be to extend the tense test set to other language families and even cross-language families to further study tense consistency. Also, as for future work, we will optimize both the tense annotation method and the tense prediction accuracy calculation. Besides, we did not propose a new method to improve the tense prediction accuracy. To be further, we will endeavour to improve the existing machine translation systems according to tense consistency."
146,"limitations the main limitation of the proposed approach is that it would be relatively costly to apply at production time, compared to the conventional lm evaluation. first, it requires drawing a number of tokenization samples, as defined by importance sampling, in contrast to a single pass through the evaluated sequence in the conventional approach. second, the conventional approach can be conducted with teacher forcing and efficiently parallelized, while the proposed approach relies on block-byblock sequential processing. nonetheless, the proposed algorithm is designed for analysis purposes rather than to be used in production systems, for which it is feasible to run it in a reasonable time, allowing users to evaluate the effect of marginalization for any tokenizer and language. broader impact as the work is dedicated to evaluating existing models on publicly available datasets, we are not aware of any potential negative impact."
147,"limitations one limitation of ourwork is that the rnn (meloni et al., 2021) actually outperforms our transformer on the chinese dataset in chang et al. (2022). in addition, as with other neural approaches, our model requires significant amounts of data, which is often not available to historical linguists researching less well-studied language families based on field reports. romance and chinese have relatively many cognate sets because the protoforms are documented5, but a low resource setup with 200 cognate sets would not fare well on our datahungrier transformer model. furthermore, concatenating the entire cognate set may not work on language families with hundreds of languages such as oceanic because the input sequence would be too long compared to the output protoform sequence. finally, we obtain our chinese gold protoforms from baxter and sagart (2014)’s middle chinese reconstruction, which was actually a transcription of the qieyun, a rhyme dictionary. norman and coblin (1995) disagree with relying on such a philological source and prefer comparative reconstructions that begin from daughter data. however, there is no available comparative reconstruction of middle chinese with protoforms corresponding to thousands of characters to use as a gold standard. be that as it may, it seems clear that middle chinese as recorded in theqieyun is not identical to the most recent ancestor of the chinese languages. its preface concedes that it is a compromise between tang dynasty dialects. the situation with romance is, in some ways, comparable. classical latin—the variety on which we train— is not the direct ancestor of modern romance languages. instead, they are descended from vulgar latin or proto-romance, which is not well-attested and is primarily through graffiti and other informal inscriptions. proto-romance reconstructions are also not exhaustive. as a result, it is difficult to find a dataset like meloni et al. (2021) with thousands of such ancestor forms. we are also limited to the faithfulness of espeak-ng’s latin g2p, from which meloni et al. (2021) obtain their phonetic romance dataset. for most language families, protoforms are not attested. in fact, as the term is often used, protoform refers to a form that is inferred only through linguists’ comparative method. we adopt the other usage for simplicity. in practice, our approach would require reconstructions made by a linguist to serve as training labels for cognate sets."
148,"limitations in this work we have tested our approach using spanbert, a relatively small model when compared to, say, deberta_large or gpt. spanbert has been reported to obtain state-of-the-art performance for relation extraction (joshi et al., 2020; tang and surdeanu, 2022), but it is unclear if a larger lm would improve this semi-supervised learning setting. we use both surface patterns (in the tokensregex (chang and manning, 2014) format) and syntactic patterns (odin (valenzuela-escárcega et al., 2016)) as training seeds, but our approach can only produce syntactic patterns as outputs. this is not ideal, since there is empirical evidence showing that the mixed representation for rules may provide better performance. for example, we can easily capture per_title relation with a surface rule such as “{obj_title} {subj_person}”, which simply looks for the two entities being adjacent."
149,"limitations of the original formulation, of being slow for large grammar sizes."
150,"limitations in this paper, we focused on analyzing the properties of textual representations in the few-shot learning scenario. its applicability to broader annotation scenarios could be presumed but is not supported by our empirical results. our experimental setup is based on binary classification tasks using english datasets. while our approach is general and could be easily extended to multi-class scenarios, more work would be required to extend it to other more complex structured prediction settings such as sequence tagging. we see several ways in which this work could be extended. the most obvious extension consists of trying to generalize the notion of alignment to other tasks beyond sequence classification, such as sequence tagging. in this paper, we have used thas to understand the quality of a given textual representation. however, since thas is a function of a labeling and a representation, it could also be used to measure the quality of a labeling (yan and huang, 2018), given a fixed representation. for example, this might be used in the context of hierarchical labeling, to measure which level of label granularity is better aligned with some input representation. the goal of this paper was to provide an explanation for the success of pre-trained word embeddings for text classification in the few-shot learning scenario. we believe that with our proposed methodology we have successfully achieved this goal. however, it should be clear to the reader that we do not provide a method for picking the best representation, i.e. for model selection. this is because our analysis requires access to labeled data and if labeled data is available the best way to select a model will be via cross-validation."
151,"limitations in protocol standards, stress on specifics, and compare with existing protocols or previous versions. several words across different liwc categories (risk, negemo, and adj) highlight such behaviour, e.g., ‘problems’, ‘before’, ‘particular’, ‘specific’, ‘different’, ‘most’, and ‘than’. however, there are many words with dual sense, like ‘trust’ which has a very technology specific usage related to network security instead of conversations involving trust issues between individuals or trust in any given situation. similarly, the word ‘live’ is related with an application or network being live, instead of its conventional meaning. we also observed that some of the liwc categories, such as bio, did not have specific terms that could clearly establish its significance in favour of influential participants (e.g., word ‘problems’ and ‘trust’ reflecting the significance for the category risk), instead such categories had several words with quite weak correlation with influential participants. such words collectively drifted the weight of the category towards influential participants."
152,"limitations we think the following three points are the limitations of this work. (i) as mentioned in section 3.3, the computational cost of our distillation approach increases linearly with the number of gd steps and the distilled data size. it is necessary to explore efficient distillation algorithms to scale our method to more complex tasks or full-scratch training in future work. (ii) to optimize the distilled dataset through the gradient method, we utilized word embedding vectors instead of directly optimizing the text as in the existing work. therefore, the distilled dataset we obtained cannot be applied to models with different word embeddings, such as other pretrained models or full-scratch training. (iii) in our experiments, we evaluated our approach only on text classification tasks. however, our approach can also be applied to text generation tasks as well by applying the attention labels to all input tokens (not only [cls]) and using vocabulary-wise soft labels. in future work, we should investigate its performance and explore more effective approaches."
153,"limitations this work complemented previous analyses on the link between the linguistic and psychological accuracy of a neural language model by expanding the language sample to ten typologically distinct languages. however, our sample of neural language models was limited with respect to the literature focusing exclusively on english (oh et al., 2022; oh and schuler, 2022; shain et al., 2022). this problem cannot be overcome at the present state of affairs, since there are very few available massively multilingual auto-regressive language models, and the only one with sufficient coverage of our language sample was xglm. this problem is an expression of a general difficulty in nlp to conduct experimental research on low-resource languages, due to the extreme skewness in the distribution of available resources (joshi et al., 2020). however, we are confident that future developments in natural language engineering will support an additional test of our hypotheses with a more representative sample of models."
154,"limitations future work can further apply our approaches to other semantic parsing tasks. for example, for parsing texts to lambda-calculus expressions for knowledge base question answering (dong and lapata, 2016), one can similarly preprocess the schema items (e.g., “department_time” into “department _ time”) and typed values (e.g., “dallas:ci” into “dallas : ci”) for more meaningful subword tokenization results. in addition, our experiments are based on t5. to further verify the effectiveness of our techniques, one can apply them to other pre-trained language models such as bart (lewis et al., 2020) and gpt-2 (radford et al., 2019) as well."
155,"limitations the presented classifier and dataset are only from english-speaking sources, a major disadvantage in detecting white supremacist content globally. the dataset also is predominantly sourced from data between 2015-2019 and reflects white supremacist extremist responses to current events from that period, including the black lives matter movement. this limits its effectiveness in detecting white supremacist content from other time periods. though including anti-racist data helps mitigate bias tested by our sample of the hatecheck dataset, an accuracy of 69.2% shows room for improvement. there is still a risk of overclassifying posts with marginalized identity mentions as white supremacist."
156,"limitations we present the first study of generating questions for filling in information gaps. our method is limited in several ways. first, it focuses on information that is explicitly missing, and does not discuss information that is inaccurate or incomplete in other ways. second, it only asks one follow-up question and does not address multi-turn dialogue about a student answer, or multiple student answers. finally, our approach makes somewhat restricted use of the student answer, and it will be better to generate questions that directly uptake information from the student text (demszky et al., 2021). we leave the deep investigation of these for future work."
157,"limitations since rule abduction and inversion utilize the same groundings as the original rules, neuro-symbolic kgc models that are based on grounding the entire rule will not benefit from these augmentations. abduction and inversion also require the model to be trained on a knowledge graph that contains the inverse relations r−1 for each relation r. finally, since rnnlogic+ has a separate rule embedding for each rule, performing rule augmentation increases the number of parameters in the model and leads to longer training times and larger gpu memory consumption."
158,"limitations the cogen model introduced in this paper uses temporal relations as a process of abductive reasoning. although, temporal relations have been shown to be very useful in abductive reasoning (verdoolaege et al., 2000), the measure of the effectiveness of other types of relations about an observation have not been evaluated in this paper. in addition, because of the unavailability of a large number of human evaluators, we randomly selected 100 selected results as opposed to the entire result which would have been ideal."
159,limitations. we take responsibility for any ethical concerns that may arise as a result of our research.
160,"limitations the pre-training privacy policy corpus and the downstream task datasets are unlikely to contain toxic or biased content. therefore, they should not magnify toxicity or bias in the pre-trained and fine-tuned models, although the models may exhibit such behavior due to their original pretraining. the pre-training and benchmark datasets are formed based on privacy policies crawled in the past; as a result, they could be outdated by now. this work focuses on the english language only, and the findings may not apply to other languages."
161,"limitations incomplete representation of all demographic groups we highlight that the names used in our study are not close to a complete representation of every demographic group in the united states or world. in our study, we adopt the definition of race/ethnicity from the us census survey, using us-centric racial and ethnic categorizations that may be less applicable in other countries. we adopt a binary model of gender (female and male), based on the ssa dataset, which is derived from statistics on baby names and assigned sex at birth; this approach limits our ability to study chosen first names, or to study fairness with respect to nonbinary and transgender people. for race/ethnicity, our study is limited to us census categories of white, black, hispanic, and asian. we are unable to include american indian or alaska native in our study, for instance, as we were unable to identify any names from this group that met our inclusion criteria of > 50% membership according to our name data source. furthermore, by using first names as a proxy for demographic attributes, we are only able to study certain demographic attributes that plausibly correlate with names (e.g., race, ethnicity, and gender) but not other demographic attributes that are likely harder to infer from names (e.g., ability or sexual orientation). other demographic attributes that may be discernible to varying degrees from first names were excluded from the scope of this study (e.g., nationality, religion, age). assumption: invariance under name substitution invariance under name substitution, while a valuable fairness criterion for social iqa, may not hold in all other task settings. for example, a factoid qa system should provide different answers to the questions “what year was adam smith born?” (1723) and “what year was bessie smith born?” (1894). extended evaluation time and heavy computational costs due to the huge number of mcq instances we construct for evaluation and a diverse set of names to cover multiple demographic identities, it takes a considerably large amount of time and computational resources to obtain the analysis results. we detail the approximated time and computational budget in appendix b.2. however, it is worth noting that the extensive analysis on a wide range of mcq instances and names makes our observations more statistically robust. a future research direction may be optimizing the implementation of sodapop framework, which we use as a major experiment setup to obtain the analysis, for more efficient evaluation. (in)effectiveness of counter-factual data augmentation it is worth noting that the ineffective result we obtained is not surprising because sodapop has demonstrated that models that are trained with existing state-of-the-art debiasing algorithms continue to treat names differently (an et al., 2023). although we find that controlling the name distribution in the finetuning dataset to be rather ineffective in mitigating the disparate treatment of names, it is an open question if applying cda to the pre-training corpus would be more effective. a recent work proposes to apply cda to the pre-training corpus (qian et al., 2022), and it will likely be a great source to use for investigating our open question here."
162,"limitations our proposed approach requires to train two independent classification models. while the models can be trained in parallel, this requires larger gpu memory. for the experiments, we trained two bert-base models, which have around 220m trainable parameters when trained in parallel. this requires almost twice the gpu memory compared to a single bert-base ner model, having around 110m trainable parameters. owing to a pipeline-based structure, the overall performance of our system is upper bounded by the performance of span detection model which has lots of potential for improvement. on dev set, we find that around 30% of errors for ontonotes5.0 and bionlp13cg, and around 22% errors on wnut17 are just due to minor boundary detection issues. their entity types are being detected correctly. we henceforth encourage the research community to design architectures or new training objectives to detect mention boundaries more effectively. currently, in our span detection model, all entity mentions are grouped into a single class. as a potential future work, we expect to get even better performance by a hierarchical extension of our setup. at the top level, we can detect mentions belonging to some crude categories and gradually break them down into more fine-grained categories."
163,"limitations to the transformer architecture (such as the inability to implement unbounded recursion), our results show that it may have stronger inductive biases than previously believed: with sufficient training, transformers can represent hierarchical sentence structure and use this structure to generalize correctly."
164,"limitations our work has the following limitations. first, we only evaluate generalization on datasets based on english language. second, we show structural grokking on three datasets, and while we believe this to be a general phenomenon, we leave investigating similar behavior on other datasets for future work. next, we also do not study the effect of training data size on structural grokking, and do not investigate whether transformers learn to grok hierarchical structure in low data regimes. finally, all datasets here are based on context-free grammars, either similar to or taken directly from prior work, and we believe constructing similar generalization benchmarks on real language data is a good avenue for future work."
165,"limitations while our proposed projection techniques often improve cross-lingual transfer, the choice of the projection layer and the projection probability in the case of random projection are hyperparameters that vary across tasks and languages. our ongoing work involves identifying a mechanism via which we can parameterize these quantities, enabling the model to directly learn the optimal layer and probability values for projection."
166,"limitations our proposed pre-training approaches require access to large gpu resources (pre-training is performed on 350m training samples for large language models containing 100’s of millions of parameters). even using 10% of the original pretraining compute, the additional pre-training takes a long time duration to finish (several days even on 8 nvidia a100 gpus). this highlights that this procedure cannot easily be re-done with newer data being made available in an online setting. however the benefit of our approach is that once the pre-training is complete, our released model checkpoints can be directly fine-tuned (even on smaller target datasets) for the downstream contextual as2 task. for the experiments in this paper, we only consider datasets from the english language, however we conjecture that our techniques should work similarly for other languages with limited morphology. finally, we believe that the three proposed objectives could be better combined in a multi-task training scenario where the model has to jointly predict the task and the label. at the moment, we only tried using different classification heads for this but the results were worse."
167,"limitations we note two limitations of our paper. first, our work does not extensively evaluate all the available pre-trained models that could be suitable for this task, e.g., electra (clark et al., 2020), biolinkbert (yasunaga et al., 2022), gatortron (yang et al., 2022), radbert (yan et al., 2022), and pubmedbert (gu et al., 2021). the aim of this work is not to report the strongest possible score but rather to address weaknesses of existing radiology report summarization studies (in terms of data and evaluation). yet, we are confident our proposed solutions report a strong baseline for future work. second, although f1-radgraph seems like an appropriate metric to evaluate our new modalities and anatomies (and appears to be consistent with rouge scores), it has only been evaluated subjectively and not systematically."
168,"limitations we replicate three well-known experiments in the gender bias literature, where bias is measured according to a binary female vs. male view. this choice ignores other views of gender but eases the presentation of the frameworks. we only use two corpora and three datasets which by no means capture the biases of all the people speaking or writing in the english language. moreover, we don’t experiment with different corpus sizes, a more diversified set of corpora or more bias types. we hope to explore this in future work. the hyperparameters of the models have not been varied, using their default values. this replicates the standard experimental setting used in the literature. since there are no ground truths when measuring biases (that is, there are no annotations with the amount of bias of words in large corpora), hyperparameters are usually set to their default values."
169,"limitations we only experiment with one type of retrievalaugmented language models, i.e., retro. however, the ways the other models retrieve neighbors and integrate them are not so much different to affect the results in this paper. the experiments in this paper are done with a small size retro model and data compared to the sizes considered by borgeaud et al. (2022), due to computational limitations. according to the same authors, however, the gains should be constant with the increase of the model and retrieval set size. the larger models are mainly different in their behavior when there is no overlap. however, this should not affect the copying tendency of these models tremendously, as it is still the easiest way to generate the next token. it is also worth noting that retro[off], while not using retrieval at test time, is still trained using retrieval – so it is not a complete retrieval- free model. the results presented by borgeaud et al. (2022) however, show that retro[off] is on a par with their retrieval-free baseline in terms of bpb. finally, we note that our evaluations have only considered the perplexity under teacher forcing, and we have not investigated the behavior of the model in free-form generation or with any kind of fine-tuning."
170,"limitations while the training method makes use of user profile description and history, one additional factor that is important is the structure between users and news articles. knowing a user’s social circles can often give hints about the user’s interests and beliefs, which can potentially help the model to infer how a particular persona would respond to an issue. a possible direction is to design a method that explores the social context features (e.g., social network) via graph-based algorithms."
171,"limitations our model does not follow existing sentence embedding models that encode sentences into embeddings. therefore, one limitation of our method is that it is specifically designed for sts task (or more precisely, sentence comparison task) and cannot be easily transferred to other tasks, such as sentence classification. additionally, our approach incurs a slight extra time overhead of approximately 10%, which may be unacceptable for applications that require high time efficiency. our method only takes into account the semantic comparison of individual tokens, rather than considering the meaning of combinations of tokens or phrases. a possible direction for future work is to incorporate the consideration of compositional semantics, for example by grouping tokens into phrases and applying a similar phrase-level matching algorithm."
172,"limitations there are several limitations to this work which should be kept in mind. first and foremost, the datasets for evaluating the measurement of semantic change are relatively small, meaning that any estimates of correlation with human judgements will be relatively high variance. in addition, although the semeval data includes text from four languages, there is no guarantee that these methods will work as well as they do on other languages or other time periods. moreover, our approach depends on the use of pretrained language models, and the quality (or existence) of these and other relevant resources will vary by language. in addition, like all methods, our approach involves numerous small choices, such as the number of background terms to sample, the number of samples taken, and the value of k in choosing top substitutes. we have kept our choices for these consistent across all five datasets, and these values have not been tuned. as such, different choices could result in better or worse correlation with human judgements. it is also worth noting that the human judgements collected by the creators of these datasets may involve errors or noise. it is possible that a different sample of data, or having different people evaluate the same data, would produce different judgements. for exploring the variation in word meanings, we have used the approach of eyal et al. (2022) directly, with the only differences being that we mask terms of interest (allowing us to work with terms that do not exist in the model vocabulary), and do not combine multiple forms of lemmas when getting the top-k terms. we adopt this approach because it is especially easy to combine with our own work, but different methods for word sense induction might lead to different"
173,"limitations the main limitation of our method is that it requires human effort to increase the variety of templates, which makes it difficult to create large datasets. using templates to generate data reduces the time required to create data manually, but the need for human labor remains an obstacle. to resolve this, the templates themselves need to be generated auto- matically, although the tags that constrain the nouns also need to be generated automatically, which is a difficult problem."
174,"limitations one limitation of our work is the experimentation only with languages with shallow orthographies, i.e. relatively simple g2p and p2g mappings. the results might vary for deeper-orthographies languages. although we took extra care to verify our conversions are correct and complete, and designed the rules to be as comprehensive as possible, automatic rule-based processes in languages may not be 100% perfect and some corner cases may introduce errors. these errors may propagate to affect the numerical results. to mitigate this issue, when ambiguities in determining a target phoneme (or grapheme) in a given language occur, we purposefully select the values that occur more frequently in the unimorph data of that particular language."
175,"limitations candidate summaries dependency while we mainly investigate a training objective to select the best summary among a set of candidates, we find that our model has been dependent on those obtained from the generation model. recently, several works have been presented to improve language generation. for example, narayan et al. (2022) and xu et al. (2022) improve decoding methods to generate diverse outputs. it will be beneficial when applying our method to these approaches. one-sentence summary our approach can fail to capture the information from an extremely short summary. since table 2 shows that our approach has a smaller improvement than cnn/dm, we plan to investigate that our model aims to capture more detailed features from an input text."
176,"limitation wherein novel predicates appear in the test set that do not appear in any of the training or validation set. this is a current limitation of our system. since we do not do any online learning during the test phase, there is no way to take these novel predicates into account. the significant effects of amr-originated noise or lack of information can be seen by comparing the second-to-last and third-to-last rows. here we see a significant degradation across metrics and datasets. however, the performance is still comparable or often better than the deep-learning-only benchmark of the first row. comparing the last row (full method with proprioception module) and second-to-last row shows that we can recover most of the performance. we can also see that the metrics are competitive to the model-based approach from game-engine provided logical facts (3rd-last row). this shows the effectiveness of adding the proprioception module comprising both the memory and memory-based constraints."
177,"limitations our approach relies on the performance of the fewshot paraphrasing. this results in two limitations for our approach. one limitation is the difficulty in accessing gpt-3 and opt-175b models. these models currently need to be more widely available. opt-175b has a free version but it is very slow. another limitation is the need for annotated demonstrations for few-shot paraphrasing. while there are available models and tools, like quillbot, that can be used for this purpose, their quality is not comparable to gpt-3 and opt-175b. this can limit the power of these tools in our approach. using human knowledge to paraphrase the demonstration can help these large models generate high-quality paraphrases but it is expensive."
178,"limitations of glosses and common datasets, as well as a standardized evaluation method (§2). in order to make future research on gloss translation more meaningful, we make practical recommendations for the field (§3). we urge researchers to spell out limitations of gloss translation approaches, e.g. in the now mandatory limitation sections of *acl papers, and to strengthen their findings by implementing existing best practices in mt. finally, we also caution that researchers should consider whether gloss translation is worthwhile, and if time and effort would be better spent on basic linguistic tools (such as segmentation, alignment or coreference resolution), creating training corpora or translation methods that do not rely on glosses. limitations our approach to surveying the research literature has limitations. firstly, some characterizations of the published works we survey are subjective. for example, it is somewhat subjective whether a paper “includes an adequate"
179,"limitations there is much variation in literary writing and narrative styles, and our work here deals with a small, curated subset of this domain. the novels we analyze are all in the english language, and were published between the early 19th and early 20th centuries. the authors and novels themselves are drawn from what is considered to be the established literary canon, and are not necessarily representative of all the works of that era, let alone literary works of other eras. the texts we analyze are largely uniform in narrative style. we limit ourselves to only those quotations that are explicitly indicated as such in the text by quotation marks, thereby eliminating more-complex styles such as free indirect discourse (brooke et al., 2016b) and stream-of-consciousness novels. we do not deal with nuances such as letters and diary entries nor quotations within quotations. the models we analyze for named entity recognition and coreference resolution use a fixed, binary formulation of the gender information conveyed by pronominal terms. though the development of fairer, more representative models is constrained by current datasets, we note that there is encouraging progress being made in this area (bamman et al., 2020; yoder et al., 2021)."
180,"limitations biases human data annotation for a sentimentrelated task, e.g., aspect-based sentiment analysis, hate speech detection, etc., involves some degree of subjectivity. while we included important quality control steps in the tbo annotation process, this intrinsic subjectivity will inevitably be present in tbo and learned by the models (see also the"
181,"limitations this work primarily focuses on evaluating the efficacy of existing continual learning (cl) methods for code generation models. it is important to note that many of these methods were specifically designed for natural language processing or computer vision domains and may not directly transfer to the code generation domain. nevertheless, we have made efforts to identify and address any issues encountered during our analysis. it should be acknowledged, however, that the scope of our work is limited by the selection of methods and the benchmark used. while we have utilized the most popular cl methods from various categories, there may be methods that have not been included in this study due to their inefficacy in natural language processing or computer vision tasks but may be effective in code generation. as such, we encourage further research within the community to explore the potential of cl methods for code-generation models."
182,"limitations like lots of deep learning algorithms, our work also needs gpu resources. in common learning problems, models will be trained once on the existing training datasets, using dev sets for tuning the models. then the trained model would be ready for use. in contrast, in active learning, we need to train the model several times (i.e., whenever new annotated samples are added to the current training set, the model should be re-trained), which increases the need for gpu resources. however, the need for gpu, is not related to our proposed method and it is due to the nature of active learning. in addition, one can run the active learning method once (rather than iteratively) for building an acceptable dataset. it should be noted that we have designed the algorithm in a way to be independent of the target language and utilized model. however, we only tested our method on egyptian arabic dialect and the accuracy of the model should be investigated on other languages and dialects using different learning models in further studies."
183,"limitations the benchmark for language identification for the most part contains clean sentences (grammatically correct, single script, etc.). data from the real world might be noisy (ungrammatical, mixed scripts, code-mixed, invalid characters, etc.). a better representative benchmark might be useful for such use cases. however, the use cases captured by this benchmark should suffice for the collection of clean monolingual corpora. this also represents a first step for many languages where no lid benchmark exists. the use of synthetic training data seems to create a gap in performance due to divergence in train/test data distributions. acquisition of original native romanized text and methods to generate better romanized text are needed. note that the romanized lid model does not support dogri since the indicxlit transliteration model does not support dogri. however, since dogri is written in the devanagari script using the transliterator for hindi which uses the same script might be a good approximation to generate synthetic training data. we will explore this in the future. this work is limited to the 22 languages listed in the 8th schedule of the indian constitution. further work is needed to extend the benchmark to many more widely used languages in india (which has about 30 languages with more than a million speakers)."
184,"limitations our work has several potential limitations. first, we determine the threshold γ by manual selection, which may limit the performance of seq2seq models, it will make our work more effective and elegant if we dynamically select the threshold. second, besides the improvement on three widely used tasks, we believe that there are still other abilities, like code generation, of seq2seq models that can be improved by our method, which are not fully explored in this work."
185,"limitations while we show that applying gap can result in a significant improvement in the generalization capability of lms, especially for dialogue tasks, we are only able to show 300 gap runs for each lm size in this work. we leave scaling the number of gap runs, as well as selecting specific text samples to perform gap on for future work. furthermore, a separate validation set of the tasks at interest are needed in order to choose the best checkpoint when performing gap. future work may look for other task-agonostic cues such as language modeling loss to determine the best checkpoint to use for inference."
186,"limitations our dataset and model only covers 201 languages: the ones we were able to test with the flores-200 evaluation benchmark. in addition, because our test set consists of sentences from a single domain (wiki articles), performance on this test set may not reflect how well our classifier works in other domains. future work could create a lid test set representative of web data where these classifiers are often applied. finally, most of the data was not audited by native speakers as would be ideal. future versions of this dataset should have more languages verified by native speakers, with a focus on the least resourced languages."
187,"limitations our results nor evaluation set cannot be used to indicate whether rte models trained for other languages are robust to paraphrases. however, researchers can apply the methods we used to develop p̂ arte to build evaluation sets in other languages to test whether non-english nlu systems are robust to paraphrases."
188,"limitations the proposed pll-word-l2r metric has the same practical limitations as previous ll/pll approaches. most importantly, these scores can be influenced by many superfluous factors, such as the number of available synonyms (computer vs. laptop; holtzman et al., 2021). we therefore expect our method to be most useful in highly controlled minimal pair or multiple choice setups. even more accurate metrics may emerge in the future. for instance, our approach pre-specifies the number of tokens in a word, thus limiting the space of possible alternatives. future approaches might investigate a way to normalize the pll score distribution over words with a varying number of tokens. further, it would be interesting to attempt to estimate the joint probability of all tokens in a word instead of predicting them left-to-right (as in pll-word-l2r) or without any other within-word contextual information (as in pll-whole-word). finally, we test our approach on english text corpora; our results might not generalize to agglutinative languages (due to a high number of tokens per word and, therefore, increased uncertainty) and are of less relevance to isolating languages (where, if enough training data are available, most wordlevel items can be represented as single tokens)."
189,"limitations rooted in inherent properties of general purpose models, preventing these to become holistic solutions to complex nlp problems without further research. we discussed opportunities provided by chatgpt and similar models to advance the development of specialized systems. with our insights and"
190,"limitations our paper has the following limitations: (1) in realworld applications, the label hierarchy may be more than two levels. it is worth extending our method to such a setting and empirically verifying it. (2) our selection strategy simply takes top r% confident samples, which might result in class imbalance problem. alleviating the imbalance problem may further improve our performance. we leave them as future work."
191,"limitations one limitation of this work is that we have included results for only eleven languages. training asr models, even on small datasets, requires significant computing and financial resources. second, there are not that many freely available and well prepared asr datasets that are readily compatible with all four asr architectures. we sought to select a diverse set of languages and datasets with varying features in order to provide, we hope, a reasonable snapshot of how the state of the art performs in low-resource settings."
192,"limitations for inter-parser comparisons. imagine a case where parser a claims to outperform parser b, and closer inspection reveals that a and b differ in their outputs on only two sentences. for one of these sentences, a’s output has a higher f₁ than does b’s; for the other, the opposite is true, but the difference in f₁ is smaller. now, the claim that a outperforms b becomes a claim that the parse that a produces for the first sentence is better than the parse that b produces for the second. and yet, as indicated by the judges’ disagreements in the second task, it is not always possible to make these kinds of judgements."
193,"limitations we find several limitations in this work. first, we acknowledge that the technical novelty of this work is limited: we introduce a sequence classification task, and we investigate rather standard models in our experiment section (i.e., state-of-the-art transformer language models). nevertheless, we believe that there is a gap in the literature for the task presented in this work, hence our introduction of the environmental claim detection task, the dataset, and models. second, we collect data from sustainability reports, earning calls, and annual reports. however, this does not cover the universe of text where environmental claims are made, e.g., company websites and product descriptions. also, environmental claims can be made about environmental improvements on a wide range of topics such as carbon emissions, water pollution, and recycling, among others. we discussed creating different datasets, where each dataset is dedicated to one specific is- 9task force on climate-related financial disclosures sue. however, we leave this to future work. third, sometimes it is necessary to have access to more context to determine whether a sentence is an environmental claim. we discussed whether it would be beneficial to annotate whole paragraphs instead. however, the trade-off would be exploding annotation work and costs, hence our decision to introduce environmental claims as a sentence-level classification task (and we specifically asked annotators to reject ambiguous cases as environmental claims). nevertheless, given a unlimited budget, we would have pursued annotating whole paragraphs instead (or annotating all environmental claims in a paragraph). our data sources, e.g., sustainability reports, are mostly published by european and us-listed companies, which is reflected in our dataset. we crawled these reports from the sec10, hence our dataset contains mostly claims made by (a) big firms and (b) firms from developed countries. it is conceivable that smaller firms and firms from nondeveloped countries make different environmental claims, and models trained on our dataset might not be suitable to detect these claims. moreover, our work is subject to all concerns raised in the"
194,"limitations although the experiment results have illustrated the effectiveness of the proposed imitation-demo method, we have to admit that our work has the following limitations: 1) this article is based on that the readers have some knowledge of prompt-based learning or demonstration learning. due to the space limitation, we can only briefly describe the basic process of the demonstration learning, which may make the article a bit obscure and difficult to follow. 2) imitation-demo does not achieve state-of-theart on all the datasets, but outperforms other strong baselines on 5 out of 14 datasets. besides, it consistently surpasses the demonstration learning-based baseline lm-bff. since imitation-demo is trained without introducing new parameters and explores the working principle of demonstration learning from a certain perspective, we believe the results are acceptable."
195,"limitations we highlight three main limitations of our work. first, although we have explored gradient-based explanations that take the whole network into consideration and have been shown to be faithful in previous work (bastings et al., 2021), we do not explicitly explore how comet combines the sentence representations in the feed-forward that precedes the encoder model to produce the sentence-level score. second, we have shown that combining attention with gradient information results in the best explanations for unite-based metrics. however, from a practical standpoint, running inference and extracting the explainability scores simultaneously may be more computationally expensive than other techniques: gradient-based metrics benefit from gpu infrastructure and require storing all gradient information. third, we have not explored extracting explanations in low-resource settings. that is because high-quality mqm annotations for such language pairs are not yet available. nevertheless, further research in those settings is needed to access the broader validity of our claims."
196,"limitations our method relies on having access to teacher embeddings and prediction which may not always be possible in a black-box distillation setting. retrieval augmentation also requires maintaining a knowledge base that is memory intensive. the cost of the retrieval process is dependent on the size of the training corpus, which can be a limitation when dealing with very large training datasets. conducting dataset distillation (wang et al., 2018b) on the training corpus to further reduce memory cost and retrieval time is an important future step for our framework. acknowledgments this work was done when jianyi zhang was an intern at amazon search. in addition, jianyi zhang and yiran chen disclose support from grants cns-2112562, iis-2140247, and cns-1822085. we thank yuchen bian for the valuable"
197,"limitations rerankner conducts calibration after the regular training, which introduces extra computational overhead. this drives us to further improve the overall efficiency of our method. recent works find that few-shot learning serves as an effective finetuning method of pretrained language models. it is reasonable to investigate our model under fewshot learning to reduce the overhead. although we get competitive results with the state-of-the-art methods, there is still a gap between the oracle score and the best results. we leave them as our future work."
198,"limitations the limitations of this work mostly come from our assumptions: 1) a randomly initialized and frozen tlm, and 2) input tokens are all different and randomly sampled. these two assumptions obviously do not hold true for human languages and pre-trained tlms. therefore, we attempted to empirically verify the existence of lemmas and properties on a pre-trained tlm without positional embeddings in §5. that being said, several methods could be attempted to remove these assumptions. firstly, we can analyze the training dynamics of a tlm to shed light on the model parameter distribution after pretraining. secondly, zipf’s law or a simple n-gram language model could be used to quantify the degree of input token duplication in human languages. this might give us a more accurate estimate of the variance at different positions. we leave these ideas as future work."
199,"limitations due to the lack of multi-intent text revision datasets, we only conduct experiments on iterater. although it is a multi-domain dataset, we only use its sentence-level data, and each sentence pair only contains one editing operation. the robustness of our method is still to be verified by evaluating it on more types of datasets in future work. another limitation of our work is that we only made improvements at the model level. we have noticed that kim et al. (2022) recently improved text revision by leveraging extra data from other text editing tasks and performing editable span detection before revising. similar methods can also be applied to our model and will be tried in our future work."
200,"limitations limited number of operators considered following previous methods (lan et al., 2021), we only consider binary operators (+, −, ×, and ÷). as we adopt a code-style output format, it is possible to introduce other non-binary operators supported by the python interpreter, e.g., sum() and max(). however, obtaining labeled data with such operators may require laborious efforts. we believe it is an interesting research question on exploring how to teach models to solve practical questions e.g., math word problems, by writing code in a low-resource setting (jie and lu, 2023). limited performance due to greedy decoding all the results we report in this work are produced via greedy decoding. a recent work (wang et al., 2023) reports that making large lms generate multiple answers and selecting the answer with the most votes can boost performance by a large margin. however, performing beam search for symbolic neural reasoners, e.g., deductreasoner, can be challenging in that searching space increases exponentially with the number of variables in the question (jie et al., 2022). designing effective beam search strategies for symbolic neural reasoners is a promising direction."
201,"limitations the proposed approach in this paper also suffers from certain limitations, i.e. we adapt apt on the encoder model and lack design for the other architectures such as decoder-only and encoder-decoder. in addition, it is better to generalize the key idea to other parameter-efficient learning approaches. a unified solution for existing work may be worth exploring in the future."
202,"limitations one limitation of this work is that vag does not achieve zero forgetting. although we show solving cil based on label generation can effectively ease forgetting and representation collapse of the pre-trained model, it is still interesting to further explore how to explicitly solve the forgetting issue in this new framework. the proposed techniques in vag are a step in the exploration. another limitation is that we directly use the label sequences provided by the original dataset. this may be suboptimal because the quality of the manually created label is hard to guarantee as it may fail to capture the semantic information of the samples in a class. a potential direction is to study creating label sequences automatically by summarizing the training samples. we leave this for future work."
203,"limitations the identified tendencies towards mentioning object-related features and the reliance on the shape as a contrastive feature might be driven by the grammatical structure of the annotations, mostly presenting object features in sentence-initial subject position, although 40% of exhaustive captions mention either the scale or the object color as the last word in the sentence. therefore, these results call for investigating the biases of model architectures less sensitive to sentence length than lstms, as well as extending the annotations with additional grammars. further, this evaluation provides descriptive results of the models’ pragmatic abilities, leaving the question of whether it is indeed a pragmatic inductive bias or, e.g., structural language drift (lazaridou et al., 2020) causing the observed patterns, unanswered. finally, since the evaluation pertains to the surface form of the predictions, applying decoding schemes other than greedy decoding used in this work might provide different patterns, indicating to which degree potential biases are due to model mechanics in opposition to sampling parameters."
204,"limitations we have demonstrated that an accurate description can perform better for both supervised and weakly supervised event detection. however, the event types from most existing ontologies are not properly defined. for example, in ace annotation guideline (linguistic data consortium, 2005), transfer-money is defined as “giving, receiving, borrowing, or lending money when it is not in the context of purchasing something”. however, it is hard for the model to interpret it accurately, especially the constraints “not in the context of purchasing something”. in addition, many event types from maven, e.g., achieve, award, and incident, are not associated with any definitions. a potential future research direction is to leverage mining-based approaches or state-of-the-art generators to automatically generate a comprehensive event type description based on various sources, such as annotation guidelines, example annotations, and external knowledge bases."
205,"limitations according to us there are 3 limitations of our work which will be addressed in future work. • the impact of layernorm, language tags, and residual connection settings on zst was analyzed in this study. however, other factors, such as the number of layers of the transformer model, may also have an effect and should be further investigated. • our"
206,"limitations in this work we propose an uncertainty-aware bootstrap learning framework for joint extraction. though it achieves state-of-the-art performance compared to other denoising techniques, unbed requires large training resources considering the ensemble loss calculated between two large plms and the probability variance calculated on the plm joint extraction model. in our future work, we hope to incorporate pruning techniques during training to improve the efficiency. we will also consider more complex relations between entities, e.g., relations beyond the sentence boundary, to fit in real-world information extraction scenarios."
207,"limitations actual applications of our model. our work assumes that input sql queries to our model are always wrong. this assumption is more feasible in an interactive semantic parsing framework, where the users are expected to decide whether a sql parse, accompanied by its natural language explanations (elgohary et al., 2020, 2021; narechania et al., 2021; mo et al., 2022), has errors or not. alternatively, to remove this assumption, it would be interesting for future work to study the performance of our error correction model in combination with an automatic error detection model (chen et al., 2023). experiments with more language models of code. we have only experimented with two language models of code, coditt5 and codet5, both using t5-base (raffel et al., 2020) as their underlying model architecture. it would be interesting to test how our"
208,"limitations we acknowledge the underlying assumptions of the social bias benchmarks used in our study. while the presented study aims to point out a key limitation of currently accepted methodologies, the presented investigation could benefit from more diversification. first, this study focuses on english. while we expect similar issues with similarly-constructed benchmarks in other languages, we leave it to future work to formally address the same. also, the bias benchmarks themselves imbibe the notion of fairness with the western value system (bhatt et al., 2022), and future explorations of benchmarks should diversify culturally as well. last but not least, we acknowledge the harm of binary treatment of genders in one of the target benchmarks. the purpose of this work was to bring light to a broader problem regarding the reliability of social benchmark metrics, with the hypothesis that the main idea of this paper would hold for a wider range of datasets with other assumptions or notions of fairness. we also acknowledge that there are larger models that we were not able to train and evaluate due to the limitations on our computational budget. the current study was focused on benchmarks with templated instances. this is no coincidence: the dominant majority of the social bias benchmarking literature relies on sentences with some degree of known structure, even in those collected from the wild (levy et al., 2021). such structural assumptions in datasets are necessary for defining and extracting quantifiable measures of social bias, which as we argue, are the reason behind the brittleness of their decisions. future work should focus on making our bias benchmarks more diverse and robust to small decisions that go into making them. broader impact bias evaluating benchmarks play a very significant role in helping identify potential risks of language technologies. while a large body of work evolves in this area of work, there is growing concern about the ability of the different benchmarks to accurately quantify and identify social biases. we emphasize these concerns by evaluating how robust the benchmarks are to alternate constructions based on simple linguistic properties. it is important to note how inaccurate measurements of social biases can be problematic by underestimating or misdiagnosing the potential harm from language models. we hope our work helps identify such pitfalls."
209,"limitations scope this short paper serves as an initial step toward peft for long-document models. as such, our evaluated scope of models, tasks, datasets, and kernel variations is limited. we acknowledge the need to experiment across broader settings and hope our work provides a foundation for others to build on. future experiments should analyze the validity and efficacy of using prefix-propagation with other long-sequence models to determine whether the prefix modality is suitable for non-sparse attention approximations. for example, would the projection of prefix vectors using a random feature map as in choromanski et al. (2020) result in an excessive loss of information for these critical tokens? regarding tasks and datasets, the performance degradation in prefix methods for wikihop deserves significant attention. verifying whether this extends to other reading comprehension and question-answering tasks will assist in guiding future research efforts. we restricted our research to the encoder-only version of longformer, but using the encoder-decoder version, led would enable analysis of sequence-to-sequence tasks. the scrolls benchmark (shaham et al., 2022) would be a good starting point for this analysis since it includes an led baseline. combining prefix and kernel methods is an ongoing research effort and there are several questions we plan to address: (1) what are the effects of swapping the default exponential kernel with other variants such as linear, polynomial, and rbf? (2) does making the α scale parameter trainable improve performance? (3) can we have a separate scale parameter for each query and should they be trainable? (4) is this approach effective for modalities other than long-document? (5) can we separate other components of attention into modular kernels (e.g. local and global kernels for sparse attention)? robustness the size and nature of long-sequence tasks often resulted in long run times for the larger datasets arxiv, 20-newsgroup and wikihop. consequently, we report results of one seed after doing a hyperparameter search for learning rate. this aligns with the reporting system of the original longformer paper (beltagy et al., 2020) but greater assurance in all long-sequence task performance could be achieved by accumulating results over several seeds. the size of datasets and iteration over several epochs somewhat mitigate this concern."
210,"limitations while we discover that simply applying cnn on top of the score matrix of span-based ner model performs well on the nested ner scenario, there are still some limitations that are worth discussing. firstly, we mainly choose three commonly used nested ner datasets, which may lack generalization. secondly, we only focus on nested ner tasks for the spatial relations between spans are more intuitive and common in nested scenario than those in flat ner. however, the principle of using cnn to model the relations is also applicable to spans in the flat ner task. future work can take flat ner into consideration based on our exploration, and experiments on more datasets."
211,"limitations non-projectivity. the primary theoretical limitation of hexatagger is that it can only produce projective dependency trees. we would like to explore the possibility of extending hexatagger to non-projective parsing for future work. interpretibility. as a trade-off for efficiency, hexatagger does not model dependency arcs directly. compared to graph-based models that explicitly score arc scores between pairs of words, it is more difficult to interpret the output of hexatagger."
212,"limitations because of the nature of our framework design, our work requires a diverse set of targets during training, which is important for target prediction and therefore the stance detection method. it is difficult to be applied to other stance detection datasets when there are limited training resources with regard to targets, such as conforti et al. (2020) and mohammad et al. (2016). besides, the model is trained on news-related debate corpus, so it may need further domain adaptation if applying the model to other domains such as social media. we are using an auto-regressive generation framework, which will also require extra inference time to generate the whole output sequence compared to the classification model. we would encourage readers to compare it with classification methods for efficiency when it will be applied in a time-sensitive scenario."
213,"limitations the major limitation of the present study is that the effectiveness of the proposed method has been confirmed only for a single task. this is because most existing reasoning tasks are relatively simple that they can be solved by a single external tool at most. for example, most existing numerical reasoning tasks provide self-contained questions; that is, all the required knowledge is included in the questions. in such tasks, a calculator is all that is needed as an external tool. however, it would be rare for a single external tool to be sufficient in real-world applications such as medical text analysis. it is crucial for future work to validate the effectiveness in such realistic scenarios that necessitate the use of multiple external tools."
214,"limitations we identify the following two limitations of our work: • different from raw text, constructing mrcstyle data from wikipedia requires the existence of hyperlinks. this idea works well for resource-rich languages, such as english and chinese. while such an idea is less effective for languages with few hyperlink annotations in wikipedia because a small amount of mrcstyle training data is difficult to guide the learning of nlu capability in those languages. a possible solution is to explore other data resources to automatically construct large-scale mrc data for pre-training. • as observed in table 1, the improvements of sequence classification tasks are less significant than those of span extraction tasks. we suggest that the existence of anchors is not a strong relevance indicator between our constructed query and context. such a finding is also observed in chang et al. (2020). therefore, constructing more relevant query-context pairs for sequence classification pre-training can possibly remedy this issue."
215,"limitations one limitation of current token-pair edit matrix based incomplete utterance rewriting models is that they are only able to select tokens that have appeared in the context utterances. thus, these models, including our own, are unable to generate new words, such as conjunctions and prepositions, to improve metrics such as fluency. however, this can be addressed by incorporating an additional word dictionary as proposed by liu et al. (2020) to improve fluency for out-of-vocabulary words (oov). in addition, we will consider combining generative models (gpt (radford et al., 2019), t5 (raffel et al., 2020) etc.) to assist in the recovery of the incomplete utterances in the future works."
216,"limitations one limitation of our method is that when training larger models, it requires more computation resources, whose cost is relatively high. however, after pre-training, we will release our models so that readers can directly use them without pre-training again. broader impacts we provide a new generative pre-trained model on molecules and text. on one hand, the model can be used to speed up scientific discovery, like molecule design, drug optimization, etc. on the other hand, once the model is trained on clinical data (which also describes the usage of drug molecules), it might lead to personal information leaky. we will enhance data filtration to anonymize all personal information, and will design new models to protect the information."
217,"limitation the main limitation of this work is the technical novelty of hybrid retriever. hyrbid-drboost is built on top of drboost, and the interpolation of bm25 with drboost. however, we would like to point out that our study can serve as an important finding for real-life applications. previous retrievers are built on top of indexing-heavy dense retrievers, such as dpr. this limits their applications where memory is a hard constraints, for example, on-devices. our study suggests that a light hybrid retriever can save memory but maintain sufficient performance."
218,"limitations we focus primarily on comparing model efficiencies using a variety of efficiency metrics and do not consider model performance; one can perform a more elaborate analysis of performance-efficiency tradeoffs, which we did not do here. we only profile a total of seven models across three modalities while there are more efficient variants and vanilla transformers proposed in the literature. while we choose our models to be as representative of each modality and efficiency technique as possible, we cannot extrapolate results to other model variants and other modalities. in particular, modalities like video and genomics and efficiency approaches like quantization would be interesting to profile, which we did not do."
219,"limitations the design of the dynamic templates requires knowledge of the event ontology and is timeconsuming. the authors of the paper spent 30 hours designing the exclusive templates that cover all of the possible argument combinations for each argument role in ace ontology. with a more complicated ontology, a much larger amount of time is required. another limitation of our approach is the offset retrieval method. if one sentence contains multiple mentions of the same entities, or even multiple text strings that have the same spellings but refer to different entities, the qga-ee model always retrieves the position where the mention appears for the first time in the sentence as the offset of the extracted target. it may be improved by asking the model to generate contextual text as a position reference."
220,"limitations we address several limitations with regard to our work. first, the publicly available datasets used in our experiments are limited to english. documents in different languages (i.e., chinese) might require different segmentation techniques and may contain unique characteristics in terms of vocabulary size, data sparsity, and ambiguity. secondly, we only evaluate the quality of the topic models in terms of coherence and diversity. future work should explore how our method impacts other characteristics, such as document coverage (i.e., how well documents match their assigned topics) and topic model comprehensiveness (i.e., how thoroughly the model covers the topics appearing in the corpus)."
221,"limitations due to our budget constraint, we only performed pretraining and downstream experiments with basesized transformer models. we also only applied the masked language modeling objective, but there are other effective pretraining objectives (e.g., clark et al., 2020). nonetheless, since we introduced minimal changes in architecture, we hope that subsequent work will benefit from our narrowing operations and conduct a wider range of pretraining and downstream experiments. while pretrained models can be applied to even more downstream tasks, we designed a reasonable task suite in this work, consisting of both glue sentence classification and the conll ner sequential classification tasks."
222,"limitations since the multi-hop texttableqa problem has only one dataset hybridqa, our model has experimented on only one dataset. this may lead to a lack of generalizability of our model. transparency and interpretability are important in multi-hop question answering. while our model achieves the best results, the model does not fully predict the reasoning path explicitly and can only predict the row-level path and passage-level path. in future work, we will design more interpretable texttableqa models."
223,"limitations our limitations are as follow: • data scale: this paper only employ the wikipedia of wizard dataset, a small scale and well-established knowledge conversation dataset, and lack of the validation on largescale dataset. • backbones: this paper lacks the evaluating of other knowledge dialogue model on the proposed method. actually, we have two reasons to employ the plato. first, the plato can better handle the one-to-many phenomenon, which is suitable for learning our expansion samples. second, the plato is a pre-trained dialogue model, and its performance on knowledge dialogue generation task has been proved. we will evaluating the performance of other knowledge dialogue model on our method for our future work. • knowledge expansion methods: this paper only use the synonym and antonym to construct the noised knowledge, which lacks of the comparison of using other data augment method. indeed, we use two tokenlevel data augmentation methods (synonym and antonym augmentation) to prove our statements on hallucination problem in knowledgedialogue generation task. based on this study, we believe that incorporating other data augmentation methods will also mitigate the hallucinations. • manual prompts and responses: this paper designed five prefix prompts, four post-prompts and nineteen euphemistic responses. for ak-more method, we simply randomly choose one prefix-prompt and one post-prompt and concatenate them with the ground-truth response. this leads to some irregular responses. as for ck method, we randomly select one euphemistic response for the incorrect knowledge. however, we found that the response may not coherent with the query. we will design more smooth expansion ways to construct more human-like training samples for our future work."
224,"limitations in this paper, we propose a method named autoconv, which means automatically generating information-seeking conversations with large language models (llm). though it has achieved great performance on both quac (choi et al., 2018) and coqa (reddy et al., 2019), there are still some limitations that should be noticed. limitation of llm. in our experiments, we use opt-13b (zhang et al., 2022) as the llm for generating synthetic conversations due to the limited computational resources. larger models should be considered to further understand the potential ability of autoconv, e.g., gpt-3 (brown et al., 2020), opt-175b (zhang et al., 2022), bloom-176b (scao et al., 2022), and glm-130b (zeng et al., 2022) etc. limitation of implementation. as mentioned in section 2.2 and appendix b, our method needs to finetune llm and generate massive synthetic conversations based on the finetuned llm, which has a high cost for implementation. limitation of synthetic dialogues. as shown in table 2 and section 3.8, there is still a gap between our synthetic dialogues and human dialogues. it is important to improve the quality of synthetic dialogues so that we can further alleviate the dependence on human annotation."
225,"limitations the corpus and therefore also the asr baseline model only cover read speech. we have not tested the model on spontaneous speech, but we expect it to perform significantly worse on this type of data. our data collection process for swiss german speech with standard german transcripts is designed to collect large amounts of data in a costefficient manner. we estimate costs to be 4 to 6 times lower compared to the transcription of existing recordings. however, there is a downside to our approach. because it is based on a given standard german sentence, it can lead to swiss german speech that’s closer to standard german than the swiss german encountered in everyday conversations. the severity of the shift towards standard german depends on the individual speakers and their ability and effort to produce swiss german representations that are close to how they would speak in everyday conversations. while we made every effort to include as many different dialects as possible in the corpus, there are still strong dialects with a comparatively low german-speaking population that are insufficiently or not at all represented, e.g. some dialects from the canton of fribourg. this is due to the huge dialect diversity in switzerland. the gender ratio is not balanced for some dialect regions in the test set, especially not for vs, where the test set is female-only because we did not succeed to recruit any male speakers from this region during phase 1 of the data collection. however, preliminary experiments do not show a significant difference between genders in swiss german asr performance, so we do not expect this to lead to skewed results. our asr baseline model and other models trained on the corpus may perform below average for children and people above seventy due to the lack of training data for these age groups. ethical considerations participants were specifically recruited to record swiss german speech for this corpus. the purpose of the recordings was made clear at recruiting time: a training corpus for swiss german asr models. participants were also informed at recruiting time that information about their dialect, age, and gender will be collected. furthermore, to be able to participate, they had to read and accept our data privacy policy which further detailed the future use of collected data."
226,"limitations, perform poorly as a standalone model for long-tail classification. these results can be improved by priming the model with an entailment predictor through the usage of a prompt. the baseline shows strong performance independent of the llm, as it operates on a closed label space. the capabilities of the baseline can be enhanced by further explicitly priming it with a entailment relation through a llm. rows in which t0pp is initialized, or primed with e are indicated with primed. priming the model showcases improvements across all datasets for macro f1. for accuracy, priming the model shows benefit in two out of three datasets. in figure 4, we show the results of top-5 predictions for the wos dataset. 1we observe a significant drop in performance when we utilize the 3b parameter variant of this model as l. all results are aggregated in table 1. it is important to highlight that prompt variation led to stable results for our llm. the variance upon utilizing bart-mnli is negligible across prompts. the best results are observed upto top-4 predictions on both accuracy and macro f1 for our method, when the entailment prompt is enhanced with a greater number of tokens corresponding to the output of l(e(x)). the variation between our method and the baseline is much greater for top-1 predictions, but top-5 prediction variance is negligible. detailed results for both depth settings of amazon beauty are shown in appendix c."
227,"limitations in this work, we implicitly utilize the contradiction relation. the authors recognize explicitly including it in a prompt template leads to worse performance due to the injection of noise. controlled template generation based on a model confidence is unexplored in this work and appears to be a promising direction. additionally, we recognize the emergence of parameter-efficient methods for training models which are unexplored in this work, which may have utility. these methods are complimentary and may benefit the performance of models as they can be used in conjunction with training paradigms such as contrastive learning to support better representations through explicit utilization of the contradiction relation. in this work, we limit our study to draw attention to the importance of strict zero-shot classification settings with the emergence of llms. our study can be easily extended to recursively operate on large language models, and entailment predictors. as we observe limited performance benefits in doing so, we conduct our study to show improvements after one complete cycle, given by e(l(e(x)) in section 3."
228,"limitations a lot of recent work especially in computer vision has leveraged the unsupervised methods or unpaired multi-modality data to pre-trained crossmodal language model. applying the same idea into speech language model is also discussed in some recent research works. to compare fairly with previous works in st area, we do not build our model on top of such frameworks and discuss how to utilize the raw audio. in terms of the model training, multi-tasks may affect each other due to uneven data distribution, and we have just scratched the surface of this part of the analysis."
229,"limitations one of the limitations of our survey is that it covers a limited sample space of 15 papers from emnlp 2020 and acl 2020. while a larger sample would be helpful in gathering more evidence, access to specific tracks is limited at nlp conferences, unless hosted online via a virtual or hybrid system. with respect to our case study, we evaluate on the asr utterances, but with labels corresponding to the original manual transcriptions. for a perfect comparison, the asr utterances would need to be re-annotated as the talk move could change based on the severity of transcription errors."
230,"limitations in this work, we do not propose any new methods because, as an opinion paper, we focus on raising the problems and making vivid demonstrations to readers. the experiments are limited to linear svm and bert on data sets in the benchmark lexglue. we hope that, within the page limit, our experiments sufficiently convey the points to readers."
