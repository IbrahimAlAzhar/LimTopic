,Text
0,"The presented results only apply to the English language. Both our benchmark dataset and the baseline model target the English language exclusively. Special text sources such as instant messaging or speech-to-text are likely under-represented in our benchmark test set; therefore, we did not evaluate classification performance in those domains. Since we used RoBERTa as the base model, our model inherits the same limitations. Specifically, the length of input sequences is limited to 512 BPE tokens, and additional pre- and post-processing is necessary to run predictions on longer inputs. However, we did not evaluate prediction aggregation methods or classification performance."
1,"Our work can be considered to have the following limitations:
1. The dataset we introduce contains 10, 000 text instances sampled from a single social media platform. However, we acknowledge this limitation and as noted in section 6, we aim to extend this work by collecting more political data across various social media platforms and using it to model aggressive behavior.
2. We obtained this dataset by crawling for tweets based on 52 keywords (as shown in Table 10). We acknowledge that these keywords may have limited the domains in which political aggression can occur. That being said, we also hope that task generalizability is not compromised due to the presence of pretrained language models at the helm of our experiments."
2,"Although SCL-Fish achieves improvement over Fish, training SCL-Fish takes longer time than Fish. Empirically, we find that SCL-Fish is approximately 1.2x slower than Fish. Moreover, we believe that the subjective nature of abusive language (Sap et al., 2019) affects the annotation process of different datasets and possibly negatively impact performance."
3,"In this exploratory study, we focused on Englishlanguage resources. Further, we examined only one social media platform, Twitter. As any other platform, Twitter has a biased demographic representation of users in terms of language, location, ethnicity, gender, age, socio-economic status, and other characteristics. In particular, Twitter is predominantly used in the United States.10 As a result, user attitudes examined in this study primarily represent Western views and may differ significantly from views common in other regions of the world. Future studies on aporophobia need to include other languages and world regions and consider cultural differences while measuring and mitigating this type of social bias.
When searching for aporophobia-related texts, we excluded derogatory terms and slurs associated with the group ‘poor’ as such explicit forms of online abuse tend to be easier to detect by human
10https://www.statista.com/statistics/242606/n umber-of-active-twitter-users-in-selected-count ries/
annotators and NLP models. Nevertheless, when designing tools for measuring and mitigating aporophobia such explicit expressions need to be taken into account. Furthermore, there is a wide variety of linguistic expressions referring to poor and homeless people, and sometimes this target group is not even mentioned at all, but could be inferred from the context (e.g., contexts referring to hunger, food stamps and/or other benefits, ghettos, etc.). To effectively confront aporophobia, NLP resources (lexicons, datasets, classification models) need to have a wide coverage of explicit and implicit linguistic expressions of the phenomenon.
Finally, we targeted only textual data. However, many social media posts combine text with other types of data, such as images and videos. Recent techniques for modeling multi-modal data can be employed to ensure a better coverage of various types of social media posts.
Ethics Statement
Confronting aporophobia, as an application similar to addressing other types of abusive and toxic language, poses a number of risks and ethical issues, including tension between freedom of speech and respect for equality and dignity, biased data sampling and data annotation, dual use, and many others, discussed in previous works by Hovy and Spruit (2016); Vidgen et al. (2019); Leins et al. (2020); Vidgen and Derczynski (2020); Cortiz and Zubiaga (2020); Kiritchenko et al. (2021); Salminen et al. (2021). Future research on this topic should comply with trustworthy AI principles of transparency, justice and fairness, non-maleficence, responsibility, and privacy (Jobin et al., 2019). Special attention should be paid to involving all legitimate stakeholders in the identification and definition of actions to counteract aporophobia, including the affected communities, non-governmental organizations (NGOs) and government officials working on poverty mitigation. In particular, the views and needs of the communities from both the Global North and the Global South should be included."
4,"The dataset created as part of our contribution leverages hate speech datasets focusing on the English language. Therefore, the model has neither seen, nor been evaluated in other languages."
5,"Our work has limitations. First, we use the TCAV framework, which assumes that concepts are encoded in the linear space of semantic representations. However, recent works show that in some cases, linear discriminants are not enough to define the semantic representations of concepts in the embedding spaces (Koh et al., 2020). Future work should consider nonlinear discriminants to accurately represent concepts in the hidden layers of NLP neural networks.
In this study, we used simple challenge sets to obtain a baseline for assessing the effectiveness of concept-based explanations in measuring false global sufficiency. Future work should focus on curating challenge sets by annotating user-generated data for the label and the concepts, in order to achieve a stronger baseline.
Our work is limited to pre-defined concepts and requires human input to define the concepts with examples. However, defining concepts in TCAV is less restrictive than pre-defining features in other explainability methods, in that concepts are abstract ideas that can be defined without requiring in-depth knowledge of the model’s inner workings or the specific features it is using. This allows for a more flexible approach where users can test the model regarding their concept of interest.
Our method can only be applied to concepts that are known to be important for the classifier and are prone to being over-represented in training sets. It’s important to check this condition independently before using our metrics. In cases where this condition does not hold true, the metrics we use in our work may be interpreted differently and may not be reliable indicators of global sufficiency. Also, we only considered two variations of emotion-related concepts. Other variations such as expression of negative emotions by the writer of the post should be investigated in future work.
Further, our metrics are limited to cases where different classifiers are being compared since the most important information is in the relative value of the metrics. Our metrics should not be used as absolute scores for testing a classifier.
Testing a classifier for false causal relationships is most valuable for detecting the potential flaws of the models. If our metrics do not reveal a false relationship between the concept and the label, that should not be interpreted as an indicator of a flawless model.
Ethical Statement
As with most AI technology, this approach can be used adversely to exploit the system’s vulnerabilities and produce toxic texts that would be undetectable by the studied classifier. Specifically, for methods that require access to the model’s inner layers, care should be taken so that only trusted parties could gain such access. The obtained knowledge should only be used for model transparency purposes, and the security concerns should be adequately addressed.
Regarding environmental concerns, contemporary NLP systems based on pre-trained large language models, such as RoBERTa, require significant computational resources to train and fine-tune. Larger training datasets, used for fine-tuning, usually result in better classification performance but also an even higher computational cost. To lower the cost of this study and its negative impact on the environment, we chose to use existing, publicly available classification models."
6,"limitations our approaches largely focus on explicit mentions of identity terms. this does not capture whether the identity term is the target of hate speech, which would require further analysis. this approach also does not capture attitudes held toward high-profile members of those groups, which play a role in circulating associations with identities (such as personal attacks on women in gaming or the use of “george soros” as shorthand for antisemitic conspiracy theories). future work may try to capture and measure these attitudes. incels.is is a large, popular forum for blackpilled incel discourse, which has a unique and extreme ideology that we argue is under-represented in current hate speech datasets. however, our analysis is limited to this forum, and the trends we identify may not apply to more moderate incel discourse (e.g., r/incelswithouthate) or related online male supremacist movements, such as mgtow and puas. though these communities are known to have related, but distinct jargon (farrell et al., 2020), we emphasize that researchers should recognize these lexical innovations in their annotations for hate speech and include a variety of these communities in training datasets for misogyny."
7,"limitations of such a study - which we discuss in the next section. we release any data (including any raw data, but only in the form of tweet ids and their respective labels for the two tasks), code, and models produced during this study publicly for further research by the community. we license this release under cc-by-sa 4.0. in the near future, we aim to annotate this data for tasks such as sarcasm detection - to develop a deeper understanding of how it is related to aggression and offensiveness. additionally, the motivation for collecting the same data instances marked with aggression and offense labels is for a multitask learning-based model also to be able to identify when 1) the tone of a text is aggressive without being offensive vs., 2) the text is offensive, despite it not being overtly aggressive. we also aim to collect more data and annotate it using weak supervision. finally, we also aim to expand on the theoretical underpinnings of sublime aggression and offense by attempting to identify these within other more tangential domains, viz., comedy."
8,"limitations this study utilizes a monolingual pre-trained language model (plm) in the english language (bert-base-uncased). although the weaklysupervised classification methods are not limited to a particular language, we have not explored applying the method to another language. social media language use may differ significantly from the data used to train the plm. moreover, the presence of code-switching (doğruöz et al., 2021) may also degrade a monolingual plm’s performance. we explored a roberta checkpoint continually trained with 60m english tweets (barbieri et al., 2020).16 however, it does not yield better performance than bert. we have not investigated whether it is due to the training regime or the dataset. moreover, in this work, we focus on classifying hate speech (hs) categories/target groups instead of hs detection (detecting whether a post contains hate speech or not). to perform hate detection and classification, we can either combine our method with another hs detection model in a pipeline or use an adaptation of weakly-supervised text classification incorporating the “others” category such as li et al. (2018) or li et al. (2021). due to limited space, we prioritized in-depth analysis instead of a comprehensive evaluation. therefore, we selected only two datasets (and two- 16https://huggingface.co/cardiffnlp/ twitter-roberta-base way cross-dataset classification). we are working in parallel on extending this work to a longer-form journal article to cover more datasets and experimental results. recent work on large language models (llms) demonstrated that when the parameters scale to a certain level, language models exhibit a drastically-increased performance in zero-shot classification (zhao et al., 2023). we reported the performance of a moderately-sized bert-large-uncased zero-shot model because of limited computational resources and lack of access to commercial apis. larger language models will likely perform much better than this baseline. lastly, understanding hs sometimes requires cultural understanding or background knowledge. it may be difficult to determine the presence and category of hs when we take the post out of its context. for example, many “sexist” posts in waseem dataset are tweets related to the australian tv show my kitchen rules (mkr), and below is a tweet labeled as “sexist”: everyone else, despite our commentary, has fought hard too. it’s not just you, kat. #mkr"
9,"limitations while promising, our work presents limitations that need to be acknowledged. firstly, we did not explore the best verbalizers for instruction finetuned language models, which could have further enhanced the performance of the models explored in this study, due to computational cost and the specific goals of the research. secondly, we selected benchmark datasets based on their popularity and diversity, which might not be representative of all possible datasets in hate speech detection. we also acknowledge that, in addition to the languages examined in this paper, there are a number of other languages that may present unique challenges and characteristics for detecting hate speech. our decision as to which languages to include in the multilingual experiment was based on a direct comparison with state-of-the-art research. finally, we utilized the latest open-source language models for our experiments, but we did not explore other recent language models, such as the gpt family, primarily because they are not open and reasonably reproducible3, and therefore the community may encounter challenges in replicating our results. these limitations provide directions for future research to improve and expand upon our work."
10,"limitations of the use of dalc-v2.0 to detect hate speech. while its abusive dimension can be considered a good proxy, all fine-tuned models systematically fails on core non-hateful functional tests, indicating limitations in the annotated data. future work will focus on extending dalcv2.0 with multiple hate speech datasets and further 2these correspond to f18–19, f21–22 in mhc. validate the functionalities of hatechek-nl. ethical statement limitations hatecheck-nl is based on mhc and it inherits its limits. however, as we have discussed in section 2, we failed to fully implement some functional tests (e.g., reappropriation of slurs) because we were not able to find evidence during our research. to address these limitations, we plan to conduct focused interviews with dutch organizations such as the black archives3. intended use hatechek-nlis a diagnostic tool for hate speech against specific protected groups. we have shown its functionalities and its impact on the evaluation of models trained both on a different language phenomenon, e.g., offensive language, and on related and comparable one, e.g., abusive language. the results have shown critical weaknesses mainly on the non-hateful tests rather than showing the strengths of the systems/models on the hateful examples. similarly, op-nl is a dynamic test for offensive language whose use is to help assessing the robustness and portability of models trained for offensive language detection. goodness of data dalc-v2.0 is the only publicly available resource for investigating the behavior of models on offensive and abusive language phenomena in dutch. none of the annotated dimensions in dalc-v2.0 explicitly address hate speech as we discussed in section 2. the results of the fine-tuned models on hatechek-nl for the abusive language dimension indicate a compatibility between abusive language in dalc-v2.0 and hate speech. the use of offensive training data on hatechek-nl better highlights the limitations of the data, especially as pointed out by the systematic failure on the functions f23–24. at the same time, the results on op-nl for offensive language show a relatively good portability of the models for this language phenomenon."
11,"limitations graph models require four or more utterances to form meaningful conversation connections and model their dynamics. in some cases, conversations that derail are not sufficiently long and may be best modeled by simpler sequential models. any of these models will work best with asynchronous conversations where there is a time lag between the turns to allow for moderation after forecasting."
12,"limitations in the available resources and the approaches of nlp researchers towards constructing them. we summarise these and make future recommendations. conceptualisation with a couple of exceptions (e.g. samory et al., 2021; strathern and pfeffer, 2022), the phenomena targeted in the reviewed resources are not clearly defined or strongly rooted in theory or expertise from outside computer science. similar observations have been made for operationalisation of related concepts, such as bias and stereotypes (blodgett et al., 2021), and value alignment (irving and askell, 2019). recommendation: resource creators should collaborate with social scientists to ground them in expert knowledge of the target phenomena. we advocate for the use of gbv as a framework, which encompasses several facets currently operationalised in different ways by computer science researchers. it recognises how all forms of online abuse affect people of every gender both online and off, and has been widely adopted by policymakers. stakeholder participation parker and ruths (2023) propose that computer scientists should: stop thinking about online hate speech as something requiring methods, and start thinking about it as something that demands solutions. this change — treating hate speech less like a task and more like the real-world problem it is — would orient cs research towards the concerns of other stakeholders, and thus begin the collaborative pursuit toward a safe internet. however, we find little evidence of such a paradigm shift having occurred when it comes to designing these resources, with stakeholder participation limited to the recruitment of loosely defined ‘expert’ annotators—where it occurs at all. recommendations: resource development projects should, as far as possible, strive to include stakeholders from the outset by including representatives in research teams. stakeholder participation should be integrated throughout development, and is especially important in the design of taxonomies, guidelines, and at annotation, when judgements about what constitutes gbv are made. due to the risks involved, annotator welfare should be prioritised by following guidelines such as those of kirk et al. (2022), and irb approval sought before any data collection. in documenting resources, authors should provide full data statements or similar (e.g. bender and friedman, 2018; díaz et al., 2022), and, to preserve minority voices, dataset releases should includenon-aggregated labels (prabhakaran et al., 2021). data collection media data for these resources is not sourced from diverse sources, with the majority from twitter, the choice of which does not appear to be driven by stakeholders. furthermore, as the datasets are static in nature, their relevance as reference sources for automated classification decays over time; and, due to data sampling methods, positively labelled (i.e. abusive) examples are skewed towards the more explicit forms of online gbv. recommendation: there is a great need for the development of new methods to surface the diversity of gbv found online. one solution is to create platforms to which victims of abuse and bystanders can submit examples. this could facilitate creation of improved resources on many of the limiting dimensions we outline in this review: dynamic datasets to which new examples are regularly added; stakeholder participation in data and platform selection and labelling; and inclusion of implicit and subtle examples of gbv, as well as multimedia data. limitations and ethical considerations we use a systematic review methodology in order to provide a reproducible and objective snapshot of the current research situation. however, we acknowledge that the choices made (such as search repositories and eligibility criteria) may not have captured every existing relevant resource. we aim to regularly update the repository of gbv resources at https://github.com/ hwu-nlp/gbv-resources and open it to submissions via push requests in order to provide a dynamic and comprehensive record. following d’ignazio and klein (2020), we acknowledge that this research is influenced by the positionalities of its authors. to situate our perspective, we are four computer science and one social science academic researchers working in public institutions in europe. three of us identify as women and two as men, and we are of european and asian nationalities. this work forms part of a project conducted in partnership with charitable organisations that work on combating gbv and supporting its victims. in this paper, we make a number of recommendations that complicate typical nlp resource creation workflows, and could have the unintended consequence of dissuading researchers from working on these problems. however, we appreciate that interdisciplinary work is difficult to instigate, organise, and carry out, and that it is not usually motivated by typical academic or industry reward structures. our intention is to point out practical ways in which resource development can be improved and to encourage researchers to move towards more participatory solutions."
