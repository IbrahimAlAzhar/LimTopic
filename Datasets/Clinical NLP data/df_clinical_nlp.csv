,Text
0,"Since hypothesis sentences were created and labeled by medical experts, the size of our current dataset is small. In particular, the number of examples of contradiction is small because the hypothesis sentences were created based on captions to efficiently construct our dataset. However, we can increase the number of examples of contradiction by rewriting phrases in the hypothesis sentences. The claim of this study is that we can relatively efficiently create a VTE dataset in the medical domain from the existing image caption dataset, and can empirically demonstrate the challenges of current vision-and-language models on the VTE dataset. Although increasing the data size is an important next step, it is beyond the scope of this paper."
1,"In our study, we tested the chatbot on the Hindi database, which humans heavily annotated. Thus, when the database size is enormous, the scalabil-
ity of the annotation approach is a critical question. Since the questions and answers could be possible in different languages, it will require considerable effort to translate them and, at the same time, preserve their context. In our study, we observed the success ratio of the developed chatbot to be 70% for Hindi queries. However, it is not indicative of its performance in different natural languages. For a given user query (q), the performance of our best approach for the FAQ-retrieval system is highly dependent on the number of different relevant questions (Q) existing in our ASHAFAQ database for the given q. Considering the large number of user queries that can be asked in the healthcare field, the small size of our ASHAFAQ database is a significant reason behind the instances where our method fails to suggest relevant questions (Q) to the user. Moreover, our work does not analyze the quality of answers present in the ASHA-FAQ database. Hence, a user study would be required to analyze the questions’ diversity and the answers’ quality in our ASHA-FAQ database."
2,"The limitation of this work was the use of ROUGE-L as the evaluation metric. Given the many acronyms and synonyms in medical writing, ROUGE-L, based on the longest common sequence, does not capture the many nuances in its score. Researchers have shown concerns for the ROUGE score and have developed metrics for summarization that are more semantically aware of the ground truth (Akter et al., 2022), but their usability is yet to be validated.
Training large language models from scratch uses a considerable amount of carbon footprint. (Patterson et al., 2021) Fine-tuning large language models for downstream tasks is one way to reduce carbon footprint but still needs to be cost-effective. As the AI community progresses in this field, developing a cost-effective and carbon-friendly solution is needed. The NLP field is moving towards prompt-based methods with larger LLMs (Lester et al., 2021), so the next step for this research is to experiment with soft prompting approaches to address low resource settings and leverage prompt tuning in LLMs for the problem summarization task."
3,"A limitation in this study is we only used opensource models. We were unable to evaluate ChatGPT, for example, because the data use agreements under which these datasets are made available forbid sending the data to outside APIs. Other models are frequently being released and we did not exhaustively test all publicly available language models. However, the focus of the paper is not to find the best LLMs but instead providing insights into using LLMs to improve transferability."
4,"There are a few limitations of this study: First, the patient sample size for the validation cohorts was limited to 954 patients from a local hospital. As annotation in the medical setting is expensive and time consuming, we only get patient level labels and cannot pay the effort for document level annotations. The size and diversity of the data sample could be improved by collecting clinical notes for patients from other hospitals in different age groups and of similar clinical complexity. We did not perform cross label check for the sampled patients, as there is a large number of uncertain patients, among those patients there are still ones who suffer from dementia but not diagnosed. Second, more statistical models can bee developed. At the moment we only tried a keyword based model and a deep neural models. Traditional statistical models like Logistic Regression with biomedical concept features can also be considered. Furthermore, our study would have benefited from more model interpretability and human error analysis on the classifier predictions. We have plans to extend our current work with the above mentioned directions."
5,"We finetuned pre-trained NLI models for TDG parsing. Both data sets we used were in English. To apply this model to other languages and to get the best results, pre-trained NLI models or NLI data sets might be required for the new language. Templates to verbalize the temporal relations in the new language are also required.
The clinical data set (i.e. THYME) we used in this work only contains EHRs from one institution: Mayo Clinic. Clinicians from different hospitals can have different writing style or use different templates when writing the notes. Future work should test the TDG representation and parsers on EHRs from other institutions, and EHRs of different patient populations."
6,"The experiments in this paper were performed using OpenAI’s GPT-3 API. While running locally does not require a large amount of computational resources, the server-side service cannot be easily replicated and requires a large amount of computational resources. Additionally, given the inherently restrictive nature of medical text, we can only evaluate our approach on a small corpus of English-language dialogues taken from the dataset of a single company’s medical service, which we cannot release due to privacy concerns. Finally, given summarization is a challenging task to evaluate, we rely on a small number of expert human annotators and automatic metrics. However, additional annotations may be helpful and it may also help to study and report labeler agreement when reporting human preferences."
7,"The methods described in this paper do not leverage any external medical knowledge, a technique that has been shown to be effective by other studies (Joshi et al., 2020; Michalopoulos et al., 2022). And like other methods based on large language models, in theory, our models are also prone to hallucinations and omission of key-clinical concepts. We plan to explore constrained beam search4 as a mitigation strategy for addressing these challenges in the future.
Although the impact of the Task C model as a data augmentation tool is undoubtedly positive (Section 5), qualitative error analysis of patientdoctor conversations produced by the model showed that the output contained a small number of dialogue turns, and each individual turn was too long, packed with information. Producing conversations with a more natural flow should yield an even better boost on downstream tasks, and we leave exploring such methods to future experimentation. We also recognize that N-pass summarization for Task B with higher values of N should be able to cover the entirety of the input conversations in the Task B datasets, albeit with diminishing returns as N increases. We hope to evaluate them in future iterations of similar shared tasks."
8,"Data. A challenge we faced was that MIMICIII was unevenly distributed across the races (e.g. ethnicities) for the patients represented. We had significantly more White and Black patients than any other race of people and even still many more White than Black patients. Therefore we continue to express the need for more representative, inclusive, and balanced datasets. Further, the dataset did include ethnic breakdowns, but due to the lack of patients present in those ethnic groups we could
not include Caribbean or Middle Eastern patients as well as many other subgroups in our analysis. We would like to use a more comprehensive dataset in the future, potentially from a facility that consistently services marginalized and privileged communities. If we had more time, we would like to partner with a medical facility that regularly serves marginalized and non-marginalized groups, steadily, to develop a dataset which captures more features that could reveal some bias and ensure they are more descriptive (i.e. has_insurance) to get higher quality data. Better Feature Selection and Using More Demographic Features. To ensure the quality of the aforementioned data, we will perform a causal analysis to identify the specific features that cause testimonial injustice. We anticipate that variables such as age and education level of patients need be included, as these factors have been shown to affect how patients are treated, particularly in the medical field (Dunsch et al., 2018; DeVoe et al., 2009). Fairness Metrics. Existing and popular, fairness metrics cannot be generalized to fit in settings where intersectionality must be considered. Another challenge we faced was having a lack of good baselines to use when analyzing intersectional differences. Intersectionality is highly unexplored, in the future we would like to develop our own metric which can be more beneficial in detecting intersectional disparate treatment between individuals. Additional Analysis. We plan to conduct additional analysis to understand if specific physicians treat similar patients similarly based on the intersection of their demographic features. Further, we plan to perform statistical significance testing on differences in how patients were treated based on the intersection of their demographic features and the occurrences of specific physicians’ use of testimonial unjust terms to other patients."
9,"One drawback of our method is that given a maximum span length, we always miss longer spans. For example, the BC2GM and JNLPBA NER
datasets contain lengthy spans so we do not do as well on those tasks. Another drawback of our method is that it requires embedding the full corpus. One of our methods for making this tractable introduces another limitation - span filtering based on token types may discard spans that are useful to the user. Additionally, although we demonstrate that our method can be robust to training time (A.1), we have not explored principled methods for selecting the model checkpoint in supervised KRISS-Search, as the user does not label a validation set. Methods for making the process more rigorous should be explored, especially for out of distribution tasks."
10,"We only explored one-shot prompting strategies with GPT-4. More examples (few-shot) may improve performance. We prompted GPT-4 with only a single randomly selected sample that included all of the annotated event types. Our post-processing included simple rules to process the generated output and may be improved. The quality of the sample and the selection method may influence performance. We explored two prompting styles. Future work could explore more prompting methods such as question & answering and chain-of-thought (Wei et al., 2023) and fine-tuning non-proprietary LLMs."
11,"Some limitations of this work are listed below:
• The architecture and configuration for the custom PLM are the same as bsc-bio-ehr-es. Another architecture and configuration could obtain better results.
• The textual data come from just one provider. Using data from several providers could help with generalization.
• The custom PLM has not been compared with other PLMs in language tasks such as named entity recognition or question answering. This
comparison can help to understand if the custom PLM can outperform available PLMs in other types of tasks.
Ethics Statement
The ethical considerations of this work are related to the data that we used and the models we built. The data was extracted from administrative and clinical records from an insurance and health provider that specialized in labor accidents. Within this data, it is possible to find personal and sensitive information such as personal and company names, addresses, health information, pre-existing conditions, and diagnoses, among others. An anonymization process was not carried out since the model will be used for internal purposes and will not be released. As a process of memorization can occur in the PLM, we believe it is best to keep the model private because privacy attacks can extract personal and sensitive information.
We did not test the models for any bias under any protected field. Therefore, the trained models could benefit certain patients or accidents over others in the insurance decision. If a biased model is deployed in this provider’s systems, it could harm patients with their insurance coverage decisions."
12,"Although our method delivers optimal results, it doesn’t comply with data protection regulations
like HIPAA, even though Azure offers a HIPAAcompliant option. From a privacy standpoint, deploying a local model such as LED might be preferable, but our findings indicate that further work is needed for this method to achieve satisfactory performance. Regardless, when creating automated conversation-generation systems, healthcare providers and developers must ensure that the entire system—including text-to-dialogue, data transmission and storage, and model inference—complies with privacy and security standards to maintain trust and avoid privacy breaches in clinical environments. Hence, developing an automated conversation generation system from clinical note entails several ethical considerations.
Firstly, obtaining informed consent is vital: patients should be informed about their recordings, and data ownership must be emphasized. Equitable access is also crucial; the system must accommodate patients from various backgrounds, including those with disabilities, limited technical literacy, or language barriers. Lastly, continuous evaluations are required to ensure that the system’s performance does not deteriorate and adversely affect the quality of care."
13,"Considering the critical nature of the domain of the task, it is of paramount importance to ensure stability in the results expected from the model. Despite setting the temperature (T) as 0 for all decodings in our experiments, we observe the variance in the generated summaries across runs. Table 4 contains the results for three runs for Task A and Task B. The in-context examples for each sample and other parameters have been kept constant across these runs to identify the degree of stochasticity. Further, in-context learning has shown to be susceptible to changes in order of in-context examples (Lu et al., 2021), as well as the template of the examples (Shin et al., 2020). A more reliable process to generate the summaries along with identification of the optimal examples (template, order) is thus required. Additionally, due to the context limit of the GPT-4 model, evaluating the impact of natural language instructions in addition to the examples could not be performed."
14,"limitations some potential limitations of this work include a relatively small sample size, which may limit the generalizability of the results. hindi is spoken differently across india, hence the translations made by the three translators may not be representative. study did not examine the potential impact of regional dialects or variations in hindi, on the accuracy of the diagnosis. finally, the study focused solely on the use of speech narratives and did not explore the other types of data, e.g., imaging or genetic data, which could be important for the diagnosis of ad."
15,"limitations 1) on plm capability for transferring to new language, in this work, we used metaai’s wmt21 multilingual pre-trained language models as our test-base for the knowledge transfer into an external language fine-tuning and translation. this new-language ability is much dependent on the mplms we used, such as wmt21fb (tran et al., 2021) as a huge size model, a conditional generation from meta-ai’s massive m2m-100 model (fan et al., 2021). if we try to fine-tune a bilingual model on an external language that the plm did not see, it will not be that good because for smaller-sized models such fine-tuning would be too much of a change, and the model will lose generalisation which leads to problems. for huge multilingual plm models, the 250k of fine-tuning data is a small set of numbers, and that’s why the model does not lose generalisation and captures new data well without losing linguistic knowledge of other languages that it was trained on. 2) on the impact of language families, the mmplm wmt21fb we deployed has both alphabetic languages and cjk (chinese, japanese, koran) character languages, as well as slavic language (russian). this might make it easier to transfer to a new language, e.g. alphabetic language. however, in situations when the mplms did not include any of the language scripts that belong to the language family of the target one, it can be much harder for it to transfer to the new target language. this needs further investigation. one possible extension of this work is using the dynamic vocabulary method proposed by lakew et al. (2018)."
16,"limitations. we employed human evaluation for assessing factual consistency, and this analysis has been conducted over a larger set of automatic metrics to provide a more comprehensive picture. furthermore, we demonstrate that further optimizing the model using reinforcement learn- ing (rl) with the metric as a reward can result in significant improvements in factual consistency. our contributions include a simple yet effective approach for two medical summarization tasks, validation of several automatic evaluation metrics for their correlation with expert-assessed factualness, and the identification of the best-correlating metric to guide generation models toward enhanced summary correctness. this work lays the foundation for the development of more robust clinical trial summarization systems, facilitating the efficient dissemination of medical knowledge to practitioners and researchers."
17,"limitations our work does not cover the full range of domainagnostic pretraining objectives, including denoising objectives such as electra (clark et al. (2020)), or contrastive objectives, such as simcse (gao et al., 2021; rethmeier and augenstein, 2023). this paper focused on comparing the masked language modeling (mlm) objective with specially designed dialog-aware objectives. it is our expectation that, given the empirical findings of this project, task-agnostic general objectives like electra, or simcse, will also outperform dialog-aware methods. in addition, due to the lack of task-related datasets, the set of corpora used during our experiments is limited."
18,"limitations to consider. firstly, minimal prompt engineering was used, and context could have been provided in the form of few-shot or chainof-thought examples, which have been shown to increase accuracy (wang et al., 2022; ye et al., 2023). strategies like self-consistency decoding (huang et al., 2022) and retrieval augmentation are also promising for healthcare where varying factual content of responses from each model even to the same prompt poses a clinical risk. additionally, we did not compare the llm responses to those of human experts, which is an important comparison for appropriateness and safety."
19,limitations we did not have the chance to explore latest large models. the obvious future work would be on applying public instruct based models if the hardware capacity is enough or private instruct based models if the budget allows. more future work could be on preprocessing of the dialogues. intuitive postprocessing approaches could also be explored.
20,"limitations our findings are limited to medication entities, the only semantic class that is annotated in all available corpora. moreover, we had to exclude bronco150 for long-span experiments due to a mismatch of entity definitions. although the label alignment decisions are somewhat subjective, they are made based on a thorough inspection of definitions and samples. the differences in annotation quality and biases may be playing an uncertain role in the models. however, making statements on the impact of the annotation quality is challenging, since each work followed a different annotation protocol and reports different measures of annotator agreement. this is another area where harmonization efforts might be warranted for future research. furthermore, exploring different hyperparameter configurations lied out of scope for our work, but could have a substantial impact. mainly, the results from the transformers comparison (table 6) could shed different"
21,"limitation of this work is the small speech database used in this paper. we are actively building a detailed annotated large speech and language database from hundreds of patients with post-stroke aphasia, with the aim of training and developing asr for pathological speech. we expect that such work will promote greater confidence in the use of ai and specifically nlp for healthcare intervention."
22,"limitations this method for designing and annotating clinical text for a specific clinical use case can be beneficial for researchers needing to annotate a corpus. however, there are some limitations. first, the experiences are based on a specific clinical case and focus on the qualitative aspects. details of certain parts of the design and annotation process will likely need to be adjusted based on resources available to other researchers. this can include the data selected for annotation, the number of annotators available, and the annotators’ level of expertise. for instance, the use case in the design process is based on using 8 annotators to annotate 100 synthetic ae notes over 5 sessions. second, expertise and additional time are required to generate synthetic notes for annotation. finally, future work is still needed to replicate the described design and annotation process on other forms of clinical text and problems. ethical considerations to protect patient privacy when designing and annotating clinical text, synthetic ae notes were manually generated and verified by a nurse to ensure the data is anonymized. additionally, the annotation guideline includes the sensitivity category to allow annotators to label potential information in the synthetic notes that could identify a patient. this process was described to provide an example for researchers who need to annotate sensitive data. the norwegian regional committees for medical and health research"
23,"limitations biodlm adopts dlms as backbone models. compared to plms with other training objectives, dlms may miss language modeling benefits and squeeze representation space. besides, our benchmarks can be extended to more biomedical discriminative tasks, such as relation extraction, document classification, and entity disambiguation. we consider extending our exploration to more dlms and biomedical tasks as valuable future works."
24,"limitations due to strict data protection regulations and a high annotation workload in the clinical domain, obtaining more diverse target tasks to validate our approach is a challenge. in this work, we focused on only two use cases in german clinical applications and need to extend our experiments to english or other non-english languages in the field. in addition, we need to conduct more experiments in future work in order to achieve a better balance between the amount of training data required for the source and target tasks."
25,"limitations while the proposed methods offer promising results in improving the performance of knowledge graph-based language representation models, there are some limitations to this work that should be noted. firstly, the experiments were conducted on a limited number of datasets, and the results may not be generalized to other datasets or domains. therefore, further experiments are needed to validate the effectiveness of the proposed methods on a broader range of datasets and nlp tasks. secondly, the proposed methods require additional computation and may increase the complexity of the models. therefore, it is important to consider the trade-off between performance improvement and computational cost when applying these methods in real-world applications. lastly, while the proposed methods address some of the limitations of existing knowledge graph-based language representation models, they still may not capture all the contextual relationships and nuances of natural language, leading to potential semantic errors and reduced accuracy. therefore, it is essential to continue exploring new approaches and techniques to further improve the performance of nlp models."
26,"limitations this paper is limited to the study of clinical ner models using an encoder-only architecture. the use of generative models with a zero-shot learning approach (hu et al., 2023) is another promising approach for low-resource languages that could be compared with clt and translation-based approaches in a future work. however such methods require a careful prompt selection strategy and cannot be directly compared to supervised models. this paper is also limited to cross-lingual transfer to french and german. ideally, this work could have included experiments with other target lan- guages and also other source languages than english, as yarmohammadi et al. (2021) do in their general-domain comparison of strategies for crosslingual transfer. however evaluation datasets are lacking for that purpose in the clinical domain. similarly, more general"
27,"limitations the work presented in this paper is subject to a number of limitations which will be addressed in future work. firstly, we evaluate umls-kgi-bert on a very narrow range of tasks limited to token classification - a broader range of information extraction and reasoning tasks would be necessary for a more complete picture of the utility of our pretraining methods. in addition, we only train models for mid-to-high-resource languages; to properly validate the applicability of this approach, in particular the lessening of the need to rely on large training corpora, it will be necessary to train and evaluate such models in more low-resource settings. table 6: macro-f1 scores for the ablation experiments. dataset base model kg tasks cas-pos cas-sg quaero-medline essai-pos drbert-4gb - 90.84 62.20 66.66 94.69 ep+lp 91.59 64.85 66.08 94.62 ep+tc 90.86 62.11 66.75 94.88 tc+lp 92.01 65.98 66.89 94.41 all 92.04 66.22 67.15 94.50 ncbi-disease biored-ner jnlpba04 pubmedbert - 93.53 83.35 81.13 ep+lp 93.24 82.40 81.25 ep+tc 93.37 83.09 82.66 tc+lp 94.13 83.38 84.30 all 94.11 83.45 84.36 pharmaconer meddocan bioroberta-es - 81.11 91.84 ep+lp 81.12 91.86 ep+tc 82.40 91.80 tc+lp 83.22 91.71 all 83.46 91.77"
28,"limitations evaluation of generated text is difficult evaluating automatically generated text, including clinical notes, is generally hard due to the inherently subjective nature of many aspects of output quality. automatic evaluation metrics such as rouge and bertscore are imperfect (deutsch et al., 2022) and may not correlate with aspects of expert judgment. however, they are frequently used to evaluate model-generated clinical notes and do correlate with certain aspects of quality (moramarco et al., 2022). to further validate our findings, we also conducted a human evaluation with three expert physicians (§4.3). as noted previously (savkov et al., 2022), even human evaluation of clinical notes is far from perfect; inter-annotator agreement is generally low, likely because physicians have differing opinions on the importance of each patient statement and whether it should be included in a consultation note. we also found low interannotator agreement in our human evaluation and speculate this is partially due to differences in specialties among the physicians. physicians 1 and 3, both from family medicine, had high agreement with each other but low agreement with physician 2 (cardiac surgery, see table 3). investigating better automatic metrics and best practices for evaluating clinical notes (and generated text more broadly) is an active field of research. we hope to integrate novel and performant metrics in future work. data privacy while our gpt-4 based solution achieves the best performance, it is not compliant with data protection regulations such as hipaa; although azure does advertise a hipaa-compliant option.13 from a privacy perspective, locally deploying a model such as led may be preferred; however, our results suggest that more work is needed for this approach to reach acceptable performance (see table 3). in either case, when implementing automated clinical note-generation systems, healthcare providers and developers should ensure that the whole system — including text-tospeech, data transmission & storage, and model inference — adheres to privacy and security requirements to maintain trust and prevent privacy violations in the clinical setting."
29,"limitations the use of machine learning models in clinical decision-making requires an understanding of the reasoning behind model predictions. our study focuses on improving the performance of smaller models using context and subtask blocks. while the subtask state labels provide some interpretability, we have not explored its impact on trust among medical professionals. in addition, the relative benefit of the different multi-pass strategies and different types of context appear to depend on the degree of domain mismatch, which should be further explored in future work."
30,"limitations in the experiments conducted by the team, several limitations were observed. firstly, the handling of the semi-structured structure was not accomplished perfectly. when generating the entire content at once, many sections did not appear, and when creating sections separately, the content of the notes was repetitive. additionally, addressing the clinical domain properly was not achieved."
31,"limitations in this work, we adopt three gcns to exploit the inter-code relations under different levels. however, this may bring extra training difficulty and the risk of over-parameterization to the model. besides, during the preprocessing stage, we adopt a wordlevel tokenizer and cbow to obtain their embeddings for mimic-iii texts and code descriptions. however, this might not be enough to represent the words since medical documents have some special characteristics, but we do not take them into consideration. we tried in our work with other pretraining strategies, such as clinicalbert (alsentzer et al., 2019), bert (devlin et al., 2019), biobert (lee et al., 2020) and biowordvec (zhang et al., 2019). we added as well the bpe tokenizer (sennrich et al., 2016) in order to capture the meaningful medical sub-word units. however, the results are all far from satisfactory."
32,"limitations and boundary conditions of our models in order to make more accurate, informed, and measured claims about their clinical outcomes. for each patient transcript, we generate 9 versions of the original text that amplify types of language errors in both semantic and syntactic categories. table 2 compares the aphasia and dysarthria models’ performance on all categories of errorinfused transcripts, with f1 as the evaluation metric."
33,"limitations due to clinical and financial constraints, both the patient and the mturk sample sizes of our study are still relatively small. this means that we cannot afford to set aside patient data as a hold-out test set, and have to use the validation set for model evaluation. as we work towards enrolling more patients and recruiting more healthy volunteers to 429 improve model generalizability, we hope to expand the scope of our pipeline beyond english to serve non-native speaker patients. one major limitation of the cookie theft picture description task is its lack of equitable assessment for an increasingly diverse patient population. steinberg et al. (2022) identify gender as a particularly fraught aspect of the picture’s expected response, as the rubrics of the initial nihss were established from a male-only corpus. although there is no alternative picture or stroke patient corpus available to our study, we try to ensure the equity of our models by maintaining a gender balance in our patient set, with 136 female patients and 132 male patients. on our patient-only evaluation set, our aphasia model performs significantly better on female patients (auc=0.909) compared to male patients (auc=0.702), while the dysarthria model exhibits better performance on male patients (auc=0.778) than female patients (auc=0.719). at present, we are unable to draw any definitive"
34,"limitations in this work, we studied only the evaluation of textual medical report generation from both automatic and human evaluation perspectives, but we did not study the evaluation of data-to-text medical report generation, which has its own specificities."
35,limitations the paper does not cover all types of possible methods and models for the generation of clinical notes. the challenge datasets are also limited in terms of size and medical specialities. further experiments and evaluations are needed to validate the best performing methods on other datasets and scenarios.
36,"limitations generating clinical notes or summaries of clinical conversations using nlp technology is a rapidly developing field with great potential. however, there are several limitations to this technology that must be considered. firstly, nlp models rely on high-quality data to achieve accurate results. in the medical field, obtaining such data can be challenging due to privacy concerns and regulations. secondly, the complex and technical nature of medical language poses a challenge to nlp models, which may struggle to understand and interpret medical terminology and abbreviations accurately. additionally, clinical conversations often involve sensitive information that requires careful handling, making it important to ensure the security and privacy of generated clinical notes. this field is considered a safety critical area, where high precision is expected, therefore, the use of nlp models in such clinical settings must be performed with caution and under medical professionals’ supervision to ensure the generated notes’ accuracy and reliability."
37,"limitations identified in this study. for classification, we will experiment with model configurations and explore alternative machine learning algorithms. for summarization, we will refine prompt strategies, incorporate domain-specific knowledge, and investigate various fine-tuning techniques. lastly, conducting user studies with medical professionals will provide valuable feedback to assess the utility and accuracy of our generated summaries in real-world clinical settings and further refine our approach."
38,"limitation of subtask a one significant limitation of our transformer-based model is that it does not directly consider length during its generation process. this often results in the production of overly verbose summaries. in contrast, models like openai’s gpt-3 and gpt-4 have much longer input and output limitations, allowing them to handle more extensive text samples more effectively. it is worth noting that our model was trained on the samsum dataset, which has longer texts compared to subtask a. consequently, our model struggles to adapt to the shorter length requirements of subtask a. moreover, the training dataset for subtask a is relatively small, which further complicates the model’s adaptation. future exploration should look at how to constrain the conciseness of generated summaries, which may involve reconsidering the generation method chosen or examining other techniques to promote brevity. developing methods to better control the length of generated summaries is essential to improve their relevance, coherence, and usability in real-world applications. length limitation of subtask b for subtask b, it is challenging to achieve reasonable results using fine-tuned models. in reality, this task is more representative of real-world scenarios, where inputs and outputs are considerably long, and the output is expected to maintain a specific structure and format. thus, we see the main advantage of using contextual examples lies in their ability to guide the structure, style, and length of the desired output. we believe that openai’s llm is wellsuited for similar real-life scenarios, provided that it is given an appropriate context. in such cases, its performance will significantly surpass that of fine-tuned transformer-based models. factual inconsistentcy while our study did not specifically investigate the following issues, we noted several factual errors that occur in summaries. a previous study has shown that llms also exhibit a noticeable occurrence of attribute errors and misinterpretation errors (tang et al., 2023a). prompting we find that prompt template and demonstration example selection both have a substantial impact on results. using more prompt examples for demonstration improves significantly. we acknowledge that we did not explore different selection strategies, such as semscore, lmscore, and tlength, which involve using top-ranked examples. these strategies have been shown to potentially improve the quality of the generated summaries by selecting more effective prompt examples. while our current approach did not incorporate these strategies, we recognize that exploring and incorporating better prompt examples could potentially yield improved results. this is an area that warrants further investigation and experimentation to enhance the performance of our models in future iterations of the study. data privacy both gpt-3 and gpt-4 are not local models; we utilize openai’s api to run these models, which actually violates data protection laws such as hipaa. ensuring data privacy during fine-tuning or testing is of paramount importance. we have not taken this aspect into consideration."
