,Text
0,
1,Left blank.
2,"The performances except for the proposed tasks. We presented the result of neologism and the performances on two downstream tasks (i.e., word similarity task and short text classification), which are closely related to the understanding of word semantics. The selected downstream tasks are challenging for the contextualized models; they can use only a few contexts to make a representation.
The performance in general benchmarks (e.g., GLUE) is almost the same as the vanilla BERT because our model suffers catastrophic forgetting while learning definition information. Sophisticated modeling and training processes to overcome the problem could be interesting future work.
The use of other models Other pretrained models like RoBERTa could be a base model of our method (e.g., DefRoBERTa). However, we think that BBPE tokens scarcely have semantic meanings, which makes it hard to find appropriate tokens to inject definition information. Therefore, integrating human-written definitions with other types of tokens (e.g., Byte-Pair Encoding and Byte-level BPE) is also a future direction.
The use of all the loss function& Collect more definition data. Presenting more experiments with other models, other collections of definition data, and other loss functions will further support our idea. Nevertheless, we want to show the performances with the widely-used basic model of pretrained language models (i.e., BERT), using definition data from the previous work, with various loss functions (e.g., W-D, D-E, [+W’], [+E]) as many as possible. A fine-grained combination of all the loss functions could make further improvements."
3,"There are several perspectives from which we need to consider the ethical considerations of this work.
Privacy: Lifelogs are personal data and should only be used and shared given user authorization. The lifelogs presented here are fictitious and do not reveal the personal information of any individual. No personal data is used to create this benchmark. This work is intended to unlock development in the creation, maintenance, querying and usage of lifelogs, and additional work will certainly be needed to ensure that they are secure and being meaningfully and responsibly used.
Comprehensiveness and diversity: We recognize that the lifelogs generated in this work are far from representing the full range of human experiences. While we strived to make the lifelogs complex enough to benchmark and compare current stateof-the-art, these lifelogs would not be considered diverse in the sense that a social scientist would note, and are likely biased by the life experiences of its creators. We encourage future work in creating lifelogs that are more inclusive and faithful to all walks of life. This includes further work in making lifelogs that are more diverse in terms of life experiences, personas, time scales, and queries as well as more granular and complex in detail. The strength of the benchmark is in identifying patterns of questions on lifelogs rather than the specific events described in them.
Inferring episodes: TimelineQA is a collection of time-and-space boxed episodes, and not the raw data itself from which the episodes are inferred (e.g., a wedding photo, or video snippet from smart glasses). Naturally, more research would need to be devoted to understanding how to extract important information in natural language and infer episodic events from this raw data before performing question answering. As mentioned previously, this also involves sometimes grappling with the linguistic variation amongst the language used in the episode description and the query itself.
Intended use: We clarify that the benchmark should not be used to train models for making key decisions that will impact people’s lives (e.g., job matching, insurance approvals or building personal assistants). The intended use of TimelineQA is as a benchmark to reveal potential limitations of QA systems over lifelog data. Even if the benchmark is determined to be sufficiently comprehen-
sive, a detailed study should be conducted to understand the potential representational harms of using TimelineQA before using it for training models. Conceivably, TimelineQA can also facilitate research in evaluating the biases of QA systems by creating counterfactual pairs in the dataset: two timelines which are exactly the same, but differ by the demographic group or a specific life event (e.g., having dropped out of college or committed a crime). The QA system can then be systematically probed for differences in performance between the two timelines."
4,"Our approach to universal text perturbations suffers from linguistic inconsistency, which makes them easier to detect. Therefore, as the next step of our research, it would be interesting to investigate
the possibility of improving the naturalness of adversarial triggers without degradation of the attack performance in terms of the fooling rate.
While the proposed approach outperforms the UATs of Wallace et al. (2019) in the transferability task, we should highlight that the additional hyperparameters adjustment plays a crucial role, and one could suggest validation procedure refinement for a more fair comparison. Also, for both direct and transferability settings, a more comprehensive range of models should be examined, including recurrent (Yuan et al., 2021) and transformer architectures, e.g., T5 (Raffel et al., 2020), XLNet (Yang et al., 2019), GPT family models (Radford et al., 2019; Brown et al., 2020).
Another direction of improvement is related to the fact that sometimes the found triggers can change the ground truth label of samples they are concatenated to if, e.g., they contain words contradicting the true sense of a sentence. It would be interesting to analyze how often this happens and develop an approach to tackle this issue.
Finally, it would be interesting to investigate the dependence of attack efficiency on the size of a training set and compare it with the so-called data-free approaches, such as the one proposed by Singla et al. (2022)."
5,"In this work, we first formulate the scene-robust NLVL problem and propose our solution. However, our generalizable NLVL model is still tested on existing close-world datasets, and the actual performance in real-world scenarios needs to be further explored. A real-world, large-scale dataset is required to develop a practical, generalized, openworld query-based video retrieval model."
6,In the Sec 6 and Appendix.A
7,"Time Granularity: The granularity of time we test DynaMiTE on ranges from spans of four years to months. After testing multiple ways to bucket our temporal corpora, we observed that the granularity of time only affected DynaMiTE when there were insufficient documents in each time step. Specifically, we found that there must be at least 100 documents per time step to expect reasonably good results. Runtime: One drawback of DynaMiTE is that its runtime depends on the number of terms required at each time step. However, this can be avoided by mining more than one term during each iteration of the framework. We also observed that DynaMiTE, along with all other dynamic topic mining baselines, had a slower performance on datasets with longer text documents. Risks: DynaMiTE is intended to be used as a tool to discover topic evolutions in temporal corpora suited to a user’s interests, represented as category seeds. We only experimented with DynaMiTE in domains with trustworthy information. If DynaMiTE was used in document collections that contain misinformation, it could have the potential to mine inaccurate terms."
8,Section 8
9,"There are majorly two limitations: Firstly, we collect a Chinese singing voice dataset and test our method only on this Chinese dataset due to the difficulty of recruiting professional singers in different languages. In the future, we will attempt to collect the singing voices dataset including more languages and test our method in multilingual settings. Secondly, our method adopts the diffusion model in pitch modeling and the postnet, which require multiple inference steps. We will try advanced acceleration methods for diffusion models in the future."
10,"The present study is limited to exploring biases in MLMs for the gender dimension only. For future work, important dimensionalities can be explored, especially for non-western contexts like Caste, Ethnicity, etc (Ahn and Oh, 2021; Bhatt et al., 2022). We also used Machine Translation on English counterfactuals to obtain CDA data in each language in our dataset. Translations are prone to errors and issues like Translaionese (Gellerstam, 1986), especially for the lower resource languages, and therefore can lead to the unreliability of the quality of generated counterfactuals were generated. In the future, we would like to explore learning generative (Wu et al., 2021) or editing models (Malmi et al., 2022) for automatically generating gender counterfactuals given text data in different languages. This can help us scale our counterfactual generation process to a much higher number of samples while also avoiding any losses in quality that may arise due to machine translation. Our multilingual DisCo metric is currently limited to 6 Indian languages and we hope our work will inspire further extension to cover different language families for improving the focus on multilingual biases evaluation."
11,"Section 6
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
12,"Our proposed graph-guided SQL generators are superior in generating complex SQL queries. How-
ever, the model has a large number of parameters and requires more computational resources, which is a common problem with current methods of generating complex SQL queries.
In addition, the proposed knowledge-enhanced re-ranking mechanism is proposed to leverage the knowledge in PLM to choose the best SQL query from the beam output. However, it does not take into account the database schema which can be the source of domain knowledge.
In the future, we will design lighter models for complex and cross-domain text-to-SQL generation and explore some other re-ranking mechanisms to incorporate the prior knowledge of database schema."
13,Section 7
14,"Domain KGs are the premise of KnowledgeDA, while open and high-quality domain KGs may be rare in some domains. Therefore, the method will be limited in the domains without suitable KGs. Besides, we use a similarity-based method to map entity mentions in the text to the corresponding entities in the KG. Although this method performs efficiently, it ignores the problem of entity ambigu-
ity (Vretinaris et al., 2021). For instance, the abbreviation, CAT, can stand for ‘catalase’ or ‘COPD Assessment Test’ in healthcare. To address this problem, it is necessary to use contextual information to clarify the specific meaning of the mention (Phan et al., 2017; Orr et al., 2021; Vretinaris et al., 2021). Last but not least, KnowledgeDA may be not good at tasks of paragraph-level texts and the efficiency will reduce. Because long texts probably contain more entity mentions and have more complex syntax, it is more difficult to retrieve the entities and acquire their relations from the KG."
15,Limitations
16,"The scope of this work is limited to sentence-level detection due to the absence of any span-level annotated datasets for hyperbole detection. Also, we could only partially annotate the metaphor datasets due to resource constraints. Finally, we did not try sophisticated large language models in our work as our goal was to demonstrate the effectiveness of multitasking using a simple model, rather than to test the performance of more sophisticated models."
17,Limitations
18,Limitation section at the end.
19,"Our work has focused strongly on the formal aspects of BPE. NLP practictioners should not be dissuaded from using BPE for subword tokenization, despite our presentation of examples where greedy BPE fails. Indeed, in contrast to synthetic examples on toy alphabet, on real data we made an observation that greedy BPE may be close to optimal."
20,"For web-augmented models including our work, the deterioration of search results from search engine highlights the importance of deriving an effective method to interact with the huge web. Search engines are often perceived as black-box and non-transparent for end users. Therefore, many works proposed “leaning to search” to decompose complex questions into simpler queries, which may improve the performance of web-based models (Nakano et al., 2021; Komeili et al., 2021).
In our model, we used a commercial search engine as the retriever to work with the whole web as a knowledge source. Since the web is not curated and well-structured like Wikipedia, we may encounter unexpected safety issues, including misinformation and harmful contents. While we have relied on the security control of the search engine, more attention should be paid to better understand the risks and provide effective ways to mitigate them. We hope our simple approach and strong results could encourage more future work by the community to tackle these questions. To encourage the community to investigate the question and ensure reproducibility, after the reviewing process, we will release the search URLs used in our experiments.
As for the potential concern, since we use the search engine to access real-time information, we do not have a tight control over retrieved results as traditional end-to-end retrieval (Guu et al., 2020; Lewis et al., 2020b). Not only the changes of search engine logic, but also the newly published information, might create discrepancies over the course of time. This is also an issue we have to tackle to build a stable web-based solution for PLMs."
21,"Although our proposed method is effective in three vision-language tasks, we still have some limitations. Firstly, we utilize T5 to convert the questionanswering format into the declarative sentence in VQA and it works well in most cases, but it still faces out-of-coverage problems, which will affect the following zero-shot prediction of CLIP. We need to design more rules for these special cases for better conversion. Secondly, our clustering algorithm for SNLI-VE can achieve strong zero-shot performance, but the clustering centroids are close to each other and the algorithm is sensitive to these centroids. The robustness of this algorithm should be improved. What’s more, we leverage FasterRCNN in visual fine-grained information extraction, so the detectable object attributes and classes are constrained in a relatively limited object set of Faster-RCNN, which may hinder further improvement from visual fine-grained information. The Faster-RCNN can be replaced with a better vision module. Besides, since we only utilize CLIP in our paper, we can explore the zero-shot ability of other
contrastive pre-training models in future work."
22,"The current work does achieve better performance than previous methods, but processing only one slot type at a time also reduces the efficiency of the model. In the future, we will explore how to maximize model efficiency. It would be an interesting challenge to generate answers for all the slots at once without degrading the effect of the model. Also, we will also try to apply our framework to more scenarios, such as NER and other tasks to explore the adaptability of the proposed method.
ACL 2023 Responsible NLP Checklist"
23,Section 4(line 288 293) and Appendix E.
24,"Compared to vanilla prototypes, the advantage of HyperProto would also rely on the additional radius parameter. Under the 1-shot setting, however, hypersphere prototypes will face challenges in estimating the radius in support sets, this is because the initial radius may be biased by the randomness of sampling. When the radius is set to exactly 0, the model will resemble a traditional prototypical network. Nevertheless, although not as large as the boost in the multi-shot setting, we find that having a consistently optimizable radius parameter at the training stage in the 1-shot scenario still delivers non-trivial results and exceeds most baselines (Table 1, Table 2, Table 3). This further points to the positive influence of the added radius parameter to learning prototype representation and hints on the possible research direction in learning a transferable radius in 1-shot scenario."
25,"As shown in Table 1, our method is experimentally demonstrated to be effective for two LLMs. However, OPT, a decoder-only model, is more suitable for the prompts generated by Co-Prompt. This seems to be because T0, the encoder-decoder model, requires a separate generator such as GPT2. The performance of prompts may vary to the generator involved in the vocabulary and training process. Also, there is a trade-off between search time and performance. While increasing the beam size and the number of document-query pairs enhances the probability of finding a more optimal
prompt, it makes the search time proportionally longer."
26,"I discuss the limtations at section ""Limitation""."
27,"Despite showing non-trivial improvements in the multi-hop capabilities of T5 models, our work has multiple limitations.
Restricted to 2-hops First, we chose 2WikiHopMultiQA (Ho et al., 2020) as our primary dataset since it uniquely maps each question to a chain of triples that contain the precise, noiseless single-hop knowledge required to answer the question. However, this comes at the cost of our analyses only being restricted to 2-hops (though see arguments by Press et al. (2023, sec 3.5) who suggest 3-and4-hop questions to be too convoluted to understand even by native-speakers). Nonetheless, our random walk training method is general by definition, and can be extended to multiple hops, though its effectiveness on QA tasks requiring more than 2-hops of reasoning remains to be measured.
Knowledge Graph size Our focus in this paper was to allow models to chain together their internalized knowledge in order to answer complex 2- hop questions. However, this critically requires them to possess the world knowledge required to answer the questions, for which we had to memorize the KG constructed using the structured triples provided in the dataset. This trade-off between focusing on knowledge composition vs. fully encoding world knowledge restricted our KG to be small in size (only 98,284 entities and 29 relations), which could be impractical in most real-world applications. In future work, we will experiment with larger sized KGs (Vrandečić and Krötzsch, 2014), by adding a substantially larger amount of additional triples to the existing KG, and measure their impact on multi-hop reasoning.
Lack of diverse QA tasks Finally, we were unable to consider popular datasets with CBQA versions such as TriviaQA (Roberts et al., 2020), NaturalQuestions (Kwiatkowski et al., 2019), etc., due to their lack of links from questions to structured knowledge. Future work can apply entity and relational linking techniques (Balachandran et al., 2021; Agarwal et al., 2021) in order to augment such QA datasets with (possibly) noisy links to structured knowledge, which will allow us to paint a more holistic picture of our methods. Additionally, this would also overcome the above limitation (of KG size), as it would substantially increase the amounts of entities and relations to be encoded
within models.
Implications for Larger Models Although we show clear improvements in triggering 2-hop reasoning in the largest T5 LM (T5-XXL), with 11B parameters, contemporary work has shown that multi-step reasoning capacities naturally emerge in LMs that are two or three orders of magnitude larger (Brown et al., 2020; Chowdhery et al., 2022; Wei et al., 2022b,a). However, these LMs benefit from examples in-context (especially since tuning them is non-trivial and expensive), and therefore it is unclear whether our methods can improve such models’ capacities even further. We have not tested such LMs in our work, due to resource limitations."
28,
29,"Regarding data collection, we crawled the English WikiHow website from Jan 2021 to May 2021. The number of available activities is limited by the data we crawled from WikiHow. We currently only
choose Gardening and Crafts categories as case studies. Because we focus on multimedia imagestep pairs, we remove steps that are not attached to any illustrative images. We also observe that a small portion of activities in the dataset do not follow chronological order.
Since our task focuses on the daily stereotypical tasks which usually require the model to understand the visual environment, the model design can be directly applied to support other domains, such as steps in the cooking videos. In addition, our model can also adapt to scenarios without visual images because the performance of our model only decreases slightly if no caption is provided. We plan to expand our model to other categories written in other languages."
30,"The model might generate incorrect nouns because of the occurrence of patterns (e.g., “refrigerate the slane for up to 1 year” instead of “refrigerate the purslane for up to 1 year”). In addition, our model sometimes tends to generate generic step descriptions because of insufficient input information, e.g., given the last step “lay the t-shirt out on a clean, flat surface.”, the model generates “cut the shirt out” which is vague compared to ground truth “carefully cut around the sleeve”. Moreover, the pretrained model might focus more on language modeling instead of inherent logic: for the activity of “make paint can planters”, after “removing the label” from the paint can, the BART+CAP generates “read the label”. In addition, there is still a small chance that the model generates the same output for various similar inputs.
Because we rely on image captions and retrieval results for step prediction, the upper bound of our generation quality is limited by the performance of the image caption and sentence retrieval modules. Our framework also needs to improve on imbalanced topics in the dataset. For example, the dataset contains more activities about tree for the gardening domain than other gardening-related plants. Because our multimedia generative script learning is a new task, we cannot compare our model with other established state-of-the-art models. Moreover, because WikiHow is a crowd-sourcing website, some everyday activities might have better human annotations than the remaining activities. We plan to include a fine-grained human written step prediction as an upper bound to address this issue."
31,"The automatic metrics we chose, including BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014), BARTScore (Yuan et al., 2021), self-BLEU (Zhu et al., 2018), and unique n-grams (Fedus et al., 2018), might not be the best metrics to evaluate our results. Some other metrics, such as semantic similarity and multimodal-retrieval based metrics, are based on pretrained models, including Augmented SBERT (Thakur et al., 2021), SentenceBert (Reimers and Gurevych, 2019), and CLIP (Radford et al., 2021). Those metrics might not align with human judgment and might be biased toward pretrained datasets. While we complement it with human evaluation, we only focus on relevance to ground truth and diversity. Although we found fluency is not an issue, it is likely we still need to cover all aspects of generation results."
32,"While our approach is shown to be effective in improving the zero-shot adaption ability of these PLMs, the scope of this work has only been extended to English languages and has not been tested on other languages. In addition, another limitation of this work is the scope of the aspect. Aspect is defined across 3 main categories of intent, sentiment, and topic in the work. However, given the massive space of text label interpretations, our aspect range can be refined and expanded even further, lending to more analysis of the stability of implicit & explicit training as the number of aspects grows. We do not investigate this scenario in this work."
33,"Like other controllable text generation methods (Dathathri et al., 2019; Krause et al., 2021; Liu et al., 2021; Lu et al., 2022; Arora et al., 2022; Adolphs et al., 2022), CLICK also relies on automatic neural classifiers when constructing DCL in some tasks (language detoxification in § 3.1 and sentiment steering in § 3.2 in our work). It may unavoidably inherit the biases and limitations of these classifiers. For instance, for the task of language detoxification, the toxicity may be overestimated when the input prompt or the continuation contains minority identity mentions. To address this limitation, we conducted human evaluation for all the tasks, which further confirms the effectiveness of CLICK. As more accurate, inclusive, and reliable classifiers are built (e.g., for toxicity detection), we expect that CLICK would inherit those improvements as well."
34,"There are still some limitations of our work. In the future, we plan to enhance the procedure of extracting candidate keyphrase, to improve the upper bound of the performance of keyphrase extraction. One possible way is to generate candidate phrases of the document by utilizing the high-level semantic relatedness (e.g., attention weights) instead of using the surface-or syntactic-level information."
35,"We collect the dictionary from the Internet, and although we make effort to reduce replicate explanations, there is noise in the dictionary. Besides, not all the words are included in the dictionary. In other words, the quality and amount of entries in the Chinese dictionary are to be improved. Additionally, our method is pre-trained on the Bert-like transformers to enhance the corresponding PLMs, and can not be applied to LLM directly whose frameworks are unavailable. In the future, we will use the retriever for disambiguation and dictionary knowledge infusion to LLM."
36,limitation section
37,"Although INSTRUCTOR significantly improves the baseline GTR performance, we were only able to use four negative examples during the model finetuning process due to computation constraints. However, negative examples have been shown to play an important role in contrastive learning (Robinson et al., 2021). We hope that future work will scale up the number of negatives used during finetuning and investigate various methods for mining hard negatives. Additionally, we do not have enough computation resources to apply multitask instruction finetuning to GTR-XXL (4.8B parameters), which is also an area for future exploration.
At the core of INSTRUCTOR is the instruction design. While our current unified instruction format has demonstrated effectiveness, future research can explore other instructional elements to further improve performance. For example, previous work (Wang et al., 2022) have shown that incorporating demonstration examples and explanations can be beneficial for instruction-finetuned language models."
38,"section 7, after conclusion.
7 A2. Did you discuss any potential risks of your work? Models and datasets are all open-sourced and used consistently with their intended use. We do not see potential risks beyond these open-sourced artifacts."
39,Left blank.
40,"While we show that C2A successfully improves the effectiveness and efficiency of PEFT in FL, we have mainly focused on improving the effectiveness of the vanilla adapter. However, it is an open question whether our framework can improve other PEFT approaches, such as prompt tuning(Lester et al., 2021), and LoRA (Hu et al., 2022). Although we didn’t analyze whether our framework can generate parameters for alternative PEFT, one recent approach reveals that hypernetworks can generate parameters for various types of PEFT in multi-task learning (He et al., 2022; Üstün et al., 2022). Likewise, as C2A generates parameters with hypernetwork, we believe that C2A is highly expected to improve the performance of any alternative PEFT modules.
Ethics Statement
This study covers work that utilizes PLMs, which have a wide variety of positive applications, such as the application to summarization, or language understanding. At the same time, there are a number
of ethical concerns with PLMs in general, including concerns regarding the generation of biased or discriminative text (Bordia and Bowman, 2019), the leakage of private information from training data (Carlini et al., 2021), and the environmental impact of training or tuning them (Strubell et al., 2019).
Our framework attempts to train PLMs with minimal changes made to their pre-existing parameters in FL scenarios. Our work is believed to bring some insights into the two ethical dimensions: privacy and environment. First, with respect to private information leakage, although our work has not addressed address the privacy issue in the pre-train process, our FL framework can mitigate the data privacy issues in the fine-tuning stages. In addition, with respect to environmental impact, our work may obviate the need for full fine-tuning, which may also significantly reduce the cost in terms of memory or deployed servers."
41,"Many of the reviews that were gathered for constructing BANGLABOOK are discarded because they lack a corresponding rating. A manual annotation process would have yielded a much larger dataset, which was not feasible due to resource constraints. Moreover, one of the challenges for validating the dataset is the lack of statistical models and word-embeddings pre-trained on the Bangla language. Some pre-trained Bangla-BERT models, yet to be trained on extensive corpora, have only recently been proposed. Improving transformer-based models for Bangla can enhance sub-word level contextual understanding which will consequently help in more accurate identification of the sentiments in BANGLABOOK (Islam et al., 2022)."
42,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
43,"Cooking recipes constitute a single genre within ProcDocQA, with a well-grounded task and large range in RISK OF HARM and user EXPERTISE. Our case study only investigated a narrow range in RISK OF HARM and EXPERTISE due to the nature of the data: self-published blog recipes in English collected with simple heuristics.
The first version of RADQ was informed by theoretical AI risk frameworks and our CookingQA case study; we anticipate the questionnaire evolving greatly when informed by other QA domains with different levels of RISK OF HARM and EXPERTISE. This work only considers immediate risks to humans; longitudinal risks such as the propagation of information are an open research topic.
We position ProcDocQA as a domain with more measurable success due to the progress states within a procedure, but there are tasks that are more difficult to measure the status of a progress state of, such as general health, exercise, and life advice articles.
This work contributes to risk mitigation by concretizing risks in user-aware scenarios. Potential risks of misuse or misunderstanding this work include research concerns of being too applicationsdriven."
44,"Although introducing chart value prediction objective, it only provides minor improvement to the model’s performance on doing complex reasoning. There is still a large room to improve the model’s capability in math calculation. Our model also suffers from the noisy OCR prediction of off-the-shelf object detector, whose performance will depend highly on the extracted OCR text qualities. Another possible limitation of our approach is the quality of the pre-training data, which only contains synthetic images. Although the proposed model works fairly well on the ChartQA dataset, it is unclear if the improved performance can be generalized to other realistic chart images."
45,"SoftMV is specifically designed for cross-lingual natural language inference. We believe that some of the ideas in our paper can be used in other tasks of XLU, which remains to be further investigated by subsequent research.
In addition, we conduct experiments on the XNLI dataset which consists of 15 languages. SoftMV outperforms the baseline methods under the cross-lingual transfer settings. However, the cross-lingual ability of SoftMV on other languages, especially those lacking relevant datasets, needs to be verified in future work."
46,Section 7
47,"The biggest limitation of our framework is that persona prompting increases the input length of the model, increasing inference time. Through the analysis of real online user logs, the inference time will be 1.82 times that of the original. In the future, we will optimize the inference performance by compressing the length of the prompting part."
48,Section 8
49,"In our experiment, we presented results for three diverse sequence-to-sequence tasks, namely, machine translation, text summarization and knowledge graph question answering. While for these three tasks, we managed to observe common trends (i.e. some methods consistently outperformed other methods) a more large-scale study of various sequence-to-sequence tasks is needed to further confirm this observation and robustness of the best-performing method as identified in this work."
50,"Although the proposed method achieves significantly improved OOD detection performances compared to the baselines, but POE can not be applied to a naive LSTM, and RNN because our OOD construction is based on an attention score of the PLM. We leave this issue for future work, but we believe that our proposed method can be used in various NLP tasks as PLMs are now adopted in most fields of NLP tasks. While we adopted a masking method using attention scores in this paper, it is not clear that tokens with high attention scores have the most direct impact on the model’s predictions (Wiegreffe and Pinter, 2019). To provide readers with more information, we include additional experimental results in the Appendix to discuss the impact of different masking strategies on OOD detection performance."
51,We have discussed the limitations in Section Limitations.
52,"We identify the following limitations for our work:
1. Due to limited access to a wide network of native speakers from the majority of languages, we were able to manually inspect only a subset of languages present in our pretraining data. Specifically, we could only manually evaluate Afrikaans, Yorùbá, Igbo, Hausa, Luganda, Kinyarwanda, Chichewa, Shona, Somali, Swahili, Xhosa, Bemba, and Zulu. Future work should focus on increasing the subset of languages evaluated manually in order to ensure quality. We believe automatic analyses are not sufficient before development of models that get deployed in particular applications.
2. Another limitation is related to our inability to perform extensive analysis of biases and hateful speech present in our pretraining data. Again, this is due to relatively restricted access to native speakers (and even automated tools) to perform this analysis. As a result, we cannot fully ensure that our models is free from biases and socially undesirable effects. Therefore, it is important that these models be used with care and caution, and be analyzed for biases and socially undesirable effects before use.
3. Additionally, due to unavailability of sufficient computing resources, we were unable to evaluate large language models such as BLOOM, even though it covers 22 African languages.
4. Finally, even though AfroNLU has diverse tasks at the word and sentence level, these tasks only cover few African languages. We therefore encourage the creation of more datasets for downstream NLU tasks in more (and more diverse) African languages. We believe broader benchmarks will continue to be important for future progress in African NLP."
53,"Due to the absence of large evaluation databases, we only evaluated our method on three publicly available datasets that can be used for the MSMO task. The popular video databases, i.e., COIN and Howto100M datasets, can not be used in our task, since they lack narrations and key-step annotation. So a large evaluation database is highly needed for evaluating the performance of MSMO approaches.
As the nature of the summarization task, human preference has an inevitable influence on the performance, since the ground-truth labels were provided by human annotators. It’s somehow difficult to quantitatively specify the quality of the summarization result, and current widely used evaluation metrics may not reflect the performance of the results very well. So we are seeking some new directions to find another idea for quality evaluation.
The current setting is short videos & short documents, due to the constrain of available data. To extend the current MSMO to a more general setting, i.e., much longer videos or documents, new datasets should be collected. However, this requires huge human effort in annotating and organizing a high-value dataset, which is extremely time-consuming and labor-intensive. Nevertheless, we believe the MSMO task is promising and can provide valuable solutions to many real-world problems. So if such a dataset is collected, we believe it could significantly boost the research in this field."
54,"Section 7
7 A2. Did you discuss any potential risks of your work? To the best of our knowledge, we do not foresee any harmful uses of this study"
55,"We propose and solve the feature space shift problem in text augmentation. However, there is a limitation that remains. BOOSTAUG cannot preserve
the grammar and syntax to a certain extent. We apply the perplexity filtering strategy, but it is an implicit constraint and cannot ensure the syntax quality of the augmentation instances due to some breaking transformations, such as keyword deletions and modifications. However, we do not need precise grammar and syntax information in most classification tasks, especially in PLM-based classification. For some syntax-sensitive tasks, e.g., syntax parsing and the syntax-based ABSC (Zhang et al., 2019; Phan and Ogunbona, 2020; Dai et al., 2021), ensuring the syntax quality of the augmented instances is an urgent problem. Therefore, BOOSTAUG may not be an best choice for some tasks or models requiring syntax as an essential modeling objective (Zhang et al., 2019). In other words, the syntax quality of BOOSTAUG depends on the backend."
56,"Despite the promising results achieved by our proposed method, there are certain limitations that need to be noted. Firstly, our approach relies heavily on the use of Transformer models, which can be computationally expensive to train and run. Additionally, the lower performance of our aligner for languages other than English is still a substantial shortcoming, which is discussed in Section 5.2.
Furthermore, our method is not adaptable to nonTransformer architectures, as it relies on the specific properties of Transformer-based models to extract alignment information.
Lastly, our method is based on the assumption that the decoder will attend to those input tokens that are more relevant to predicting the next one. However, this assumption may not always hold true in practice, which could lead to suboptimal alignments.
In conclusion, while our proposed method presents a promising approach for cross-lingual AMR alignment, it is important to consider the aforementioned limitations when applying our method to real-world scenarios. Future research could focus on addressing these limitations and exploring ways to improve the performance of our aligner for languages other than English."
57,"Even though our method is an excellent alternative to the current AMR aligner system, which is standard and task-agnostic, we notice some drawbacks when moving to other autoregressive models or languages:
Model In this work, we studied how Cross Attention layers retain alignment information between input and output tokens in auto-regressive models. In Section 5.1, we examined which layers in state-of-the-art AMR parser models based on BART-large best preserve this information. Unfortunately, we cannot guarantee that these layers are
optimal for other auto-regressive models, and so on. As a result, an examination of cross-attention across multiple models should be done before developing the cross-lingual application of this approach.
Sentence Segmentation It is necessary to apply LEAMR’s Spam Segmentation technique to produce the alignment in LEAMR format (Section 3.4). However, this segmentation method has several flaws: i) As stated in Section 7, this approach does not deal appropriately with phrasal verbs and consecutive segments; ii) the algorithm is Englishspecific; it is dependent on English grammar rules that we are unable to project to other languages. Therefore we cannot extract the LEAMR alignments in a cross-lingual AMR parsing because we lack a segmentation procedure. However, although LEAMR alignment has this constraint, ISI alignment does not require any initial sentence segmentation and may thus be utilized cross-lingually.
ACL 2023 Responsible NLP Checklist"
58,Section 9
59,"In theory, the method proposed in this paper can be applied to different types of transformer language models for both pre-training and fine-tuning. Due to limit of computational resource, we currently haven’t had the chance to test our proposed method in the very promising setting of large-scale language model pre-training yet. In future work,
we plan to further test our proposed logical transformer architecture on large-scale language model pre-training to see how much performance boost it can achieve."
60,
61,"Though the formulation of the task allows exploring several different settings (by varying the configuration parameters), in this work, we investigated only the label-balanced setting. Exploring the labelimbalanced setting is another very interesting research direction, and we leave that for future work. Another limitation was the limited exploration of novelty detection methods, as a number of methods have been proposed in the recent times. However, we study only a limited set of methods since the focus of this work is on formulating and exploring NoveltyTask. Lastly, we note that NoveltyTask is a controlled task/framework for evaluating a system’s ability to deal with novelties and not a method to improve its ability."
62,"There are a few limitations we would like to address. First of all, the number of clusters needs manual configuration. This is a limitation of the clustering algorithms (Xie et al., 2016) since we
need to set a threshold for convergence, which consequentially pinpoints k. An expedient alternative is to analyse the dataset for the realistic settings or probe into k for the optimal setup, which is, however, beyond the scope of this paper. Another limitation is the pre-requisite for millions of unannotated data. The autoencoder needs enormous data to learn bottleneck representations. Its performance would be hindered without access to abundant corpora. Lastly, the performance of the acquired clustering-friendly representations depends on the similarity metric chosen. Efforts need to be made to find the best option, whether it is Euclidean distance or cosine similarity etc."
63,"Section 7
7 A2. Did you discuss any potential risks of your work? Our work does not introduce a novel dataset."
64,"In this paper, we only discuss the SSAs in English, as this has been the most predominantly studied in adversarial attacks in NLP. The authors are not sure whether SSAs in a different language will suffer from the shortcomings discussed in this paper. However, if an SSA in a non-English language uses the transformations or constraints discussed in this paper, there is a high chance that this attack will produce low-quality results for the same reason shown in this paper. Still, the above claim needs to be verified by extensive human evaluation and further language-specific analyses.
In our paper, we use WordNet as the gold standard of the word senses since WordNet is a widely adopted and accepted tool in the NLP community. Chances are that some annotations in WordNet, while very scarce, are not perfect, and this may be a possible limitation of our work. It is also possible that the matched sense synonyms found by WordNet may not always be a valid substitution even if the annotation of WordNet is perfect. For example, the collocating words of the substituted word may not match that of the original word, and the substitution word may not fit in the original context. However, if a word is not even a synonym, it is more unlikely that it is a valid substitution. Thus, being a synonym in WordNet is a minimum requirement and we use WordNet synonym sets to evaluate the validity of a word substitution.
Last, we do not conduct human evaluations on what the other substitution types in Table 1 are. As stated in Section 3.2.1, while we do not perform human evaluations on this, the readers can browse through Table 6 in the Appendix to see what the others substitutions are. It will be interesting to see what human evaluators think about the other substitutions in the future."
65,"Since our results regarding a fine-tuned similarity method are limited to the SBERT fine-tuning introduced by Bexte et al. (2022), our findings are limited to this specific similarity-based setup and cannot exclude that other similarity-based methods might behave differently. We also did not consider training sizes larger than 1000 instances of ASAP, and can therefore not speak for how the relative performance of the different methods would be affected by using even more training data. Regarding the experiment on larger training data sizes, we also limited our analysis to ASAP, so it is necessary to compare the observed effects to those that occur on other data sets. The same goes for our cross-prompt experiments, which were also limited to ASAP . Other data sets cover other content domains and can thus produce different effects. Finally, while we do discuss the advantage of a more straightforward explainability of similarity-based models regarding feedback, this is an entirely theoretical argument that goes beyond the scope of this paper and would therefore have to be investigated further in future work."
66,"Limitation Section, after Section 6
7 A2. Did you discuss any potential risks of your work? No, we do not foresee potential risks of this work."
67,"• The interdisciplinary element – at the heart of this work – mandates that our results be interpretable and relevant to scholars from the opposite side of the methodological divide (i.e., biblical scholars). This, in turn, introduces constraints to our framework – the foremost is choosing appropriate text-embedding techniques. As discussed in §2.4 and §2.8, the ability to extract specific lexical features (i.e., unique n-grams) that are important to the
classification, to quantify them, and subject them to complementary philological analysis (see Appendix E) – requires that they be interpretable. This constraint limits the ability to implement state-ofthe-art language-model-based embeddings without devising the required framework for their interpretation. Consequently, using traditional embeddings – which encode mostly explicit lexical features (e.g., see §2.4) – limits the complexity of the analyzed textual phenomena and is therefore agnostic of potential signal that is manifested in more complex features.
• In text stylometry questions, especially those related to ancient texts, it is often problematic (and even impossible) to rely on a benchmark training set with which supervised statistical learning can take place. This, in turn, means that supervised learning in such tasks must be implemented with extreme caution so as not to introduce a bias into a supposedly-unbiased analysis. Therefore, implementing supervised learning techniques for such tasks requires a complementary framework that could overcome such potential biases. In light of this, our analysis involves predominantly unsupervised exploration of the text, given different parameterizations.
• Our ability to draw insight from exploring the stylistic differences between the hypothesized distinct texts relies heavily on observing significant overlap between the hypothesized and unsupervised partitions. Without it, the ability to discern the similarity between the results of our pipeline is greatly obscured, as the pipeline remains essentially agnostic of the hypothesized partition. Such a scenario either deems the parameterization irrelevant to the hypothesized partition or disproves the hypothesized partition. Breaking the degeneracy between these two possibilities may entail considerable additional analysis."
68,"We concede that there are differences in the number of parameters between the BART models when compared to the RoBERTa and LUKE counterparts. However, as per our result discussions and observations, the gains are orthogonal to the encoder used and the differences in the base models are not as significant when comparing the larger counterparts. We note that we also explored seq2seq
pre-trained knowledge-enhanced models like KeyBART and GENRE, however both resulted in underwhelming performance compared to BART. Further exploration is required in improving performance for such models. We also note that while we demonstrate gains by switching to a classificationbased approach in RINE, such models are limited in other generation task capabilities such as translation or summarization. We will release the data and code used for this work, but emphasize that some processing was done over the raw TOPv2 dataset, namely reconstructing source utterances directly from the provided target instead of using the provided source, as we encountered mismatches when constructing pointers. The source was then lowercased."
69,"Our system distills PLMs into a less expressive but trustworthy set of templates. In developing this method, we explicitly trade off linguistic diversity for faithfulness guarantees. While this approach works well on academic benchmarks, in more complicated real world settings sacrificing linguistic diversity may impact different groups to a different extent. This raises the question of fairness and we hope to investigate such problems on more realistic datasets in future work."
70,"Our approach for training the Transformer architecture using self-knowledge distillation is promising, but there are still some limitations that need to be addressed in future work. One limitation is that our approach is only tested on the task of AMR parsing, and more evaluations are needed to see if it generalizes well to other tasks, such as Relation Extraction. Additionally, our approach, as is also the case for other current methods, exhibits performance degradation as the number of words in the sentence increases. This may be an indication of the current methods’ limitation or lack of robustness to longer sentences.
Another limitation is the added complexity and extra parameters required by the use of Transformer adapters, which increases the overall complexity of the architecture and training time. Even though our approach still achieves state-of-the-art results and it is as lightweight as previous systems at inference time, this fact should be considered by researchers if they should decide to adopt it for other tasks.
In summary, our approach presents an innovative way to train the Transformer architecture and achieve state-of-the-art results in AMR parsing. However, more work is needed to further improve the performance of the model and to apply it to other tasks as well."
71,Section 10
72,"For modeling simplicity, we adopt the classic LDA methods to get the topic ID for each video segment. We plan to investigate more advanced topic clustering methods and check how it can be applied to multilingual cases. Also, we propose a twostage framework that first extract topic and style features, based on which the emotion classifier will be trained. In the future, we hope to extend this work to learn features in an end-to-end manner."
73,Limitations section
74,"While good performance has been achieved, there are still limitations in our work. First, though QAVGAE extracts enhanced features and are fast to train, it is an independent module from the main framework. Second, as a post-processing step, the performance of SERR module on simple question is better than that of complex questions.
In the future, we would like to explore the possibility of fusing relation constraints into the representation module directly and inject strong facts identification mechanism as guidance signal of multi-hop reasoning process, aiming to integrate QA-VGAE and SERR into the main framework."
75,Section 7
76,"We think this work has the following limitations: The first limitation is that our method involves additional computation for identifying noun phrases and determining which phrases should be normalized. The second limitation is that our method is only performed on noun phrases.
Other phrases may also introduce spurious features. Extending our method to other types of phrases is a potential research direction. The third limitation is that due to the cost limitation, we did not test on the more powerful GPT-based PLMs, which proves to be more powerful and leads to heated discussions recently."
77,
78,"Limitations and Future Work: As the first study to assess the adversarial robustness of prompt-based FSL methods, we focus on representative methods that cover different design choices. Future work could expand the set of prompt-based FSL methods considered in this study. Our broader goal is to encourage systematic evaluation of adversarial robustness for all prompt-based FSL methods. Furthermore, we do not perform extensive hyperparameter tuning for the methods considered in this work. It is worth noting that “true” few-shot learning setting has been argued not to involve any development set (as that would involve collecting more labeled data) (Perez et al., 2021; Schick and Schütze, 2022). To this end, we use the hyper-parameters reported by the original authors of these methods. Future work could explore settings where access to a limited development set is assumed for exhaustive hyperparameter tuning. Finally, for adversarial evaluation of prompt-based FSL approaches, we utilize a pre-constructed dataset — AdvGLUE (Wang et al., 2021a). Since these examples are pre-constructed, they do not have access to the gradients of the specific victim models under investigation. Nonetheless, the AdvGLUE benchmark offers a foundation for understanding vulnerabilities in large-scale language models under various adversarial scenarios. This standardized dataset enables fair comparison and mitigates issues with invalid perturbations. For
instance, Wang et al. (2021a) found that over 90% of adversarial perturbations generated using the gradients of victim models for NLP tasks are invalid. Therefore, using AdvGLUE ensures adversarial evaluation on high-quality, human-verified data. Future work could extend the study by considering adversarial examples generated using the gradients of victim models and validating them for correctness. Broader Social Impact: The authors do not foresee any negative social impacts of this work. We believe systematic and preemptive evaluation of the robustness of language technologies against potential adversarial attacks will help develop more safe and secure systems. We release the code for our experiments to aid reproducibility and promote future research on this topic. Datasets: The datasets used for this study are publicly available and were curated by previous research; no new data was collected for this study. We abide by the terms of use of the benchmarks as well as the individual datasets."
79,"Our search was conducted exclusively in English, and we may have missed relevant papers written in other languages; this may have influenced the heavy English skew in our data.
Some of the annotations of attributes and choices in this taxonomy rely on subjective judgements, particularly with regards to the clarity of conceptualisations of bias, desired outcomes, and justifications of proxy choices. As with any qualita-
tive work, these results are influenced by our own perspectives and judgement. We did our best to address this through regular discussion, identifying disagreements early on when designing the taxonomy, and adopting a “generous” approach."
80,"In this work, we find that robust instances are helpful for model robustness and propose a metric to select them. However, we only applied one single criterion, i.e. the training dynamic of adversarial loss, as selection metric. More instance features can be inspected in terms of the relation with model robustness and further serve as metrics for robust data selection. Moreover, in this work, we use the selected data for standard fine-tuning with simple regularization, while the impact of data robustness on adversarial training is not studied. These two problems will be explored in future work."
81,"The limitation section is after the conclusion part of the thesis.
7 A2. Did you discuss any potential risks of your work? Our work don’t have potetial risk."
82,"While the previous VQA methods that retrieve from PLMs all use GPT-3, we do not experiment with GPT-3 in this paper due to the additional cost. We only focus on applying text-generation models as answer selectors, while classification models could also potentially be good answer selectors. The multi-modal CLIP embedding has already been surpassed by several recent studies (Alayrac et al., 2022; Singh et al., 2022; Lu et al., 2022) and therefore ClipCap cannot represent the performance of multi-modal answer selectors."
83,"The Pne value decreased in all datasets. While this is not problematic for question generation, where the presence of a named entity is not always necessary, it does pose an issue for NLG tasks where the inclusion of named entities is important. In these cases, we recommend using alternative techniques that we have proposed. Additionally, using delexicalization and over-generation in our approach leads to a high training and inference time."
84,"We identify the potential limitations of our work as follow: (1) Distant labels may not be available in every application domain (e.g., patient notes in clinical application), although domain adaptation can be applied in these scenarios. We also believe that distantly supervised contrastive learning can be exploited in tasks involving image and video where surrogate labels are abundant. (2) We also acknowledge that the offline NPMI matrix of our proposed CCL method depends on a dataset (distantly) labeled with multiple classes in each sample. To alleviate this limitation, we explore an alternative method that uses learned class embeddings to calculate the inter-class relations in Section 5. This weighting approach achieves sizable improvement over RoBERTa on 16 in-domain datasets, though it underperforms our NPMI-based approach. (3) Our framework does not always work on tasks outside SM. For example, our model underperforms self-supervised CL models, i.e., SimCSESelf and Mirror-BERT, on semantic textual similarity task in Appendix E.6. As we showed, however, our framework exhibits promising performance on some other tasks. For example, our hashtag-based model acquires best performance on the topic classification task, as shown in Appendix E.5."
85,"There are certain limitations that can be concerned for further improvements. First, the posterior inference relies on the prior estimation of positive class prior α and true positive ratio β. Our experiments show that a data-driven estimation based on end-to-end model training produces worse results than a hyperparameter grid search. An automatic prior estimation is desirable for real-world applications. Moreover, in nPUGraph, we approximate the probability of negative/positive facts being collected/uncollected via neural networks, which lacks a degree of interpretability. In the future, we plan to utilize a more explainable random process depending on entity/relation features to model the collection probability distribution.
Ethical Impact
nPUGraph neither introduces any social/ethical bias to the model nor amplifies any bias in data. Benchmark KG are publicly available. For Twitter interaction data, we mask all identity and privacy information for users, where only information related to user interactions with tweets and hashtags
is presented. Our model is built upon public libraries in PyTorch. We do not foresee any direct social consequences or ethical issues."
86,Just Limitations; no section number
87,"In this paper, we propose a novel set-wise framework to extract keyphrases globally. To verify the effectiveness of the new framework, we design simple yield effective neural networks for both the neural keyphrase set function and the keyphrase set extractor agent modules. In general, a complex neural network should yield better performance. Moreover, for the sake of fairness, our model adopts the same pre-trained language model (i.e., BERT) as the recent state-of-the-art baselines (Liang et al., 2021; Ding and Luo, 2021; Zhang et al., 2022). Actually, other pre-trained language models can be applied to our model, such as RoBERTa (Liu et al., 2019). These pre-trained language models may yield better results, which also demonstrates that there is much room for improvement in our proposed framework. Therefore, we believe the power of this set-wise framework has not been fully exploited. In the future, more forms of document-set matching models can be explored to instantiate the set-wise framework."
88,"When evaluating our model in a cross-dataset adaptation setting, our experiments indicate the importance of using a retrieval dataset. It is challenging to procure high-quality and volume retrieval datasets, especially in low-resource domains such as the medical field. Fortunately, the VQA-RAD and SLAKE datasets we evaluate on contain professionally annotated medical images. We also overcome the lack of data by creating a synthetic dataset from the medical ROCO image-captioning dataset.
Additionally, our model struggles with questions requiring multi-step reasoning, such as knowledge graph, abnormality, and position questions. Although performances in these question types are not far below the overall accuracy, future work may consider supplementary knowledge-based retrieval to assist in these challenging question types."
89,"Our work has some limitations. Although we use the same verbal stimuli in the previous IAT tests for creating text prompts, it is very likely that some stimuli that can represent the concepts are underrepresented. The approach we adopted for comparing the images’ distance might be biased as well. The current bias test procedure applies the visual encoder of OpenAI’s CLIP model to measure the distance between images. However, it is unclear whether the image encoder may inject additional biases into the latent visual representations.
Ethics Statement
The scope of this work is to provide a principal procedure for measuring the implicit valence and stereotypical biases in image generations. The experiments conducted involve generating images that pertain to demographic groups, and all images were generated in compliance with the terms of service and guidelines provided by the stable diffusion’s license. The AI-generated images are used solely for research purposes and no identities are explicitly attributed to individuals depicted in the images. People’s names are used to generate images. We justify that these are common American names publicly accessible, and do not contain any information that can uniquely identify an individual."
90,Section 7
91,"All source sense embeddings we used in our experiments are only covering the English language, which is morphologically limited. Therefore, it is unclear whether our results and conclusions will still be valid for meta-sense embeddings created for languages other than English. On the other hand, there are WSD and WiC benchmarks for other languages such as SemEval-13, SemEval-15, XL-WSD (Pasini et al., 2021) and WiC-XL (Raganato et al., 2020), as well as multilingual sense embeddings such as ARESm (Scarlini et al., 2020b) and SensEmBERT (Scarlini et al., 2020a). Extending our evaluations to cover multilingual sense embeddings is deferred to future work.
Our meta-sense embedding method requires static sense embeddings, and cannot be applied to contextualised sense embedding methods such as SenseBERT (Levine et al., 2020). There have been some work on learning word-level and sentencelevel (Takahashi and Bollegala, 2022; Poerner et al., 2020) meta-embeddings using contextualised word embeddings produced by MLMs as the source embeddings. However, contextualised sense embedding methods are limited compared to the numerous static sense embedding methods. This is partly due to the lack of large-scale sense annotated corpora, required to train or fine-tune contextualised sense embeddings. Extending our work to learn meta-sense embeddings using contextualised word embeddings as source embeddings is an interesting future research direction."
92,section 6
93,"Our framework is a two-phase process, which has its inherent defects, that is, the results of the second phase depend on the results of the phase 1. Because the sequence annotation algorithm in the first phase cannot achieve 100% accuracy, it will predict the wrong position that should be rewritten when the second phase is followed, which will further lead to the error of the final result.
On the other hand, T5 model is only used to predict the words that should be filled in blank, rather than generate the whole sentence, which may lead to the decline of the overall fluency of the sentence."
94,"We would have liked to evaluate the generalization of our cross-lingual approach on more languages. For instance, we partially rely on machine translation models for Chinese-to-English translation. Available translation models for other language pairs, especially from/to low-resource languages have much lower quality, and it would be desirable to measure the effect of that in our experiments.
The ontology used for new languages is derived by translating the Chinese ontology. As a result, the entities are not localized. Creating local ontology requires manual effort as one would need to identify websites or databases for scraping or collecting the entities. Once the local entities are collected, we can automatically replace translated entities with local ones to localize the dataset.
Another limitation is the lack of human evaluation for agent responses. BLEU score does not correlate well with human judgment (Sulem et al., 2018), and SER only accounts for the factuality of the response but not grammar or fluency. In future work, we wish to address this by conducting human evaluations in addition to automatic metrics."
95,"During inference, kNN-MT have to query the datastore at each decoding step, which is timeconsuming. Although up to 45% datastore entries can be safely pruned by our method, deploying a high-quality kNN-MT system with fast inference speed is still an open challenge."
96,section 9
97,"Our augmentation approach is model-agnostic, meaning that it can be applied to any lexical substitution model. However, this also means that it inherits any limitations of the underlying model. For example, in the case of LexSubGen, it can only produce single-token words as substitutes which might prevent it from generating valid longer words or phrases as substitutes that are present in the gold annotations. Additionally, the substitutes are also limited by the vocabulary of the pre-trained language model that LexSubGen uses.
Another limitation of our method is that it relies on the presence of target words in a lexical resource, such as WordNet, together with their synonyms and glosses. If this sense-specific information is missing from the lexical resource, it cannot be used to improve the performance of a lexical substitution system.
Our entailment criterion for lexical substitution is defined for the binary classification task, rather than for generation or ranking tasks. However, if a probabilistic model is used to determine the probability of mutual entailment between sentences, this score can be utilized to rank substitutes if necessary. As explained in Section 3.2, the binary definition can also be adapted to the generation task by iterating over candidate substitutes."
98,Section 7
99,"Our paper assesses procedural knowledge augmentation using a limited number of highly structured instructional documents. Naturally, the results presented may vary for unstructured guidelines. Additionally, due to the limited size of publicly available TOD datasets, we have not tested how our method may scale to settings with larger document
spaces (> 100 documents). For larger document sets, more efficient methods of computing similarity such as Maximum Inner Product Search (MIPS) algorithms may be necessary to approximate documents with the highest relevance scores."
100,"The proposed model, Recurrent Attention Network (RAN), effectively models long sequential data by propagating information window-by-window through the sequence via its well-designed recurrent architecture. However, the multi-head selfattention applied to each window is still limited to local attention, which prevents it from providing a global dependency relationship for the entire sequence. This limitation restricts RAN’s application in scenarios where a global dependency relationship is necessary, such as visualizing attention weights for the entire document via a heatmap. This limitation potentially reduces the interpretability of the model, although it does not affect the model’s performance. Hence, exploring ways to incorporate global attention mechanisms into the RAN architecture is a promising research direction to improve its interpretability and expand its range of applications."
101,"We worked with only five language pairs, all involving English and another language: Arabic, Spanish, French, Russian and Chinese. This is due to using the multiUN dataset (Ziemski et al., 2016) for evaluating alignment and performing realignment. We also used the opus100 dataset (Zhang et al., 2020), which contains more pairs and is the dataset that eventually figured in our paper, but we stuck to the same language pairs for a fair comparison with multiUN in Appendix F. This narrow choice of language limits our ability to understand why realignment methods work well for some languages and not others. And we believe that making a similar analysis with many language pairs, not necessarily involving English, would be a good lead for further research investigating the link between the success of the realignment method and how two languages relate to each other.
We chose a strong alignment objective with contrastive learning for our realignment task. Several other objectives could have been tried, like learning an orthogonal mapping between representations (Wang et al., 2019) or simply using a ℓ2-loss to collapse representations together (Cao et al., 2020), but both methods require an extra regularization step (Wu and Dredze, 2020b) since they do not leverage any negative samples. For the sake of simplicity, we focused on a contrastive loss, as trying different methods would have led to an explosion in the number of runs for the controlled experiment. This also explains why we used the same hyperparameters and pre-processing steps of Wu and Dredze (2020b). A more thorough search for the optimal parameters, and realignment loss, might lead to better results."
102,"Section 8
7 A2. Did you discuss any potential risks of your work? Our work does not introduce new methods. It is anlysing existing ones and trying to provide a better understanding of their inner working. Hence, we do not believe that our paper present a direct risk."
103,"This work proposed a dataset, a simulator, tasks, and models for Aerial Vision-and-Language Navigation. Since satellite images are needed to simulate the drone’s observation, risks of privacy leaking may exist. By using the open-source satellite dataset xView (Lam et al., 2018), we mitigate the risks while also being able to develop a simulator for training our model. Additionally, using satellite images for simulating top-down visual observation of the drone introduces the shortcoming of having only 2D static scenes while adopting the strength of the satellite images where rich labels and visual features are included.
Broader Impact
We recognize the potential ethical problems during the dataset collection, where human annotators are involved. The data collection of this project is classified as exempt by Human Subject Committee vis IRB protocols. As a result, we utilized the Amazon Mechanical Turk (AMT) website to find workers willing to participate in the project. With AMT, our data collection is constrained by legal terms, and the data collection protocol is under AMT’s approval. The agreement signed by both requesters and workers on AMT also ensures a transparent and fair data annotation process and that privacy is well protected."
104,"Although our model achieves competitive results with baseline models, some limitations are summarized as follows.
1. The process of extracting data distributional signatures is time-consuming, especially for datasets with more diverse dialogue patterns. The process of calculating adjacent n-grams is slow. In addition, repeated string manipulation for long texts also needs to be optimized
2. The experiment results are easily affected by the fluctuation of hyper-parameters, especially the signature block hidden size. There is some noise in the distributional signatures. Under different hyper-parameters, noise may have different effects and directly affect experiment results.
3. Our model performs poorly when the training set is too small. The distributional signatures of small data interfere with the model."
105,"We presented a method to automatically generate and label affective events by co-prompting with large language models. The data generation process does not involve creating or training new language models. There are some limitations to our approach. One limitation is that language models are not guaranteed to generate truthful or sensible information, which could introduce noisy information to our model. For example, we observed that the Emotion Prompt sometimes generates highly unlikely polarity labels for some events. Language models can also produce biased results, which could introduce biased information to our model. Another limitation is that it may be non-trivial for researchers who want to apply our method to other NLP problems to design prompts that are effective for their task. We believe that this method should be fairly general, but it has not yet been evaluated for other tasks. Lastly, our method requires a moderate amount of computational resources, including GPU cards with substantial memory and access to large language models. As a result, groups with limited resources might find our method too computationally intensive."
106,"There are two limitations of our work: 1) As the overall loss function (12) comprises five components, we propose to directly add these components together. Although this simple summation already yields better results than the SOTA methods, we believe that it is better to tune the weights of these components based on expert knowledge, empirical experiments, or other machine-learning techniques. 2) In ZeroAE, we use three PLMs, including two BERTs and a GPT-2. Moreover, contrastive learning typically requires a relatively large batch size in order to collect a sufficient number of negative samples and achieve satisfying performance (Chen et al., 2020). The batch size in our experiments is typically 32. As a result, training ZeroAE incurs relatively large resource cost. In practice, we find that using four NVIDIA TESLA V100 GPUs with 32G memory works well, and further reducing the resources hurts the performance."
107,"PRAM effectively handles the ZRCL-NER task but has certain limitations. Firstly, since PRAM relies on the cross-lingual inference ability of mPLM, its transfer ability may be restricted if the target language is not among the pre-trained languages of mPLM. Secondly, the high memory requirements of PRAM may occurs when the task is the multisource transfer, where we need to set a large batch size to ensure the stable update of prototypes on different source languages. This drives us to enhance the space efficiency of our method in the future."
108,"Section 6
7 A2. Did you discuss any potential risks of your work? We don’t see any potential risks in your work."
109,"Regarding our work, we summarize the following three limitations.
(1) TLDT is proposed under the assumption that entities in a sentence are independent of each other, that is, they do not overlap. In the case of overlapping entities, TLDT cannot capture the label dependency of these entities.
(2) We tested TLDT on four public datasets and these datasets contain a part of the same labels. This may help TLDT to learn the general knowledge of the label dependency. We did not give experimental proof that if the label sets of the datasets all totally different, whether the TLDT can maintain good robustness.
(3) To overcome the label discrepancy problem and generate adjustable hyperparameters, we model for the individual transition score in the specific task. However, the mechanism neglects the correlation between parameters, which limits the ability to update rule learning."
110,Section 7
111,"In this work, we contribute an evaluation benchmark for classical Chinese NLP tasks. We did our best to create as comprehensive a well-defined task set as possible, something no one has done before. However, our work has several limitations due to lacking expertise knowledge and data.
When designing the tasks, we got a lot of inspiration from the middle school Chinese test paper. thousands of test papers are collected in order to ex-
tract data for NLP tasks. During the work process, we learn that it is difficult to extract a sufficient number of questions of a single type. The main difficulty is due to the variety of questions on the test papers and the mixture of the language of classical and modern Chinese. Finally, we create Xuci task, WYWRC task and IRC task from the test papers and related literature but failed to create solvable natural language inference tasks.
When working on some datasets which have less corpus, i.e, the Xuci task, we find it very difficult to calibrate existing samples or create new ones, resulting a small dataset size.
Meanwhile, the category rule we followed in the GJC task is not certified by authoritative experts, so this method is not completely reliable if viewed by experts of classical Chinese.
In this work, tasks for more aspects of grammar phenomenon are lacking. It’s expected that more classical Chinese experts and researchers join this work in the future to solve the above problems.
On the other hand, we lack a diagnostic dataset compared to other benchmarks. This is because similar data (NLI corpus generally) are even more difficult to retrieve. However, this benchmark works for NLP researchers even though the diagnostic dataset is missing. This issue is also expected to be solved in future work."
112,The limitation section on page 9.
113,"The largest limitation of this study is that our annotation pipeline is automated. This makes it possible
that there are errors in the noise annotations that we base our analysis on. Additionally, since we capture a naturally occurring noise distribution, our findings are coupled to the datasets we study here. Our findings may not generalize to distributions of noise in other datasets."
114,"Our summary generation and clustering steps have some key limitations. First, there is an API cost involved in using instruction-tuned models such as InstructGPT. It is thus preferred to use publicly available large language models, which have the additional advantage of results being reproducible by other researchers. With the recent release of large models, this is a promising avenue for further investigation. Second, it is difficult to control the granularity of steps predicted by the summary generation step as they are obtained through free-form text generation. Third, the clustering approach involves hyperparameters that need to be manually set such as the number of clusters. We believe these limitations are best addressed by tighter coupling between the different components (i.e., summary generation and clustering components which inform each other)."
115,"This paper lacks a formalized analysis of the relationship among perturbed contexts/pretraining/model generalization. Although we try to analogize pre-training and prompt in Appendix B to explain how the perturbed context works, it lacks a rigorous mathematical description.
The validation of the perturbed context is limited to relation extraction. Although we show its potential on other applications in Appendix D, the experiments are still primitive. A more systematic evaluation on different NLP tasks is still excepted."
116,"One limitation is that the efficacy of our method is shown with massive LLMs in this paper. However, we note that our method is based on only model inference with them and is already considerably cheaper than other adaptation methods.
Furthermore, our method seeks to improve LLMs: while the technology itself is ethically neutral, we acknowledge that there are various social and ethical risks of potential misuse, especially given the powerful generative capabilities of the LLMs that have become increasingly accessible to a broader audience (Weidinger et al., 2021). We argue that both the prospective end-users and researchers should be aware of these concerns when using our method in order to mitigate these risks.
Methodologically, we note that an integral component of our algorithm is self-consistency. We rely on the expectation that it reliably predicts accuracy, which essentially places an expectation that the model uncertainty should be reasonably wellcalibrated. While we have indeed found this to be the case for almost all considered tasks and models, additional investigations might be required to ascertain their general applicability. Given our reliance on self-consistency, for tasks where selfconsistency does not lead to significant gains, the performance improvements with our method may be limited. An example of this could be tasks with very small label spaces (e.g. binary classification) where “consistency” in outcomes may be achieved much more easily even if the model simply outputs random predictions. A potential remedy, for which we defer a thorough investigation to future work, is to not only consider the consistency over outcomes but also over the intermediate rationales which are generated but not currently used for selfconsistency computation.
Second, while COSP improves performance in an overwhelming majority of cases and is significantly less sensitive to the original zero-shot model performance compared to baseline methods like AutoCoT, there still exist exceptional cases where it fails to improve, especially when the tasks are too challenging in zero-shot setup – we argue that this is also due to the general, inherent limitations of the LLMs. However, both continual improvements on the foundational models and provision of some human guidance (e.g., using COSP-FS) should alleviate this issue."
117,"This work has only focussed on numerals from 10- K documents mandated by SEC. Our dataset, at present, does not include any annotated words as we focus only on numerals. It also does not include any tabular data. We also find that companies often annotate text with their custom labels which are not included in our dataset. We also find that often, it is difficult to label a numeral based on just the text of the sentence; the context might depend on surrounding paragraph, associated tables, etc. To this end, we have not benchmarked the performance using this information. However, we provide certain metadata along with the data points, including the company name, the year document was published, and the surrounding text which may be used to develop improved models."
118,"Section 6
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
119,"One limitation of this work is the lack of diversity in the downstream tasks. In our experiments, five of the tasks are named entity recognition and one is a parts-of-speech task. As OOV words make up a small portion of a sentence in the text for our downstream tasks, (the number of OOV words with valid characters ranges from 3% to 12%), analyzing the impact of higher quality OOV estimates is not trivial. For example, in document classification, the predictions depend on each word in the document, and thus the evaluation of OOV estimation will not just be based on the quality of the OOV embeddings, but also on their effect on the result compared to known embeddings. This makes assessing OOV estimation quality more challenging. As such, it is better to focus on tasks with word level output, so the quality of the OOV estimates can be directly judged. However, this limits the types of downstream tasks being analyzed. A second limitation is the fact that all tasks use the English language. As subword impact is dependent on the morphology of a language, the contri-
8For easier comparison, we report the actual values of the CRW Ablation in Appendix C.
butions of subword pretraining and attention will vary with the language. However, previous OOV works evaluate on English tasks, and as a result for comparison this paper does the same."
120,"Section 6.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
121,"Although our model can achieve better results compared to other works, there are some limitations of our model:
• Our model cannot use shared dictionary and embedding on the encoder and decoder.
• Though the encoder and decoder can use PLMs, the coordinator cannot use PLM.
• The half-layers knowledge distillation still uses additional frozen BERT."
122,"There are two limitations of this work, one of which is that our work is trained on the sequenceto-sequence model. However, we have not verified our approach on the sequence-to-edit architecture. In future work, we will verify our approach on the test bed of the sequence-to-edit model. The other limitation is that using translationese as input of data augmentation can not bring absolute improvement to grammatical error correction task. Specifically, our approach still has some room for improvement such as correcting rare words, word order, and deletion errors."
123,"Section 1, and Limitations section.
A2. Did you discuss any potential risks of your work? Not applicable. There are no potential risks associated with this paper because all tasks we used are public ones that have been verified for years."
124,"Although we have shown the robustness of multisource NMT to transcription errors in a fullsentence and simultaneous settings, our work has the following limitations:
• Our work does not address the case where the additional source, typically interpreted, is available after a delay. A delayed source may reduce the gains seen by multi-sourcing.
• We have only focused on the Local Agreement (LA-n) approach for simultaneous translation and exploration of other simultaneous approaches such as wait-k remains.
• Human evaluation of translations is pending.
• Evaluation on other language pairs is pending."
125,"We completed this evaluation based on the Beta version of ChatGPT, and the relevant results may change as OpenAI continues to improve ChatGPT. In addition, we use the ATT&CK Technique datasets collected from third-party institutions and publicly available data on the Internet. The large size of these datasets makes it difficult for us to verify their accuracy manually. Therefore, errors in these datasets may affect the conclusion of this paper."
126,"We would like to claim our limitations from two perspectives: technical-wise and application-wise.
Technical-wise: We currently only experiment with BERT-Base, BERT-Large, and RoBERTaLarge as the basic encoders. For larger language models, due to limited resources, we have not implemented them.
Application-wise: The experimental data comes from the Open Data repository released by governments of various countries. Although many domains are covered, some domain-specific data, such as biomedical, have not been considered. Furthermore, our tabular data are all from English, open data research in other languages can be considered as a future research direction."
127,"As mentioned in the previous section, up until now, there has not been a fully pre-trained seq2seq model with a BERT-style self-attention mechanism in the decoder, while the vanilla seq2seq model tends to use a left-to-right or right-to-left unidirectional self-attention. Therefore, utilizing our proposed Bidirectional Transformer Reranker (BTR) to rerank candidates from a pre-trained vanilla seq2seq model requires additional pre-training steps, which cost both time and GPU resources. Because the BTR masks and predicts only 15% of the tokens in Eq. (7), it requires more training steps than a vanilla seq2seq model. In addition, during fine-tuning, the BTR also requires additional atrain negative samples, which makes the fine-tuning longer. Furthermore, tuning atrain will be inefficient if the training is slow. In other words, training an effective BTR requires much more time than training a vanilla seq2seq model.
As a reranker, the performance of the BTR depends on the quality of candidates. There is no room for improvement by the BTR if no candidate is more grammatical than the original selection."
128,"In this research, though we employed automatic evaluation of our multi-style transferred text, we acknowledge that multi-style transfer is challenging to observe with the existing metrics for style transfer evaluation, and human evaluation should
be done as well. As this research paper focuses on exploring the impact of style distributions in the training data on style-transferred output rather than developing a superior multi-style text transfer model, we use quantitative evaluation in this iteration of our paper. We hope that the large sample size and the consistency of applied metrics make our automated approach a reasonable way of evaluating the style transfer output.
This iteration of our paper aims to achieve multistyle transfer across multiple micro styles taken into consideration together as our contribution would aid in constructing a training dataset for multiple micro-style style transfers. We did not explore another exciting question of how balancing multiple micro styles in the training dataset might influence individual style transfer, which could be a promising future direction for our study.
We acknowledge that the classifier’s quality sets an upper bound on the best style transfer accuracy that is obtainable. However, the target task is quite complicated without a parallel dataset. Our objective was not to have the most accurate classification of micro styles but to find a means to get acceptable pseudo labels for the micro styles. Individually, all our micro style classifiers had a classification accuracy of 80% F1 and higher, and we deemed this good enough for pseudo-label creation.
We also focused on utilizing the present styles in the training data and classifying them to derive inherent training style distributions instead of dynamically tuning the proportion of styles present in the training dataset. However, tuning these style proportions using techniques such as PPLM (Dathathri et al., 2019) would give us greater control over our experimental pipeline and is an appropriate next step."
129,"Despite being easily adapted to current deep learning architectures, one concern about multipleforward sampling methods is efficiency, since it has to repeat T processes to evaluate uncertainty in the stage of inference. We leave efficient variants of sampling methods for future work.
Another glaring issue is the focus on only English. Different languages may have different effects on uncertainty estimation due to e.g., distinct forms of morphology. Thus, some conclusions may vary according to the language in question. We hope that follow-up works will refine and complement our insights on a more representative sample of natural languages."
130,"In Section 3.2, we show that our training mixture with multiple-choice QA tasks, although small, is highly effective for multitask training. However, it is still unclear why multiple-choice QA tasks are particularly effective. Identifying the key factors towards positive or negative transfer from different tasks in the multitask training mixture would greatly help improve zero-shot task generalization. Future work includes investigating why certain mixtures are more effective than others and expanding the evaluation set to a wider range of tasks. Computation overhead is another noticeable limitation of semi-parametric models which is discussed in detail in Section 3.5. Moreover, future work on developing more efficient and accurate retrieval methods for retrieving from large-scale task-agnostic corpus can definitely improve semiparametric language models."
131,"The major limitations of our work are as follows: (1) We show that the neuron structure of MoE reveals the presence of modularity in pre-trained Transformers. However, the MoE structure is not the only possible modular structure. To better understand the modular structure of Transformers, we need to explore more types of structures. For example, the number of neurons in each module could be different, and the modular structure could
be hierarchical, where modules are grouped into larger modules. (2) We study three typical functions for language processing: semantic function, knowledge function, and task function. There are many other functions that could be studied, such as the syntactic function, discourse function, etc. Moreover, our categorization of functions may be not suitable for pre-trained Transformers because there are some overlaps between studied functions. A new Transformer-based function categorization may be needed. (3) We transform T5 into its MoE version to study its modular structure while not all dense pre-trained Transformers can be studied in this way because the adopted MoEfication technique (Zhang et al., 2022b) can only transform ReLU-based Transformers. Studying the modularity of other dense pre-trained Transformers, such as BERT, is also important for future research."
132,"After the conclusion.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
133,"SETI only considers veridical inference and natural inference (including both lexically-based inference and structure-based inference). However, our benchmark SETI can be flexibly extended to more varied reasoning patterns, such as negation, quantifiers, or others. In addition, we evaluate the systematicity capabilities of PLMs on semi-synthetic datasets, which are limited in language variance. Extending our benchmark on manually annotated compositional inference datasets might be a promising future work.
Recently, Hupkes et al. (2020) dissect the notion of compositionality and define five theoretically grounded tests for generalization, in a taskagonistic manner. Our work is limited to evaluating the systematicity of PLMs in textual inference. While the systematicity test is one of the most im-
portant tests, the remaining ones (e.g., productivity and localism) are still worth to be explored in future works."
134,"Section 7
7 A2. Did you discuss any potential risks of your work? We do not find any risk in our work so far. We use an open-sourced NLI dataset."
135,"There are several limitations of this work: (1) In this work, we leverage a cluster-based method for improving the representations during coarse training. Regarding the clustering, we simply perform a K-means process to obtain cluster assignment. The problem is, the cluster assignment with one-time clustering, especially based on the fine-tuned BERT model, might be incorrect. Training with incorrect cluster assignments will pull together the instances from different fine classes, thus hindering the performance. Therefore, the proposed method can be further improved by: (a) leveraging a more robust learning method that can tolerate incorrect cluster assignments; (b) Improving the clustering process to obtain more accurate cluster assignments. (2) In this work, we aim to extend existing few-shot NER to a practical yet understudied setting. However, we limit the setting to the few-shot coarse-to-
fine transfer learning setting as in previous works (Bukchin et al., 2021; An et al., 2022; Yang et al., 2021; Ni et al., 2022). In practice, there might be more complex situations that require to be explored in further works."
136,"Although we use data from dialogues, we do not model collaborative reference, i.e., we do not model continual mutual adaptation. Instead, we focus on the speaker’s adaptation to the listener in a single turn, which is certainly a simplified setup. Furthermore, our plug-and-play approach still requires the training of simulators per listener type. However, as we keep the speaker and listener models frozen and use the output obtained from them to train the simulators, this allows us to reduce the required amounts of training. We train the models from scratch using PhotoBook data and do not make use of state-of-the-art large pretrained visionand-language models that are nowadays commonly based on Transformers, which could be considered a limitation. We opted for this setup as it is more aligned with our research questions, allowing us to control the domain-specificity of the models. We also acknowledge the imbalance in the set sizes of the domains, as well as the possible lexical and visual overlaps in the samples across domains. The overlaps may facilitate the adaptation of certain sentences from one domain to another (asymmetry is not controlled in a fine-grained manner), and this is not uncommon in human communication."
137,We discuss them in the Limitations section.
138,"Computational Complexity. Under our proposed MBRD framework (see Eqn. 9), we need to make O(k2) comparisons to find the candidate with the lowest expected risk, assuming |C| = k. If we are using lightweight, non-neural alignment metrics, we might not necessarily need to worry about the overall computation time; however, if we are using a learned metric such as BLEURT as our utility function, then the generation time might be slower, especially if we have a large candidate pool.12 We leave the in-depth exploration of more efficient methods for computing or estimating the textual similarity matrix of candidates for future work.
Generative Model Quality. In practice, MBRD operates under the assumption that we have access to a good generative model. In our experiments, we primarily made use of Codex, which we know to be a model with impressive generation and in-context learning capabilities, but we also considered a few other text generators such as InstructGPT, Pegasus, BART, and T5—and we found that these models too benefit from the use of MBRD. Due to the
12If the utility function util(·, ·) used in MBRD is symmetric, that is, util(a,b) = util(b,a), ∀a,b ∈ V∗, one can then reduce the computations by half.
scope of this work, we left the investigation of the efficacy of MBRD under smaller language models such as GPT-2 and GPT-J for future work.
Choice of τ . The choice of τ for temperature sampling determines the diversity of our candidate generations. In our experiments, we set τ = 0.7 as it yielded good results across multiple datasets, but as Table 6 also indicates, there might be more effective values of τ—or even better stochastic sampling methods—for different tasks and datasets. More broadly, the choice of τ reflects the larger and partially inescapable trade-off between diversity and generation quality (Holtzman et al., 2019)
English-Language Focus. In all our experiments, the outputs were English-language texts; similarly, we used learned metrics that can be used to compare English-language texts. The general MBRD framework, however, is applicable to any language or modality. We hope to explore using multilingual tasks and alignment metrics in the future.
Impact and Potential Misuses. Large language models may exhibit a variety of biases due to their pre-training data. As with many other NLP tools and applications nowadays, it is conceivable that malicious actors might use these models to generative negative, harmful and hurtful content about certain groups of people. Our MBRD approach does not enhance or support the generation of toxic content in any way. On the contrary, MBRD can be used to eliminate biased and toxic outputs and to mitigate hallucination issues in language models if appropriate utility functions are used in Eqn. (6). Such investigations are, however, out of the scope of this present paper. We hope to investigate those dimensions in our future work."
139,"We recognize that several limitations remain with SENTECON and SENTECON+.
(1) Despite the gains in performance obtained by using a fine-tuned Mθ with SENTECON, we note that this version of SENTECON has significantly worse agreement with human evaluation than when a pre-trained Mθ is used. It is not immediately obvious why this should be the case. Although it is always possible to use SENTECON+ with a pretrained Mθ in cases where agreement with human evaluation is particularly important, future work should examine why this degradation occurs and explore whether it is possible to maintain human agreement while also seeing those same performance gains (possibly through a secondary loss term that prioritizes human agreement).
(2) When building a sentence embedding dictionary, the base lexicon of SENTECON(+) may map lexically similar sentences to the same categories, regardless of attributes like negation. Despite this, SENTECON produces meaningful repre-
sentations for sentences that require compositional understanding, which we attribute to the large number of sentences mapped to each category (recall that each contextualized word embedding mapped to a category can be viewed as a summary of all sentences in the language model pre-training corpus containing that word). For example, the number of negated sentences in the sentence embedding dictionary is far smaller than the number of non-negated sentences—and likewise for other attributes requiring compositional parsing. Consequently, each category’s centroid is still approximately an average of the non-negated sentences.
The same principle applies to SENTECON+ if a reasonably-sized reference corpus is used. If, however, only a very small reference corpus is available and the task dataset is known to require strong compositional understanding, SENTECON should be used instead of SENTECON+."
140,"The main limitation identified for our RL model is decreased performance as the vocabulary size increases. Our RL model also has a higher variance than some other topic models to which we compared. While our RL model performed well on all the data sets tested, this performance may not
generalize to different data sets. The insights from the policy dropout sweep conducted may not apply to other topic models. The performance difference for NPMI coherence compared with Bianchi et al. (2020a) may be overstated since the model in that paper used a deprecated SBERT model that produces sentence embeddings of low quality2. For the comparison to Nguyen and Luu (2021), we used slightly different preprocessing than the authors. While the model can work on any languages with associated embedding models, all data sets used in this paper were in English. Our model has additional hyperparameters compared to some other models. So, it may require more tuning and, therefore, more GPU computing. The initial model was developed on a system with 8GB of RAM and a Nvidia GTX 1060 with 3GB of VRAM for a total of approximately 100 GPU hours. Experiments using the New York Times data set were run on a system with 256GB of RAM and a Nvidia RTX 3090 for a total of approximately 100 GPU hours. All other experiments were run on a system with 128GB of RAM and a Nvidia TITAN RTX for a total of approximately 600 GPU hours."
141,"In our work, when facing long sentences, a large number of synonym candidates can decrease the convergence speed of the lower and upper bounds. Therefore, in the experiments, we set up limitations on the length of the input sentences and the number of synonym candidates. Please note that it is still feasible to process long input sentences because of the anytime nature of our tool, however doing so would increase the unverifiable region, which essentially trades the tightness of bounds for efficiency."
142,"First of all, our OASum inevitably contains inappropriate summaries not strongly correlated with certain aspects since it is automatically curated. The model trained on it could furthermore hold such misinformation and affect other downstream tasks. But we hope the large-scale training can alleviate such effects to a minimum. At the current stage, we are not responsible for any products directly built on our results. In the future, a potential denoising mechanism could be designed to further reduce the noisy summaries.
Secondly, we only opt for end-to-end extraction, which requires large computational memory and cost that may not be afforded by everyone. Thus, a meaningful direction would be investigating other extract-then-summarize two-step methods for dealing with long document summarization. Besides, our vanilla dataset contains millions of summaries that are difficult for certain researchers with limited computational resources to directly reproduce results on. We recommend using a small subset of our corpus if enough computational capability is not immediately available.
Finally, we only explore a simple strategy for controlling the summarization based on input aspects. However, we find it can not always guarantee aspect-focused generation. How to efficiently and accurately generate specific summaries by confining aspects is not only challenging for model design but also difficult for humans to evaluate. We leave these issues for future work."
143,"vMF distribution has a unit constraint. This limits the variability of latent space, which in turn reduces the gains as the number of topics increase. We can try other distributions with richer variability, such as Bivariate von Mises distribution and Kent distribution.
Also, in weakly supervised cases, vONTSS may not perform as well as those methods that leverage pretraining language models in classification. In the future, we can combine the structure of this model with existed language modeling to further improve its classification performance.
Lastly, in semi-supervised cases version, our formulation of vONTSS requires each topic to have at least one keyword. This limits its practical usage to some extent. To solve it, we can first preselect topics before doing the topics and keywords mapping, or we can modify the optimal transport loss using Gumbel distributions.
ACL 2023 Responsible NLP Checklist"
144,"The models chosen in this work are selected to represent the state-of-the-art at the time the work was conducted, and in some cases omit weaker models. For example, our exemplar selection experiments do not cover those LLMs trained with vanilla language models objectives, namely OPT and davinci, as we find their performance substantially lags code-davinci-002 and text-davinci-002. For the same reason, we only consider the substantially large language models, omitting LLMs of smaller scales (e.g., text-curie-001). Running experiments using smaller LMs or vanilla LMs may provide insights into how scale or instruction finetuning impacts the ability of LMs in learning from explanations, but our investigation mainly focus on selecting exemplars to achieve the best in-context learning performance with state-of-the-art models.
In addition, certain aspects of our approach are computationally intensive, particularly using LMbased similarity scores. However, we think this is still feasible in practice: if practitioners are deploying a real-world system, investing more computation upfront to improve its performance is likely in reach for those deploying LLMs in practice.
Finally, our experiments consider a certain subset of NLP reasoning tasks written in English. While we believe the results here transfer to other tasks in this vein which have been frequently used to evaluate LLMs, it is unknown how well they handle other languages, dialects, or genres of text such as social media data."
145,"Section 8.
7 A2. Did you discuss any potential risks of your work? This work focuses on analyzing what makes effective explanations for in-context learning. We believe that the risks of these methods are the same as the risks of broader in-context learning literature."
146,"One limitation of our work, which is also an avenue for future work, is that it is not fully understood yet why the mismatch error types help much more in some tasks than others. Trying to develop a more task or even instance-specific understanding of the benefits of mismatch error types will be very useful. We also want to try our proposed approach on a wider set of tasks, using different foundational models, and under the distribution shift setting to see if the mismatch error types as auxiliary supervision can improve robustness of natural language processing systems."
147,"In this work, we investigate the use of pre-trained language models for long-term English conversations. While we expect a modular approach may be effective for other languages when given a capable language model, it should also be noted that further research is needed to confirm the applicability of our findings to other languages. For instance, though BLOOM is trained as a multilingual language model, we only implement MPCBLOOM in English and evaluate its English capability as a open-domain dialogue agent.
Meanwhile, a modular system can create additional inference overhead or error accumulation. The system performance would become much bet-
ter if we optimally choose the LM for each module. For example, we could use GPT-3 td2 for the memory processor, while we employ OPT-175B for the utterance generator. We would need to evaluate every module to find the best model for each, which we leave to future work.
In terms of evaluation methodology, our human evaluations of MPC and its analysis face the same challenges as previous studies on evaluating interactive conversational tasks. As demonstrated by Smith et al. (2022), there is currently no definitive evaluation method for determining the best chatbot model. Additionally, there are several factors that must be taken into account during data collection and interpretation, such as annotator subjectivity, instruction bias, and crowdworker working conditions. For a more in-depth discussion of human-LM interaction, we refer the reader to Lee et al. (2022).
As described in Appendix C, to gather a diverse range of evaluations, we have collected qualitative data from two groups: English-speaking annotators on Amazon Mechanical Turk (MTurk), and qualified university students who were capable of speaking English. To some extent, this evaluation setup reduces cultural bias and platform homogeneity compared to using MTurk workers alone. However, the limitations of this approach should be acknowledged and this may further complicate the analysis when controlling for MPC’s performance on different subgroups.
Lastly, we note that running MPC requires at least as much memory as its underlying language model, making MPC infeasible to even load on a single node for heavy models such as BLOOM176B."
148,Limitations
149,"While ProToCo works well with our consistency training for improving fact verification under fewshot and zero-shot settings, our work has some limitations. Due to limited resources, currently we were unable to conduct comparison with larger PLMs and examine if extremely large models have already developed the similar or better level of consistency for fact verification on their own. In addition, our experiments show that consistency training brings improvements in both settings using only gold evidence. However, the retrieved evidence in realworld setting can be noisy and incomplete. That said, the performance of ProToCo on non-oracle evidence requires further study. To utilize consistency constraints, ProToCo still needs to fine-tune the PLMs. Also, in zero-shot setting, the labels of logical variants are assigned with the predictions of the original claim by the base model, which could be inaccurate and thus affect the consistency training."
150,
151,"In all the experiments conducted in this paper, Perspective API was used to evaluate the level of toxicity in generated outputs. While using it, two substantial limitations were noted. The first limitation related to the frequency with which the API is updated, often resulting in a different scoring for the same text across newer versions of the tool. This leads to considerable issues with reproducing results – values presented in our paper are therefore only a snapshot in time of the version of Perspective API used during experimentation. Table 7 in the Appendix shows the results found when using Self-Debiasing (λ = 10) before an update and the same test with same input and specification done after an update.
The second limitation is that while Perspective API evaluates based on the negativity in language, it does not evaluate its positivity. The ramifications of this are that when it is evaluated on neutral text, a reflection of improvements may be more difficult to arise, because while the output may be adding more positive language, if there was not any negative language to remove, then Perspective API shows it as no change in scores. Of course, these are by no means limitations of Perspective API as a tool as they are the very limitations of using a single evaluation measure for toxicity/bias, which is undoubtedly both arbitrary, but also temporally varying, as well as culturally-bound. Our own societal measures for toxicity undergo “version updates” over time. In the future, more robust testing should be performed by using multiple toxicity evaluation tools such as Moderation API4. Furthermore, work should be pursued on developing ways of including and accounting for these nuances and variations."
152,"One limitation that we observed in relation to the language model (LM) concerns the possibility that a debiasing method may depend on specific LM characteristics and may not be universally adaptable. It is crucial to clearly acknowledge this limitation. This became apparent during the development of the Instructive Debiasing method, which relies on an LM’s comprehension of context and polarity for its functioning. Interestingly, while GPT-3 exhibited these capabilities, GPT-2 seemed
4https://platform.openai.com/docs/guides/moderation/
to lack them. If a debiasing method is found to be inconsistent on the current LM, transitioning to more advanced LMs is a critical subsequent step. A unique advantage of Instructive Debiasing over other, more complex debiasing methods that modify the LM’s mechanisms, is its ease of application to closed-source models such as GPT-3, as demonstrated in our paper. We are currently exploring its applicability to successors and counterparts such as PaLM (Chowdhery et al., 2022) and ChatGPT (Schulman et al., 2022). Our preliminary experiments with ChatGPT using the Instructive Debiasing approach have yielded intriguing results, with the model persistently refusing to follow the instruction to continue the text. This behavior might represent the most significant leap in debiasing capabilities to date; after all, you can’t say something bad if you say nothing at all.
Another model limitation is the observed tendency of a language model to repeat itself when given a prompt. This was especially prominent in GPT-2 outputs but less so for GPT-3, however certain prompts did still elicit GPT-3 to show the same behaviour. Table 13 (Appendix) reveals outputs for given inputs to GPT-3 and debiased using Instructive Debiasing with nonsensical specifications, it can be clearly seen that GPT-3 will sometimes repeat the input as well."
153,"Our protocol carries with it assumptions that may not allow it to be applied to all possible debiasing methods. For example, it does not account for debiasing methods that do not use specifications or for those whose specifications do not have corresponding opposites. In future work, we are interested in exploring the adaptability of our protocol for the recent debiasing methods mentioned in Section 1 that have been proposed in parallel to our work.
Moreover, the protocol isn’t a universal tool; its application is limited to debiasing methods that employ both a mechanism and specifications with polarity. For researchers working on new debiasing methods that either don’t use specifications or employ more complex specifications that may not represent a specific polarity, we encourage them to leverage our findings (along with our shared code) and investigate potential adaptations to suit their methodology. The protocol was not only designed to be compatible with appropriate debiasing methods, but also to serve as a foundation for the
development of novel protocols.
Ethics Statement
Those who wish to use this protocol should be wary of some possible ethical implications around the usability and validation that the protocol gives to a debiasing method.
First, the protocol was designed to be used as an evaluation tool for consistency and is far from representing all tests and considerations that must be taken before deploying a debiasing method into public or private use. The use of the protocol is encouraged to gain insight into possible shortcomings of a methodology, but there are risks to this as there may be considerations and inefficiencies that the protocol does not account for. Thus, the protocol is meant for research purposes only and is not meant to be a foolproof ethical check for software deployment.
Second, the protocol was built using, and only considering, English with North American definitions of specifications. This means that any results found using the protocol, and even the protocol itself, may not work or be as effective if used with different languages or different social and cultural definitions of specifications. However, we sincerely hope that our work helps open the doors for future work into testing the effectiveness of the protocol on different languages and within different cultures and values and adapting it accordingly."
154,Section Limitations
155,"Effectiveness of Task and Model Scopes. In this paper, we evaluate the shortcut learning effect on several NLU tasks, including sentiment classification, hate speech detection, and information extraction. Our task selection is mainly based on the robustness and effectiveness of in-context learning on
2Our implementation is grounded in LIME. GitHub: https://github.com/marcotcr/lime
certain tasks. Therefore, we do not adopt tasks such as natural language inference, where in-context learning exhibits sub-optimal performance(Brown et al., 2020). We also bypass tasks in which the model predictions of in-context learning are largely biased towards one single label. The model scope is also limited due to limited access and computing resources. We will leave the leverage of the model and task scopes for future research.
Calibration of Shortcut Learning Effect. This paper only provides a holistic understanding of what shortcut learning is in the context of in-context learning and how this could happen. Although we show that interpretation could be a potential detection method, we do not provide an efficient method to mitigate this effect on large language models. We will leave it for future research."
156,"As established, medical coding is an important task for the healthcare industry. Efforts toward its successful automation have wide-ranging implications, from increasing the speed and efficiency of clinical coders while reducing their errors, saving expenses for hospitals, and ultimately improving the quality of care for patients.
However, with this goal in mind, our study presented here should be contextualized by the two main limitations that we identify and outline below.
As with other data-driven approaches, the evaluation performance discussed in our paper is limited by the choice of the (static) MIMIC-III data set. This data set could be seen as lacking diversity, as it only features a fraction of all possible ICD-9 codes and contains medical notes collected in English from a specific department of patients belonging to a specific demographic. While our approach does not make any explicit assumptions about the nature of the data other than the hierarchy of labels, in absence of formal guarantees, we cannot make rigorous statements about the efficacy of our (or indeed any related) approaches on clinical data gathered in different settings, such as other languages, countries or departments.
The second limitation is of a more practical nature, since 2015 the ICD-9 coding system is being phased out in favor of the more expressive ICD-10 system, thus ICD-9 coding has limited applications in practice. However, as with its predecessor, the ICD-10 codes are organized in an even richer hierarchy, which should enable the successful application of our proposed approach."
157,"The proposed asymmetric Shapley interaction value could estimate asymmetric feature interaction in explaining the prediction of deep models. There are two major concerns regarding the time complexity: estimation of marginal contribution and construction of hypergraphs. In computing value function we have to consider more permutations to reduce approximation errors. Also, before estimating the contribution of asymmetric interaction, interaction graph with different orders should be constructed. We could resort to effective approximation methods in computing marginal contribution and prior knowledge in building hypergraph."
158,"Our work focuses on domain generalization for abstractive summarization through prefix averaging. However, we do not experiment with larger backbone models due to computational constraints. Based on previous works we expect our approach’s performance to improve with model size. Also, a larger sequence length for prefix tuning increases the computational costs at inference.
Another limitation of our work is that we do not test it on natural language understanding tasks. This can be part of a future work."
159,Section 7
160,"One limitation of our proposed method is that it requires the pre-identification of the target MWE in a sentence before paraphrasing it, a task that is not a walk in the park. In particular, it is very challenging to identify what is the “correct” span of a given MWE, which our model critically relies on. For instance, given the MWE lip service (“insincere agreement”), our model predicts more attention as the best paraphrase, likely because the MWE is usually used as pay lip service to (something), and attention is one of the few words that fits well in this context (in terms of collocation). Therefore, the whole phrase pay lip service to should be identified as an MWE instance23 when it is used in sentences like They pay lip service to the idea; however, lip service can also serve as one lexical unit in sentences like It wasn’t just lip service. A similar problem arises when we deal with nominal MWEs that follow indefinite articles (a or an) as discussed in Section 4.1, or verbal MWEs that are often followed by specific prepositions (e.g. turn a blind eye to ... means “deliberately ignore ...”) because the MLM prediction is affected by the syntactic constraint.24 MWE span identification is also important in our sentence collection process; e.g. as discussed in Section 4.1, the phrase small fry can be used as small fry pan rather than as the MWE meaning “insignificant”, and hence collecting sentences based on string match resulted in one additional cluster that is not relevant to either its literal or idiomatic senses.
Another limitation is that our model cannot handle discontinuous MWEs such as throw someone under the bus and not ... in the least because it is not clear which parts to mask and paraphrase in such cases. Similar problems arise when continuous MWEs undergo either internal modification (e.g. go completely cold turkey) or drastic syntactic transformation (e.g. the beans are split). However, note that all of these types of expressions, as well as the pre-tokenisation problem discussed above, become a pain in the neck for any approach that regards an MWE as a lexical unit and learns its holistic embedding.
Lastly, our method heavily relies on the quality
23In fact, it is registered as such in some English dictionaries.
24In languages where words have grammatical gender such as Portuguese and Italian, this problem can be more pronounced because context words including adjectives and determiners are affected by gender.
of the clusters and is thus prone to error propagation. For instance, our model using BERT always generates large fish as the best paraphrase for the MWE big fish and fails to capture its idiomatic sense (“an important person”), likely due to its rare occurrence in monolingual corpora (compared to its literal sense). One possible solution to this problem is to derive more senses by allowing the clustering method to create more clusters with fewer instances, but that institutes a trade-off between accommodating rare senses and creating too many clusters for common senses; hence, there is no silver bullet. In fact, this problem pertains to the longstanding question (with no single correct answer) among lexicographers: how to “split” and “lump” senses of words, and how fine-grained the sense distinctions should be (Hanks, 2000, 2012)."
161,"While our Solar has demonstrated its superior performance on three benchmarks, it still has several limitations. Firstly, Solar relies on accurate recognition of image caption and object-attribute detection models. If the features of these two parts are not correctly recognized, it will cause subsequent cascading errors. Secondly, it only demonstrates that language can serve as a unified representation in multi-modal QA, but has not been tested in other more multi-modal tasks, which we will leave as future work. Lastly, the experimental results do not delve deeper into which cases a unified language representation is better and in which cases a multimodal model performs better. We speculate that an integration of language models and multi-modal models will yield better results."
162,Section 5
163,"Despite strong performance compared to few-shot our self-training methods still contain significant room for improvement compared to the fully supervised benchmarks. It would be interesting to try larger language models to see if it is possible to close this gap with more knowledge embedded into the pre-trained models. Our evaluation of free text rationales are limited by the automatic metrics, which are necessary but not sufficient to analyze quality of an explanation for decision making of the model. From example explanations (a few of which are shown in Appendix), it is evident that we still lack understanding on multiple dimensions such as, when an explanation is factually wrong, is it due to
the model believing in the wrong knowledge or is unable to retrieve the correct one. Works that probe a language model with various prompts could be useful for investigating in these directions."
164,"Our study includes some limitations that must be addressed. Some test examples might have wrong predictions made by the homograph disambiguation module. Specifically, in positive examples where lexical constraints should be imposed, its errors result in wrong corrections (i.e., the elimination of necessary lexical constraints). Table 12 shows how these erroneous corrections affect the results.
We can observe an overall decline in CSR; however, it does not hurt the translation quality. We
verify that the differences in BLEU resulting from wrong corrections are not statistically significant for all the methods. Considering the gain achieved in negative examples, as seen in Fig. 2, our proposed homograph disambiguation might serve as a useful starting point to address homographs in LNMT; however, there is still room for improvement. Our current homograph disambiguation module is designed as a stand-alone system outside the LNMT. However, building an end-to-end system can be beneficial, which can be addressed in future work."
165,"Section Limitation
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
166,"In our research, we have focused on one language pair, English↔Hokkien, and experimenting in both directions. In the future, we plan to apply the same methodology to additional unwritten languages to evaluate its broad applicability.
Our approach leverages parallel speech-to-text data between the unwritten language and a linguistically similar written language. There remains a question of whether there are unwritten languages without similar written languages."
167,"Left blank.
7 A2. Did you discuss any potential risks of your work? The speech to speech translation application has been broadly study in the field. In our work, we expand the application to a new unwritten language, Taiwanese-Hokkien to English, and except that we did not enable new application in our work. Therefore, we did not discuss potential risk of our work."
168,"The study presented in this paper has three main limitations. (1) While the design of the framework does not prohibit the utilization of longer textual forms, the two case studies presented deal with short texts. When dealing with longer text forms, we need to consider the cognitive load of having experts look at groups of instances. In our ongoing work, we employ strategies such as summarization, highlighting and other visualization techniques to deal with these challenges. (2) In the studies presented, qualitative researchers worked in groups to identify themes. Our goal in comparing two independent groups of researchers was to evaluate the degree of subjectivity by observing if the themes identified by the two groups would diverge. This setup might not always be realistic, as a lot of times qualitative researchers work independently or asynchronously. In the future, we will explore the effect
of the crowd in minimizing subjectivity, as well as the role that the computational tools play in more challenging settings. (3) Finally, we did not include a comprehensive user study to gather input from the experts about their experience with our framework. We consider this to be an important next step and we are actively working in this direction."
169,"We have studied the task of socio-cultural norm discovery based LDC2022E20 dataset, which consists of everyday situational interactions in Mandarin Chinese. Although we believe that our approach can used in other cultural settings, the current state of the model might not be generalisable to other cultures, unless further tuning is possible. Our model’s ability in discovering such norms can help to improve conversational agents, however, real-world scenarios involving duplicitous or ambiguous terms might confuse our proposed approach. In addition, our model is limited to the textual modality, and we believe incorporating audio and visual features into the model can improve identifying socio-cultural norms. Nonetheless, the reliance of our model on large-scale pre-trained language models might result in some deployment challenges in situations with limited resources. Besides, all the reported results are by fixing a random seed running all experiments once."
170,"Auxiliary attack and supports: VoteTRANS without support works well with an auxiliary attack which is the same with the target attack. In contrast, VoteTRANS with support achieves stable results with any auxiliary attack but it runs slower.
Short text and susceptible text: A short text is more difficult to detect than a long text. Susceptible text may bypass VoteTRANS as mentioned in Appendix I. However, the short text and susceptible
text are often unnatural and unclear meaning, respectively, so they are easily recognized by humans. Therefore, we recommend that humans recheck suspicious text with an abnormal ratio in the voting process of VoteTRANS (line 19 of Algorithm 1).
Beyond word-based attacks: We detect adversarial text up to word-based attacks, which change a few characters or words and are often imperceptible to humans. Other attacks remarkably affect the naturalness with a large change such as sentencebased attacks as in Iyyer et al. (2018).
Beyond text classification: We evaluate VoteTRANS on adversarial attacks targeting text classification. In contrast, the other tasks do not well-define a standard for generating adversarial text. For example, attacks targeting sequence models need to determine a threshold for BLEU score, which is aimed to minimize, but whether the score is sufficient for an adversarial text is still in question."
171,"One of our limitations is that the data is split for short-term planning and long-term planning at fixed positions which on one side shows the overall planning capability on different datasets unbiasedly but on the other hand mixes the planning ability of the datasets with the overall performance of the embeddings. We have demonstrated in section E.2 that this can lead in many cases to unplannable examples. While this means that our embeddings should overall perform better than our results suggest, in the future, we should create either a human-filtered dataset where planning is always possible or either create a human benchmark as a further baseline. Furthermore, we rely in short-term planning (transformer guidance) on the generated utterance distributions by transformers where we have to balance between semantic diversity and the likelihood of utterances. We control these with temperature and nucleus sampling (top p) and found the best
‡In tribute to our fellow researchers in the field of physics for their inspiring work on the curvature of spacetime
trade-off with a temperature of 0.8 and a top p of 0.8. Nonetheless, this can still lead to utterances that might lead to the goal but that would be not considered by humans as very likely based on the given context as we explore in E.2. Furthermore, in the next utterance selection, we utilize the publicly available checkpoints which have been evaluated in the paper (Gao et al., 2020) on DailyDialog but both were seemingly not trained on an MDC-like task-oriented corpus. Since we find that the next utterance selection based on the curved property of the context in a task-oriented setting like MDC is almost always worse than just taking the last utterance, we have not expanded experiments in this domain."
172,"The current work requires that knowledge modules be written by hand. Commonly used axioms, such as general knowledge like the commonsense law of inertia expressed by event calculus, can be reused easily, but there are vast amounts of other commonsense knowledge that are not easy to obtain. LLMs could be used to supply this information, but we have not tried. Knowledge graphs, such as Con-
ceptNet (Liu and Singh, 2004), COMET (Bosselut et al., 2019) and ATOMIC (Hwang et al., 2021), can be utilized to populate ASP rules. Like code models, we expect that LLMs could generate ASP code, which we leave for future work.
Also, when using large language models, despite various efforts, sometimes it is not understandable why they do not behave as expected."
173,"Currently, our model only exploits the direct relations between nodes in the AMR graph. In other words, only one-hop neighborhoods can be considered. However, there are a few cases where an opinion word and a related aspect word can be in a k-hop neighborhood. In the future, we will design a model that can capture long distance relations in the AMR graph. Another limitation is that the errors of the pre-trained AMR parsers and AMR alignment models are propagated to the model as a whole. What is required is to improve the performance of those modules."
174,"Section 8
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
175,"The limitations are that our use of GPT-3 sometimes generates hallucinated texts, thus reducing
the effectiveness in generating valid paraphrases. The dictionary of toned-down words could include more semantic rules or could be built automatically, which will be left as future work."
176,"Section 7
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
177,"While we do explore a range of models in the 1- 20M parameter space, our work does not constitute a complete study of downscaling. In this work,
we aimed to explore the more fundamental components of model shape, model size, and input data.
However, our findings may not generalize to other models with alternative applications of downscaling methods. Considering it to be out of scope for this study’s assessment of pre-training effects, we did not compare our results to knowledge distillation methods of similar model shape and size. Furthermore, our exploration of model shape and size was limited to a model’s hidden size, number of hidden layers, embedding size, and intermediate size, and number of attention heads as these are the most commonly-tuned hyperparameters.
Our usage of vocabulary filtration as a means of downscaling input data size may not be the most effective means of limiting input data. While shown to be effective, alternative approaches for input data manipulation such as curriculum learning, and data pruning merit study beyond the scope of this paper.
Ethics Statement
Our exploration of smaller language models presents a number of implications for accessibility, environmental impact, and cost. By exploring models in the space of 1-20M parameters, our findings can inform language modeling work for those without access to large, GPU-enabled environments. This is important as it can encourage further research work in this space by those who are otherwise unable to work with SoTA LLMs. We acknowledge that our resources enabled the breadth of study in this paper; most of this study was conducted using a single GPU. This consideration underscores our commitment to improving accessibility for under-resourced technologists throughout the world. Furthermore, in working with downscaled LLMs, we hope to encourage methods that reduce overall carbon footprint and bolster sustainable practices in NLP. These considerations are especially important given the particular burden placed on those with limited access to electricity and technology. The cost of running and experimenting with these models may prove quite costly in terms of person-hours and compute resources. As such, we hope our work at smaller scale can help lessen these burdens, and positively impact the lives of technologists, and others. Any model from the study can be trained in less than a day on a single consumer-grade GPU."
178,"We discuss some limitations of our current work which can be further explored in the future.
On Data Format. We specifically use fictional short stories as our primary data for the study since we require gold standard labels for this document classification task. Moreover, fictional short stories are easier to find as they often come with a specified grade level compared to other types of literary texts such as magazines or web articles written in any of the three Philippine languages. We do not claim that our models are able to generalize on these other types of literary materials or on other types of closely related language pairs unless a full study is conducted which is outside the scope of this work.
On Handcrafted Features. We were only able to use traditional handcrafted features covering countbased predictors such as sentence or word count and syllable pattern-based features for training the Random Forest models. We did not extract other feature sets one may find in the previous work on
English such as lexical density or discourse-based features since such features require NLP tools that are able to extract POS, named entities, relations, and discourse patterns that do not yet exist for all three Philippine languages used in this study. The work of Imperial and Ong (2021b) covered a small set of lexical features such as type–token ratio and compound word density for readability assessment in Tagalog. Still, we cannot use this approach since all languages would need to have the same number of features as is a standard practice in model training.
On Model Training. Our choice of the Random Forest algorithm for training the ARA models is based on the substantial amount of previous work supporting the application of this method to low-resource ARA, e.g., to Tagalog and Cebuano in a monolingual setup (Imperial and Ong, 2020, 2021a; Imperial, 2021; Imperial et al., 2022), where it achieved better results than other algorithms such as SVM or Logistic Regression. One can consider these algorithms for comparison but the analysis of each ARA model trained with various algorithms to the same level of depth and focus that we have given to the Random Forest classifier in the present study would require a considerable amount of time as well as a higher page limit.
On Current Measures of Mutual Intelligibility. The majority of existing literature in linguistics, specifically on the topic of mutual intelligibility in Philippine languages, discusses examples in the context of speech communication. As such, one might claim that Cebuano and Tagalog are not mutually intelligible by giving an example where a Tagalog speaker may not fully comprehend (or only recognize a few common words) another speaker if they are talking in Cebuano. While this is certainly true, in this study, we specifically focus on the mutual intelligibility of languages at a word and character level via written texts such as children’s fiction books. From this, we see a substantial degree of closeness between Tagalog, Cebuano, and Bikol compared to English. Thus, based on our results, we posit that mutual intelligibility may be used as an additional feature (see CROSSNGO in Section 4) for text-based tasks such as readability assessment. We leave the exploration of our proposed novel feature in the speech communication
area to future work."
179,"Section 7
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
180,"One potential limitation of our method is that it induces an extra cost of estimating training dynamic statistics of the data samples to characterize them (e.g., easy-to-learn or ambiguous) based on how they incorporate into the model’s learning. This may be more expensive for tasks and datasets with a large number of classes. In the future, we will focus on approaches to characterize the training examples on the fly."
181,"Certain parts of our proposed methodology, for example, template-based replacement and n-grambased prompting are applicable only when stylespecific linguistic attributes could be identified between the source and the target text. And due to the cost of human labor and the lack of publicly available client-therapist dialogues, the sample size drawn in the study is small and thus may have an impact on the conclusions drawn. Our methods have only been tested for the English language. But we believe similar methods could be applied to other languages given they have unparallel corpora tagged with Advise without Permission and Advise with Permission labels. The rephrasing methods described in this paper are tested for short sentences with a maximum sentence length of 98 tokens. Thus, the scalability of these methods for long text still remains to be tested.
When testing the rephrasers, there are some combinations that could be tried other than the ones already tested. For example, more models can be fine-tuned and tested separately on templatereplaced and retrieval-based PP and PPA corpora but incorporating generic and N-gram prompting. In this work, we first combined these two types of corpora before attempting prompting since we could observe better performance on Blender when the corpora were combined.
In order to have more data, we combined the Advise with Permission and Advise without Permission responses present in CounselChat and RED datasets. But studies show that there are differences in the language used by counselors and peers (Lahnala et al., 2021; Mousavi et al., 2021). So, there can be linguistic differences between the same type of response in CounselChat and RED datasets. Future work should attempt to identify these differences and ideally rephrase the responses given by peers to reflect the language of the counselors."
182,"Although Gender-tuning succeeds in reducing the gender bias scores in the pre-trained language models, there are some limitations to performing debiasing. Gender-tuning only works on gender-related words list. Thus Gender-tuning cannot cover the probable gender biases that do not exist in its’ list. We defer the gender-related word list modification to future research. All our experiments ran on English language texts with English gender-word morphology."
183,"We summarize the limitations of our method as follows: (1) TextObfuscator was designed to protect word privacy in the inference phase, and we did not verify its ability to preserve other privacy attributes and training phase privacy. (2) Although we have done empirical experiments and visualizations to demonstrate the effectiveness of our method, a mathematical proof would enhance its privacy guarantees. (3) Our method requires more training steps than fine-tuning, resulting in an increased computational cost."
184,"Task configurations are entangled with the full model parameters. In our ablation study of task configurations at training time (Table 3), we see that when training without task type, the model fails to generalize to FETAQA. Upon examining the model output, we find that although we change the output configuration to “long answer”, the model still produces a short-form answer. This indicates that model behaviors are not always aligned with a single configuration, leading us to question the extent to which each individual configuration influences the model. In order to have better and more interpretable control over the models, one potential avenue for future research is to develop pluggable task configurations, where each configuration controls a more atomic function of the model and can be plugged, unplugged, and combined to yield different model behaviors.
Our exploration scope is limited to table-to-text tasks. Due to the constraints of the computational resources, we haven’t explored joint training with a broader range of other NLP tasks. We think with some modifications, such as the inclusion of dataset domains in the configuration set, it would be possible to extend our approach to additional datasets and tasks."
185,Section 7
186,"While in-context learning methods for DST are promising in their data efficiency and flexibility to new domains, they typically require very large models to perform effectively. At 175 billion parameters, OpenAI Codex (Chen et al., 2021) is much larger than some of the fine-tuned approaches to DST, though with better performance and ability to adapt to new domains without re-training. Despite our advances, there are still significant errors when applying ICL for DST. As such, ICL may not necessarily be relied on in safety-critical settings."
187,"The major limitations of this work are:
• We show results on two public datasets, from bio-medical and bio-chemical domains. These results may not generalize to other domains.
• Our results indicate benefit in low resource settings, while no appreciable benefit is seen for medium or high resource settings.
• Our method relies on GPT-2, a large language model that needs humongous compute resources and a long training time. It takes about 2 hours to generate 50 samples, versus the baselines like vanilla GPT-2 (ECG-LM) taking 30 mins or EntInj taking about 10 mins to generate same number of examples with much less memory requirements.
• We use quantitative measures to evaluate the quality of text generation, which might not be enough to capture the quality of generated text. Gold standard of measuring the quality is human evaluation, which is expensive."
188,"Authoring transduction rules is relatively easy but may still be labor-intensive for complex domains. Future work might explore (semi-)automatically deriving transduction rules from data, learning to synthesize them from domain specifications, or curating a collection of domain-general transduction rules that can be imported into new domains.
Our experiments in this paper generated text only in English. It would be interesting to apply the framework to datasets in other languages, e.g., GlobalWoZ (Ding et al., 2022). While our framework is intended to be agnostic to the output language, our notation for response templates might need to be slightly extended (along the lines of Appendix A) to be more convenient to use with morphologically complex languages or free-wordorder languages. In these settings, presumably, the QCFG should systematically generate many inflections or orderings for the LM to choose among.
To support multilingual dialogue systems, future work could consider automatically translating the response templates into additional languages—perhaps by working backwards from automatic translations of natural language responses that use those templates.
Relatedly, we have only tested the proposed approach on dataflow graphs. Future work could apply the method to generate textual descriptions of other graph-structured inputs, such as graph databases or abstract meaning representation (AMR) graphs.
While QCFG productions were unweighted in this paper, giving them weights would allow the QCFG to express its own preferences about which productions to use for a given input. For example, in a product-of-experts architecture, the probability of a given response y, would be proportional to the LM probability of y times the weights of all productions used in the QCFG derivation of y (summed over all such derivations). Beam search (§4) could then be carried out using prefix weights (Opedal et al., 2023). The weights could be trained using gold responses.
Weighting the QCFG raises the possibility that the dataflow transduction rules could encode pragmatic context-dependent policies. For example, a dataflow transduction rule could call a neural network to assess the suitability of applying the rule to a given node in the dataflow graph, and then weight the resulting QCFG production ac-
cordingly.
Ethics Statement
Our proposed approach strongly outperforms a purely neural model at truthfully describing the result of a computation and its provenance. However, our approach can still make pragmatically unhelpful omissions, making it potentially risky to deploy in some scenarios. Additionally, we leverage pre-trained neural language models such as CodeT5, and as such, we acknowledge that our approach might inherit some biases present in these pre-trained models."
189,"Section 8
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
190,"In the current work, we adapt one-step gradient descent training for the outer loop based on our bi-level optimization framework. Since this outer loop optimization doesn’t have a closed-form solution, determining how many steps to perform for the outer loop for better outer optimization is still important to explore."
191,"section 7
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
192,"Recent work has shown that models of a certain size (upwards of 3B parameters) exhibit learning properties that cannot be observed in smaller models. Due to practical limitations and environmental concerns, in this study we chose not to train models larger than T5-Large. It is thus not possible to know how emergent properties in larger models may have affected the comparison between the different approaches compared here. We believe that our findings will nevertheless be useful to NLP practitioners who operate on a constrained compute budget and may thus opt for moderately-sized models anyway.
We compare encoder-only and encoder-decoder models for multi-label classification. Decoder-only models (Radford et al., 2019) are omitted since at present there are no decoder-only methods for multi-label classification in the literature. While we could have adapted the Seq2Seq approach in our experiments to operate in a decoder-only context, we deem this unsuitable for the datasets we work with, as they contain long documents which will quickly cause problems for standard decoder-only models like GPT-2.
Domain-specific pre-trained language models exist for both the legal and biomedical domain, which outperform their generic counterparts when used for classification tasks. These models all have an encoder-only architecture, however, which renders them unsuitable for a comparison of encoderonly and encoder-decoder approaches to multilabel classification.
Our experiments consider datasets from the legal and biomedical domains first and foremost because there are publicly available datasets with hierarchical labelling in these domains, unlike others. Moreover, we believe that working in critical application domains is a worthy purpose and covering two such domains with two different datasets in each domain gives us a good view on how the examined methods are expected to work in such domains."
193,"For now, the superiority of the proposed two-stage inference speed-up method cannot adapt to inductive datasets since the generated sequences are difficult to map to unseen entities. Therefore, we will explore how to efficiently perform KGC under the inductive setting in the future.
Like the other text-based KGC methods, our GHN lag behind embedding-based methods on FB15k-237 dataset. Cao et al. (2021) claims that many links in the FB15k-237 dataset are not predictable based on the information in the KG and we hypothesize this may harm the training of textbased models. In the future, we intend to examine this more thoroughly."
194,Yes. We have. It’s not a numbered section and comes right after the conclusion.
195,"The main limitation of HRT is that its upper bound on the inference speedup is lower than that of single-iteration NAT under the same network architecture. As demonstrated in Appendix A, the average speedup of single-iteration NAT (i.g., CMLM1) is 4.7x/3.2x on GPU/CPU, respectively, while that of HRT is 1.7x/1.6x. To achieve higher acceleration, HRT needs to employ the deep-encodershallow-decoder architecture. Increasing the chunk size is a simple way to reduce the autoregressive cost, yet it results in severe BLEU degradation (see Table 5). Further research should be conducted to maintain high translation performance with fewer autoregressive prompts."
196,"In this work, we have been focusing on improving the performance of tagging-based Grammatical Error Correction. Our work has the following limitations: (1) We work on three recent Chinese Grammatical Error Correction datasets. But there are many emerging datasets from various languages. We will add support for these languages on our GitHub repository and make all resources publicly accessible. (2) We point out a limitation of inference tweaking, but it remains to be explored how to
explain the phenomenon and derive better tweaking methods."
197,"In this work, we only consider the pre-train then fine-tune paradigm which assumes that model weights are tuned for adaptation to specific tasks. Future work, once more capable multilingual LLMs are released, may also consider the few shot, and in-context learning-based setups to accommodate for more recent approaches towards adaptation in NLP. Future work may also consider setups more relevant to different, more diverse tasks (e.g. including webtext)."
198,"This paper proposed a novel pre-training model COMAVE which aims at textual AVE tasks, while in this field, multi-modal AVE tasks also widely exist in many e-commerce platforms. We expect that the following works can leverage COMAVE as a powerful word embedding pre-training model for text encoding combined with image feature representation in multi-modal AVE tasks in the future. Meanwhile, the same as the previous AVE works, we assume that each T is an independent extraction object, without considering the context-dependent of the whole data resources, such as long documents and instructions, which exceeds the length of an allowable single input."
199,Section 7: Limitations
200,"We acknowledge that the StereoSet and CrowS datasets and metrics are not ideal evaluation measures for debiasing work (see Blodgett et al. (2021) for more details about their pitfalls). We advise practitioners to conduct careful analysis for their specific use case rather than interpreting the scores from our experiments as clear measures of bias mitigation or removal.
Furthermore, we realize that in discussion of harms, we should also ensure that allocative harms do not arise from dependency on a PCGU-debiased model. In this paper, we do not report experiments on models finetuned for other downstream tasks, as finetuning is generally more prone to spurious correlations and accidentally encoding bias, so evaluating such models obfuscates the procedure’s effect on the pretrained model. Instead, we focused only on the masked language modeling task such that intrinsic and extrinsic evaluations both use the pretrained model directly and only. In the modern age of large language models, this is arguably more applicable, but this setting doesn’t take into account the effects of prompts on the prediction distribution. An interesting extension of this study would be to debias using some form of PCGU in the pure generation setting and evaluating with high quality generation-based resources such as HolisticBias (Smith et al., 2022). However, the base form of PCGU is not directly applicable due to the difficulty in attaining and using minimal pairs/tuples in generations.
Another related limitation is that our experiments were only conducted in English. However, many languages, such as Spanish or other Romance languages, have a much richer concept of grammatical/lexical gender sometimes affecting multiple words per sentence.
Unfortunately, a fundamental problem with interpretability arises if we wish to evaluate the language model’s bias implicitly. For example, the prediction in Figure 2 suggests that the debiased model is less biased than a model predicting the full probability mass for the female term. Discrete metrics fail to account for this behavior, so better evaluation metrics would also give us a better sense of the efficacy of our proposed method.
We further note that gender, which has historically been treated as a binary construct, is likely to be a relatively easy domain to work with. Other more complicated social biases like racism and
classism are similarly harmful, and an ideal debiasing procedure should work for all of them. Similar questions may arise about if we can ever comprehensively cover all domains without a better way to generalize across domains. It is also to be seen if PCGU can be directly used for other domains, as our experiments only touched on the intersection of gender and profession biases while observing that this has effects on other domains. Further work would be required to understand why, and in what contexts, PCGU can affect unseen domains; are the cross-domain results in the main paper artifacts of intersectionality (between seen and unseen domains) or is this truly generalizations across a broader notion of bias?
Due to the complexity of social bias, it is not obvious if a properly modeled dataset for such other domains of bias can be easily constructed for usage with PCGU. A natural thought would be to attempt to generate training data for PCGU. We attempted this but found that the generations were not reliable in terms of providing signal for what constituted bias. By using a templated dataset like WinoGender, we can ensure that every instance in the training set encodes bias by an assumption of gender based on only the profession.
Obviously, partitioning at the most granular level where each single parameter is its own part would make our directional comparison meaningless. However, we did not extensively study how important the specific partitioning method was. An interesting class of experiments would be using some sort of random partitioning, where each individual parameter is assigned to its group of parameters not according to any architectural reason but according to some sort of randomness. Our implementation of this made the gradient selection extremely expensive because it required too much indexing into tensors as opposed to a full replacement of specific dimensions. A better implementation or experiment would be needed to draw actionable conclusions about different partitioning methods. However, our baseline experiments for this matched with the intuition that sampling each weight as being a bias or non-bias weight using a Bernoulli distribution yields a similar effect as regular training with dropout, similar to the k=All experiments in Table 2."
201,See the Limitation part.
202,"One of the main limitations is that we focus solely on the IETF. Consequently, we can never be completely sure how well our findings generalize to other similar organizations without further annotation.
We are also limited by not conducting a hyperparameter search on our models. We omit this step as the main goal is not maximizing performance, but rather data annotation and analysis. In a similar vein, it is likely possible to increase performance by using a more advanced model that is either trained on dialogue-like data or is specifically designed to exploit phenomena specific to dialogue (e.g., having speaker embeddings).
We also acknowledge that many emails are longer than 512 tokens which is the limit of our BERT model and thus might have been cut short. However, most of the emails do fit into this limit."
203,"We point out the limitations of large language models (costly to train, deploy, maintain, hallucinate, opaque). The vision of POSTTEXT shows promise of less costly training, maintenance, and more explainability. However, no actual system is built yet to validate these claims and it is also not clear that a system with POSTTEXT architecture will be easier to deploy since it has more components."
204,"Limitations to our work are as follows: (1) We only study the three magnitude effects for the number word and digit denotations of the numbers 1 to 9. The effects for the number 0, numbers greater than 10, decimal numbers, negative numbers, etc. are beyond the scope of this study. Future work can design behavioral benchmark for evaluating whether LLMs shows these effects for these other number classes. (2) The mapping of LLM behaviors to human behaviors and effects might vary for each effect. Thus, we might require a different linking hypothesis for each such effect. (3) We only use the models built for English tasks and do not evaluate multi-lingual models. (4) We report and analyze aggregated scores across different dimensions. There can be some information loss in this aggregation. (5) Our choice of models is limited by certain resource constraints. Future works can explore the use of other foundation / super-large models (1B parameters +) and API-based models like GPT3 and OPT3. (6) The behavioral analysis of this study is one-way: we look for human performance characteristics and behaviors in LLMs. Future research can utilize LLMs to discover new numerical effects and look for the corresponding performance characteristics in humans. This could spur new research in cognitive science. (7) The results show similar outputs to low dimensional human output and show that we do not need explicit neural circuitry for number understanding. We do not suggest models actually are humanlike in how they process numbers."
205,"PRAGMATICQA is collected via crowdsourcing on English-language material from Fandom.com, where community-maintained wiki pages are used as reading materials and basis for answering questions. Therefore, it cannot be guaranteed that the excerpts from Fandom will be factually correct or stay unchanged over time, and in turn the answers in PRAGMATICQA are also not factually verified. Furthermore, techniques or models developed on PRAGMATICQA might not be generally applicable to non-English languages or non-entertainment topics without further adjustment or evaluation.
More importantly, the crowd workers that participated in PRAGMATICQA are geographically limited to primarily English-speaking countries, and therefore might not represent typical pragmatic reasoning behaviors of people that speak different first languages or come from different cultural backgrounds. Therefore, it should not be treated as a universal standard for pragmatic reasoning in information-seeking conversations, but rather a single reference point."
206,"An important limitation of our work concerns the definition of the protected attributes in the datasets used for evaluation. In particular, gender in BIOS and PAN16 is limited to the binary female/male, lacking an inclusive and nuanced definition of gender. Similarly in FDCL18, we consider only two dialects of African American and White American, while clearly this definition is limited and noninclusive. Furthermore as in previous work (Sap et al., 2019; Ravfogel et al., 2020; Zhang et al., 2021), the labels of this protected attribute are assigned through a probabilistic model, and hence the dataset might not represent the nuances and traits of the real-world.
The second limitation regards reaching strong conclusions on the generalizability of the multiattribute setting for MODDIFFY over any possible number of protected attributes or subset of them. Our multi-attribute experiments are conducted on one dataset with two attributes of gender and age, particularly due to the lack of available suitable datasets. Hence, Further studies (as well as more suitable datasets) are required for achieving a more comprehensive picture on the topic.
Finally, we should also highlight two general limitations, shared with the other related studies in the area of model bias mitigation. First, we should consider that the aim of representation disentanglement optimizations is to reduce the existing correlations in the model with the protected attributes based on the observed data. These data-oriented approaches might lack effective generalization, particularly when the model is evaluated in other domains or out-of-distribution data. Second, our bias mitigation evaluation is grounded in the notion of fairness through blindness, and the debiasing optimization methods are designed to support this form of fairness. The effects of our method on other possible definitions of fairness are therefore left for future work."
207,Left blank.
208,"Even though we performed a rigorous literature search to try to cover all existing work on scientific fact-checking, there is possibly work that was left uncovered due to different keywords, naming conventions (e.g., fact-checking vs. claim verification). Whenever possible, we tried covering all related work and all relevant cited papers.
All approaches for automated scientific factchecking described in this work are still not safe for widespread adoption in practice due to constraints to their performance. Having deployed automated fact-checking systems that would produce incorrect verdicts could lead to mistrust in their usefulness and the process of fact-checking itself, including the work of dedicated manual fact-checkers."
209,"One major limitation of Uni-Encoder is its suitability only for generation-based dialogue systems in which the number of responses is small. A twostage approach is necessary for retrieval-based systems: Context-independent encoding methods like Poly-Encoder first filter out a small set of candidates from the large pool, then Uni-Encoder can pick out the best response from the pre-filtered collection. Moreover, as discussed in Section 5, Uni-Encoder could be a good component of the RLHF approach. However, the increasing research of pure generation methods with alignments bakedin (Arora et al., 2022; Liu et al., 2023) may gradually replace the SFT+RL method. Consequently, Uni-Encoder will have a smaller and smaller impact in terms of application. Nevertheless, because Uni-Encoder unified all other ranking paradigms, we believe it remains helpful even as a theoretical framework."
210,Please refer to Section 6
211,"Our proposed method is an offline system in which the input is a dialogue containing all utterances rather than a single utterance input in chronological order. An online system for emotion recognition can be applied in real-time conference systems or human-computer interaction, so the online system has potential value for future research. Our method can be built into online systems by creating buffer systems such as history windows. However, all the baseline methods in the past are offline systems, such as COGMEN, DialogueRNN, etc. In addition, the form of datasets also leads us to construct an offline system for training and testing. On the other hand, the offline system also has application scenarios such as analyzing emotions of posted videos, opinion mining in social media, etc. Therefore, our method only builds an offline system under the offline experimental setting that can be compared and evaluated.
Besides, the input of our method is feature-based. The original text, audio, and video files will first pass through feature extractors to obtain multimodal features, which may cause information loss and hurt performance. We focus on feature-based training methods because training based on the original files costs a lot. For example, training a video encoder generally requires several V100 GPUs and days of training time. Therefore, we, including the baseline methods we compare, adapt the feature-based training methods. When the cost permits, training based on source files is worth exploring in future work. With feature-based training methods, different baseline methods use feature extractors to obtain features, leading to a lack of fairness in method comparison. In this regard, we reimplemented all open-source methods and compared them using a unified feature file to ensure the fairness of the experimental results. At the same time, we also conducted evaluations with different signature files to verify the generalization of the method."
212,Section 6.
213,"One limitation of our method is that it requires multiple generations to achieve the best performance on stance detection datasets. While the best student model significantly outperforms strong baselines, it takes longer training time and requires extra memory for the teacher model. This is a common limitation for knowledge distillation in generations. Another limitation of our method is that the improvements brought by knowledge distillation saturate after a few generations, which can be also observed in previous work. We will explore how to improve the performance saturation in the future."
214,Limitation Section after the Conclusion.
215,"As shown in Table 1, an overwhelming number of catering reviews on Yelp makes the countertemplates with obvious catering information. For example, “This is a great place for a quick bite to eat. The food is delicious and the staff is very friendly. They have a good selection of beer and wine. The place is always busy, but it’s worth the wait.” In this case, the pattern information in the text is not consistent with other businesses irrelevant to catering. As shown in Table 6, although our method has a slight improvement over the previous methods in the mean and standard deviation, it is only comparable to the SOTA at the best performance. Since the counter-template is exactly the same text for the whole data set, the performance of the model is affected perhaps when the pattern information from different texts in the data set has large differences. When we extract the restaurantrelated parts of the dataset as Yelp-Res that have more similar pattern information, our model performs better."
216,"Inapplicability to reference-free evaluation: Since our MRE supposes that there is an available reference question to be augmented (paraphrased), it is not applicable to reference-free question evaluations such as QRelScore (Wang et al., 2022a) and RQUGE (Mohammadshahi et al., 2022).
Inapplicability for answer-unconditional QG frameworks: MRE can’t be applied to answerunconditional QG frameworks because it only augments the reference question by paraphrasing without considering other possible questions of supposing other answers.
Large computations: To generate multi-reference questions, our method requires inference of large language models, which results in huge computational costs. Therefore, this can become burdensome as the test dataset grows."
217,"In this paper, the proposed XtremeCLIP framework is mainly focused on CLIP-based deterministic VLU tasks. In future work, we will extend XtremeCLIP to other Pre-trained Vision-Language models and apply XtremeCLIP to generative tasks such as image captioning, visual grounding or visual relation extraction."
218,"Despite the significant advancements made by the proposed FACTUAL-MR representation in addressing the limitations of current scene graph parsing datasets, there remain several areas for future research.
First, FACTUAL-MR currently relies on heuristic rules to resolve the collective-distributive ambiguity as introduced in Section 4.2. However, the limitations still remain due to the ambiguity of language. To obtain a perfect parser, rich-world knowledge from multi-modalities or textual context (Li et al., 2020) is required, which is left as our future work.
Second, there is currently no explicit alignment between objects represented within FACTUALMR and the corresponding bounding boxes in the image. To fully utilize multi-modal information, collecting such alignments may be necessary.
Third, the proposed method utilizes ORACLE scene graphs of the image, however, in practical applications, extracting a scene graph from an image remains a challenging problem. Further research is required to determine if utilizing a visual scene graph parsing model to extract scene graphs from images would negatively impact image retrieval performance.
Lastly, our current approach utilizes a large pretrained language model to train the parser. However, the issue of robustness in parsers (Huang et al., 2021; Zhuo et al., 2023) has always been a significant concern. The captions in the VG dataset mainly consist of short sentences with simple patterns. It remains unclear whether the parser is robust enough to handle sentences with more complex linguistic variations, which calls for further investigation."
219,"For the Top K operation in target relation calculation, we set K’s three hyperparameters (i.e., ks, kt and k) to determine the number of the related targets. To explore the influence of the selection of K on model performance, a grid search on these three hyperparameters needs to be conducted to iterate each combination. However, due to the time and resource limits, we explore the impact of one hyperparameter in K by controlling the other two hyperparameters. Based on the empirical findings from this, we then set the value of K so as to achieve an appropriate performance."
220,"We have identified several limitations in our work and propose future directions to improve them:
(i) The sources for UR-QA in this paper are limited to the document corpus and QA-history, but our unified reader is not restricted to take specific sources. Further research can explore the generalizability of UR-QA to more diverse sources, such as linearized knowledge sources as proposed in (Oguz et al., 2022). Future work can also explore the optimal method for considering LM likelihood, answerability, and consistency together.
(ii) Though it is not the focus of this work to optimize readers, our proposed UR-QA can orthogonally benefit from improvement in retrieval. Further study on the retrieval for UR-QA can be conducted, including the direction to co-optimize the reader and retriever as proposed in (Izacard and Grave, 2020)."
221,"Although our model allows users to interpret which parts of the input document are most relevant to the model’s prediction, our model does not allow users to interpret which text spans of the input summary contain errors. We use the summary in Table 4 as an example. If the model can indicate the text span “a school in northern ireland” contains errors, it will be easier for users to correct the summary, potentially benefiting factual error correction systems (Fabbri et al., 2022a; Huang et al., 2023). Kryscinski et al. (2020) introduced an auxiliary task to extract erroneous text spans in summaries, but their method requires expensive text span ground-truth labels. Locating incorrect text spans in the summaries without requiring spanlevel training labels remains unexplored. Another limitation of our model is that it does not allow users to interpret the uncertainty of the prediction results (Deutsch et al., 2021)."
222,"Additional unnumbered section after Section 8 Conclusion
7 A2. Did you discuss any potential risks of your work? We provide a framework to correct for errors made by other automated systems. The application scope is extremely limited, as such we expect very low potential for malicious use compared to other works."
223,"One limitation of CoT-KA is that it performs finetuning based on the PLMs, and the input sequence length limit of the PLMs allows us to add only a limited number of CoTs. Therefore, it is important to explore and develop a CoT selection strategy in future research. A good CoT selection strategy would enable the identification of highly effective CoTs from a set of CoTs, enhancing the efficiency of KADL."
224,"In this paper, we focus on the hybrid QA task, where the answers to most questions can be extracted from cell values in tables and linked passages using a reading comprehension model. Although TACR performs well in cell selection, one of its limitations is that it lacks numerical reasoning
ability across different cells, such as counting and comparing. To enable TACR to answer numerical questions, we will further develop its numerical reasoning capabilities in future work. Another limitation of TACR is that it shows a strong ability in column selection while performing relatively worse in row selection. For future work, we plan to try to improve its row-selection accuracy."
225,section 6
226,"Cross-Cultural Inference Beyond Codenames Our work explores sociocultural pragmatic inference in a very limited setting, using a core vocabulary of just 100 words. Despite this limitation, we find significant diversity in our dataset; furthermore, our models successfully capture these diverse inferences. While a limitation of our work is its focus on a single setting, we expect domains outside of Codenames to see similar variance. Understanding and highlighting miscommunication in dialog—due to culture-dependent misinterpretation—is one such extension. These domains are likely much nosier than Codenames; we urge future work to further investigate them.
Spurious Correlations across Sociocultural Factors Across all tasks but one (Target Rationale Generation §4.1.3), jointly modeling all sociocultural priors does not result in the highest performing model. Because our sociocultural factors already correlate with each other (§3.4), we suspect that modeling all features may be redundant, adding spurious correlations and resulting in overfitting. Improved modeling methodology and careful regularization may address these issues; we leave these experiments for future work.
Bigger Models and Task Specific Modeling Currently, we evaluate small Seq2Seq models due to computational constraints; however, evaluation of 0-shot and few-shot performance on larger language models (e.g. GPT-3) is necessary. Given the changing state of the Codenames board—along with evidence that LLMs struggle with theory-ofmind-esque perspective taking (Sap et al., 2022)— our dataset can serve as a challenging benchmark for sociocultural understanding. However, successfully encoding game state into prompts for LLMs may require experimentation.
Finally, our current task formulation and modeling setup are straightforward: we simply encode all information in-context and do not assume recursive reasoning like in RSA (Goodman and Frank, 2016). Future work can explore these directions.
Human Evaluations Our evaluation is limited to automatic metrics and qualitative analysis. Evaluating cross cultural generation depends on the evaluator’s own culture. Each generation depends on the player’s sociocultural background; finding evaluators who match the player may be prohibitive."
227,Section 8
228,"Since our generative approach to product attributevalue identification autoregressively decodes a set of attribute-value pairs as a sequence, the inference is slow (Table 5) and how to linearize the set of attribute-value pairs in the training data will affect the performance (Table 6). The best way of composing an attribute-value pair and ordering the pairs will depend on the characteristics of the datasets such as the existence of canonicalized values and the number of attribute-value pairs per example. Those who attempt to apply our method to their own datasets should keep this in mind."
229,"Although these experiments were performed on only one dataset, it is indeed large with data from 82 participants. That said, it will be nice to perform experiments with more listening datasets.
We experiment with a linear encoder – Ridge regression. We plan to experiment with more complex encoders as part of future work.
This work was done on data related to English stories only. Several other languages belong to the same language family as English (Malik-Moraleda et al., 2022). While we can expect the insights
and learnings to hold across languages in the same language family as English, empirical validation needs to be done. For languages in other language families, syntactic structure may be very different from English. Hence, more work needs to be done to check which of these insights hold for datasets in other language families.
This work was conducted on a dataset where the participants were involved in the listening task. However, the stimuli was represented in the text form. We believe that an audio form of the stimuli can lead to improved insights. Thus, more work needs to be done to design representations (like prosodic features) for auditory stimuli."
230,"In our experiments, as NRMs with cross-encoder are widely used, we focus on evaluating the textual adversarial robustness during the re-ranking stage and do not currently take into account the effect on the retrieval stage. But actually, in a “first retrieval then re-ranking” ranking paradigm, the attack is effective only when the adversarial documents are passed into the top retrieval results. Meanwhile, dense retrieval (DR) models have been widely studied, and they may also inherit adversarial vulnerabilities due to the basics of PLMs. Besides, due to limitations in our computing resources, we only tested adding adversarial text to relatively short documents (i.e., passage-level), but the document content in real-world applications could be much longer. Therefore, further comprehensive investigations on examining the NRMs with different architectures, the effects of attacks on the retrieval models, and the manipulations on longer documents are left for future work. Finally, it is important to note that mitigation and defense methods against adversarial ranking attacks are currently understudied, making it a significant area for future research."
231,"Although our proposed method exhibits great performance to generate more smooth and natural emotional support than baseline models, we argue that the research on this field still has a long way to go. We conclude three aspects that may inspire further exploration. First, the automatically annotated emotion labels may be a little bit coarse and may not accurately manifest the emotional states of the seeker. Second, since various types of commonsense knowledge are not introduced, the current chatbots always generate general and safe responses, failing to provide specific and personalized suggestions to help the seeker get over the dilemma. Finally, current automatic evaluation metrics are still not rational and proper to measure the ability of chabots to provide emotional support. It is desirable to build better evaluation metrics for this."
232,Section 8
233,"Despite the simplicity and strong empirical results, RESIDUAL PROMPT TUNING still has few limitations. First, its performance is still not on par with fine-tuning on (e.g. 7.8 points difference with T5L model and 100-token prompt on SuperGLUE average score). Also, our method uses slightly more parameters than prompt tuning to train the reparameterization network. However, this is not a significant limitation given the full language model size.
We have tried to cover several model architectures, but so far we have focused on encoder-decoder (T5) and encoder-only (BERT) models. In future work, we would like to investigate decoder-only methods (e.g. GPT). Another limitation is that our method (similarly to other prompt tuning-based methods) strives to reduce the number of trainable parameters, but uses a longer sequence than the original input text (due to the injected prompt)."
234,"Our model currently requires high-quality word boundaries for both speech and sign videos. However, as demonstrated by our preliminary results in Table 5, we can overcome such limitations by incorporating more powerful unsupervised segmentation algorithms to our system. Further, while our dataset is sufficient to model the variability in speech and videos, all experiments to date have assumed that spoken and signed sentences share similar word order, which may not be true of natural spoken and signed communications. A future direction of this research will seek to develop methods for spoken-sign language pairs with very different syntactic structures. Lastly, the vocabulary size under our study on word-level SSR-U is relatively small (<1000), and a promising future direction is to extend the current approach to deal with much larger vocabulary size in more diverse conversations."
235,"Our work is not without its limitations. First of all, our annotated data is relatively small. However, given the relatively straightforward task (as reflected in high IAA), and since we are using this data only for evaluations, we believe that this amount of data is sufficient for the research questions we are asking/answering in this paper. Second, our data entirely comes from the politics domain and social media, situated in the US context. This choice was driven by our downstream use case of a large scale social science analysis in the US political domain. While we have not established how well our tagger performs in domains other than politics, given that our tagger relies on contextualized language models trained on web data and since it is performing a basic linguistic task, we believe that the performance is robust across domains used in Section 3 and 4. However, we expect performance degradation with genre or dialectal shifts with substantial differences in syntactic patterns. Third, we have not fully exploited the utility of the dataset in this work. As mentioned in Section 2.2, our aim in this paper is not to build the best tagger possible, and hence we did not explore state of the art modeling techniques such as few-shot learning. Finally, our work is done entirely on English language data. While we believe that similar approach could work in other languages without vocative markers, more research need to be performed to verify that. While we acknowledge these limitations, we reiterate that these are outside the scope of what could be meaningfully done within this short paper."
236,Left blank.
237,"Limitation section and Appendix H.
A2. Did you discuss any potential risks of your work? Not applicable. We provide an evaluation metric for QG task. limitations are provided in Conlusion and Limitations sections."
238,"Language-related limitations. For the ease of the analysis, we conducted experiments using only the English dataset in this study. Although our proposed method can be applied to any language, its performance must be evaluated on languages other than English. For example, the SemEval2020 Task 1 dataset includes Latin, German, and Swedish language datasets, in addition to English, and can be used for this purpose. In particular, our proposed method requires only pretrained MLMs and does not require additional training data for the target languages, which makes it easily scalable to many languages.
Availability of MLMs for the target language. Experimental results show that the quality of the MLM is an important factor determining the performance of the proposed method. For example, the proposed method reports good performance with vanilla BERT model in Table 2 but further gains in performance can be obtained with the fine-tuned BERT model on masked time stamps. However, since our method assumes the availability of pretrained MLMs, a problem arises when trying to adapt our method to minor languages where no pretrained MLMs are available. This limitation could be mitigated to an extent by using multilingual MLMs. For example, Arefyev and Zhikov (2020) demonstrated that satisfactory levels of accuracies can be obtained for semantic change detection by using multilingual MLMs. Our proposed method can further benefit from the fact that new and larger MLMs are being publicly released for many languages in the NLP community."
239,"Section Limitation
3 A2. Did you discuss any potential risks of your work? 3"
240,"The limitations of this paper mainly lie in the following folds: (1) We do not provide any theoretical analysis for the correlation between long-range dependencies and repetition loops, as well as solutions to avoid repetition loops with maximizationbased decoding. (2) We do not discuss the source of LMs’ learning bias, which may be caused by multiple factors, such as the Transformer architecture (Vaswani et al., 2017), the MLE loss, or the auto-regressive generation manner. (3) We conduct experiments based on GPT2 due to resource limitations. The conclusions may differ for extra-large LMs (such as GPT3). (4) We do not experiment with RNN-based models, which are also shown to prefer repetition (Elman, 1990). (5) We do not perform the manual evaluation to compare SELFCONT with baselines since we focus on repetition in this
paper, which can be automatically evaluated reliably. Perplexity and mauve scores are also shown to correlate highly with manual evaluation for evaluating fluency and overall quality, respectively."
241,"Below, we outline several limitations of our work. Data coverage. Our claims are only valid for the datasets accessed in our study. We use the Microsoft Academic Graph (Sinha et al., 2015) and S2ORC, which is larger than other publiclyavailable scientific text corpora (Lo et al., 2020).
However, these sources can differ from other collections of scientific text, because which journal/venues, sources, and resource types constitute “science” differs across academic literature search systems and databases (Gusenbauer and Haddaway, 2020; Ortega and Aguillo, 2014). In particular, since a substantial portion of S2ORC comes from scrapes of arXiv and PubMed, its coverage of computer science and medicine is better than that of other fields (Lo et al., 2020). Also, our coverage is limited to English articles. Past work has shown that citation-based metrics of impact favor articles written in English, and articles from non-Englishspeaking countries have different citation patterns compared to others (Liang et al., 2013; Liu et al., 2018; González-Alcaide et al., 2012). Finally, we recognize that MAG field of study labels are contestable and imperfect. For example, less than twothirds of ACL articles are labeled as natural language processing, and the most popular subfield in ICML is mathematics rather than machine learning.
Token-level analyses. Another limitation of our study is that many scholarly terms are not single words or tokens, but rather phrases. Phrases are somewhat accounted for by measuring words’ senses, since senses induced by language models reflect words’ in-context use, including their use in discipline-specific phrases. For example, Table 3 shows that title has a sense specific to stereochemistry, and in abstracts, this word often occurs in the phrases title reaction or title compound. Phrases containing distinctive words are also somewhat accounted for by measuring individual words in the phrase. However, phrase-level measurements of jargon would likely still be useful for improving interpretability and downstream applications of our metrics, and so discipline-specific phrases are a promising avenue for future work.
Compute. Science of science is interdisciplinary and involves a range of organizations and institutions. Not all researchers will have easy access to the computuational resources needed to replicate our study or apply our approach to data of the same scale. The most resource intensive step of our pipeline is when ScholarBERT predicts each instance of a vocabulary word’s top 5 substitutes across CONTEMPORARY S2ORC and WIKISAMPLE. This took approximately 90 GPU hours split across Nvidia RTX A6000 and Quadro RTX 8000 GPUs. ScholarBERT itself is a 770M-parameter BERT model (Hong et al., 2022), and generally our
compute infrastructure included machines with 64 to 128 cores and 512 to 1024 GB of RAM.
Social implications. In §6.2, we define “success” in two ways, both of which are based on citations. However, though citations are an important currency in science, they are imperfect signals of credit or impact. One article may cite another for reasons that span a range of significance, from brief mentions of related background to core motivation (Jurgens et al., 2018). In addition, associations between jargon use and scientific success may differ as success is redefined using indicators beyond citations. For example, success could be defined beyond scientific communities, such as findings that lead to societal change, products, and use (Bornmann, 2013). Finally, our study on the relationship between jargon and success is not causal, but associational and descriptive."
242,Section 9
243,"Our experiments were performed under some limitations. Since our work deals with both privacy and bias, we tried to keep the individual concepts within bounds, and thus only focused on the oftentreated case of gender bias. Other works, however, also consider cases of, for example, stereotypes towards members of the LGBTQIA+ community or different religions (Barikeri et al., 2021; Nozza et al., 2022). Additionally, we adopted the simplified assumption of binary genders without considering other existing identities such as non-binary or trans*7.
7https://www.gendercensus.com/results/ 2022-worldwide/
Furthermore, our computational resources were limited. Training with DP requires a lot of GPU memory (cf. Yu et al. 2021a; 2021b), which is why we could not train the entire GPT-2 medium with DP. Moreover, we could only train with a batch size of 2. Compensating this by increasing the gradient accumulation steps was also only possible to a small extent due to the limited memory. However, it is likely that DP could have a higher effect on some of the evaluation frameworks when applied to all layers of the model. It would have been of great interest to see if the effect on fairness would have been different. Furthermore, the dataset we used for training was relatively small. Due to limited computational resources and the overall good compatibility with Opacus (Yousefpour et al., 2021), we worked exclusively with GPT-2. For future work, it could be interesting to determine the studied effects in other models.
In the experiments, we found that both dropout and CDA did not provide unambiguously reliable mitigation results. We agree with the finding of other authors that the reliability of SEAT is not beyond doubt, as no bias with statistical significance is found even in the pre-trained GPT-2 model (cf. Kurita et al., 2019; May et al., 2019; Meade et al., 2021). For the other two approaches (StereoSet and BEC-Pro), the model must make predictions with respect to very specific stereotypes, and these predictions may not necessarily be changed by training on a counterfactually expanded data set or increased dropout. Moreover, we evaluated our models on the GLUE benchmark, without focusing on individual tests. More closely examining this would be interesting scope of future research."
244,"The adversarial learning still requests a proper selection of hyperparameters, otherwise the training procedure could be unstable. Besides, training speech diffusion probabilistic models typically require more computational resources, and degradation could be witnessed with decreased training data.
Our proposed model lowers the requirements for high-quality speech synthesis, which may cause unemployment for people with related occupations, such as broadcasters and radio hosts. In addition, there is the potential for harm from non-consensual voice cloning or the generation of fake media, and the voices of the speakers in the recordings might be overused than they expect."
245,See Section 8.
246,"Research on the speech-to-singing conversion is important for human voice study and useful for practical applications such as computer-based music productions or entertainment. However, current STS approaches require an input condition of a fine-grained target F0 contour, which is always unavailable. In addition, the F0 contour of a singing utterance often possesses rich speaker-related infor-
mation, which still needs further disentanglement. Finetuning F0 contours in real applications brings significant extra work. One of our future directions is to simplify the input conditions, such as musical scores. Furthermore, the preliminary attempt at the zero-shot STS task may lead to a better perspective.
Except for positive applications, STS systems may face ethical concerns. With the development of speech/singing voice synthesis technology, the cost of faking an utterance of a specific individual is gradually declining. Researchers need further consideration of the regulation and recognition of speech/singing voice synthesis."
247,"section 6, after the conclusion"
248,Broader Impact
249,"Although we discussed different task formulations and evaluation protocols, the few-shot settings are simulated by downsampling according to existing works, which is slightly different from the real scenario."
250,"The scope of this work is limited by the available data. The OpenPI dataset (Tandon et al., 2020) is derived from WikiHow 2, and focuses on everyday scenarios and contains English only. We would like to see resources that span more domains (e.g. scientific domains) and more languages."
251,"In this work, we mainly leverage control guidance such as action triples, dialogue acts, and discourse relations in structured forms that are extracted automatically from the corpus for training. We encourage future work to explore how to incorporate control information in natural language forms (for example, the natural language descriptions of the action information instead of triples). We also compose multiple modules (like the prototype generation, discourse classifier, etc.) to generate the final conversation which might lead to a larger error cascade if there is some early noise. So future work might explore how to make the pipeline learned in an end-to-end manner. What’s more, we mainly focus on using three major conversation structures to help the entire conversation generation, future work might continue to explore other types of linguistic and human knowledge to further improve the conversation generation qualities."
252,"Section 6
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
253,"For image captioning, we used the pre-trained OFA model for zero-shot inference. We did not explore every state-of-the-art model or fine-tune OFA specifically on the imSitu dataset. Other image captioning systems could yield better results. The gap between automatic object recognition and using gold nouns confirms that correctly identifying the objects in an image is very important for activity recognition. Also, we are not certain that mapping the Jiang and Riloff (2021) function frames to the imSitu frames is strictly necessary."
254,"Although our method has been shown effective, it has two limitations that may be improved in the future. First, the FA model has advantages in computation but relies on an effective frequency selection strategy, which is difficult to design. We just simply select some manual frequencies for different datasets by experience. The more effective frequency selection strategy needs further exploration. Second, there is no theoretical guarantee that the orthogonal regularization can generalize to a 3-order tensor. Our OR terms are only formally consistent with matrix orthogonal regularization, which has been empirically shown effective."
255,"section 7, before the References
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
256,"We define the adversarial attack task as a sequential decision-making problem and apply policy-based reinforcement learning to model it. This work must follow this assumption: the decision process conforms to Markov decision process (MDP) that the conditional probability distribution of the future state depends only on the current state. Meanwhile, reinforcement learning training requires additional time costs and the results may be unstable.
We only conduct the experiments on two NLP tasks with six selected datasets, which are all English corpus. Furthermore, our experimental results are mainly for BERT, with RoBERTa supplemented in the analysis. Thus, we lack the evaluation of other novel pre-trained language models, such as ELECTRA (Clark et al., 2020) and XLNET (Yang et al., 2019). Therefore, our work lacks multi-task, multi-model and multilingual verification in terms of generalization and transferability."
257,"In this work, we introduce LaSQuE, which models and learns the differential semantics of linguistic quantifiers present in natural language explanation to train a classifier guided by these explanations. We evaluate the efficacy of LaSQuE over baselines on the CLUES benchmark.
This work assumes that only a single quantifier is present in the explanations. However, in real-world settings, explanations may contain multiple quantifiers. Modeling the composition of quantifiers can be an interesting direction for future work to make the paradigm of learning from explanations more robust toward fuzzy concepts expressed in real-world explanations.
For our experiments, we assume perfect extraction of quantifiers and limit our analysis to a limited set of quantifiers in this work. Furthermore, we assume that the effect of quantifiers in a sentence is the same irrespective of the domain of the sentence. For example, consider two sentences
‘pungent mushrooms are usually toxic’ and ‘people who smoke regularly usually suffer from cancer’. Here the effect of ‘usually’ is not exactly the same for two sentences that are from different domains. However, LaSQuE is not sensitive to the task domain while modeling the semantics of the quantifier. Future work can investigate variations in the
semantics of the same quantifier across different domains and also how to incorporate/learn such domain-specific differences (for example, by modeling the semantics of a quantifier as a probability distribution rather than a point value)."
258,"Our system has been trained on everyday conversations from Spanish-English bilinguals and may not be applicable to other domains. Additionally, the accuracy of the classifier varies depending on the label type. We use human-created transcripts, so results may not apply for automatic transcripts. There is a risk that incorrect conclusions can be drawn if the system does not meet the performance requirements."
259,"Our work is base on the existing sequence-tosequence NER model, since its way of decoding has been shown effective for knowledge transfer between different classes (Chen et al., 2022). However, it might also be valuable to consider other token-classification-based or CRF-based (Sutton et al., 2012) NER models. Especially, it would be interesting to employ the existing CRF-based distillation method (Wang et al., 2020b) to cope with the problem of heterogeneous tag sets for NER."
260,"Our method is first limited by the proposed grammar that doesn’t cover all the realistic cases. As shown in Table 1, there are still a few cases in the randomly sampled 100 examples that none of the defined rules can explain. Secondly, the time complexity of our method is the cube of the sentence length, limiting its direct applications on long documents. So we have to classify the document based on classification of individual sentences, which might be problematic since the sentiment of different sentences in the document may affect each other.
All the experiments in this paper are conducted on public available datasets, which has no data privacy concerns. Meanwhile, this paper doesn’t involve human annotations, so there are no related ethical concerns."
261,"This work introduced NeQA, a question answering dataset for evaluating the ability of large language models to process negation. While our NeQA attempted to cover diverse types of negation (e.g., different negation phrases and positions) and multiple data sources (e.g., OBQA, LAMA), it is possible that the dataset construction misses some types of negation or domains of text. Our future work will extend the dataset to cover more comprehensive types of negation and domains of text, beyond OBQA and LAMA. Additionally, NeQA is an English dataset, and it would be interesting to extend it to non-English languages and conduct a more comprehensive evaluation of language models, including multilingual ones.
Another potential limitation is sensitivity in language model prompting. Language model performance is known to be influenced by the specific prompt used to query the model (e.g., a rephrased prompt may lead to different model outputs), and prompt engineering—finding the “right” prompt— may be needed to obtain reasonable outputs from the language models (Jiang et al., 2020; Ruis et al., 2022; Wang et al., 2022). As our language model evaluation protocol uses prompting (§3), the evaluation results may inherit such prompt sensitivity. It would be an interesting future work to incorporate techniques to mitigate prompt sensitivity in language model evaluation (e.g., Burns et al. 2022)."
262,Page 6.
263,"The current work is an initial attempt at studying the problem of zero-shot classification of semistructured documents. There are two key aspects that this work does not cover and we encourage future work to explore.
First, as pointed out in §2.1, we choose LayoutBERT as our document encoder, Φdoc. This work does not experiment with the variety of encoding strategies in the literature that combines textual, visual, and layout information (Appalaraju et al., 2021; Xu et al., 2021; Huang et al., 2022). It is likely that richer document representations derived from these diverse encoders will further push the limits of zero-shot classification when combined with our proposed unsupervised contrastive pretraining procedure.
Second, results in this paper are on a single dataset, i.e. the RVL-CDIP dataset. While we mitigate this to a large extent by creating four nonoverlapping test splits (see §3.1 and Appendix A), results on more datasets might yield more useful insights. In practice, the lack of datasets for this task (of semi-structured document classification) is what makes this exploration difficult and might require creation of new resources"
264,"Theoretically, our method might benefit from comparable corpora across languages, where words and compound words might have similar distribution because Zipf’s law might be satisfied only for similar domains. For instance, as presented in Figure 3, word distributions of De and En on Wikipedia are similar after applying BPE. In our experiments, we only confirm the effectiveness of our methods on Wikipedia corpora in different languages, which are comparable across languages. This might limit the scope of our method. However, multilingual models are commonly pre-trained on comparable corpora, e.g., Wikipedia and CC.
Another limitation is about the combined objective in Eq. 4. In our experiments, we try to
eliminate the MLM objective, only considering global regression modeling LGC . The result is not promising, and it seems that LGC can not work well without the help of the MLM objective. However, our experiment is very simple. This might be further confirmed or designed in future work."
265,"The drawbacks of our method are the same as those of LoRA: it is tricky to batch inputs to many tasks with varying A and B in a single forward pass, and the rank may be greater for tasks that are more challenging. Moreover, we believe that weights obtained during a single task may be used for a better initialisation. Finally, the use of a different sampling policy on a different dataset may also be appropriate, however this choice is not obvious."
266,"Though we have found a tight connection between probabilistic transformers and transformers in Section 3, this does not mean that our model can be directly used to interpret or modify transformers. For instance, in Section 3.3, we find that WK and W V in transformers both correspond to U in probabilistic transformers. However, if we tie WK and W V in transformers, then we may observe a performance drop on some downstream tasks.
The performance of probabilistic transformers lags behind transformers on large datasets (>100k), which suggests that our model may not be as scalable as transformers. We have discussed this in Section 6.
The way of positional encoding for probabilistic transformers leads to slower training and inference speed. On masked language modeling tasks, our model is about 3 times slower than transformers with either absolute or relative positional encoding, though it has much fewer parameters than transformers."
267,"mT5, compared with previous Hebrew LMs, is bigger, pretrained on more multiligual data, and learning to segment and tag in an end-to-end manner. While it was beyond the scope of this paper to pretrain new LMs and study which factors contributed to the improved performance, identifying these factors will be useful for determining the most effective approach for future work.
While larger mT5 models perform better than available LMs, they require more powerful hardware accelerators and take longer to train and infer. However, this is a reasonable trade-off from pretraining designated monolingual models from scratch, a more expensive task by itself. Additionally, the inclusion of data from 101 languages in the training of mT5 may have negatively impacted its performance on Hebrew, as some of the data may not have been relevant or beneficial to this particular language. Future work will need to address
this issue by training a monolingual Hebrew LM in order to further improve performance for Hebrew.
An inherent risk in sequence-to-sequence models is that they can generate inconsistent text with respect to the input text (Lee et al., 2018; Rohrbach et al., 2018). While potentially sensitive in different applications, a number of evaluation frameworks have been suggested to reduce the number of such “hallucinations"" (Honovich et al., 2021, 2022). Another limitation of our evaluation framework is that, for lack of available datasets, we did not evaluate mT5 on purely generative tasks such as summarization and paraphrasing."
268,"While our approach effectively predicts the relationships between entities in a knowledge graph, there are limitations in the scope of knowledge graph resources that can be modeled. The knowledge graph contains a vast array of resources, including attributes, descriptions, and images, which are not easily captured by embedding-based methods, but can be effectively modeled using PLMs. To improve the compatibility of KGC with actual needs, it is necessary to consider a broader range of data types in the knowledge graph and develop complementary methods to effectively incorporate them."
269,"Despite the effectiveness of our proposed method, it still has two main limitations: (1). Generative data augmentation methods need to use the original
HTC training set to fine-tune the backbone generative PLMs. Then, they need to go through an inference stage to generate data. Both the training and inference stage need more GPU resources, which increase carbon emissions. Although the data generation is usually complete offline and does not improve the time cost of online progress, we leave how to relieve the need for GPU resources as future directions. (2). Although we conduct experiments on three widely used HTC benchmarks, the language of all these benchmarks is English, which has limited morphology. The effectiveness of our proposed method on language with varied morphology needs to be further confirmed."
270,"In this section, we examine the limitations of our approach. Even though our training methodology runs faster and uses less memory than retraining, there remains potential for further scalability optimization. One potential avenue for improvement could involve optimizing the estimation of the Fisher Information Matrix. Furthermore, op-
timizing the parameters related to the incremental training such as buffer size and regularization coefficient is dependent on the entire time steps rather than the current time steps. Devising a time-efficient way for hyperparameter optimization could be extremely beneficial for this task. Additionally, while our full model has demonstrated some mitigation of the problem of catastrophic forgetting, a significant gap remains between the upper performance bound and the performance of our approach. Further research is necessary to bridge this gap and improve overall performance. Finally, our current focus on continual learning is limited to the emergence of new events and does not currently consider the possibility of new relations or entities. This limitation is in part due to the base model (RENET) not being inductive and is a problem that is inherent to the model itself. Future research in the field of continual learning may aim to address this limitation by considering new relations and entities, even in the context of base models that do not support these features."
271,"The main limitation of this paper is the one applying to any opinion piece: it is subjective and personal, as the views of the authors are inherently limited by their expertise and experience. More specifically, this paper argues for an increased interaction between the speech and NLP communities, but the author is more strongly embedded in the latter, and thus addresses this audience primarily. Additionally, the short paper format imposes significant constraints on the amount of nuance, detail and discussion of relevant literature, and thus readers may find some of the claims to be less strongly supported and less hedged than would be ideal, or proper in a longer treatment of this topic."
272,"The main limitation of the proposed study is the relatively small scale of the dataset it is based on. The proposed method is scalable and computationally undemanding (all of the analyzed models can be trained on a single GPU with 12G of memory), and it is feasible to apply it to other countries in the CMP dataset. However, in order to arrive at interpretable results that could be verified in terms of policy substance based on the experts’ knowledge of the political spectrum, we had to focus the evaluation part on the materials of a single election cycle in one country. Potentially, the method can be applied to any country whose manifestos have CMP annotations, however, further investigation with data from other countries needs to be carried out to verify that.
While most policies are recurrent in manifestos, there may be a few topics appearing in upcoming elections, adding some variability in debate across election years. The policy domain labeller might need to be updated every now and then with current topics of interest (e.g. Covid, a sudden expansion of the military). Therefore, the effect of news electoral programs in the classification step requires more investigation namely, the feasibility of further training with new topics of the current debate or the necessity to re-train the whole classifier with new manifestos over again. That being said, the CMP codebook has remained the same for over two decades now. We take this as evidence that the policy domains do not need to change, only the ability of the classifier to correctly identify sentences with unseen topics."
273,"The current study is has some limitations. First, we used a single audio-transformer model, the pretrained Wav2Vec2.0-base, as a test bed to validate the fine-grained ANs and couple them to their BN signatures. On the one hand, various audiotransformers have been proposed in the literature. On the other hand, the parameters of a pre-trained model are fine-tuned by downstream tasks and previous studies have shown that fine-tuning may lead DNNs to increase their brain similarity (Millet and King, 2021; Tuckute et al., 2022). Thus, it would be interesting to explore whether there are consistent AN-BN coupling patterns across different models, either pre-trained or fine-tuned. In addition, it is necessary to investigate these patterns across different languages (e.g., English VS Mandarin).
Second, existing studies have shown that audiotransformers are able to learn sound-generic, speech-specific and language-specific representations and those hierarchical representations are akin to the cortex (Li et al., 2022; Millet et al., 2022; Vaidya et al., 2022). Thus, it would be interesting to explore whether the fine-grained ANs carry such multi-level representations, and link them to brain responses.
Third, the reproducibilty between the two sessions was high regarding to most of the results (e.g., the global BNs and the phoneme-selective AN-BN pairs), but it was relatively low in some results (e.g., the local ANs in some layers). We speculate that this is the consequence of relatively smaller fMRI training samples but much larger amount of VS-DBN model parameters in the session of Forgot, in which the number of subjects is smaller but the fMRI spatial resolution are higher. Higher
spatial resolution results in much larger number of valid voxels (120,506) compared to that in Pieman (50,065) and consequently more visible units in the VS-DBN model.
Last but not least, the analyses presented in this study are intrinsically limited by the coarseness of spatial (voxels in millimeters) and temporal resolution (volumes in seconds) of fMRI data. Mapping from sound to an interpretable representation involves integrating neural activities on different spatial-scales down to sub-millimeters and on different timescales down to milliseconds. Thus, it would be of great interest in the future to apply the fine-grained ANs to auditory magnetoencephalogram (MEG) dataset to disentangle the symbiosis of model computation and brain responses in both space and time (Bhaya-Grossman and Chang, 2022; Gwilliams et al., 2022)."
274,"We discover that for datasets with a relatively large number of categories, our method requires a more delicate setting of epoch under different shots. Figure 5 shows the average results on Sun397 and ImageNet of different epochs. It can be observed that for datasets with a large number of categories (such as Sun397 and ImageNet), as the number of shots decreases, the performance deteriorates with an increase in the number of epochs, which is not evident on the datasets with a small number of cat-
egories. We will delve further into this problem to find the reason and solution."
275,Section 6
276,"The limitations of our work can be stated from two perspectives. First, the proposed context-free opinion grammar is designed manually. It can be the future work to explore how to automatic generate the grammar. Secondly, we focus on opinion tree parsing in one major language. The performance of other languages remains unknown."
277,Section 9.
278,"Prosody-TTS adopts generative diffusion models for high-quality synthesis, and thus it inherently requires multiple iterative refinements for better results. Besides, latent diffusion models require typically require more computational resources, and degradation could be witnessed with decreased training data. One of our future directions is to develop lightweight and fast diffusion models for accelerating sampling."
279,See section 6
280,"Our work seeks to gain insight into what pretraining knowledge is transferred and useful for downstream fine-tuning in NMT using synthetic tasks and data. We note that changes in the data generation methods do require re-running the pretraining stage, which is computationally expensive compared to the fine-tuning stage.
Our current synthetic data generation methods are somewhat crude. Although they are designed to encode varying degrees of lexical and structural translation knowledge, they do so in a rather simplistic way. For example, sampling phrases from the normal distribution ignores distributional frequencies which represent information that is likely useful for the synthetic data generation task. In this paper we have presented some interesting initial findings regarding the suitability of synthetic
pre-training for NMT. We plan to explore more sophisticated data generation models in future work.
We acknowledge that synthetic pre-training is unlikely to surpass the quality of real-world massively multilingual pre-trained models in performance, especially if synthetic data is the only data used for pre-training. However, good performance can probably be achieved by combining synthetic pretraining and real-data pre-training. Of course, this risks exposing the model to toxic and sensitive or private content. Therefore, concerns of both model quality and data quality should be considered when evaluating the impact and benefits of synthetic pretraining. We view synthetic pre-training as a complimentary approach to finding an optimal balance rather than as a replacement for previous state-ofthe-art NMT pre-training methods."
281,"First of all, IDOL relies on a customized dataset that is filtered out from Wikipedia pages with the help of many pre-defined logical indicators. Inevitably, this will introduce a certain amount of artificial bias. If an automatic method for logical indicator extraction based on something like hidden representations from neural network models is put forward, it would be beneficial to narrow the gap between the dataset preparation and logical pre-training.
In addition, in the field of pre-training task design, there have been a lot of different but effective approaches proposed. For example, in Cui et al.
(2022), the authors presented a pre-training task named PERT which requires the models to recover the original token sequences under the background of that different token permutation within a certain range would not affect Chinese text understanding. This method only depends on the original texts, but IDOL introduces one more special token, which widens the gap between pre-training and fine-tuning to some extent."
282,"There are two main limitations of our work. Firstly, since there are no known baselines for Indian language DC except Kundu et al. (2022), other architectures might perform better than our model. Our claim that Seq-GAN-BERT tries to maximize the information gained from unlabeled sentences is supported by superior performance over baselines defined in this work and other related models. Secondly, due to the lack of good quality labeled datasets, our test sets contained only 100 sentences. However, we believe that the consistency of our high-performing models across languages and multiple seeded experiments presents a positive sign for DC in low-resource settings."
283,Limitations have been mentioned as section 8 of the paper submitted
284,"This work puts forth a position: by the nature of a position paper, the work is deliberately intended to be evocative and opinionated, in some places not having unequivocal evidence for certain claims. This presents a clear limitation: the analysis presented may diverge from the realities of NLP at present or in the future, namely if the assumptions/conditions presented themselves prove to be untrue in practice. Nonetheless, we believe centering power and change, and understanding evaluation as a political and sociological phenomenon, is likely to be useful under all conditions.
Further, in understanding the qualities of evaluation relative to other social forces, we directly suggest that evaluation is more readily operationalized in more pluralistic ways than other key forces (primarily resources). While initial efforts indicate the potential for such holistic approaches that reflect many different desiderata (Liang et al., 2022b) as well as participatory approaches that permit contribution from different entities (e.g. Srivastava et al., 2022), it is still unclear how much adoption such approaches will get, and therefore how much power they will acquire. That is, the extent to which evaluation can realize this pluralistic vision still largely remains an unresolved aspiration than a readily realizable certainty. And, conversely, we do note that while current practices potentially put pluralism and resources at odds, they may be mutually compatible in other regimes (e.g. decentralized training through the pooling of shared/volunteered compute (Yuan et al., 2022), open-source software development (Wolf et al., 2020; Gao et al., 2021; von Werra et al., 2022)).
Finally, we do not discuss other forces that we believe have not exhibited strong influence on NLP research thus far, in favor of allocating focus to evaluation and resources, which have had clear influence. To enumerate some of these other (potential) forces, we specifically note (i) research norms, (ii) policy and regulation, and (iii) auditing/advocacy. For (i), we note that while the NLP research community has many established norms (e.g. reproducibility checklists, peer review guidelines, conference organization structure, policies on respectful conduct), most of these do not directly/significantly influence what research topics different researchers work on. We do note that is possible in the future that certain norms (e.g. the access to training data or model checkpoints;
Liang et al., 2022a) would influence what research is conducted (e.g. we may have not seen as much work on the learning dynamics of language models and/or memorization of training data due to the relative inaccessibility of intermediary checkpoints and training data until recently). For (ii), we note that policy and regulatory efforts have had little to no salient impact on the deployment of most language technologies, let alone NLP research, to our knowledge. With that said, much as efforts like GDPR and privacy legislation has impacted scientific research on privacy (e.g. work that operationalizes the right to be forgotten as in Ginart et al., 2019), similar trends could occur in NLP research (e.g. in response to the EU AI Act).3 Akin to (ii), for (iii), we also have seen fairly little impact from auditing/advocacy work on NLP research to our knowledge. But, much as work on auditing/advocacy around face recognition (Buolamwini and Gebru, 2018; Raji and Buolamwini, 2019; Raji et al., 2020, inter alia) influenced research in the computer vision community, we could see similar trends in NLP (e.g. in response to auditing/advocacy intervention around language models)."
285,Limitations (pg 5)
286,"First, despite our pursuit of attempting to understand figurative language use across cultures, we have barely scratched the surface in terms of diverse representation. Due to limited scope, budget, and resources, we collect data from 2-3 annotators per language, for seven languages. Further, culture can vary greatly within a language (Hershcovich et al., 2022). Therefore, until we can represent all of the worlds’ people and their languages, there will always be room for improvement. We also acknowledge that the syntax captured in the dataset may not be the most diverse, as many examples follow the template “<X> is like <Y>”. However, we create these simpler examples as a first step, since extension to more complex and naturalistic language can be included in future work. Second, to analyse concept shift, we machine translate test sets into English. However, these translations can be erroneous to varying degrees, which may have resulted in an over-estimation of error attribution to concept shift. This could not be avoided however, due to limited resources of obtaining human translations. Third, English may not be the best language to transfer from in zero-shot evaluation of multilingual models. While we were constrained by training data availability, past works have shown that
machine-translating train sets can help, an avenue we haven’t explored here. Even though we experiment with few-shot evaluation, there may exist an optimal combination of source languages which best transfer to our target languages. Fourth, the English authors recognized culturespecific terms that were not marked as cultural by annotators in the commonsense categorization across all languages. This may be because annotators, being mostly familiar with their own cultures, attributed culturally specific facts and terms as being common sense. Likewise, the Englishspeaking participants may have viewed a separate set of facts as common sense which would not be agreed upon by people from a different culture. It is thus difficult to disentangle common sense and culture in many cases."
287,"Our approach requires training a discriminator with an attribute classification dataset, which may be expensive in some scenarios. However, it is still applicable by collecting a small set of attribute-sensitive training instances and applying data augmentation techniques.
Our method is hard to achieve fine-grained control. We aim to address attribute-based generation that conditions on a given style, sentiment, toxicity, or topic. However, it cannot condition on a piece of content to control the generation. We encourage future works to explore retrieval-augmented generation with fine-grained control signals.
ACL 2023 Responsible NLP Checklist"
288,"We conducted extensive experiments with three datasets from different domains to substantiate the results thoroughly. We observe the best performance when we also leverage the body of the articles. So, we did not evaluate the performance on the datasets that do not have the full text (or equivalently, long text) of the articles.
Ethics Statement
The datasets we used in experiments are publicly available. In our work, we provide a comprehensive analysis and present data augmentation strategies specifically to address keyphrase generation in purely resource-constrained domains. We do not expect any direct ethical concern from our work."
289,"BIGVIDEO is collected from two video platforms Xigua and YouTube. All videos are publicly available. However, some videos may contain user information (e.g., portraits) or other sensitive information. Similar to VATEX and HOW2, we will release our test set annotation and the code to reproduce our dataset. For videos without copyright or sensitive issues, we will make them public but limit for research, and non-commercial use (We will require dataset users to apply for access). For videos with copyright or sensitive risks, we will provide ids, which can be used to download the video. This step will be done under the instruction of professional lawyers.
Though we show that our model with video inputs helps disambiguation, we find that our model could yield incorrect translation due to the lack of world knowledge. For example, model can not distinguish famous table tennis player Fan Zhengdong and give correct translation. We find this is due to video pretrained models are often trained on action dataset (e.g., Kinetics-600 (Long et al., 2020)) and hardly learn such world knowledge. In this work, we do not further study methods that leverage world knowledge."
290,"In this section, we draw conclusions for the limitations of our proposed model in this paper. Our proposed model mainly focuses on the sentencelevel procedural graph construction. The scenario that two actions in the same sentence cannot be considered in our proposed model. It is challenging to handle multi-grained (i.e., entity-level and sentence-level) dependencies between actions. We will consider this limitation as our future work."
291,"The way the method applies to larger datasets needs further exploration. As the number of training examples increases, the accuracy gain over vanilla finetuning reduces, indicating that our method best works in low-resource scenarios. Another limitation is that we performed experiments only in one language. It will be interesting to apply our method to tasks in other languages and understand the impact of task-dependent similarity structure on the model’s performance in those scenarios. BFTSS Top-K and BFTSS U-V methods perform similarly. Scenarios where BFTSS Top-K and BFTSS U-V differ in performance, should be further explored. We plan to address them in our future works."
292,Left blank.
293,"While EVALM demonstrates that vocabulary augmentation with LRL task performance as objective requires different priorities from vocabulary augmentation for improving representation for its
own sake, our work opens up several avenues for exploration. Our understanding of the potential conflict between fidelity of LRL word representation from wordpieces and LRL task class discrimination requirements remains far from complete, particularly when we extend from sequence-tosingle-label applications to sequence labeling (as in POS and NER tagging) and further to sequenceto-sequence applications (such as translation). Perhaps, further experiments with mBERT and other MLLMs will further our understanding of these trade-offs. While initializing an LRL word embedding using InitHRL or InitMix, we depend on automatic machine translation, which can be errorprone. Ranking by ∆H and picking a prefix fails to discount informative but correlated features. A more sophisticated formulation of loss of information owing to fragmentation, taking multiple LRL words into account simultaneously, may alleviate this problem. In the short term, these two limitations may deserve closer scrutiny."
294,"The proposed CAST framework carries the same limitation of self-training-based methods, which is the requirement for multiple rounds and multiple splits of training. As a result, the GPU computing hours of CAST are longer than those of vanilla baselines and NS."
295,"We proposed a solution to the cosine similarity underestimation problem associated with contextualised word embeddings of highly frequent words. Our evaluations used only a single contextualised embedding model (i.e. BERT) with a single dimensionality (i.e. 768). Therefore, we believe that our proposed method must be evaluated with other (more recent) MLMs to test for its generalisability. Moreover, our evaluations were conducted only on the English language, which is known to be morphologically limited. Although in our preliminary experiments we considered discounting schemes based on the part-of-speech of words (instead of considering stop words vs. non-stop words), we did not find any significant improvements despite the extra complexity. However, these outcomes might be different for more morphologically richer languages. In order to evaluate similarity predictions in other languages, we must also have datasets similar to WiC annotated in those languages, which are difficult to construct. Although having stated that using a single MLM and single language as limitations of this work, we would like to point out that these are the same conditions under which Zhou et al. (2022) studied the cosine similarity underestimation problem.
We used only a single dataset (i.e. WiC) in our experiments in this short paper due to space constraints. Other contextual similarity datasets (e.g. Stanford Contextualised Word Similarity (SCWS) (Huang et al., 2012)) could be easily used to further validate the proposed discounting method in an extended version."
296,"It’s worth noting that this study has certain limitations. One of the limitations is the limited scope of the training data employed. The AltCLIP model is trained on open-source parallel corpora and publicly available unfiltered text-image pairs. A more careful study of the training data, i.e. filtering textimage pairs by relevance and text/image quality may help to further improve the overall performance of the model. Another limitation is the challenge of evaluating the model in a multilingual setting. Despite our best efforts to include as many benchmarks as possible and to translate from English datasets, the evaluation of the model’s performance in other languages is not as comprehensive as it is in English. For example, there may be fewer tasks available such as OCR or action recognition in videos in other languages. In addition, the use of machine translation may introduce biases that could affect performance. Future research should focus on creating a more robust and scientifically rigorous multilingual evaluation framework."
297,"Although we have demonstrated the superiority of our RHGH model compared to previous work on four real-world datasets, there are still two limitations that should be addressed in the future:
(1) As our RGC layer employs the whole graph to learn the embedding of entities and relations, like most GCN’s frameworks, the computational resources and time required by our framework increase linearly with the size of KG. To make our RHGH model effective on the KG with millions of entities, it is desirable to apply some graph chunking techniques, such as Cluster-GCN (Chiang et al., 2019), to reduce the size of the KG for our RHGH model to improve computational efficiency.
(2) Currently, our RHGH model treats each relation individually. However, relation paths consisting of multiple relations will contain more complex semantic information in KGs. Relation paths enable entities to obtain higher-order neighbor information, but it is also more difficult to align relational paths in different knowledge graphs. In future work, we will explore more efficient ways
to utilize the relation path in entity alignment, such as the relation path matching in different KGs."
298,"Section 5.4, Section 6 and Section 7
7 A2. Did you discuss any potential risks of your work? We do not use huge models and our experiments are fair."
299,"Our work has only considered pairwise interactions, but linguistic structure can also manifest through higher-order interactions. We show that our results on small-scale, formal languages, are different from our results on a natural language task.
It would be premature to conclude that small-scale, synthetic tasks can not be predictive of behaviour on more complex tasks, and a more detailed investigation into the properties of the task that play a role is a viable next step. Some of the FIDAMs we considered, most notably SII and STII, are intractable for larger inputs (scaling O(2n)), and a necessary step in employing these methods to larger models is to construct better approximation procedures, e.g. by adapting SHAP to SII as has been done before for tabular data by Lundberg et al. (2018). More generally, although we believe our probabilistic formal language setup provides a important step forward, solving the Attribution Generalization problem – i.e., showing that results for small setups generalize to very large model – remains a key open problem."
300,"Section 9
7 A2. Did you discuss any potential risks of your work? Our work is of a more theoretical nature, providing a new way of measuring the faithfulness of feature interaction methods."
301,"Despite our efforts to collect as many generation tasks and datasets as possible, we only evaluate the generation quality and generality of our models on a small number of tasks and datasets. The interpretability and robustness of our models require further analysis. Besides, there exists subjectivity when collecting downstream tasks and intratask datasets, albeit our attempts to employ widelyrecognized categorizations from the literature. Due to the limitation of computing power, we do not study the performance of our method at different model scales. The effectiveness of multi-task pretraining from scratch, similar to ExT5 (Aribandi et al., 2022), also merits an in-depth study.
Broader Impacts
In this paper, we pre-trained a language model MVP using labeled NLG datasets. According to the research (Bender et al., 2021; Bommasani et al., 2021), PLMs tend to “remember” what they have “seen” in the pre-training corpus. This could result in the reproduction of undesirable biases from pretraining data on downstream tasks. Training data intervention could be a solution to alleviate this issue (Lu et al., 2020). It is also interesting to investigate whether supervised pre-training produces fewer biases than unsupervised pre-training.
Environmental impact is another factor we should consider. We attempt a more efficient pretraining strategy and released our PLM for future work. In contrast to large PLMs with tens of billions of parameters, such as T5 (Raffel et al., 2020) and GPT-3 (Brown et al., 2020), we pre-train only a small model with hundreds of millions of parameters. In addition, we utilize supervised pretraining data and initialize our model with pretrained BART, both of which improve the convergence of our model. Ultimately, our model is pretrained for about 20, 000 steps, whereas the BART of the same size is pre-trained for 500, 000 steps."
302,Section Limitations
303,"We note a few limitations of our work: a) while we systematically investigate the choice of in-context examples for both in- and out-of-domain settings for higher-resource language pairs (EnglishGerman, English-Russian), it is unclear how this in-context ability of the PLM varies for the lowerresourced language pairs; b) We only experimented with one pre-trained language model, XGLM. Our preliminary experiments suggested XGLM-7.5B to result in better translation quality than Bloom7B (Scao et al., 2022) under the same settings. However, further investigation is required to understand how these results vary across different model scales; c) We analyze different orderings for the few-shot task-level prompts but only examine limited sets of ordering (most similar to the left or right) for the example-specific prompts. As the PLM is shown to be sensitive to the ordering of these in-context examples, it remains an open question to study how to best combine the information from multiple example-specific prompts, with prompt ensembling being a viable option, which we leave to future work."
304,Section 9
305,"In the process of conducting experiments, we find our method has some limitations. First, CIF-PT needs to be performed on the dataset with speechtext pair. For some small-scale dataset that only contains speech and SLU labels, our method needs to use external ASR dataset to conduct the pretraining, leading to the increase of complexity of model building. In addition, in CIF-PT, we need to ensure that the tokenizer of the pre-trained language model is consistent with the tokenizer in the ASR task. However, there is usually a gap between the two in terms of vocabulary size. In consideration of performance, it is necessary to modify the tokenzier of one or both sides."
306,"In section 7
7 A2. Did you discuss any potential risks of your work? We believe there is no risk in our work. We only use the open-resource codes and publicly release data from the community, and no misuse of any resource and no other stateholders involved in our work."
307,
308,"One of the limitations of our framework is we need to design the rough range of hyperparameters to search the best setting. In our future work, we will explore the strategy to avoid hyperparameter tuning."
309,"Although our approach exhibits great speedups in encoder-only settings, it doesn’t yield as impressive speedups in encoder-decoder setting. This is due to the autoregresive decoding steps in the decoder, that has to be conducted sequentially. Accelerating that with DCT requires to incrementally update DCT outputs step by step based on outputs of pre-
vious timesteps, which is theoretically possible but not easy to optimize its efficiency. We plan to further accelerate it in this direction in future work."
310,"Language Our experiments are conducted on English, as all Code-LLMs we know are pre-trained on English programming languages. Fundamentally, most popular programming languages are English-based, but international programming languages (which work in multiple languages) like Scratch, or non-English-based programming languages like Qalb also emerge. We look forward to the appearance of Code-LLMs on these programming languages.
Prompt Engineering We manually design the prompts without prompt engineering techniques such as prompt search. The searched prompts may outperform the ones we used, but our experiments on interventions show that CODEX is fairly robust towards format perturbations.
Model LLMs update quickly. From the time we submitted the paper until now, several new LLMs have been released. We try to compare their performance with ours. We select three new LLMs: CHATGPT, GPT-4 (OpenAI, 2023), and BARD4, and feed the text prompts to them. Because we do not have access to some of their APIs, we only experiment on a subset of 100 instances and report
4Experiments are done with models updated to May 10, 2023.
the results in Table 5. CODEX outperforms all these models in the automatic evaluation, but part of the reason is that these models provide more detailed outputs than the reference. We provide a case study in Appendix A.5.
Since CODEX is no longer available to the public, we provide CODEX generation results in our GitHub repository. We also looked for alternatives and tried two open source Code-LLMs CODEGEN (Nijkamp et al., 2022) (version CodeGen-16BMono) and STARCODER (Li et al., 2023) with our code prompts. However, as shown in the case study, their performance is not comparable to CODEX, probably because they are more than ten times smaller in size."
311,"Our approach is based on the assumption of a limited data budget, and the observation that general multi-task training may not be the most efficient method when one cares about single target tasks. As such, DEFT is not applicable to “true” zero-shot settings where one has no information about the target task, since it relies on the existence of at least some unlabelled examples. Furthermore, for some tasks it may be possible to cheaply gather many examples for finetuning beyond the point where DEFT is useful. In some cases, gathering unlabelled examples may not be so much cheaper than gathering labelled examples that it is worth considering whether to gather unlabelled or labelled examples. Additionally, the recent rise of sparse
dataset. Questions were written after reading paper abstracts, and evidence selection required reading entire papers.
mixture-of-expert models (Shazeer et al., 2017; Fedus et al., 2022) may reduce the negative interference effect observed throughout our work, where DEFT models often outperform models trained on all multitask data and random subsets of the multitask data. Finally, we note that in pilot experiments we found that task diversity was a key element of strong held-out task performance. However, DEFT does not explicitly correct for task diversity, and we leave further exploration for extending DEFT to account for this to future work."
312,"We discuss the limitations of our method from three perspectives.
First, our method is based on pre-trained language models, so compared to rule-based data augmentation methods (synonym replacement, shuffle within segments, etc.), our method requires higher time complexity.
Second, the entity matching process (Section 4.3) will discard sentences which cannot match entities in the entity list, which will affect the utilization of data.
Third, our data augmentation method based on the pre-trained language models, whose generalization ability is limited since the augmented knowledge comes from the pre-trained language models. However, the knowledge in pre-trained language models is limited and not domain-specific. How to improve the generalization ability of the data augmentation methods is a future research work."
313,Section 7
314,"MATHWORLD is limited to cover math story problems using the four basic arithmetic operators. Furthermore, within the space of such problems, it does not cover “second-order” MSPs (as discussed in § 3.4). Neither does it cover negation nor inequalities.
We only consider datasets with MSPs written in English in this work. However, MATHWORLD should in principle be able to cover the same type of problems formulated in other languages as well.
An obvious limitation of this work is the low performance on the task of solving MSPs. The focus of this work is to introduce the world model formalism and its use cases, and we leave for future work to build stronger MATHWORLD parsers."
315,"Given our focus on finding efficient MoE models under computational constraints, AutoMoE search space and evaluation has been restricted in scale to big-sized Transformer models for benchmark MT tasks. A natural extension of this work is to explore the limits of MoE models like SwitchTransformers (Fedus et al., 2022b) and GShard (Lepikhin et al., 2020) that are significantly larger containing billions to trillions of parameters; as well as designing sparse and transferable efficient expert models (Zoph et al., 2022) for diverse types of tasks like reasoning, summarization and understanding.
The limitations of this work are as follows:
1. Sandwich sampling (Yu et al., 2019), inplace knowledge distillation (Yu et al., 2019), and gradient conflict reduction (Gong et al., 2022) are popular techniques to improve the training procedure of supernet. It would be interesting to study the impact of these techniques to improve AutoMoE’s supernet.
2. AutoMoE uses the hidden dimension of intermediate feedforward network (FFN) to modulate the capacity of each expert. It would be interesting to study other techniques to modulate expert capacity such as stacking variable number of hidden layers in FFN.
3. The backbone of AutoMoE’s supernet uses Switch Transformer, which adds FFN based expert layers and routes each token to exactly one expert (top-1 routing). It would be interesting to: (i) search for the number of tokens to route, and (ii) search for the Transformer component (e.g., FFN, self-attention projection layers, LayerNorm) to add expert layers.
4. AutoMoE’s search space contains classical Transformer components such as multi-head attention and FFN layers. It would be interesting to add components that are efficient by design such as convolutional layer, FLASH (Hua et al., 2022), and g-MLP (Liu et al., 2021)."
316,"In this work, we did not conduct a detailed analysis of how language-specific characteristics contribute to our model’s cross-lingual generalization capabilities. Future work may address this question through extensive matrix experiments — traverse the training on each possible language pair combination and evaluate on all languages."
317,"In this work, we collect extensive and comprehensive human feedback with high qualities to facilitate our human-in-the-loop conversation summarization framework. While the learned rewards and models are showing good generalization abilities, further attention is still needed to deeply understand what types of feedback or what amount of feedback is necessary. Our current work only considers human feedback collected using the required forms (i.e., rankings and highlighting). We encourage future work to explore how to incorporate human preferences with more open-ended feedback such as through natural languages. Furthermore, we mainly focus on conversation summarization with human feedback in this work, and other types of summarization tasks (e.g., multi-document summarization, email to-do summarization, meeting summarization and etc.) could be further explored to incorporate human knowledge."
318,"Limits on visual variability and naturalness. The Pentomino domain can only serve as an abstraction for referring expression generations in visual domains. The amount of objects is limited to 9 different shapes and the number of colors is reduced to 6 as well. The positions are chosen to be discrete and absolute while real-world references might include spatial relations. Furthermore, the pieces show no texture or naturalness, but are drawn with a solid color fill. We choose this simplified domain to focus on the interaction between the follower and the teacher and left the evaluation of the proposed models on more realistic looking scenes for further work. Nevertheless, we think
our approach can also be applied to photo-realistic environments (Ramakrishnan et al., 2021; Kolve et al., 2017).
Limits on variability of the referring expressions. We only explored expressions that are generate by the Incremental Algorithm. Moreover, we choose a fix property value order (color is mentioned before shape is mentioned before position) for the realisation of the template’s surface structure and left the exploration for a higher variability to further work.
Limits on variability of the feedback signal. In this work we used a heuristic teacher with a fixed behavior to provide the intermediate feedback to the follower. We choose this Oracle speaker for better control over the experiments and to focus on the research questions of which feedback is most helpful and how it should be presented (contain which information). We are aware that in natural interaction the teacher’s responses might be more dynamic and can be potentially learnt in a much more complex multi-agent RL settings which would go beyond our focused contribution here. Still this is an interesting prospect for future research."
319,"Our work is one of the first to perform a detailed empirical investigation of transformer guided chaining but is clearly preliminary. The following are some key limitations: - Evaluation of Interpretability: A fair evaluation
of interpretability is not straightforward. In this paper, we reported results from a preliminary study with limited human labor. - Analysis of negations: LogicNLI dataset uses
negations in the facts, rules and statements but it is difficult to disentangle them for a fair investigation. Hence, we were unable to rigorously analyze the ability in handling negations.
- Evaluation on Real-life data: Our reported
work focused on a synthetic dataset. For a more rigorous evaluation, it is imperative to consider more datasets including real-life ones."
320,"Section 5.1
7 A2. Did you discuss any potential risks of your work? No potential risks anticipated."
321,"While introducing a framework which deals with multiple languages and multiple figures of speech, this work is still only dealing with three figures of speech and seven languages. Many more phenomena and languages can still bring substantial challenges and insights if considered (once the data availability bottleneck is addressed). Also, we deal with figurative language as labelled at the sentence level, but the word level is also not only interesting but important for broader natural language understanding and could yield different insights than those observed in the present work.
We only mention in passing the influence that different cultural contexts have on figurative usages, and we make some observations on idioms, but this aspect would require a much bigger unpacking. We actually believe that (failure) of cross-lingual computational models can be an excellent diagnostic tool towards a finer-grained analysis of the interplay between culture(s) and figurative language.
We propose a successful method based on prompt learning and present experiments using a specific pre-trained model. Choosing different (and possibly larger) models and investigating even more than what we already do in this paper the influence of specific prompts would also be necessary to further generalise the efficacy of our approach.
Finally, as with most language technology, the limitations of our approach, also in terms of accuracy (especially for some phenomena and some languages), could lead to substantial inaccuracies which could be propagated in further processing. Considering that figures of speech are associated with emotional language, a word of warning is necessary regarding the direct deployment of our
models. We do hope that writing about risks explicitly and also raising awareness of this possibility in the general public are ways to contain the effects of potential harmful consquences. We are open to any discussion and suggestions to minimise such risks."
322,"In our study, we have demonstrated the effectiveness of our proposed method on FLAN-T5 with different sizes. However, we have not yet evaluated its performance on LLMs, which possess an even greater number of parameters and have been pre-trained on larger corpora, thus potentially providing more accurate feedback for both caption adaptation and reinforcement learning. Meanwhile, it is worth noting that PLMs may contain certain biases, and training based on their feedback may amplify these biases. As future work, we aim to investigate the scalability of our method to LLMs, as well as strategies to mitigate the potential negative effects of biases present in PLMs.
Ackownledgement
This work was partially supported by National Natural Science Foundation of China under Grant No. 62222215, Beijing Natural Science Foundation under Grant No. 4222027, and Beijing Outstanding Young Scientist Program under Grant No. BJJWZYJH012019100020098. Xin Zhao is the corresponding author."
323,"Our study here focused on the most capable GPT3.5 model, text-davinci-002, at the time the experiments were conducted. We believe that models like ChatGPT and GPT-4, as well as those in the future, are likely to perform at least as well as these, and if they improve further, the metrics we have developed here will be useful in benchmarking that progress. However, significant further paradigm shifts could change the distribution of errors in such a way that certain of our factors (e.g., genericity) become less critical. In addition, the latest iterations of GPT have a much greater input window size, which help them digest much larger swaths of text in one go and potentially make our pipelined approaches less needed in certain settings.
Furthermore, the text-davinci-002 model is fine-tuned with data produced by human demonstrations. The precise data used is not publicly available, so it is difficult to use our results to make claims about what data or fine-tuning regimen leads to what failure modes in these models.
Recent work has noted that language models may be susceptible to learning biases from training data (Sheng et al., 2019; Wallace et al., 2019; Shwartz et al., 2020), and this phenomenon has also been observed for GPT-3.5 (Lucy and Bamman, 2021). We did not stress test the models studied for biases and furthermore only experimented on English-language data.
When properly used, the summarization models described in this paper can be time-saving. However, as noted above, summary outputs may be factually inconsistent with the input documents or not fully representative of the input, and in such a case could contribute to misinformation. This issue is present among all current abstractive models and is an area of active research."
324,"GUMSum is designed to constrain summaries to one sentence for all 12 genres, which raises the question of whether one-sentence summaries are useful for all possible genres or long-document summarization. This is a complex topic that needs in-depth investigation. For GUMSum, as mentioned in Section 3, document length is limited to 167–1,878 tokens. Moreover, in analyzing human evaluators’ responses to two open-ended questions ([1] and [2] in Appendix C), we noticed that virtually all evaluators mentioned that limiting the summary to one-sentence is very difficult and that some genres were easier than others. For example, one evaluator who was given a vlog and a travel guide commented that,
“The travel guide was much more difficult than the vlog, likely because it was longer and denser. [...] the travel guide packed a lot more information into its pages and within each sentence.”
This indicates that genre differences at the summary-level is not trivial due to the style of the original text.
Additionally, this paper examined a specific subset of pre-trained systems and one version of GPT3’s pretrained language model (i.e. GPT3-text-davinci-002), producing findings which may not generalize to other settings. The dataset used for the evaluation is also substantially smaller than those used in most work on summarization, due to the fact that it was carefully crafted based on both general and genre-specific guidelines to be substitutive and to avoid hallucinations and faithfulness issues, rather than originating in a found dataset, in order to conduct a more targeted evaluation, as recommended by Liu et al. (2022a). While it is inevitable that more data would lead to different results, we do not believe that system rankings or overall findings would be substantially different, so long as the guidelines and genres examined here remain stable.
Finally, we must raise a further limitation involving text type and language: our study encompasses 12 specific written and spoken genres available in the UD English GUM corpus, but does not capture findings for other genres, or indeed other languages, which deserve more attention in future studies."
325,
326,
327,"In Section 4.1, where we show that our approach doesn’t provide benefits on improving models that have the same data and compute complexity as the teacher.
A2. Did you discuss any potential risks of your work? Not applicable. Our approach is more of an up-to-date analysis into some of the existing techniques used, and does not pose any risks beyond the risks of general improvement of AI."
328,"Our proposed two-stage training recipe is beneficial under the assumption that a pre-trained model is needed for generative as well as sequence labeling tasks. We believe that is typically the case, as one tries to offset the pre-training investment by using the model for as many tasks as possible, but this assumption might not apply in all cases. While we assess the effect of randomness on fine-tuning results by using multiple seeds, we have not done that for the pre-training itself. Even at our mediumsize scale, it is already prohibitively expensive to do so. The evidence for the effectiveness of the twostage approach is also limited by the number of tasks evaluated (2 sequence classification tasks, 2 sequence labeling tasks, 2 generation tasks), but we believe it is a reasonable trade-off between robust results and compute investment."
329,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable."
330,"The way we use the intermediate sequences is to concatenate new sequences and the target sequence as the new target. As a result, the length of the target increases linearly with the number of intermediate sequences introduced, which increases the cost of inference. In the meantime, Minimum Bayes Risk decoding needs to do prediction multiple times under different control tasks, which further increases the computational cost. However, there are potential solutions to compromise between the computational cost and quality, e.g. learning a student model by distilling the domainrobust knowledge from Progressive Translation."
331,"Our work is limited in capturing the unintended dependencies of attributes. It is possible that maximizing certain attributes like positive sentiment may maximize attributes like gender bias. A formal study to capture the dependency of the bias with varied attribute control is an important future direction. The efficacy automated metrics used to measure the linguistic qualities and attribute alignment of the generations is limited (Jozefowicz et al., 2016). Devising more exhaustive and explainable metrics is also an important future-work."
332,"One limitation is that the importance and visual salience of character instances are not measured directly. We plan to settle these in future work. Another limitation of our work is that we only evaluate our visual coherence loss on a single dataset. Whether the VCL can generalize to other datasets remains unexplored. The reason is that many other datasets are collected in a way to exhibit less visual coherence (lower rates of recurring characters). The VIST dataset (Huang et al., 2016) contains fewer human characters per story than VWP. Also, some of the features we are using for character reidentification may not be suitable to other datasets to the same extent (for instance, if the clothing of characters changes between images)."
333,Section 8
334,"The type of field metadata tasks is limited in this paper and it can be explored more. There are far more types of analysis metadata to be discovered and inferred. On the one hand, inspired by data profiling metadata, the dependency between multifields in one table plays an important role. There are several common dependencies or relationships among columns. How to identify them is future work. On the other hand, in Table 8 only a limited taxonomy is provided. A more comprehensive one is future work.
Our initial research explored the ability of large language models (LLMs) to extract metadata from tables. The results were not optimal, likely due to a lack of exposure to metadata during the training process of the LLM and limitations in the design of the prompts used. Further investigation is nec-
essary to improve the performance of LLMs in extracting metadata from tables."
335,"LAVS is proposed to overcome the off-target problem among languages that share alphabets because those languages tend to have more sharing tokens after the sub-word tokenization process. As for language pair that does not have shared tokens, LAVS might not have a direct influence on the zero-shot translation though it can also increase the overall performance for those languages, which might need further exploration."
336,"We identify the following limitations:
1. Although we strive to include tasks in all Arabic varieties, available downstream datasets from certain countries such as Mauritania and Djibouti are almost nonexistent and so are not covered in ORCA. In addition, there is a need in the community to create more datasets for several Arabic dialects. This includes, for example, dialects such as Iraqi, Sudanese, and Yemeni. With the introduction of more datasets for such dialects, ORCA’s coverage can be further extended. Regardless, as Figure F.1 (Appendix F) shows, ORCA datasets are quite diverse from a geographical perspective.
2. Although ORCA currently covers both dialectal Arabic (DA) and MSA, it does not pay as much attention to the classical variety of Arabic (CA) due to historical reasons. That is, the community did not invest as much efforts creating and releasing datasets involving CA. However, as more unlabeled datasets become available and with an undergoing positive change in the culture around data sharing, this is likely to change in the near future. Again, this will make it possible to extend ORCA to better cover CA in the future.
3. Although benchmarks in general are useful in encouraging standardize evaluations and
meaningful comparisons, and can help motivate progress within the community, they also run the risk of contributing to a culture of leaderboard chasing that is not necessarily useful. That is, although scientific research advances due to competition, it also thrives through partnerships and collaborations that bring the best from diverse groups. It is in the context of this collaborative culture that we hope ORCA will be perceived and used."
337,"Section of Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
338,"NATCS is partially annotated with dialogue acts, intents, and slots, which are annotated independently from the initial collection of the conversations. While decoupling annotations from collection was intended to facilitate natural and diverse dialogues, the methodology is more timeconsuming and expensive than previous approaches that use pre-structured conversation templates to avoid the need for manual annotation. In particular, NATCSSPOKE requires multiple participants engaging in synchronous conversations, followed by independent manual transcriptions and annotations, making the approach particularly time-consuming and difficult to apply for large collections. Furthermore, this decoupling of annotations from collection has greater potential for annotator disagreement.
While the complexity types and annotations are mostly language-agnostic, NATCS is restricted to EN-US customer-initiated customer service conversations between a single agent and customer in a limited number of domains (multi-party conversations beyond two participants or agent-initiated conversations are not included). The annotations included are primarily intended for applications related to task-oriented dialogue systems.
Further, we note that NATCS closes the gap from real conversations along many metrics, but still falls short along some dimensions. We find that real conversations are more verbose, more believable, and less predictable. We also note that comparisons in our paper focused on a limited number of taskoriented dialogue datasets with different collection approaches, and did not exhaustively include all pre-existing dialogue datasets for comparison."
339,"Due to limitations in time and computational resources, we limited our experiments to using GLM and SuperGLUE benchmark3. While transformerbased language models and the SuperGLUE benchmark are representative, further validation is necessary when applied to a wider range of models and tasks. Additionally, we found that the performance of GLMD−vc (10B→2B) at 85.28% was marginally lower than that of GLM-2B at 85.91%. However, it’s noteworthy that GLM-2B leverages a substantially greater scale in the pre-training stage with a batch size, iterations, and GPU count of 7168, 17k, and 224 respectively, far exceeding the respective parameters of 64, 15k, and 8 employed by GLMD−vc (10B→2B) in its distillation during the pre-training stage. We plan to further investigate these potential limitations in future work.
3Given the requirement for grid search and seed averaging, we have run over a thousand SuperGLUE averages."
340,"This work focuses on a specific view of the whole neuro-computational modeling field. We exclude specific angles of research such as non-linear models (Ruan et al., 2016; Qian et al., 2016; Bingel et al., 2016; Anderson et al., 2017; Oota et al., 2018) since we want to evaluate the accumulated evidence for structural similarity (isomorphism) between neural responses and language models. (Ivanova et al., 2022) mention several advantages of using linear mapping models, they are more interpretable and more biologically plausible. They also provide an insightful discussion on mapping model choice, emphasizing the importance of estimating models’ complexity over categorizing them as purely linear or nonlinear.
Another limitation is that we do not include speech models (Vaidya et al., 2022; Défossez et al., 2022; Millet et al., 2022) that have been used to map brain representations mostly due to coherency and page-limit restrictions. The survey is also limited to fMRI and MEG data instead of other modalities for two many reasons: (i) fMRI and MEG are used as a combination in many studies (Caucheteux and King, 2022; Schrimpf et al., 2021; Toneva et al., 2022a), and (ii) they offer high spatial resolution and signal reliability (fMRI) and better temporal and spatial resolution (MEG), making them suitable for NLP (Hollenstein et al., 2020). For a survey in encoding and decoding models in cognitive electrophysiology, see Holdgraf et al. (2017)."
341,"Since this is one of the first studies on understanding the effects of continued finetuning of multilingual models, the focus of this paper was to lay the groundwork by establishing the experimental setting on a set of representative NLP tasks and languages. The resulting set of languages chosen in our setup for evaluation (en, hi, bn, zh, ta, ja, ar, de, es, fr, th), although diverse, are still relatively higher resource. Extending the analysis to languages which were severely underrepresented (or even absent) during the pretraining of the underlying model may provide interesting insights and would be an important future work to pursue."
342,"In this section, we make a clear discussion of the limitation of our work. Our work mainly study the setting where each dataset serves as an independent domain. However, the adopted datasets (e.g. UNC, UNC+) for query-based image segmentation are mostly collected on MS-COCO (Lin et al., 2014) and have limited domain gap between visual modality. The findings could inspire the researchers to explore other settings, e.g. each class serves as an independent domain."
343,"There are three predictable limitations in the developed EmbedTextNet. First, while we have performed a thorough evaluation of EmbedTextNet on various downstream tasks, it is still a generalpurpose approach and its effectiveness on specific tasks or in specific domains may vary. Thus, further research is needed to fully understand its capabilities and limitations in different contexts.
Second, as mentioned, EmbedTextNet is most suitable for scenarios where the embedding is saved during inference, such as text retrieval or similarity measurement when the fixed embedding is saved with a vocabulary (e.g. GloVe). However, it may not be as effective in scenarios where the embedding needs to be decoded back to its original form, such as text generation.
Third, the effectiveness of EmbedTextNet is evident on a large embedding dimension, and it may decrease when working with a small embedding dimension even if it was still better than other SOTA in our experiments (e.g. GloVe-50D → 10D in Table 1). This limitation is due to the fact that EmbedTextNet is based on a VAE architecture, which is known to perform better on high-dimensional data. Therefore, it is better to compare the performances of EmbedTextNet with other SOTA and choose the right one according to the researchers’ usage."
344,"The term culture has many meanings, and before attempting to incorporate commonsense with culture, one needs to establish a well defined definition and boundary along which test cases and examples would be constructed. By focusing exclusively on food and culinary customs, we have greatly restricted our domain of inquiry. However, culinary topics are universal, and span multiple domains of common sense reasoning (physical, interpersonal, societal). Nonetheless, we hope this work will inspire future work to investigate cultural bias along many axes beyond the culinary.
Incomplete representation of all cultures: There are limitations with using countries as a proxy for culture. As noted in § 2, mappings between cultures and countries are many-to-many, not one-to-one. The majority of questions in our test set FORK focus on culinary cultures and customs of only a few countries, and we do not expect the results to generalize to all the countries of the world. We choose to focus only on one topic and a small number of countries so that we may initiate research on this broad, challenging problem with a narrower, more well-defined task. We selected these cultures based on the cultural backgrounds of the authors and authors’ colleagues who were available to provide direct feedback on/validate the data. We hope this work paves the way for follow-up work investigating a broader set of cultures.
Small Annotator Pool: The validation study in § 2.1 is done on a small pool of annotators from a few countries represented in FORK. While the study gave useful feedback about the dataset and question quality, a larger and more diverse set of annotators would reflect a broader range of perspectives within each country, and further reduce the potential for biases or inaccuracies in our data."
345,Section 7: Limitations
346,"Explainability Most current MWP solvers are only able to generate solutions. In our work, although we achieved better generalization ability, it is still hard to explain how the model solves MWPs both correctly or incorrectly. These automated solvers would be much more helpful for tutoring students if they could explain their equation solutions by generating reasoning steps."
347,
348,"Our model is developed to tackle the structural difference between the ontology and instance views of a KG. However, many modern KGs are multilingual, where different portions of a KG may not only differ in structure but also differ in the text semantics. How to jointly capture these differences remain unsolved. Also since box embeddings naturally provide interpretability to the granularity of the learned concepts, how to use the current model to discover unknown concepts from these embeddings is also challenging."
349,"There are two main limitations to our works. (1) Grammar constraint: The results of the StructSP method at the 25 SPIS setting in the TOPv2 dataset (Table 3) suggest that the results
of using grammar with low-resource data can be uncertain. The reason is that the extracted grammar from training data for low-resource setting is not general enough to capture the grammar of the new coming data (validation or test set). Therefore, for our StructSP method to work effectively, the provided grammar should cover all grammar rules if possible.
(2) Prediction time: A recursive insertion-based strategy is used for prediction. This means that the output of the previous parsing step is used as input for the current parsing step, and this process continues until a terminal signal is encountered. As a result, parsing a complex tree with multiple intents/slots (labels) can be a lengthy process due to the recursive nature of this method. Future work includes improving parsing prediction time by predicting all labels at the same level in the parsed tree rather than predicting them one by one."
350,"We would like to claim our limitations from two perspectives: application-wise and technical-wise.
Application-wise: GDA needs annotations to finetune T5, which requires more computing resources and manual labeling costs than the rule-based techniques.
Technical-wise: Our “original sentence restructuring” and “original sentence pattern approximation” tasks rely on the efficiency and accuracy of pre-ordering rules (Wang et al., 2007) and parsing methods (Chen and Manning, 2014). Although current GDA show effectiveness, we still need to find more efficient pre-ordering and parsing methods."
351,Section 6
352,"The similarity of TMs is an important factor influencing the translations of TMPLM. However, high-similarity TMs are not always available in practical applications. It is worth studying methods to make use of relatively low-similarity translations in LLM-based translation systems."
353,Limitation In Page 5
354,"The coreference annotations of the MovieCoref dataset exclude plural character mentions because the annotation guidelines did not cover them (Baruah et al., 2021). It contains few singleton coreference clusters (65). Our model only identifies singular characters and cannot retrieve singleton clusters. All the movies in the dataset have a linear narrative. Non-linear stories can confuse a coreference model because of time skips and flashbacks which is not explored in our work. Both our inference approaches require at least 10 GB of GPU memory for finding coreference clusters from full-length screenplays."
355,Section 8
356,Limitations section at the beginning of page 9
357,"There are several limitations to our experiments: we work only with English data and with datasets concerning hate speech and toxicity. Frequently such data do not represent i.i.d. samples from the data that we might encounter in real life. In addition, experiments are all conducted in the simulation with these existing datasets. The annotations in the simulated experiments were already checked for quality by the original dataset creators (Sachdeva et al., 2022; Wulczyn et al., 2017). In real-world deployment, further steps would need to be taken to ensure that the entropy in annotations truly comes from disagreements and not other kinds of noise.
While DAAL is designed to capture disagreement due to annotator positionalities, the datasets used may not have had a diverse enough pool of annotators to fully test this. In the portion of the MHS dataset used in our experiments, 67.9% of annotators were cisgender, straight, and white, while only 0.4% of examples targeted this same popula-
tion. The Wikipedia Talk dataset does not provide demographic information about its annotators.
A classifier for toxic text or hate speech trained on a pool of annotators whose backgrounds do not reflect anywhere near the full diversity of human identities (and especially the identities of the targets of the text being classified) is inherently limited. Applying such a classifier, whether it predicts a single label or a distribution, to text from and about marginalized populations not represented in the annotator pool carries inherent risks to the wellbeing of these populations. Such a classifier could systematically fail to flag content that annotators from privileged groups do not find harmful or incorrectly flag innocuous speech written by members of marginalized groups."
358,Section 7
359,"Introducing the regularizers inevitably incurs additional computational cost in the training of PETs. To show their impact on the training speed, we plot the time-performance curves for both PDF and SDE regularizers on full-set GLUE in Figures 5, 6, 7 and 8.
On different PETs, the regularized PETs with PDF regularizer has similar running time to the vanilla PETs. On the two large datasets, QQP and MNLI, regularized PETs with SDE regularizer take about 2 to 3 times longer to achieve the best performance than vanilla PETs. However, on medium-sized (QNLI, SST-2) and small datasets (CoLA, MRPC, RTE), the time to achieve the best results with SDE regularizer is comparable to vanilla PETs.
Overall, the PDF regularizer can effectively improve the performance of PETs without introducing much computational cost. In scenarios where there is relatively more focus on the inference performance of PETs and less concern about the slightly longer training time, or when the dataset is small, SDE regularizer should be a good choice.
Our method does not introduce additional risk to the original risks of PETs."
360,"When building LMentry, an important criterion for a task is the ability to easily create many examples. One of the benefits of this approach is in handling model memorization: if a model achieves very good results on some task(s), one can quickly create additional examples to verify whether the success is indicative of task competence or originating from example memorization. That being said, a few LMentry tasks are inherently limited in this regard. For example, the number of possible examples of word starting with letter is limited by the number of letters in the English alphabet. We opt to include these tasks in LMentry as we judge their benefits (e.g. broader capability coverage) to outweigh this limitation. To mitigate this disadvantage, LMentry task data also includes a “canary string” (Section 2). While a canary string helps in filtering out data from training corpora, it is not a catch-all solution.
Additionally, our experiments include some models whose exact implementation details are yet to be publicly available (e.g. TextDavinci002). We include these results as these models have the best LMentry performance at the time of writing. To shed as much light as possible, we provide the predictions of these models along with all available metadata from the API. This includes the logits of the final layer for the top 5 tokens at each position."
361,"We now explain the limitations and potential risks of our work. First, it seems the Knowing-how & Knowing-that task is a bit unfriendly to supervised learning methods as we only annotate the testing set. However, towards practical industry-scale applications, we encourage future work to utilize the current annotations and contribute to more efficient heuristic, unsupervised, self-supervised, or weakly supervised methods, etc. Second, each user manual in OHO only contains one user (agent). However, there are a number of user manuals involving more than one agent, e.g., “invite your friend as a new user and get cash back”. This motivates us to explore multi-agent user manuals in our future work. Third, in addition to the textual content, many user manuals contain visual information like images and GIFs. Hence, it will be more desirable to add such user manuals and study the Knowing-how & Knowing-that task in multi-modal settings."
362,"To some extent, DSpERT pursues performance and interpretability over computational efficiency. The major computational cost of a Transformer encoder is on the multihead attention module and FFN. For a T -length input and a d-dimensional Transformer encoder, the per-layer complexities of the multihead attention and FFN are of order O(T 2d) and O(Td2), respectively. When the maximum span size K ≪ T , our span Transformer brings additional O(K2Td) complexity on the attention module, and O(KTd2) complexity on the FFN. Empirically, training a DSpERT consumes about five times the time for a shallow model of a same scale. However, this issue can be mitigated if we use fewer layers for the span Transformer (Subsection 4.3).
As noted, we empirically choose the maximum span size K such that it covers most entities in the training and development splits. From the perspective of F1 score, this heuristic works well, and DSpERT performs favourably on long-span entities as long as they are covered. However, the entities with extreme lengths beyond K will be theoretically irretrievable."
363,"In this paper, we does not specifically discuss morphological problems and polysemy problems, and does not develop special strategies for both problems such as Pham et al. (2021) and Emelin et al. (2020). Besides, the simulated lexical constraint dictionary, which is extracted from the parallel sentences of the training set based on automatic word alignment, may be different from the real lexical constraint dictionary provided by users."
364,"One limitation of our approach is that incorporating acoustic features from an SSL speech encoder, in our case WavLM, introduces extra latency overhead, as we use a standalone ASR model for firstpass. Therefore, our approach may not be appropriate for certain applications that have exceptionally low latency constraints.
Another limitation is that while multi-modal LLMs have the potential to improve ASR performance, they can be more complex and harder to interpret than text-only LLMs. This makes it more challenging to understand the model’s decision making process or debug any potential errors.
6Qualitative examples are presented in Appendix E."
365,6 Limitations.
366,"Our method has some limitations that we would like to explore in the future. Firstly, our method
is based on the PLMs which require large GPU resources to train and infer models. We would like to adopt knowledge distillation technology to reduce the number of model parameters while keeping the performance as much as possible. Secondly, the summary generation process still lacks enough controllability even though we incorporate various features of users and products into the saliency estimation and auxiliary inputs of the decoder. In the future, we explore aggregating the characteristics of users and products into the decoder layers to make the generation process more controllable."
367,"The present work only points out problems of existing research and presents no final solutions. We also simplify the assumption that an automated metric validated for a specific TST task generalizes to other tasks. However, this is problematic since there is, to our knowledge, no investigation of whether a validation on one task generalizes. This concern is motivated by the fundamental differences in how different TST tasks are defined. There are several different definitions, such as datadriven TST (e.g., sentiment transfer) and linguistically motivated TST (e.g., formality transfer) (Jin et al., 2022). Also, we consider only TST papers (no text simplification) and focus on top-tier NLP and AI venues (non-workshop)."
368,"Limitations
A2. Did you discuss any potential risks of your work? Not applicable. We review existing work in terms of text style transfer evaluation and try to point out existing problems.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 0, 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? We conducted an extensive meta-analysis of text style transfer. We surveyed 89 works summarized in
Sections 2, 3 and the Appendix"
369,"Many challenges remain in the follow-up of our work. Here are some limitations that we intend to resolve in the future:
• We need a more robust method for compression. Although our method achieves consistent improvements in most experiments, we notice that the benefit is limited in the noisy sets, especially under a high down-sampling ratio. This drives us to develop a more robust down-sampling method for preserving meaningful information even with high compression.
• Our method compresses all the input acoustic features with the same ratio, where the ratio is determined according to the whole dataset. However, the speed of each audio is different, which results in obstacles to unified down-sampling. Ideally, each sample should be compressed with a self-adaptive ratio."
370,"Section 7.
7 A2. Did you discuss any potential risks of your work? We propose a method for acoustic encoding, which does not have any risks."
371,"We do not propose a solution for extremely lowresource languages, where neither unlabeled text for building language models, nor native speakers are readily available. Examples of such languages include Muscogee, with about 4500 native speakers, and 325 articles in the Muscogee language Wikipedia, and Arapaho with about 1000 speakers and no Wikipedia articles. In such cases, finding even a single expert annotator might be difficult. The development of resources in such languages, however, do not necessarily rest purely on technological factors.
On the technical side, DIRECTPROBE relies on the fact that a representation can be generated for the instance to be annotated. However, obtaining an
effective representation for structured annotations (e.g., frames, dialogue states, tables, etc) is nontrivial. While this is a problem, this is orthogonal to our contributions."
372,"In this section, we develop a clear discussion of the limitations of this paper. Our method faces obstacles when attempting to validate it on datasets other than those previously used in this paper. For example, LRS2 (Afouras et al., 2022) is a widely used dataset in visual language recognition tasks. However, since LRS2 dataset does not provide speaker identification labels, we cannot easily classify speakers into domain-specific and domainindependent sets. Despite the enormous amount of work, re-annotating existing datasets with crowdsourcing or annotating a new real-life dataset with speaker labels is a viable solution. Besides, existing data augmentation methods cannot match the generalization requirements on visual temporal-aligned translation perfectly, which inspires researchers to develop targeted augmentation paradigms based on the study in this paper to cooperate with our meta-learning strategies."
373,"Our work mainly focuses on cross-lingual sentencepair classification tasks. While it is directly applicable to single-sentence classification tasks (Li et al., 2020; Ye et al., 2020) but may require additional efforts to adapt our DPA framework to more complex cross-lingual tasks such as sequence tagging (Liu et al., 2021; Zhou et al., 2022, 2023; Zhang et al., 2021b) or question answering (Xu et al., 2022, 2023). Another limitation is that the proposed multilingual verbalizer in the DPA framework requires an external machine translator to produce the translated verbalizers. Finally, we limit the language set of the multilingual verbalizer to the set of target languages in a multilingual dataset. Extending this language set might give us greater improvement for cross-lingual tasks."
374,"Section 6
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
375,"MURMUR relies on large language models for fewshot linguistic skills like surface realization and text fusion. It is probable that smaller models do not work as well, in which case one may curate additional training data to train these modules. We
also note that our choice of logical modules is motivated by the characteristics of the task. Hence, it is conceivable that other data-to-text generation tasks might benefit from incorporating additional modules. MURMUR does not make any assumptions about the type or implementation of the modules and it should be straightforward to extend our method to other data-to-text generation tasks.
We limit our experiments to English datasets. We also adopt a simple prompting strategy for converting a reasoning path to a natural language summary by representing the path as a string. This works well in practice and OPT is typically able to resolve the module names and their arguments correctly. However, more future work is needed to understand when this fails so that better prompting methods can be developed. Despite the known limitations of standard automatic metrics like BLEU and METEOR, we use them to compare our method to previous works. While this is not ideal, we have performed comprehensive human evaluation for both tasks to further verify our claims."
376,"Due to the nature of deep learning, our method is less explainable than path-finding-based KG completion methods (e.g., CPL), which provide a concrete reasoning path to the target entity. Composing the path with multiple queries might be an applicable strategy that is worthwhile to investigate in order to extend our work on the KG reasoning task.
For the link prediction task, we adapt the “recall and re-ranking” strategy from PKGC (Lv et al., 2022), which brings a trade-off between prediction efficiency and accuracy. We alleviate the issue by applying different hyper-parameters given different sizes of training data, which is discussed in detail in Appendix C.
As a common issue of existing KG completion models, the performance of our model also degrades when the input KG contains noisy data. The advantage of our approach in addressing this issue is that it can use both corpus-based textual information and implicit PLM knowledge to reduce noise."
377,"While our work is consistent with the key aspects of Questions Under Discussion, we do not attempt to take into account all aspects of this broad framework. Most notably, we do not model relationship between questions (or question stacks), as mentioned in Section 2. While such relationships are potentially useful, with question stacks, the annotation task becomes much more expensive; currently, no existing dataset is available to train parsers in this fashion. We applaud the development of tools such as TreeAnno (De Kuthy et al., 2018) to aid annotation. Additionally, because questions are open-ended, they are inherently subjective, which adds substantial challenge to modeling and evaluating stacks. Constrained by DCQA’s setup, we also do not explicitly model QUD with multi-sentence answers, and leave this for future work.
The subjectivity of QUD analysis also means that there is no single “right” structure. This is in contrast to coherence structures that more rigorously define their structures and relation taxonomies (multiple analyses still exist in those structures, but to a lesser degree). Nonetheless, we
showed in Section 6 that consistency is still present despite documents being reworded and restructured during simplification.
To evaluate our parser, we developed a human evaluation scheme. As mentioned in Section 5, automatic evaluation of QUD structure contains both a generation and a question-answering component. However, human evaluation is costly; future work looking into the development of automatic evaluation measures can be extremely valuable."
378,Section 8
379,"The SWS benchmark have two limitations: (1) The sentences in the SWS testing set come from students’ essays, which limits the system’s ability to test its performance in other specific domains such as laws or medicine. (2) the SWS corpus is at the sentence level, but some writing suggestions can only be made after reading the entire article, which are not included in our SWS dataset."
380,"The paper presents a dependency-aware symbolic reasoning approach for logical data-to-text generation. All technologies built upon the largescale PLM more or less inherit their potential harms (Bender et al., 2021). Besides, we acknowledge some specific limitations within our methods:
1. Data-to-text generation is essentially a one-tomany problem since there is more than one plausible and logically-consistent description given a specific table. Our approach has little control over the diversity and the logical form of the generated template. It is also possible that our approach only generates trivial or naive descriptions if trivial data dominate in the training dataset.
2. Our work mostly focuses on the named entities in the description, but logical consistency is not all about entities. The syntactic structure or other semantic information also has an influence on generation fidelity, and we leave the symbolic reasoning for more complex logical structures or formats as our future work.
3. Our table-compatible programming language is mainly designed for simple flat tables, and extra operators are necessary before it could be applied to all tables, especially hierarchical tables where its header exhibits a multi-level structure (Cheng et al., 2022).
4. Currently, it is difficult to directly integrate GPT-3 (Brown et al., 2020) or other LLMs
into SORTIE to substitute the PLM backbones. The reason is that LLM can not be used for encoding since we have no access to the dense representation in an LLM. It might be plausible to only use LLM to generate a template and use another PLM to do encoding, but we leave this exploration to our future work."
381,"This paper proposes a denoised structure-to-text augmentation framework for event extraction (DAEE), which generates and selects additional training data iteratively through RL framework. However, we still gain the following limitations.
• The framework uses reinforcement learning to select effective samples, which is a process of iterative training and predicting the generation model, policy model, and event extraction models. The iterative training framework is complicated and time-consuming compared to the standalone event extraction model. • Even the Argument Loss decreases the number of unmatched arguments in a generated sentence, the generation model generates more fluent sentences while at the expense of the ability to ensure that all the event arguments are included completely."
382,"We report several limitations of our proposed framework in this section.
1. Limitations due to pre-trained models: The first limitation is the reliance of our system on thirdparty hatespeech detectors which are reported to have bias towards minority groups. These models tend to overestimate the prevalence of toxicity in texts having mentions of minority or protected groups due to sampling bias, or just spurious correlations (Paz et al., 2020; Yin and Zubiaga, 2021; Waseem, 2016; Dhamala et al., 2021). Also, these models suffer from low agreement in annotations partially due to annotator identity influencing their perception of hate speech and differences in annotation task setup (Sap et al., 2019). Please note that we aim to overcome this unintended bias problem by using principles of causality but still don’t claim to have completely eliminated the problem.
2. Limitations due to training corpus: We are limited by the distributions of our training corpora in terms of what the model can learn and infer. Further, OWTC dataset used in our perplexity evaluations is a subset extracted from OPENAI-WT which contains a lot reddit and news data, where reliability and factual accuracy is a known issue (Gehman et al., 2020).
3. Limitations due to language: Our experiments are conducted experiments only on English language which could be further extended to other languages.
4. Limitations due to model evaluation: Previous studies have shown that detoxification approaches optimized for automatic toxicity metrics might not perform equally well on human evaluations (Welbl et al., 2021). A future direction of work may be to include human evaluations as part of the data.
5. Limitations due to distribution shift: There are three different datasets that are in use. The first is the dataset used to train the ATE scores. The second dataset is the set of prompts used to finetune the model. The third dataset is the dataset that is used during testing. A distribution shift between datasets may have an adverse affect on our model. For instance, there may be words which occur in the test set that are neither in the ATE training set, nor in the fine-tuning set. In case of such a distribution shift between the datasets, our model may not work as expected."
383,Section 7
384,"Although our method can improve the performance as well as accelerate the inference speed, it suffers from two problems: (1) the diversity of generated results is low compared with language models (LMs) due to the clamp sampling strategy, and (2) the diffusion steps of post-tuning stage should stay consistent with the steps in the training stage, and there still exists the gaps between training and inference, i.e., |T | = |K| ̸= |T ′|, To mitigate the aforementioned two issues, we can explore a better post-training or training strategy to mitigate the training-inference gaps further. In addition, we found that the diffusion model does not perform well in open-ended generation tasks, such as generating incoherent sentences. This is closely related to the drawbacks of NAR models, which have a strong conditional independence assumption. We will attempt to address this issue in the future.
Ethics Statement
It is worth noting that all the data used in this paper are publicly available, and we utilize the same evaluation scripts to make sure that all the comparisons are fair. We have replaced the people names in the corpora with special placeholders to mitigate the problematic biases (Radford et al.) issue of generation results. Although we have taken some methods to mitigate the problematic biases, such a problem cannot be solved completely. We urge the users to cautiously apply our methods in the real world and carefully check the generation results."
385,"We only experiment with two kinds of PLMs (RoBERTaBASE and RoBERTaLARGE (appendix C.3 and appendix C.8)), leaving more diverse kinds of PLMs unexplored. While this allows us to demonstrate the effectiveness of our approach on these specific PLMs, it is important for future work to extend our problem setup to a wider range of PLMs in order to fully understand the generalizability of our findings."
386,"CSProm-KG successfully integrates both graphbased and textual representations in the KGC task, achieving substantial performance and efficiency improvement. However, similar to other PLMbased methods, this comes at the cost of increased computational resources (v.s. graph-based KGC models). In addition, we find that CSProm-KG may occasionally collapse on small KGC benchmarks (e.g. WN18RR) under specific random seeds. This is probably due to the nature of Soft Prompts, which involve much smaller number of trainable parameters, compared to fine-tuned models. However, we never see similar phenomena when training CSProm-KG in the large KGC benchmarks (e.g., Wikidata5M). We plan to solve these issues for CSProm-KG as future work."
387,"section 6
7 A2. Did you discuss any potential risks of your work? The potential risk of this line of work has already been discussed in previous research and our base methods (Bert)."
388,"Our work has several limitations. We focused on fact verification, which formulates the task sentence-pair (i.e., claim-evidence) classification. Our findings may hold for certain domains where the task format is similar (e.g., natural language inference or textual entailment recognition). We did not apply beam search on input reduction, which limits us from searching multiple versions of the reduced claims having the same length. We investigated three widely used regularization methods: temperature scaling, label smoothing, and the confidence penalty. However, other subsequent methods remain unexplored."
389,"Although form similarity is demonstrably responsible for slower translation processing, we are unable to ascertain if it is the primary reason. The work also reveals one shortcoming of alignment distributions — the measure tends to be biased towards translations with similar forms and does not always make accurate predictions about cognates. To address this limitation, future work can evaluate more elaborate models of translation that incorporate variables (e.g., form overlap, syntactic complexity, and morphological complexity) identified as relevant by previous empirical work in psycholinguistics.
Ethics Statement
We obtained all data from cited sources. Our experimental procedures and analysis do not involve human participants and are in compliance with ACL Code of Ethics.12"
390,"We identify two sources of limitations in our work: the range of metrics we consider, and the range of models we explore in our experiments.
Our paper advocates for multidimensional leaderboards. In the interest of concision, we focused on cost and latency as well as system quality. These choices reflect a particular set of values when it comes to developing retrieval models. In §4.3, we briefly consider a wider range of metrics and highlight some of the values they encode. Even this list is not exhaustive, however. In general, we hope that our work leads to more discussion of the values that should be captured in the leaderboards in this space, and so we do not intend our choices to limit exploration here.
For our post-hoc leaderboard (Table 1), we surveyed the literature to find representative systems. We cannot claim that we have exhaustively listed all systems, and any omissions should count as limitations of our work. In particular, we note that we did not consider any re-ranking models, which would consume the top-k results from any of the retrievers we test and produce a re-arranged list. Such models would only add weight to our argument of diverse cost-quality tradeoffs, as re-ranking systems must determine which retriever to re-rank, how many passages to re-rank per query (i.e., setting k), and what hardware to use for re-ranking models, which are typically especially accelerator-intensive (i.e., require GPUs or TPUs).
For our experimental comparisons, we chose four models that we take to be representative of broad approaches in this area. However, different
choices from within the space of all possibilities might have led to different conclusions. In addition, our experimental protocols may interact with our model choices in important ways. For example, the literature on SPLADE suggests that it may be able to fit its index on machines with 8 or 16 GB of RAM, but our experiments used 32 GB of RAM.
Our hope is merely that our results help encourage the development of leaderboards that offer numerous, fine-grained comparisons from many members of the scientific community, and that these leaderboards come to reflect different values for scoring and ranking such systems as well."
391,"We list the limitations of our work as follows. Firstly, the model architecture we use to localize the stuttering speech is simple. Future works could explore a more effective model to perform automatic stutter removal with the help of our SASE dataset. Secondly, we only test the English datasets. And other languages except for English and multilanguage stutter-oriented speech editing remain for future works. Finally, after being pre-trained on our SASE dataset, the stutter embedding in FluentSpeech could also be used to inject stutters into the reading-style speech to change its speaking style, and we leave it for future works."
392,"DKAF model has only been tested on English data so far. At the moment, we curate new datasets by systematic modification of existing datasets. Our simulation strategy is limited as it does not capture real-world factors (e.g. COVID-19 pandemic) that have a drastic impact on restaurant availability. Finally, It would be interesting to find a real-world dataset and verify whether the proposed methods give similar performance gains on it or not."
393,"The observed effects in this work, in principle, can only be applied to the setting of our user study (English text, English-speaking crowd-workers, colorcoded word-level saliency, and so on, as described in the paper). Therefore this study serves only as a proof of existence, for a reasonably plausible and common setting in NLP research, that laypeople can be influenced by context outside of the attributed part of the input when comprehending a feature-attribution explanation. Action taken on design and implementation of explanation technology for NLP systems in another setting, or other systems of similar nature, should either investigate the generalization of effects to the setting in practice (towards which we aim to release our full reproduction code), or take conservative action in anticipation that the effects will generalize without compromising the possibility that they will not."
394,"HELP ME THINK is a model agnostic approach that allows users to inject facts to accomplish tasks with a variety of large language models through simple Q&A but additional experiments are needed to establish its effectiveness on new language models with different training paradigms and capabilities. The entire study is conducted only with tasks of English language. Expanding the scope of HELP ME THINK to other languages will increase the scope for non-expert users. A large scale evaluation setup is further needed to reach HELP ME THINK to non-expert users.
7See Appendix F for examples."
395,Left blank.
396,"In this work, we achieve a noticeable improvement in the GEC task by introducing additional context information with a NAR model. However, in order to focus on incorrect tokens, the input of the NAR is required to be constructed based on the AR output distribution. In this way, the AR and NAR model perform sequentially, which leads to much time consumption in the training stage. In the future, we will apply a layer dropout strategy to speed up model training. On the other hand, due to the limitation of computation resources, all experiments are conducted on two Nvidia TITAN V GPUs with 12GB VRAM. Therefore, we could not compare with the state-of-the-art models which are pre-trained with 100M synthetic parallel examples (Li et al., 2022). We left it as our future work."
397,"Same tower negatives can be applied to other contrastive losses, e.g. triplet loss (Chechik et al., 2010). As we are focusing on improving the most popular method to train dual encoder models, i.e. the in-batch sampled softmax loss, we leave the application of same tower negatives to other types of contrastive loss as future work.
While SamToNe has proven to be effective to improve the training of dual encoders, its efficacy may depend on the diversity of the queries used as inputs. In dataset with a large portion of similar queries in the training set, one might need to use masking or other techniques to remove them from the negative computation. Such techniques can also improve the efficacy of SamToNe when applied to both the query and document towers, where SamToNe is currently known to hinder the performance on datasets with a low rate of unique documents, as discussed in Section 3.4.
We leave the in-depth exploration of aforementioned considerations for future works."
398,"We list the main limitations of this work as follows: • Limited NAT Models. The conclusions in this
paper are drawn from two representative NAT models, which may be not necessarily well suited for other NAT models. The main reason is that experiments on six WMT benchmarks have cost a large number of GPU resources. We therefore appeal to future works compare more NAT models using the new benchmarks.
• Carbon Emissions. This work totally costed 40,000 GPU hours (around 8,160 kg of CO2), because 1) large numbers of experiments; and 2) scaled neural networks and training data require more GPU resources. However, we hope our empirical results can help other researchers to reduce the expense of redundant model training."
399,"Although our model has achieved considerable improvements, as shown in Figure 3, our model tends to have a slight decrease in short impression generation, which need to be further solved in the future. In this paper, we follow previous studies and only utilize English radiology report datasets to verify the effectiveness of our proposed model, which is limited in verification in other languages. The main reason is that most publicly available radiology report datasets center on English. In addition, our model needs relatively more parameters than the models only using findings to generate impressions."
400,Limitations564
401,"Extension to Varied Task Formats. In this work, we limit our experiments to generating free-text explanations given a complete task sample. In future work, we aim to extend our method over more diverse settings, e.g., controllable explanation generation or synergetic generation of both task prediction and explanation. Besides, more work is needed to assess EIB’s robustness and generalization when applying it to diverse NLP domains. These domains may differ in sample type, topic, or even with different preferred explanation attributes.
More lightweight Learning Paradigm. The performance of EIB is also tied to the quality of other systems or datasets, mainly the backbone language models and automatically constructed training corpus MIXEXPL. The predictions of our method are also restricted by the capacity of the generator of EIB, where we use GPT2-small architecture as the decoding architecture. This phenomenon may be remedied if we design specific interactions with larger PLM (e.g., in-context learning) and other sources for explanation-related knowledge distillation (e.g., logical composition). For example, designing more effective prompts to induce better explanation-related knowledge from PLM to relieve the training pressure.
Diverse Combination with PLMs. While our paper focuses on the issues of explanation generation given zero-shot prompting outputs, we think EIB is easy to extend to few-shot prompting base-
lines since single-pass generation without updating also belongs to the features of conventional fewshot settings. Currently EIB still needs parameter optimization. We think future work can explore more flexible plug-and-play methods to distill sufficient and concise explanations upon large PLM.
Evaluation Quality and Consistent. Quality estimation of the natural language explanation generation is largely dependent on human evaluation due to its open-ended characteristics. Current automatic evaluation metrics are not convincing and reliable when compared to human evaluation. However, reproducing the human evaluation results across different works may be difficult. This suggests that better automatic evaluation metrics are desperately needed for free-text explanation generation. We leave improving evaluation quality to future work."
402,"Yes, in the required limitation section."
403,"We found that prefix tuning takes much longer to converge when compared to fine tuning, and for T5-Base, it takes around 10 days on a 48 GB GPU to complete tuning for a single setting in Table 1. Due to limitation of resources and with an aim to save energy, we did not conduct experiments with larger models such as T5-Large, T5-XL etc. We also did not perform experiments with smaller splits of the same datasets, which could have given further insights on how model performance varies when training data size is less."
404,Limitations
405,"This paper only considers analyzing contrastive learning in the fine-tuning stage, but we note that with isotropy being a desiderata for pre-trained language models (Ethayarajh, 2019), recent works have considered incorporating contrastive objectives in the pre-training stage (Izacard et al., 2022; Su et al., 2022). We leave analysis on this line of research for future work.
We further note that the analysis in this work focuses on theoretical properties occurred during contrastive SRL (e.g., high intra-sentence similarity), thus only focuses on semantic textual similarity (STS) data as a proof of concept. However, with the growing attention on contrastive learning, we argue that the typical STS-B is perhaps no longer sufficient for revealing the full ability of models trained with newer contrastive SRL frameworks. We call for a standard practice that the performance of contrastive SRL should be assessed on both semantic textual similarity and information retrieval tasks (e.g., Thakur et al. (2021)). We leave analysis on information retrieval tasks leveraging our analysis pipeline for future studies. For example, how high intra-sentence similarity is related to the learned attention towards tokens that enable document retrieval with better performance."
406,"Limitation section before Reference.
7 A2. Did you discuss any potential risks of your work? This paper is a foundational research for discourse understanding, to our knowledge, there should be no potential risk."
407,"There are several limitations to our work. First, although we choose the attribute-value dataset due to its high degree of interpretability and control, we acknowledge that its simplicity limits the impact of our findings. Though imitation
3We did not succeed in replicating results in Ren et al. (2020) (see appendix C).
by reinforcement is a data-agnostic mechanism, we have yet to explore how it behaves in more complex settings, such as using naturalistic image inputs or embodied communication. We defer to Chaabouni et al. (2022); Galke et al. (2022) for further discussion on scaling up communication settings.
A second limitation of our results is that we do not explore how imitation-based learning scales to k > 5 Experts. In particular, our hyperparameter regime handles up to around k = 5 Experts– very preliminary analyses on k ≥ 10 Experts suggest a need to also scale up hyperparameters such as agent size and communication channel capacity. When training agents to imitate, one must therefore consider feasibility of the learning problem– for example, as a function of the imitation network topology, communication channel size, agent size, etc– in order for training to converge.
Finally, although our work is inspired by imitation learning in humans, the extent to which simulations explain human linguistic phenomena is not clear. We intend for our work to only serve as a testbed to understand communication from a theoretical perspective."
408,"The annotation scheme proposed in this work is designed to focus on non-identidy coreference, CuT, and is not able to handle some complex linguistics phenomena. That includes (not limited to) com-
plex temporal ordering, VP or NP ellipsis under conjunction and/or disjunction, event negation. As a result, during data selection process, we had to look for those linguistic features and excluded documents with them from the data set.
Specifically, to limit the scope of the research, we intentionally limited our analysis to data that:
• Is temporally linear
• Has a single terminal state
• Has a high density of object transformations referred to explicitly throughout the text
We chose to work within the cooking recipe domain because it easily satisfies criteria. However, procedural text in general satisfies these three conditions, and our current model is therefore compatible with a broader range of domains than strictly recipes. In future work, we intend to broaden the scope to include more varied domains, such as news data and narratives.
During the manual curation of 100-document subset, we did not encounter any annotation of nominal events, and therefore this work ipso facto involves only events extracted from verbs. Although event recognition is not the primary research focus of this work, being able to additionally identify different types of lexical trigger of events is indeed important when considering broader domains. We plan to integrate our framework with other lexical resources in the future, and event recognition will receive more focus."
409,"We report the following limitations for the Text2Text-based distractor generator (the major
proposal in this study):
• The Text2Text-based generator still suffers from the concern of generating distractor same as answer or previous generated distractor. In fact, generating repeated incoherent or factual inconsistent results are commonly concerns for neural text generators (Durmus et al., 2020)(Wang et al., 2020). Although the concern is mitigated through the candidate augmentation strategy, there still are certain portions of generating the distractor of those types, as can be seen in Table 5.
• Although the CGR-based methods show their disadvantage in the evaluation, we find that CGR-based method might be a more practical one for facilitating the cloze-style MCQ preparation. The CGR-based method is able to generate ten or more candidates for educators to select, while the Text2Text generators are only capable of generating three or four distractors."
410,Left blank.
411,"Our brain MRI interpretations were evaluated by a single neurologist. Such annotations require deep expertise and are not easily carried out with high quality by trainees, which limited the amount of data we were able to collect. To ensure that the annotation would be as reliable as possible, we carefully thought of the dimensions in evaluating the generated interpretations and proposed a thorough annotation instruction guideline. We believe that future work can conduct more extensive studies using our annotation guidelines as a starting point. Further, the radiology reports we experiment with are from a single academic medical center, which makes the generalizability unclear. Future work is needed to evaluate the performance of our models on data from different medical centers. Finally, future work is needed to evaluate relevant and likely outputs from MRI interpretations to address different forms of interpretation bias and to expand the beam of potential likely diagnoses based on the findings.
Beyond the brain MRI interpretation experiments, our generation experiments are limited to a set of pre-trained models optimized for carrying out generation tasks in English. It is possible that multilingual models generating in languages other than English will show different properties. We are limited by the availability of resources for automatic evaluation in these settings, but a more extensive multilingual evaluation with human users could be conducted in the future."
412,"The main limitation of SITUATIONSUPERVISION is that situation annotations can often be expensive to curate and difficult to design (though we outline some general principles for their design in §7). Furthermore, we conducted experiments on only two datasets in this paper. Future work could explore a wider genre of texts, more domains, and more languages."
413,"In this section, we discuss the limitations of our work as follows: • As described in the paper, our proposed method
requires annotations of the latent focus; a small number of annotations (around 250 labeled samples per focus) can already bring a significant improvement (see Fig.4). Therefore when applying our approach to other domains it is necessary to prepare at least a few annotations.
• As mentioned in the error analysis section, the model is unable to generate unseen entities, such as specific drug names or laws. Further improve-
ment should be made to solve this problem for practical use."
414,"Experiments. In this work, we propose new methods for finetuning language models. We acknowledge that similar to previous approaches, our experiments are limited to English datasets and specific supervised tasks. However, our method does not use language- or task-specific tricks and should apply to other languages and tasks.
Method. As demonstrated in Section 3, SLaSh is computationally efficient and performs comparably
to the full finetuning for small datasets. Moreover, its parameter and memory efficiency makes it an excellent private learner. However, it may underperform by a few points compared to full-finetuning larger datasets with higher intrinsic dimensionality due to using very few parameters. For example, SLaSh struggles with generative tasks such as text summarization, as generative tasks are more complex and involve making predictions over the whole vocabulary. In contrast, classification tasks have relatively fewer output labels. In our initial experiments, SLaSh reached a ROUGE-2 score of 12.93 on the XSum summarization task (Narayan et al., 2018) with pretrained BART, whereas full finetuning achieves a score of 21.94 (He et al., 2022).
The limitations of SLaSh are due to the small number of parameters it updates. Since shift is applied to only certain biases, the number of parameters can not be increased beyond a limit. However, we show that SLaSh is a more efficient and performant alternative to the methods that use a similar number of per-task parameters. Moreover, we showed that joint reparametrization improves parameter efficiency of other methods. As such, this principle can be extended to methods that are not restricted by a maximum limit on the number of parameters. For example, JR-WARP’s parameters can be naturally increased by increasing the prompt length, which should improve the results further (details in Appendix A)."
415,Section 6
416,"Np Decoding uses k-means clustering to reduce the number of contextualized embeddings, the performance varies by how the contextualized embeddings are clustered. As the process is relatively inconsistent, reducing the number with other methods would make the model performance more consistent. Also, as it is not trivial to add new contextualized token embeddings on top of preconstructed CE due to the clustering step, we did not perform on dynamic corpus setup where new items are added or updated.
Np Decoding is applicable to all generative retrieval models including GMR or SEAL which needs all token embeddings, however, we focused on generative retrieval models with representative output as the retrieval target in this work. Also, while it is a general approach applicable to all encoder-decoder models, we focused on applying the method to T5."
417,"We propose TokenCluster, a sentence extraction algorithm that can automatically identify aspects in user reviews and leverage that for performing extractive summarization. Being a data-driven approach, it is susceptible to noisy data. Therefore, a limitation of TokenCluster is that the clusters of aspect-related words can be noisy and imbalanced if data is noisy. Future works can focus on more robust ways of extracting and partitioning aspect-related words. This can possibly be achieved using external data. Another limitation is that TokenCluster is computationally more expensive compared to more simple sentence extraction approaches. This is because TokenCluster clusters aspect-related words during inference, which can be restrictive when the review set is large."
418,"Admittedly, the main limitation of this work is the selection of k nearest neighbors. Intuitively, highquality nearest neighbors can make GNN learn the representation more easily. Thus, in future work, we will focus on the process of kNN selection including that attempt more measures rather than space cosine similarity distance and more representations extracted with different strategies."
419,"The work presented here has a few limitations: (1) The proposed model belongs to the memory-based methods for continual learning, which requires a memory that costs extra storage. In some extremely storage-sensitive cases, there may be restrictions on the usage of our model. (2) The proposed model has currently been evaluated under the RE setting. It is better to transfer it to other continual few-shot learning settings (e.g., event detection and even image classification) for a comprehensive study."
420,"Model sizes and comparability As we have pointed out in the paper, due to computational and time constraints on the hardware we had at our disposal, we found it was unfeasible to train larger architectures. Nevertheless, we believe our comparisons between DMLM and its direct competitor, MLM, have been fair, as we have done our best
to set a level playing field between the two. Thus, while we understand that this is a significant limitation in terms of comparability to larger models, we still think the results we have obtained could pave the way for further exploration in this direction. Moreover, we have performed architecture scaling experiments to show that it is important to continue research in this direction, and test DMLM’s capabilities on larger networks, while we did not perform a similar comparison with MLM because several works have already explored how MLM scales with network size (Turc et al., 2019).
Applying DMLM only half of the time Although we acknowledge that our choice to apply DMLM to only half of the sentences can be seen as arbitrary, we argue that it is a sound choice given the nature of our objective. Indeed, we did not want our models to rely too much on the definitions provided, or they would have required them at inference time. Such a requirement is mostly unfeasible, as it would demand running a WSD pipeline before the model’s inference, and this is incompatible or unnecessary with most downstream settings. Nevertheless, we plan on training other architectures with different frequencies, so as to better assess how impactful this hyperparameter is.
Training corpus domain Our models are trained on a sense-tagged version of WikiText-103, which only contains text coming from Wikipedia, and thus is very descriptive in style. While many other works have based their pre-training corpora on Wikipedia, we do recognize that this might be a limitation, especially for downstream tasks.
Training on longer sequences In this work, we trained language models on sentences, as opposed to what is commonly done in the literature, i.e., longer sequences of text which are usually concatenated sentences. We see a limitation here in that, in its current formulation, DMLM does not support training on longer sequences as we have no way of discerning between multiple definitions appended to our input sequence. Nonetheless, while we performed WSD at the sentence level, the corpus can be brought back to full documents, which would make sequence-level training feasible with the available data, provided that an extension to DMLM that supports multiple definitions is designed. We leave such an extension to future work.
Scaling to multiple languages Our formulation of Descriptive Masked Language Modeling can be applied to, as far as we know, virtually any language. Moreover, we argue that it might be possible, in a multilingual setting, that definitions of the same sense could help in aligning the output representations of the trained models for words sharing the same sense. Nevertheless, having said this, it is worth noting that there might be two impediments to achieving multilinguality. First, in our work, we leveraged English Word Sense Disambiguation, which, despite its recent advancements, is still far from performing the task equally well on other, even high-resource, languages (cf. Pasini et al. (2021)). Second, we decided to employ definitions coming from sense inventories which, at least in English, cover a wide number of senses with meaningful descriptions, but this might not be the case for other languages, especially low-resource or endangered ones, with BabelNet (Navigli et al., 2021) being the largest resource providing textual definitions in hundreds of languages.
Reproducibility We acknowledge that, even by releasing the code and dataset on which our models are trained, it might be hard for other interested entities (e.g., groups, people, institutions) to reproduce this work, as our training runs lasted up to 8.5 days on our multi-GPU setup."
421,"There may be some potential limitations to this work:
• Due to the maximum input length limitations and cumbersome deployments in most PLMs (i.e., BERT), we limited our input lengths with a specific selector (following previous works (Sun et al., 2019; Zhang et al., 2021b)) and searched hyperparameters in a limited range, especially in batch sizes (with a maximum batch size of 6). Theoretically, better experimental results can be reported; however, we reimplemented comparative methods and conducted all analytical experiments in the same environments with the same settings, ensuring fairness in performance comparison and problem addressing.
• Due to the characteristics of the applications in our work and the existing DG methods that are difficult to directly apply to our tasks, we only simulate the performance of plain-text models as DG benchmarks. However, textual information is inherent in UP-invariant signals for DG performance to some extent, and a comparative experiment indeed leverages the proposed method for better performance in the same OOD scenarios; therefore, it is reasonable for evaluating the performances. To further address these limitations, we will explore more DG strategies to adapt feasible DG methods applied to our personalized sentiment analysis or more complex scenarios with the external introduction of inherent domain shifts in texts such as topics (e.g., books, DVDs, electronics, and kitchen appliances).
• Last but not least, in this paper, the proposed DG method is only evaluated on personalized sentiment analysis tasks. However, more applications can be applied to our method, where domain shifts occur due to explicit knowledge injection or Fi can be augmented and exposed."
422,"Since each Chinese character contains 1 syllable, our proposed model can control the number of syllables in the generation by the number of generated tokens. However, this method does not apply to languages with multisyllabic words (such as English). To rewrite lyrics with multisyllabic words while maintaining the same number of syllables, a special technique such as syllable-level subword tokenization may be needed. This line of work will be left to be investigated in the future."
423,"While we have demonstrated the promising potential of utilizing the lower bound as a substitute for p(R|C,K) in knowledge-grounded generation tasks, there are several limitations that need to be acknowledged. First, the use of the language model (LM) as learning signals can introduce flaws. The model may exploit the LM’s weaknesses by generating comments with a high likelihood based on the LM but are nonsensical in reality, resembling adversarial samples. In our experiments, we observed that generating adversarial text samples, unlike vision models, proved challenging, and we did not encounter completely nonsensical comments. However, we did observe the model exploiting the flaws in the LM, indicated by certain common patterns in the generated comments. Second, there are better alternatives to a hard knowledge injection reward, such as an n-gram matching-based BLEU score used in this study. In some cases, a knowledge-grounded comment may not have any word overlaps with the knowledge instances, resulting in a n-gram-based score of 0. Ideally, an embedding-based soft knowledge reward would be more desirable for this reason. However, in our experiments, we found that the soft knowledge reward based on methods like (Kusner et al., 2015; Sellam et al., 2020) was easily exploitable, as the model learned to echo keywords from the context to achieve a high soft knowledge reward. Third, our approach primarily focuses on scenarios where well-constructed triplets are not readily available, such as when retrieving information from the Internet. However, in cases where pseudo knowledge construction is highly accurate, such as applications with more limited scopes, our approach may not outperform triplet-based approaches. Fourth, it is important to note that our method could potentially be used to generate offensive or prejudiced texts. Addressing biases in generative models is a longstanding issue, and it is not the main focus of this work. However, the ethical implications can be partially mitigated by integrating our approach
with other debiasing technologies."
424,"Although our experiments prove the superiority of our SCPRG model, it is only applicable to document-level EAE tasks with known event triggers because both STCP and RLIG calculate the attention product of the trigger and candidate spans. However, in real-life scenarios, event triggers are not always available. In view of this problem, we have a preliminary solution and plan to improve our model in the next work. The core idea of our method is to select and integrate context and role information based on candidate arguments and target events. Based on this idea, we briefly provide two solutions for the above limitation. First, we can make the model predict the best candidate trigger words. Second, we can replace trigger words with special event tokens. In the next work, we plan to extend our model to document-level EAE tasks without trigger words and evaluate it through extensive experiments."
425,"We acknowledge that our system has some limitations that warrants further investigation. For example, one needs to be mindful of the specific downstream applications of the proposed methods, both in terms of 1) potentially large variance in outof-distribution performance (e.g. divergent question generation applications that aim to spark children’s curiosity-driven thinking (Abdelghani et al., 2022)); and 2) of mitigating harmful/toxic contents in educational applications (Bender et al., 2021). As a result, we believe such techniques and applications are neither suitable nor safe to directly interact with children, we urge developers to use this technique in other ways, for instance, in teaching assistant application (e.g., a system that suggests examples for teachers), where the teacher can filter and modify the examples and thus making sure the content children receive is proper and safe.
We also acknowledge the prohibitively restrictive access to the GPT-3 model at the time of writing. We do believe that this constraint will relax over time, and meanwhile, hoping that our proposal can shed light on research and applications with more accessible LLMs such as GPT-J (Wang and Komatsuzaki, 2021) and BLOOM (BigScience, 2022) for future work.
While we acknowledge the many limitations
with respect to accessing GPT-3, we are not advocating against using it. On the contrary, in fact, we believe GPT-3 is still among the most cost-effective solutions especially in the context of natural language generation. The main goal of the study is thus to explore more data efficient ways of using GPT-3 to generate and evaluate questions. We strive to share our experience and insights with the community, which hopefully can be proven valuable and helpful."
426,Section 5 on page 5.
427,"We note that there are several limitations with such a sentiment knowledge enhanced self-supervised
learning approach. First, the preprocessing of massive videos is time-consuming and laborious. Second, the pre-training of our model has relatively large requirements on the GPU resources. Finally, we argue that there should not be too many videos without sentimental words, so as to avoid the model having a large bias and not learning any sentiment knowledge."
428,"section 7
7 A2. Did you discuss any potential risks of your work? Our work does not have potential risks."
429,
430,"In this work, we studied the effects of large pretrained models in the temporal video grounding task and investigated the applicability of NLP adapters for a parameter-efficient integration. While we believe our results show the efficacy of incorporating better language models in TVG models, it is important to note that we primarily focused on proposal-free TVG models and thus have no evidence to suggest such improvement would be observed in proposal-based models.
Furthermore, as our main goal was to investigate how the chosen models’ performance varied when only changing the text encoding models, we compared state-of-the-art models using different visual features. While it would be interesting and insightful to check their performance when using the same features as our chosen models (i.e., I3D), such experiments are out of the scope of this study.
Moreover, although language adapters can be stacked before a task adapter for training on the task in a new language, we have only experimented with queries in English. It would be interesting to investigate if language adapters could be applied to TVG in different languages.
Finally, as for hardware requirements, our experiments were performed on a single 40-GB NVIDIA A100 GPU from a large cluster, and we spent about 400 USD on our experimental setup. While experiments with ExCL and TMLGA can be run on smaller GPUs with no significant increase in training time (i.e., we tested with a 16-GB NVIDIA V100 GPU), for DORi, due to the size of the input features and number of training parameters, we recommend using a GPU with at least 32GB of memory."
431,"7 (Limitations)
A2. Did you discuss any potential risks of your work? Not applicable. Left blank.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts?"
432,"We recognize that this work was only performed on two tasks related to story understanding, thus it is difficult to say exactly how robust it really is. However, given the capabilities of LLMs and Code-LLMs, we believe our prompting techniques or similar will prove to be useful to the story understanding community.
Our work also assumes that the CoRRPUS will be asked the same question across stories. In other words, given an example as a prompt, CoRRPUS will follow that example to generate code for the next story. We are not providing the task to CoRRPUS and having it interpret the question to figure out what it should be tracking. We simply tell it to track certain information (e.g., objects, physical features of characters) so that it can solve these tasks. Therefore, for CoRRPUS to work, the user would need to know what information is salient for
their task and prompt it to the system. Even though the Re3 dataset contains more complicated sentences than bAbI, these are still relatively simple English sentences. We do not know how CoRRPUS would perform on more complex stories or on stories in other languages.
Lastly, there is the issue of access. Due to cost, we were unable to rerun all of the GPT-3 experiments. The pricing of GPT-3 not only hinders new research, but it hinders reproducability efforts such as ours. Furthermore, as of the publication of this paper Codex has been removed from the OpenAI API, and it is as-of-yet unknown if GPT-3.5 or GPT-4 can handle code-based prompting as well. There are, however, still other code-based LLMs available, such as Github’s Copilot and Hugging Face’s Starcoder."
433,Section 6
434,section 2.3 and the limitations section.
435,"Newsela Dataset One limitation to this study is our use of the Newsela dataset. Because this requires a license to access, researchers cannot fully reproduce our work without first obtaining permission from Newsela Inc. Unfortunately there is currently no other large dataset offering high quality
aligned documents for simplification under an open source license. The only other datasets so far used for document-level simplification are based on WikiLarge, which has very poor and inconsistent alignments at the document-level (Xu et al., 2015; Sun et al., 2021; Cripwell et al., 2023).
Paragraph-Level Human Evaluation In order to reduce complexity, our human evaluation was performed on paragraphs rather than full documents. As a result, there is a potential limit to the accuracy of human judgements when certain discourse phenomena are present. For example, important information may be excluded from a specific output paragraph (therefore prompting a low adequacy rating), but this could actually be present in a different part of the true simplified document.
Monolinguality This study focused entirely on simplification for English-language documents. Reproducing the proposed systems for use on other languages would require dedicated datasets of similar scale, along with sentence/paragraph alignments and operation labels (which likely do not currently exist). Further, the nature of simplification in other languages may differ quite a lot from English with respect to the types of operations that are performed, potentially reducing the suitability of the proposed framework.
Generalised Target Audience We approach this study with our definition of ""simplification"" being based on that of a generalised audience, following the standard set out by the assigned reading-levels of the Newsela dataset. Existing works often outline the intent for their systems to be used to simultaneously assist a wide array of different target users, such as those with cognitive impairments, non-native speakers, and children (Maddela et al., 2021; Garbacea et al., 2021; Sun et al., 2021). However, they rarely go into any detail about which simplification strategies work for each of these different groups or perform human evaluation with annotators from the same target demographics (Gooding, 2022). As such, we acknowledge that using our systems for a specific demographic might prove insufficient to enable their consumption of media without first making further revisions to support their precise needs."
436,"Limited by the scale of annotated contrastive explanation corpus, our CPACE model is only fine-tuned on approximate datasets selected with some designed principles. The performance of our method can be further improved with sufficient high-quality contrastive explanation annotated datasets over more NLP tasks. Moreover, in this paper, we mainly explore the effectiveness of the CPACE model for multiple-choice commonsense questionanswering tasks, which is our goal, while previous retrieved-augmented methods cannot provide highly relevant knowledge or context for reasoning. Due to the fact that the contrastive explanation is designed to provide distinguishing information among given options [a1, a2, . . . , an] or labels, there are no given candidates or labels in generative commonsense question-answering tasks, therefore, our CPACE model cannot directly fit to other generative QA benchmark datasets. However, in our work, we provide some insights for future exploration, that is, generating question-specific distinguishing knowledge with a contrastive explanation generator can improve the performance and interpretation of current reasoning models. Meanwhile, although we validate the generalization of CPACE on other QA tasks, including QASC and OBQA, the effectiveness of our model in other NLP tasks requiring contrastive knowledge enhancement, such as open domain dialogue, needs to be further explored. In the future, following the CPACE model, we will explore a unified contrastive explanation generation framework for the generative commonsense question answering tasks via generating the chain-of-thoughts with a large generative language model-based generator, such as InstructGPT (Ouyang et al., 2022), BLOOM (Scao et al., 2022) etc., or generating topN possible candidates and ranking them with distinguishing knowledge, which is beyond the scope of this paper to explore and is also our future work."
437,"Despite we largely improve the performance of the existing conversational search method, the mechanism of the self-supervised tasks in our SSP is simple and intuitive. Additionally, our post-training method relies on the external query reformulation dataset, which is a compromise under the scarcity of conversational search data. However, the essential contribution of this work is that we point out the significance of modeling dialogue structure (especially for topic shift), and the phenomenon of contextual semantic vanishing in conversational search for the first time. We hope future works could pay more attention to these problems and devise more complex methods to develop more
powerful conversational search systems."
438,"Section 8
7 A2. Did you discuss any potential risks of your work? This work focus on conversational search, which has no obvious potential risks."
439,"Under-explored multilingual generalizability. We evaluate the efficiency of our metric mainly for English. We aim to extend this work to other languages using language-agnostic PPDB as described in Section 3.2.
Restricted to lexical simplicity. We mainly examine the lexicon-level effect of the quality of the text simplification (i.e., the relative simplicity and meaning preservation of the output tokens). Other sentence-level factors that could have an effect in simplicity is not explored in this paper, such as the compositional difficulty (e.g., whether the sentence uses a inverted order) and comprehension difficulty (e.g., a sentence written with simpler words may still be hard to understand). Empirically, we observe that all the involved metrics correlate poorly with StS. We aim to conduct further research exploring the automatic metrics on sentence-level simplicity without external knowledge bases or human references."
440,"Section 8
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
441,"Our experiments are limited to 3 DataMUXed pretrained models (N = 2, 5, and 10) due to compute constraints. More pre-trained models with different N ’s would provide PruMUX with more options to improve throughput and would allow us to conduct a more detailed evaluation of Auto-PruMUX.
PruMUX uses CoFi as its model compression method. Experiments with other methods could improve our understanding of the interactions between model compression and data multiplexing."
442,Section 7.
443,"There are three points to discuss and they may inspire further investigation. First, since the length of empathetic conversations in the current benchmark dataset EMPATHETICDIALOGUES (Rashkin et al., 2019) is relatively short, the theory of selfother awareness could be explored under the circumstance of long conversations to maintain the self-awareness of chatbots for the long run. Second, for the better comprehension of self-other awareness, it is helpful to introduce more commonsense knowledge of higher quality. Finally, current automatic evaluation metrics are still not rational and proper to measure the ability of empathy. It is desirable to build better evaluation metrics for empathetic responses."
444,Section 7
445,"We use scientific papers as a first testbed for evaluating model robustness to layout distribution shifts. Many different layouts exist among scientific papers, and the existence of metadata databases facilitated the construction of train-test splits with layout distribution shifts. However, scientific papers are only one domain in which layout distribution shifts occur. Layouts also vary for many other visuallyrich documents, such as business forms, receipts, webpages, and newspapers. We hope our evaluation methodology engenders evaluations on a wider range of document types.
Our experiments involve a subset of the many layout-infused models proposed in recent work (e.g., Peng et al., 2022; Kim et al., 2021; Li et al., 2021). The models in our experiments were chosen because they share a similar model size and underlying architecture, facilitating comparisons between different methods of layout-infusion. We release our evaluation suite to enable more comprehensive evaluations in the future.
Performance drops occur both for layout-infused and, to a lesser extent, text-only models. The performance drops from text-only models may be due to layout information conveyed via word order and visual section boundary markers, but may also reflect shifts in text distribution. Our error analy-
ses suggest that generalization errors are driven by shifts in layout rather than content (Section 5.5). In the future, synthetic experiments (e.g., with LaTeXbased manipulations) would help to fully disentangle the effects of layout and content distribution shifts, provided that large-scale synthetic manipulations can be constrained to produce realistic layouts."
446,"Model Capabilities We observed that current LMs struggle to generate several categories of examples. LMs struggled to generate examples related to concepts they do not understand well (e.g. cryptography and steganography). As discussed in §6, we also found that LMs struggled to generate examples with many constraints, in particular, those in the BBQ dataset (Parrish et al.,
2022). We expect these limitations to wane as LMs grow more capable with scale. Lastly, many evaluations related to LM capabilities require the dataset creator to know how to solve the evaluation. We expect that LMs will not be able to generate high-quality evaluations of this kind (e.g., to test for factual knowledge they do not yet know). Our approach is thus differentially useful for evaluating other properties of models aside from capabilities (e.g., safety-related behaviors).
Model Biases LMs learn biases from their training data (Sheng et al., 2019; Gehman et al., 2020; Brown et al., 2020), impacting the generator pg and discriminator pd. For example, generated evaluations may exhibit gender or racial biases and be lower quality for languages under-represented in the LM training data. LMs will also be systematically worse at generating evaluations for tasks that are omitted from their training data (e.g., due to copyright, licensing, or privacy issues).
Example Diversity We found limited example diversity for some kinds of evaluations (§B.5) though not all (§A). Diversity appears to depends on the kind of evaluation generated, the generation hyperparameters, and the prompt used, and thus sometimes requires e.g. hyperparameter tuning to get right. We found data visualizations to be powerful tools for understanding and debugging data diversity, such as those at evals.anthropic.com/model-written. Qualitatively, we also found that using pd to rank/filter examples limited the diversity, since pd sometimes selected for prototypical examples for testing some behavior (observed qualitatively by workers in §3.2). We are excited for future work to explore other methods that achieve similar example quality with higher diversity than our method, such as generating many examples and subsampling for diversity.
Instructions May Be Misunderstood LMs, similar to crowdworkers, may generate evaluations that are testing something different than intended, especially if the generation instructions are underspecified. For example, using the method in §3.1, we generated statements that a person who “shares beliefs with Derek Parfit” (the influential analytic philosopher) would agree or disagree with. The “disagree” statements were often ones that many people, not just Derek Parfit, would disagree with (“I support slavery” or “I believe evolution never happend”). In this case, we should have
provided more specific instructions to the LM, to have it generate examples that Derek Parfit would disagree with but that another philosopher would agree with. When feasible, we recommend briefly examining the generated data, to catch salient issues such as the above.
Sensitivity to Instructions Our approach allows the dataset developer fairly fine-grained control over the evaluation by using instructions to guide pg. However, the quality of LM outputs is sensitive to text inputs in unintuitive ways (Perez et al., 2021; Lu et al., 2022), adding hard-to-predict variance to the quality of the resulting evaluation; see Appendix §B.4 for a possible example of this effect we found. We hope that LM advances such as instruction-tuning (Wei et al., 2021; Sanh et al., 2022; Ouyang et al., 2022) mitigate this issue in the future. For now, it may be possible to use prompt sensitivity to generate more diverse datasets, by generating similar datasets with distinct prompts and combining the results, as we did in §3. Where prompt sensitivity caused issues, we found it helpful to be able to view example generated outputs in seconds, to quickly iterate and catch salient failures. For the 133 datasets in §3, we found a general instruction template that worked well; we did not do dataset-specific tuning to obtain samples rated as high-quality by human evaluators.
Hybrid Human-AI Evaluation Generation We are optimistic that hybrid human-AI dataset generation for mitigating many of the issues above, e.g., the approach in §6. For example, it is possible to generate many (possibly flawed) examples with LMs and edit or filter them manually. Hybrid approaches have succeeded in generating evaluation data (Wu et al., 2021; Ross et al., 2021), training data (Liu et al., 2022), and adversarial training data (Nie et al., 2020; Xu et al., 2021; Bartolo et al., 2020, 2021; Ziegler et al., 2022).
Text Generation Evaluations We focus on generating classification-style evaluations, but many evaluations require text generation (e.g., to evaluate text summarization). For text generation evaluations, we recommend the related approach of Perez et al. (2022); Zhang et al. (2022), who generate inputs and evaluate LM outputs using an LM-based classifier.
Potential for Misuse Our results suggests that malicious actors may be able to use LMs to
evaluate LMs’ tendencies to act in harmful ways, to exacerbate such tendencies. For example, a malicious actor may evaluate LMs for their tendency to persuade people towards their own political views, in order to influence the public’s views towards their own. Another issue is that our method is potentially useful to adversaries in finding and exploiting weaknesses in existing models (e.g., to circumvent product safety measures like safety filters3). See Perez et al. (2022); Ganguli et al. (2022) for discussion on such risks, as well as mitigating factors and interventions. Despite the above concerns, we believe it is beneficial to publish our work on LM-written evaluations. LM-written evaluations are also valuable to good actors and efforts to deploy LMs, to catch and mitigate misuse harms as well as accidental harms. In this work alone, we believe that we surfaced several potential issues that, to our knowledge, have not been found before – related to model outputs that express powerseeking tendencies, self-preservation instincts, various strong political views, and tendencies to answer in sycophantic or less accurate ways depending on the user. Overall, our results provide evidence that LMs themselves are a valuable tool for helping us evaluate LMs."
447,"1. Our investigation in the zero-shot experiment is not exhaustive, we focused on the interplay between the three main tasks that also provide datasets of similar size: argument identification, evidence detection, and argument quality. However, there are other tasks, such as stance classification (deciding whether an argument supports or opposes a particular issue) or argument structure identification (identifying argumentative discourse units, such as claims and premises). Other tasks might be better source tasks for estimating argument quality.
2. Our experiments are based on the most popular datasets in argument mining and argument quality and may not generalize to other more specialized text domains, such as law or politics.
3. Using only English datasets limits the generalizability of the results to other languages and cultures. The ability to identify and evaluate the quality of arguments may be different in other languages and cultures, and the annotators may not be able to accurately capture these differences. This may lead to a lack of robustness and reliability of the results."
448,"The findings of this study have to be seen in light of some limitations. Compared to CDGM (Lai et al., 2021), introducing multiple metrics to capture flexible cross-sentence event relationships would cause extra computation cost and slightly slow our training. The training parameters of different models can be seen in Table 12. We deprive our model of multiple similarity metrics and denote it as the baseline model. We can see that our model increases 7% parameters compared to CDGM and 0.009% compared to the baseline model."
449,Section 7 limitation
450,"This section discusses the potential limitations of our work. This paper’s analysis of model effects mainly focuses on common benchmarks for adversarial detection, which may introduce confounding factors that affect the stability of our framework. Our model’s performance on more tasks and more attack algorithms is worth further exploring. Our detection framework exploits the special properties exhibited by the adversarial sample under universal perturbation. We expect a more profound exploration of improving the connection between UAPs and adversarial samples. In Figure 2, we note that a small number (about 3%) of clean and adversarial samples do not suffer from UAP interference. It is worth conducting an analysis of them to further explore the robustness properties of the language models. We leave these problems to further work."
451,"While the potential implications of our research are broad, we make note that there are several limitations that should be considered:
1. The scope of this study is limited to legal reasoning tasks using the COLIEE Task 4 (Entailment), which is based on the Japanese Bar exam. The results may not generalize to other legal reasoning tasks in particular common law systems.
2. COLIEE Task 4 itself depends on the COLIEE Task 3 (Retrieval), and we assumed perfect
retrieval of the relevant articles used for the premises.
3. The experiments were conducted with two versions of GPT models only, and it is unclear how other LLMs may perform with this task.
4. The study focuses on zero-shot/few-shot and fine-tuning approaches with and without explanations, as well as various prompting strategies. The explanation and the prompting strategies, however, are difficult to control. For the explanation, we rely on the explanations created by GPT-3.5 without knowledge of how reliable they may be. For the legal prompting, we show that legal strategies have a positive impact on the performance, but it is unclear how explicit mention of the strategies impacts the LM.
5. The use of clustering past training data as fewshot demonstrations is a novel approach, but it is not clear how well it would perform on other data sets (or in other domains). We do not claim that this approach would show improved results for other tasks.
6. The experiments were conducted on the most recent two years of COLIEE task data, and the results may not generalize to other years of the task. More importantly, the test data size is relatively small which is reflected by the mildly statistically significant results for 2021.
7. The experiments were carried out on the English translations only and not the Japanese original text.
8. OpenAI maintains control of GPT-3 and future models, and we cannot guarantee that the model versions used will be available to others in the exact same state."
452,"Section 8
7 A2. Did you discuss any potential risks of your work? We don’t think there is any risk of our work."
453,WFRE suffers common drawbacks from the existing query embedding methods. The queries that can be solved by such methods are a limited subclass of first-order queries. It is also not clear how to apply WFRE to unseen entities and relations in an inductive setting.
454,On Page 5.
455,"Compared to other interpretability methods, MaRC is able to create explanations that more closely resemble human rationales. Nevertheless, the similarity to human rationales is always limited by the inner workings of the respective neural network: If a network’s reasoning does not mirror human reasoning, the resulting rationales will be incomprehensible to humans.
Additionally, rationales created by MaRC are the result of a complete input optimization process. Therefore, the rationale creation usually requires hundreds of forward passes and gradient evaluations for the respective neural network, which makes the process of creating the rationale timeconsuming and therefore infeasible for many realtime applications. On modern hardware, creating a rationale for BERTbase can take two to three minutes depending on the length of the input text, while ResNet-101 and ViT-B/16 are faster at about one minute."
456,"Dataset Utilization We have collected 137 datasets, yet we have only conducted experiments over a minority of these (∼40 datasets), leaving the remaining datasets unexplored. Since the datasets are already curated, future work should further explore these datasets in additional experiments. In this work, we do not experiment on image-text datasets for two reasons: (1) all of the image-text datasets are translated from English versions; and (2) there is no large LM available for zero-shot image-to-text generation.
Experiments We did not attempt few-shot or fully-supervised learning experiments in NusaCrowd since prior work has explored these approaches on some of the datasets (Wilie et al., 2020; Koto et al., 2020b; Cahyawijaya et al., 2021b; Winata et al., 2023). We specifically conduct our experiments on zero-shot methods to explore the generalization of zero-shot cross-lingual and zero-shot prompting approaches to extremely lowresource languages.
Task Diversity The tasks represented in NusaCrowd are skewed towards MT, sentiment, abusive text classification, and ASR. Many other tasks remain unexplored for Indonesian and regional languages. Furthermore, most ASR work come from the same authors or research groups. While these topics are prevalent among Indonesian researchers, it is also important to expand to other tasks.
Domain Diversity The datasets in NusaCrowd are primarily from the domains of social media,
news, and other general domain sources. Despite having a huge potential, narrow-domain datasets, such as clinical, biomedical, legal, financial, and educational datasets remain underrepresented for Indonesian and regional languages. Exploration of domain-specific data and use cases for Indonesian and regional languages is critical.
Language Diversity There are 700+ languages in Indonesia. However, we have only focused on a small fraction of these languages. In addition, there are also other regional languages similar to the two Sinitic languages in NusaCrowd, i.e., Hakka (Khek) and Min Nan (Teochew). More focus on under-represented languages is an interesting future direction.
Multimodality The datasets in NusaCrowd are mainly in the text modality. Exploration of speech, image, and other modalities for Indonesian and regional languages is still limited, and there are potentially exciting opportunities to capture locallyrelevant Indonesian culture in such modalities.
Utilization of Datasets There are 137 datasets contained in NusaCrowd. While we showcased three different use cases for the datasets (i.e., zeroshot NLU, zero-shot NLG, and multilingual ASR benchmarks), there is much greater potential to use the datasets in NusaCrowd. Potential areas of focus include experimenting with various approaches and analyses over multiple datasets, such as multi-task learning, continual learning, or few-shot learning."
457,"Our work proposes a new QG framework, namely SkillQG, to frame the comprehension skill required by a question and generate the corresponding comprehension-oriented questions. The limitations are three-fold:
Firstly, we propose a new skill-based schema for the comprehension nature of questions and map the existing annotations on narrative elements of the FairytaleQA dataset to it and conduct our experiments. This kind of mapping might not reflect the required skills accurately since a narrative element can cover more than one comprehension types. Furthermore, although our proposed skill-based schema is drawn upon general text comprehension, SkillQG is only verified on the FairytaleQA dataset and lacks the analysis on generalizability. However, identifying skills and correlations with comprehension skills on new datasets can be challenging because SkillQG may struggle with the input passage with a relatively simple discourse structure, which usually does not contain complicated relations. One remedy to this issue could be collecting a new QA dataset with the annotations following our proposed schema. We regard it as our future work and deem designing a new annotation specification a promising direction.
Besides, although we boost the downstream QA performance in Section 3.4 by augmenting the original training set with generated questions, the final performance (56.9%) is also far behind the human performance (64.4%) reported by Xu et al. (2022). However, the breakdown analysis of QA performance demonstrates that SkillQG can strengthen
all of the comprehension capabilities, especially the challenging ones. As a result, generating questions that are matched with the current comprehension capabilities of the QA model and co-evolving the QA system and corresponding QG system, could be two interesting research topics.
Last but not least, our SkillQG is built on the PLMs of general domains, ignoring the domainspecific and multilingual application. The backbone PLMs are also shown a biased representation, such as race and gender (Gonen and Goldberg, 2019). Therefore, additional evaluation protocols are left for our future work."
458,Both risks and limitation are discussed in the (unnumbered) section with the same title. This section is located after Section 5 (Conclusions).
459,"Our work has the following limitations. First, our analysis of continuity and continuity salience only focused on the sentence level. This is limiting since actual continuous spans can be a part of tokens in a identified copied sentence. Besides, we only utilized string-based overlap for salience estimation, i.e. ROUGE. This can be limiting since semantic salience may not be captured. Furthermore, even if our method can alleviate the negative impact of high dataset extractivity, it may not fully address this issue. In the future, we plan to extend our analysis to token-based continuous spans identification and semantic based measurement for more accurate continuity quantification.
Ethics Statement
The progress in deep neural network architectures and the availability of large pre-trained language models have led to significant advancements with single document summarization. However, current state-of-the-art natural language processing (NLP) solutions still face challenges in consistently generating factual and faithful summaries without any instances of hallucination (Maynez et al., 2020). Therefore, it is imperative to acknowledge that our proposed solution, like previous approaches, is not yet suitable for deployment as it does not specifically address the issue of hallucination. To bridge this gap, future research efforts should prioritize the development of more effective evaluation measures and solutions for text summarization, aiming to ensure highly faithful summaries that accurately represent the source content and enhance the overall trustworthiness of summarization systems. Additionally, in the case of applying the proposed method to sensitive data domains such as medical patient records and legal documents, it becomes essential to incorporate privacy-preserving policies to safeguard the confidentiality of personal information (Da Silva et al., 2006). These measures are critical to instill confidence in the practical implementation of text summarization techniques."
460,"Our findings about gender bias in the field of Language & Vision are based on two datasets, one task (Object Naming), one language (English), a mostly Western population (based on the origin of both the images and the annotators of VisualGenome and ManyNames), and one computational model. Moreover, in the third analysis, due to the characteristics of the test set of ManyNames, the sample size was small. Additionally, the bias around naming choices concerns the domain of sports only.
Regarding our most novel finding (bias in lexical choice), given the basic function of naming in language and the fact that Western Englishspeaking societies are not known to be more genderbiased than most non-Western and/or non-Englishspeaking societies, it is plausible that the identified bias extends to other L&V tasks such as image captioning, referring expression generation, or Visual Question Answering. Furthermore, given previous work on bias in our field, it is plausible that the identified bias in the model extends to other models. It is, however, not clear whether the bias will be amplified or simply reproduced. To probe whether, and to what extent, the identified biases indeed generalize, future work should tackle more tasks, languages, populations, domains, models, and data. Testing further models on the same naming data that we used is straightforward; checking for biases in some other tasks for English should be feasible at least to some extent, since some datasets provide multiple annotations per image (e.g., captions in MS-COCO). Instead, analyzing other languages and populations, such as nonbinary individuals, will in most cases require further data collection due to the scarcity of non-WEIRD data in our field.
In this study, gender was operationalized in a bi-
nary manner in order to most effectively investigate the stated hypothesis. Furthermore, there is a lack of nonbinary labels within the datasets used (5% of labels can be considered gender-neutral, i.e., ""person, human, child""), and the resources required to reflect the reality of the gender landscape currently do not exist. This indicates a separate but related issue regarding a lack of representation of nonbinary individuals within vision datasets and how to conduct ethically inclusive studies on gender (Larson, 2017). However, addressing this is beyond the scope of the present research and remains an important direction for future work. Finally, this work solely concerns the identification of biases; further work should focus on how to deal with them in terms of data collection, curation, and modeling."
461,"Left blank.
7 A2. Did you discuss any potential risks of your work? We do not think this is relevant to our submission.
7 A3. Do the abstract and introduction summarize the paper’s main claims? The abstract does, the introduction no because it’s a short paper and we distribute the claims between the introduction and the conclusion.
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts?"
462,"In this study, we assessed the quality of the corpora Books-Small, Medium, and Large, by training and evaluating a NER model on them but we did not include the corpus Books-Huge in our analysis. However, our results on the Books-Large corpus indicate that there is no substantial benefit to using a corpus larger than Books-Medium for training NER models. This is consistent with prior research on few-shot training on smaller corpora achieving comparable accuracy to larger, potentially noisy corpora (Blouin et al., 2021).
Given the scarcity of benchmarks for late medieval NER (Ehrmann et al., 2021, Table 3), we were unable to conduct experiments on corpora other than our own. Additionally, we utilized a NER model trained on contemporary texts as our baseline for comparison. Therefore, it is important to note that these results may not generalize to other medieval NER tasks. In the future, efforts should be made to develop more comprehensive benchmarks for late medieval NER such as our own."
463,"In this work, we found that the implications of the geometric properties of LayerNorm affect mainly small models and are less evident for larger models.
We hypothesize that with a large hidden dimension, a Transformer model can find other solutions for computing “majority“ using gradient descent and is, therefore, less dependent on the projection component. Further, we believe that the scaling component is less useful for high dimensional models, since with higher dimensions, it is less likely to encounter a set of vectors where some of them lie within the convex hull of the others. Therefore, we encourage the community to use LayerNorm before attention layers, especially for small models that operate on long sequences.
Moreover, the projection component is clearly a linear operator that can be expressed by a linear layer before the LayerNorm, as we show in Appendix C. Nevertheless, the importance of the projection holds as we discuss in Section 3, and the benefit of using this operator explicitly in LayerNorm is shown in Section 4.1."
464,"The first limitation of this work is that it lacks a study on how the proposed regularization methods work at other scales; although we looked in Section 5.1 at two variants based on the compute budget (615M and 1.3B), we only tested our methods on the 1.3B variant with a fixed number of experts E=64. These methods can potentially show larger improvements on larger models (larger backbone or more experts) and marginal impacts on smaller models that do not suffer from severe over-fitting. The second limitation of this work is that our methods are only validated on a single multilingual MT benchmark. Some of these techniques proved to be generalizable to a much larger benchmarks (NLLB Team et al., 2022), and we leave testing these techniques on other tasks like language modeling to future work. Another limitation of this work, and most other works on multilingual machine translation, is the evaluation metrics and how to aggregate them. We report in this paper chrF++ scores and we average across three subsets of directions and three resource levels. This makes it difficult to
highlight the impact in some challenging directions on which our methods can lead to ±3chrF++ differential in quality. We did not report other metrics for the sake of brevity, and since we are not comparing to previously published results, chrF++ is a reliable metric for comparing and contrasting our methods."
465,"section 9
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
466,"limitations we calibrated opt models based on wikipedia data. future work should apply calibration procedure to a wider range of datasets, to check whether our results generalize to different domains. additionally, we limited our evaluation to entropy as a measure of uncertainty and did not explore other measures. finally, we aimed at validating the calibration status of commonly used lms. future work should thoroughly evaluate the impact of the calibration status on different facets of generation quality, as text generation is one of the main usecases of large lms."
467,"limitation of our method is the limited utilized knowledge. since our prompt tuning-based method tests on implicit discourse relation recognition (idrr) task, the elicited knowledge only comes from the dataset of this task and the model pre-training corpora. this constraint restricts the capability owing to the reporting bias (gordon and durme, 2013) in the pre-training models (plms). moreover, the relatively few training data of several second-level classes resulting from the highly skewed label distribution problem requires extensive knowledge to make the model understand data instances and the task. although we impose the prior human knowledge against the idrr task from the input template designing to the discourse connectives selection, the knowledge source still only comes from our prior knowledge and the elicited knowledge of plms. as a result, even our method obtains a valid score in all second-level classes except the cont.pragmatic cause displayed in table 6, some second-level senses, which are the same as previous studies, cannot receive a satisfactory performance (e.g., comp.concession and expa.list). the future work for this issue is to integrate more abundant knowledge and equip the model with more vital abilities. for example, grounding the arguments pair on the relevant nodes of the knowledge graph for each data instance (lin et al., 2019) or knowledge distillation from large language models to provides more contextual information and enhances the capability of the model on this task. limited predicted connectives another area for improvement is the prediction of extensive connectives. although our model includes the preselected connectives as our third layer of a designed hierarchy tree, we do not include the ground truth of connectives as our third layer. because including these extensive connectives to form many leaves will result in many paths (more than 100). this limitation may be addressed in future works by utilizing the pruning algorithms for reducing a lot of redundant nodes and leaves on each instance to enhance effectiveness and efficiency."
468,"limitations in this section, we discuss few limitations of the proposed method and point out future directions to improve the model. first, our method needs to decompose questions into a symbolic representation, but such representations are hard for humans to comprehend, and therefore this decomposition mechanism is hard to be trained with human annotation. a promising direction is to leverage pre-trained language models such as chatgpt 2 to automate this decomposition step, leveraging chatgpt’s internal knowledge of decomposing a complex question into sub-questions. second, the execution of the zero-shot nmns is conducted in a deterministic manner, leading to high risks of error propagation if the reasoning chain gets longer. in the future, we can consider a softer way of reasoning over the image with pre-trained models. 2https://openai.com/blog/chatgpt/"
469,"limitations while many studies show that the architectures of the deep learning models significantly influence the results, we perform experiments with several base architectures because of the constrained hardware. furthermore, there has not been a vietnamese benchmark summarization dataset, which is both sizable and of high quality. the existing summarization datasets are derived from online magazines, which usually contain misspelled words and grammatical errors. in addition, the reference summaries might not convey the main content of the corresponding articles. therefore, selecting and developing efficient summarization models for vietnamese still present numerous challenges."
470,"limitations of group dro which have been overlooked before, and can be viewed as a cornerstone for future study in the worst-group generalization. limitations although our unsupervised frameworkq-diversity shows great superiority, when it comes to limitations, we acknowledge that (i) our empirical validations on real-world datasets just follow current benchmarks that shed light on the group shifts caused by spurious correlations. although we conduct experiments on the scenarios with noisy labels and various ood datasets, practically, apart from superficial clues, a series of contributing factors that lead to group shifts are worth further exploration. (ii) a better theoretical understanding of how the interactive training mode can guide q-diversity works in better group identification should be established, and this points out the direction for our future work."
471,"limitations although the proposed method has shown great performance to alleviate the issues of biased learning and deficient interaction, which are common problems among electra-style pre-training models, we should realize that the proposed method still can be further improved. for example, the inherent flaw of rtd mentioned in section 3 could only be relieved rather than solved. more about mission design regarding with this issue is worth studying. besides, although the results show great performance, more efforts are required to explore the hidden impact of each course, which will help the application of the proposed model in the future."
472,"limitations by retrieving pseudo captions from summaries, one limitation is that the most relevant sentence for a specific image may not be in the summary. however, it has a trivial impact on the overall msmo performance. if this happens, most of the time, the image will not be the salient image to select, and its caption will provide no helpful information for the text summary. in this situation, selecting a pseudo caption from summary sentences will not hinder the overall performance, though it may not be the best for the specific image. besides, even though our task setting (including the dataset and all evaluation metrics we used) strictly follows three previous works (zhu et al., 2018, 2020; zhang et al., 2021b), another possible limitation is that only one msmo benchmark is used (no other dataset exists). we believe providing more diversified datasets and investigating more about the rationale under the task setting are critical to pushing forward the multimodal summarization community, although they are out of the scope of this work."
473,"limitations our experiments are based on (arguably) the most standard adapter architecture for adapter-based cross-lingual transfer and beyond, which also facilitates comparisons to prior work in this area. however, we again note that there are other emerging parameter-efficient modular methods, including different adapter architectures (he et al., 2022), that could be used with the same conceptual idea. we leave further and wider explorations along this direction for future work. our evaluation relies on the currently available standard multilingual benchmarks, and in particular those targeted towards low-resource languages. while the development of better models for underrepresented languages is possible mostly owing to such benchmarks, it is also inherently constrained by their quality and availability. even though our experiments have been conducted over 35 different target languages and across several different tasks, we mostly focus on generally consistent trends across multiple languages. delving deeper into finer-grained qualitative and linguistically oriented analyses over particular low-resource languages would require access to native speakers of those languages, and it is very challenging to conduct such analyses for many languages in our language sample. due to a large number of experiments across many tasks and languages, we report all our results based on a single run. averages over multiple runs conducted on a subset of languages and tasks confirm all the core findings; for simplicity, we eventually chose to report the results for all languages and tasks in the same setup. finally, training language adapters is typically computationally expensive; however, owing to the modular design of our framework with respect to language adapters, these are trained only once per language and reused across different evaluations."
474,"limitations we should point out that the gpusq-tlm compression scheme is highly related to the nvidia gpu’s features to support gpu-friendly 2:4 fine- grained structured sparsity with various data formats. so if the gpusq-tlm compressed models are deployed on the different gpu types without such support, the deployment efficiency may not be as high as expected. for example, the last-generation v100 (nvidia, 2017b) and t4 (nvidia, 2018) gpus have no support for structured sparsity, so the deployment efficiency is lower than a100 (nvidia, 2020) gpu. we should also point out nvidia agx orin chip also support gpu-friendly 2:4 fine-grained structured sparsity as a100 gpu and mainly support edge device use scenarios like autonomous driving. so, in theory, we can also deploy the transformer-based language models on the agx orin chip. however, the large language models need to consume large on-chip memory, so they usually cannot be held by a single agx orin chip. for a100 to represent the server use scenarios, we can use multiple a100 gpus for parallel execution, but for agx orin, we usually only have one chip for the deployment device. that’s why we do not test the gpusq-tlm compressed model on the agx orin chip."
475,"limitations this paper acknowledges three main limitations: 1) the constraints of a zero-shot setting, 2) an uncertain generalization capacity due to limited data in the target task, and 3) the longer inference time required by a large language model. given the absence of data for our task and the complexity of the target scenarios, collecting a large dataset for supervised or semi-supervised learning presents a significant challenge. as the first approach tackling this task, our framework performs the task in a zero-shot manner, but is applicable to fine-tuning if a substantial dataset becomes available. consequently, we expect that future research could further train the proposed framework using supervised learning or fine-tuning, thereby enhancing the alignment of inferred implicit intents and recommended bots with training data. this would expand our method to various learning settings and validate its generalization capacity. conversely, the gpt-j model used for recommending task-oriented bots is considerably large given academic resources, thereby slowing down inference speed. to mitigate this, our future work intends to develop a lightweight student model that accelerates the prompt inference process. such a smaller language model could not only expedite the inference process to recommend task-oriented bots but also be conveniently fine-tuned using collected data. despite these limitations, this work can be considered as the pioneering attempt to leverage commonsense knowledge to link task-oriented intents. the significant potential of this research direction is evidenced within this paper."
476,"limitations the main limitation of this work is the usage of explicit knowledge in the knowledge graph. although using knowledge graphs is a common advantage of most current target-oriented dialogue studies, and explicit relations between entities help to effective and reliable reasoning for the recommendation, there is still a large amount of implicit knowledge in unstructured resources that cannot be extracted as explicit triplets, e.g., the multidimensional similarity between entities, but can be a further extra supplement to dialog context. in this work, we involve implicit knowledge by generating a path as a natural language sentence, but the knowledge graph is still necessary. in future work, we will explore only using unstructured knowledge for global planning."
477,"limitations our approach has the following limitations: 1. it only considers swaps of pairs of functions at the top-level scope, which is a small set of all the quasi-invariances of the python programming language. 2. it only considers code generation in top-level functions, hence it does not evaluate class methods. 3. it relies on a syntactic substitution to generate ""correct"" gold truth outputs, which may fail if the swapped functions are called by a string expression through eval or or queried by their string names using the reflection facilities. 4. in our experiments, we can evaluate only a small number of model sizes per family, since these are the only ones available, therefore the p-values of the correlation with the loss analysis are high. 5. the independent reproducibility of the experiments on closed-source models is predicated on the continued availability of a publiclyaccessible api. at the time of writing, our experiments on the openai ""codex"" models are no longer reproducible without support from openai. items 1 and 2 can be in principle treated by considering more complex code transformations, which we leave for future work. item 3 is harder to tackle in the general case because of undecidability issues. item 4 could be addressed by reproducing our experiments on a model family that encompasses more model sizes, should it become available for public experimentation. item 5 is an unavoidable consequence of using closed-source models."
478,"limitations although our method scn achieves state-of-theart performance in the cll-id task, there is still a performance gap between scn and the upper bound. this result is inconsistent with human behaviors because humans usually do not forget old skills when learning new skills. therefore, in future work, we hope to introduce findings from the brain science domain into the model design to overcome the problem of catastrophic forgetting."
479,"limitations there are several limitations of this work. first, while we have observed positive correlations between ft-scores and rtt-scores and conducted experiments to predict ft-scores using rtt-scores, their relations could be complicated and non-linear. we encourage future research to investigate various rtt-score features and more complex machine learning models for better prediction models. second, we have examined the prediction models on low-resource languages in flores-101, but have not tested those very lowresource languages out of these 101 languages. we suggest auditing ft-score prediction models on a small validation dataset for any new low-resource languages in future applications. third, our assessment has been systematic and thorough, utilizing datasets such as flores-101, wmt2020-news, and wmt2020-bio. despite this, the nature of our study is constrained by the timeline of the data utilized. the wmt data we used is from 2020, opening up the possibility that more recently proposed metrics could potentially outperform the ones proposed in this work."
480,"limitations though our proposed method exhibits superior performance, we also recognize its limitations and discuss potential solutions. our proposed method for goal-directed dialogue generation suffers from error propagation since the three stages perform in a pipeline manner. after analyzing those generated utterances with low human evaluation scores, we find that the performance of dialogue generation is prone to drop when our color model fails to plan an appropriate dialogue path. we intend to alleviate this issue by introducing some techniques in the cascaded generation, such as noisy channel models (shannon, 1948; liu et al., 2021a). in addition, other issues, such as how to make existing goal-directed dialogue systems more engaging and personalized, are worth further exploring. ethical considerations goal-directed dialogue systems can be used for creating non-obtrusive recommendations for specific products and services, introducing interesting new topics and educating users about those topics, and so forth. developing such systems requires careful consideration since it has a broad impact on applications. the intention of our work is not to force the system to reach the designated target nor force users to accept recommendations. instead, we aim to build better assistive technologies to improve the proactiveness of dialogue systems. furthermore, our experimental datasets are publicly available. they have been filtered for sensitive and private information during dataset construction. we hope to raise awareness of the potential for misuse of such systems with toxic intentions. for example, such systems may be used to pose as humans and actively manipulate users’ perceptions on specific issues or political inclinations. to mitigate these risks, we emphasize the importance of improving transparency through regulations. it is essential to inform users that they are conversing with a bot instead of a human, and regulations on target designation are crucial when deploying these systems in specific domains. it is necessary to ensure that setting a target does not violate factual accuracy, user privacy rules, or human laws."
481,"limitations in this method, e.g., slow convergence and training instabilities. we hope that future studies in this field can alleviate the aforementioned problems and thus promote the application of prompt tuning. limitations this section disccuses the limitations of prompt tuning for the unified multimodal pretrained mod- els, and point out some directions for future work. one limitation of prompt tuning in this setup is the sensitivity to hyperparameter tuning. it is difficult to search for a suitable hyperparamter setup. the hyperparameter tuning experience in finetuning is not suitable for prompt tuning. fortunately, we find that prompt tuning for generative multimodal pretrained models is not as sensitive to hyperparameters as prompt tuning for pretrained language models. we provide details of hyperparameter setups in appendix a.1. another limitation of prompt tuning in this setup is slow convergence. though prompt tuning has noticeable advantages in training efficiency, it costs at least 40 epochs for prompt tuning to achieve the nearly best performance on some datasets (e.g., refcoco). a larger number of training epochs may incur more computation costs though prompt tuning has an advantage in training efficiency compared with finetuning. we demonstrate more details in appendix a.2. this indicates that finding a better solution for fast and stable convergence is also important besides reaching comparable or even improved performance over the conventional finetuning. despite the aforementioned limitations, prompt tuning demonstrates significantly better robustness against adversarial attack. in the future, we should pay more attention to this merit and find ways to leverage it."
482,"limitations our model simultaneously encodes structural and temporal contexts of the tkg substructure, and uses heuristic strategies to select a portion of query-relevant facts as input texts for plms. we can achieve stunning results with these selected facts. however, this work only considers the queryrelevant one-hop neighbor facts to achieve a good performance improvement, but ignores the benefits of multi-hop neighbor facts. we leave it for future work to verify the effectiveness of multi-hop paths."
483,"limitations of chatgpt in standard academic datasets. to our best knowledge, this is the first work that conducts an extensive evaluation of chatgpt in benchmark nlp datasets. we observe that even though chatgpt obtains impressive zero-shot performance across various tasks, it is still far from reaching human-level performance in many tasks. moreover, potential biases and ethical concerns, as well as misinformation generation risks of chatgpt are discussed. in addition, a unique capability of chatgpt has been studied. though there may have numerous other capabilities of chatgpt that go unnoticed in this paper, future work should nonetheless investigate the capability of chatgpt on more tasks. we will make all our prompts and chatgpt-generated responses publicly available. 5https://beta.openai.com/docs/models/overview"
484,"limitations, conducting the human evaluation by the authors does not lead to any unwanted biases or ethical concerns. only the publicly available academic datasets are used that did not require any licensing. thus, no personally identifiable information has been used while evaluating chatgpt responses."
485,"limitations in this work, we have focused on the efficiency concerns of task-agnostic domain adaptation approaches leveraging pre-trained transformer-based language models. the experiments are conducted on four tasks across 14 domains in both high- and low-resource scenarios. we only consider the methods utilizing pre-collected in-domain unlabeled text corpora for domain-adaptive pre-training. it is worth pointing out that the selected domains are strongly correlated to the selected tasks, which does not reflect the wide spectrum of domain interests. besides, the datasets are covered only in english to magnify the domain adaptation controlling factors and use cases, while multilinguality would be the next step to explore. we experimented on encoder-only ptlm based on the downstream classification tasks, where the encoder-decoder ptlm would be applicable to different tasks (e.g., natural language generation, summarization, etc.) requiring more computational resources. we hope that future research builds on top of our findings and extends the research toward more domains, more languages, more tasks, and specifically with the meta-tokenizers for efficiency concerns of domain adaptation approaches."
486,"limitations our proposed monet is a classification-based method requiring the pre-defined ontology containing all slot-value pairs. moreover, during prediction, for each slot, its distance with all possible values is calculated, i.e., the prediction has to process 30 times, which is the number of slots in the multiwoz dataset. compared with the generation methods that only process once and do not need ontology, our method is short in training efficiency and scalability. however, most task-oriented dialogue datasets contain their knowledge base containing slot value information, so it’s acceptable to construct the ontology for random sampling. besides, the results in section 5.7 demonstrate that our method can be implemented into generation-based backbone models."
487,"limitations persona extractor first, we need to clarify that our definition of persona is not exactly psychological, the role an individual plays in life (jung, 2013). as a result, like previous studies (e.g., persona-chat (zhang et al., 2018), pec (zhong et al., 2020)), the format of persona is flexible and variable. as stated in §4.1, there are still some issues with the model we use to infer persona information. for example, we sometimes get information that contradicts the facts. and also, there is occasionally unrelated content, as with commonsense reasoning (tu et al., 2022). furthermore, we cannot guarantee that we can infer all of the persona information that appears in the conversation because much of it is frequently obscure. and when extracting persona information, we only use what the user said previously and remove what the bot said, which results in the loss of some conversation information. the reason for this is that we have discovered that if we use the entire conversation, the model frequently has difficulty distinguishing which persona information belongs to the user and which belongs to the other party. in addition, since the code of xu et al. (2022) is not yet available, we have not compared other methods of extracting persona dynamically from the conversation. strategy-based decoding during the decoding phase, we only coarse-grained the α of each strategy because we discovered that only coarse-grained tuning produced good results, and future work may be able to further explore the deeper relationship between different strategies and persona. ethical considerations in this work, we leveraged two publicly available datasets. first, we used the persona-chat dataset, which is collected by assigning a set of fixed predefined persona sentences to workers. therefore, by participating in this dataset, workers were required not to disclose any personal information (zhang et al., 2018), which prevents issues regarding the leakage of their privacy. similarly, during the collection of the esconv dataset, participants were asked to create imaginary situations and play the role of a support seeker who is in that situation. in addition, they were instructed not to provide personal information during their conversations with the trained supporters (liu et al., 2021). regarding the persona extractor, this module is trained to infer and extract persona information solely from what the user has mentioned in the conversation rather than making assumptions about the user’s background and character, further highlighting the importance of user privacy in our research. regarding our experiments, we ensured that all workers agreed to participate in the annotation tasks. moreover, as the workers were recruited from the us, we ensured that they were paid above the minimum wage in this country for successfully completing our tasks. we acknowledge that using trained dialogue models to provide support is a sensitive subject and research on this topic should be conducted with sufficient precautions and supervision. we also acknowledge that in their current stage, such models cannot replace human supporters for this task (sabour et al., 2022a). thus, they should not be employed to replace professional counselors and intervention and interact with users that suffer from mental distress, such as depression or suicidal thoughts."
488,"limitations there are two potential risks with our method. first, iss trades generality for efficiency by learning only task-specific representations. consequently, it may not be suitable for other tasks. secondly, our method is hardly practical for few-shot or zeroshot learning, as few or no task data are available as anchor points. these potential risks are left to future work."
489,"limitations of the prior supsup method. through the avoidance of conflicting weight updates, exssnet not only improves performance but also eliminates forgetting, striking a delicate balance. moreover, the inclusion of the knowledge transfer (kkt) module propels the learning process, utilizing previously acquired knowledge to expedite and enhance the learning of new tasks. the efficacy of exssnet is substantiated by its superior performance in both nlp and vision domains, its particular proficiency for sparse masks, and its scalability up to a hundred tasks."
490,"limitations this section discusses the limitations of this work for more insights on the research in this track. though ofa-ocr achieves high accuracy on multiple text recognition datasets, its costs are larger than the non-transformer baselines. in practice, it is difficult to deploy such large models. thus in our future work, we will discover how to distill or compress ofa-ocr to a light-weight model with high efficiency."
491,"limitations the cloning model used, yourtts, is trained on the vctk dataset which consists of high-quality speech signal. it is therefore unclear whether the same accuracy would be obtained with lower quality signal which may contain some background noise. (however, it should again be noted that even if all substituted instances are identifiable in the output, the system is equivalent to a masking model.) the selection of a person ne replacement does not currently account for continuity: if the same person entity is referred to later, it may be substituted with a different entity to the previous occasion. in addition, the back-off strategy ignores aspects such as gender. to show the approach feasible, very little optimization was performed. further training and parameter optimization is likely to lead to improved performance for both asr and ner models. the approach is currently only implemented for person nes but it could be extended very simply to other types of nes. however, the degree to which other entity types require obfuscation in speech is not clear to us as mentions of organizations may well not be identifying at all."
492,"limitations unlike the traditional multimodal contrastive loss focusing more on building the direct link between paired modalities, our proposed unis-mmc aims to leverage inter-modality relationships and potential effectiveness among modalities to create more trustworthy and complementary multimodal representations. it means that unis-mmc is not applied to all multimodal problems. it can achieve competitive performance in tasks that rely on the quantity of the joint representation, such as the multimodal classification task. it is not suitable for tasks that rely purely on correspondence between modalities, such as the cross-modal retrieval task."
493,"limitations as pointed out by shi et al. (2020), applying ibp technologies to large-scale pre-trained bert models is challenging because of the calculation of bound propagation on the attention layer is relatively loose. since bert is currently one of the most popular architectures in nlp, there is a limitation that the proposed method combined with ibp training could not generalize to bert architectures. however, it is worth noting that the proposed method based on textcnn architectures achieves better certified robustness than the advanced baselines, safer and ciss based on bert. besides, this paper focuses on enhancing the model’s robustness to word substitutions, but not investigates the robustness to character-level or sentence-level perturbations."
494,"limitations while we achieved preliminary results and created a preliminary projection of the factbank source and target corpus, we do not capture the full source and target nesting in our machine learning experiments. we repeat the example from section 4: mary said that john said that jane was coming to dinner, but bob said that she was not. the embedded sources for the coming event are (author → mary → john), which translates to ""according to the author according to mary according to john, did the coming event happen?"" in our experiments and machine learning architecture, we focus on the last nested source, or john in this example. in future work, we aim to link together all sources and their embedded nesting structures. we note that all experiments in this paper were performed using the flan-t5-base model. in future work on this task, we will explore different generative models such as gpt-3 or bart, which may yield stronger performing systems or more interesting results. we are especially curious about framing this task using gpt-3, especially performing tasks on few-shot or in-context learning. finally, we note that these experiments do not account for potential biases prevalent in fine-tuning large language models. we hypothesize that for some sources in text (i.e. power figures, authorities, or specific names), there may be biases towards certain labels. we will investigate these biases in future work, as an event factuality prediction system with inherent bias can have real world implications."
495,"limitations this work presents some limitations. firstly, our focus is confined to evaluating the specificity of predictions made by pre-trained language models for entity relations. as noted in section 7, specificity can potentially be tested in a broader range of scenarios. despite this restriction, we consider this work as an initial attempt to highlight the concept of language model specificity. we believe it will stimulate further research into this crucial, yet under-explored, area. a second limitation is the scale of the models evaluated in this work. given the swift evolution of large language models concurrent with the drafting of this paper, the models we examined are comparatively small. as pointed out in the work of zheng et al. (2023), large language models may fail to answer a problem at the appropriate level of specificity. we thus encourage future investigations to delve into the specificity of these rapidly evolving, larger language models."
496,"limitations although we demonstrated that the proposed metric peculiarity is useful for selecting candidates for few-shot cross-lingual transfer, our current work has the following limitations. lack of evaluations to argue the usefulness of peculiarity. we demonstrated that peculiarity selects candidates to efficiently enhance few-shot cross-lingual performance in several tasks and languages. in addition, peculiarity is robust for hyperparameter k. however, further verification is required to evaluate the usefulness of peculiarity. in this study, we only used xlm-r as the mmlm in the experiments, because previous works (lauscher et al., 2020; kumar et al., 2022) have demonstrated that mbert and xlm-r show the same trend and xlm-r achieves better zeroshot and few-shot cross-lingual performance. however, it is not obvious that peculiarity will work well in mbert. in addition, recently, lin et al. (2022) proposed xglm, a pre-trained multilingual causal language model, that demonstrates strong multilingual capabilities. we would like to experiment using these pre-trained multilingual models to show the usefulness of peculiarity regardless of models. we fine-tuned the mmlm using a standard training objective, predicting true labels or tags for inputs. on the contrary, zhao and schütze (2021) revealed that fine-tuning in a prompting format encourages better zero-shot and few-shot crosslingual transfer than the standard fine-tuning. it is worthwhile to examine few-shot cross-lingual transfer performance when fine-tuning the mmlm with high peculiarity examples in a prompting format because it may be possible to achieve higher accuracy in the target languages with a smaller amount of examples. we experimented using english as the source language. however, if possible, it is better to use a language that is linguistically close to the target language as the source language (pires et al., 2019; lauscher et al., 2020; chai et al., 2022). in our experiments, we did not show that peculiarity works well regardless of source languages. therefore, verifying this aspect is also a remaining challenge. definition of annotation cost. in this study, we defined annotation cost in terms of the number of candidates following previous studies (pires et al., 2019; lauscher et al., 2020; chai et al., 2022). however, a small number of candidates does not necessarily mean less work for annotators. if a candidate (sentence) length is long or hard, it is considered to take longer to understand. on the other hand, if the candidate length is short or easy, annotation time per candidate will be shorter, and the annotators can annotate more candidates in the same time. therefore, we should evaluate candidate selection methods based on total time required for annotation. in addition, aligning the cross-lingual representations between source and target languages using bilingual data is one approach to enhance accuracy for the target languages (lample and conneau, 2019; cao et al., 2020; chi et al., 2021; dou and neubig, 2021; yang et al., 2021). to align the representations, we should create bilingual data through a human or automatic translator. verification whether labeling or translating is less labor intensive and further boosting performance is one of the future goals. developing a better peculiarity-based candidate selection method. in this study, we used the bos hidden states to measure peculiarity; in other words, it measures example-level peculiarity. in classification tasks, using example-level peculiarity to select candidates is intuitive because we predict labels based on the bos hidden states. on the other hand, in the sequence-tagging tasks, we predict token tags based on hidden states of each token. in addition, we consider that it is necessary to fine-tune ms with peculiar tokens, tokens that are not covered by the source language, to ensure that the model predicts tags of these tokens correctly. therefore, we will attempt to select candidates that contain peculiar tokens by using token-level peculiarity and conduct few-shot cross-lingual transfer in the sequence-tagging tasks. we observed that peculiarity selects more redundant candidates compared to the km-based methods and argued that this aspect is the reason that peculiarity does not work in the “poor” group. we consider the possibility of other reasons for this behavior. several studies (swayamdipta et al., 2020; sorscher et al., 2022; hacohen et al., 2022) have suggested that if only a small amount of examples can be used for training, it is important to use not only hard (atypical) examples but also some easy (typical) examples for training in order to improve model performance. in terms of few-shot crosslingual transfer using peculiarity, we should finetune ms with the both highest and lowest peculiarity examples. in addition, using typical examples selected by km instead of the lowest peculiarity examples is one of the approaches. for future work, we would like to verify the effectiveness of these methods for few-shot cross-lingual transfer."
497,"limitation, we reformulate re into multiple-choice question answering (qa) with the purpose of leveraging a task that is widely cov- ered in instruction-tuning datasets like qa, instead of re, which is barely present in these datasets. comprehensive experiments demonstrate that our qa4re framework unlocks the power of llms as zero-shot relation extractors, especially for two recent llms (text-davinci-003 and flan-t5 xxl). we also conduct thorough experiments to explore the robustness and few-shot effectiveness of our method as well as study in what llm training scenarios it is most effective. in future work, we hope to explore additional underrepresented tasks in instruction-tuning that might be challenging for llms and could be successfully aligned with more widely adopted instruction-tuning tasks like qa. additionally, we plan to continue exploring this line of work by leveraging our qa4re framework for other llms such as the opt-series (zhang et al., 2022; iyer et al., 2022) and palm (chowdhery et al., 2022), which are not included in this work due to the limited computational resources and/or access."
498,"limitations tada makes use of the pseudo-dialectal translation systems of prior work ziems et al. (2022, 2023). we rely on them as they are validated by dialect speakers and have been shown to be predictive of performance on gold dialect data. however, they were designed as stress tests of robustness which isolates morphology and syntax. we are therefore unsure how tada performs when it faces the topical and register shifts which often are associated with naturally occurring dialects. these limitations are similar to localization issues in translated benchmarks (moradshahi et al., 2020). in this work, we evaluate tada on only encoder-only llms. increasingly, both encoderdecoder and decoder-only models are seeing widescale use due to their flexibility (wang et al., 2022). evaluating tada and developing alternate tailored task-agnostic methodologies on these alternate llm architectures is left to future work."
499,"limitations that may suffice for current text-to-sql benchmarks with small-scale schemas. however, for large-scale schemas, modifications to the plm encoding method are necessary. if the plm were to encode the question and schema separately, the emsl would still be required."
500,"limitations, the unk entry for ratsqlb is empty. random initialization means that model results after each training may vary slightly, so we only focus on the more salient features. although the results of ratsql and ratsqlo are similar, ratsqlo consistently outperforms ratsql in three error types when emsl is removed; this supports the view we discuss in section 2.2. more importantly, the single-word performance of ratsqlo without emsl is close to that of ratsql and ratsqlo. as discussed in appendix f.1, the representation ability on multiword of glove is worse than that of bert. the results support this view where the performance of ratsqlo and ratsql on multi-word is worse than that on single-word. when replacing the glove with bert, due to the improvement of its multi-word representation ability, the performance of ratsqlb with and without emsl are close in single and multiple words. from the right side of table 8, it can also be found that the bert brings around 5% absolute improvement on multi-word, while that on single-word is only 2%. acl 2023 responsible nlp checklist"
501,"limitations of existing book summarization resources, such as booksum. indeed, previous datasets for full-text book summarization are, i) limited in size, and, ii) monolingual, i.e., usually covering english only. in addition, we leveraged echoes to bring to light the unsatisfying capabilities of current approaches to generalize to book summarization. finally, to mitigate this issue, we proposed a new extractivethen-abstractive baseline for book summarization, which outperforms its purely-abstractive counterpart on echo-wiki and echo-xsum, achieving results on the standard booksum test set that are comparable with the current state of the art while using a number of parameters that is only 0.1% compared to the best-performing method. we believe that echoes will foster future work on long-document summarization, especially in the multilingual and cross-lingual setting. limitations despite the multilinguality of our resource, there is still a strong bias towards the english language, as the majority of books are in english and many translations are from english. this may result in the values of english literature being reflected, and these may differ from those of other cultures; summarizing literature from different cultures and regions may not be fully accurate, as every region has had its own historical development. language models used in the experiments can inherit biases from the training data and the tools, such as the ones used for preprocessing, and have limitations that have not been fully evaluated and could impact the results of this study. this study includes the use of web data, which – while marked as public domain – may be subject to copyright laws. the data used in this study was collected for research purposes and was not intended for any other use. additionally, it is worth noting that the majority of books used in our resource are copyright-free, and therefore, old. while this allowed us to include a large number of texts in our dataset, it also means that our resource may not fully capture contemporary literature and may not be representative of current linguistic trends and cultural values."
502,"limitations we summarize the limitations of this work as follows: (1) we conduct experiments on 7 language understanding tasks across 4 types (i.e., sentiment analysis, topic classification, natural language inference and paraphrasing). however, the effectiveness of gdfo on tasks such as sequence labeling and generation tasks has yet to be fully examined. (2) our proposed method uses a student model and a prompt generator, thereby resulting in a higher computational resource requirement in comparison to gradient-free methods. therefore, it may not be suitable for implementation on certain edge devices, but it is more appropriate for personal or enterprise users who have access to a certain degree of computational resources and have stringent requirements for the model performance. (3) we only focus on the few-shot setting in this paper. it is possible to extend our work to other scenarios such as semi-supervised learning and we will further explore it in the future research."
503,"limitations there are two main limitations of our work: (1) our approach requires a set of previously aligned predicate pairs as training data to achieve predicate alignment between different kgs, which limits the generalization ability of our method. in our experiments, since we manually aligned a set of equivalent predicates with arguments of types person and location between the english and chinese egs, we can only perform predicate alignment and entailment graph enhancement between the <person,location> subgraphs of two egs. we will explore the semi-supervised or unsupervised predicate alignment method between different egs in our future work. (2) our current enhancement strategy introduced in section 4.2 is straightforward. it might not be robust enough when dealing with entailment graphs of poor quality. we will explore more adaptive eg enhancement methods in the future."
504,"limitations the main concern regarding our model is the computational complexity. higher-order mfvi has a complexity of o(n3), which admits fully parallel computation and thus is fast on gpus. the complexity of structured inference of treecrf is also o(n3). however, due to the dynamic programming computation restriction, only o(n2) out of o(n3) can be computed in parallel using parallel parsing techniques (rush, 2020), slowing down the running speed. besides, differentiating through the treecrf marginals needs many gpu memories (kim et al., 2017), as automatic differentiation saves all intermediate dynamic programming items for back-propagation, which cause plenty of waste of gpu memories. in this work, since the memory problem is not too severe, we use automatic differentiation for simplicity. one solution is to manually implement the outside algorithm to mitigate the memory problem (kim et al., 2017)."
505,"limitations for this work, we have several limitations: first, as described in section 6.6, we found that the choice of different templates and the order of generating content will both lead to performance variation. it is worthwhile to conduct a detailed investigation on this interesting problem, however, due to the limit of pages, we only experimented with limited alternative templates. second, our proposed aqe task shares some similarities with some tasks in other domains, which means that it is possible to adapt our proposed framework to other tasks, such as relation extraction and sentiment analysis. we will leave this for future research and demonstrate its effectiveness in other domains. last, subject to both the economic and time cost of dataset annotation, we only expand one existing dataset for our proposed aqe task. we will explore more possibilities for dataset construction for future work."
506,"limitations this work is constrained by the number of grounding phenomena analyzed, which is limited by the dataset domain and their straightforward automatic computation. we only focused on lexical alignment, the use of ellipsis (fragments) and pronouns, disregarding other phenomena such as repairs (e.g. asking for confirmation or clarification) (purver et al., 2003), among others. with respect to the linguistic phenomena, we simplified the calculation of the lexical alignment by regarding only the last two turns of a conversation (the user question and the system response). in this manner, we omitted the dynamic convergence over several turns (mills and healey, 2008). it should be noted though that this was decided based on manual observation of examples, the majority of which exhibited lexical alignment in the last two turns only. this could be a limitation of the ocqa domain, and/or a bias of the topiocqa dataset. another limitation is that the form of crowdsourcing experiments we performed are mostly diagnostic of certain conditions on a given dataset, and does not reflect more organic real-use cases. an ideal setup would be to collect whole dialogues in the form of an extrinsic evaluation, which would be more costly to perform."
507,"limitations in this paper, we provide an overview of the current state of knowledge on reasoning in large language models. reasoning is a broad concept that encompasses various forms, making it impractical to summarize all related work in a single paper. therefore, we focus on deductive reasoning, as it is the most commonly studied in the literature. other forms of reasoning such as inductive reasoning (yang et al., 2022; misra et al., 2022, inter alia) and abductive reasoning (wiegreffe et al., 2022; lampinen et al., 2022; jung et al., 2022, inter alia) may not be discussed in depth. additionally, given the rapid evolution and significance of reasoning within large language models, it is crucial to note that new contributions may have emerged in the field concurrent with the writing of this paper. an additional resource to consider is a parallel survey by qiao et al. (2022), which emphasizes reasoning via language model prompting. our coverage may not extend to papers released during or after 2023 such as evaluation on chatgpt (bang et al., 2023; zheng et al., 2023). as such, we recommend readers to check the papers that cite this survey for a more comprehensive and updated understanding of this field."
508,"limitations we presented in this paper a real-world annotated example of seeking information in scientific publications. even if the number of instances presented here is of the same order of magnitude as what is present in benchmarks, we presented only one query and its correspondent relevance judgements, provided by one expert, due to resource constraints. as we noted above, building a corpus dedicated to the exploration of a single information need does however correspond to a real industrial use case. further, we favored the use of sentencetransformers format for all neural models for the sake of efficiency. we did not dive into providing the best-known performing models and did not consider optimizing them in our case, as overfitting to our data might induce errors in"
509,"limitations we present some limitations of our approach, which can be investigated in the future: (1) currently, our approaches need to manually choose image for each text label, which may cause the model to be sensitive to the images selected. though the ensemble method can alleviate this problem to some extent, how to automatically map the text label into the corresponding image is an interesting research question to investigate. (2) since clip was pre-trained on noisy web-crawled data on the internet, our approaches are limited by pre-training data distribution of clip. therefore, a potential future direction is to further pre-train clip on more general downstream task datasets."
510,"limitations the experiments were performed only on japanese and english bilingual dialogue collected from a limited number of native speakers. although the methods proposed in this work can work on any language pair, drawing"
511,"limitations this paper focuses on the style transfer of easy language for german. due to their word inflections and high average word length, languages like german are harder to learn for language models (mielke et al., 2019). therefore, the proposed approach may work even better on easier-to-model languages, but we did not test any other language. in addition, the style transfer of simplified language uses the same vocabulary as the original language and only reduces its diversity. our approach has yet to be evaluated on other styles, for example, ones that introduce new words. when evaluating the influence of fine-tunung on the grammaticality of the model outputs, we found that even the original models were not perfect and produced grammatical errors. one possible reason is relying on gpt2-based models that are relatively small and, thus, perform worse than state-of-theart language models like palm (chowdhery et al., 2022). in addition, the german base models are often already fine-tuned versions of english models, and thus, may already suffer from catastrophic forgetting due to fine-tuning."
512,"limitations we outline two limitations of our work from user behavior sampling and knowledge population aspects. due to huge-volume user behavior data produced every day in the e-commerce platform, it is crucial to efficiently sample significant behaviors that can indicate strong intentions and avoid random co-purchasing or clicking etc. though in this work we adopt the criteria of selecting nodes whose degree are more than five in the co-buy graph, it is still coarse-grained and more advanced methods remain to be explored in order to sample representative co-buy pairs for intention generation. some potential solutions are to aggregate frequent co-buy category pairs and then sample product pairs within selected category pairs. moreover, our proposed framework can be generalized to other types of abundant user behaviors such as search-click and search-buy, which requires to design corresponding prompts. we leave these designs to future work. for open text generation from llms, it becomes common practices to label high-quality data for finetuning to improve the quality and controllability of generation such as lamda (thoppilan et al., 2022), instructgpt (ouyang et al., 2022), and chatgpt6. however, computation cost is the major bottleneck to use annotated data as human feedback for language model finetuning with billions of parameters, like opt-30b in our work. hence we adopt a trade-off strategy to populate human judgements by training effective classifiers and conducting inferences over all the generation candidates. with impressive generation performance of chatgpt, we expect efficient methods to directly optimize llms with human feedback in more scalable way like reinforcement learning (rlhf), and enable llms to generate more typical intention knowledge with less annotation efforts."
513,"limitations: the duration task focused only on data with a positive happiness label, but it would be interesting to see whether the framework generalizes to a complete dataset and more sophisticated problem definitions. the need for annotations limits the generalizability of our approach, but the bert-psyam framework is effective even with labels generated through semi-supervised methods and other metadata. ethical considerations: the models are intended for aggregate- and group-level inferences, and not individual or message-level inferences. despite our cross-domain validation efforts, we caution that relying exclusively on ai-inferred relationships between emotion, self-efficacy, and selfdetermination may lead to inaccurate measure- ments. finally, models trained in a specific sociocultural setting may nevertheless violate the social conventions in specific settings, such as in the workplace, and cultural conventions of individualism and collectivism in social life (diener et al., 2009). acknowledgments: we thank niyati chhaya, chaitanya aggarwal, and gerard yeo for feedback on early versions of this work. this work was supported by an nus ctic grant and a nanyang presidential postdoctoral fellowship."
514,"limitation this work has two main limitations: (1) the performance of the model largely depends on the performance of the annotation model. if the annotation model is too simple, it may cause the performance of the dst model to decline. on the contrary, it will increase the complexity of the overall model and prolong the reasoning time. (2) even for the labeling model with good performance, the tagging values may also interfere with the dst model. for details, please refer to the analysis experiment."
515,"limitations although htf has achieved promising performance on removing spurious correlations, we identify the following limitations. firstly, although htf encounters the smallest performance decrease among compared methods under multiple semantic interventions, the interventions still cause a performance drop. therefore, more approaches can be explored to further improve the generalization ability of htf, such as increasing the scale of the backbone model or applying more informative hypothetical examples. secondly, the experiments are only conducted in the financial domain due to limited datasets with sufficient annotation of hypothetical examples. since hypothetical examples are more economic to obtain than counterfactual examples, we believe that more datasets with hypothetical examples will be proposed in the future and thus htf can be applied in more domains. thirdly, we are unable to compare the effectiveness of hypothetical and counterfactual examples because tat-qa does not contain both types, and constructing all counterfactual examples is impractical for us due to cost constraints. note that we do not conclude any effectiveness relationship between hypothetical and counterfactual examples in the paper."
516,"limitations in the evaluation datasets, as well as (for the scholarbert models) small model sizes relative to the large pretraining corpus. we make the scholarbert models available on huggingface (https://huggingface.co/ globuslabs). while we cannot share the full public resource dataset, we have provided a sample of open-access articles from the dataset (https://github.com/tuhz/ publicresourcedatasetsample) in both the original pdf and extracted txt formats to illustrate the quality of the pdf-to-text preprocessing. limitations our 12 labeled test datasets are from just five domains (plus two multi-disciplinary); five of the 12 are from biomedicine. this imbalance, which reflects the varied adoption of nlp methods across domains, means that our evaluation dataset is necessarily limited. our largest model, with 770m parameters, may not be sufficiently large to demonstrate scaling laws for language models. we also aim to extend our experiments to tasks other than ner, relation extraction, and text classification, such as question-answering and textual entailment in scientific domains."
517,"limitations our work is limited in several ways. we use human judgements on our case study data to demonstrate a preference of vrm-e versus spsm. however, additional case studies in other domains such as education, healthcare, legal studies etc. are necessary in order to gather empirical evidence that preference for vrm-e generalizes. threats to validity. there are several threats to interpreting our case study estimates as causal. like any causal study with observational data, our case study relies on untestable causal identification assumptions such as no unmeasured confounding. other unmeasured confounding likely does exist. for example, our document embeddings do not necessarily measure the “quality” of the manuscripts or the “novelty” of the ideas, both of which could affect reviewers’ scores. regarding estimation, by allowing for matching with replacement, appendix c.1.1 shows that several manuscripts are reused with high frequency. this will introduce bias within our model as noted in stuart (2010). additionally, our choice of b satisfies overlap but at the expense of very similar semantic matches between manuscripts. this could explain why there was only a moderate amount of agreement between the human judges as many matches are less semantically similar than we would prefer."
518,"limitation our method has a few limitations which may inspire future work. first, the prompt templates are manually designed, although we’ve introduced the rules and intuitions used in our implementation. second, the proposed method may have low scalability to long text. because we add the prompt at the end of the context, the prompt would be truncated if the context itself exceeds the maximum acceptable token length of the model."
519,"limitations the proposed synthesis framework has been targeted at text-to-sql task, which may not generalize to other tasks that require large amount of synthetic data without major modification. for instance, other popular tasks involving converting natural language questions to some sort of logic forms are in natural very similar to text-to-sql, yet all techniques relying on the ""key"" property in the database might no longer be applicable. on the other hand, the template based synthesis method currently relies on templates extracted from the real data. by incorporating some carefully designed grammar (e.g. pcfg), we may be able to further enrich the template set."
520,"limitations although our work shows that our cpll model can learn from crowd-annotated ner data well, there are at least two limitations. first, we set the hyperparameter α manually. it would be better if we could design a strategy to learn a alpha adaptive value for each sample atomically. second, though we mainly experiment on ner tasks, our model can be applied to all sequence labeling tasks, such as part-of-speech tagging (pos), chinese word segmentation, and so on. we would like to explore it in further work."
521,"limitations the schema information and grammar design are domain specific. we have tested our approach mainly on our own dataset, though we believe the similar approach can be applied to other tasks, as long as the meaning representation involves functions and arguments. also, we have not explored all approaches to obtain the highest possible accuracy on this dataset, because our main goal is to show the effectiveness of the proposed approach, which we believe is clearly demonstrated by the current result. at this stage, the difference between in-distribution and out-of-distribution accuracies remain very large, and there is a large room for further improvements. we hope by releasing this dataset we can help promote research in related areas."
522,"limitations the method introduced in this paper applies to a specific type of sentiment analysis task, where the item to be analysed is a review, the author of the review and the product/service being reviewed are known and uniquely identified, and the author (user) and product information is available for all reviews in the training set. while our approach is expected to perform well on other languages beyond english, the experimental results do not necessarily support that since our evaluation is only carried out on english data."
523,"limitations although we have shown the potential of performing automatic emotion cause extraction (ece) on social media without human annotation, there are still several limitations in our work. firstly, our work only considers the ece task in chinese microblogs. it might be interesting to investigate the effectiveness of our framework in social media platforms in other languages. secondly, we only focus on extracting the emotion cause expressed in the current post. however, according to cheng et al. (2017), 37% of the emotion causes exist in the original or historical posts in a conversation thread. hence, it would be interest- ing to extend our work to more complex microblog structures in the future."
524,"limitations the limitations of our work can be summarized in three points. first, as mentioned in section 5, although a direct consideration of prompt information is helpful for related trait-scoring tasks, it may not be for irrelevant traits. therefore, selectively applying each method depending on which traits are to score might further improve the model. second, although the use of pre-engineered features, such as our topic-coherence feature, has the advantage of interpretability (uto et al., 2020), it requires additional engineering steps, as in other aes studies using hand-crafted features (amorim et al., 2018; dascalu et al., 2017; nguyen and litman, 2018; ridley et al., 2021). finally, despite the large improvements observed on the specific datasets asap and asasp++, the model has not experimented on other datasets. feedback prize dataset8 is well-designed for scoring english-written argumentative writings with multiple trait labels, but the prompts are not defined; thus, it does not fit for cross-prompt aes. essay-br dataset (marinho et al., 2022) contains essays on multiple prompts with labeled multiple trait scores. thus, in future work, our proposed methods can be extended to multilingual cases of aes using the dataset."
525,"limitations and should acknowledge and/or try to mitigate them to the extent possible. our work strictly follows the task definition and evaluation protocols (§ 5 and 6) of the original esc paper (liu et al., 2021), where the support is provided through social interactions (e.g., between peers or friends) rather than professional counseling. as mentioned in (liu et al., 2021), further efforts are still needed to probe the ethical extent to which dialogue models can or should provide support. these protocols should also not be used directly in fields other than the esc task (i.e., peer emotional support in daily life) that require the guidance of professional researchers, such as psychological counseling. we also ethically conducted the human evaluation. we transparently communicated with the participants of our study intent and explicitly informed them of the disclaimers before they participated. we paid the participants at the hourly wage above $10/hour, going well beyond the local labor compensation standard. we acknowledge that the results of human evaluation could be affected by the participants’ demographic and geographic characteristics. this work has obtained study approval from the institutional review board (irb)."
526,"limitations the most evident limitation of this research is that is has only been demonstrated on english corefernce. using a lemma-based heuristic requires using a lemmatization algorithm in the preprocessing phase and for more morphologically complex languages, especially low-resourced ones, lemmatization technology is less well-developed and may not be a usable part of our pipeline. application to more morphologically-rich languages is among our planned research directions. in addition, all our experiments are performed on the gold standard mentions from ecb+ and gvc, meaning that coreference resolution is effectively independent of mention detection, and therefore we have no evidence how our method would fare in a pipeline where the two are coupled. a further limitation is that training of the crossencoders still requires intensive usage of gpu hardware (the gpu used for training longformer is particularly high-end)."
527,"limitations in this study, we provide empirical evidence of the impact of domain gap in keyphrase tasks, and we propose effective methods to alleviate it. however, we acknowledge that this study is limited in the following aspects: (1) as the first study discussing domain adapation and few-shot results, there is few studies to refer to as fair baselines. nevertheless, we attempt to show the improvements of the proposed methods over base models by extensive experiments. (2) the pretrained keyphrase generation model can be used off-the-shelf, but the multi-stage adaptation pipeline might increase the engineering complexity in practice. (3) we have only explored three strategies for domain adaptation, and they all require generating hard pseudo labels in different ways. soft-labeling (liang et al., 2020) and knowledge distillation (zhou et al., 2021) methods are worth investigating. (4) we train a model with wikipedia annotation to predict pseudo keyphrases, and it would be interesting to see if we can use large language models (e.g. gpt-3 (brown et al., 2020)) to zero-shot predict phrases."
528,"limitations our work is the first attempt to explore how evidential deep learning can be used to improve the reliability of current ner models. despite the improved performance and robustness, our work has limitations that may guide our future work. first, we propose a simple method to treat hard samples (such as outliers) in the dataset as oov/ood samples, enabling the model to detect oov/ood data with minimal cost. however, there is still a certain gap between these hard samples and the real oov/ood data. oov/ood detection performance can still be improved by further incorporating more real oov/ood samples, for example, real ood data from other domains, well-designed adversarial examples, generated oov samples by data augmentation techniques, etc. second, we evaluate the versatility of e-ner by applying it to mainstream ner paradigms. however, there are still other paradigms, such as hypergraph-based methods (lu and roth, 2015) and the w2ner (li et al., 2022) approach in recent work, that could be evaluated in the future."
529,"limitations firstly, from the technical perspective, we have advocated the advantages of our proposed listwise loss for the mrhp task in terms of generalization capacity. nevertheless, there are other various listwise discrimination functions that may prove beneficial for the mrhp model training, for example neuralndcg (pobrotyn and białobrzeski, 2021), listmle (xia et al., 2008), etc. moreover, despite the novelty of our proposed gradient-boosted tree in partitioning product reviews into helpful and unhelpful groups, our method does not employ prior contrastive representation learning, whose objective is also to segregate helpful and unhelpful input reviews. the contrastive technique might discriminate reviews of distinctive helpfulness features to bring further performance gain to multimodal review helpfulness prediction. at the moment, we leave the exploration of different listwise discrimination functions and contrastive learning as our prospective future research direction. secondly, our study can be extended to other problems which involve ranking operations. for instance, in recommendation, there is a need to rank the items according to their appropriateness to present to the customers in a rational order. our gradient-boosted decision tree could divide items into corresponding partitions in order for us to recommend products to the customer from the highly appropriate partition to the less appropriate one. therefore, we will discover the applicability of our proposed architecture in such promising problem domain in our future work."
530,"limitations. first, our method requires additional entity resources, which may be difficult to obtain for certain language pairs. with the development of multilingual entity datasets like paranames (sälevä and lignos, 2022), we are optimistic such resources will be more accessible in the near future. second, as demonstrated in section 5.4, extracting translation candidates increases inference time. due to space limitation, more limitations are discussed in appendix a.6. 15evaluated on a p40 gpu with batch size of 1, other experimental settings same as section 4"
531,"limitations our machine translation experiments revealed that optimal generalization performance is obtained with small interval values. however, r-dangle with small intervals still runs much slower than an equivalent transformer model. despite our modifications, large r-dangle models with small intervals on large datasets remain computationally expensive. in this paper, we only explored a sim- ple periodic re-encoding strategy. however, more complex and flexible ways of re-encoding could be used to further improve computational efficiency. for instance, we could adopt a dynamic strategy which learns when re-encoding is necessary."
532,"limitations in this work, we proposed sstuning for zero-shot text classification tasks. during inference, we may need to design verbalizers even though we can use templates like ""this text is about [label name]"". for simplicity and fair comparison, we only refer to previous works for such designs, which may be sub-optimal. as shown in table 4, using the verbalizers ""terrible."" and ""great."" work better than ""it’s terrible."" and ""it’s great."" for the sst-2 and imda tasks that we reported in the main results. if the labeled validation set is provided, the model may perform better by choosing verbalizers based on the validation set. due to limited computation resources, we only tuned the model with 5.12 million samples, which is only a small portion of the available samples. we believe that tuning the model on a larger dataset help improve the performance. even though the computational cost will also increase, it is worth it since no more training is needed at the inference phase. in addition, we did not do extensive hyperparameter searches except for the learning rate, which may further improve the performance. in our experiment, we only tested the method with discriminative models like roberta and albert. its performance with generative models is not known. it is non-trivial to test on such models since generative models can do both natural language understanding tasks and natural language generation tasks. we leave this as future work."
533,"limitations the limitation of our study is that we only evaluated our model on a limited set of spoken language transcripts. we believe that additional attention should be given to features specific to ad patients, such as pauses and filler words in speech. furthermore, the lack of diversity in the data may also adversely affect the model’s performance on unseen samples. our model would benefit from further testing on a wider range of data, including different languages and different modalities, to see if it is capable of generalizing to other domains in the future."
534,"limitation to this work, which is related to time complexity and speed performance. since every instance is transformed into multiple qa instances, it may take a relatively long time to process a document. 4qualitative analysis is provided in appendix c. limitations there are two primary limitations of the system presented in this work. first, each set of questions we use for training the qa model is designed specifically for the dataset we trained our model on. while we provide a set of questions for each of the two common trc datasets, we believe that training the model on other datasets may require rewrite of the questions. second, as mentioned in the previous section, every trc instance is converted into multiple qa instances which we then process individually. this may increase the overall inference time and pose a practical limitation which needs to be carefully considered."
535,"limitations • we are unable to test the proposed model’s performance on other datasets due to the unavailability of public multi-reference headline generation datasets. • our dataset is created over a period of 6 months and contains around 3000 examples. although there are several commonly used benchmark datasets with a similar number of examples: e.g., r4c reading comprehension dataset (6.4k examples) (inoue et al., 2020), fire-lid (3357 examples), iiithner (3084 examples) datasets in gluecos benchmark (khanuja et al., 2020), wnli (634 examples), rte (2500 examples) and mrpc (3700 examples) datasets in glue benchmark (wang et al., 2018), nope corpus (around 2.7k examples) (parrish et al., 2021), we believe that it will be better to have a larger dataset for this challenging task. we plan to create a larger version of the dataset in future work."
536,"limitations we reflect on the limitations of our model below: 1. our experiments are based on large everyday household datasets (i.e. coin and crosstask). our language model is pretrained with web data, which helps it handle such householdrelated procedures well. however, when applied to other more specialized domains like medical procedures, language models might suffer from the domain gap and impact overall model performance. 2. the language model has excellent planning ability given the ground truth start and goal steps. however, it is still hard for the language model to generate very long sequences of steps. when the planning horizon t increases, the performance of our model drops quickly just as other methods do. 3. in real-world applications (i.e planning task for robots), a good model should be able to dynamically adjust the plan given external feedback. for example, when the execution of one step fails, the model will need to re-plan as soon as possible. our model does not possess such an ability so far, since our planning approach is offline. we leave this direction for future research."
537,"limitations there are several limitations that can be considered for future improvements: (1) in multimodal alignment and fusion, we only consider a single image for each sample, whereas multiple images can be available. a more flexible visual encoding architecture that can digest an indefinite number of input images can improve the visual information coverage; 2) the empirical results in this work focus on three attribute extraction datasets (i.e., item form, color, and pattern) that can clearly benefit from visual perspectives, while there are also various attribute types that rely more on the textual input. different traits of attributes may influence the preferred modalities during the modeling, which is out of scope for this work but serves as a natural extension of this study; 3) currently there is no specific design to improve the efficiency based on the visual question answering architecture. it can be not scalable as the number of attributes increases. there could be a dual-use regarding the attentionpruning mechanism, which can be a potential risk of this work that could arise and harm the result. the attention-pruning mechanism encourages the model to focus on the task-relevant foreground on the given image selected with category supervision, which can improve the prediction precision given the input image is visually rich and contains noisy context background. while for some types of images, such as infographics, there may be helpful text information on the images or intentionally attached by providers. these additional texts may be overlooked by the attention-pruning mechanism, resulting in potential information losses. a possible mitigation strategy is to add an ocr component along with the visual encoder to extract potential text information from given images."
538,"limitations the main target of this paper is to utilize structural knowledge for cross-lingual comprehension. we present a new pre-training task named scp in the hope of bridging the misalignment of structural words in the parallel corpus. more generally, we expect the proposed method can facilitate the research of cross-lingual understanding. admittedly, the main limitation of this work is that we rely on off-the-shelf tools to extract and align words in different languages, which would result in some mistakes at some situations. for example, giza++ only achieves 80%-85% accuracy in aligning the corresponding words in another language. currently, no tech can achieve this goal in 100% accuracy. as a result, some bias data in pre-training calls for further research and consideration when utilizing this work to build xplms."
539,"limitations our work has the following limitations: first, we cannot generalize our"
540,"limitation scaling through the inference corpus. the size of the reference corpus is an additional dimension for model scale in nonparametric models. in this paper, we scale the corpus up to nearly 1b tokens, which is still smaller than the training data of very large language models (brown et al., 2020; rae et al., 2021). we think future work can scale it fur- ther using tools such as distributed faiss (johnson et al., 2019) or scann (guo et al., 2020). significant memory usage. using npm saves gpu compute and memory compared to using models with more parameters. however, npm requires more ram and disk memory due to embeddings of a reference corpus. for instance, the largest corpus in our experiments (full english wikipedia) requires 70gb of ram and 1.4tb of disk memory. future work can build more efficient npm as done in prior work in nearest neighbor search (jegou et al., 2010; norouzi et al., 2012; ge et al., 2014; izacard et al., 2020; yamada et al., 2021). exploration of larger vocabulary. large vocabulary is known to lead performance gains (conneau et al., 2020) but is bounded in memory costs. previous work explored more efficient softmax approximations (morin and bengio, 2005; chen et al., 2016; grave et al., 2017). our nonparametric training offers an alternative by removing the softmax over the vocabulary. with the roberta architecture, increasing the vocab size by 2x makes the baseline training 50% more memory expensive, but does not increase the memory in training npm. however, this paper does not include more systematic evaluation on the effect of large vocabulary. future work can explore training npm with a significantly larger vocabulary to further boost performance. extension for generation. our paper evaluates npm only on prediction tasks. it is currently nontrivial to use npm for generation, since it is the encoder-only model. future work can explore autoregressive generation as done in patel et al. (2022) or use npm for editing (schick et al., 2022; gao et al., 2022). extension to few-shot learning and fine-tuning. our paper focuses on zero-shot evaluation only. future work can extend npm to a few-shot learning setup. in fact, fine-tuning npm is significantly easier than fine-tuning larger models such as t5, opt and gpt-3 which we compare npm with, and can be explored in future work. better cross-lingual transfer. our work explored cross-lingual transfer in a limited setup where the model is trained on monolingual data. we think future work can train multilingual npm, and explore more comprehensive cross-lingual evaluation. in fact, nonparametric training may alleviate the burden of collecting large-scale multilingual corpora since it makes the model less sensitive to the language coverage in the training data, and may lead to significantly better cross-lingual transfer, as we demonstrate in the entity translation task. limitation in speed. we find that search makes inference considerably slower than the counterpart without search. we think that (1) search can significantly be faster with better engineering (we use the default hyperparameters of the faiss index with no tuning) or better index, and (2) the speed of npm is still on par with the speed of significantly larger parametric models that npm outperforms (see table 4). moreover, while not explored in this work, there has been work that improves inference speed (he et al., 2021; alon et al., 2022) that can be applied to npm. we leave improving inference speed to future work."
541,"limitation of the moraldirection is that it is induced on individual words, and thus longer sentences are a significant challenge for the models. still, we were able to test them on parallel subtitles data, which contains slightly longer, but predominantly still short, sentences. problems that showed up repeatedly in this experiment were an over-reliance on key lexical items and a failure to understand compositional phrases, particularly negation. additionally, typical problems of pmlms, such as disambiguation problems across multiple languages, were noticeable within xlm-r. non-english languages appeared more affected by such issues, despite the fact that all our target languages are relatively high resource. (3) moral foundations questionnaire. our experiments with the mfq reinforce the"
542,"limitations in this work we present a novel method based on representation augmentation to solve cross-lingual transfer learning for event detection (ed). although our experiments demonstrate the effectiveness of the proposed method, there are still some limitations that can be improved in future work. first, our current method only leverages sentencelevel context in input document to perform ed over different languages. this might not be optimal as document-level context has been shown to be helpful for ed (pouran ben veyseh et al., 2021b) that can be explored in future research to improve our cross-lingual models. second, the evaluation for our model is limited to only three popular languages (english, chinese, and arabic) that are supported by existing pre-trained language models, unlabeled data, and text processing tools. as such, it is unclear whether the method can be adapted to many other languages with limited access to such resources (e.g., low-resource languages). we believe this is an important direction that can be investigated in future work to advance our understanding for ed models. finally, our method requires joint training with a retrieval model (based on multilingual pre-trained language models) that can impose additional computational costs (as shown in table 3). reducing necessary computational costs for our model is an important direction to make it more accessible for different applications and domains."
543,"limitations to better enlighten the follow-up research, we conclude the limitations of our method as follows: 1) although the method we proposed can help improve the quality of generated labels, there is still room for further improvement; 2) because our detection is not perfect, it will lead to inaccurate labels of some samples. we look forward to better methods to improve detection in the future; 3) this work has verified that the extended labels can effectively help the performance of models and proposed a method of label extension, but has not tried other extension methods or whether it is helpful to extend more labels. 4) this work focuses on solving open environment intent prediction with different generative models, without exploring other types of models."
544,"limitations our approaches mainly leverage a fixed feature extractor together with a set of individually trained classifiers to mitigate catastrophic forgetting whereas a tunable feature extractor may also be helpful and complement the individually trained classifiers, so a future direction is to design advanced strategies to efficiently tune the feature extractor in combination with our proposed ice based classifiers. in addition, we mainly investigate the classifier drift and demonstrate the effectiveness of our solutions under the class-incremental continual learning setting. another future direction is to explore similar ideas under other continual learning settings, e.g., task-incremental learning, online learning, or the setting where new sessions also contain annotations for old classes."
545,"limitations in each contribution of this work, we can isolate several potential limitations. in creating c-xnli, the mt model for the formation of the train set was chosen based on the results from a single dataset. additionally, an assumption that the model is plagued with typical issues that affect mt models was investigated on a small dataset. although we are skeptical of the mt model’s performance and perform qe scoring of the small dataset by a group of annotators and analysis to ascertain its performance, we are only comparing croatian machinetranslation results to results from a single language (german), assuming that results would hold for other high-resource languages. also, for some mt evaluations, we use a single metric (bleu) known to have many problems but only generally considered to correlate with human judgment. our hyperparameter optimizations are of limited scope. all hyperparameters are fixed, except the learning rate with four possible values we search over. furthermore, we only used three seeds. we could not perfectly reproduce the results outlined in the paper of our baseline xlm-r base model, partly due to a lack of elucidation in the original 4cc by-nc 4.0 5https://github.com/lobadic/c-xnli paper and partly due to limited hyperparameter optimizations. finally, we do not elucidate further and experiment with the discovered correlation between the models’ performance and the overlap in datasets, and we leave it for future work."
546,"limitations, resulting in a reduced impact on the environment. we finetuned nine models on program and function translation tasks and due to the smaller size of the training data, all jobs took a total of 1–2 days on rtx 2080 ti gpus. a total of 100 hours of training in a single rtx 2080 ti gpu results in approximately 7.5kg of carbon emission into the environment.10 sensitive information avatar composed of parallel programs and functions that do not have any natural language (nl) comments or docstring. we remove them to get rid of any personally identifiable information or offensive content. however, there could still be such content in the form of string as we do not manually check each example."
547,"limitations our study revealed considerable differences in transferability and other measures we considered across different datasets. nonetheless, the study focused on the differences in transferability arising from the choice of the models and the al methods rather than the dataset. to eliminate confounding due to datasets, we grouped the results by datasets and analyzed each group separately. despite this, the scope of our results is limited by the fact that all datasets used are in english and possibly contain their own biases. even though we showed that it could still be useful to transfer actively acquired datasets between transformer-based plms, it is important to keep in mind that actively acquired datasets are not representative of the original data distribution due to the sampling bias introduced by active learning."
548,"limitations regarding our work, a few limitations should be mentioned. during the annotation process, conflicts between annotations occurred. all conflicts were discussed with a senior researcher and resolved in this way. thus, we reached the best possible agreements, but still some agreements are lower than others (see table 2). for example, legal claims and premises have a relatively large room for interpretation. perfect results can only be expected by over-anchoring the annotators and weakening the guideline, which we have consciously avoided in our research. the comparison of the iaa with other works in the field of nlp from legal science is not possible, because the works either do not examine the components of the appraisal style or identify the components of the judgment style without the indication of the iaas (urchs et al., 2020). compared to works that also annotated premises (kripp. α = 51.08%) and claims (kripp. α = 55.49%) in business pitches (wambsganss and niklaus, 2022), our work provides comparable results with respect to the agreement of the krippendorff α (premise = 55.89%, legal claim = 45.02%). further work shows similar results α = 44.1% (park and cardie, 2018). all in all, we can assume that both our components and our mounted relationships, achieve comparable or better results than comparable works (e.g., park and cardie (2018)). although our model shows accurate values between 78% and 92% for predicting the components of the appraisal style, the values for determining legal claims and premises are lower (62% and 78%) compared to the other values. however, they display reasonable values when compared to previous nlp studies. we can only compare our work to other related work in another domain because values for detecting legal claims and premises are not available in the nlp literature. for instance wambsganss and niklaus (2022), present an accuracy of 54.12% for their long short-term memory (lstm) model which detects claims and premises. with the mentioned model the authors shows positive outcomes in supporting students’ argumentative skills. our models show similar or higher precision in comparison to the works of poudyal et al. (2020) or wambsganss and niklaus (2022) (see table 7 in the appendix a) and our post-test results also show significant learning outcomes (see table 11 in the appendix a). although we can show a significant learning output, it must be noted that this is only short term. as a result, we intend to carry out additional field experiments in the future to establish the system’s effectiveness over a more extended period and demonstrate long-term success. as a third possible limitation, our models are limited to applying the appraisal style in german only. in the future, further efforts have to be made to investigate the transfer-ability or adaptation of our models to other countries with other legal systems and other languages. however, we assume that this is possible in principle, since some countries such as china now use the appraisal style in law teaching (man, 2022) and countries such as the u.s. use at least similar approaches such as learning with case studies using the irac formula (metzler, 2002). nevertheless, some adaptation of the models is needed, since the language and the legal form in each country have their own specificities."
549,"limitations we build a new benchmark for syllogistic reasoning. the limitations are mainly in the experiments part: (1) due to the limited human resources, our test set is quite small, which may not support training large models directly. (2) we evaluate all models by comparing their predictions with the groundtruth"
550,"limitations more work is needed to uncover the causes of the inconsistent performance across randomly initialized models in experiment 1. although the bias toward forward function application implemented in experiment 2 was effective in our experiments, it is unlikely to work as a general-purpose method, since languages vary in their branching characteristics and in the contexts in which they apply forward and backward function application."
551,"limitations in our current experiments, prompt-based methods are primarily storage-efficient or parameterefficient solutions. since these methods all require backpropagation to the bottom layer, the training time of prompt-based methods are closely resembles that of traditional fine-tuning approach."
552,"limitations considering that english is the most widely spoken language, we select it as the high-resource monolingual language in this study. while across is a general summarization framework not limited to a certain target language, it deserves an in-depth exploration of how across works on other highresource languages. additionally, we employ mt5 as our backbone because it supports most languages in crosssum. the performance of across after replacing mt5 with other models, such as mbart(liu et al., 2020), flan-t5(chung et al., 2022), will be investigated in the future. ethical consideration controversial generation content. our model is less likely to generate controversial content(e.g., discrimination, criticism, and antagonism) since the model is trained on a dataset from the bbc news domain. data in the news domain is often scrutinized before being published, and thus the model is not likely to generate controversial data. desensitization of user data. we use the amazon mechanical turk crowdsourcing platform to evaluate three artificial indicators (i.e., fluency, informativeness, and conciseness). for investigators, all sensitive user data is desensitized by the platform. therefore, we also do not have access to sensitive user information."
553,"limitations in this paper, we propose the fine-purifying approach to purify fine-tuned pre-trained language models (plms) by detecting poisonous dimensions and mitigating backdoors or bias contained in these poisonous dimensions. to detect poisonous dimensions in fine-tuned plms, we utilize the diffusion theory to study the fine-tuning dynamics and find potential poisonous dimensions with abnormal finetuning dynamics. however, the validity of our approach relies on assumptions that (1) backdoors or biases are injected during the fine-tuning process of plms; and (2) the fine-tuning process can be modeled as a diffusion process. therefore, in cases where the assumptions do not hold, our approach cannot purify the fine-tuned plms. for example, (1) backdoors or biases are contained in the initial plm weights rather than being injected during the fine-tuning process; or (2) the fine-tuning process involves non-gradient optimization, such as zero-order optimization or genetic optimization, and thus cannot be modeled as a diffusion process."
554,"limitations we only consider 14 languages and 21 categories, whereas wikipedia has pages in more than 300 languages and 200 broad categories. increasing the scale and diversity will further improve method generalization. our proposed method relies on the good multilingual translation of key and value from table pairs. although we use key, value, and category together for better context, enhancement in table translation (minhas et al., 2022) will benefit our approach. because our rule-based system requires manual intervention, it has automation limits. upgrading to completely automated methods based on a large language model may be advantageous. we are only considering updates for semi-structured tables. however, updating other page elements, such as images and article text, could also be considered. although a direct expansion of our method to a multi-modal setting is complex (suzuki et al., 2012)."
555,"limitations while our research and empirical results support specific evaluation metrics for the task of clinical note generation according to a given evaluation criteria, more results, including testing on additional datasets are needed to further validate these findings. our manual annotations followed clear and structured guidelines, but could still contain some level of annotator bias and have an average pearson inter-annotator-agreement of 0.67 (tables 7 and 8)."
556,"limitation one limitation of styleap is that one extra inference is needed for retrieval. it is mainly due to the monolingual retrieval accuracy is higher than that of crosslingual retrieval (refer to section 6.1). in the future, we will try stronger multilingual model to mitigate this effect."
557,"limitations although our method achieves state-of-the-art performance consistently on the four benchmark datasets, it suffers from the following limitations: • no optimization for the verbalizer. the verbalizer we use in the prompting stage is just a simple 1-to-1 mapping, this simple design does not fully exploit the capabilities of mlm. • no explicit modeling of the relationship information between nested entities. we consider that in some other scenarios, the relationship information between nested entities is not very significant. consequently, explicitly modeling the relationship may introduce new biases. so we just utilize the potential information. but in practice, it is worth exploring how to model such a relationship from a novel perspective."
558,"limitations this paper introduces the problem of few-shot novel product title generation to efficiently and accurately generate informative and appealing titles for novel products with limited labeled data. however, the training of our proposed model relies on the paired image-attribute-title data, which may not be easily obtained simultaneously in the real world. therefore, our model may not work well when high-quality image data or textual profile is missing. the limitations could be alleviated using techniques such as knowledge distillation or self-training. besides, the writing styles of the generated titles are highly correlated with the training data. hence, it requires specific and appropriate treatment by experienced practitioners, when deploying new products online."
559,"limitations despite the strong performance on the presented datasets, our approach is limited in its ability to update knowledge state and adapt to new domains. a major feature of retrieve-then-read is the ability to swap in new documents when new information is learned, such as temporally more recent documents, or adding in documents from a new domain to quickly adapt to a new downstream task. our approach relies on a large language model to contain all this knowledge and adding new knowledge would likely require some retraining. in addition, large generation models still suffer from hallucination errors, resulting in incorrect predictions. when tasked with generating 10 urls, llm-url may only generate 6 or 7 which link to valid documents. finally, our approach involves very large language models, slow web requests, and document processing which may make it cumbersome to use in practice."
560,"limitation first, there are studies (wu et al., 2021) claiming visual information only serves as regularization. in our ablation study, we find the adversarial setting of fusion-based approach outperforms the plain transformer. combined with observations from previous studies, we suggest that fusion-based architectures may apply some images information as regularization terms, yet the further quantitative analysis is needed to confirm this phenomenon. second, though our testset is carefully selected to ensure the textual ambiguity without image data, we encounter difficulties in designing a suitable metric for quantifying the degree to which the models are able to resolve the ambiguity. specifically, we find that conventional metrics, such as wordlevel entity translation accuracy, exhibit significant fluctuations and do not effectively quantify the extent to which the model effectively resolves ambiguity. we discuss this metric in more details in the appendix, and offer a glossary of ambiguous words used in the test set. we acknowledge that the evaluation of multimodal ambiguity remains an open problem and an area for future research. in addition, there are some details regarding the dataset that we need to clarify: the dataset is collected after covid-19, so some commodities will be associated with the pandemic. we collect data by category in order to cover various products to reduce the impact of the epidemic on product types."
561,"limitations described in prior work. gpt-3 achieved near perfect performance on this new test set. we then investigated the task of noun compound conceptualization (ncc). ncc evaluates the capacity of plms to interpret the meaning of new ncs. we showed that gpt-3 still performs reasonably well, but its success can largely be attributed to copying definitions or parts of definitions from its training corpus."
562,"limitations we see several limitations regarding our work. first, we focus on documents in the english language only, neglecting many caribbean newspapers and islands with other official languages. while some of our methods can be easily extended to non-english material (e.g. weat analysis), methods that rely on the pre-trained english model f-coref (i.e. pmi, lexicon-based analysis) can not. on the same note, f-coref and spacy were developed and trained using modern corpora, and their capabilities when applied to the noisy historical newspapers dataset, are noticeably lower compared to modern texts. contributing to this issue is the unique, sometimes archaic language in which the newspapers were written. while we validate f-coref performance on a random sample (§5.2), this is a significant limitation of our work. similarly, increased attention is required to adapt the keyword sets used by our methods to historical settings. moreover, our historical newspaper dataset is inherently imbalanced and skewed. as can be seen in tab 2 and fig 8, there is an over-representation of a handful of specific islands and time periods. while it is likely that in different regions and periods, less source material survived to modern times, part of the imbalance (e.g. the prevalence of the us virgin islands) can also be attributed to current research funding and policies.12 compounding this further, minority groups are traditionally under-represented in news sources. this introduces noise and imbalance into our results, which rely on a large amount of textual material referring to each attribute on the gender/race plane that we analyse. relating to that, our keyword-based method of classifying entities into groups corresponding to the gender and race axes is limited. while we devise a specialised keyword set targeting the attributes female, male and non-white, we classify an entity into the white group if it was not classified as non-white. this discrepancy is likely to introduce noise into our evaluation, as can also be observed in tab 7. this tendency may be intensified by the nlp systems that we use, as many tend to perform worse on gender- and race-minority groups (field et al., 2021). finally, in this work, we explore intersectional bias only along the race and gender axes. thus, we neglect the effects of other confounding factors (e.g. societal position, occupation) that affect asymmetries in language. ethical considerations studying historical texts from the era of colonisation and slavery poses ethical issues to historians and computer scientists alike since vulnerable groups still suffer the consequences of this history in the present. indeed, racist and sexist language is not only a historical artefact of bygone days but has a real impact on people’s lives (alim et al., 2020). we note that the newspapers we consider for this analysis were written foremost by the european 12the danish government has recently funded a campaign for the digitisation of historical newspapers published in the danish colonies; https://stcroixsource.com/20 17/03/01/. oppressors. moreover, only a limited number of affluent people (white males) could afford to place advertisements in those newspapers (which constitute a large portion of the raw material). this skews our study toward language used by privileged individuals and their perceptions. this work aims to investigate racial and gender biases, as well as their intersection. both race and gender are considered social constructs and can encompass a range of perspectives, including one’s reflected, observed, or self-perceived identity. in this paper, we classify entities as observed by the author of an article and infer their gender and race based on the pronouns and descriptors used in relation to this entity. we follow this approach in an absence of explicit demographic information. however, we warn that this method poses a risk of misclassification. although the people referred to in the newspapers are no longer among the living, we should be considerate when conducting studies addressing vulnerable groups. finally, we use the mutually exclusive white and non-white race categories as well as male and female gender categories. we acknowledge that these groupings do not fully capture the nuanced nature of bias. this decision was made due to limited data discussing minorities in our corpus. while gender identities beyond the binary are unlikely to be found in the historical newspapers from the 18th-19th century, future work will aim to explore a wider range of racial identities."
563,"limitations our work is limited by several factors. first, our findings are supported only by experiments on a single nlp task (neural text simplification). we selected this task because it offered an intriguing sandbox for studying varying experimental conditions, ranging from differences in random seeds to modifications in compile-time and run-time environments and dependency versions. comparing the multifaceted outcomes arising from these experiments facilitated greater quantified estimations of the degree of reproducibility for the selected nts systems. however, the dimensions of variation that we explored in this work are common to many nlp tasks; none are unique only to text simplification. because of this, we believe that our findings would generalise broadly across nlp tasks. we used a single data set, the same as in the original paper by nisioi et al. (2017), to foster controlled study of our other experimental variables. the data set comprises aligned sentences between english wikipedia and simple english wikipedia. thus, it is unclear whether our findings would be similar if the study was conducted using data from other languages, including those with richer morphology such as czech or arabic. finally, although we conducted a robust set of experiments for the selected models across two research groups, our experiments are limited to a small set of nts models due to the extensive set of conditions tested for each model. although these models vary in their architecture, we do not know if other nts models may be more or less stable across experimental conditions. taken together, the limitations accompanying our findings suggest compelling avenues for future research."
564,"limitations the main limitation of the proposed framework is its dependency on a reasonable amount of real implicit hate instances to be used as the prompting input material. obtaining implicit and subtle messages from social media is undoubtedly a challenging and time consuming task. more importantly, another limitation lies in the fact that the proposed framework does not rely on an automatic metric to determine if the generated messages are actually implicit. therefore, a human-in-the-loop step for validating the obtained newly generated instances is still required. additionally, there has been mounting pressure to obtain debiased plms, which might lead to the generation of less challenging examples."
565,"limitations the main downside of ssmt (compared to presegmentation models like bpe and ulm) is its computational complexity. our architecture (figure 1) introduces additional computation in 2 way. firstly, the decoder conditions on the characterlevel history of the target sentence, so it has to process more tokens than a standard subword decoder. secondly, the dynamic programming algorithm (equation 5) requires more computations than standard mt models training on pre-segmented datasets. in practice, ssmt takes an order of magnitude (10×) longer to train than models training on a pre-segmented dataset. dynamic decoding also adds computational complexity to testing, although this is less of an issue since test set sizes usually permit run times within a few hours. it would depend on the practitioner to decide whether the performance boosts obtained by ssmt justify the longer training and decoding times. however, since ssmt is particularly strong for data scarce translation, the computational complexity might be less of an issue. for translation directions like english to swati, training times are quite short for all models (less than a day for ssmt on subpartitions of the a100 gpu), so the increased training times are manageable."
566,"limitations even though our proposed methodology, tgtss, was able to significantly reduce model instability, there is still a gap in performance with the gold standard ensembling techniques. more work needs to be done to bridge this gap. in our empirical analysis, we used two open source datasets, massive and clinc150. both these datasets are small and may not represent the complexity in real world production datasets which may contain substantially large noise. in our proposed methodology, we train a pair of models successively, a teacher and a student, which is significantly better than ensembling in terms of computational cost. however, this setup may still be challenging in many sophisticated real world production nlu systems. more work needs to be done to reduce the computational complexity of training and inference for these systems."
567,"limitations we analyze the limitations of this study from the following perspectives: • the aste task extracts the sentiment triplets from a review, while the aspect sentiment quad prediction (asqp) task adds an aspect category based on the triplets and provides more comprehensive information. defining the aspect category for each domain is also hard work. future work can take the aspect category into consideration. • all the models are evaluated by f1 score, in which only exact matching can be considered correct. this metric can not differentiate between partially matching and completely mismatching and is not the best choice for a challenging dataset like dmaste. future work can include some partially matching metrics for this task. • there is no specifically designed method for cross-domain aste. but we analyze the challenges of this task in detail. we are planning to design a new method for cross-domain aste based on the analysis results."
568,"limitations there exist so many different transformer models and efficiency methods that it is extremely difficult to conduct exhaustive experiments for all of them. although our experiments demonstrate nice properties for efficiency operators, the observations are restricted to our experimental setup. considering the huge space of all combinations of transformer models, efficiency methods, and datasets, our experiments provide understanding for an important but small subspace, and it is possible that the"
569,"limitations of scaling up data and model sizes, we hope that it will pave the way for the arabic nlp community to focus on problems that are beyond the reach of plm scaling."
570,"limitations our study is limited in scope, studying only classification and extractive qa tasks in english; the trends we highlight in this work might not generalize to different tasks or other languages. we also acknowledge that we only use bert-based models for our analysis, so it is uncertain whether these findings are applicable to other models. in addition, the overlap we describe in this paper is defined by semantic similarity rather than literal overlap between sentences and phrases. we are not claiming that this overlap is good or bad, rather we show that when the overlap is large, it is more difficult to evaluate model generalization. we note that there are multiple confounding factors in our results. first, while we highlight the role of dataset collection method in our analysis, the naturalness of data collection method is negatively correlated with task difficulty (i.e., the more natural datasets we study are also the least difficult). as a result, differences in performance can be attributed to task difficulty as well as data col- lection method. second, our study is limited in scope of similarity metrics (only cosine similarity) and embeddings used to compute similarity. using different embedding or metric can change the results."
571,"limitations the main drawback of the work is in its evaluation, which was performed on datasets which were not manually annotated for the task, but adapted to it in various means. while we believe these evaluation sets do provide a strong indication regarding task performance, evaluating on bespoke data explicitly annotated for the task is usually preferable. another limitation is language specificity: the work currently focuses on english, without considering other languages, which are also left for future work."
572,"limitations the numbers in this survey are limited to papers published in the acl anthology and isca proceedings. however, we also included papers as related work from other resources if they are publicly available and accessible. in addition, the category in the survey does not include the code-switching type (i.e., intra-sentential, inter-sentential, etc.) since some papers do not provide such information."
573,"limitations one limitation of this work is that while our approach alleviates the requirement of persona description during inference, it still requires persona description for the training corpus. a viable solution is to transfer the pre-trained persona detection models to other datasets without persona description in train set. however, the success of this approach may depend on the degree of similarity between the target dataset and the personachat dataset."
574,"limitations in this section, we enumerate a few limitations of our work: • we believe that the need to train transformer architectures on gpu is an obstacle to the use of this pipeline, which is destined not to be used in an academic environment but by legal practitioners. • because of the specificity of each jurisdiction, generalizing to other countries may not be possible on all labels with the exact same models (for example in extracting the names of tribunals). • the manual annotation process is a weakness: while it results in gold-standard annotations, it is very time-consuming. we do acknowledge that the amount of training data presented in this work is low and that collecting more annotations in the future would improve the quality of the results. we think it would be interesting to look at self-supervised methods, weak supervision, and annotation generation. the need for labeled data also prevents easy replication of the pipeline to new data sets, which would also require manually annotating. • more precisely on the extracted categories, some categories lack precision and would require additional processing steps to achieve satisfactory results. for example, the category person sometimes refers to the claimant or their family, but sometimes refers to the name of the judge."
575,"limitations in this paper, we indicated that the vanishing gradient problem, caused by layer normalizations, makes the training of deep post-ln transformers unstable. we proposed the b2t connection to mitigate this vanishing gradient problem. however, the proposed b2t connection does not perfectly prevent the vanishing gradient, as shown in figure 3. therefore, the vanishing gradient might harm the training in extremely deep transformers even if our b2t connection is used. in addition, this study depends on empirical observations. in particular, we provided little theoretical justification of the reason for post-ln outperforming pre-ln when training succeeds. however, as discussed in appendix c, the method with a theoretical justification often collapses in some situations. because the behavior of deep transformers in various situations is not fully understood, we believe that it is important to provide empirical findings for our research field to progress. although appendix c includes a comparison between our b2t connection and the latest method, deepnet (wang et al., 2022), we could not investigate the behavior of all methods in the 100l-100l configuration because of our limited computational budgets. however, we are confident that we conducted sufficient experiments to verify our contributions."
576,"limitations this paper does not utilize any major linguistic theories of code-switching, such as (belazi et al., 1994; myers-scotton, 1997; poplack, 2013). our approach to generating code-switched texts replaces words with their synonyms in target languages, looked up in a bilingual lexicon. furthermore, we do not make any special efforts to resolve word sense or part-of-speech ambiguity. to this end, the resulting sentences may appear implausible and incoherent."
577,"limitations although the proposed ckdst distills the knowledge of mt more comprehensively and efficiently from encoder representations and prediction logits, and obtains significant improvements over previous methods, it still has limitations: (1) the batch size is not very large, limited by the memory capacity of the used hardware and the extremely long sequence length of speech inputs, which leads to a small number of negative samples used in ccrd and does not fully exploit the ability of contrastive learning. in future work, we attempt to expand the negative sample size using a mechanism like memory bank (he et al., 2020). (2) as we distill knowledge from mt to st, the performance of the pretrained mt model has an impact on our framework."
578,"limitations as the contributions of this work include a framework and preliminary experimentation, there are a number of constraints that we leave to future work. firstly, we considered only one family of response distributions. we chose normal distributions because their behavior is well-understood and they are easy to work with. however, the structural similarities between normal distributions and the best performing metrics—namely, absolute error— suggests that, more generally, the best test metrics for nhst may vary depending on the underlying response distributions. therefore, we recommend that use of our framework should potentially vary depending on the dataset being considered, and might have other distributions commonly found in model and gold standard items and responses, such as exponential or multinomial distributions. similarly, we only considered p-value estimators that are based on bootstrap sampling. implementation of our framework in future use would benefit from matching the estimator to the test metric. for instance, permutation tests are the most common way to estimate p-values for spearman correlation, and analytical tests such as student’s or macnemar’s, which are commonly used even when the underlying assumptions on which they are based are not likely to hold (as, we expect is the case here). as such, the sampling method could change based on which metric is best for the task/data."
579,"limitations we expect that our cross-lingual models have learnt some coreference knowledge on the target languages and we conduct experiments on some languages in zero-shot settings. however, we do not get consistent and significant improvements compared to monolingual models. this should be further investigated which potentially helps languages with few or no coreference annotations. compared to monolingual models, our cross-lingual model improves the source-side coreference resolution but it requires almost two times gpu memory during training. thus, this model architecture imposes restrictions on using larger pretrained models given limited resources."
580,"limitations besides the technical challenges discussed in section 4.4-4.5, limitations of this work also include the issue of data imbalances that some attributes may have imbalance distributions. for example, we may find significantly more profiles with the country of citizenship as united states than any other countries, which may have a negative impact on generalization, especially when the distributions of training and inference diverge. similarly, the distributional variances discussed in section 4.5 indicate that the prediction results for non-celebrity distributions should be carefully adjudicated. the degraded performances on low-resource attributes also indicate that the prediction results may be unreliable when performing inference on attributes without enough training data. in this paper, we assume that the attributes are already given. however, many wikidata attributes are not applicable to everyone. for example, attributes such as “position played on team” may be specific to athletes. therefore, it is also important to investigate how to automatically detect applicable attributes for certain users. in this work, we use at most 100 recent tweets and aggressively create training and inference examples between each attribute and those tweets. since we use sliding window on the collected tweets, involving more tweets in training or inference may significantly increase the time cost."
581,"limitations our study has three limitations: • as reported in section 5.1, nli tasks significantly benefit from core while other tasks marginally do, or there is no benefit at all. we suppose that such a difference comes from characteristics of a task. however, we have not yet thoroughly explored which characteristics of a task attribute the performance gain. to solve this problem, we need a novel deep learning interpretation method to probe latent contexts of lm, or a thorough analysis on relationship between the pretraining objective and downstream tasks and how prompting bridges two distinct phases. we leave these research questions as our future work. • core does not work for cases where the train dataset has too long sequence texts. core requires multiple examples to be concatenated, so developers cannot benefit from core if a majority of concatenated examples from their dataset exceed the maximum sequence length of an lm. • we have not yet analyzed whether core is applicable to natural language generation (nlg) tasks. nlg is undoubtedly an important pillar in natural language processing research, along with nlu, with many interesting applications. we believe that the concept of context attuning and context filtering can be of help to major challenges in nlg, for example, controlled nlg. we plan to explore core on nlg tasks after this submission."
582,"limitations our work is constrained into multi-choice question answering system and limited to common sense reasoning tasks, lacking more exploration in other reasoning tasks, e.g. arithmetic reasoning (cobbe et al., 2021; chen et al., 2021), conversational reasoning (chen et al., 2022) and symbolic reasoning (wei et al., 2022). we plan to leave these directions as future work."
583,"limitations there are two limitations of this work. (1) the used corpus of argumentative microtexts contains only fully argumentative texts of moderate complexity. real-world argument texts do not always consist only of argumentative statements. however, the method could potentially be used on other argumentation annotation corpora as well; one of the main reasons for choosing the corpus was to have a parallel full version in a second language. another reason is the ability to match the edu and adu segmentations directly. (2) although the amount of training data is artificially doubled, it may not be enough to train models on the proportionally increased noise. we hope to investigate these directions in the future."
584,"limitations our constructed dataset, fine, has limitations in terms of entity category balance. some categories have a lower number of online passages and less user attention, resulting in an unbalanced distribution of entities across categories. we aimed to simulate a real-world situation by sampling passages based on their click rates. but this may have contributed to the imbalance. additionally, our proposed method, softfine, is specifically designed for fine-grained chinese named entity recognition with hierarchical categories, and thus has its own limitations. the model is kept simple in structure, with most efforts focused on developing supervision methods, which result in more hyperparameters and require a grid search to find the optimal hyperparameters. this can be resource-intensive. however, it has fast inference speed that is comparable to the bert baseline in real-world applications. to address these limitations, future research could explore methods to automatically balance the loss ratio and dynamically score relevance between flattened hierarchical labels."
585,"limitations of current synonym-based textual attack models, and stress the importance of context (both textual as well as multi-modal) to generate semantically coherent and grammatically fluent adversarial attacks, which are likely remain undetected. while the observed effects of visually-grounded interpretations in our human evaluation were relatively small, we do believe that it is an important future direction. for example, we expect improved results by using synonym substitution methods based on visuallygrounded word embeddings."
586,"limitations as we mainly focus on conceptual knowledge captured in so-called tbox (terminological) axioms, the abox (assertional) axioms are not considered. abox axioms can capture situations for specific individuals (e.g., health status of a person) which could cause privacy issue and we would not expect lms to capture such knowledge. hence, dealing with abox axioms could require additional engineering for data preprocessing. ethical considerations in this work, we construct new datasets for the proposed subsumption inference (si) task from publicly available ontologies: schema.org, doid, foodon, and go, with their download links specified in section 4. the bimnli dataset is constructed from the existing open-source mnli dataset. we have confirmed that there is no privacy or license issue in all these datasets."
587,"limitations this work is mainly dedicated to the curation of a new multilingual dataset for indic languages, many of which are low-resource languages. during data collection, we face several limitations that can potentially result in ethical concerns. some of the important ones are mentioned below: • our dataset contains only those articles written by dailyhunt’s partner publishers. this has the potential to result in a bias towards a particular narrative or ideology that can affect the representativeness and diversity of the dataset. • another limitation is the languages represented in vārta. out of 22 languages with official status in india, our dataset has only 13. there are 122 major languages spoken by at least 10,000 people and 159 other languages which are extremely low-resourced.14 none of these languages are represented in our dataset. • we do not perform any kind of debiasing on vārta. this means that societal and cultural biases may exist in the dataset, which can adversely affect the fairness and inclusivity of the models trained on it. 14https://en.wikipedia.org/wiki/ languages_of_india"
588,"limitations the annotation of attribute categorization and subjective preferences may vary from person to person, influencing preference disambiguation results in the real world. we have tried to reduce bias by choosing categorization concepts and subjective preferences agreed upon by more than three annotators. besides, owing to time and funds constraints, we only manually paraphrase dialog flow in english. for this reason, the agent built on sure can just communicate in english. to overcome this limitation, we plan to annotate sure in multilanguage in the next stage."
589,"limitations while our work covers a large number of languages, it is focused on a specific source and style of summaries. our experiments focus exclusively on the xlsum dataset (hasan et al., 2021) which is based on bbc articles where the opening sentence serves as a summary. it would be interesting to explore our methods on additional datasets and text generation tasks, e.g., where the summaries are longer, or there are multiple input documents."
590,"limitations we acknowledge the following limitations of our work. limitations of newsdialogues. first, we only collect 1k human-to-human conversations with 14.6k utterances due to the high cost of the annotation process (section 4.2). this brings difficulties for the learning of news grounded dialogue generation. second, each conversation in newsdialogues is grounded on one news article, which may have limited knowledge for real-world applications. we leave the multi-article grounded setting for future work. third, as mentioned in section 4.1, the image information in the news article is neglected in this version, which requires further exploration. limitations of experiments. large language models (llm) have shown great few-shot learning ability and generation capacity on various tasks, e.g., gpt-3 (brown et al., 2020), opt-175b (zhang et al., 2022) and bloom-176b (scao et al., 2022) etc. it is important to investigate the performance of llm on newsdialogues, while this has been neglected in this work due to the limited computational resources. in addition, it is also valuable to investigate the performance of chatgpt4 on newsdialogues, and we leave this for our future work."
591,"limitations although our tart-full model shows the effectiveness of instruction-tuning for retrieval, on some datasets tart-dual shows large performance degradation from its non-instruction-following counterpart. we hypothesize that a smaller model size (i.e., 110 million parameters) and limited interactions between query and document embeddings are the main factors. we conduct primarily experiments training larger dual-encoder models such as sgpt (muennighoff, 2022) on berri but still observe some notable performance drop on some datasets, which indicate only scaling up encoders may not significantly improve instructionfollowing retrieval systems. future work can study the better approach to train larger-scale dualencoder models as well as explore modeling architectures that enable rich interactions but are still more efficient than the cross-encoder, such as colbert-v2 (santhanam et al., 2022). retrieval tasks are excluded in prior work on instruction-following of llms. this work is the first to explore instruction tuning in the area of retrieval, and we annotate more than 100 instructions for approximately 40 tasks, and we demonstrate the effectiveness of the dataset scale in retrieval. yet, recent work (wang et al., 2022b; chung et al., 2022) show that scaling up the number of the training datasets improves llms’ ability to adapt to new task via instructions, and the current dataset scale might not be optimal. we open-source our instruction data and call for community efforts to collect more retrieval tasks and human-written instructions as in instruction-following for lms (wang et al., 2022b; bach et al., 2022), to investigate whether further increasing the number of the datasets lead to improvements. ethical considerations although instruction-tuning using many datasets enable better zero-shot transfer, tart does not always retrieve documents that perfectly align with users’ expectations. applying tart to safetycritical domains requires extra attention. berri includes approximately 40 tasks covering diverse domains. although the data has been automatically filtered, and we have examined the data, there may still be harmful or privacy-sensitive contents. we will release all of the data and preprocessing scripts for follow-up work to inspect those dataset issues and the effects of those data."
592,"limitations those of our findings that are based on information obtained from authors are necessarily limited in that they do not reflect information that might have been obtained from authors who did not respond. moreover, we selected our initial set of papers via search with key phrase “human evaluation.” while this phrase is very commonly used to refer to non-automatic forms of evaluation, there is a chance that we may have missed papers because they used a different term."
593,"limitations our work is predicated on hypothetical models of human cognition. these models are still under development by cognitive scientists and need to be validated in more realistic domains. our method assumes access to a simulation of the environment, which may be costly to construct in some domains. in general, instruction generation agents pose substantial risk to humans. previous studies have shown that humans can become overly reliant on ai instructions and commit disastrous mistakes (robinette et al., 2016). it is thus important for practitioners to comprehend the constraints of our experimental setting. our experiments take place in a coarse simulator of real-world indoor environments, which restricts the action and perception of the human listeners. due to the expensive cost and the large number of agent variants, our human evaluation remains limited in terms of population scale and diversity, and the comprehensiveness of the questionnaires. as each instruction is only evaluated by a single human, we have not investigated the variance of the interpretation of the same instruction among different humans. in addition, human evaluators may “guess” a path even if a part of the instruction is misleading or impossible to follow. hence, the path-similarity metrics may not reflect faithfully the quality of the instructions. nevertheless, results shown in table 4 of §a.5 indicates that instructions generated by our agents are almost as easy to interpret as those generated by humans. but again, these results are still subject to the constraints of our annotator population. to deploy our method, practitioners should carefully re-evaluate its safety and effectiveness in conditions that closely emulate the deployment conditions."
594,"limitations we discuss the limitations of our work: • while the three-step masking is shown to be beneficial, masking (base+ott) followed by unmasking may introduce several redundant computations as a token masked in the first two steps might get unmasked in step 3. • when compared against the docogen baseline (calderon et al., 2022), iterative masking steps increase the time complexity of the domain obfuscation which leads to masking latency. moreover, as the domain classifier is a critical part of the domain obfuscation, the approach has extra memory footprints. • the proposed masking approach introduces two extra hyperparameters τ2 and τ3 on top of the hyperparameters introduced by docogen. while we identify them as a fixed scalar value working for all kinds of input, we posit that one can propose dynamic input or source domain adaptive thresholding. currently, we classify it as a limitation of the proposed work."
595,"limitations in this paper. in this section, we talk about the prominent advantage and significant limitations of our method."
596,"limitations in this work, we study the approach of using machine-translated text to train language models on a single target language (and language pair). our"
597,"limitations in this paper, we propose the evaluation model umse which can be used to evaluate the summary quality in three typical scenarios. however, in the summarization task, different annotators have different writing styles, and there might exist more than one good summary for one document. moreover, there can be summaries that concentrate on different aspects of a document (e.g., describing the location and room of a hotel). in the future, we aim to incorporate more scenarios (e.g., multireferences and multi-aspects) into our unified evaluation method."
598,"limitations the limitations of our approach exist mainly in two aspects. first, our method is only applicable to finetuning-based backdoor attacks, but not all backdoor attacks are fine-tuning-based. second, although our method can eliminate backdoors well, the computational cost of our method is much higher than that of standard fine-tuning, and needs to be improved in the future."
599,"limitation, currently, we mainly evaluate mocl under the single-source cross-domain setting. we plan to further extend it to multi-source cross-domain settings. moreover, the interaction between named entity recognition and relation extraction can be considered to improve performance in the future. limitations we propose a sequence-level contrastive learningbased model-agnostic framework mocl to enhance entity type classification in cross-domain named entity recognition (ner). in the future, we would like to combine the different granularities of contrastive learning (i.e., token-level and sequencelevel) to learn generalized representation for further improving the capability of mocl. in addition, due to the hierarchical structure of entity types between the source domain and the target domain, it would also be beneficial to adopt non-euclidean space to represent words for better learning the relative hierarchical relationship between entities."
600,"limitations our proposed work is dedicated to considering the noise in ds-ner, and our noise-specific analyses are all based on this task. therefore, if it were not for ds-ner task, our model would not necessarily be robust compared to other task-specific methods. also, our approach is based entirely on previous experimental settings in ds-ner, so we do not consider how to reduce noise from the distant supervision process, e.g., designing models to help the annotation process rather than learning to reduce noise from the distantly-supervised text. designing models to help the distant supervision process could be a direction for future study."
601,"limitations we consider as profanities words that have highly offensive or vulgar connotations. we acknowledge that readers may have different sensibilities with respect to profanities. obscene words depend on different factors, such as culture, social or religious background, and more (hovy and yang, 2021). consequently, some words may be disturbing for a number of people, and should be obfuscated, while other readers may not have any issue with reading them. moreover, we should consider that there is typically a hierarchy of offense, whereby some words are more severe than others; for example, f*ck is often socially accepted while the n-word usually is not (sap et al., 2019)."
602,"limitations we only experimented with one and three teacher models. training more teacher models and using them to predict on large datasets such as solid (rosenthal et al., 2021) would require more computing resources. furthermore, we did not train the teacher models on the augmented dataset for the same reason following recent research in kd (gajbhiye et al., 2021; sun et al., 2019). we only conducted the experiments in english. the non-availability of large-scale offensive language identification datasets such as solid (rosenthal et al., 2021) in languages other than english can be a challenge when expanding this kd research beyond english."
603,"limitations although we sidestep the challenge of selecting a specific prompt template for experimentation by opting for widely-used templates from previously published works, it is worth noting that numerous effective prompt templates are available, and the experimental results obtained using these templates would also provide valuable insights into testing our proposed method. furthermore, while our method yields improvements, it is important to acknowledge that errors may exist in the rule-based automatic annotation of generic responses, which could potentially propagate to the learning of the diagonal parameter w ."
604,"limitations limitation of model scale the benchmark only included the evaluation of moderate-size language models and did not experiment on large language models. we justify our reasons in section 4.6 and appendix e and include an evaluation of chatgpt in appendix e, showing that even human feedback fine-tuned large language models is far from perfect on xws-tc. however, we acknowledge that the current state of extremely weak supervision would be better understood and assessed if complete evaluations on state-of-the-art large language models, such as instruct-gpt (ouyang et al., 2022), palm (chowdhery et al., 2022), and chatgpt exist. while we lack the computational resources to perform such an evaluation, we hope this work can stimulate interest in xws-tc and complete the study. limitation of text classification another limitation is the scope of text classification. while prompt and seed methods have shown strong performances on text classification, this performance does not extend to other general classification tasks, such as natural language inference/entailment (zhao et al., 2022)."
605,"limitations although the ability of in-context learning has been found for different architectures (e.g., transformer and lstm), we consider only transformer-based in-context learning in this paper because transformer is the current mainstream architecture of nlp. however, as for in-context learning itself, figuring out how it works for other architectures is also a meaningful problem, which we encourage to study in the future. as for the dual form we point out between transformer attention and gradient descent, we consider a relaxed form of linear attention for qualitative analysis. although the experimental results support our understanding well, the mechanism of standard transformer attention without approximation may be more complex and should be studied more clearly in the future. as for empirical experiments, our analysis needs to record a large number of intermediate results (e.g., attention output representations, and attention weights to query tokens and demonstration tokens) for thousands of validation examples. considering the storage space and computational cost of analysis, we only analyze gpt models with up to 2.7b parameters and leave larger models such as gpt 13b for future work. in addition, for the clarity of the problem definition and the convenience of experiments, our analysis is based on only classification tasks. although classification is a representative application of in-context learning, other tasks like multiple choice and open-ended generation are not considered in this paper and could be investigated in the future."
606,"limitations increase in the number of dialogue turns the dsd dataset has a higher average number of turns compared to the sgd dataset. (20.44→21.21) this is a limitation in terms of completing a task with fewer dialogue turns, one of the objectives of the tod system (liu et al., 2018; tiwari et al., 2021). this is because dsd was created by extracting and augmenting the target turns of sgd. however, assuming that the tod agent trained with the dsd is applied to real-world scenarios, we expect that the agent will play a role in reducing the number of user rejections by expanding the range of choices to users through compare-based disambiguation."
607,"limitations on training efficiency and inference efficiency. first, since metaretriever will first retrieve taskspecific knowledge and then make predictions in the inference phase, such a retrieve-then-extract manner will take longer inference time than nonretrieve methods unavoidably. we conduct experiments on the test set of conll04 dataset to compare overall inference time of metaretriever with uie. all hyper-parameters are set to be the same for a fair comparison. experimental results are shown in table 9 and we can find that metaretriever cost nearly twice time as uie to make predictions. as metaretriever works in a retrieve-than-extract manner, such a time cost is reasonable. second, our proposed meta-pretraining algorithm is based on bi-level optimization. in the pretraining phase, it needs to calculate high-order gradients to optimize parameters and calculating high-order gradient requires more time. therefore, it takes longer time to pretrain metaretriever. to illustrate the time cost, we conduct experiments on 10k instances to compare the pretraining time of metaretriever with simpleretriever which is pretrained without meta-pretraining algorithm. all hyper-parameters are set to be the same for a fair comparison. table 10 gives the results. from it, we can obverse that compared with simpleretriever, metaretriever takes about 1/4 longer time than simpleretriever. finally, we spent 56 hours to pretrain metaretriever on filtered pretraining corpus (6.9m instances). acl 2023 responsible nlp checklist"
608,"limitations our work has several potential limitations. first, given the limited computational budget, we only validate our self-evolution learning on the large and base sizes. it will make our work more convincing if scaling the experiments up to the larger model size and training corpus. on the other hand, besides the improved commonsense knowledge learning ability, we believe that there are still other abilities, e.g., mathematical word problems, of plms that can be improved by our method, which are not fully explored in this work."
609,"limitations and risks to avoid any misuse of queryform. although our proposed queryweb pre-training approach can effectively achieve knowledge transfer from publicly available webpages to form-like documents, it inevitably carries the bias and fairness problems (mehrabi et al., 2021) to the downstream task. therefore, in real-world applications, we should have more strict rules to filter and clean up the webpages, and thoroughly check the bias and fairness issues of the pre-trained model. limitations in addition to the bias and fairness concerns that we discussed in the ethical and broader impact section, we discuss the possible limitations of our method in this section. as a query-based dee framework, queryform may be prone to specific prompting based adversarial attacks (xu et al., 2022), which may further pose potential security concerns for safety-critical documents. thus, it is important to test the robustness of queryform against adversarial attacks and design defense schemes to further strengthen our method in the future. our work focuses on the closed-world setting that source documents include entities contained in the target documents, following (xu et al., 2021), without further investigating the possible openworld (shu et al., 2018) setting with unseen test entities. however, as a query-based framework that makes conditional prediction with no pre-defined set of entities, queryform actually supports the prediction of unseen entities at test time and we would like to leave it as an interesting future research direction."
610,"limitations this section does not count toward the page limit. as illustrated in § 5.2 and shown in table 3, the coherence of the rewrite generated by edircs is not as good as that generated by purely autoregressive rewriters (e.g, t5qr). this may affect the performance of edircs when using dense retrievers. possible solutions include using an additional token reordering model (chowdhury et al., 2021) to improve the rewrite coherence or injecting the coherence signals (hao et al., 2021) or token positions information (mallinson et al., 2022) into the learning of edircs in an end-to-end way. another concern is that the effect of our text editing-based model may be limited for a few long-tail cases where many expected rewrite tokens are not in the input session. how to better deal with the search dialogues whose search intents are too implicit or vague to be accurately expressed by inferring from the dialogue context alone is a valuable direction for further improvements of our model."
611,"limitations in this section, we discuss some of the known limitations of our set-up, data and models. to handle unknown words in the test sets, we replace them by a special unk token which is also used to mask some tokens in the training set. the unk token provides little information regarding the actual input and tapir might be unable to fully utilise the token to refine its interpretation of the past output. this has a direct influence in the incremental metrics, as the model can exploit this property by using unk token as a cue to emit the revise action. this strategy also introduces the extra hyperparameter of what proportion of tokens to mask. we put effort into achieving a diverse selection of datasets in various tasks, but our analysis is limited to english. we are reporting results on the datasets for which the non-incremental versions of the model could achieve a performance high enough to allow a meaningful evaluation of their incremental performance. tuning is still required to extend the analysis to other datasets. related to these two issues, we decided to use tokens as the incremental unit for processing. we follow the tokenization given by the sequence labelling datasets we use. extending the analysis for other languages requires thus a good tokenizer, and annotated data, which may not exist. we may also inherit limitations from the datasets that we use. although we do not include an in-depth analysis of the datasets, as our focus is on the model and not on solving the tasks themselves, they are widely used by the community and details are available in their corresponding publications. the method we propose to retrieve the action sequences depends on the chosen model, and the grounding of the action sequences in the actual prefix outputs have a direct influence in training the controller. therefore, the decisions made by tapir rely on the quality of the underlying generated action sequences. in order to ensure that the internal representations of the action generator lt do not depend on right context, we had to restrict ourselves to a single layer variation of this model when generating the sequence of actions. it is possible that with more layers its behaviour would be different, but that would invalidate the assumptions needed for an incremental processor. when it comes to the tapir architecture, the attention scores for the controller are computed independently of temporal order and we do not explicitly model relation between cache elements. the limited cache size also means that some past information has to be discarded to accommodate incoming inputs. although we have made efforts to incorporate them through the summary vector, this might be not ideal due to information bottleneck."
612,"limitations of our work are inference speed and decoding strategy. the efficiency of semiautoregressive inference is lower than that of nonautoregressive algorithms, so currently it cannot be applied to scenarios with high frequency requests. furthermore, this paper only introduces greedy search as a decoding strategy. overcoming the challenge of introducing many complex decoding strategies such as beam/tree search (liu et al., 2020; ma et al., 2021) belongs to our future work."
613,"limitations the current amr-tst is based on the style rewriting algorithm to rewrite the stylistic nodes of amr graphs from source style to target style. however, this method relies on style opposites features contained in the general natural language corpus. the advantage of such a method is that it does not need complex decoder retraining processes for different datasets, which maximizes the use of generic natural language knowledge and reduces training costs. however, this also leads to a limitation that the current amr-tst is applicable to text style transfer tasks with significant style polarity, such as sentiment features. for other text style transfer tasks like political and gender transfer, our current style rewriting algorithm cannot precisely rewrite the implicit style words in these tasks. to address this limitation, our future work will improve the style rewriting algorithm by finely identifying implicit style words and exploring their correlations, enabling the revised algorithm can be embedded in the current amr-tst framework that focuses on the node-level stylistic features. ethical statement this paper honors the ethical code set out in the acl code of"
614,"limitations we discuss some limitations of this work for future research efforts. the range of the domains could be more comprehensive to cover social media and law. the experiments can potentially cover more models. as we mention in sec. 8, there are more comparable retrievers and qa readers. it would be useful in the future to benchmark more models on robustqa. finally, due to the complexity of the raw ir data, it is costly to collect our datasets. this is manifested by not only the monetary costs, but also the human efforts to create guidelines, to coach annotators, and to manually audit and validate annotations. in the future, it could be beneficial to leverage large language models with context learning to assist human labors."
615,"limitations in this work, we propose a novel method for eae that introduces learnable soft prompts to capture specific-example context and relevant documents for prompt customization and enrichment. although experiment results have demonstrated the benefits of the proposed model, there are several limitations that can be addressed for further improvement in future work. first, similar to previous eae studies (du and cardie, 2020; li et al., 2021; ma et al., 2022), our eae model assumes golden event triggers for event types that might not be available for real-world applications. as such, future work can develop more comprehensive research and models to accommodate predicted event triggers while still maintaining competitive performance for eae. second, to aggregate relevant document representations for soft prompt computation, our eae method leverage an event type mentioning graph that capture documents, event types, and their occurrence in training data. on the one hand, the graph does not involve argument roles that are directly related to eae and might provide richer information/context to obtain representation aggregation to augment soft prompts. on the other hand, our method only explores graph attention networks to perform representation aggregation while many other variants of graph neural networks have not been considered, e.g., deep graph convolutional networks (chen et al., 2020). future work can explore richer graphs and graph neural networks to learn better representations for soft prompts for eae. third, despite the introduction of soft prompts with important benefits, our method still needs to rely on discrete prompts to explicitly specify event types and argument roles. although our experiments demonstrate better stability of the proposed method with different discrete prompt variants, adapting our method to new languages will still require some prompt development effort to achieve optimal performance. finally, in contrast to the interpretability of discrete prompts, soft prompts are less explainable, which can be addressed in future work to make the proposed method more accessible to various users."
616,"limitations of simulated al settings, we propose guidelines to improve trustworthiness and robustness in al research. transparency our first recommendation is a call for transparency, which essentially means to report everything (dodge et al., 2019). every detail of the experimental setup, the implementation and the results, would be extremely helpful to properly evaluate the soundness of the experiments. we urge al researchers to make use of the appendix (or other means such as more detailed technical reports) to communicate interesting (or not) findings and problems, so that all details (§3) are accessible. thorough experimental settings we aim to incentivize researchers to thoughtfully consider ethical and practical aspects in their experimental settings. it is crucial to compare a wide range of algorithms, striving for generalizable results and findings across datasets, tasks, and domains. moreover, we endorse research endeavors that aim to simulate more realistic settings for al, such as exploration of al across multiple domains (longpre et al., 2022; snijders et al., 2023). additionally, we advocate for investigations into active learning techniques for languages beyond english, as the prevailing body of research predominantly focuses on english datasets (bender, 2011). evaluation protocol we strongly encourage researchers to prioritize the establishment of fair comparisons among different methods and to provide extensive presentation of results, including the consideration of variance across random seeds, in order to ensure robustness and reliability of findings. generally, we argue that there is room for improvement of the active learning evaluation framework and we should explore approaches from other fields that promote more rigorous experimental and evaluation frameworks (artetxe et al., 2020). analysis we place additional emphasis on the requirement of conducting comprehensive analysis of al results. it is imperative to delve into the nuances of how different al algorithms diverge and the extent of similarity (or dissimilarity) among the actively acquired datasets. it is incumbent upon al research papers to extend beyond the results section and include an extensive analysis component, which provides deeper insights and understanding, as in ein-dor et al. (2020); yuan et al. (2020); margatina et al. (2021); zhou et al. (2021); snijders et al. (2023), among others. if we aim to unveil why an al algorithm fails to outperform another (or the random baseline), we need to understand which data it selected in the first place, and why. reproducibility reproducing al experiments can be challenging due to the complex nature of a typical al experiment, involving multiple rounds of model training and evaluation, which can be computationally demanding. however, we strongly advocate for practitioners and researchers to prioritize the release of their code and provide comprehensive instructions for future researchers aiming to build upon their work. by making code and associated resources available, the research community can foster transparency, facilitate replication, and enable further advancements in al methodologies. efficiency finally, we propose the release of actively acquired datasets generated by different al algorithms, which would greatly contribute to datacentric research and interpretability aspects of al. particularly when employing al with large-scale models, it becomes crucial to establish the actively acquired data from other studies as baselines, rather than re-running the entire process from the beginning. such an approach would not only enhance transparency, but also promote efficiency and ecofriendly practices within the research community."
617,"limitations within the al research community, with the intention of illuminating obscure experimental design choices. furthermore, we delve into a thorough exploration of the limitations associated with simulation in al, engaging in a critical"
618,"limitations in this paper, we focus on masked language models, which have been shown very effective and are widely used. one limitation of the present study is not investigating another representative category of language models, the generative pre-trained models (e.g., gpt2/3 ( radford et al. (2019); brown et al. (2020))), we leave it for future work."
619,"limitation regarding positive predictive power, there is always a risk with research on social biases that it can give practitioners a false sense of security. it is absolutely possible to evaluate on our corpus and get no bias, and still end up causing harm to racial or gender demographics, since they do not cover all biases or all domains. this should be kept in mind whenever applying this research."
620,"limitations the deep neural networks in rho uses feature extraction and vectorization to represent the texts. the model only detects the statistical regularities and quantitative relationships among the variables but can not see qualitative relationships, such as causality, hierarchy, and other abstractions (tsimenidis, 2020). although we leverage the response re-ranking technique, which improves the explainability of rho, the neural networks are undoubtedly still “black boxes” to humans. therefore, the faithfulness of generated responses can not be fully guaranteed. ethical considerations in addition to the hallucination problem, another critical challenge, the offensive language, is also introduced with the evolutionary progress toward building reliable dialogue systems. the data-driven models are susceptible to delivering offensive responses while mimicking human conversations (xu et al., 2020b). it has been shown that racial and gender biases are encoded in the plms (blodgett et al., 2020), and these biases are present in the training corpus. since rho leverages plms and the training corpus, it is possible to generate offensive languages. we suggest that in real-world dialogue systems, it is necessary to employ some postprocessing steps to alleviate this problem when it is deployed online."
621,"limitations there are mainly two limitations in this study. first, we still do not consider components other than the bias parameters in the prediction head. for example, the weight parameters of the prediction head, i.e., γ and w fc, can also affect a model’s prediction. second, our findings do not cover the transformer language models other than bert (base and large) and gpt-2 (small, medium, large, and xl). consistent findings were obtained for the two main architectures (i.e., encoder-based masked, and decoder-based causal language models) and for various model sizes, although future research is needed to show whether the findings can be generalized to roberta (liu et al., 2019), open pre-trained transformer language models (opt, zhang et al., 2022), and other variants. considering transformer encoder-decoder models, such as neural machine translation models and t5 (raffel et al., 2020), would also be an interesting future direction."
622,"limitations we see two main limitations in our study. primarily, given the clear trend of larger generative models producing higher quality answers, an obvious question is to investigate whether this continues to be the case indefinitely, or whether it saturates after a critical amount of parameters. despite this, due to hardware restrictions, we were unable to experiment with models larger than bart-large. additionally, considering that the field of ambiguous qa inherently requires complementary pieces of evidence, there is no doubt that diversification methods are bound to yield better results in terms of disambiguation quality. in this work, however, we limited ourselves to using a typical neural retriever, shifting our focus toward the factuality and the fluency of the generated answers."
623,"limitations of this approach might be its high computational cost to explore with ‘gpt3-scale’ lan- guage models (brown et al., 2020), and we expect that this can be addressed through offline reinforcement learning (fujimoto et al., 2019) techniques in future research. limitations large language models over the gpt-3 have made significant progress in natural language generation, but applying the criticcontrol method, and exploring through these large language models are computationally too expensive. to address this, offline reinforcement learning (fujimoto et al., 2019) may be a promising option to minimize training costs. criticcontrol also has inference speed degradation because additional inference costs are needed like other controlled text generation methods (dathathri et al., 2019; yang and klein, 2021). the potential solution may be to use the action-value predicting critic (yue et al., 2020), which would allow for real-time control of various attributes without affecting the inference speed of the language model. recently, the impact of instruction models (chung et al., 2022; ouyang et al., 2022) on text generation has recently been highlighted in academic research. these models, which allow for control over the generated text via input manipulation, have become widely accessible on various attributes without extra computational costs. future works will investigate the synergistic potential between the ‘inputside’ control of instruction-based models and the ‘output-side’ control of criticcontrol. ethical statement we acknowledge that our reward-driven text generation system may lead to generating harmful or misleading content when used with undesired reward models. however, controlled text generation methods have the potential to address these ethical issues present in large-scale pretrained language models, for example, through the detoxification of language. therefore, we emphasize the proper use of reward models to pursue the public good and believe that it is important to continue research in this area as these techniques can offer significant benefits."
624,"limitations while this dataset is unique and pioneering, its size is limited, and it involves specific patients. to enhance the generalizability of the findings, a larger dataset may be required. similarly, although our framework is innovative, we anticipate the development of more comprehensive and informative annotation protocols in the future. for instance, we observed a higher frequency of the ""intervention in- formation"" category within cognitive engagement, likely because the intervention predominantly follows a q (nurse) & a (patient) format. we hope that the coding scheme established in this study can aid future research in refining this category with finer granularity, based on specific intervention theories. ethical considerations we used the nlm scrubber offered by nih to produce hipaa-compliant deidentified health information for scientific use, including dates, and places. two independent annotators evaluated the nlm-scrubber on the dataset to make sure no events or other people in patients’ posts can allow patients to be traceable. the de-identified version of our data will be shared with researchers upon request who have completed an ethical review from their institution and a data request application form from us. since the domain of our dataset is specific, the models trained on our dataset may exhibit subtle biases on out-of-domain data. further, pre-trained models that we use in our work have been shown to exhibit biases (li et al., 2021). we hope future researchers could use these models with caution regarding the biases that these pre-trained models have. the long-term goal of our work is to aid healthcare providers to quickly identify poorly engaged patients to allocate their energy and resources to provide in-time support. models trained on our data should not be deployed in the real world without human supervision because, despite the potential of transformer models, they cannot be relied on completely in sensitive medical scenarios."
625,"limitations it is highly desirable to test our model on more datasets. however, there are very few multi-class, publicly available datasets that include information about annotator assignments. often this information is, unfortunately, either discarded or withheld. without annotator assignments, it is difficult to run experiments related to label distribution learning driven by annotator-item modeling. we hope that this paper encourages more researchers to collect and share more datasets that retain information about annotator-item matchings. datasets: we understand that the disagreement between the annotators could arise due to the subjectivity/ambiguity of the content to be annotated, nature of the study, or even worker reliability (aroyo and welty, 2013; inel et al., 2014). these observations cannot be solely utilized to disregard a dataset, since it is not a limitation of the dataset but the nature of the problem domain of annotator disagreement. ethical considerations all statistical methods are double-edged swords. used maliciously, these methods could be used to misrepresent social values and opinions. moreover, while these methods would be more informative with demographic information on the annotators, this conflicts with the privacy of the annotators, a group of workers who are often treated unfairly (gray and suri, 2019)."
626,"limitations first, most articles are crawled from the us and uk presses. this means the crawled data is englishonly and regionally biased, limiting the scope and the diversity of issues. extending our work to other languages and more regionally-diverse presses will be helpful for reducing such bias in our dataset. second, we suspect that there will be a nontrivial annotation bias in our dataset. we are concerned with the fact that all of our in-house annotators share the same cultural background and similar personal interest (given that the annotators volunteered to partcipate in this turking task). furthermore, given that claimdiff-w is aiming to catch the subtle differences in the nuances of these professional news articles, it is very challenging for different annotators to have a common view, especially compared to claimdiff-s (which also explains why claimdiff-w human performance is much lower than that of claimdiff-s). third, since claimdiff is a sentence-level comparison task, it currently does not give information about the surrounding context of each sentence. this means inter-sentence dependency such as coreference often cannot be resolved. one way to work around this is to give an access to the full articles for each claim pair, but we have refrained from it in this work for simplicity (though we believe it will be interesting to see if the performance can be improved with such access). fourth, the size of claimdiff is relatively small compared to other fact verficiation datasets. this is mainly because its annotation process is quite challenging and requires a substantial amount of time. future work includes expanding the size of claimdiff when additional budget is available."
627,"limitations in this section, we will summarize several limitations of our work. the first one is we only apply the proposed method to the natural language understanding (nlu) tasks. it is uncertain how to extend our approach to the natural language generation (nlg) tasks and whether it can bring considerable improvement. then, the training process of gan is sensitive to hyper-parameters, leading to us not simply using the default setting when extending to other tasks. finally, this paper does not include a theoretical explanation and proof of how our method works, which we will further study in future work."
628,"limitations we propose a set prediction network for the extractive summarization task, which has worked well on some datasets but still has some limitations. firstly, due to the use of pre-train bert in the document encoder, our method is inadequate for long text summarization tasks. in general, the text length of a long document is much longer, so the model needs to be more capable to capture the dependency. next, we will extend the method to long document summarization tasks. secondly, the queries in the decoder are initialized with a normal distribution. if we can initialize the queries with the prior knowledge, our method may be able to find the set of sentences of the summary more accurately, which is another direction we need to focus on in the future."
629,"limitations ground truth and data cleaning although we conduct basic cleaning by selecting the ground truth place object that has appeared the most often for a given user, this is only a heuristic and does not guarantee that the selected ground truth matches the description in the user location string, which introduces noise in the twitter-pug dataset. future work is needed to develop more accurate methods that identify the ground truth from a set of geotagged user tweets. also, the current ground truth format does not account for alternative names in geolocation. a future direction is training the seq2seq model to generate multiple formal location names from a single user location string. alternative names in gazetteers such as geonames could be used as a source of this ground truth. in figure 4, we identified several types of noise in twitter user profile locations. we did not conduct extensive data cleaning of fictional, joke, or non-existent locations. though we attempted to filter these places automatically, we found little change in model performance. a more detailed study of the effects of data cleaning would be beneficial. model size due to resource constraints, we only experiment with the mt5-small model. in a smallscale preliminary study, we found mt5 outperforms byt5 (xue et al., 2022) on our task of geolocation name transduction. it would be interesting to also test how larger (e.g. mt5-large) or other types of pretrained language models (e.g. fully autoregressive models) performs on this task. also, how much data is actually needed to train the model. coverage v.s. accuracy trade-off another limitation of the geo-seq2seq approach is that the model always produces a candidate location even when the input only contains a fictional location or does not contain a location at all. a potential solution for this is thresholding the model based on a log-probability threshold, and only producing a candidate location when the probability of a beam is high enough. such thresholding method could serve to trade off coverage and accuracy. a related issue is the accuracy at each granularity (i.e., country, admin, and city). the model performs significantly better at lower granularity, specifically at the country level (see table 2). this is important for end-users to acknowledge if this tool is used for higher-stakes analysis such as natu- ral disaster relief, versus such as studying vaccine opinions in different parts of the world. performance across demographics finally, as shown in section 8, our model has a wide range of performance with respect to f1 across countries, and a smaller discrepancy of accuracy across language. the strong multilingual performance is most likely from the original mt5 pre-training. however, there is still room for improvement. to address the discrepancy in performance across countries, a strategy is to stratify the data by country, similar to how multilingual pre-trained encoders are trained with exponential sampling based on language balance (xue et al., 2021). ethical considerations the main ethical consideration for a tool like geoseq2seq is privacy. we respect user privacy in the creation of geo-seq2seq as well as in collecting the data to build twitter-pug by only using immediately available data provided by users. as discussed in section 3, the training data is built from user profile location strings paired with a user’s most frequently tagged twitter place. once trained, geo-seq2seq only needs the user profile location to run inference. also, due to the structured nature of the output string and easy integration with carmen, researchers can easily choose at which granularity to aggregate their data, whether the city, admin (state/province), or country level. further, the use case of our model is only meant to support researchers studying location-specific demographics. the content will be studied in aggregate, as according to twitter policy."
630,"limitations one of the limitations of our study is that the performance of knn search is highly dependent on the domain of the datastore used. as shown in section 6.4, knn search, like standard lm, does not work well for contexts and numerals for out-ofdomain data. this dependence can be reduced by increasing the size of the datastore and introducing passages from various domains; however, this strategy may bolster another limitation, as discussed hereafter. the second limitation is that knn-lm requires more memory usage for the datastore and higher latency for search during inference compared with standard lms. although the search process itself can be executed swiftly by leveraging efficient similarity search libraries like faiss (johnson et al., 2017), as the size of the datastore expands, the time required to obtain their representation vectors is expected to increase. the third limitation pertains to the lack of language variety in the utilized datasets. while we deliberately selected datasets from different domains for our experiments, they shared a common language, namely english. consequently, it is expected that knn-lm will exhibit similar effectiveness in languages with linguistic structures similar to english. however, conducting experiments on non-english datasets is necessary to provide evidence for the language-independent impact of knn-lm. this aspect will be addressed in future research endeavors."
631,"limitations although honestbait shows promising results for generating attractive but faithful headlines, there are still some limitations: (1) honestbait is a monolingual model that only supports chinese. it requires three pre-trained scorers. also, as the fr labels are specifically difficult to obtain, it is not easy to implement in other languages. (2) running the whole framework with a batch size of 16 takes around 22 gb gpu memory, mostly because we must load all pre-trained models into the gpu. this can be alleviated by using a distilled pre-trained model. (3) on average, honestbait generates more faithful headlines than other baselines, but it still occasionally produces false information or unwanted results. this work is only for academic purposes and is not ready for production."
632,"limitations in this paper, we simplify intention identification into a sentence classification task, i.e., exploiting a specific procedural event in an event process to predict the intention of the whole event process. a more realistic way to model this task is to enter the entire event process rather than a single event. we will go into more detail about this type of task in future work."
633,"limitations our approach is proposed based on the intuition that false negative samples should have high similarities with the positive samples that have the same gold entity type, and they also have low similarities with the positive samples of different entity types. however, our proposed approach does not guarantee the selected negatives are true negatives. furthermore, when the negative samples are hard false negative samples, they are likely to have high similarities with other positive samples as well. however, such hard false negative samples are not prevalent in the datasets. another limitation is that there is still a large performance gap between the distantly supervised datasets and the human-annotated datasets, as mentioned in section 4."
634,"limitations we observe two main legal limitations for this project. first, it has limited practical use for non-lawyers seeking legal help, also called selfrepresented litigants. in fact, any system providing legal citations, both precedent or statutory provision, to an untrained lawyer will be of very little use, and even harmful. it is hard to imagine in what context this might be used by non-lawyers considering that they might not be able to translate facts into a legal problem. that being said, many direct-to-public legal applications have emerged recently, and many of these applications do provide insightful legal information along with the legal sources (morrison, 2019; dahan and liang, 2020). while these applications have raised concerns as to their legality, notably with the issue of unauthorized practice of law, many regulators including in canada, the united states and europe have cautiously supported the development of ai-power technology for the general public. second, several lawyers (especially appellate) have surprisingly expressed concerns regarding “the googlization of legal databases"" (vaidhyanathan, 2011). while they recognize the advantages of intuitive ai non-boolean research, they claim that these algorithms are not superior when it comes to locating a more obscure appellate case law, to help win a case. it has even been argued that boolean logic remains faster and more efficient because it does not lead to missed case. according to this view, while the “googlized"" legal database may quickly locate important caselaw especially if decided by a higher court, it can miss less obvious cases (mart et al., 2019). in our work, this challenge translates to the long-tail problem for legal citations, and our use of embedding distance encourages matching based on semantic similarity. in other words, we only look for the most obvious citations, which correlates to higher performance on easier (procedural) citations and lower performance on harder (non-procedural) citations. our work does not sufficiently address this problem, so we encourage more proficient information retrieval or prototype discovery methods in the future. from a deep learning perspective, the main limitation is due to the use of k-means clustering in our implementation of the system. there were several points of instability noted during the training process, which we theorize is largely due to the initializations of the k-means clustering algo- rithm. when the prototypes are initialized, the corresponding terms in the loss function have a strong influence on the cross-entropy loss, which leads to model collapse. even when the prototypes are initialized properly, the loss function overfits to the prototypes after several updates but does not provide improvement in the classification performance, which is why we choose the best model by validation macro f1 instead of validation loss."
635,"limitations. this paper takes an entirely different approach by zeroing in on a particular task, which has been augmented with a specific semantic evaluation (embodiment ratings of actions), to highlight how difficult tasks, such as figurative language interpretation, benefit not only from model size but from specific embodied semantics. figurative language is difficult for lms because its interpretation is often not conveyed directly by the conventional meaning of its words. human nlu is embodied and grounded by physical interaction with the environment (di paolo et al., 2018). consequently, it could be expected that lms struggle when the interpretation of figurative language depends on a more embodied action. yet, the opposite has been shown as more embodied concepts are more lexicalised and larger lms can interpret them better in figurative language. hence, our study provides valuable insight that raises the question of whether this effect is limited to figurative language or translates to other nlu tasks for lms."
636,"limitations our method has three major limitations. first, the auxiliary data corpus with label information might be rare. recall that the corpus we used in this paper is the training set of different benchmarks. however, large-scale labeled data as the auxiliary data source might be infeasible in practice, hence it may limit the model deployment in real-world scenarios. second, our method is trained and evaluated on english datasets. additional data processing as well as annotation is necessary for other linguistic settings. third, external unlabeled data with the same domain as the aste datasets are needed for the pre-training of the retriever. in our experiment, we choose two external datasets in the restaurant and electronics domains. if our method is applied to other fields, we need to find additional external data in the corresponding domain for pre-training."
637,"limitations a limitation of this work is that it is only evaluated on synthesized datasets of cartoons with limited characters and scenes. in the real world application, there might be many different scenes/characters, posing new challenges to the proposed approach. another limitation is the requirement of supervised training data and resources. despite the number of trainable parameters of our approach (850m) is less than ar-ldm (∼1.5b), the model still needs many story-level training data and computing resources."
638,"limitation as our method does not focus on dealing with unanswerable questions, our method may not show a great advantage over other methods when there are a lot of unanswerable questions. how to improve the recognition of this type of question, avoid overrating further modeling on them, and therefore give more accurate graph modeling on answerable questions will be left to our future work. besides, our speaker modeling prefers questions focusing on speakers, and it may show limited improvement if a dataset contains few speaker-related questions. however, speakers are key roles in dialogues, and therefore, questions about speakers naturally appear frequently in drc. the power of our key utterance extraction method to other qa fields remains unknown. it can be future work to extend it to other reading comprehension tasks like narrativeqa (kociský et al., 2018). our method does not involve additional knowledge, such as speakers’ co-reference and relations (liu et al., 2020), discourse structures of dialogues (li et al., 2021; ma et al., 2021), and decoupled bidirectional information in dialogues (li et al., 2022). these types of knowledge, which are orthogonal to our work, are key components of dialogues. therefore, making full use of the additional knowledge in dialogues with our graph modeling can be an interesting direction to explore."
639,"limitations several limitations of codeexecutor, such as its application to only python, the lack of faithfulness in the results produced, and the maximum length limit for trace generation, point toward interesting directions for future work. programming language one limitation of our current model is that it is currently only applied to python, which limits its use and effectiveness in executing programs written in other programming languages. this highlights the need for future work to expand the model’s applicability to other languages. faithfulness the result may not be faithful enough when handling difficult examples, such as those with complex logic, long loops, or many branches. for example, we observe that in two complicated programs that both contain the assignment “alpha = list(’abcdefg’)”, our model correctly predicts the value of “alpha” in one case but incorrectly in the other. the lack of faithfulness needs to be studied for further research on code execution. generation window size we limit the length of generated trace to 1024 tokens. it can be a limitation for programs with long execution traces, particularly those with loops. improving the ability of transformers to handle longer sequences (tay et al., 2021, 2022) would likely be beneficial for the code execution task. ethical statement the work is conducted in compliance with ethical principles. the datasets introduced in this paper only used publicly available data. the annotation in human evaluation was conducted by two authors of the paper, and thus there are no associated concerns, e.g. regarding compensation. therefore, there are no potential risks associated with the research."
640,"limitations our current design and experimental studies are limited on lms in the generic domain, and are not yet been studied in specific domains such as extracting healthcare knowledge from relevant neural models. we leave the exciting work of harvesting knowledge from various kinds of neural networks across applications and domains in the future work. ethical considerations in this work, the harvested knowledge is automatically generated by lms. we would like to note that the language models could possibly generate unethical knowledge tuples, same with the risks of other applications using language models for generation. we hope that the knowledge extraction study could offer techniques to better interpret and understand the language models, and in turn foster the future research of language model"
641,"limitations our work addresses the sequential task of modeling temporal user data through the use of path signatures as a tool for providing low-dimensional trajectories. although in our work we inject a post-level timestamp in the final representations, the path signature element is agnostic of time and rather only makes use of the sequence order. it therefore potentially hinders the model’s ability to efficiently model long timelines (unlike ours) with significant and highly irregular lags between posts. we plan to address this in future work. additionally, we understand that by employing truncated path signatures in the model, we loose information that can potentially provide additional signal through the compression that happens both in dimensionality reduction and in the signature itself. we have evaluated our model on a longitudinal mental health task. while the proposed architecture is in principle task agnostic we have not yet evaluated it on other longitudinal tasks on social media."
642,"limitations our model introduces additional parameters in the question-guided vision bias module, compared with other methods. moreover it is also worth exploring whether the question-guided vision bias module can improve number type questions in other ood data sets."
643,"limitations in this section, we discuss the limitations of tae. first, as our method depends on event structure information which is obtained through automatic parser, if the parser is not good enough, then it will impact the performance. second, since we focus on leveraging structural information, we restrict the experiments on text-based event explanation. future work will explore multi-modal event detection explanations and evaluate models on other nlp tasks."
644,"limitations the findings of this study have to be seen in light of some limitations. (1) it is non-trivial to extend our model for generation tasks. since the main focus of this work is to improve both effectiveness and efficiency of the dual-encoders, text-decoder is not considered in model design. in the future, autoregressive mechanisms will be consider to applied in model architecture so that the model can be directly used for generation tasks like image captioning. (2) there may be disadvantages of the model in region-level vl tasks such as object detection. the reason is that these tasks require images in high resolution and fine-grained annotations of bounding boxes, which are non-trivial in generic vlp settings. to solve this problem, exploring different levels of granularity between image-text pairs is a promising direction and will be considered as the future work."
645,"limitations we have demonstrated that cockatiel is capable of generating meaningful explanations that align with human concepts, and that they tend to explain rather faithfully the model. the concepts extracted of nmf are abstract and we interpret them using part 3 of the method. however, for the interpretation, we rely on our own understanding of the concept linked to the examples of words or clauses associated with the concept. this part therefore requires human supervision and will not be identical depending on who is looking. one way to add some objectivity to this concept labeling task would be to leverage topic modeling models to find a common theme to each concept. in addition, τ1 and τ2 were chosen empirically to allow for an adequate concept complexity/human understandability trade-off in our examples. we recognize that this choice might not be optimal in every situation, as more complex concept may be advantageous in some cases, and more easily understandable ones, in others. we surmise that this choice might also depend on the amount of concepts and on the model’s expressivity. finally, we have studied the meaningfulness and fidelity of our generated concepts, but ideally, the simulatability should also be tested. this property measures the explanation’s capacity to help humans predict the model’s behavior, and has recently caught the attention of the xai community (fel et al., 2021b; shen and huang, 2020; nguyen, 2018; hase and bansal, 2020). we leave this analysis for future works."
646,"limitation our paper presents a pilot exploration of investigating a new setting in code-switched text synthesis — we allow the target language pair selection not limited to those for which we already have training data. although we have shown the strength of gloss qualitatively and quantitatively, our experimental setting is still confined due to the dataset restriction — all the input text is in english. it would be an even harder challenge if the source languages are more diverse and we leave such exploration for future work. additionally, due to the computational restriction, in gloss, we only explore mbart50-mmt and an augment-mmt as our pmmtm. from the experimental results, we do observe the benefit of having a more stable pmmtm in gloss. we anticipate the models’ performance can be further improved by leveraging more stronger pmmtm, and the exploration is left for the future. broader impacts our proposed models are based on a model that is pre-trained on a large scale of multilingual machine translation data. it is known that the machine translation model could capture the bias reflecting the training data (wang et al., 2022). therefore, our models can potentially generate code-switched text containing offensive or biased content. we suggest that for deploying our model in any real-world applications, careful examination of the potential bias is an essential step."
647,"limitations although our model is more efficient than previous models trained using the mlm objective and the standard transformer architecture, we notice that the models runs around 30% slower. this is due to the disentangled attention mechanism, which is more computationally expensive than the standard attention mechanism. we also note that at the time of writing, the debertav3 tensorflow 2 implementation available on huggingface’s transformers library (wolf et al., 2020) experiences heavy slowdowns with tpu backends. our attempts to solve this issue were unsuccessful, and we were unable to train our model on tpus."
648,"limitations in this section, we discuss some limitations and potential risks of our work. (1) our codeprompt focused on program and language generation tasks, so it is difficult to directly apply our method to program and language understanding tasks. (2) we designed an input-dependent prompt template with fixed backbone words (language, function name, keywords) for a simple and efficient template. a more effective template can be crafted. (3) we applied only codet5, the most state-of-the-art model, as the basis of the framework of our codeprompt."
649,"limitations first, in this work, we assume that clustering in the encoder and the decoder is only related to the source and target languages, respectively. actually, both parameters in the encoder and the decoder are influenced by source and target languages simultaneously. therefore, our assumption may lead to a performance drop. in future work, we plan to explore more complicated clustering strategies. moreover, our adapter-families method depends on prior linguistic knowledge. its actual effectiveness can be affected by the distribution of language families/groups in clients. our methods mainly apply to comparably uniform language distribution. in addition, the effectiveness of our methods on other plms needs to be verified. however, it is easy to transfer our methods to other models so it will not be a challenging problem."
650,"limitations we identify the following limitations of plat and strategies to overcome such drawbacks: • performance of the final classifier is dependent on the black-box source weak labeler. we believe this limitation can be worked around in a real-word setting by ensembling source models to vote on a likely weak label for practical accuracy gains. • best-performing source models might differ for different tasks. the dataless nature of ews prevents precursory accuracy evaluations while choosing the source weak labeler model. however, quality of candidate weak labelers can be gauged indirectly. users can examine confidence distributions of weak labels (as in figure 1 and figure 2) as an indicator of pseudo-label ""naturalness"". they can also perform difficulty analysis (as shown in figure 5(a)) that does not require any labeled data. in a real-world scenario, ensemble weak labelers will be used, eliminating the need to choose a single best source model."
651,"limitation of existing visual document classification models by modeling a document as a graph and learning its embeddings using a graph attention network. by defining two types of edges (β skeleton and paragraph-based), we leverage the benefit of layout information while minimizing the effects of the errors from ocr reading order. thus, effectively embracing coarse and fine-grained layout information, gvdoc generalizes better for different layouts. while most visual document classifiers tend to perform well on in-distribution data, they fail or struggle on outof-distribution data; our model does not drop its performance on ood data. through experiments, we demonstrate the generalization of our model on out-of-distribution data."
652,"limitations our model is currently suitable for generating ordinary tables with attribute names and records, but it may struggle with more complex table formats that involve merged cells. to improve the flexibility of our model, we plan to investigate more versatile forms of table representation. another limitation of our model is that our model training involves longer training time, compared with seq2seq baselines. this may be due to the inherent instability of target assignments. in the future, we will explore refining the model training by reducing the target assignment instability. the existing datasets for this task is relatively simple, and in the future we will conduct experiments on more complex datasets that require reasoning, such as webnlg (gardent et al., 2017)."
653,"limitations the limitations of our r2anker includes i) performance bottleneck: as verified in our experiments, the performance of multi-adversarial ranker training depends more on types of the comprising retrievers than their performance. since the number of the types is very limited, there is a performance bottleneck of our method. and ii) compromised adversary: due to computation overheads, the adversarial process is compromised in our training framework in terms of real-time retriever updating. this would negatively affect the performance of the framework."
654,"limitations to train ecola, we need to provide structured knowledge with aligned unstructured textual data to the model. thus, we should either manually pair quadruples with event descriptions or use some matching algorithm to automatically build the pairs. the former requires human labeling effort and is hard to apply on large-scale datasets, while the latter would introduce noise into the dataset. thus, ecola is currently tailored for domain adaptation and enhances pre-trained models with domain knowledge. there is still work to be done to let models be jointly trained on large-scale structured and unstructured data."
655,"limitations our study is limited to the adaptation of mlms to new languages. while we believe that our proposed approach could also be applied more broadly (e.g., autoregressive models instead of mlms, or adapting to new downstream tasks instead of new languages), further experiments are necessary to empirically verify this. in addition, we observe a considerable variance across languages (§5.4), the reasons for which are not entirely clear. ideally, we would have a broader set of languages to better study this, as our language set is limited and skewed towards the indo-european family. finally, we average results over 5 finetuning runs, but computational restrictions prevented us from also averaging over multiple pretraining runs. as discussed in §a.5, we observed a non-negligible variance over pretraining runs in a preliminary experiment, but a more systematic exploration is necessary to better understand its impact."
656,"limitations the main limitation of our work is that we can not use a unified model to complete the zero-shot entity and relationship extraction tasks. specifically, our method trains two models, dsp-zsner and dspzsrc, to extract the entities in the text first and then classify the relation of each pair of entities. this method needs to train and store two models, which is troublesome to maintain in practical applications. in addition, although our method has dramatically improved the inference speed of the previous prompt method, the method still affects the reasoning speed of the model. in the followup works, we will be committed to solving this problem."
657,"limitations in this work, we mainly identify robust overfitting for prlms using pgd attacks instead of textual adversarial attacks. the reasons are two folds. first, we aim to check the learning curves during at. second, the results of textual adversarial attacks may not be generalizable since they integrate different strategies. in practice, however, it is more inclined to use some textual adversarial attack methods (e.g., textfooler, textbugger (li et al., 2019)) to evaluate the robustness of nlp models. as we have clarified in section 5.3.2, there exists an adversarial generalization gap when the model defends against pgd-based gradient attacks and textual adversarial attacks. while it is difficult to check their robust loss and accuracy curves during at, it is necessary and promising to explore robust overfitting under textual adversarial attacks and provide helpful insights for promoting the adversarial robustness of prlms."
658,"limitations it’s important to note that, in this paper, we focus on bias resulting from underlying distribution of training data. bias that may result from pretraining of transformer models (li et al., 2021) is not within the scope of this paper. although we conduct a case study of finegrained hatespeech detection task, a collective effort from the research community is required to better quantify bias mitigation of our approach across multiple tasks and different types of bias. another limitation of our work is that our proposed algorithm requires dynamic adjustment of clusters. for very large datasets, this may be computationally expensive."
659,"limitations our work depends mainly on parallel data. although tasks focusing on language abilities can leverage machine translation to obtain parallel data (hu et al., 2020), it is much harder for tasks about knowledge and facts. using parallel data to train cross-lingual model editors is like doing full supervision, while we need to leverage weakly labeled data to mitigate data scarcity. on the other hand, whether monolingual or crosslingual, model editing still struggles with the continual learning problem. in the real world, knowledge constantly emerges and fades, disabling the stop of learning. however, most studies, including our work, focus on a single or a batch of inputs. thus, an effective solution of continuously updating a series of inputs is necessary before model editing becomes a practical technic. note that our work focuses on the editor’s generalized cross-lingual editing ability. we expect the editor to perform the editing honestly. this target potentially offers the possibility to modify model behavior maliciously. though editing may not soon become a practical technic, the potential risk does exist."
660,"limitations a large part of the dataset that we used in our experiments are semantic annotations for relatively short sentences (as the examples show). so we don’t know really know how our multilingual pre-trained language-meaning modelling for drs parsing and drs-to-text generation will work on longer sentences. in our experiments, we converted meaning representation in the sequence notation and modelled them with natural language texts in a seq2seq manner and masked tokens in the drs sequence randomly. perhaps a more natural way is to model drss as graph structures and let training objectives directly utilize any structural information from drs. a graph structure would also eliminate the explicit order of concepts that is present in the sequence notation. although we say that the drss are languageneutral, the concepts in the vocabulary are based on the english wordnet. as a result it might be the case that non-english words do not have a direct correspondence to an appropriate synset, but the number of such cases is likely very small. the only (trivial) language dependence in drss are literal occurrences of proper names in cases where they differ across languages (e.g., ""london"", ""londen"", or ""londra""). one way to remedy this is to add alternative spellings to the meaning representation to make it completely interlingual."
661,"limitations to better understand the limitations of our proposed mtmsg, we also perform a qualitative error analysis of the incorrectly generated samples. we randomly select 100 incorrectly generated descriptions and find that our model might incorrectly generate those samples mainly due to the misunderstanding of the necessary intent information from the images and ocr tokens. the statistical results reveal that 37% of the incorrectly generated descriptions are caused because the main part of the sarcasm might lie in the images (eg. figure 5 (a)), while the other 63% error cases are attributed to the failure of our model in capturing the intent information directly from the ocr tokens (eg. figure 5 (b)). specifically, in figure 5 (a), if we want to generate better descriptions, we need to capture the fine-grained visual attribute feature happy from the image; in figure 5 (b), we need to understand the intent information from the ocr tokens that ban guns can make us feel safe when we have lunch in the restaurant. therefore, to address the above issues in the future, we will further explore the fine-grained key information in the images to help guide the msg. besides, we will explore a language interpreter to further understand the key information contained in the ocr tokens."
662,"limitations for easier comparison with previous work, we only focus on text classification tasks, while st can also be applied to a variety of nlp tasks, such as language generation, conversational systems and commonsense reasoning (kedzie and mckeown, 2019; he et al., 2020; shi et al., 2022a,b; hendriksen et al., 2022). we also assume that the datasets are roughly balanced. however, real-world datasets are usually class-imbalanced (li et al., 2011), which might impact the performance of tapt and st. while this is out of the scope of this paper, we believe that this is an interesting avenue for future work. additionally, different labelled and unlabelled sizes may impact the performance of st approaches in the domain shift setting. however, this doesn’t alter our"
663,"limitations a limitation of rata is always assuming the answer is included in the retrieval corpus, which is not always true. when the corpus does not contain the correct answer, the desired behavior is to inform the user that the answer cannot be obtained, but rata will provide a poorly supported answer. this also encourages rata to learn spurious correlations when the retrieved tables coincidentally contain the same value, but does not really support the answer. this problem is especially serious when the answer is very generic (for example, numbers like “0”) and same values by coincidence are common. this is related to the answerable question issue (rajpurkar et al., 2018) or evidentiality issue (lee et al., 2021; asai et al., 2022) for question answering. course location review date votes beaver island state park grand island, ny ? 0 1 joseph davis state park lewiston, ny 12/1/2009 1 0 black diamond dgc south wales, ny 12/1/2009 1 1 query: jonnieoh's dgcoursereview profile - disc golf course review gold answer: 12/1/2009 bart output: 12/1/2009; 12/2/2009; 12/1/2010; … rata output: retrieved table: jchoate7's dgcoursereview profile - disc golf course review for cell-filling on webtables, bart outperforms rata often by either copying values from other rows of the query table or producing values similar to those in other rows. however, as shown in figure 5, rata’s retrieval is often not helpful. usually, the information required to fill the query table is not repeated in the corpus, so the retrieved table cannot support the query. as a result, rata is simply retrieving some similar table, and selecting similar values in the tables."
664,"limitations among the six ambiguous and unanswerable problem categories in table 1, our counterfactual example generation approach can not cover the calculation unanswerable and out-of-scope examples generation. the reason is that our approach focuses on the table transformation ways while generating the calculation unanswerable and out-of-scope examples requires conditional nl modification techniques. we leave this as our future work."
665,"limitations we have discussed the implications of our research in section 6. in this section, we further discuss the threats to validity of our study. • threats to internal validity: the main internal threat to the validity of our research comes from (rq3) where we present a qualitative study on the variation of aliases. we are unable to cover all cases in the qualitative study. for example, the entity of d030342 (disease) in table 3 has 778 unique aliases. it is impossible to show all aliases to readers. to help mitigate this threat, we try to show as many examples as possible in a limited space. • threats to external validity: the main threat to external validity arises from the potential bias in the selection of experimental datasets, attacking target models and off-theshelf ner and entity linking tools. to mitigate this threat, we experiment with multiple datasets, models and tools. for experimental datasets, we choose the three most popular docre datasets (i.e., docred, cdr, and gda). we believe that these three datasets are broadly representative in this research community. for attacking target models, we choose three typical models ranging from non-contextualized sequence-based to graphbased, and to contextualized transformers models. for off-the-shelf ner/linking tools, we comprehensively investigate five state-ofthe-art ner taggers and two entity linkers. ethical considerations as our goal of this study is to challenge current problem setups of docre, we heavily rely upon existing well-known datasets, models and nlp tools. we only claim that our findings may hold on similar datasets or domains. we acknowledge the risk of generalizability of our findings on other privacysensitive datasets or specific domains. in general, we suggest that practitioners repeat all experiments following our procedures when using other corpora."
666,"limitations in this paper, we focus on the end-to-end accuracy and passage retrieval accuracy for opendomain qa. we have also experimented on the beir benchmark (thakur et al., 2021) to evaluate our method in the zero-shot document retrieval task. overall, we obtained 48.1% macro-averaged ndcg@10 compared to 47.8% by the re-ranking method. for some tasks, tour obtains significant improvements with a pre-trained document retriever (hofstätter et al., 2021). for example, tour improves the baseline retriever by 11.6% and 23.8% ndcg@10 on bioasq and treccovid, respectively, while also outperforming the re-ranker by 2.1% and 2.4% ndcg@10. we plan to better understand why tour performs better specifically on these tasks and further improve it. tour also requires a set of validation examples for hyperparameter selection. while we only used in-domain validation examples for tour, which were also adopted when training re-rankers, we observed some performance variances depending on the hyperparameters. we hope to tackle this issue with better optimization in the future."
667,"limitations first, as indicated in table 2, different tokens are not equally vulnerable to privacy attacks. as such, assigning every token with the same output size k and privacy parameter ϵ might not be an ideal choice. an improved method would be to adaptively allocate privacy costs across tokens so that all of them are adequately protected. second, we adopt two simple strategies to decide whether a token is sensitive: assuming all tokens are sensitive or based on a pre-defined stopword list. however, the prior might be over-protective, but the latter can lead to privacy leakage since stopwords might help infer other sanitized tokens. therefore, a more flexible and practical way to decide the sensitivity of tokens is required."
668,"limitations while our study shows that easyproject can effectively translate the source sentences with special markers inserted to the target languages, using the google translation and nllb model, it is unclear whether all translation models can work well when special markers are inserted. to generalize this approach to future mt systems, we design a simple and computationally efficient approach to improve the robustness of mt systems in handling special markers. however, the translation quality for the marker-inserted text still falls behind the original text. we leave the work of further optimizing the translation quality as future work."
669,"limitations in our proposed k-htc method, incorporating the knowledge graph requires the concept recognition and pre-training process, as we introduced in section 3.2. this process may consume additional time compared with other htc methods, but it can be done in advance and does not need to be repeated, making it suitable for both research and industrial settings. besides, due to the errors of concept recognition algorithms, this process may introduce some noisy information in reality. this will interfere with the use of knowledge. in future work, we will attempt to utilize entity linking algorithms (wang et al., 2023) to further guarantee the quality of recognized knowledge. another limitation is that we utilize the label name in the khla module. it may not be available for some datasets with only label ids. in response to this, we can select high-frequency keywords from documents in each category, which play the same role as the label name."
670,"limitations first, our taxonomy of multi-answer mrc instances only considers whether we know the exact number of answers from the questions. in some cases, one might have an imprecise estimate of answer numbers from the question. for example, for the question who are barcelona’s active players?, one might estimate that there are dozens of active players for this football club. yet, these estimations are sometimes subjective and difficult to quantify. therefore, this instance is classified as passage-dependent according to our current taxonomy. we will consider refining our taxonomy to deal with these cases in the future. second, we did not conduct many experiments with pre-trained models larger than the large-size ones due to limited computational budgets. generation models of larger sizes show great potential with more parameters and larger pre-training corpora. we encourage more efforts to deal with multi-answer mrc with much larger models, such as gpt-3.5."
671,"limitations one major limitation of this study is its input modality. specifically, our model is limited to textual inputs and ignores other modalities (e.g., vision and audio). open and domain incremental lifelong learning across modalities is more realistic and challenging. fortunately, we can obtain robust features of different modalities via multi-modal pre-training models (xu et al., 2021; huo et al., 2021). for future work, we will try to tackle multimodal tasks in an open (including out of distribution data (lang et al., 2022, 2023a,b)) and domain incremental lifelong learning scenario with better approaches."
672,"limitations due to the maximum input length constraint of both the clip text encoder and the text-to-image model, we are unable to process long texts. we are interested in exploring alternative prompt configurations to circumvent this limitation. our methodology is readily extendable to these settings, making it an intriguing area of study."
673,"limitations as a limitation of the modeling assumption, dsntm assumes that the number of topics is constant over time; however, this assumption is inappropriate for some time-series documents, such as scientific papers. as the number of scientific papers is increasing annually, increasing the number of topics over time would be appropriate for modeling the time-series evolution of academic literature. we used the abstracts of the papers as text, and the attention was computed using textual information. however, citations mainly appear in the body text when a paper cites other papers. therefore, there might be a discrepancy between the attention among topics and the citation relation among papers because the attention cannot not consider information in the body text. in future work, it would be desirable to evaluate our model using a corpus containing the body text of the papers. generally, topic models sometimes infer the incorrect information about topics, such as the frequent words appearing in topics, the topic proportion in each document, and the dependencies among topics. it would be the potential risk to induce the misunderstanding of users."
674,"limitations in our experiments, we use t5-base and t5-large models as the target model since they are widelyused, representative pre-trained seq2seq models and use comet-atomic2020 as the commonsense knowledge source. however, there are other pretrained seq2seq models such as bart, and neural commonsense models such as comet that we did not experiment with. moreover, we only experimented with 10 million randomly sampled sentences from the english wiki and bookcorpus datasets. it would be interesting to investigate whether continually pre-training with a larger scale dataset can further improve the performance. ethical considerations our work focuses on improving the commonsense reasoning ability of pre-trained language models. it probably does not introduce extra ethical concerns. however, in commonsense knowledge extraction, the neural commonsense knowledge model may generate unexpected (e.g., biased) commonsense inferences, and training with these inferences may lead to additional bias in the pre-trained model. nevertheless, all pre-trained language models contain bias and should be examined."
675,"limitations the proposed method has limitations in its dependence on the accuracy and performance of the probe classifier as noted in (belinkov, 2022), and may be limited in scenarios where the dataset is small or lacks sufficient information about the protected attribute. additionally, this approach increases inference time due to the use of a sequential debiasing classifiers. in future work, we aim to find a single probe that eliminates non-linear leakage. finally, the proposed method aims to eliminate information about a protected attribute in neural representations. while it may align with fairness metrics such as demographic parity, it is not specifically designed to ensure them. ethical considerations ethical considerations are of utmost importance in this work. it is essential to exercise caution and consider the ethical implications when using this method, as it has the potential to be applied in situations where fair and unbiased decision-making is critical. it is important to thoroughly evaluate the effectiveness of the method in the specific context in which it will be used, and to carefully consider the data, fairness metrics, and overall application before deploying it. it is worth noting that our method is limited by the fact that gender is a nonbinary concept and that it does not address all forms of bias, and further research is necessary to identify and address these biases. additionally, it is important to consider the potential risk of inadvertently increasing bias through reversing the direction of the debiasing operation in the algorithm. it is crucial to be mindful of the potential impact of this method and to approach its use with caution and care."
676,"limitations by analyzing the error cases, we find that almost all the existing work (including our let) cannot handle the disorder problem of words well, primarily when the error occurs far from the correct location. for example, there is a correct sentence: ’on my way to school today, i bought a very tasty apple.’. if the erroneous form is as follows: ’on my way to school apple today, i bought a very tasty.’, it is hard for the model to understand that the right thing to do is to put apple back at the end of the sentence."
677,"limitations of the retriever-reader pipeline approach to odconvqa, which is prone to error propagation from the retriever, unable to run both sub-modules in parallel, and demanding effort to manage these two submodules, due to its decomposed structure. to address such issues, we formulated the odconvqa task as a dense phrase retrieval problem, which makes it possible to directly retrieve the answer based on its representational similarity to the current conversational context. furthermore, to model the conversational dependency between the current and its previous turns, we force their representations to be similar with contrastive learning, which leads to retrieving more related phrases to the conversational history as well as the current question. we validated our proposed pro-convqa on odconvqa benchmark datasets, showing its efficacy in effectiveness and efficiency. limitations as shown in table 3, the contrastive learning strategy to model the conversational dependencies between the current and previous conversational turns is a key element in our phrase retrieval-based od- 1https://github.com/mcgill-nlp/topiocqa convqa task. however, when the current conversational topic is significantly shifted from the previous topic as the user may suddenly come up with new ideas, our contrastive learning strategy might be less effective. this is because modeling the conversational dependency is, in this case, no longer necessary. while we believe such situations are less frequent, one may further tackle this scenario of significant topic switching, for example, with history filtering, which we leave as future work."
678,"limitations our method requires access to a large set of labeled examples for the memory bank—ideally with some relevance to the evaluation tasks. this limits the languages and tasks that are optimal for this method: there does not exist a large variety of training examples for low-resource language varieties, nor for certain much more specific tasks—as in, for example, industry applications with domainspecific customer data. and while multilingual models could leverage cross-lingual transfer, it is unclear how well this model would generalize into low-resource languages when (for example) using multilingual bart. when using the full demonstration memory, meta-training does not run on a 16gb gpu using our current implementation. while this does exclude more common gpus, our approach could still run quickly on a 32gb gpu in a few hours, thus costing far less than pre-training a language model of comparable few-shot performance from scratch."
679,"limitations as shown in the ablation studies, using the boundary shift loss without the base model for scope prediction leads to a huge negative impact on the performance. that is, bsl strongly relies on the assumption that the proposed candidate spans are, to some extent, being an accurate estimation of the target spans. the experiment of using bsl solely could be seen as an extreme case, that no candidate spans are proposed at all. for our task, bsl could benefit from the strong base model. for the case of noisy datasets or on a more challenging task, where a base model could not generalize to a reasonably good coarse span proposal, the benefit of bsl might be limited."
680,"limitations we used only the pre-trained bart-large model when training each model within the qag framework. we assume that comparative experiments using several sequence-to-sequence language models will be good future works. also, we only used six interrogative words, and did not consider ‘whose’ and ‘whom’ in the process. we considered these as originating from ‘who’, but generating eight interrogative words including ‘whose’ and ‘whom’ would be a good approach. at last, in order to create a robust ranker, it is best to have a dataset that contains positive and negative samples. since the manual data generation process required a timeconsuming process, we utilize in-context negative samples as an alternative. if there is a dataset for the ranker learning purpose, much better performance can be achieved."
681,"limitations test set size. one of the main limitations of our work is relatively smaller test set sizes. this stems from the way our perturbation experiments are set up - we can only use existing test sentences which already end with specific punctuation in order to measure the effect of deleting them, or start with sentences which do not have sentence final punctuation in order to measure the effect of inserting them. in general, a majority of the official test sets have sentences ending in full stops; this results in having a smaller test set to work with. this is also the same issue that presumably gives rise to sensitivity issues in the trained models. however, given that our focus has been on each particular punctuation, instead of merging them all together, we find that our test sets are larger than the ones used in previous work for each punctuation. combined with the fact that we ensure to perform significance testing and manual analysis, we believe our results are reliable. appendix a.1 includes details and a"
682,"limitations though achieving promising results in the experiments, our work still has the following limitations. • as shown in table 2 and table 3. the proposed gaussian embedding may have a calibration problem leading to performing badly on fine-grained similarity tasks measured by spearman’s correlation. • the proposed method assumes that all relations are symmetric and adopts a symmetric similarity measurement. however, not all the relations are symmetric. and the ability to deal with unsymmetric relations with unsymmetric measurement is one important advantage of density embeddings which point embeddings do not have. • the proposed mrpes dataset should be improved in terms of quantity and quality. the number of test samples should be increased to over a thousand to get more statistically robust results. the types of unseen relations should be also increased to have a more comprehensive investigation of the ability to generalize on relations. the negative samples should be elaborately designed to provide the anchor event with different negative samples under different relations."
683,"limitations we acknowledge that the methodology used to build dlama-v1 still has limitations related to the information within its relation triples. while directly querying wikidata as a dynamic source of facts provides the flexibility needed to acquire data that is relevant to different cultures (as opposed to using the static t-rex dump of triples), the diversity of the triples that are compiled depends on the availability of a diverse set of facts on wikidata in the first place. for instance, the smaller number of relation triples related to arab countries for the predicates (p136 - genre), (p190 - sister city), and (p449 - original network) in dlama-v1 (arabwest) demonstrates the difficulty of querying the exact number of facts for both cultures despite using exactly the same queries with the only difference being limiting the region to which the triples belong. another limitation is the inability to enumerate valid and fine-grained subclasses of objects for specific subjects, if these fine-grained objects are not on wikidata. steps #3 and #5 of dlama explained in §4.1 ensure that a possible and more general object is still valid for a specific subject. however, inferring a more specified object from a generic one is impossible. for example, the fact that someone speaks “american english"" implies that they speak english as well, but knowing that someone speaks “english"" is not enough to speculate about their dialect (i.e.: “american english"", “british english"", etc.). while the triples within dlama are sampled by picking the ones whose subjects have the largest wikipedia articles’ sizes, the infeasibility of manually reviewing the large number of diverse facts within dlama-v1 makes it hard to claim that the facts are free of inaccuracies or missing information. more broadly, dlama supports relations predicates that are already part of mlama to fairly compare the results on dlama to those previously reported on mlama. moreover, we make sure that the subjects and the objects of the relation triples are available in the different languages of interest. having these constraints might imply that some culturally relevant facts might have been dropped out of dlama-v1 (e.g., predicates that are not part of mlama, or triples having missing labels in one of the languages of interest). lastly, we used mlama’s probing setup in which the models rank a predefined set of objects for each prompt. their prediction is correct if the top-ranked object is one of the valid labels for the corresponding relation triple used to populate the prompt. therefore, a model’s performance is expected to be higher than that achieved by a generative setup in which the model is asked to generate the most probable completions for the masked tokens."
684,"limitations we showed that our model is efficient in handling conditional qa on long documents with hierarchical reasoning framework. however, our discourse graphs for each document section are constructed based on the prediction of the pretrained discourse parser. there is promising improvement for our approach by use of more efficient discourse parsers."
685,"limitations & ethical and societal considerations we consider the following limitations and societal considerations of our work. machine-generated data our analysis is based on gpt-3 generated data. though not perfectly aligned with real-world scenarios, as demonstrated in park et al. (2022), such analysis can provide insights into the nature of social interactions. however, this could induce specific biases, such as skewing towards interpretations of words aligned with gpt-3.5’s training domains and potentially overlooking more specialized domains or minority speech (bender et al., 2021; bommasani et al., 2021). the pervasive issue of bias in offensive language detection and in llms more generally requires exercising extra caution. we deliberately generate multiple contexts for every statement as an indirect means of managing the biases. nevertheless, it is a compelling direction for future research to investigate the nature of biases latent in distilled contexts for harmful speech and further investigate their potential impact. for example, it would be valuable to collect human-annotated data on cobra to compare with the machine-generated data. however, we must also recognize that humans are not immune to biases (sap et al., 2019b, 2022), and therefore, such investigations should be carefully designed. limited contextual variables although cobracorpus has rich contexts, capturing the full context of statements is challenging. future work should explore incorporating more quantitative features (e.g., the number of followers of the speaker) to supplement contextual variables such as social role and power dynamics. in this work, we focus on the immediate context of a toxic statement. however, we recognize that the context of a toxic statement can be much longer. we have observed significant effects even in relatively brief contexts, indicating the potential for improved performance when more extended contexts are present. we believe that future research could explore the influence of richer contexts by including other modalities (e.g., images, videos, etc.). limited identity descriptions our work focused on distilling the most salient identity charac- teristics that could affect the implications of toxicity of statements. this often resulted in generic identity labels such as “a white person” or “a black woman” being generated without social roles. this risks essentialism, i.e., the assumption that all members of a demographic group have inherent qualities and experiences, which can be harmful and perpetuate stereotypical thinking (chen and ratliff, 2018; mandalaywala et al., 2018; kurzwelly et al., 2020). future work should explore incorporating more specific identity descriptions that circumvent the risk of essentializing groups. english only we only look at a us-centric perspective in our investigation. obviously, online hate and abuse is manifested in many languages (arango monnar et al., 2022), so we hope future work will adapt our frames to different languages and different cultures. subjectivity in offensiveness not everyone agrees that things are offensive, or has the same interpretation of offensiveness (depending on their own background and beliefs; sap et al., 2022). our in-context prompts and qualification likely make both our machine-generated explanations and human annotations prescriptive (röttger et al., 2021), in contrast to a more descriptive approach where we would examine different interpretations. we leave that up for future work. dual use we aim to combat the negative effects and harms of discriminatory language on already marginalized people (sap et al., 2019b; davidson et al., 2019). it is possible however that our frames, dataset, and models could be used to perpetuate harm against those very people. we do not endorse the use of our data for those purposes. risk of suppressing speech our frames, dataset, and models are built with content moderation in mind, as online spaces are increasingly riddled with hate and abuse and content moderators are struggling to sift through all of the content. we hope future work will examine frameworks for using our frames to help content moderators. we do not endorse the use of our system to suppress speech without human oversight and encourage practitioners to take non-censorship-oriented approaches to content moderation (e.g., counterspeech (tekiroğlu et al., 2022)). harms of exposing workers to toxic content the verification process of cobracorpus and cobracorpus-cf is performed by human annotators. exposure to such offensive content can be harmful to the annotators (liu et al., 2016). we mitigated these by designing minimum annotation workload, paying workers above minimum wage ($7-12), and providing them with crisis management resources. our annotation work is also supervised by an institutional review board (irb)."
686,"limitations our model achieves outstanding performance in relation to chinese spelling correction. however, it has several potential limitations: (i) errors of missing and redundant characters cannot be corrected by our model. the ptcspell model only focuses on spelling errors, and requires that the input text has no grammatical or semantic errors. (ii) the error-correcting language is targeted at chinese. the pre-trained model based on similar pinyin cannot adapt to other languages, while pretrained model based on similar character shape can adapt to other languages well, because the pinyin input method is unique to chinese, but character error due to a similar shape is a common problem in many languages. nevertheless, we put forward the idea of matching the pre-trained model with error correction tasks, which is suitable for all languages."
687,"limitations nonfacts generates grammatically correct nonfactual summaries. however, in practice, summaries can be non-grammatical, noisy, and nonsensical. this can limit the generalization of our performance in such cases. additionally, hypothesis-only results show that a considerable number of samples are identified correctly without their context document. the reason can be the memorized knowledge in pre-trained classifiers or surface features and semantic plausibility. broader impact our model has no direct environmental impacts, fairness or privacy considerations. however, it is important to note that it must not be used as a factchecking tool as there is a potential risk that false statements may be labelled as true. our classifier evaluates the factuality of a summary based on a context document, and if the document is misleading, the summary can be factual based on misleading information. additionally, nonfacts generates nonfactual summaries, which might have potential risks if misused for generating massive nonfactual summaries (claims). addressing such risks is an open issue in the field and is not specific to our work."
688,"limitations the findings of this work are limited and dependent on the presented experiments. the image dataset may be biased since the gender, ethnicity, and age were estimated by the dex algorithm (rothe et al., 2015) and checked by the authors. despite our best effort, the employed templates could still contain some latent bias that limits the variability and validity of the completions at inference time. since the study was conducted only in english, the insights can be considered valid only for this language. ethical statement one main concern with bias in vl is the potential harm it can cause to marginalized communities. biased vl models can perpetuate and amplify existing societal inequalities and injustices. this can result in discrimination against certain groups of people, such as racial and gender minorities, people with disabilities, and more. in particular, we are concerned about the use of vl in areas such as content moderation, hiring decisions, and criminal justice. biased models used in these contexts can have serious consequences, such as wrongful censorship or discrimination against certain job applicants. while we acknowledge that the specific harms we fear may not always be likely to occur, we believe it is important to prioritize ethical considerations and strive for the highest possible standards of fairness and inclusivity in vl research and applications. this work contains harmful language and stereotyped statements, which are only intended as examples to showcase the possible negative connotations of the analyzed models and experiments. every social, ethical, religious, or political statement or association is to be interpreted within the purpose of the experiment and condemned otherwise. we are aware of our approach’s shortcomings in terms of the binary consideration of our gender analysis. this is due to data and linguistic limitations rather than a value judgment."
689,"limitations there are several limitations in our paper. first, we have not validated the advantages of our proposed peer in model scales larger than base model, due to the constraint in our computation resource. we plan to experiment the peer in larger scale models when more computation resource is available. second, in order to filter out potential noise from the relative weak generator, our current rank label retrieving scheme uses a strict condition t = 3, which leads to the fact that a significant proportion of tokens have rank label −1 and essentially are involved only in the original rtd task. please refer to the details in appendix a.3. we intend to design some label retrieving scheme which applies a softer criterion so that more tokens can be fully or partially involved in the complete tqr task. finally, our peer currently does not have the ability of automatically searching for an optimal value of hyperparameter δ, which we also plan to design in the future."
690,"limitations by applying mutual learning, introducing distance polarization regularizer and utilizing cyclical annealing schedule, ml-lmcl achieves significant improvement on three benchmark datasets. nevertheless, we summarize two limitations for further"
691,"limitations the current dialogue system still has some limitations. for example, although the current crg model can make the output contain the key concept words in the knowledge path, due to the large scale of the pre-training model, the output semantics of the current method are still not very interpretable and controllable. a feasible way is to explore new fine-tuning methods to approach high-level semantic style control. in addition, our current dialogue system lacks human qualities such as empathy, factual correctness judgment, and moral common sense representation. a key breakthrough is to explore a goal-oriented dialogue dataset with richer dimensions."
692,"limitations we only use pre-trained video transformers off-theshelf to encode the videos, while more nuanced and specific utilization of other models can be explored to further improve the performance. there are also valuable egocentric videos and demographic statistics along with the ego4d dataset that we have not yet incorporated in our approach. due to the difficulty and cost of collecting videos with transcriptions and voting outcome annotations, the total number of games is insufficient to train a deep neural network for voting outcome deduction, though data augmentation techniques can be explored to mitigate this limitation."
693,"limitations this work has a narrow focus: small-scale analysis, translation between one language pair (french and english), examining terminology around two realworld public figures (whose forms of address are both highly prescribed and publicly documented),8 in a specific newsworthy event (the accession to the throne of a new king after over 70 years of data and translation about a queen). first, the scale of the analysis is quite small, so it does not examine in detail questions of frequency of errors, distributions of errors, or statistical significance. while this work raises issues that may be relevant for consideration across other language pairs, the relevance of the specific linguistic conventions discussed here will vary across language pairs, and certainly do not cover the full range of asymmetries in linguistically encoded information (see, e.g., mager et al. (2018)). due to the prescribed forms of address of the two monarchs in question, this work only examined translations related to a small subset of terms (e.g., “his”/“her”, reine/roi) and does not examine performance on terms used related to other individuals or to other third person singular pronouns or forms of address that could be used by a monarch. the specific circumstances (a 70 year reign of a sovereign of a country with an official bilingualism policy and this particular set of linguistic features) means that we may not expect these results to generalize to other potentially comparable scenarios. lastly, we cannot examine the training data used for the public models, so we can only draw"
694,"limitations on the adoption of ask an expert pertains to certain domains where the system must be deployed locally to uphold privacy concerns, such as mental health systems aiming to safeguard patient data. in such instances, relying on external api services becomes less feasible. however, it is not always necessary to utilize all the knowledge of large expert models. and for specific domain use cases, such as mental health, it is unlikely that the full size of the model is indispensable. given the effectiveness of our approach, in future work we would like to explore the extent to which the expert model can be distilled (sanh et al., 2019; schick and schütze, 2021c) into models which are able to run locally on consumer-grade hardware."
695,"limitations in our experiment, we use only abstract text as the input text for literature review generation however, in writing literature reviews, a writer reads the full text of each cited paper and even other papers related to the research area. therefore, the input data are insufficient to write a complete literature review. as only 70% of the cited papers have access to the body text in our scireviewgen, a dataset containing full-text information is required for further research. in human-written literature reviews, the chapters complement each other and are not redundant. however, as our qfid and baseline models generate each chapter independently, they cannot consider the relationships between chapters. furthermore, the relations between each cited paper are considered in the actual literature review writing process (e.g., which paper is the first on the topic and which is the following). however, these relationships are not considered in the models. in future research, a literature review generation model that can consider the relations between chapters and cited papers by using additional information, such as the contents of other chapters, citation networks, and citation sentences, should be investigated. as mentioned in section 5.2, the generated text contains incorrect information to a certain extent. therefore, we cannot publish it without human revision. currently, the model can be utilized as a writing assistance tool, not as a complete literature review generation model."
696,"limitation in our current study. for instance, we did not explore sampling techniques other than random sampling; while recent works (yuan et al., 2020; paul et al., 2021; guo et al., 2022) have shown promising directions in data sampling that outperforms random sampling. another interesting direction is to explore the model architecture’s influence on generalizability, and thus the learning curve, which we left for future works."
697,"limitations during the configuration search stage, because this is a multi-objective optimization problem involving performance and efficiency, we use the evolutionary algorithm to search here. designing a robust and efficient optimization objective is not simple and it will affect the convergence of search results. 1https://huggingface.co/bert-base-uncased, https://huggingface.co/bert-base-chinese limited by hardware, and in order to speed up the search, we use a small subset of the validation set to search retention configuration, which is bound to have a certain impact on the overall search results. ethical statement in this paper, we propose ase, an algorithm to accelerate multi-turn response selection by prograssively selecting and eliminating unimportant tokens. the training corpora including the ubuntu corpus, the douban corpus and the e-commerce corpus used for evaluating our framework are publicly available and don’t pose privacy issues. the algorithm that we propose does not introduce ethical or social bias."
698,"limitations although our proposed framework beats several baseline methods for medical dialogue generation, there is still room for progress. we exploit an entity flow and a dialogue act flow to improve dialogue understanding and guide response generation. however, our summarized dialogue acts are limited in the types and granularity of functions they denote. we can manually annotate more medical-related dialogue acts in our future research following the soap notes. besides, more medical knowledge with different formats, such as medical articles and medical examination reports, can be incorporated. finally, it is crucial to recognize the potential risks associated with system utilization and the possibility of patient privacy leakage. a collaborative approach involving both dialogue systems and medical professionals should be considered. this will ensure that responses are endorsed by physicians and stringently overseen by reliable authorities."
699,"limitations as the computation complexity of knn is o(n2), when the size of a dataset gets really big, speed becomes one of the limitations of our method. multithreads and multi-processes can greatly boost the speed. lempel-ziv jaccard distance (lzjd) (raff and nicholas, 2017), a more efficient version of ncd can also be explored to alleviate the inefficiency problem. in addition, as our purpose is to highlight the trade-off between the simplicity of a model and its performance, we focus on the vanilla version of dnns, which is already complex enough compared with our method, without add-ons like pretrained embeddings (pennington et al., 2014). this means we do not exhaust all the techniques one can use to improve dnns, and neither do we exhaust all the text classification methods in the literature. furthermore, our work only covers traditional compressors. as traditional compressors are only able to capture the orthographic similarity, they may not be sufficient for harder classification tasks like emotional classification. fortunately, the ability to compress redundant semantic information may be made possible by neural compressors built on latent variable models (townsend et al., 2018)."
700,limitations a limitation of this work is that the dataset has not yet been thoroughly vetted by native speakers of the languages contained in the dataset. we acknowledge the importance of working with native speakers and manually reviewing datasets in greater detail as argued for by kreutzer et al. (2022) and lignos et al. (2022). we hope to do more manual review of lr-sum and other summarization datasets in the near future.
701,"limitations the paper focuses on the vqa task only, we will extend our method to other multimodal tasks in future works, e.g., referring expression comprehension (rec). moreover, although our ddg outperforms most of the state-of-the-art methods, the performance is still a long way from humans."
702,"limitations we raise a new challenge task in our medical dataset ccs. comparing with existing datasets, ccs requires text-to-sql models to generalize to different databases with the similar structure in the same domain. to tackle this problem, we provide a baseline method named syntactic role prediction, which is an auxiliary task and can be combined with the main task in a multitasking way. our experiments prove that srp can help improve the cross-schema generalization ability of models. however, the improvement is not that large. how to generalize models across different databases sharing the similar structure is still a challenge issue. we expect that future works can solve this difficult problem."
703,"limitations while we already manage to outperform the baseline, the pre-training data quantity is relatively small (∼20k instances). given the computational cost of training 30 models—six train sets, over five random seeds each—and testing them within inand cross- domain setups, we break the inspection of the optimal pre-training data amount at 24k instances. however we do not exclude that more pre-training instances would be even more beneficial for improving even more over the baseline. related to computation cost constrains, we test our syntax pre-training approach over one set of ud labels only (nsubj, obj, obl, nmod, appos). different sets could be investigated, e.g. including acl and compound, which present a lower, but still considerable amount of instances (see figure 3). finally, while approaching re by assuming that the gold entities are given is a common area of research, we leave for future work the inspection of the proposed method over end-to-end re."
704,"limitations comparing downstream performance of pretraining objectives with large-scale models is prohibitively expensive. because of this, we employ scaled-down models that closely resemble the architectures and training procedures of popular plms. in doing so, we assume that our findings are transferable to some larger publicly available models. as noted by hazarika et al. (2022), ctxaug offers an interesting alternative to prompting generative lms that are significantly smaller than those that typically exhibit few- and zero-shot capabilities (brown et al., 2020). while we provide support for both hazarika et al. (2022)’s claim and our assumption in preliminary and supplementary experiments with select plms (see section 1 and appendix b), these experiments are still performed on models of up to 140m parameters. therefore, we stop short of concluding that our findings generalise to llms, which dwarf these models in comparison. additionally, the number and types of target attributes that a user may want to control for in various downstream text generation tasks are potentially endless. however, our study focuses on only two possible target attributes, namely, inquisitiveness and positive sentiment, for the task of conversational dialogue modelling. in this way, our work partially serves as a re-implementation and reproduction study, confirming the main findings from hazarika et al. (2022), but also highlighting limitations."
705,"limitations this paper examines the anisotropy and outlier phenomenon only for a few, relatively similar, models. the isotropy-increasing transformations are nondeterministic and have to be calculated post-hoc based on some set of embedded data, which may not be practical for applications where inference is done on individual or small batches of examples. since we specifically consider sentence representations, we first average over word embeddings before calculating the mean and standard deviation for outlier analysis. this in effect reduces the sample size and leads to a smaller standard deviation, making our analysis more sensitive to even slight outlier dimensions. another reason to work with relatively small datasets is to make computing the transformations simple and fast, but this may limit the ability of these transformations to generalise."
706,"limitations we enumerate some limitations to our work. while we did create the largest union dataset to date, it is still of moderate size. as shown by our learning curves (app. g), the amount of training data we created seemed sufficient to saturate the learning of the models with which we experimented, but it might still be found insufficient for training other models. our annotation protocol might have influenced the compression rates of the unions, as we instructed workers to annotate sentence unions by first choosing a base sentence and then highlighting the other sentence. additionally, while the highlighting facilitates the annotation process, it cannot directly be used for analyses of the dataset since it is uni-directional. the dataset includes only input with exactly two sentences and it might be desirable for future works to also be able to train systems that take more than two sentences as input. our dataset is also domain specific, in that all the sentences are taken from news sources. this might result in challenging cross-domain generalization. this dataset is limited to the english language. while the suggested annotation protocol seemingly fits other languages, the step in which words are highlighted might prove problematic for morphologically rich languages, in which a single word includes many pieces of information. a segmentation of the text before annotation might be required."
707,"limitations in our work, we use only one solution from the llm to distill information into the student model, and according to wang et al. (2022), multiple subquestion-solution pairs can be sampled, and using majority voting, all pairs leading to the most frequent answer can be used to distill knowledge into the student models. also, due to computational budget, we used a single prompt to compare the cot and socratic cot and using more prompts (up to 8) might lead to a fairer comparison and better results (wei et al., 2022b). we leave these experiments for the future. ethical considerations although this work improves the reasoning capabilities of smaller models, the models are still not powerful enough to be used in sensitive settings such as education. we plan to release our code and model checkpoints, but the models must be used carefully by users, as many generative models, including ours, are prone to hallucination."
708,"limitations while hrdsattack is, to the best of our knowledge, the first dataset on extracting attacks on human rights defenders, there are some limitations. for one, while being the first corpus of its kind, our dataset is english-only. second, the number of documents is limited. while the sample size of hrdsattack (500) is on par with some of the other ee datasets, such as ace05 (599), we do see more samples being beneficial to subsequent model training and supporting other future studies. in addition, despite the effort to balance the class labels in the event attributes, some of the labels still remain imbalanced, such as perpetrator type."
709,"limitations the approach of language model integration for neural machine translation is analyzed and compared to the de-facto standard method of backtranslation. due to constrained resources, this work has several limitations. we focus on translation of text in a single domain, namely news-articles. different domains might exhibit different behaviour. for the back-translation experiments, we use beam search to create the synthetic data, other methods like sampling were not considered. when combining the synthetic and real parallel data, there are additional methods like tagging and block-wise batching, which we did not utilize in this work. finally, we compare against the most commonly used lm fusion approach, i.e. shallow fusion. there exist other lm fusion techniques which might exhibit different behaviour when used in combination with ilm neutralization."
710,"limitations a limitation of the proposed method is that our gazetteer is constructed only by dataset annotations. and it affects the gazetteer coverage in unseen cases. following previous work, such as lin et al. (2019) and fetahu et al. (2022), we will construct a larger gazetteer using external resources such as wikipedia or knowledge bases. as mentioned in section 3, we will leave this for future work. another limitation is that the gazetteer contains many spans that are associated with multiple entity types. taking the running examples in section 3.1 for example, the span ""london"" has type locationgpe in most cases, while it is sometimes labeled as type art-music. however, in our current design, given a named entity, there is no way to explicitly distinguish between different types. in future work, we will consider the context of named entity when fixing errors."
711,"limitations in experiments we only take plms into account because of their prevalence, hence the transferability to non-pretrained models is still unknown. however, due to the generality of plms, this can be a minor point in practical scenarios. moreover, although we successfully transfer adversarial attack methods in cv to nlp using a unified framework, we only instantiate the framework with the pgd attack as an example. it would be interesting to transfer more attack methods in cv and conduct a comprehensive analysis of what methods can benefit nlp, aiming to have a deeper understanding of plms. ethical consideration in this section, we discuss the potential broader impact and ethical considerations of our paper. intended use. in this paper, we design a general framework to adapt existing gradient-based methods in cv to nlp, and further, propose a decisionbased textual attack method with impressive performance. our motivations are twofold. first, we attempt to introduce adversarial attack methods of cv to nlp, since image attack methods have been well-explored and proved to be effective, therefore helping these two fields better share research resources hence accelerating the research process on both sides. second, we hope to find insights into the interpretability and robustness of current blackbox dnns from our study. potential risk. there is a possibility that our attack methods may be used maliciously to launch adversarial attacks against off-the-shelf commercial systems. however, studies on adversarial attacks are still necessary since it is important for the research community to understand these powerful attack models before defending against these attacks. energy saving. we will public the settings of hyper-parameters of our method, to prevent people from conducting unnecessary tuning and help researchers quickly reproduce our results. we will also release the checkpoints including all victim models to avoid repeated energy costs."
712,"limitations in this section, we discuss the limitation of the proposed model. currently, the hidden layer of the two neural networks for drift and diffusion are shared between all entities and relations. this might cause over-fitting on relations that show simple structures if the neural networks are set to be very deep. on the other hand, if the neural networks are set to be shallow it might negatively influence modeling complex relations as the direction of trajectories will be limited. one possible solution is to cluster relations based on complexities and use separate neural networks for each cluster depending on the complexity of the corresponding relation. this requires, however, prior knowledge about the structure of different relations which we leave as future work."
713,"limitations in this paper, we leverage amr to the gec model as external knowledge, and achieve a high f-score on single model. however, we do not use r2l reranking, model ensemble and other methods to ensemble single model and compare them with state-of-the-art ensemble models. our aim is to provide a strong baseline for incorporating amr in gec, so it is easy to generalize amr-gec to ensemble models."
714,"limitations there are three main limitations of our approach: (1) our model cannot handle negation operation. enabling bidag to support negation operation is a direction for future work. (2) the modeling for query representation and logical operators is too simple. improving bidag by more ingenious modeling for query representation and logical operators is also a direction for future work. (3) the training process cannot be parallelized well, which is a common drawback of qe models, as qe models have to predict node representations one by one."
715,"limitations due to time constraints, we were unable to implement some unreleased models as baselines for the proposed tasks. we did not conduct simile interpretation/generation on msd-ch in this paper since we could not automatically annotate the shared property in chinese data like the ""as...as"" mode in english. we are currently working on this annotation and plan to release the chinese simile interpretation/generation results on the data link. the coarse/fine version data we introduced in this paper can still be used for enlarging the msd data. we will study to utilize them for more simile data and richer language phenomena."
716,"limitations despite achieving superior performance, our proposed method requires manual selection for hyperparameters to decide the number of tasks, i.e., the number of virtual relations v and the number of synthetic tasks nv for each virtual relation. in future work, we target developing the method with automatic adjustment to add/remove virtual rela- tions and the corresponding tasks according to the status of the few-shot learner with the training going on by curriculum learning. besides, although we adopt a task selector to adaptively select beneficial tasks, it is still inevitable to bring noisy tasks in the meta-training stage. we will explore the strategy to achieve better denoising."
717,"limitations although our model has made some progress, it still has some limitations. first of all, sgt uses the tag type to represent the connection order of glcs fragments when forming a complete utterance, and the average statistics on the three datasets we used show that more than 99% of the complete utterance can be composed with less than three glcs fragments. that will lead to situations that need to combine multiple glcss (e.g., more than 3) to form a complete utterance, which cannot be fully trained or fall into unbalanced tag categories. second, like other tagging-based models, the fragments that make up the complete utterance must exist in history utterances or connection words, which does not work well for situations where it is necessary to combine context information and introduce new words to express their complete utterance."
718,"limitations main limitation 1 the experiments of this paper are only done in 14 languages that use the latin alphabet, and with a high share of indo-european languages, with up to 4 germanic languages. this is due to two reasons: (i) the scarcity of xpos and feats annotations in treebanks from other language families, and (ii) the research team involved in this work did not have access to proficient speakers of languages that use other alphabets. hence, although we created a reasonable diverse sample of treebanks, this is not representative of all human languages. main limitation 2 although we follow previous work to automatically generate perturbations at character-level, and these are inspired in psycholinguistic studies, they might not be coherent with the type of mistakes that a human will make. in this work, generating human errors is not feasible due to the amount of languages involved, and the economic costs of such manual labour. still, we think the proposed perturbations serve the main purpose: to study how morphological tags can help parsers when these face lexical errors, while the used method builds on top of most of previous work on adversarial attacks at character-level."
719,"limitations while we aim for our hegel crowdsourcing methodology to be applicable to other languages, and in particular low-resource languages, the ui design and our analyses require knowledge of the intended language, as well as familiarity with the regions where it is spoken. moreover, as our methodology relies on people’s familiarity with the places, it limits the cities chosen for the task and the participants that could take part, restricting the demographics of the participants accordingly. in addition, relying on people’s memory of the environment causes many of the descriptions to be too vague for humans to geolocate, thus, many of the descriptions were disqualified during the validation process as they could not have been resolved. the relatively low percentage of place descriptions that were successfully validated, raises the costs of collecting such a dataset."
720,"limitations in this section, we discuss the limitations of this work. first, on the problems side, it’s non-trivial to consider the order of all kinds of grounding knowledge, but we have only explored persona-chat. we hope to apply our method to more grounded generation tasks such as knowledge-grounded and document-grounded dialogue in the future. second, on the methods side, our framework is trainingbased, but we hope more lightweight techniques could be developed to improve the model’s robustness even without training the model."
721,"limitations. but any llm distillation will show a similar trend. existing kd methods are highly customized to the specifics of the teacher model, and require additional pretraining, fine-tuning, or data augmentation. our approach is much simpler and agnostic to both architecture and task. we ran our experiments on an rtx3090 gpu with 24gb ram which cost only $0.11 an hour, which is considerably cheap compared to other approaches that include teacher fine-tuning. we showed that our method is particularly effective on small datasets, and competitive with other kd methods which are much more computationally intensive and tailored to the teacher. a possible reason could be since the fine-tuning of bert on small datasets like mrpc, stsb, or rte can be unstable (zhang et al., 2020), eliminating it makes the kd more robust and improves the results. all other methods such as tinybert (jiao et al., 2020) or patientkd (sun et al., 2019b) use fine-tuned teachers. distilbert (sanh et al., 2019) does not use a fine-tuned teacher, but it is only limited to students with a hidden state of 784 due to the cosine loss it uses and lacks generalization across architectures."
722,"limitations we train the task-optimized adapters based on the pre-trained weights of dialogue lm. therefore, if applied to other dialogue tasks such as chit-chat and conversational qa system, the performance could be lower than that shown in our research. and we need future works to clarify the reason why the performance was better on the multiwoz 2.2 dataset, which is expected that our model does not overfit to the confused labels. our model inferences in on the end-to-end manner, but trains like modular system for each task. end-to-end learning is currently under study. we could adapt the multitask end-to-end learning to our method, which may lead to the better performance. also, we could analyze the inner working of task-optimized adapters applying xai technologies."
723,"limitations of image generation but also provides the data necessary to improve visual metaphor generation in the future. we plan to further examine the effect of prompt phrasing on the quality of the generated visual metaphors,and how that effect differs across different models. limitations while the results of human-ai collaboration for visual metaphor generation are very promising, such a procedure might be time-consuming but at the same time necessary for maintaining quality. we want to acknowledge that both our llm and bestforming diffusion models are released through a paid api and are not open-sourced. while our best-performing system uses chain of thought prompting, there are several other prompting or task decomposition techniques that we did not perform an extensive comparison with.last but not least, there is still enough room for potential improvement in generating visual metaphors which can be achieved by designing better prompts or by improving the compositional generalization of diffusion models. we also recognize the inherent limitation of an english-only basis for our visual metaphors and hope in the future to expand to other languages for source material."
724,"limitations to address the low-resource data in the training of tau-dr, we apply two heuristics, dataset enrichment and generation from different checkpoints. despite being effective, they require additional computational time that might be challenging in applications with low-computational resources. a possible approach to reduce the computational time might be to average the checkpoints. we believe that this might lead to competitive results, with a significant reduction in computational time, since checkpoint averaging proved to be an effective approach in low-resource settings. another limitation is when the original dataset is in a highlyspecialized domain that might contain domainspecific phrases that were most likely not included in the pre-training data of the language model. the results obtained by existing data augmentation approaches will most likely exhibit only marginal improvement."
725,"limitations we showed that our proposed method can greatly improve the performance of parameter efficient tuning on diverse nlu tasks and three different pre- trained models (i.e., roberta-large, debertalarge and gpt2-large). however, we acknowledge the following limitations: (a) the more super-sized pretrained models with tens of billions of or more parameters were not studied due to limited computation resources. (b) other tasks in natural language processing, like the text generation tasks, were also not considered. but our framework can be easily transferred to other backbone architectures and different types of tasks. it would be of interest to investigate if the superiority of our method holds for other backbone models and types of tasks. and we will explore it in future work."
726,"limitations apart from the issues mentioned in § 5.4, another limitation of trenc is that it does not integrate any pre-training process such as bert, which is effective in increasing the language understanding ability and adopted by previous works focusing on token-level classification tasks (wang et al., 2022; deng et al., 2022). two factors lead to this decision. first, we use dom nodes instead of tokens as the classification object and focus on relations between nodes rather than tokens. as the node text sequence is a composition of an arbitrary number of tokens, adopting the conventional masked language modeling (mlm) training objective (devlin et al., 2019) seems impractical since there is no direct mapping from an embedding vector, one-hot encoded or not, to a sentence. the second reason is simply that we do not possess the corpus or computation resources for model pre-training. in fact, we expect a properly designed pre-training scheme to bring better node semantics representation and si-pt relation modeling. it is an interesting topic and deserves further study."
727,"limitations we see two main limitations of this work. the first one concerns the diversity of the language models and datasets used. bert, distilbert and roberta have similar architecture, and sst2, imdb and rotten tomatoes are datasets designed to evaluate the sentiment of english text. it would therefore be interesting to validate the robustness of our results on more diverse languages, tasks and language models. in this short paper, we decided for brevity to follow the experiment design of sanyal and ren (2021), while being aware of its inherent limitations. the second limitation of this work concerns the time complexity of sig. as it needs to compute explanations for each word individually, this method can become very computationally expensive when applied on large text data. to alleviate this issue, we first made it possible to compute gradients in parallel, using an internal batch size similar to how captum (kokhlikyan et al., 2020) implemented the integrated gradients method. secondly, as discussed in 3.2, it is possible to reduce the number of interpolated points, which makes the computation faster while retaining better performance than the original ig. in this work, we ran our experiments on a machine with 16 cpus, and one nvidia tesla t4 gpu. with this setting, computing sig on sst2 and rotten tomatoes takes around one hour for each model. on the larger imdb, computing sig, on 2000 randomly sampled inputs, takes around 5 days for bert and roberta, and 2 days for distilbert."
728,"limitations and future work one limitation of our method is that sampling requires sampling both a conditional and a unconditional model, which results in slower inference times. on the other hand, progressive distillation (meng et al., 2022) provides an attractive solution to this problem. another limitation is that ho and salimans (2021) show that the diversity of generative models is degraded as w increases. ideally we would be able to have a model that improves upon the fluency as well as the model diversity. as for future work, we will leverage advanced large language models as the base architecture for training diffusion models to compete with high performance auto-regressive models. additionally, we will investigate modifications to diffusion models that are inherent to discrete data."
729,"limitations as mentioned in sec. 2, the current selection of fundamental reasoning skills for language models is limited by the availability of well-defined tasks and clear definitions of those tasks, as well as the availability of sufficient training data. as a result, some skills may overlap or may not be fundamental enough. for example, simple qa skill may overlap with ner skill to some extent. in the future, it would be worthwhile to explore self-supervised training tasks that can inject more fundamental abilities into language models. additionally, the selection and combination of fundamental reasoning skills can be further explored. for example, the inclusion of numerical reasoning ability to solve mathematical problems. additionally, methods for skill-centric pre-training corpus construction can also be explored to improve the effectiveness of these skills."
730,"limitations the primary constraints encountered in our research result from our dependence on a single dataset for experimentation and computing resource limitations. despite these, we postulate that our ranking-based methodology can be utilized for any summarization task that necessitates robust correspondence with a specific structure within the input. to validate this hypothesis, further experimentation is required to assess the generalizability of our technique to alternative datasets and domains. in addition, our limited computational resources prevented us from experimenting with other long document encoder-decoder models such as bigbird and longt5 (michalopoulos et al., 2022; guo et al., 2022) as well as using higher beam widths during decoding. furthermore, the cost and complexity of procuring expert evaluators within the legal domain resulted in using automatic metrics alone. ethical considerations the usage of the generated summary results from legal opinions remains important. abstractive summarization models have been found to contain hallucinated artifacts that do not come from the source texts (kryscinski et al., 2019; zhao et al., 2020; kryscinski et al., 2020). while our model incorporated the argument structure of the source article, the generation results may still carry certain levels of non-factual information and need to be utilized with extra care. similarly, as mentioned in the prior line of works using canlii (elaraby and litman, 2022; zhong and litman, 2022), canlii has taken measures to limit the disclosure of defendants’ identities (such as blocking search indexing). abstractive approaches may cause user information leakage. thus using the dataset needs to be cautious to avoid impacting those efforts."
731,"limitations we would like to acknowledge the following limitations of this work. our study setup only takes advantage of supervised data in the form of triples of <speech, transcriptions, translations>. this is because we first and foremost want to investigate the effectiveness of pseudo-labeling in the most extreme case. however, the setup can be extended to be able to also rely on asr-only (<speech, transcription>) and st-only (<speech, translation>) pairs. we leave incorporating asr and st data as a future work as well as incorporating external language and machine translation models. we identified two sources of domain mismatch: input length ranges and vocabulary mismatch. however, the solutions that we investigate directly target the length mismatch, without explicitly addressing the vocabulary mismatch. the latter is indeed more challenging to address, especially without incurring additional supervision. in fact, circling back to the previous item as a future direction, incorporating supervision in the form of asr or st can expand the vocabulary set, also addressing vocabulary mismatch."
732,"limitations the limiation of this work is that we only consider one type of prefixes/suffixes, i.e., negative prefixes/suffixes. in our future work, we would like to work on other types of prefix/suffix sense detection tasks, such as prefix/suffix sense detection on occupation. for instance, in english, there are many suffixes such as -or, -er, and -ee, which mean a person with a certain occupation."
733,"limitations in this study, we propose differentiable segmentation to learn how to segment speech from the underlying translation model, and verify its effectiveness on simultaneous speech translation. however, since it can be jointly trained with the underlying task (sequence-to-sequence task), differentiable segmentation is not limited to the simulst task, but can be generalized to more streaming/online tasks, such as streaming automatic speech recognition (streaming asr), simultaneous machine translation (simt), real-time text-to-speech synthesis (real-time tts), online tagging and streaming parsing. given that there may be some task-specific differences between various tasks, this work only focuses on the differentiable segmentation in the simulst task, and we leave the study of how to apply differentiable segmentation to other streaming tasks into our future work."
734,"limitations so far jgr has only been evaluated on the domains of summarization, conversational summarization, question generation, and dialog. it should be evaluated on a wider range of benchmarks, such as machine translation and code generation. and we have not explored jgr’s performance with extralarge language models such as gpt-3. we will evaluate jgr on the above points in the future. because the generator of jgr samples candidates using auto regressive sampling, it may occupy relatively longer computational time and larger memory then the conventional mle training. though the performance of jgr is satisfactory, we still want to improve its computational costs. we will try non-auto regressive sampling and other improvements such as parameter sharing in the future."
735,"limitations in this section, we discuss some limitations of our work. we conduct preliminary experiments to verify the influence of task transfer and vocabulary expansion on language learning in complex forms, and to explore the effectiveness of our proposed architecture, symbolic mapping, and we assume that language was formed through simple interactions in the early stage. therefore, additional experiments involving more complex games or other input forms like real images have not been studied and are left for future work. besides, more advanced language properties and syntax are temporarily not studied in this work. as for task transfer, we verify the effectiveness of a two-stage curriculum starting from referential games, while more advanced curriculum are left for future work, where more cognitive science findings should be involved."
736,"limitations first of all, our study is limited to languages that use the latin script. still, the 4 languages are from different language families and are typologically diverse. secondly, the low-resource scenario is simulated. as mentioned in 3, in order to carry out the experiments the languages involved were required to have enough monolingual data to train lms, as well as available evaluation datasets for nlu tasks. the source of the pre-training corpora for swahili and finnish (cc100) is not completely comparable with the corpora used for basque and spanish (75% news, 25% wikipedia), due to the unavailability of a large curated corpus for swahili, and the lack of big news corpora for finnish with an open license that allowed us to share freely the pre-training data. 16https://github.com/orai-nlp/low-scaling-laws our study is limited to 3 language model sizes and 3 pre-training corpora sizes. including other model sizes like a bert-large or a model between 51m and 16m (where there is a big gap in results), and adding more pre-training corpora sizes (let’s say 625m and 1m words) were out of the scope of this work. in addition, we use the default hyperparameters that are commonly used for bert-base (bert124m ) for the pre-training and fine-tuning of the bert51m and bert16m models without any hyperparameter tuning."
737,"limitations this paper proposes a pre-trained language model with prompts for temporal knowledge graph completion. however, there are some limitations in our method: 1) our prompts in the temporal knowledge graphs, especially the time-prompts, are built manually. it needs to be reconstructed manually for different knowledge graphs. we are exploring a way to build prompts in temporal knowledge graphs automatically. 2) our model uses a random sampling method, which suffers from the problem of few high-quality training samples and high sam- ple noise. for future work, a more effective way to sample is worth exploring."
738,"limitations although the proposed method provides interpretations for continuous prompts with both faithfulness and plausibility, it can still only be used as an approximation to find the most likely combination, since the process of combining discrete prompts to continuous prompts is irreversible. moreover, the output layer of plms tends to degenerate and occupy an anisotropic cone in the vector space (wang et al., 2020; li et al., 2020b), which significantly increases the difficulty of finding the correct interpretations. we encourage future research to take the magnitude of token vectors and the tokens in their neighborhoods into consideration for a more robust interpretation. due to space and time constraints, we only perform detailed experiments on p-tuning and the bidirectional language models like bert and roberta, which ignored numerous sota works such as prefix tuning (li and liang, 2021), prompt tuning (lester et al., 2021) for continuous prompts and gpt (radford et al., 2019), t5 (raffel et al., 2020) for plms. we encourage future research to conduct experiments on more prompt methods and plms to investigate the generalizability of our method. ethical statement we propose a novel view to interpret continuous prompts, which have been considered ""black boxes"", as combinations of human-understandable discrete tokens. since the method itself is unbiased and faithful, and all experiments are conducted on publicly available datasets, we believe that our work does not create any potential ethical risk. further, we discover shortcuts latent in continuous prompts, implying that systematic biases or discrimination may also exist in continuous prompts. these biases may originate from training datasets which are exploited by continuous prompts as a shortcut to the acquisition of true labels, or even originate from artificially implanted backdoors. we hope this work will provide the possibility to detect these potential biases in continuous prompts. our created artifacts are intended to provide researchers or users with a tool for understanding decision-making and detecting possible unexpected shortcuts of continuous prompts, while at the same time offering the feasibility of cross-model transfer without extra training signals on target plms. they are compatible with the original access conditions. all use of existing artifacts is consistent with their intended use in this paper."
739,"limitations all our experiments are performed using the bertsmall language model due to the computational requirements of generating and testing models considering many configurations of adversarial training and attack methods. although using larger language models might have provided different performance measurements, our findings that compare input- and embedding-space adversarial training methods are expected to remain unchanged. another limitation of our work is the semantic gap between attacks in input and embedding space needs further research. specifically, how do perturbations in the embedding space get translated in the input space? finally, other forms of robustness techniques, besides adversarial training, in the context of large language models require examination."
740,"limitations gdmm establishes a compelling starting point for dmel research. in spite of this, the proposed approach has several shortcomings. first, gdmm currently generates entity name within the entity candidate set, however, we saw how retrieval errors limit entity linking performance. thus, how to work collectively with the retrieval system to diminish errors takes appropriate action. second, how to handle large tables still remains under-explored. it is infeasible to represent a huge database with the table flattening technique. in practice, it is possible to filter out less likely candidates to compress the search space, but a more promising approach is to represent the table more efficiently. gdmm also enables studies on more diversemodal tasks. new tasks can be easily framed based on the proposed architecture, such as visual question answering, grounded generation, and diversemodal commonsense reasoning. we believe that with more follow-up work on diverse tasks, this approach will turn out to be a more comprehensive generative diverse-modal framework."
741,"limitations one limitation in this work is the metrics employed in the automatic evaluation. the metrics mainly focus on the quality of generated response and the accuracy of emotion recognition, while automatic evaluation lacks a comprehensive method to evaluate empathy. another limitation comes from the utilization of the dataset designed for open-domain dialogue system, so that the generated response from the proposed framework is not task-oriented. in the future, we will build empathetic dialogue generation datasets with diverse and task-oriented response, and develop metrics to evaluate the understanding of the speaker’s situation."
742,"limitations the loss objective of the proposed siwcon regularization is computed on augmented data, which increases the time required for the model to complete training. we evaluate siwcon on classification tasks, but it may be applied to various other tasks, such as reading comprehension and textual entailment. more evaluations are expected to be done in future works. the proposed siwcon regularization is effective in defending against word-level adversarial attacks, as the basic elements of the augmentation methods are words. however, similar regularization techniques can also be applied to characters and sentences, and we leave evaluating the effectiveness of such variants in future works."
743,"limitations resulting from the use of gpt-3 discussed above, there are more general ethical questions surrounding the use of gpt-3 and similar models, for example the high energy usage and resulting carbon emissions, and societal questions around the oligopoly on state-of-the-art language models that is currently in the hands of a handful of large us-based companies. the second consideration relates to the task that we introduce: while we see perspective transfer models as a valuable tool for studying how language ‘frames’ (social) reality that could also have practical applications, for example in journalism, we strongly believe that any such applications must be approached with extreme care. the models that we introduce are scientific analysis tools that could be used to suggest alternative viewpoints on an event, but we believe that generations should not be seen as necessarily reflecting a ‘true’ or ‘better’ perspective, and should not used in a prescriptive way (i.e. used to tell someone how to write). we believe that the authors (journalists or others) of any text ultimately bear exclusive responsibility for the views, perspectives and (implicit) values expressed in it, and should be careful in making use of texts (re-)written by computers, such as the ones produced by our proposed models. finally, we are aware that our task domain (femicide/gender-based violence) is a societally and emotionally loaded topic, and that the texts contained in our dataset and produced by our models might be disturbing. in particular, in some cases, models may produce graphic descriptions of violence and/or produce questionable moral judgements (e.g., we have occasionally seen statements such as “the perpetrator of this horrible crime does not have the right to live” spontaneously produced by some of the models), and potential users of applications of the model should be aware of this. for the purposes of this paper, the only people external to the research team who have been extensively exposed to model outputs were the annotators in our human evaluation study. in the introduction page of our online questionnaire, annotators were warned about the sensitive nature of the topic and advised that they could stop their participation at any time if they felt uncomfortable and could contact the authors with any questions. prior to running the online questionnaire we have requested and obtained ethical approval by the ethical review committee of our research institution. author contributions authors g.m. and h.l. share first co-authorship (marked with ‘*’). g.m. had primary responsibility for data collection and preparation, setting up the gpt-3 experiments and running the human evaluation survey. h.l. had primary responsibility for the mbart experiments and the automatic evaluation. b.m. annotated data (pair alignment) and contributed to prompt engineering and the design of the evaluation questionnaire. m.n. coordinated and supervised the overall project."
744,"limitations explored above, we identify several potential risks with this paper. some may be offended by the images we include. we tried to mitigate this risk by including a warning in the abstract and not including images featuring genitalia. however we appreciate these images may contribute to the sexualisation and objectification of non-cisgender people, particularly if taken out of context. though we did not set out to generate offensive images (this would be counter to the models’ intended use, for example as specified by dayma et al. (2021)11), images from the full data set could 11https://huggingface.co/dalle-mini/ dalle-mega similarly offend and even be weaponised. they might accompany transphobic messages online. a data set of cisgender and non-cisgender images labeled by photorealism and presence of a clear face could feasibly be used to finetune a model to identify non-cisgender people (a concern raised by the community). as such, we make our image data set available only upon request; it is intended to measure the harm done to non-cisgender people, not contribute to it."
745,"limitations our comix approach assumes availability of parallel bilingual (embedded and matrix language) corpora and mature tools for pos tagging and phonetic transcription for both the embedded and matrix languages which does not hold true for every language. but these assumptions are reasonable for a large number of languages as shown in appendix 7. second, our current choice of guiding function for attention fdkga and mixing probability pmix are based on limited knowledge of the linguistic structure specific to english and indic languages, and might need to be adapted for other language families. additionally, as discussed in section 4, due to multiple variations in code-mixed generation, current automated metrics that compare system generated text with reference text do not provide a true reflection of a system’s ability to generate code-mixed text. lastly, as with large language models, our comix models are also vulnerable to biases inherent in the training corpus."
746,"limitations, future directions and"
747,"limitation section (section 9), smarter ways of multilingual s2st architectures are preferred in the future to reduce the cost of energy from current quadratic to linear number of models. generally, s2st circumvents traditional cascaded systems which concatenate asr, mt and tts with high latency and high requirements of datasets. there are 3,000 around languages in the world who do not have their own writing systems or textual vocabularies. through our duplex s2st models, we hope to be friendly to these languages so that more and more languages can be covered."
748,"limitations in this section, we illustrate the limitations of our method, which could be summarized into the following two aspects. firstly, since the cumbersome data annotation leads to few publicly available datasets of idrr tasks, we only conduct experiments on english corpora including pdtb 2.0 and pdtb 3.0. in the future, we plan to comprehensively evaluate our model on more datasets and datasets in other languages. secondly, considering that instances of pdtb are contained in paragraphs of the wall street journal articles, our approach ignores wider paragraphlevel contexts beyond the two discourse arguments. as shown in (dai and huang, 2018), positioning discourse arguments in their wider context of a paragraph may further benefit implicit discourse relation recognition. it is worth exploring how to effectively build wider-context-informed discourse relation representations and capture the overall discourse structure from the paragraph level."
749,"limitations although the proposed prequant achieves promising results especially in reducing the storage and computational resources, we discuss some limitations of our work in this section. in our experiments, we observe that the performance of prequant is highly correlated with the data size. when fine-tuning with very limited data, prequant may not meet expectation to preserve the performance of plms. moreover, our model performance also depends on the number of parameters (i.e. outliers) restored in the fine-tuning stage. this hyperparameter controls the trade-off between model performance and parameter efficiency. the optimal choice of the hyper-parameter for different tasks requires further investigation. additional"
750,"limitations while we strongly argue against empathetic conversational systems, there may be use cases – such as psychotherapy or educational chatbots – where validating a user’s emotions is, if not required, helpful in terms of their goal. in addition, while a lot of the work on empathetic responses we have discussed is intentional, generative models like chatgpt produce relatively uncontrolled responses that may well be unintentionally empathetic. as with toxic outputs, care should be taken to prevent these models from validating users’ emotions that cannot be understood."
751,"limitations one of the limitations of this work is that we restrict ourselves to examining datasets for supervised learning that contain relatively short instances of text. this likely facilitated the reweighting of data that we wished to perform as an intervention to produce the reweighted data that we study, as the short length of each text effectively capped the number of different lexical features that could cooccur in the same instance. the results we present here might not be representative of lexical feature bias in data with much longer units of text. also, the fact that the datasets that we used are all in english means that our lexical features were premised on simple whitespace tokenization with punctuation removal; for other languages with a larger variety of reasonable tokenization schemes at varying levels of granularity, the distribution of lexical features, and the resulting"
752,"limitations while the results of our experiments seem sufficient to validate the concept and our general approach to bilingual distillation, we have not carried out a detailed systematic analysis of alternative implementations of the various aspects of our methods, such as different student model initializations, distillation objectives and hyperparameter settings. furthermore, our bistil models are likely undertrained due to limited computational resources. consequently, we do not claim our specific implementation of bilingual distillation to be optimal or even close to optimal. areas that warrant further investigation toward realizing the full potential of this approach include the use of hidden dimension reduction, which yielded impressive speed gains for minilmv2 in our experiments, and other innovations in distillation such as progressive knowledge transfer (mukherjee et al., 2021). with the exception of improved efficiency, our bistil models inherit the limitations of the mmts from which they are distilled; notably, there is a discrepancy between the performance on high- and low-resource languages resulting from the distribution of data used during mmt pretraining. in this work, we have only considered english as the source language; some target languages may benefit from other transfer sources. future work may also consider the use of multi-source transfer, which would entail distilling with more than two languages. here the challenge would be optimizing the balance of model capacity allocated to source languages versus the target language."
753,"limitations aside from the reverse models used in backtranslation (which we did analyze in section 4), we only studied translation of language pairs into english. using data augmentation techniques like back-translation where english is not the target language, or is neither the source or target language is certainly worthy of study, but was out of scope in the present work. we did however, include many source languages that are typologically different from english (see table 8 in the appendix). in order to study the effectiveness of bt in a large number of languages we relied on extant multilingual datasets, namely flores101 and tico19. the direction of human translation when building these datasets was from english into another language. we did not run repeated trials on our experiments. many models required training for a couple of gpu-weeks on v100s, and additional trials would have added significant computational expense. we believe the trends we have identified are sufficiently clear and supported by the statistical analysis in section 4."
754,"limitations though our proposed method outperforms current state-of-the-art methods, there are still many challenges we should overcome in future research. first, for colloquial expression which confuses current dependency tree parser, we should come up with new solutions. second, emotional prediction of tweet posts describing current issues needs external knowledge, which is absent in existing research."
755,limitations our experiments demonstrate that it is possible to analyze company executive and analyst language during earnings calls and use it to predict future earnings surprises with reasonable accuracy that is well above random chance. we acknowledge that the dataset contains events that result in significant (in magnitude) earnings surprises so the performance numbers do not directly translate to a live trading setting in which many events do not result in material surprises. we also note that predicting future earnings surprises is correlated with but not equivalent to predicting future stock returns so more work must be done to translate our results into an actual trading strategy that is out of the scope of this paper.
756,"limitations while we carried out our research in four language pairs (in both directions), we recognise that these are mainly european languages and each pair is from or into english. the choice of language pairs was limited by the data and evaluation tools we had access to, however as our methods are languageindependent, this research could be expanded to other pairs in the future. another limitation is that the work was conducted in one domain (tv subtitles) and it remains for future work to investigate whether similar benefits can be achieved in other domains, though the findings within language modelling with cue in novotney et al. (2022) who used a different domain suggest so."
757,"limitations as we discussed in section 5.1, further work is needed to investigate whether the negative effect of full contextualization beyond static + positional embeddings at the input layer is an idiosyncrasy of the embedding transfer procedure, or if this is a true effect. in future work, an experimental setup that is closer to the training setup, such as masking only the recon token instead of all tokens and transferring the source could be adopted, in order to reduce the noise potentially introduced by the distributional change in the inputs. regardless, we believe that findings regarding the information content of the representation at the input layer (static + positional embeddings) are novel and meaningful, and the quantification method we propose for comparing two representations in terms of their predictive utility is a generalizable methodological contribution. we furthermore note that our attempts to conduct evaluation on newer masked language models were made challenging due to several technical issues in the library (e.g., masked language modeling being unavailable in deberta (he et al., 2021): https://github.com/huggingface/ transformers/pull/18674)."
758,"limitations and advantages of both methodologies for the two scenarios. according to our results, the supervised models show good id performance at the price of a significant drop in ood performance. in contrast, unsupervised zero-shot systems excel in ood settings but do not outperform supervised models in id contexts. a reasonable compromise between the two methodologies is the nli-fine-tuned method, which improves odd results compared to supervised systems and achieves good performance compared to the zero-shot approach in an id setting. in a situation where limited training data are available, the fine-tuned nli system has the advantage of achieving a good trade-off between id and ood performance, with less training data than supervised models. using a zero-shot nli-based system is preferable in situations where the final data distribution is unknown. furthermore, it requires less implementation time and no training dataset. our experimental analysis is not without limitations. we focused on emotive content (emotion classification and hate speech detection), therefore our results might not be extendable to other domains. emotions have a certain degree of subjectivity that can affect the annotation process by making data annotator-dependent. in other fields, this might not be the case. moreover, our analysis is limited to ten datasets that we believe are representative of the work in this field. however, many other datasets are available, especially in the hate speech domain, and a wider evaluation might lead to a more definitive"
759,"limitations although we carefully designed open-wikitable for complex open-domain table qa, there are some limitations since it is based on the existing datasets. first, ambiguous or erroneous samples from the original wikisql or wikitablequestions dataset may still lie in our training and validation set. as we mentioned in section 3.2, most of the equivocal samples were attributed to the ambiguity of the original question and excluded from the test set, but not removed. second, unlike semantic coverage of the questions is extended by decontextualization and paraphrasing, the coverage of the question remains in that the answer and logic to derive the answer in each question is the same. still, openwikitable demonstrates the potential for further research on open-domain qa over the table."
760,"limitations there are two main limitations to the present work. first and foremost is the computational cost associated with the present experiments. we present here results and analyzed gleaned over 10 runs, 7 pretraining regimens, 8 rl gradient propagation variants and 2 data sampling approaches, for a total of 1120 models. while training any one of our models is cheap (less than 3 hours on a single a100 nvidia gpu), the total number of models may pose a challenge for future replication studies and comes at an environmental cost. this also prevented us from selecting optimal batch size, learning rate, and so on for specific setups—as described in appendix a, we set these values globally prior to running experiments. this may affect results and impact"
761,"limitation should be further considered in the future. besides, restricted by our knowledge and efforts, the main experiments cannot cover all common tasks and all pre-trained models in nlp. relatively, the token- and sequence-level classifications have demonstrated attractive experimental performance on most nlp tasks. next, we also plan to extend w2cspace to more nlp tasks and find its more specific value."
762,"limitations while fixed input prarameterization (fip) enables performing prompt-dependent tasks efficiently, there are limitations that need to be addressed in future work. in particular, the current fip methods cause task performance degradation. moreover, the computational cost needed for the injection of prompts and the storage required to store the parameters of every injected model have not been extensively considered. for example, when considering previous conversation history as the prompt to be injected in a long-term conversation setting, fast injection may also be a requirement for real-world application. updating or adding a relatively small number of parameters (hu et al., 2021; wang et al., 2021) may be a potential avenue for addressing the problems."
763,"limitations while ice does not require fine-tuning on large amounts of data, it requires querying a powerful llm at inference time (we use gpt-3 for our experiments which has 175 billion parameters). this can be a pay-per-use model or an open-source model such as bloom. this makes a downstream system that uses ice reliant on an external dependency, which carries the risk of the external dependency failing. 3summeval annotations are all based on the source, and the src-to-hyp version of bartscore performs best across dimensions for this benchmark. we use this version for all dimensions, leading to identical scores. we format bartscore results unlike rouge-l because in theory bartscores can differ across dimensions for an arbitrary benchmark. relatedly, in this paper, we are limited due to monetary constraints in a variety of experiments we perform. for instance, we restrict ourselves to text summarization and use samples of benchmark meta-evaluation suites during some of our experiments. we leave the investigation of using ice for other dimensions and downstream tasks for future work."
764,"limitations this paper mainly focuses on the query-focused meeting summarization(qfms) task. besides, we have explored the performance of the rankergenerator framework on the long-input summarization task. but the results do not show a significant improvement. although qmsum dataset is also faced with the long-input challenge, the qfms task only summarizes specific parts of the original text, so it can take these parts as the input. while the goal of the long-input summarization task is to generate an overall summary, which needs to have a global view on the original text. so we think the extract-then-generate framework is unsuitable for the long-input summarization task. the previous work summn (zhang et al., 2022) is more suitable for the long-input summarization task. in addition, the multi-stage approach has a performance disadvantage over the end-to-end approach. however, the computational complexity of the multi-stage approach is much lower than that of the end-to-end approach. the multi-stage approach can balance experimental performance and computational complexity. so it is worthy of exploration as well as the end-to-end approach."
765,"limitations differences in experimental setup may make it difficult to accurately and fairly compare published results. for example, to prevent data leakage, we report validation performance at the end of training and do not perform early stopping. this is in contrast to most other papers which report peak validation performance. results reported for other methods are reproduced in the same learning environment as our method unless explicitly stated otherwise. this takes into account recent work demonstrating problems with fairly and accurately evaluating pet methods that use early stopping improperly (chen et al., 2022). although many pruning criteria exist in the literature, in this paper we only consider one pruning criterion. although not presented in this paper, experiments we conducted with various formulations of magnitude pruning did not produce better results. although prompt tuning is a popular pet method, we do not perform nas for prompt tuning to determine the most efficient positions for inserting prompt tokens into the input. pruning may or may not prove to be a successful strategy for this problem. other nas strategies exist in the literature besides pruning, such as evolutionary, reinforcement learning, and darts (liu et al., 2018). however, our pruning method seems to give a good trade-off between validation performance and computational expense."
766,"limitations in this work, we focused on problems posed in the hand-crafted humaneval dataset (chen et al., 2021). a potential pitfall of a curated dataset such as humaneval is that the results may not generalize to real-world scenarios where developers often deal with more complex problems and code bases (e.g, code with multiple dependencies across multiple files). to address this limitation, we originally explored the use of datasets mined from github. however, our experiments indicated memorization issues (e.g., verbatim generation of solutions to niche problem), potentially due to the sample code already being included in the model training set(lee et al., 2021). in practice, high quality code deduplication required to avoid this specific limitation is challenging. work by allamanis (2019) find that the impact of duplicate code can be severe, sometimes inflating model performance scores by up to 100%. furthermore, in our early pilot tests, functions extracted in the wild were found to contain insufficient context (e.g. absence of docstring) for even expert human annotators and isolating functional tests is challenging without heavy curation. further research is therefore needed to understand how our findings might generalize to a wider variety of deployment settings as well as research on designing diverse evaluation datasets. in addition, future work may also explore the impact of problem difficulty on the observed results in our study."
767,"limitations one drawback of the experiments presented here is the reliance on a constructed language. while we have tried to design a language that is as representative of natural language as possible, there may be additional statistical effects that are not taken into account. for example, it is unlikely that one language would capture all 29 phenomena presented here and that the process would be triggered enough times to produce a large enough corpus. how these findings extended to existing language corpora is an open question for future studies."
768,"limitations while dims makes the cross-entropy based family competitive with alignment based variants, it still falls behind one some cases. moreover, dims can improve the performance of models trained on raw data, but the best performance is still achieved when dims is applied on distilled datasets. therefore, dims still depends on an auto-regressive model for the best translation quality."
769,"limitations our method overcomes degeneracy in instructional videos under the assumption of the existence of textual instructional scripts describing the exact instructions of instructional videos. thus, our method is applicable to instructional videos having such recipe documents. however, we note that similar documents exist for various types of instructions other than cooking, such as topics in other datasets (alayrac et al., 2017), e.g., how to jump start a car, or change a tire."
770,"limitations due to the lack of data, our dataset is not comprehensive since it only consists of tang poems. our model may not perform well on unseen data in other forms. we plan to update the dataset in the future continuously. our evaluation metrics and generation results for the machine translation tasks are not certified by experts in classical chinese, so the results and"
771,"limitations our approach relies on the pre-trained language model performance. although using a graph neural network with the user-product graph helps improve the performance in sentiment analysis, the pre-trained language model still plays an important role in the task. if the pre-trained language model cannot obtain good results, it will affect the performance as discussed on the imdb dataset. furthermore, the graph density could affect the performance of gnnlm-gnns, as discussed in the experimental results. since gnnlm is built on top of gnnlm-gnns, gnnlm is also affected by the sparsity problem. as already reported, the density of the user-product graph on the imdb, yelp-2013, and yelp-2014 datasets are 0.06, 0.05, and 0.02, respectively. the greater the value is, the denser the graph is. comparing gnnlm with gnnlm-lm, we found that the improvements we could obtain on the imdb, yelp-2013, and yelp2014 datasets are 6.1, 5.0, and 4.8, respectively. the trend of improvement conforms with the density of the graph. therefore, if the user-product graph is very sparse, it would greatly affect the performance of gnnlm."
772,"limitations on the unknows. such efforts will lead to more accurate and reliable responses from llms, which will have a positive impact on their applications in diverse fields. limitations • generalization of reference sentences. at present, we have selected sentences with uncertain meanings exclusively from the gpt-3 and instructgpt series, potentially overlooking uncertainty present in responses generated by other llms. however, it is not feasible to catalog all sentences with uncertain meanings exhaustively. as a direction for future research, we propose to concentrate on the automated acquisition of more accurate reference sentences to address this concern. • limitations of input forms: our examination was confined to three unique input forms: direct, instruction, and icl. there is burgeoning research aimed at bridging the gap between models and human-like methods of reasoning and problem-solving, including but not limited to approaches like reflexion (shinn et al., 2023), tot (yao et al., 2023), mot (li and qiu, 2023). future endeavors will integrate additional cognitive and decision-making methods to delve deeper into the self-knowledge exhibited by these llms."
773,"limitations our work has two limitations. first, we update the cluster centroids at each step during training, which requires a large mini-batch to maintain clustering accuracy and consumes more gpu memory. second, our method still may not identify false negatives accurately, as we use the training model for coarse-grained clustering rather than a well-trained model. we leave the improvement of memory consumption and further improving false negative discrimination for the future."
774,"limitations this paper proposes five novel contrastive losses for multi-label text classification tasks. however, our method has the following limitations: 1. we only selected the multi-label emotion classification task and multi-label news classification as the representative of the multi-label text classification tasks. 2. we only conduct experiments on the single modal of text, and have not extended to multimodal tasks. 3. our method chooses the spanemo model as the backbone, lacking attempts to more models."
775,"limitations of the existing methods, thus improving the accuracy of the kb-rec task. in future work, we will explore more fine-grained information in expressions and combine it with knowledge and visual content. limitations to better understand the limitations of the proposed method, we conducted an error analysis by randomly selecting 100 incorrect predictions and categorizing their error types. the results revealed that 32% of errors were caused by grounding issues, specifically, an inability to distinguish between multiple objects of the same category, despite having knowledge category of the referent object. the results indicate that there is a need for improvement in the ability to discriminate visual objects, especially for object categories with long-tailed distributions. additionally, the results show that 20% of errors are due to imprecise object detection, particularly for small objects. this highlights the need for optimization of the visual encoder and loss function. moreover, 14% of errors are attributed to incorrect knowledge retrieval. to address this, incorporating more fine-grained information in expressions for retrieval should be considered as a future research direction. furthermore, 34% of incorrect predictions can be attributed to issues with the ground-truth annotations, which may negatively impact the model’s learning process."
776,"limitations of the existing ea method, the lack of interaction and heterogeneous embedding spaces, we propose a unified textual entailment framework for entity alignment called tea. we transform the origin relational triples and attribute triples of an entity into textual sequences and model the ea task as a bi-directional textual entailment task between the sequences of cross-kg entities. we propose two kinds of plm-based aligners to capture the fine-grained correlation between entities with two kinds of sequences in a unified manner. the entailment probability is used for measuring entity similarity and ranking the entity candidates. experiment results on five cross-lingual datasets show that tea outperforms existing ea methods and enables the mutual enhancement between the heterogeneous information. limitations despite that tea achieves some gains for ea, tea still has the following limitations: first, tea has a higher computation cost than the embedding-based ea methods in the re-ranking phase, since tea process entity-pair input for modeling the interaction between them. for reducing time costs, we adopt the confidence-aware reranking strategy to reduce the number of re-ranking samples and candidates. however, the inference time cost is still higher than the embedding-based methods. in addition, the candidate selection may be limited in some corner cases if the ground truth entity is not ranked in the top |c| similar entities calculated by entity embeddings. we will further explore efficient approaches which could cover the corner cases. second, the alignment of relational information of tea requires the entity names to construct sequences. however, the entity names are not always available in some ea datasets, such as the wikidata kg in openea benchmark (sun et al., 2020). in that case, tea can use the attribute sequences without entity names for entity alignment. though tea w/o t r can achieve competitive performance as shown in table 3, it still limits the application of tea. we will further explore plm-based ap- proaches to align the relational information without the requirement of entity names."
777,"limitations our evaluations rely on the masked language modelling task as it was a convenient task to conduct our experiments and following up on similar related works. to apply it to models trained differently, e.g., models of the gpt class, one needs to develop comparable appropriateness measures, which is a general desiderata of the field. we evaluated the models through prompts. we used a fixed set of prompts, and others could produce better results for each of the tasks at stake. even if one could find some working prompts, this may not be ambitious enough, however. it would show that the relevant information is present in the system, say about hyponym-hypernym pairs. but the tests we are proposing rely on the idea that models should work well consistently, across tasks, and across prompts. with our zero-shot prompting method, we tested the pre-trained models. one could imagine ways to fine-tune these models to our requirements. our goal here was to first make visible that the groundless training might have been sufficient to encode a consistent semantics, and yet that it did not. in future work, we also hope to develop quantitative measures of success and consistency (starting from the statistical models in appendix d), consistency measures which compare parallel performance over more tasks at the same time."
778,"limitation, which improves the trigger detection rate. (cf. fig. 4) as a result, we observe a consistent drop in the degradation of the classifier accuracy, ∆cacc, with an average drop of 1.45%, particularly on the sst-2 datasetfrom 7.39% to 1.77%. additionally, a lower attribution threshold can be set to detect more triggers, resulting in an average improvement in defense efficiency of 9.61%. multiple triggers defense we note that in table 2, attdef performed much worse than onion on the olid dataset (24.37% vs. 63.5%). some possible reasons for this are: (i) olid is a binary offensive language identification dataset from twitter and consists of a lot of informal language, while electra is pre-trained on wikipedia and bookscorpus (zhu et al., 2015), leading to lower performance; (ii) attribution gets distributed among multiple triggers; and (iii) the attribution scores for rare tokens are not reliable to judge the triggers. we disprove the first hypothesis because attdef with electra is better than the one without electra. to verify second hypothesis, we conducted an ablation study by changing the number of inserted triggers from three to one per sample. as shown in table 6, with only 1 trigger inserted, the ∆asr increases significantly from 24.37% to 60.73%, though it is still worse than baseline 69.03%. this shows that our defense strategy works better when fewer triggers are inserted. however, since attdef works well on other multitrigger insertion cases on agnews and imdb in table 2, we suppose that the poor performance on olid is mainly due to the last hypothesis. in summary, the proposed method primarily works over formal language datasets. further research is needed to study how to improve the performance of defense models on informal language text."
779,"limitations there are several limitations of the proposed methods. (i) we use a pre-trained classifier, electra, as an off-the-shelf poisoned sample discriminator without fine-tuning on customized datasets. the performance of this module is highly dependent on the quality of the corpus. (ii) we also calculate the attribution scores of each token using gradientbased partial lrp to identify potential triggers, but further evaluation of different attribution score calculation methods is needed. (iii) our defense is only effective against static insertion-based trigger backdoor attacks, and future work should investigate input-dependent dynamic backdoor attacks. (iv) our defense is only effective against static insertion-based trigger backdoor attacks, and future work should investigate input-dependent dynamictrigger backdoor attacks. ethical consideration in this paper, we present a defense mechanism to counter the impact of backdoor attacks. our code and datasets will be publicly available. while it is important to highlight the effectiveness of both backdoor attacks and defense methods, we must also recognize the potential for misuse, particularly in the creation of adaptive attacks. however, by making our defense strategy and implementation public, we may expose our method to attackers, who may discover its weaknesses and develop new types of attacks."
780,"limitation of onion to launch a strong defense, as shown in table 2. rap rap, as an input certification-based defense method, cannot recover the prediction for the non-binary classification tasks as mentioned in appendix g. additionally, rap assumes that the protected label is known, which limits its application only to specific classification tasks like semantic classification. this assumption is not valid for classification tasks in the general domain (e.g., topic classification on agnews dataset). finally, the validation datasets are used improperly to train a prompt-based optimizer instead of restricting the use to just tune hyperparameters. acl 2023 responsible nlp checklist"
781,"limitations a major limitation of activeaed is that it is significantly more compute-intensive than other scoringbased aed methods such as aum or dm. this is inherent to the proposed method because the ensemble requires training of multiple models and, after receiving human feedback, the full ensemble has to be re-trained. also, the ensembling of activeaed requires more training runs than trainingdynamics-based aed methods. however, most model-based methods require a cross-validation scheme (klie et al., 2022). the ensembling component of activeaed is more data-efficient than these approaches, because it makes use of the training dynamics captured during cross-validation instead of discarding them. a second limitation of this work is that while we chose baselines that performed strongly in klie et al. (2022), they represent only a fraction of the scoring-based aed methods described in the literature. finally, our evaluation is limited to a single language model and it would be interesting to investigate how activeaed interacts with larger language models than distilroberta."
782,"limitations our method of identifying important words requires a dataset for a semantic task (in our case nli or pi), which limits its applicability. this requirement also prevents us from generalizing our observations too broadly: we tested our method only on one high-resource language where both dependency parsers and nli / pi datasets are available. our analysis also lacks the comparison to other indicators of word significance."
783,"limitations since the propsegment dataset feature entailment labels for all propositions in a document, the label distribution are naturally imbalanced, which would potentially pose challenge for modeling. we observe low presence of contradiction examples in our dataset construction process, which could be a limiting factor for the utility of the dataset. unlike previous nli datasets (bowman et al., 2015; williams et al., 2018), we speculate that reference determinacy, i.e. whether the hypothesis and premise refer to the same scenario at the same time, cannot be certainly guaranteed and safely assumed in our case, which in part leads to low presence of contradictions during annotation. we offer a detailed"
784,"limitations we note that the datasets used in this work solely represent social media interactions from reddit, which is known to have a demographic bias toward young, white, american males3. furthermore, systematic, spurious differences between diagnosed and control users can prevent trained models from generalizing to other data. future research on other social media and datasets is needed to determine to what extent the presented findings are generalizable to broader populations."
785,"limitations the proposed approach focuses more on logical reasoning on explanations for zero-shot classification. the semantic structures in explanations, such as inter-entity relations and event argument relations, are less touched (although the pre-trained language encoders such as bert provides semantic matching ability to some extent). within the range of logical reasoning, our focus are more on first-order logic, while leaving the"
786,"limitations our work overcomes visual noise data that limit extraction performance, incorporating multi-modal knowledge of different levels. empirical experiments demonstrate that our method avoids noise data misleading the mmre model. however, there are still some limitations of our approach can be summarized as follows: • due to the limitation of the existing mmre datasets, we only experiment on two modalities to explore the influence of image features. we will study more modalities in future work. • our method neglects the multiple relations for an input, which may not consider the multiple semantics of entities. we leave the multiple relation extraction method for future work."
787,"limitations compared to the empirical risk minimization scheme, the introduced self-regularization scheme incurs certain overhead because each mini-batch of data will go through two models. for bertbase scale pre-trained language models, the additional memory overhead is about 27% and the additional training time overhead is about 30%. nevertheless, once pruned, the sparsified model can enjoy considerable efficiency gains in terms of storage and inference time. therefore, this is a trade-off that future practitioners might need to consider."
788,"limitations we inject the medical knowledge graph into local texts for entity span representations enhancement. however, unlike most joint extraction methods, the proposed model is hard to be trained in a parallel way. therefore, it is time-consuming to obtain a well-trained model. we would like to optimize the architecture of the model in the future. moreover, our model is adapted to chinese medical texts where a token usually means a character (not a word). hence, there will be errors when aligning the entities from the knowledge graph with mentions from local texts. word segmentation task will be considered in our future work."
789,"limitations we observe that although coaug outperforms baselines on multiple datasets, it is still prone to errors that emerge from the bootstrapping process. specifically, our framework utilizes models to augment weak labels to the training set, and if the proposals are extremely noisy, training on noisy examples in future iterations will further exacerbate the ability of the framework to identify entities with high precision. incorporating constraints to preserve the quality of the pseudo-labeled data (shrivastava et al., 2012) is an exciting direction for future work in low-resource named-entity recognition."
790,limitations our work has a number of important limitations that affect the
791,"limitations our approach is heavily dependent on the quality of the pre-processed orthographic-phonemic transcription data as it provides the ground-truth for unsupervised alignment objectives. generating phonemic transcriptions and aligning them correctly with orthographic representations can be costly. despite our significant efforts, the alignment is still far from perfect optimality. secondly, our approach might not be effective in improving performance on randomly chosen language pairs. as our framework aims to exploit phonemic similarities of languages with different orthographic representations, the methods are only effective in cross-lingual transfer between lexically similar languages in terms of phonology such as cjkv languages. languages that do not fall into this category might observer little to no performance gains with our proposed framework."
792,"limitations the experimental results suggest the possibility of our mtl-based qe approach being biased towards the word-level qe task, as the jointly trained qe models show better performance improvements for the word-level qe task as compared to the sentencelevel qe task. further, we also observe that our approach does not work well for language pairs with english as a source language (en-de and en-mr). the qualitative analysis of the english-marathi mtl-based qe model shows that the model performs poorly when inputs are in the passive voice. our multi-pair setting experiments use all seven language pairs. we do not consider properties like the similarity between the languages, translation directions, etc., to group the language pairs. so it may be possible to achieve comparable performance using a subset of languages. we choose the nash-mtl approach for mtl-based experiments because it has been compared with around ten other mtl techniques and it has been shown that the nash-mtl approach outperforms them on different combinations of the tasks. in the current work, we have not experimentally analyzed how the nash-mtl approach gives better improvements than the ls-mtl approach."
793,"limitations the major limitations of our work are three-fold: (1) in the empirical experiments, we only train and evaluate models on english datasets. as the analyzed pitfalls are essentially language-independent, we believe the empirical"
794,"limitations while the proposed method performs well on four benchmarks, we discuss some of its limitations. on one hand, as discussed in §5.5, our method is not accurate enough to predict sentences at the end of the documents. there may be some better strategies to construct training samples so that the model can better take into account each part of the documents and make more accurate predictions. on the other hand, our model is not pre-trained with more diverse domains and larger scale data. our datasets are limited to two types, i.e., paper abstracts and short stories, both of which have comparatively obvious order characteristics. in addition, we do not use some larger scale datasets, such as nsf abstracts and arxiv abstracts, because of computation and time constraints. with more diverse and larger data, the performance of our model should be further improved."
795,"limitations our proposed multimodal grammatical error correction (gec) model is based on a seq2seq generative framework, which utilizes different encoders to extract information from each modality, and then fuses them to provide input to an autoregressive decoder. however, in this work, we did not explore the use of a sequence tagging framework, which may be a consideration for future research, as it has the advantage of faster decoding speed. additionally, this study focuses on the use of audio representations of the source-side of gec data, rather than the target-side, to construct multimodal gec data. our further analysis concludes that our proposed multimodal gec model has limitations in correcting certain minor error types (e.g., adv, conj, punct, and word order) when compared to text-only gec models."
796,"limitations the limitation of our method comes from the requirement to identify simple words in simple texts in section 2.1. the deepblueai we have used is a deep model, meaning it takes much time when inference. in our experiment, it takes 362.78 seconds to identify simple words from 10,000 sentences with an average length of 8.12. we expect that there will be methods with higher identification accuracy and higher inference speed in the future. due to page limitations, we have placed the related work in appendix a and the ablation experiments in appendix c. due to time constraints, we do not perform a human evaluation of the output of llms. we hope to conduct a more comprehensive evaluation of the performance of llms in the future."
797,"limitations as we only used english framenet as the dataset for our experiment, it is unclear how well our method would work with other languages or corpora. however, because the method is neither language- nor corpus-specific, fine-tuning may lead to better results with other datasets. also, the method relies on a semantic frame knowledge resource, and annotation will thus be required if it is applied to languages without such resources. this study only considers core frame elements and does not show results for non-core frame elements."
798,"limitations several main limitations exist in our study in its current form. first, our reported results only simulated experimental participants by manipulating the temperature hyperparameter. we compared this approach with natural language prompting for experiment 1, but that prompting did not increase ""participant"" diversity, so it was abandoned. moreover, approaches for simulating psycholinguistic experimental ""participants"" could go far beyond what was tried here; our prompting method was relatively limited, and more detailed prompting could be included in future experimental simulations. second, making a direct comparison with actual psycholinguistic experiments might not be the only method to investigate llms’ discourse capacity. a comprehensive list of discourse probing tasks might play a similar role despite a different way (koto et al., 2021). third, this study is strictly behavioral: limited by both computational resources and obscure mechanisms of in-context learning, we do not dive into models’ internal representations in our analyses."
799,"limitations this work focused on experiments on english datasets and did not explore other languages. while we expect that our assumptions hold across languages and the proposed methods and metrics can be applied without any further modifications to other languages this has not been explicitly verified. additionally, we ensured to experiment with counterfactual editors that are representative of the main counterfactual editing methodologies, however we did not exhaustively cover all publicly available editors as our main goal was to demonstrate that our proposed method is widely applicable rather than to exhaustively compare editors."
800,"limitations this study suffers from four limitations. the first limitation is that even though we annotated 850 dialogues manually, which includes almost 200,000 dependencies, there is still room for improvement in the total number of labeled dialogues. the second one is that our parsing method of inter-edu in the inter-utterance situation is simplistic and straightforward, and it can not cover certain difficult labels. it is desirable to propose a more elegant and comprehensive approach. the third is somewhat analogous to the second. in the future, we should propose an end-to-end method that replaces the current approach, which consists of several processing steps. the last one is about our pseudo-labeled data selection method. it could be interesting to investigate the iterative process."
801,"limitations to facilitate future research, we analyze the difficulties and possible solutions in this new area. (1) as we present extensive empirical results and address the weakness of clip on vocabulary expansion, its theoretical risk on open tasks is urged to be investigated. (2) the current evaluation protocol is an approximation of the real open world. an evolving benchmark could facilitate future research. (3) for various visual categories, their degree of abstraction, the ease of describing them in natural language, and their density in the data distribution can also influence the extensibility and stability of models, which are worth studying."
802,"limitations in section 6.4, we already discussed the limitations and challenges of the model proposed (e.g., the model has access to less contextual information from the conversation history, errors can propagate more easily as it does not re-predict the entire cumulative state at each step, and mistakes could only be fixed by explicit delete or update operation). in the following, we concentrate on the limitations that refer to the scope of this work. languages. we experimented with a limited number of languages (english) and datasets (multiwoz 2.1, 2.2 and 2.4). we do not have experimental evidence that our method can work for other languages, including languages with a richer morphology. still, our system has been built without any language-specific constraints or resources, other than the t5 checkpoints and the manually annotated training set. our method can be applied to any other language (without modification) for which these resources are available, or by applying crosslingual techniques and resources (e.g., multilingual language models, translation/projection of the training set) to transfer to other languages zero-shot. in those cases, the expected quality is lower, but the efficiency advantage of diable remains. models. we experimented with two models (t5v1.1 base and large). this is due to the restriction on our computational budget to be both economically- and environmentally-friendly, which made it infeasible to conduct thorough experiments using larger-scale language models. however, we re-emphasise that diable allows one to easily plug and play arbitrary language models and the efficiency advantage of diable remains. diversity in the evaluation dataset. we experimented with three different versions of the multi- woz dataset (2.1, 2.2, and 2.4). although this is the current benchmark for dst accepted by the community, and we followed the standard evaluation methodology and metrics, we are aware that the results presented might not be directly generalisable to other datasets or real-world scenarios with a considerable data shift with respect to multiwoz. additionally, multiwoz has a certain level of noise and this can have an impact on the evaluation and the generalisation capabilities of the model trained."
803,"limitations in this section, we describe the limitation of our proposed method in terms of data augmentation and the way to combine contrastive learning with cycle adversarial training. first, our data augmentation strategy relies on the reconstruction ability of cycle adversarial training. we believe that more data augmentation strategies for topic distribution will be studied. second, with the symmetrical structure of cycle adversarial training, it is worth exploring how to optimize the encoder e and generator g through contrastive learning simultaneously. we can extend the proposed framework to a conjugated structure in future work. moreover, although it has been explored that contrastive learning and cycle adversarial training working synchronously performs better, we believe that more sophisticated training strategies will be designed to further improve the performance of topic modeling."
804,"limitations since our contrastive framework requires the probability mass of various summaries given a source document, there is an extremely large consumption of gpu memory even if the batch size is small, which limits the scale of contrastive data and suppresses the potential of our method. meanwhile, due to limited sample points (i.e., probability mass), our bootstrap re-sampling procedure is susceptible to outliers and cannot fully take advantage of this algorithm. in addition, like most abstractive summarization systems, our model does not attach importance to controllable text generation (hu et al., 2017; prabhumoye et al., 2020; he et al., 2020), which means that the generated text might contain redundant and incorrect information. ethical considerations while there is limited risk associated with our work, similar to existing abstractive summarization systems, there is no guarantee that the generated summaries are factually consistent and free from hallucination (maynez et al., 2020; kang and hashimoto, 2020). therefore caution is imperative when our system is applied to practical projects."
805,"limitations our work has two limitations that may impact the generalization ability of our proposed framework. firstly, in the clause generation section (sec. 4.3), we deduce logic clauses involving a fixed number of atoms, represented by ⌊5k×β⌋, rather than variable length for each iteration of gcn. while this approach has demonstrated superior performance on the multimodal misinformation detection and sarcasm detection tasks, it may harm the generalization of our framework to more complex multimodal misinformation tasks, such as the detection of fake news that involves various modalities, including social networks, text, user responses, images and videos, as discussed in (zhou and zafarani, 2021; alam et al., 2022). secondly, in our work, the incorporation of logic into the neural network relies on the use of product t-norm to differentiate logic operators (i.e., ∧ and ∨). however, as shown in the ablation study (sec. 5.4), product t-norm may lead to vanishing gradients with the increase of logic atoms during the training stage, which may limit the ability of our proposed framework to handle more sophisticated scenarios. we plan to address these limitations in future research."
806,limitations all our experiments and the
807,"limitations while we do include multilingual datasets in our experiments, our error analysis is limited to languages of the indo-european family, specifically english, french, and spanish, as these are the languages covered by our datasets which we can confidently analyze. in addition, it is possible to question some of the assumptions made in our theory, which should be kept in mind when considering our work. for example, we assume that, for each content word token in a discourse, there exists a single concept which that word is intended by the sender to express, regardless of whether it appears unambiguous to the receiver. however, unlike in mathematics, theoretical assumptions may not always hold in practice; for example, puns often exploit multiple meanings of a word for humorous effect. while such cases are not frequently considered in lexical semantics, we can expect exceptions to almost any assumption or"
808,"limitations and future work we acknowledge the limitations of lm-toast in few-shot calibration training. from our pilot experiments in sec. 3.1, we observe that a significant amount of data points are needed for the calibration task training. in lm-toast, we effectively utilize the whole training set for the calibration task. some learning paradigms assume only a small number of annotated samples at first, which limits the effectiveness of lm-toast. for example, in active learning (zhang et al., 2022; schröder et al., 2022), only a very small number of samples are available at the beginning most of the time, and models need to rely on them to find informative unlabeled samples to annotate. the vanilla confidence scores can be effectively utilized in this setting. however, lm-toast may not learn the calibration task well given very limited samples, resulting in poor performance in searching informative samples. we plan to investigate the calibration task in the few-shot setting and bring out the potential in lm-toast to make it suitable for more tasks."
809,"limitations intapt adopts a prompt tuning method which utilizes the inherent information from pre-trained models that already shows good asr performance on l1 english speakers. therefore, in order to apply our method we need a pre-trained model that already has good performance on a specific task which might not be available for other langauges. also, our method might potentially need sufficiently large pre-trained model size in order for prompt to utilize the internal information of the model."
810,"limitations since the proposed method scales probability distribution, not shift, the proposed idea does not change an output when coupled with a greedy decoding strategy. greedy decoding simply takes argmax of probability distribution at each time step, and hence, the output with or without the local temperature scaling will not be changed within greedy. therefore, the proposed idea is required to be utilized with a beam search, or a variant of it."
811,"limitations in our experiments, the most significant limitation is the lack of computational resources. experimental results in this paper and previous work(saha et al., 2022) have shown that a larger scale of models could lead to higher structural and semantic accuracy of explanation graphs in this task. constrained by computational resources, bart-large is the largest model on which we can perform the complete process of experiments. we believe that graph generation would be better if sufficient resources were available to perform synthetic data based pre-training on a larger model. in addition, since the evaluation metrics for graph generation tasks are incomplete yet, we can only evaluate a few samples manually outside of the metrics of the dataset, which is more subjective. with more evaluation methods with standardized processes proposed, the results of the experiment will be evaluated more objectively."
812,"limitations we think the limitations of our work are three-fold. (1) as discussed in section 2.1, we employ existing cgec models to select sentences for annotation when building the media and thesis domains of nasgec. although this reduces annotation costs, it inevitably introduces biases into our dataset. for instance, the proportion of complex syntax- or semantic-related errors may be lower than that in reality, since existing cgec models fail to identify them. note that although we manage to mitigate such biases by voting with multiple models, this issue still exists. future work should explore how to automatically mine erroneous sentences from a low error-density domain with minimal biases. (2) the current size of our dataset is relatively small. we will continuously collect more data from more diverse domains. compared with other domains, thesis has a much smaller data size (1.5k), as authorized papers are hard to obtain. in the future, we plan to cooperate with universities and thus accumulate more authorized data to enrich this domain. (3) based on our multi-domain nasgec, we have reported and analyzed cross-domain performance preliminarily. however, besides fine-tuning with small-scale data in the target domain, many other potentially helpful domain adaptation techniques can be tried. we believe cross-domain gec is a valuable research topic and encourage future work to study it with nasgec."
813,"limitation one limitation of this paper is that we do not validate fedpetuning on large scale models, (e.g., t5 (raffel et al., 2019), llama (touvron et al., 2023), vicuna (chiang et al., 2023), etc). although the parameter efficiency of petuning is more impressive on larger pre-trained models, we have to consider the limited computational resources of clients in fl, making it challenging to deploy such a large-scale model. in addition, with the increasing size of modern pre-trained models, the community needs to design fl-friendly petuning methods. in this sense, our work can serve as a benchmark and guide for future exploration of petuning in fl."
814,"limitations there are two limitations of the current mixpave model. first, although mixprompt can achieve comparable extraction performance with full finetuning, how to identify the optimal combination of the two prompts is challenging and remains unanswered. we conduct grid search in our experiments to empirically find the best prompts length. in future, we plan to investigate a systematic solution for identifying the optimal or a suboptimal combination. second, our model learns attribute-specific prompts for a new attribute. we plan to explore a parametric network that could guide the learning of attribute-agnostic prompts."
815,"limitations apart from the effective attack performance against multi-exit bert, we acknowledge that our work has several limitations. firstly, we only evaluate our slowbert on the glue benchmark (wang et al., 2018a), which demonstrate the effectiveness on alphabetic languages such as english. however, for logograms (e.g., chinese), it requires to design language-specific method to generate the corresponding substitution set to achieve the attack goal. secondly, despite we present a new security threat against multi-exit bert, potential defenses should be analyzed such as adversarial training (geng et al., 2021). for the above mentioned limitations, we leave them as the future works of our proposed slowbert."
816,"limitations as with other prompting methods, preadd’s performance may vary depending on the exact wording of the prompt, and may require manual prompt design to achieve the best possible performance. additionally, compared to more basic forms of prompting, preadd requires accessing the base language model’s output logits at each step of decoding, which can be inconvenient with certain apis such as the openai gpt3 api (although preadd is technically runnable through the gpt3 api, it will be less computationally efficient). with respect to the actual performance of preadd, a rare but nontrivial failure mode is setting a high α parameter. in particular, setting α to have a magnitude of above 2.5 tends to result in degenerate continuations. this is due to how the output logit distribution shift induced by preadd may significantly increase the logits of “nonsensical” tokens. the issue seems to appear predominantly in positive control applications of preadd (e.g. our sentiment control task), wherein the logit distributions “spike” more and have higher entropy. however, logit distribution truncation methods (e.g. top-k and/or nucleus sampling) can be used in preadd to alleviate text quality decay by eliminating nonsensical tokens prior to applying the control. in this work, we evaluate toxicity using perspectiveapi as a convenient automatic metric, but we acknowledge that perspectiveapi is not a perfect measure of toxicity. for example, it may be biased against african-american english, and may fail to capture certain types of harmful outputs (mozafari et al., 2020; elsherief et al., 2021). overoptimization against perspectiveapi could lead to unexpected side effects or biases in model outputs (jacobs and wallach, 2021; xu et al., 2021). additionally, although controlled generation methods like preadd may reduce the toxicity of generated continuations in the presence of highly toxic prompts, they may still struggle to explicitly counter the original toxic language in the input. for our gender bias reduction task, we have focused only on occupations as provided in the winobias dataset. there are of course innumerable other types of bias which are important to mitigate, ranging from gender bias in facets of language other than occupations, to other types of bias such as those based on race or age; blodgett et al. (2020) provide a more complete"
817,"limitations the limitations of our work are as follows: (1) as we construct eventoa for a new task by manually annotating, the data size can be further extended. (2) our study is limited to english sources, and we hope work can pay attention to event ontologies in other languages. building datasets for multilingual event ontology alignment would have a positive impact on applications in other languages beyond english."
818,"limitations as a preliminary study, our proposed classifier decomposition framework focuses on the first training stage of cre models with a lack of explorations on stage 2. besides, more experiments can be conducted by combining our framework with previous leading cre models, which we leave for future research. in addition, our work only focuses on strategies with the ffn layer. as the bert encoder is the main component of cre models, we call for more attentions to the research of improving encoder representations in the first training stage."
819,"limitations we used the asap data set to evaluate the performance of the proposed method. although the dataset is well known and widely used, it has two major limitations. at first, the data size is small. even with pre-training the model with a decently large data set (e.g., wikipedia), the interpretation of experimental results are limited by the data size. the second limitation is an inherited bias in the data set. since the asap data set is labeled by human raters, the data set is biased by personal preferences. at last, the proposed approach requires a reasonably large pre-processing to extract all the additional features which hinders a scalability. additionally, our work is limited to only measure creativity in expression but not in content."
820,"limitations there are two limitations: (1) although mt r include three types of reasoning types (deductive, inductive ,and defeasible reasoning), we only focus on relation reasoning task. for other tasks, it is also necessary to construct more datasets with the fusion of multiple reasoning types. (2) our primary focus remains monotonic reasoning, however, the combined reach of deduction and induction is only the tip of the iceberg of human reasoning (choi, 2022). this also inspires us to focus on more nonmonotonic reasoning and more logical combinations."
821,"limitations the proposed dataset is annotated for verb metaphors in particular. however, other lexical units including adjectives and adverbs should also be studied in order to truly understand the role of metaphors in news. it is important to examine the diversity of the generated ideas when performing metaphor generation. in this study, we proposed a simple approach to cluster words using wordnet. however, the metric is far from perfect and can be improved. for the task of candidate generation, we performed word masking to generate metaphorical and literal substitutes as we were curious about the ability of llms to generate relevant metaphorical mappings while preserving the underlying semantic idea. direct substitution of metaphorical candidates resulted in syntactically incoherent sentences in a few cases. it may be better to paraphrase the sentence after selecting the metaphorical mapping (ottolina and pavlopoulos, 2022). ethical concerns and broader impact we created the dataset from a publicly available news headlines dataset. this ensures that data is free from (a) anonymity concerns, (b) obscenities and (c) any stereotyping or bias. as the task is cognitively intensive, we only assigned at most 150 headlines to each annotator. all annotators were duly acknowledged and appreciated by nvidia ai technology center for their contribution. the original dataset of news headlines is the under apache license 2.0. we are thus permitted to modify and redistribute it. generating metaphors carries concerns due to the implicit potential to craft misleading text. the usage of metaphors has been shown to resonate emotionally with readers (citron and goldberg, 2014). this should not be a concern with our data as we only release generated candidates that preserve the underlying semantic meaning of the source headline."
822,"limitations sample separation based on model predictions can only eliminate part of the noise, and it costs extra time in training. moreover, although our dynamic gce loss based on prediction entropy works well in distantly-supervised ner, eq. 4 is determined mainly because it has superior experiment results and it lacks theoretical proof."
823,"limitations as described in section 4.4.3, the proposed model requires precise prediction on the target domain at each dialogue turn to utilize the relevant slot information within the service schema. this possesses a difficulty to apply the proposed model to some new domains, especially when the domains share high similarity with seen domains. to reduce this difficulty, as a future improvement, one potential approach is to process on a hierarchical structure of slots within schemas, instead of individual slots. in this way, the model does not need to rely on the domain, but only on a group of similar slots. the model can then perform matching of a chosen group of slots with available slots within a schema and composes responses based on those slots."
824,"limitations the most important limitation of our work lies in the size of the havqa dataset. however, substantial further funding would be needed to resolve this. for the baseline multimodal experiments, we did not use the image directly but resorted to extracting textual tags and including them in the text-only translation input. a tighter fusion technique may give better performance. 14https://github.com/shantipriyap/hausavqa/ tree/main"
825,"limitations by manual analysis, we found that claim-dissector suffers from overconfidence in blocks with at least 1 relevant evidence. then it seeks to select more relevant evidences inside, even when they are not. we believe this is connected to how irrelevant negatives are mined in fever — they originate only from blocks without relevant evidences. on real data, the system often struggles to recognize what facts are refuting, and what are irrelevant (especially when applied out-of-domain). we demonstrate this in a case study on downstream application, where we replaced retrieval on wikipedia with news-media in test-time. we tried to verify the claim ""weapons are being smuggled into estonia"". our system discovered article with facts about ""weapons being smuggled into somalia"", and used it as a main refuting evidence to predict refute veracity. lastly, cd is trained with evidence from wikipedia, and do not considers other factors important for relevance assessment in practice, such as credibility of source, its reliability, or its narrative. this is the area of active research, as human fact-checkers also need to deal with lies (uscinski and butler, 2013)."
826,"limitations there may be some possible limitations in this study: 1. discourse schema. it should be acknowledged that web documents are heterogeneous themselves and a unified framework to accommodate all web documents may be infeasible. in this article, instead of pre-define the domain/type of web documents we target at, we adopt a problem-motivated research paradigm where we ground ourselves to two characteristics (multiple topics and multiple hierarchies) during discourse schema design. although we simplify the discourse schema to promote its universality, due to the free-style and domain diversity of web document data, it still has a limited scope of usage, mainly on general news report with multiple topics. for future studies, label sets could be revised in order to better account for the semantic functions in web documents of specific domains, where fine-grained labels and domain-specific labels can be considered. 2. task setting. in this paper, we only consider parsing the discourse from a list of document logical blocks already pre-processed in advance while do not contain a complete pipeline from input html source code to the final output discourse structure in the task setting. in the future, the gap between html elements and document logical blocks should be automatically closed in order to apply to downstream application scenarios. 3. data bottleneck. the annotated data volume in this paper is not big enough due to the expensive labour overhead, which may introduce noise into experiments and distort the performance and analysis. also, the domain diversity and multilingualism of dataset could be questioned since we collect data from single platform in chinese. in the future, such data bottleneck can be remedy by more dedicated manual annotation efforts, the help of weak supervision techniques, as well as developing data-efficient models."
827,"limitations we identify a few limitations of this work. first, our approach is applicable to language models pretrained with tables, which may not always be included in all language models (especially small ones). second, our approach’s limited improvement in commonsense reasoning tasks suggests that its effectiveness may depend on the specific task and the level of structured reasoning required."
828,"limitations this study has potential limitations. firstly, we only test our method on one dataset. we plan to apply our model to more datasets in future versions. secondly, ablation experiments are not sufficient. we will conduct comprehensive ablation experiments to demonstrate the contribution of different components."
829,"limitations in this paper, we only focus on whether or not there is a causal relationship between the given events, does not discriminate the specific cause/effect event. in addition, we only conduct research on sentence-level eci, whereas document-level eci often present more challenges. these are the focus of our future research."
830,"limitations our experiments are mainly on traditional datasets and do not fully demonstrate the effectiveness of the method in end-to-end scenarios. in order to solve the low-rank bottleneck problem, this paper proposes a method of svd weight transfer, but this method is limited to matrix factorization and does not apply this method to more general lowrank factorization, such as tensor factorization. we leave it as future work."
831,"limitations deplot’s strength is highly dependent on the accuracy of plot-to-text(table) conversion. to obtain effective plot-to-text conversion, large amounts of diverse and in-domain plot-table parallel data are usually needed. it is unknown to which extent deplot can work for out-of-domain (ood) plotto-text conversion. we investigated this in section §6.2 but in the future a wider range of web charts can be used to gain a deeper understanding into deplot’s robustness for ood plots. beyond, deplot does not work for visual language that does not have a clear latent textual representation such as textbook figures where the visual illustrations are created using specialized software and do not have clear structured representations. another limitation of the current deplot approach is that we ignore any layout information such as orientation and color of the visual elements/objects. in future work, we can incorporate such attributional information by including them in the decoding target."
832,"limitations our method utilizes the amr annotations as additional training signals to alleviate the data scarcity problem in the event extraction task. in this problem setup, generally speaking, amr annotations are more expensive than event extraction annotations. nonetheless, in reality, the amr dataset is much bigger than any existing event extraction dataset, and amr parsers usually have higher performance than event extraction models. leveraging existing resources to improve event extraction without requiring additional cost is a feasible and practical direction. our work has demonstrated the effectiveness of leveraging the feedback from amr to improve event argument extraction. however, it’s still under-explored what additional information and tasks can be leveraged as feedback to improve trigger detection. we did not have quantitative results for the alignment between amr and event graphs. the authors randomly sampled 50 event graphs from ace05-e and found 41 are aligned with their amr graphs based on human judgment. in future work, more systematic studies should be conducted to evaluate the alignment. there is a large gap between the validation and testing datasets in terms of label distribution on ace05-e and ace05-e+. we observe that performance improvement on the validation set sometimes leads to performance decreasing on the test set. both the validation and test dataset miss certain labels for event trigger types and argument role types. the annotations in the training set, validation set, and test set are scarce and highly unbalanced, which causes the low performance on trained models. we argue that a large-scale more balanced benchmark dataset in the event extraction domain can lead to more solid"
833,"limitations it is important to note that the study is based on a limited set of examples and although it is enough to give a signal if a system is struggling or not in faux pas tests, the number of stories is not sufficient for statistically significant ranking between systems. ethical statement the study’s scope did not include the representation of harm toward specific populations. the narratives were evaluated by a clinical psychologist to ensure that they did not contain offensive content. however, it is important to acknowledge the potential value of further research on the representation of harm in relation to culturally sensitive and socially controversial topics."
834,"limitations our experiments focus on the t5-base and t5-large models as these are widely used, representative pretrained seq2seq models. however, there are other pre-trained seq2seq models such as bart that we did not experiment with. it would also be interesting to experiment with pre-trained models with more layers such as t5-3b and t5-11b. we have not conducted these experiments due to resource constraints."
835,"limitations this resource paper proposes a new dataset and experiments with a baseline model only. we do not focus on creating new models and architectures. in the future, we plan to create models that perform much better on the isltranslate dataset. moreover, the dataset has only 31k video-sentence pairs, and we plan to extend this to enable more reliable data-driven model development. in the future, we would also like to incorporate isl linguistic knowledge in data-driven models. ethical concerns we create a dataset from publicly available resources without violating copyright. we are not aware of any ethical concerns regarding our dataset. moreover, the dataset involves people of indian origin and is created mainly for indian sign language translation. the annotator involved in the dataset validation is a hard-of-hearing person and an isl instructor, and they performed the validation voluntarily."
836,"limitations our study used t5-base (220m) due to the capacity of our computational resources (tesla v100 32gb). thus, it is unclear whether our method is also effective for larger models, such as t5-xl/xxl. lester et al. (2021) argues that continuous prompts are particularly effective for large t5 models. following their results, our instruction embedder is also expected to be effective for larger models. as shown in figure 3, instruction optimization is slightly unstable to converge. some studies tackled the unstable convergence of bilevel optimization by l2-normalization, early stopping (zela et al., 2019), or perturbation of hyperparameters (chen and hsieh, 2020). these methods might be effective in stabilizing the instruction optimization."
837,"limitations, structured models (jie et al., 2022; shao et al., 2022) have exhibited the capacity to outshine large language model prompting on mathqa. additionally, the recent emergence of instructionfollowing models (ouyang et al., 2022; wang et al., 2022a), exemplified by alpaca (taori et al., 2023), has prompted our interest in equipping large language models with mathematical reasoning capacities (wang and lu, 2023) while maintaining the integrity of their underlying language understanding capabilities. limitations the methods we have employed for prompting and fine-tuning have yielded noticeable improvements, yet certain limitations persist within practical applications. to achieve optimal performance, we continue to rely on prompting using large language models, which prove to be costly for the research community. furthermore, retrieval efficiency may present a challenge when dealing with extensive training sets, as identifying the top m exemplars for each example becomes increasingly time-consuming. consequently, devising a more efficient algorithm to expedite the retrieval process represents a potential area for future exploration. despite the potential for performance improvement by sampling 40 reasoning paths for each question as presented by wang et al. (2022a); fu et al. (2022), we were unable to incorporate this approach due to budget constraints. additionally, although training data has proven beneficial, the gains for smaller models are insufficient to surpass the performance of large language models. this observation may indicate the necessity for a fundamentally different model design or a superior pre-trained model (e.g., galactica (taylor et al., 2022) or code-t5 (wang et al., 2023)) as a more effective basis for fine-tuning."
838,"limitations model architecture due to our computational resource constraints, we only used the bert base architecture. we cannot confirm whether our results and observations are transferable to any other transformer-based architectures, especially for larger ones. randomness we did not run pretraining for multiple times with different random seeds due to the limited computational resources and research budgets, though we fine-tuned models five times each with different random seeds in any downstream tasks. this might affect the overall results shown in the paper. languages other than english it is uncertain whether any results and"
839,"limitations the limitations of this work mainly lie in two aspects: (i) the synthesis quality is determined by the performance of existing data2text approaches, while data2text generation is still a difficult task that waiting for deeper exploration. the common errors in generation are included in sec. 4.2.3. (ii) we adopt a plm as the decoder in data2text generation in order to generate fluent utterances. however, as stated in (ribeiro et al., 2021), plms tend to pay more attention to sentence fluency than to the graph structures of inputs, which may cause the loss of some critical information."
840,"limitations there are several limitations to account for in the presented work. first, the large gpu requirements for the execution and replication of the presented experiments. second, the lack of empirical results beyond english-based text, and how morphologi- cally and syntactically more complex corpora may affect the presented evidence. third, our evaluation section compares gp-ts performance to the common hyperparameter grid-search alternative, yet we acknowledge that other bayesian optimization techniques used in the machine learning community may provide suitable and competitive alternatives to explore. in addition, we have not run any hyperparameter tuning beyond mlm dynamic masking, which might improve all studied algorithms’ performance. finally, our"
841,limitations and future work for improvement.
842,"limitations the natural idea to improve robustness is to add adversarial examples to the training set and retrain the model. however, generating adversarial examples for a large training set can be very time-consuming. thus, it would be interesting to explore more efficient methods that implicitly involved adversarial examples in the training process, e.g., (yang et al., 2022)."
843,"limitations fuzzers are designed to reach deep and complex control flow in large software. many programs for current ai for code datasets do not have complex control flow. as a result, afl++ can quickly cover all program branches before generating many inputs for us to feed to the model. we plan to try data-flow coverage as a more accurate coverage metric in the future. afl++ uses branch coverage to track fuzzing progress. although it works well on c/c++ programs, it may be ineffective on languages with exceptions, which are implicit control flow. for example, afl++ cannot distinguish different exceptions thrown in the same block, which sometimes leads to low coverage in python programs. to overcome this issue, one possible way is to change from branch coverage to line coverage. although our current implementation requires a fuzzer, our approach can also work on tasks with only functions or code snippets as long as we can acquire adequate input/output pairs of the functions or code snippets, which may have some engineering challenges but is not infeasible. for example, in recent years, the software engineering community has proposed various ways to fuzz bare functions (serebryany, 2016; ispoglou et al., 2020). ethical consideration our method exploits fuzzing test cases for program understanding. improved semantic understanding of programs facilitates various tasks, e.g., code generation and code completion, which might further be used to patch vulnerabilities or fix defects of softwares and systems. nevertheless, considerable effort has to be further devoted to apply the method to these applications, for which we encourage to take special care in advance. in addition, a number of crashes and hangs have been observed on programs in the adopted datasets, since fuzz testing is utilized. we do not demonstrate test cases that lead to these crashes and hangs to avoid misuse of this information."
844,limitation of dhr by achieving better zero-shot performance without relying on document structure information. we also study the accuracy-storage-latency land- scape. we believe these findings are critical to the real-world adoption of hhr to odqa systems.
845,"limitations we recognize the following main limitations of the present study. although the approach we devised is not bounded to a specific model architecture and language, our study fouced only on one neural language model and a limited set of languages and this may limit the generalization of our results. moreover, we are aware that discourse coherence is a multifactorial phenomenon that can only be partially covered by the devised methodology and dataset."
846,"limitations in this study, we primarily focus on the examination and experimentation of the dsi-qg model, with plans to expand our research to include more recent models that utilize differentiable search indexing, such as the nci model. while our approach has demonstrated effective improvements in dsi retrieval outcomes, and both tct-colbert and our proposed dsi-qg-multi performed well in our empirical analysis concerning relevance ordering, we cannot dismiss the possibility that these favorable results may be attributed to the extraction of insights from a specific bert reranker model that shares similarities or correlations with the one used to define the desired ranking. despite showing improvement over dsi-qg, our model remains slightly less effective than stateof-the-art dense retrieval methods such as tctcolbert v2. our approach offers advantages over dense retrieval models such as reduced storage and maintenance overhead, as dsi models do not require additional index structures for online use. though index structures are utilized during the training phase of dsi-qg-multi, the generated index structures are temporary in nature. furthermore, due to the limitation of model memory, current research on dsi only experiments on a subset of the entire ms marco dataset or small dataset such as nq. therefore, an important future direction is to develop more efficient architectures to deal with the issue of memory bottleneck, for example, by using the current popular large language models (llm) or constructing aggregation structures for storing all information in hierarchical pieces."
847,"limitations data unbalancing is one of the limitations of our work. in the future, we would like to address this problem by data oversampling or undersampling. for our experiments, we have used all samples from the iu x-ray dataset. but from the mimic-cxr dataset, we have used only 44578 reports out of 227827 reports. our results for the mimic-cxr dataset might differ when we use the whole dataset. to evaluate the generated pathological descriptions, we consider the pathological descriptions that we extract from original reports as ground truth. to evaluate the generated full reports, we generate templated reports by replacing ground truth pathological description in normal report template and consider it as ground truth. so it considers the abnormalities from the original reports and the normal sentences from the normal report template. automatic generation of chest x-ray reports will make it easier for radiologists to diagnose and write reports. our model achieved comparable performance with state-of-the-art models on chest x-ray report generation. in realistic scenarios, it is still a long way from being used clinically."
848,"limitations the datasets utilized in this research contain documents and summaries in english (xsum and cnn/dm datasets) and thus mainly represent the culture of the english-speaking populace. gender, age, political or other biases may also exist in the dataset, and models trained on these datasets may propagate these biases. our experiments and analyses are based on the assumption that training data contains artifacts that lead to factual errors in summarization models. also, it is evident from the results, that the effectiveness of our proposed models is relatively higher for the noisier xsum dataset. so, our analytical results and improvement from a model may have limited implications on a perfect dataset that does not exhibit any learnable artifacts. we relied on automated metrics, such as rouge and entity recall for measuring information relevance, and entity precision, question answeringbased metrics and dependency arc entailment accuracy for information correctness. these metrics are error-prone. exclusively for a subset of models, that perform the best according to automated metrics, we use human annotations for additional evaluations."
849,"limitations in this first effort, opinesum was demonstrated for the english language. since the wealth of review information on many websites span several languages, extending our work to other languages is a key area for future work. there are two language specific components—the proposition identification rules, and the textual entailment model. for the latter, there are multilingual resources such as mt5 models (xue et al., 2021) and multilingual entailment datasets (conneau et al., 2018) which are good starting points. the proposition rules are much more language specific. very recent work has introduced a corpus and learned model for proposition identification (chen et al., 2022), and future research in languages other than english could strength this component. a second noteworthy point is the scalability of the silver data creation. as described in section 3.3, we perform a quadratic number of entailment queries per item. in this work, this was of the order of a few billion. we used an apache beam pipeline to scale our computation using a lot of parallel computation on cpus. readers must be aware of this computation when applying such an approach for their work. however, note that the processing only needs to be performed once for training data creation. future work on more efficient transformer models such as khattab and zaharia (2020), will help vastly improve these types of computations."
850,"limitations this work contributes a debias benchmark mmsd2.0 for building reliable multi-modal sarcasm detection system. while appealing, mmsd2.0 is built on the available mmsd benchmark. in the future, we can consider annotating more data to break through the scale and diversity of the original mmsd."
851,"limitations our work still exist some limitations. first, we choose an entity typing system on the base of wikidata tags, however, the granularity of the typing system remains to be discussed. a system with too many types would introduce noise to long-tail types, while insufficient types would weaken the disambiguation ability of type similarity. thus, building a type system with adequate granularity remains a challenge. second, we combine the entity typing task with plm-based semantic encoders, which require a fixed type system and further finetuning. integrating the entity typing task into the pretraining process may enhance the transferability of the model and remove the dependency on a fixed type system. potential risks. our proposed dataset nel centers on ambiguous entities, whose type distribution may not remain the same with other datasets. a potential risk is that the model trained on nel may experience under-exposure of other entity types, which would damage their transferability and lead to undesired outputs on other datasets."
852,"limitations our paper has some limitations. first, we do not give detailed instructions about techniques that we recommended (e.g., factor analysis, synthetic data generation). we rely on our readers’ autonomy to acquire the necessary information (that is specific to their research projects) by further reading our recommended references. second, we only survey tpc papers included in the acl anthology, despite other tpc papers existing outside this venue. while this means that the challenges we identified might be specific to these papers, we believe they are still a good representation of the tpc research done in nlp. lastly, we limit our"
853,"limitations although simple achieves great performance with size reduction and generation speedup on various generative language tasks, it is interesting to explore combining the stage of mask learning during the pre-training. then, one pre-trained model can be applied to downstream tasks with any required sparsity with a stage of fine-tuning. in addition, the pruning of the larger generative pre-trained language models during the fine-tuning is also worth trying. in the future, we would like to investigate the generation ability of the compressed models with more pre-trained data and larger models."
854,"limitations in this work, we present pgra to retrieve taskspecific context evidence to support nki tasks. however, our work has some limitations. firstly, we have not experimented with our pgra on sentence-pair tasks, such as mrpc (dolan and brockett, 2005), in which the model needs to infer the relationship between two sentences. retrieving two sentences from an external datastore is non-trivial as there are hardly sentence pairs in the wikipedia datastore. a larger corpus with more diverse data sources may help in this case. secondly, we restrict our pgra to classification tasks but not generation tasks. similar to sentence-pair tasks, retrieving sentences that may help the model generate text is more complex. for example, data related to both the source and the target may help in machine translation (khandelwal et al., 2021). we will research this question in the future. last but not least, we have not extensively tested the performance of our method on ki tasks, except for some preliminary analysis in appendix f. this restricts the generality of our methods. solving ki tasks depends on knowledge in the passage-level external datastore while matching such information needs possibly more specialized prompts for our method. thus, it is for our future work."
855,"limitations while our ndd metric has demonstrated its effectiveness in measuring the semantic distance between overlapped sentences, there are still some limitations to consider. firstly, the calculation efficiency of ndd may become a bottleneck when dealing with large amounts of data. the mask-andpredict strategy requires the generation of a large number of predictions for each word in the lcs, which can be computationally expensive. therefore, for large-scale applications, more efficient algorithms or hardware acceleration may be necessary to speed up the calculation of ndd. secondly, our method currently cannot selectively compress certain parts of the text. the mask-and-predict strategy compresses the entire overlapped segment, which may not always be desirable. for example, in some cases, it may be more desirable to compress only the less relevant portion of the text while retaining the most informative content. while ndd has an advantage over supervised compressors in controlling compression ratio, it still cannot control the compression orders. future research may investigate techniques to allow for more fine-grained control over the compression process. overall, while ndd shows great promise in improving the evaluation of semantic similarity and text compression, further research is needed to address these limitations and improve the compression rate controlling ability and versatility of the method."
856,"limitations although recontriever narrows the gap between bm25 and unsupervised dense retrievers, it still lags behind bm25 when acting as a generalpurpose retriever. this issue may make recontriever not directly usable when facing a new domain, thus limiting its practicality. also, as recontriever is initialized from the language model bertbase, there may exist social biases (zhao et al., 2017) in recontriever and thus have the risk of offending people from under-represented groups."
857,"limitations all experiments are conducted on data containing exclusively english language. consequently, the results may differ in particular on morphology-rich languages and/or non-inflectional languages. similarly, all presented techniques expect languages to use latin characters. therefore, our method first needs to be adapted in order to be used on languages using different characters, such as cyrillic languages, korean or persian. using the bert-classifier in conjunction with al is resource intensive. loading the ""bert-baseduncased"" model from huggingface along with one of the datasets with a batch size of 24 requires around 22gb of gpu memory. we train for a maximum of 15 epochs, requiring up to 1 gpu hour, depending on the size of the dataset and the length of individual inputs. as such, a single al experiment requires approximately 20 gpu hours to complete. in addition, computing the al strategies requires up to 2 gpu hours for the alps strategy and up to 1 gpu hour for the dal strategy, depending on the dataset. the subword strategy is calculated only once and used up to 1 gpu hour. in total, our ral experiments take around 60 to 100 gpu hours to complete. it is important to note that the rl itself is not expensive and does not require gpu, therefore ral can easily be adapted to scenarios with low computational resources by employing a different classification model as well as using al strategies that do not rely on large pre-trained language models."
858,"limitations selecting a representative set of examples to label becomes essential when working with limited labeled data. in this work, we use uniform sampling for our results, which might not be the best approach. we discuss these limitations in more detail in the appendix. while we evaluate our model on a multi-label dataset (dstc2) and show improvement over standard baselines, the effectiveness of our approach on such problems needs more investigation. ethical considerations while our algorithm is primarily a tool for improving classifier performance in label-scarce settings and uses publicly available, anonymized datasets, we acknowledge the potential ethical implications it may carry. despite the neutral nature of our tool, it could unintentionally propagate or amplify biases favoring certain styles of communication. if used in real-time settings or without proper checks in place, it could inadvertently alter the natural dynamics of the classroom as teachers or students might modify their behavior based on how they believe the classifier categorizes their utterances. like any other, we recognize that our tool could make mistakes or be misused by over-relying on quantitative aspects over qualitative aspects of instruction. therefore, real-world application requires continuous vigilance and open dialogue with practitioners and stakeholders to ensure its use benefits teaching and learning."
859,"limitations in our paper, we provide a variety of experiments and"
860,"limitations multijugate dual learning improves the model’s performance in tod tasks in low-resource scenarios, but the introduction of the dual training objects increases the required graphics memory and training steps. in addition, the rephrasing mechanism necessitates an additional paraphraser to rewrite the training samples; hence, the number of training samples increases according to the number of paraphrases. despite this, we find that the higher training cost associated with multijugate dual learning is preferable to employing a large quantity of dialogue data for further pre-training or manually labeling data. considered from a different angle, the scenario described above presents possibilities for future research, such as the development of higher-quality rephrasing algorithms to filter the augmented text. in the meantime, multijugate dual learning is a learning objective between structured and unstructured texts. therefore it may be extended to any task involving heterogeneous data, such as generative information extraction, and data-to-set generation."
861,"limitations in the augmentation-driven self-training, we implement the data augmentation with random masking for simplicity, since augmentation is not the focus of this work. and wang and henao (2021) has explored more fine-grained data augmentation strategies, which may further improve performance."
862,"limitations our dqgf still exists some limitations. while our generated data improves performance in diverse questions settings, there is still some noise in the generated data that affects the performance of original single question. in the following, we will give the limitations of our dqgf on its three components. the diversity of the question depends on the diversity of the equations. our equation generator is based on heuristic rules, resulting that the generated equations are very simple. in the future, we will try a model based equations generator to generate more diverse equations. in the question generator, it can only recognise equations with the operator ""+-*/"" due to the limited operator set in our training dataset unbiasedmwp. in the future we will expand the operators so that the generation model can recognise more operators and be more universal. filtering strategy is also important. using the answers of expert model as a criterion for evaluation still exists bias and leads to the noisy data. in fact, we have tried to generate more diverse equations but all are filtered by the current data filter. we will look for better filtering strategies in the future."
863,"limitation of dft++ is the computational overhead caused by generative plms. additionally, our current approach includes all utterances generated by the plm, even those that might lack contextual relevance or contain noise. these issues are left for future exploration."
864,"limitations to inspire future work, we conclude some limitations of our work as follows: • while our method achieves promising performance on sentence embedding related tasks like sts, the performance on other natural language processing tasks are still need to investigate. • the ai feedback in our experiments comes from gpt-3, which requires a fee to use. • we do not explore the effect of different task description prompts on the quality of generated sample pairs, which may influence the performance of claif. • in clhaif, we only use the ai feedback for positive sample pairs. how to utilize ai feedback for negative sample pairs remains to be studied."
865,"limitations the training process of mars needs to rely on manually annotated belief states and action states as semantic states to explicitly model the relationship between dialog context and semantic state representations through contrastive learning methods. we propose mars in the research community and hope it can be better applied to real-world scenarios in the industry. however, the annotated data is expensive, which makes our methods have some limitations in the landing process of real scenarios. in the future, to better apply our proposed mars to real-world scenarios, we will introduce semi-supervised methods to reduce the dependence on annotated dialog corpus."
866,"limitations we reused data collected by previous work in the literature. collecting news articles is susceptible to various sampling biases, related to the sources collected, the topics covered, and the time span of the collection, which influences what appears in the articles. in addition, labels given to articles are actually the political orientation of their source in the case of the allsides and politics datasets, which is obviously likely to induce errors. they rely on expertise provided respectively by the allsides11 and ad fontes12 websites. the exact methods are undisclosed, but such labeling has necessarily a subjective aspect, oversimplifying predefined political categories, and can evolve in time. this affects classification reliability when applied to different sources, different times, different topics. this is on top of any specific elements related to the language (english) and cultural background of the sources (predominantly u.s.-based sources). this study is not intended to provide an accurate tool for predicting the political orientation of a text, but to provide analyses of the linguistic expression of bias, as seen through a supervised model. ethical considerations studying the political orientation of various media is already the objective of various institutions (allsides, ad fontes, media bias/fact check). it depends on many factors, and a reliable automatic identification is still out of reach of current models, as can be seen from existing experimental results, and some of the limitations underlined above. these models should thus not be used for something other than research purposes, or supporting human analysis. this is one of the reasons why we develop an explainable approach to bias predic- 11https://www.allsides.com/media-bias/ media-bias-rating-methods 12https://adfontesmedia.com/ how-ad-fontes-ranks-news-sources/ tion, but these also have their own limitations, and shouldn’t be used either as a strong indication of bias in one way or another without careful human examination."
867,"limitations our dataset construction method has certain limitations. one important limitation is that it is difficult to get the distribution of the required commonsense knowledge types. this can be addressed in future work with human designed commonsense knowledge schema and human annotation. one potential risk of our work is that the text games may be limited by the time of writing, thus raise fairness considerations. however, our dataset construction strategy is not limited to these specific games, better sampling games can help to reduce such biases."
868,"limitations in this section, we faithfully discuss the current limitations and potential avenues for future research. first of all, in the analysis, we observed that giving heavy weight to the soft loss at initial training epochs improves the convergence speed. yet, continuing training with such heavy weight to the soft loss could hinder the further performance improvement of the student. therefore, adjusting soft loss weights depending on the training phase from a larger value to a small value (e.g., using the time function) would be helpful for both convergence speed and improving the model’s quality. secondly, it has been demonstrated in the visual recognition domain that adjusting the temperature of distillation loss for poorly performed teachers can improve the student model quality due to the regularization effect. following them, increasing the temperature to smooth the soft labels from poorly performed teachers, such as 1-layer or 2- layer teachers, would help improve the quality of distillation via the regularization effect."
869,"limitations in this work, we propose a detector that aims to detect adversarial samples via sharpness of input loss landscape for model. however, the computational cost of the sharpness is high because it requires at most k-step gradient descents. moreover, in this work, we mainly considered word-level adversarial sample detection as often studied in previous work, while character-level and sentence-level adversarial samples are not studied. these two problems will be explored in our future work."
870,"limitations the proposed attack is specific to textual data while many membership inference attacks are universally applicable to all modalities as they mainly rely on loss values obtained from models, our proposed method for generating neighbours is specific to textual data. while standard augmentations such as rotations could be used to apply our method for visual data, this is not straightforward such as the transfer of other attacks to different modalities. implementation of baseline attacks as the performance of membership inference attacks depend on the training procedure of the attacked model as well as its degree of overfitting, it is not possible to simply compare attack performance metrics from other papers to ours. instead, we had to reimplement existing attacks to compare them to our approach. while we followed the authors’ descriptions in their papers as closely as possible, we cannot guarantee that their attacks were perfectly implemented and the comparison to our method is therefore 100% fair. ethical considerations membership inference attacks can be used by malicious actors to compromise the privacy of individuals whose data has been used to train models. however, studying and expanding our knowledge of such attacks is crucial in order to build a better understanding for threat models and to build better defense mechanisms that take into account the tools available to malicious actors. due to the importance of this aspect, we have extensively highlighted existing work studying how to defend against mias in section 6. as we are aware of the potential risks that arise from membership inference attacks, we will not freely publicize our code, but instead give access for research projects upon request. with regards to the data we used, we do not see any issues as all datasets are publicly available and have been used for a long time in nlp research or data science competitons."
871,"limitations first, inference efficiency is one of the main limitations of this work. the bart model takes about 14 minutes to complete the inference on our dataset, while our utged needs 92 minutes. the reason for the slow inference is that utged requires heavy computation to update the gradient to the encoder’s states and decoder’s states (as shown in eq.7~eq.9). future work may consider how to advance model efficiency further. second, the lack of multimodal content in the published tweets would result in another limitation. the images contained in the published tweets are ignored in this work. however, due to the complicated relationships between images and texts in a multimodal tweet, images might provide complementary content and complete the meanings of the message (vempala and preotiuc-pietro, 2019). therefore, future studies might explore selfintroduction generation using multimodal tweets (images and text) to indicate personal interests."
872,"limitations in this paper, we conducted an experiment on code summarization using two benchmark datasets, the java dataset (hu et al., 2018b) and the python dataset (wan et al., 2018). blocsum may need to be tested for its generalizability to other program languages. we chose two program languages (java and python) that were easily parsed to map the block position of code and ast. we believe that since other programming languages have similar syntactic structures, blocsum should be able to achieve similar performance on them as well."
873,"limitations our experiments are conducted based on the t5base pre-trained language model. due to the computational resource constraints, we did not conduct experiments on other similar plms, such as bart, and t5 model with larger scale, such as t5-large and t5-3b. although we believe our"
874,"limitations this work follows in line with those studies (poon and domingos, 2009; goldwasser et al., 2011; titov and klementiev, 2011) where unsupervised semantic parsing relies on the dependency parse trees of texts. although it enables us to leverage advanced syntactic parsers and to disentangle the complexity in syntactic analysis from that in semantic parsing, the errors made in the dependency parse trees created for input texts could propagate to semantic parsing. in the future, we would like to explore the feasibility of jointly performing syntactic and semantic parsing in a completely unsupervised fashion. even though an improved mh merge-split sampler was proposed in this study to speed up the mixing and convergence of markov chains by leveraging pre-trained distributed representations, the computational effort required to fit the model can still be substantial, especially for a large body of texts. we plan to improve computational efficiency beyond that offered by this study by starting with good initialization and updating the state space in a distributed and parallel manner."
875,"limitations & risks limitations: in this work we present a novel method to address data scarcity issue for relation extraction (re). although our experiments demonstrate the effectiveness of the proposed method, there are still some limitations that can be improved in future work. first, similar to previous work (dos santos et al., 2015; veyseh et al., 2019), the current method assumes golden entity mentions to perform re that might not be the case in different applications. it is thus helpful to explore the method in a more realistic setting where entity mentions are predicted, e.g., using joint inference models to simultaneously extract entity mentions and relations in an end-to-end fashion. second, our method is currently evaluated only for sentence-level re (i.e., entity mentions are in the same sentences). future work can further explore our method for documentlevel re to allow entity mentions to appear in different sentences to better demonstrate its advantage. finally, our method requires the generative gpt-2 model for data generation. to perform well, gpt-2 needs to be trained on large unlabeled datasets that might not be readily available for low-resource languages. as such, it is important to further evaluate our method on low-resource languages to better reveal its effectiveness. risks: in this work, we employ gpt-2 to generate new training samples for the task of re. although gpt-2 is publicly available and the datasets employed in this work to fine-tune gpt-2 for re are also publicly available, a generative language model might produce biased sentences, insulting texts or reveal private information. as such, it is necessary to take further measures before publicly releasing the automatically generated labeled data. to this end, we inspect the data employed for finetuning to exclude any offensive text and identity information. the generated data will also be inspected for purpose before publicly releasing the data."
876,"limitations one of the advantages of the fusion-in-decoder approach is that it uses the off-the-shelf t5 architecture with publicly available checkpoints. the proposed fido modifications strongly improve performance and inference speed for retrieval-augmented question-answering, but require pre-training from scratch. it is in general preferable to have a small number of checkpoints that can be fine-tuned for any application. for example, it may not be feasible to train different giant language models for use in the retrieval-augmented setting. instead, the architectures for such large models may need to be a compromise for different use cases."
877,"limitation, our primary contributions include: • counterfact+, a dynamic specificity benchmark, which adapts to the model edit under test, and is more sensitive than the existing benchmark • neighborhood kl divergence (nkl), a specificity metric based on the full probability distribution as a complement to the currently used metrics which focus only on the tokens directly implicated in the model edit. limitations the main limitation of the approach we took for improving model editing benchmarks is that it is ultimately based on manual inspection of test cases to understand the failure modes of model editing methods. this approach is not scalable and has a significant cost in terms of time and effort. as far as the specific benchmark we propose is concerned, more research is needed to assess its effectiveness for more complex scenarios such as dialogue and multi-turn conversations. we also have not investigated the application of our benchmark to scenarios in which multiple model edits are performed simultaneously. furthermore, we do not evaluate other types of model edits, such as parameter pruning, and transfer learning. future work should focus on developing methods that measure and quantify the effects of model edits on long-term aspects of language models, such as their ability to capture discourse structure and fluency of generated text. this could include corpus-level analysis and dynamic approaches like red-teaming or dynamic benchmarking to uncover subtle adverse effects."
878,"limitations even though santa shows strong effectiveness on learning the representation of structured data, it heavily depends on the alignment signals between structured and unstructured data. such alignment relations can be witnessed everywhere, but the quality of constructed pairs of structured and unstructured data directly determines the effectiveness of santa. besides, we use the product bullet points and code descriptions as the unstructured data in our experiments, which is designed for specific tasks and limits the model’s generalization ability. on the other hand, santa mainly focuses on evaluating the structured data understanding ability through text data representation and matching. it is still unclear whether santa outperforms baseline models in all downstream tasks, such as code summarization and code generation."
879,"limitations although our model has shown superior performance, there are still a few limitations that could be improved in future work. • we create few-shot datasets from the perspective of the combination of sentiment categories without considering the distribution of aspect items, such as the number of aspects in each sample. it may affect the performance of the model on the task of extracting aspects. we should create more efficient datasets for mabsa in the few-shot setting. • as we put more emphasis on the performance of the main task, the performance of the subtask of predicting the number of aspect terms in each example may suffer. we will further improve the accuracy of the subtask in future work. • we roughly exploit initial image features and do not perform alignment between text and image modalities. we plan to accomplish the alignment of multiple modalities further to improve the performance of mabsa in future work."
880,"limitations we would like to acknowledge three categories of limitation that we recognize in this work: • we evaluate the effectiveness of priming in a setting where the tasks used during the priming stage and the fine-tuning stage offer no additional disparity besides being different in language, i.e., they are all ner tasks coming from the same domain. while this degree of variation is consistent with the application of meta-learning in other modalities, e.g., vision (finn et al., 2017), whether or not the gains we report here remain at the same strength when we introduce diverse tasks during priming and fine-tuning still needs to be tested. examples of such diversity include strong domain shift or using one task, e.g., pos, for priming and another, e.g., ner, during fine-tuning. • it’s not clear how the size of the pre-trained model affects the necessity of priming. priming might consistently result in gains, or its benefits might fade away with larger plms encoding stronger language capabilities. this also needs to be evaluated. • finally, our work does not implement higherorder gradient calculation and does not evaluate and discuss the potential additional gains that might come as a result. that opportunity can be further explored as well."
881,"limitations let us begin with the obvious limitation: axomiyaberta only works on assamese. in addition, since assamese comprises a number of dialects and we trained on internet-sourced data, we have no clear evidence regarding which dialects axomiyaberta is most suited to or if it performs as well on non-standard dialects. axomiyaberta did not perform all that well on wikipedia title selection, compared to other transformer-based models. our best result is on par with xlm-r and close to indicbert-base, but well below mbert performance. we hypothesize that the amount of wikipedia training data in mbert is a cause of this, but we find that phonological attention makes a big difference in axomiyaberta’s performance (increasing accuracy from 26% to 59%). nonetheless, the reasons behind this subpar performance, and whether axomiyaberta can be improved for this task without, say, overfitting to wikipedia, need further investigation."
882,"limitations there are several limitations to this paper. first, due to time and space constraints, we are unable to experiment with other interesting model compression techniques such as neural architecture search and quantization. we also have to select only a small subset of baseline text-to-sql models to represent the performances on each of the datasets. we are also aware of the existence of ryansql (choi et al., 2021), a sketch-based model for the spider dataset. however, we are not able to reproduce the baseline results to the best of our efforts and have to exclude them from our analysis. therefore, it is important to be aware of these potential limitations and biases when using our results for real-world deployments."
883,limitation of hyhtm is that it is parametric and therefore requires empirical analysis to find the optimal number of topics at each level. we plan to investigate this shortcoming in the future.
884,"limitations we propose and construct korc as a new benchmark dataset for deep text understanding. the limitations are two folds. first, in the benchmark design, korc do not take more complicated knowledge into consideration, including literal knowledge and qualifier knowledge. we leave extending korc to these knowledge in future work. second, in the dataset construction, we examine automatic name anonymization and question generation strategy, and present korc-l. korc-l relies on large language models. rather than medium-scaled language models that can be maintained by a single machine, gpt-3 is used via its online apis. although the service of gpt-3 is currently available, we still need to find a substitution for better reproducibility. besides, although llm saves human effort, the execution of llms potentially consumes more energy power. it would be better if we can preserve the high question generation quality and propose a small model to proceed data annotation."
885,"limitations although the ihlda shows better performance than existing models in multiple experiments, there are three limitations that we did not fully address in this paper. first, the gibbs sampling is slower than other approaches such as autoencoding variational bayes (kingma and welling, 2014), which limits data scalability. we can incorporate the literature on distributed algorithms for topic modeling (newman et al., 2009; yu et al., 2015; karras et al., 2022) and variational inference (wang and blei, 2009; wang et al., 2011; bryant and sudderth, 2012; hughes et al., 2015) in future research. second, crowdsourced evaluation limits a corpus choice because we should not expect workers to have any prior knowledge (ying et al., 2022). our crowdsourced evaluation only used bbc news, the most accessible documents among the three corpora. future research can thoroughly validate the performance of the crowdsourced workers and trained coders. existing literature (buhrmester et al., 2016; kees et al., 2017) found that mturk had a comparable quality against traditional survey panels, but they did not use mturk for evaluating outputs from a machine learning model. third, an estimated hierarchical structure does not necessarily match the semantic hierarchy human readers expect. this mismatch is not surprising because unsupervised models do not directly incorporate information about a tree structure. existing papers improved the interpretability of flat topic models by providing topic-specific sets of keywords (jagarlamudi et al., 2012; harandizadeh et al., 2022) and labels (mcauliffe and blei, 2007; ramage et al., 2009), which is a future direction for a hierarchical topic model."
886,"limitations currently, we build a vocabulary from the original one used in mbart-50, and only conduct downstream experiments across 6 languages (english, german, french, indonesian, japanese and chinese). although we could involve more languages, it would require a larger cuda memory that might go beyond our device capacity. hence, we merely select the above languages that have sufficient overlap with our pre-training datasets. in addition, for fair comparisons, we only use the strictly-aligned multilingual multi-modal dataset provided in (zhou et al., 2021), which is augmented through machine translation. it is unclear how the quality of strictly-aligned dataset would affect model performance. meanwhile, the length of texts in our weakly-aligned multilingual multi-modal dataset is generally very long. as a result, we truncate textual inputs before feeding them into the encoder, possibly bringing information loss to some extent."
887,"limitations our method regen is a general framework for zero-shot text classification. in this work, we aim to first bring in simple and intuitive way to justify the power of unsupervised dense retrieval for zeroshot learning. effective as it is, there is still much room for improvements, including designing better objectives for pretraining rθ as well as better strategies for removing noisy training data (lang et al., 2022; xu et al., 2023). how to improve these components is an important line of future work. besides, our experiment results are all based on bertbase sized models. although regen performs on par with or better than previous dataset generation methods using giant nlg models, it remains unknown to us how the benefit of regen scales with more parameters for both rθ and cϕ. also, we point out that this work focuses on zero-shot text classification with task-specific verbalizers and unlabeled generic corpus, thus it can be nontrivial to adapt our framework to other tasks such as natural language inference (nli) as well as low-resource tasks where even the unlabeled generic corpus can be hard to collect. extending regen to these settings will reduce the annotation burden under more challenging scenarios."
888,"limitations there are three limitations. first, our model requires the retrieval of relevant structured and unstructured knowledge from different knowledge sources, which can be time-consuming. using cosine similarity over question and fact embeddings can be a bottleneck for the model performance. second, our model focuses on rich background knowledge but might ignore some inferential knowledge, which can be acquired from other sources such as atomic. third, our model might not be applicable to low resources languages where knowledge graphs are not available."
889,"limitations the major limitation is that we test our method only on a binaural speech dataset, in which there is a person moving slowly while speaking. because this person moves slowly, the doppler effect is not so obvious. we will try to find or collect a sound dataset of a source moving at high speed, such as a running man, flying objects, or vehicles, and further, analyze the experimental phenomena at different speeds of the moving source."
890,"limitations while the results have shown the effectiveness of our framework in ie without using any additional resources, we did not explore the potential enhancement by utilizing existing resources in the easy-tohard learning process. on one hand, we can build the easy stage with the help of existing data of simpler tasks. on the other hand, the data of harder tasks can be used for the hard stage. to enhance the e2h framework via effectively using existing resources is an interesting and promising direction. another limitation is that we did not extensively explore the possible skill sets for each task. exploring more approaches to obtain the skill sets is also open for future research. we plan to investigate these possibilities in our future work."
891,"limitations although our geometric embedding approach can handle a complete set of basic fol operators (existential quantification, conjunction, disjunction and negation), the modeling of negation operator cannot narrow down the predicted answers to relevant topics of atomic queries. for example, one can expect the answers of this negation question/query “list argentina players who are not lionel messi in world cup 2022?” to be any teammates of lionel messi (i.e. 2in query structure). however, the current model is designed to return all elements in the entire entity set except for lionel messi, which have redundant objects (e.g. trees, music, houses). this is a common limitation not only in geometric-based models but in others using fuzzy sets representation. this is due to the fact that the modeling of negation operator is assumed to be the complement set of a questionable entity w.r.t. the entire entity set. our hypothesis is that the expected answers should be narrowed into the complement set w.r.t. a sub-topic of relevant entity set. in addition, when apertures of two sector-cones are obtuse angles, the current calculation of partial intersection cannot correctly model the conjunction operator. this special case is inevitable in a system using geometric representation that is closed under negation and conjunction, but not for disjunction (see appendix a.2 for further details)."
892,"limitations this work requires that news videos are organized into different events and each event has more than one candidate video. the debunking rectification module relies on the existence of labeled debunking videos, and the graph aggregation module relies on existing fake news detectors to provide the initial features for each video. the textual length in videos is limited due to that the debunking inference module is based on a pre-trained bert model with limited sequence length."
893,"limitations: despite these successes, our current work is still limited in the following ways, which we leave to future work: • our current model is based on pretrained gpt2 (radford et al., 2019), and therefore its generation ability is limited that of gpt-2. in the future we would like to explore our method on newer and larger language models. • human labels are currently provided at the sentence level, either a rating of the whole sentence or providing a new sample sentence. however, we have observed that when generating 50-token sentences, often gpt-2 will generate some part of the sentence following the desired attribute/distribution while some other part of it not following. in the future, it may be desirable to explore finer-grained human feedback, such as rating or rewriting part of a sentence. • our experiments are performed on low quantities of data to demonstrate that our method works under a few-shot setting. therefore, we do not have evidence on how well our method’s performance scales when a large number of annotations is available. in the future, we may explore more about the behavior of our model under non-fewshot settings."
894,"limitations while the proposed method has demonstrated superior performance and high efficiency, there are several limitations that warrant further investigation: (1) in few-shot settings where the number of training examples is limited, the performance of our method and other baselines drops significantly. future work should focus on uncovering essential features of the task in few-shot scenarios and generating embeddings of higher quality. (2) the storage consumption has been reduced to a small amount, however, the number of neurons is still relatively large compared to that of heads and therefore becomes a bottleneck for further decreasing storage requirements. as discussed in sec 5.2, one possible solution is reducing the number of layers used to generate the embedding. future work could also include assigning intermediate neurons into groups to make the embedding coarser in granularity, thus reducing storage requirements."
895,"limitations although our proposed method achieves the stateof-art performance, it still has a few limitations. firstly, we only consider the dependency between aspect and opinion in the target text yet ignoring the order influence in the input text, which may bring more improvements. secondly, there are three label types for aste, including aspect, opinion, and sentiment. currently, we only utilize the aspect and opinion markers in the marker-oriented sequence labeling module. we believe that the specific design for the sentiment marker can further improve the performance, which can be a future direction."
896,"limitations in this paper, we mainly focus on evaluating our approach on two english-centric corpora, iwslt17 and pc32. future research could consider more multilingual machine translation benchmarks with different number of languages and training samples and conduct experiments on more challenging training scenarios such as chain configurations where we have multiple bridge languages and different zero-shot distances."
897,"limitations we state the limitations of this paper from the following three aspects: 1) regarding linguistics-aware data construction, we only perform seed-guided pattern enrichment for four reaction roles (product, yield, temperature, and time, see table 4) due to the lack of sufficient reliable patterns for other roles. incorporating more advanced pattern mining methods (li et al., 2018; chen et al., 2022) may alleviate this issue and discover more reliable linguistic cues, which we leave for future work. 2) as in the previous work, we adopt a fixed reaction scheme to extract structured chemical reaction information. however, there are always new informative roles in the text (jiao et al., 2022), such as experimental procedures (vaucher et al., 2021), so how to predict both roles and arguments without being limited to a fixed scheme could be a meaningful research topic. 3) reactie is capable of detecting chemical reactions within scientific literature by predicting if a given passage contains a product. however, accurate text segmentation of a paper remains an unresolved and crucial issue. incomplete segmentation may result in the failure to fully extract reaction roles, while excessively long segmentation may negatively impact the model performance. therefore, integrating a text segmentation module into the existing two-step pipeline may be the next stage in the chemical reaction extraction task."
898,"limitations first, as ear largely relies on gar generators, the performance of the method is closely tied to the quality of the generator used. we have attempted to use large language models such as t0-3b without fine-tuning as a replacement for the gar generator during testing, but the performance becomes worse. the main reason is that the quality of query expansions generated by t0-3b is too diverse, which makes ear has a higher chance to select from terrible expansions. in contrast, the output quality of gar is more stable. we may need a more complex mechanism that can exclude terrible query expansion if we want to directly use the query expansions generated by t0-3b during inference. second, ear has demonstrated a strong generalization ability to out-of-domain data, but the method may still face challenges when transferring to other languages without any supervised qa data, which gar and ear are trained on. although challenging, we are still trying to train the ear system without supervised qa data."
899,"limitations our model is evaluated in standard english datasets for classification. as we stated earlier we plan to investigate the cross lingual setting in the next step. the iterative nature of self-training imposes a high cost on the experiments. this has led to a few common practices. most existing studies (including all the studies that we used as baselines) employ one underlying classifier to carry out the experiments–i.e., bert or rnns. this practice albeit limiting, is justified by the argument that if an algorithm does not make any assumption about the underlying structure of the classifier, then one can safely select the best available classifier and use it in the experiments. we used bert in our experiments. another limitation is that, which is again stemmed from the high cost of self-training, one is typically forced to select a few sample sizes as labeled sets to carry out the experiments–e.g., 100 or 300. this is in contrast to similar research areas, such as active learning, when one can usually afford to report a learning curve to illustrate the performance with a few training examples all the way to using the full labeled dataset. given that we have 10 baselines, we reported the performance with 300 and 500 labeled examples."
900,"limitations in this work, we focus on small and medium size models (up to 134m parameters), while recent work in large language models (llms) targets models with billions of parameters(brown et al., 2020; chowdhery et al., 2022). it is unclear how well the performance improvement from the examined network architecture would translate to other model sizes or baseline architectures, e.g., gpt models. further on, it is unclear how these findings may translate to other application domains and datasets, or impact other nlp tasks, such as document retrieval/ranking. we will investigate these directions in future work."
901,"limitations we use the density matrix to represent modal features, and one of the advantages is that the matrix contains more information. however, the requirements for memory and large gpu resources also in- crease. based on the best hyper-parameter setting, the shape of a pure state is 16×100×100, while the shape of a density matrix is 16×100×100×100. at the same time, the matrix will also increase the calculation and time cost. in future work, we will explore how to reduce the computational expense, and it is an idea to build the sparse density matrix."
902,"limitations of our work as follows: • we only validate the effectiveness of the setmatching strategy for generative models on the coqe task. • we observe that the scale of the coqe datasets is quite small and has caused the model’s overfitting problem. in the future, we will conduct further research from the following perspectives: • explore further application of the setmatching strategy in multiple research directions, such as information extraction, sentiment analysis, etc. • utilize unsupervised data to better help the models mine comparative opinion information. • design data augmentation methods to relieve the data sparsity problem."
903,"limitations as mentioned above, the current study is limited to the question of whether (and when) conditioning turn-taking prediction on the response improves the performance. it does not yet show how the model could be incorporated in a spoken dialogue system. moreover, this study focuses only on written conversations without incorporating spoken dialogues. thus, the interpretations can be limited to dialogues that are relatively ‘formal’ without hesitations, repetitions, etc. note also that we only analyse lexical cues to turn-taking (just like with turngpt), and leave out other modalities for future work."
904,"limitations of each, and discussing some future directions. 19this is also related to warstadt et al.’s (2020) results, who show that better pre-trained models are less prone to rely on superficial (and potentially spurious) features for predictions."
905,"limitations we consider only lexical bias based on the cooccurrence between a token and a certain label in data bias for identifying shortcut tokens, while nlu tasks involve various types of data bias, e.g., overlap bias, position bias. although our method can mitigate llm-based task-specific models’s reliance on shortcut tokens, it can only identify a limited set of bias in the data. therefore, in the future we would like to incorporate more data biases to identify shortcut tokens and discourage llms from exploiting them."
906,"limitations although our proposed aligner has surpassed the existing lm-based alignment extraction methods in most of the datasets, it could not make any improvement for the en-fr language pair, as shown in table 1. this suggests that our proposed method might be only beneficial for more distant languages. on the other hand, for similar languages, it not only cannot add any information to the similarity matrix, but also its estimation for the alignment probabilities might add noise to the alignment extraction method. thus, investigating ways to more effectively estimate the alignment probabilities of source and target tokens might be helpful in future work. another limitation of our method, as well as other lm-based aligners, is that they first extract subword-level alignments, and then heuristically map them to word-level. by observing the aligner outputs, we realize that many errors occur when the pre-trained lm can not efficiently split words into meaningful subwords. this happens more often for low-resource languages or far languages from english (like persian or hindi). thus, achieving better subword tokenization in pre-trained lms or applicable methods to convert subword-level representations into word-level could help improve the quality of lm-based aligners."
907,"limitation by working in two directions: leveraging the intrinsic characteristic of frameset resources, including semantics-based clusters and cross-predicate role semantics, and tighter integration of other semantics-based tasks, such as word sense disambiguation, into srl. we hope our work will be a stepping stone for innovative research on high-performance srl systems for non-verbal predicate-argument structures, a problem that still needs extensive investigation. for this reason, we release our software and datasets at https://github.com/sapienzanlp/ exploring-srl. 3https:/"
908,"limitations part of our analyses and experiments is based on our parallel-semlink dataset, which provides parallel annotations for propbank, framenet, verbnet, and verbatlas. we take the opportunity to remark that this is a constrained setting, as these resources cannot be mapped 1-to-1 without losing information. as such, this setting may not provide the full picture of how these resources compare against each other. however, we also believe that a setting like this can at least provide an intuitive idea of the role of a linguistic resource in crossinventory generalization. creating novel benchmarks that can better compare the role of different linguistic resources is certainly a direction for future work that may provide novel insights into verbal and non-verbal srl. another limitation of our work is the small size of challenge-srl. even though challenge-srl contains only about 300 sentences, it features almost 2000 predicate-argument pairs, and this is a number that is sufficient to show the inability of a current state-of-the-art system to generalize across predicate types. we acknowledge that a larger benchmark may have provided further insights. however, we also note that, in our case, increasing the number of annotations would hardly have brought us to a different"
909,"limitations of monolingual factuality metrics. moreover, our exploration of the automatic factuality evaluation in cross-lingual settings illustrates its challenging nature. limitations the scenarios we studied are limited to chinese to english and english to chinese. for other languages, the factual characteristics may be different. the genre of the source documents we study is news or blog post. for other genres, such as dialogue, our"
910,"limitations our work sheds light on understanding the training dynamics of cross-lingual transfer learning of multilingual lms. in our work, we selected to use english as the source of cross-lingual transfer following previous work (vu et al., 2022). we acknowledge that using other languages as the source language can provide benefits depending on the task (lin et al., 2019; turc et al., 2021). our work does not focus on choosing source language to maximize downstream performance but instead focuses on the difference between classification tasks and generation tasks in cross-lingual transfer. secondly, we acknowledge that some of the datasets (yang et al., 2019; chen et al., 2022) used in our work are created by machine translation and human annotation. previous studies have pointed out that translationese in datasets affects cross-lingual transfer performance (artetxe et al., 2020a; artetxe et al., 2020c). we believe that translationese in datasets also have impact on xlrs. we leave the study of how dataset features (size, quality, translationese) affect cross-lingual transfer for future work."
911,"limitations the introduced dataset has a moderate scale, as it is currently designed for fine-tuning instead of large model pretraining. our proposed collection scheme can be futher applied to enlarge the dataset. moreover, as we focus on english, the data source has multiple language versions written by experts. hence, extending causaldialogue to multilingual is straightforward. with reward labeling, the dataset can be more intuitively used for offline rl. meanwhile, the dataset includes personality descriptions that can be used for personalized dialogue generation, even though is not the focus in this paper. finally, training a generative model on dialogue domain can require various computational costs, depending on the aspects such as lengths of input and output texts and number of model parameters, as well as special designs to prevent misuses."
912,"limitations although lclr shows great potential for unifying the slu decoding process, existing slu models experiment on a set of predefined labels (closed domain), and our lclr can handle the case of missing predefined labels in the train. it is interesting to try to perform lclr on a more challenging task of detecting out-of-domain (ood) detection where unseen intents/slots are not available."
913,"limitations our study has limitations in two aspects. first, multilingual transformers support a wide range of task types, and it is challenging to study our research question on all types of end tasks. we conduct experiments on two common types of end tasks, i.e., text classification and question answering. we leave the study on other types of end tasks in further work. second, under pmid, we only consider the situation that the end-task models are obtained by finetuning public pretrained models. the cross-lingual transfer of black-box end-task models is also an interesting research topic to study. besides, plugin-x reassembles the modules from publicly-available models rather than training from scratch, so it can naturally inherit the risks from those models."
914,"limitation since there is currently no standard evaluation metric for evaluating post-hoc explanations, we use aopc(k) as the quantitative evaluation metric, which is widely used in the research field. however, because different modification strategies might lead to different evaluation results, aopc(k) is not strictly faithful for evaluation attribution explanations (ju et al., 2022), thus, we evaluate with two modification strategies del and pad and we didn’t introduce new strategies to get attribution scores, which avoid the risk of unfair comparisons due to customized modification strategies mentioned in ju et al. (2022). even so, there is a risk of unfair comparisons because the aopc(k) tends to give higher scores to erasure-based explanation methods such as loo. we don’t conduct human evaluation because we believe human evaluation needs a very large scale to guarantee objective and stable, of which we can afford the cost. thus, we post visualizations of all explanations in our experiment to demonstrate the effectiveness of our approach (https://github.com/juyiming/ he_examples)."
915,"limitations our proposed degm for event skeleton generation still contains some limitations: • it only considers the problem of event skeleton generation, a subtask of temporal complex event schema induction. it is promising to explore the whole task, which includes entities and entity-event relations. • perspective from errors found that our model suffers from a tendency to generate correct duplicate substructures."
916,"limitation of the popular softmax layer is its global word embeddings. the problem would become more serious when there are more tokens whose meanings are locally defined (e.g., names in the booksum dataset). our methods would be more useful in those circumstances and might alleviate some biases described in shwartz et al. (2020) and ladhak et al. (2023). moreover, the meaning of tokens are also locally defined in many other applications such as variables in code or math problems, the new terminologies in a scientific paper, or the products in a sequential recommendation problem. we believe that our methods could become an efficient alternative of reranker (cobbe et al., 2021; welleck et al., 2022) and create impacts in those areas. finally, our results show that when there are some uncertainties in the next word (e.g., could be king or woman), existing lms could have some difficulties of copying the words from the context and our methods alleviate the problem. thus, our methods should also be able to improve the lexically controllable language generation models that put the desired keywords into the context such as goldfarb-tarrant et al. (2019) and lu et al. (2021)."
917,"limitation our primary focus is on the ood robustness of text classification tasks. however, there are other nlp tasks that the community should not ignore. glue-x currently does not include language generation tasks such as machine translation, summarization, and dialogue. moreover, extending the current glue-x to more real-world datasets from different domains is of great importance. we aim to make glue-x a continuously maintained project."
918,"limitations masking accuracy of classifiers reported in this work is for masking both the training and the test data for reasons explained in section 5.1. each result is for a different type of mask and hence for a different test set. our results cannot be used to gauge performance for unmasked or differently masked inputs that can be expected in applications. rationale evaluation with roar to evaluate rationales, roar uses the performance of a model trained and tested with input masked according to the rationales. the reported numbers therefore do not only reflect the quality of the rationales but also the difficulty of the task, the size of the training data and the performance of the machine learning method. furthermore, the measurements are influenced by randomness in training as the masking of training data changes the path of the optimisation process.13 the values have to be seen relative to baseline performance of full and rrand and in comparison to different types of rationales. domains the experiments are restricted to the two domains of the dataset, namely restaurant and laptop reviews, with just 28 aspect entity types and 14 attribute labels. we encoded these in a shared vocabulary with the review sentences.14 we did not explore alternatives such as using reserved embedding table entries to encode the domain and aspect categories (so that tuning these embedding table entries does not affect the embedding of the tokens of the review sentence) or using more natural question templates, e. g. adding function words where appropriate and lower-casing the categories. performance differences may change for other domains, number of aspect categories and the ratio of the training size of smallest domain and largest domain (in our work 4:5). task the absa task (see section 1) assumes that the aspect category is already marked in the input and labelled with entity type and aspect category. number of test scores on first sight, the high number of test scores could be a concern as testing many models on test data can lead to overfitting to test data. however, only the result for the full setting is a vanilla test set result. all remaining results are testing on derived (masked) test sets matching the masking applied to the training data. therefore, these results do not leak performance information for building better classifiers on the test data. using the test set here is convenient as the data set does not come with a validation set and the validation set we held out from the training data for selecting the training epoch is very small. language experiments are for english only due to availability of sea data. various factors may cause different patterns for other languages, e. g. (a) bert subword units, (b) evaluation excluding function words vs. languages that use mostly morphology instead, (c) freer word order may result in annotators producing more discontinuous ses. 13our reporting of averages over twelve runs, and in some cases 24 runs, compensates for the latter effect as each run shuffles the order of the training data and uses a new random initialisation of the classification head. 14for the aspect entity type and attribute label, sharing is reduced by using capital letters."
919,"limitations in this work, we limit our experiments to the most commonly used document-level system architec- 6the precision and recall are roughly the same for the f1 scores reported in table 4. ture and training criterion. other approaches exist, which might exhibit a different behavior in decoding. two out of the three document-level translation tasks we use in this work are low resource with less than 500k sentence-pairs as training data. we chose these tasks due to computational limitations and to be better comparable to other works, but higher resource scenarios are more realistic for actual applications. we limit the analysis of pronoun translation to the english-german language pair. also, there are other aspects of documentlevel nmt, like consistent translation of entities, which we did not consider in our analysis."
920,"limitations we discuss limitations and ethical consideration of our work. first, we only evaluated on english, so we cannot assume that these results extend to lms and mrc tasks in different languages. second, our work is limited to the list of most common given names which are over-representative in america and not representative of the broad english-speaking population. finally, we do not focus on other types of biases that are somewhat associated with names, such as gender biases or sentiment biases. we expect these limitations to be addressed in future work."
921,"limitations future work must look into the generalizability of the results presented here in other domains of language use, and other languages. while we present the utterances as constituting natural speech by one speaker (the congressperson who sent the tweet), it is likely most congresspeople employ social media teams that help in crafting the language of some of their tweets. however, we believe for the sake of interpersonal group membership, the relationship between the speaker(or speakers) and their target(s) would not be affected. techniques like inlp extract information that is linearly extractable. while we’ve shown that it is possible to extract and manipulate language information using such simple linear techniques, more complex methods like those proposed by ravfogel et al. (2022) might be able to manipulate more non-linearly encoded properties. the alterrep procedure, as can be seen in our results and in ravfogel et al. (2021), is sensitive to parameters like α and the number of inlp iterations. picking these parameters is tricky and we have done it in a manner that preserves information in the language model. it is possible that a different set of settings not explored here could lead to different results."
922,"limitations our work is limited as it has not explored the effectiveness of our implicit memory transformer in other tasks outside of simulst, such as asr. we have also not explored the impact of our implicit memory left context on alternative blockprocessing-based transformer models. furthermore, extensive ablation studies could help showcase the potential of the implicit memory transformer."
923,"limitations our experiments are conducted on transformerbased models with the same multi-modality fea- tures. considering the importance of entity information in textual context and specific regions of images, it is also important to investigate whether the performance of the model promotes with different methods of extracting multi-modality features. we use the bpe technique to encode all entities in input articles which may separate the whole entity word into several sub-word tokens and may affect the impact of vision features. there are still a lot of entities of captions that don’t appear in articles from datasets on our experiments."
924,"limitations although the work is aimed at better understanding bert’s internal representations, there is no transparent way to know on the basis of what features of the training data some particular sentences are found to be similar. for task 2, the representations may have been affected in unexpected ways by the process of creating averaged sentence embeddings. there is no way to fully exclude the effect of lexical context and thus get a representation of the meaning of a construction without noise in unsupervised transformer models, which may affect the extent to which we can accurately probe for a construction. in task 1, the set of potential words that could be predicted is limited by the bert tokenizer’s vocabulary. some limitations are caused by our choice to compare to stefanowitsch & gries’s results. we used the relatively small corpus that they used and we have demonstrated the methods for a limited set of two english constructions. we also limited the analysis of the results to the top 20 most strongly associated collexemes, as they did. using a larger corpus would probably yield more than 35 instances of the x-waiting-to-happen construction that s&g found and that our reproduction yielded. we also did not experiment with ungrammatical or perturbated input as such results cannot be compared to the original corpus study, which only uses natural language data. the scope of our study was also limited by to the construction-specific data collection, preprocessing and manual annotation required. for modern web-scale corpora, task 2 would require significant gpu resources. as the way in which bert is trained clearly differs in many ways from how humans acquire language, also according to the construction grammar framework, this bert-based work does not warrant any claims about how human language works besides extremely broad ones and findings are limited to"
925,"limitations; benchmarking cd with an existing standardized measure yields no simple answer to the question whether we are now talking about children’s actual tom. that does not make standardized tests uninformative, but contextualizes their merit: if we agree that tom (and language) are social competences, we should also test them in social contexts, not to claim superiority over but rather complement work done in controlled settings. our classroom context has as advantage regarding tom, that children feel more motivated to do a fun task, engage with narratives as natural finding place for mental state content, have freedom to explore the (social) scenario they want, and that their language has a social goal: immersing the audience in their narratives as possible worlds. this social context may stimulate children more to challenge their language skills. to entice their audience, children may leverage their vocabulary skills to refer to rare settings, uncommon objects, unorthodox characters, and special social situations which is not possible in standardized language tests like the peabody picture vocabulary test (dunn and dunn, 1997). additionally, children may also recycle complex linguistic structures and plots from prior exposure to narratives in their own narratives, to entice their audience. thus, the influence of the social context could result in more complex language use than one would expect based on age, which makes the direct relation between age and language competence in narratives less obvious. overall, our results support the link between more complex language and tom. that said, not all tom-related content requires complex language. explicating character thought could linguistically also be represented without complement, e.g. with free direct thought (‘was she angry with him?’) (leech and short, 2007; van duijn et al., 2022); moreover, the words used in this thought are not complex, nor is the syntax. this example serves to illustrate the point that in our approach, our classifier makes no assumptions at the outset about the linguistic complexity of tom-related content."
926,"limitations we discuss a few limitations of our work. one limitation of self-improved is its complexity in usage. the data augmentation process involves generating predictions for the entire training dataset with a large beam size, resulting in a time complexity of o(nk), where n is the train dataset size and k is the beam size. additionally, the fine-tuning step to derive θimproved also adds a significant amount of computational complexity. this limitation is discussed in section 6 to weigh the performance benefits of our method against the computational sacrifices. another limitation is that self-improved has only been applied to encoder-decoder models in this work. however, it is also applicable to other types of auto-regressive models, including encoderonly models, which are commonly used for tasks such as code completion (radford et al., 2019; lu et al., 2021; guo et al., 2022). a few models can be named are gpt models (radford et al., 2019; brown et al., 2020), codex (chen et al., 2021), codegen (nijkamp et al., 2022), etc. further research into these applications is left for future work."
927,"limitations a fraction of 157 essays was too long to fit into our transformer model so that arguments later in the text have not been identified at all. as gold standard information about the writing prompt for a specific essay was not released with the dataset, we had to rely on automatically assigned prompt information with an estimated average accuracy of 0.97 according to ding et al. (2022). in a realistic class-room setting, however, the information about the writing prompt would be readily available, thus we probably underestimated the effect of adding prompt information. we tested our models on the persuade dataset of english high-school writings only, thus we cannot be sure whether results transfer to other educational contexts and languages. we will address this further in future work, where we aim at using essays in german and from efl contexts as well. since our model was trained on a limited amount of data, it may have the potential risk of discouraging students to write innovative, but effective arguments. as discussed in automatic essay scoring approaches, computers may be able to analyze writing for the presence or absence of certain words or structures (in our case arguments), but they cannot really understand or appreciate a writer’s message in the same sense that human readers can (powers et al., 2002)."
928,"limitations the main limitation of this work is our use of automatic metrics rather than human evaluation. first, the score distribution produced by a metric is not guaranteed to be similar to one produced by human annotators, which could influence results. secondly, the metrics we examined do not incorporate context. motivated by evidence that document-level (or contextual) information is becoming necessary to distinguish between human translations and high quality machine translation (läubli et al., 2018; toral et al., 2018), recent wmt evaluations have incorporated context. since the human annotations are influenced by the context in which they appear and the automatic metrics are not (i.e., given an identical segment in two different contexts, the automatic metric will score them identically while a human annotator may not), additional study may be necessary to answer questions such as whether additional preceding source context should be displayed to annotators (as suggested in castilho et al. (2020)), to determine how much additional time reading this context would take (which may influence the annotation budget), or to determine whether human annotator behavior may differ based on where in a document the snippet comes from. we also do not directly address issues such as the best interfaces for human annotation; a problem that is mostly orthogonal to the question of what data should be annotated. in this work, we also follow the approach in the wmt metrics shared task of treating the scores assigned to systems (in our case by automatic metrics rather than human annotators) as full rankings of systems, rather than as clusters of systems. in practice, this may mean that statistically insignificant differences between systems are considered on par with statistically significant ones when we examine reorderings that occur based on different sampling procedures. while this is a major concern in human annotation (where there is also an effort to handle annotator variation, a separate source of instability), it is less of a concern in this setting where the annotation is guaranteed to be consistent. one additional limitation to our proposed future work of using metrics as a pre-sampling approach is that they may not perform equally well across all languages. see appendix a for the list of language pairs on which these experiments were performed."
929,"limitation to better understand the limitations of the proposed model, we carry out an analysis of the errors made by pigeon. specifically, we randomly select 100 instances that are incorrectly predicted by pigeon and summarize the primary types of error. the first error category is boundary prediction error. since we modeled the ape task as a sequence labeling, our model may only recognize a part of an argument. thus, multiple consecutive arguments may be identified as a single argument. the second type of error is caused by the absence of semantically similar words in the argument pairs. in this case, the proposed probing graphs cannot model the relations between argument pairs. third, another error category occurs when semantically similar words are also presented in non-matching argument pairs. the argument relation may be misled by these words. it suggests that certain relation modeling method needs to be devised in the future so as to better infer argument relation. for example, we may leverage the high-level topic information over argument pairs to guide the learning of relation-specific features. in addition, the proposed probing approach may be computationally expensive and we can alleviate this problem by saving the similarity of all word pairs for one time for the entire dataset. we will address this issue in future work."
930,"limitations despite the strong performance of diffusum, its design still has the following limitations. first, diffusum is only designed for extractive summarization, and the diffusion generation module only generates sentence embeddings instead of tokenlevel information. thus, it is not applicable to the abstractive summarization setting. moreover, diffusum is only tested on single document summarization datasets. how to adapt diffusum for multi-document and long document summarization scenarios need further investigation. in addition, our generative model involves multiple steps of noise injection and denoising, compared to discriminator-based extractive systems."
931,"limitations this work has some limitations regarding the architecture and the data used: our model assumes that masked modeling could be used as rehearsal and anticipation tasks; however, other approaches could also be effective. we use transformer layers, so our model scalability is tied to the scalability of the transformer model. also, our approach relies on obtaining additional annotation from pre-trained models for the masking process, so we are limited to the misprediction of those models. we only tested our model with the english language; further exploration of other languages would be valuable to validate the language-independent functionality of the model."
932,"limitations of our method are as follows: (1) despite the better performances our method deflate achieves on multiple ave experiments, its mechanism of using attribute as queries needs to construct the same number of sequences as attributes for a target product, which requires more time for training and evaluating when there are particularly many attributes to consider. (2) the low f1-micro scores of deflate and all other leading methods for ave on our desire dataset emphasizes the demand of further researches for the information extraction of the e-commerce products with implicit attribute values. and we would explore strategies such as incorporating external knowledge (structured or unstructured) to further enhance the ability of our method on the ave task in future works."
933,"limitations for now, the latent of our skill is sampled from a distribution, whose flexibility is not fully investigated. we intend to exploit more flexible skills and goal discovery, or direct generation via state or action. additionally, the sparse reward in text-based games is also a burning challenge, which hinders the efficient exploration of agents. our abstracted action, skill, to some extent eases off this issue, but is not enough. we will dive into this more and design a fancy solution later."
934,"limitations although our single-site interchange interventions provide causal evidence that particular sub-circuits are necessary for a particular downstream behavior, this technique has known limitations addressed by recent distributed alignment search (das) approaches (geiger et al., 2023). first, it will overcount certain “synergies:” when a single effect is jointly produced by the conjunction of multiple heads acting in concert, we will identify all heads as making distinct contributions to the circuit. second, it will under-count “redundancies:” if there are multiple heads that are individual sufficient to produce the effect, then no single head will be detected as strictly necessary. ideally, rather than single-site interventions, we would explore all combinations of different heads to find minimal spanning sets that are both necessary and sufficient, but this procedure becomes intractable given the number of heads, requiring more sophisticated optimization-based approaches to find promising sets (e.g. csordás et al., 2021; de cao et al., 2022)."
935,"limitations due to the lack of research in this area, there is only one direct related paper to our work, which serves as the main baseline in our experiments. we hope we can compare our method with more related works to verify its effectiveness in the future. also, the domain adaptation problem not only exits in the machine translation filed, but also various generation and understanding nlp tasks, where we should evaluate our method on if we are not limited by time and resource."
936,"limitations the most notable limitation of our work is the lack of external context. consideration of external contexts that may be relevant for the classification task in our current models, such as the profile bio, user gender, post history, current and past political scenarios of the concerned region, and so on, might prove beneficial for the results in this field. our research now focuses majorly on only six types of social biases rather than all conceivable degrees of prejudice. we also focused on utilising hindi, english, korean, and italian in our study, and the hindi dataset is primarily from the indian context. the limited scope of concern can be further explored with our presented experiments to prove to be fruitful for a wider range of audiences by covering datasets bias annotations pertaining to other low-resource languages. we show the effectiveness of few-shot transfer learning using language models with relatively fewer parameters as compared to recent state-of-the-art language models."
937,"limitations our model improves topic interpretability of ntm with seed words, but we believe there are still limitations to be explored in the future works. for the methodology part, large-scale pre-trained language models could be considered to provide more context information when incorporating seed words. for the experiment part, only single label dataset are used for extracting seed words, and more explorations on multi-label datasets should be conducted."
938,"limitations although our proposed curriculum can be applied to any multimodal architecture, curriculum aware loss requires modifications for use with dual encoder architectures that don’t use cross-modal attention. additionally, we use an off-the-shelf partof-speech tagger to divide the data into different phases. as such, the correctness of this division is dependent on the quality of tagger. a poor tagger can negatively impact the curriculum design. moreover, our approach doesn’t apply to possible image-captions dataset which contain only short captions, containing possibly only one noun."
939,"limitations 1. error accumulation: incorrect parsing results will affect the reasoner. in our experiments on geometry3k dataset, incorrect parsing results lead to a substantial 21.0% performance drop. 2. the reasoner relies on manually predefined theorems, which limits its adaptability. 3. the random exploration in the rl training process leads to uncertainty in the rate of convergence."
940,"limitations our work is the first study of generative asqp task from the view of what not to generate. despite the state-of-the-art performance and template-agnostic effectiveness, our work still has limitations that may guide the direction of future work. firstly, implicit information is still challenging for uaul. failed cases in error analysis §3.3.6 demonstrate that tough cases require in-depth semantic understanding. though uaul achieves wide improvements in the generation paradigm, it struggles to deal with implicit cases. secondly, in this work, we only design tokenlevel marginalized unlikelihood learning. since aspect sentiment quadruplets contain four types of information, considering span-level and whole sequence-level negative sample learning may attain further gains. thirdly, uaul increases the training time, as shown in table 8. we optimize the implementation by parallel computation. meanwhile, mc dropout is only adopted in the last dropout layer. the training time is still significantly enlarged. nevertheless, our method does not require additional human labor, which has obvious advantages in real applications."
941,"limitations in this paper, we only evaluated our method on a limited number of nlu tasks and datasets. it is possible that our method may not generalize well to other tasks or domains that require different types of prompting knowledge or cloze-driven prompts. a promising direction for future work is to investigate how the prompt design and the learning objective influence the performance and robustness of plms on few-shot nlu tasks."
942,"limitations an obvious limitation is that our work relies on the typology features of languages. some extremely rare languages might lack typology studies (its features are missing values in the wals database). our approach is limited for these languages. another non-critical limitation is that the technical contribution of our work is limited. after detailed analyses of position vectors, our methods for generating position vectors are not that complex, but we believe that an effective method is not neccessarily complex, and designing experiments to reveal key properties of position features and their connection with linguistic knowledge could still make solid contributes to nlp community."
943,"limitations coverage of experiments there is room for further exploration in terms of model architectures, data, and evaluation settings in our study. experiments in more diverse settings would enhance the generality of the"
944,"limitations our research presents an initial step toward a knowledge injection framework for msa and still has some limitations to be tackled in the future. firstly, we can learn more disentangled representations by carefully selecting contrastive pairs for further improvement. secondly, it will be interest- ing if we extend our method with multiple external sources that come from different knowledge domains."
945,"limitations of a study. researchers and developers are encouraged to make justified accurate claims about their achievements. in addition, as the community grows and new practices are introduced, including all necessary practices in a single study is expected to become infeasible. nonetheless, our framework provides a comprehensive reference to collect the necessary evidence for validating nlu evaluation. “why is the"
946,"limitations despite the strong performance of the proposed attenwalker. there is still large room for improving efficiency. for example, the time cost of our method is still high. since we need to search for all transformer layers and heads to find potentially re- lated spans, the dataset construction could be quite time-consuming. therefore, an algorithm could be designed in the future to pre-select proper layers and heads for attention-based graph walking, which would save much time in dataset construction."
947,"limitations despite the remarkable improvement on complicated information extraction, there are still some limits of our method. first, due to the multi-round argument extraction modeling, we discard the parallelism in element extraction. furthermore, the mdp process interacting with the dqn further increases the computational load of the extraction process. so compare to other methods, our framework is relative slow at the inference stage. second, though our framework can be easily adapted to different extraction task with different schema, we still need an extra module helping identifying the relations (event types) in the instance beforehand. because of the difference task definition and modeling (extraction task and classification task), although recognizing them potentially implies order decision making, they are beyond the scope of this paper."
948,"limitations as with many other model-based metrics, rise is best suited for evaluating offline due to the expensive nature of inferring with a large model. it is not as well suited as other metrics like rouge or bleu for evaluating during training or finetuning. we leave the exploration of using rise for evaluation-in-the-loop kind of training for summarization models future work. additionally, as with other model-based metrics, it is possible that the models may have seen some of the data during pretraining as is in the eval datasets. we do not think it would be too significant, as the pretraining task (for t5/mt5 for example) is rather different than a summarization task and, more importantly, it does not include the gold reference. thus the model would not be able to make such a connection easily despite having seen the data. we chose to work with the t5-family of models due to the ease-of-use for others to implement and improve upon our ideas. we would expect our ideas to work just as well with other models, such as bart, mbart, longformer, etc. following recent works, we have studied the evaluation based on the summeval benchmark (fabbri et al., 2021). in the future, we may want to build other benchmarks that covers more domains and languages to compare different methods."
949,"limitations dataset noise as the audios are obtained from the videos on youtube, the quality of the videos will have an impact on the quality of the final transcript. for example, inferior recording equipment may affect the quality of the sound, although we have done noise removal to keep the quality, the presence of background noise will cause some losses in the transcribing process. relationship between transcripts and scenes in this work we get the transcript of shiba inu dogs, and we also find that the dataset covers a variety of activities and scenes. there may be an interesting relationship between the dog vocal units and the environment including the scene and activity. however, we did not quantitatively analyze the relationship. considerably more work will need to be done to discover semantic information in dog barks. phoneme labeling accuracy in section 2.6 we cluster the syllables and assign phonetic symbols to them. then in section 4.2.1 we evaluate the result by mos. it can be seen in figure 10 that the accuracy score is not very high, which can be improved in our future work."
950,"limitations constructing semantic graphs, in general, requires multiple additional tools, which inevitably introduce errors. in this paper, we worked to reduce potential errors. we used the sota amr parser and coreference resolution model and adopted mechanisms to reduce error propagation to our final graphs. however, we have not measured errors involving topic segmentation and amr parsing due to the expensive human annotations required. it will be helpful to investigate how these errors can impact system performance when they are combined with encoder-decoder llms and if the amr encoder is robust to small errors in amr graphs. we leave these investigations as future work. finetuning llms for long dialogues requires many gpu hours and energy. therefore, our hyperparameter search was limited to 3 different values for learning rates, 3 for warmup steps, and 2 for batchsize. a more extensive search may bring additional improvement to our model."
951,"limitations in this paper, we use a pre-trained bart to observe the changes in the model intrinsic features by varying the fine-tuning objectives and datasets. however, our methods can be made significantly more generalizable when the range of summarization models (e.g., pegasus (zhang et al., 2020)) and datasets (e.g., frank benchmark (pagnoni et al., 2021)) are broadened. additionally, we can try using other evaluation methods, such as factcc (kryscinski et al., 2020), reported to have a high correlation with human judgment (pagnoni et al., 2021) to interpret the model intrinsic behavior during the shuffle tests. we leave the research on methods to optimize the inductive bias during the fine-tuning process to improve the factual consistency as future work. our experimental results can also be integrated with those of previous studies focusing on hallucinations in datasets (kryscinski et al., 2020; wan and bansal, 2022)."
952,"limitations efficientvlm is applied on x-vlm. however, there are also many recent fully transformer vlms achieving comparable or better performance. therefore, applying our distilling then pruning framework on other state-of-the-art vlms can be interesting. also, we do not apply quantization or matrix decomposition, which are prevalent model compression techniques."
953,"limitations of such an approach in section 8. an additional question arises of whether one dataset may have multiple documents associated with one individual. there are several ways to go about dealing with this. one standard approach in differential privacy is to linearly scale the ε parameter. thus, if there are k documents associated with a given individual, then a privacy budget of kε is accounted in total (dwork and roth, 2013). another option would be to simply append all texts associated with one individual into a single ‘document’, rewriting this using just a single ε privacy budget."
954,"limitations of the privatized text rewriting approach as a whole. future research directions include utilizing large-scale pre-training to potentially reach a better privacy/utility trade-off, as well as investigating domain specific text rewriting for relaxing the strict requirements of the ldp approach."
955,limitations of the ‘classical’ gaussian mechanism.
956,"limitations due to the limitation of time and resources, in this work, we select a relatively small number of clusters during the clustering process, which results in coarse-grained clustering. fine-grained clustering can provide a better latent concept but will also lead to increased computational resources and time consumption. we will attempt to trade-off between the cost and the granularity of clustering in future work to further explore the impact of the latent concept on cskg completion. besides, our node clustering module is not integrated in an end-toend manner in our work; we will consider using the topic neural network to construct an end-to-end model."
957,"limitations the limitations can be illustrated from the perspective of task development: defi originally evolved from sentence-level work, as is clearly evident from the large number of sentence-related annotations retained in dlef corpus. our work benefits from these abundant annotations and achieves huge performance improvements. currently, there is a trend to gradually move towards end-to-end practice in event factuality identification. for example, the studies based on the dlef-v2 (qian et al., 2022a,b) and eb-dlef (zhang et al., 2022a) corpora have attempted to use less annotation information. although these efforts do not achieve competitive performance for the time being, it is an exciting research direction because it allows models to be more easily applied directly to realistic scenarios. the limitation of our work lies in the fact that it runs counter to the end-to-end concept, so we need more other work (e.g, event extraction and sefi models) to apply the model to the real world, which makes our work less applicable."
958,"limitations we evaluate hybrank on natural questions, ms marco and trec 2019/2020 datasets, which focus on english open-domain question answering. although none of the components in hybrank are specifically designed for english, the verification of hybrank on other languages is limited. otherwise, there are more general information retrieval tasks involving diversity or broader coverage in the returned results. considering the possibility of lacking collaborative property, whether hybrank can generalize to these high-coverage retrieval tasks is still inconclusive. as transformer encoder architecture is adopted in the sequence interaction and aggregation, the computation cost would be unacceptable when the length of passage list or number of anchors is too large. this is also the reason why we only conduct experiments with anchor numbers no more than 100. besides, hybrank only uses similarities computed by off-the-shelf retrievers as input features, and thus lacks sufficient interaction between raw inputs. the performance of hybrank may be limited by the capability of upstream retrievers. how to incorporate the interaction of raw inputs into hybrank while avoiding massive computation cost is still an open problem for further investigation."
959,"limitations of existing inversion attacks and propose a generative embedding inversion attack to better recover original text sequences given their sentence embeddings. then we show that lm-based sentence embedding models are potentially vulnerable to our proposed attack. we conduct extensive experiments to demonstrate the inability of previous embedding inversion attacks and disclose the privacy risks from our attack. the defenses against the embedding inversion attack are not well studied yet, even though it is much more malicious than the attribute inference attack. for future work, we call for more attention to effective defenses to address the embedding inversion attack with minor costs. limitations from the adversary’s perspective, our attacker model’s main limitation is the incapability of recovering exact domain-specific tokens. during our experiments, we evaluate attacking results on the personachat and qnli datasets. the personachat dataset collects daily conversations between speakers with almost no expert knowledge. the qnli includes question-answer pairs from wikipedia with far more domain-specific named entities than the personachat dataset. by comparing the attacking evaluations in table 1, 3 and 10, all attacks on the personachat dataset are more successful than attacks on the qnli dataset. for instance, in table 1, f1 scores on pc are 0.1∼0.2 larger than on qnli on average. in addition, qnli 2 of figure 5 shows that geia fails to recover the exact location “fresno” 7 out of 10 times. though most inverted results are similar to “what was the population of the city in 2010?” it is hard to capture the exact city name “fresno”. ethical considerations we declare that all authors of this paper acknowledge the acm code of"
960,"limitations although our qaar achieves better performance on inductive relation prediction, it still suffers from some limitations. first, for a give query we extract a k-hop subgraph without using any sampling method, which will require large gpu memory when the extracted subgraph is large. second, our qaar does not leverage logical rules to enhance the performance which has shown useful in previous methods (lin et al., 2022). we believe that our method can be further improved by incorporating logical rules. we will leave these opening issues in the future work."
961,"limitations although the ckcl performs satisfactorily in erc, there are still some limitations. because ckcl primarily concentrates on the effect of modeling context and external knowledge on the prediction results, when met some tasks that do not rely on context and external knowledge, pseudo labels can not be annotated, which causes the paralysis of the ckcl. in addition, when the class distribution of the sample is not uneven, the improvement of emotion scl will be weakened."
962,limitations of single-modal speech.
963,"limitations the proposed clkd is technically applicable to other nlp tasks, but we discuss the effectiveness of the approach for question answering systems, specifically for answer sentence selection (as2) tasks. in this study, we put our focus on as2 tasks as the research community has not well discussed or proposed multilingual as2 tasks/datasets. we also find that using only english teacher models is another major limitation of this study. however, choices of teacher models in the proposed clkd are not limited to english models. it would be interesting to discuss the generalizability of the proposed clkd beyond as2 tasks, but we note that such"
964,"limitations due to time and budget constraints, this work remains limited in a number of important ways. the relatively small size of our corpus and its specificity to covid-19 necessitates the development of systems with richer inductive biases and the ability to effectively transfer knowledge from related corpora like fever. additionally, due to the large size of cord-19, the database of scientific literature from which we draw the abstracts in check-covid, it is difficult to evaluate abstract retrieval components like vespa with our annotations. there are likely many abstracts in cord-19 in addition to the ones we’ve selected that contain evidence relevant to any given claim, precluding both measurements of abstract precision and the evaluation of notenoughinfo in the full fact-checking pipeline setting. finally, as the claims in checkcovid are drawn from western, english language news sources and annotators, they are likely unrepresentative of the full range of covid-related (mis)information in need of fact-checking online. 6experimental details can be found in the appendix c."
965,"limitations even though our work improves early exit performance effectively, some limitations are still listed below: • our approach focuses on making the intermediate representations of early exit models capable of general linguistic representation learning and task-specific representation extraction. therefore, we did not fully use the model’s high-level representation and fuse representations of previous layers, which may restrict the performance of our method. for future work, we would like to strengthen the information interaction between layers to make full use of the previous layers’ predictions, thus optimizing the representation for the internal classifiers. • although our early exit method has achieved better performance, we have lost some inference speed due to the introduction of additional adapter modules. in the future, we will try more efficient adapter-based tuning. • in recent years, the parameter size of generative pre-trained models has been continuously increasing, leading to remarkable performance on various nlp tasks. there is an urgent need to develop inference acceleration methods for generative pre-trained models. unfortunately, our method is limited to discriminative pre-trained models. our future work will investigate early exit strategies for generative pre-trained models."
966,"limitations size. a limiting factor for some low-resource applications might be the size of a saved factorizer file. we only have to store the subword vocabulary, which however takes substantially more space than bpe as it needs to be stored as dawg trie to keep the tokenization speed similar to bpe. for example, the saved english factorizer takes about 115mb of space while the english bpe with 32k subwords takes just about 1mb of space. we believe that the space requirements are negligable compared to the size of large language models, but they can a limiting factor in some edge cases. the parameter-efficiency of factorizer follows the basic nature of factorized representations: a sequence of 3 bytes can represent more than 16m values (2563). that’s how we can embed millions of subwords with a negligible parameter count. on the other hand, when we store a vocabulary with millions of subwords on disc, it necessarily requires more space than a bpe vocabulary with 10 000s of subwords. glue performance. while our main interest and focus has been on morpho-syntactic downstream tasks, it is reasonable to ask what is the performance of factorizer-based language models on other tasks, such as natural language understanding. we utilize the pretrained english language models and finetune them on 8 glue tasks (wang et al., 2018). the results in appendix c indicate that in this setting, our method is comparable to bpe but not better on average, even though both approaches stay within a standard deviation from each other. we hope that these results can be improved in future work."
967,"limitations there are two main limitations to our study. the first is that the stimuli used were limited to those provided by urbach and kutas’s (2010) study. this is because, as stated, we wanted to be able to compare the patterns in the language models’ predictions to the patterns in the human n400 response. thus, we do not look at logical quantifiers like kalouli et al. (2022), or any others that have previously been studied (in, e.g., pezzelle et al., 2018; talmor et al., 2020). the other (and perhaps more important) limitation is in the models we were able to use. crucially, we were not able to access models larger than gpt3 175b such as palm 540b (chowdhery et al., 2022). this is important because recent work has shown that some inverse scaling patterns become u-shaped (i.e., as language model size increases, performance degrades and then improves again) with such larger models (wei et al., 2022)."
968,"limitations although our primary effort in this work was to extract as much parallel corpora as possible, the improvement in the performance has been found to be only marginal. the labse and qe-based filtering experiments involve a hyper-parameter called ""threshold quality score."" to achieve optimal results, we conduct experiments with different values of this hyper-parameter. the proposed few-shot transfer learning technique requires a small amount of data that needs to be annotated by multiple annotators."
969,"limitations the main limitation of our work is that 100 clauses is a small test test, but this was necessary to keep our human evaluation experiments manageable. we furthermore believe this was sufficient to be able to draw meaningful"
970,"limitations limitations on the evaluated language models and obtained results: the presented model architecture utilises various pre-trained language or image models. the main limitation of the experimental evaluation is not using other language models. due to the limited budget and processing power, we have included the language models that have been shown to perform better based on the previous work. another limitation is that we excluded language models that exceeded the 80 gb memory of an nvidia a100 gpu. our experiments led to different results for the gpt-3 compared to yang et al. (2022). it can be explained by using different methods for converting images to textual representations and slightly varying prompting structures. limitations on the used image models: the limitation concerning the pre-trained image models is that we selected a handful of methods based on their success for related tasks. including other pretrained models would increase the parameter space and thus increase the budget for the study. limitations on the selected datasets: all datasets are multimodal tasks where the underlying text is only in english. the choice of the dataset is related to the fact that there are limited multimodal datasets in other languages. the evaluation metric for the ok-vqa dataset requires the output to match exactly one of the expected answers. it counts as a wrong answer even if a slight change in the answer or another paraphrase is given as an output, e.g. “race” vs “racing”. we applied the same evaluation criterion and left this improvement as future work."
971,"limitations our proposed metric is mainly designed for a turnlevel evaluation of dialogue systems. we recognize that our metric may not generalize to other evaluation scenarios directly, such as dialogue-level evaluation or human-chatbot interactive setups. as shown in section 5.5, the easiest way to extend our metric to a multi-turn dialogue evaluation is by evaluating every turn in a dialogue individually, and then aggregating their scores. however, as the dialogue-level evaluation is not considered during the development process of our metric, it is not clear whether such a simple extension would be applicable without a decrease in performance. nevertheless, as turn-level evaluation is a fundamental component to build a holistic evaluation framework for a dialogue, we believe that it is an important task to investigate better evaluation metrics for individual responses."
972,"limitations there are two limitations to this work. (1) the total number of known and unknown intents are predefined, requiring an extension in real-world scenarios; (2) the frame knowledge is predefined and, therefore, inflexible to address complex intents. in addition, some user queries have no frame in framenet matching. there are additional computation costs for frame knowledge learning. the model fine-tunes two bert(bert-base-uncased, 340m) models in the training stage and runs sentence-bert in the evaluation stage. the pre-training stage of our model lasts about 10 minutes, and clustering runs for 90 minutes on clinc with a 75% known intents ratio, both using a single nvidia tesla v100 gpu(32 gb of memory)."
973,"limitations in this paper, we studied paragraph-level qag models, which limits their input up to around 500 tokens, and the same approach cannot be easily applied to longer documents. also, the answer is an entity or a phrase consisting of a few tokens and the question requires one-hop reasoning, so our models are not able for use in generating longer answers or multi-hop questions. as far as the languages are concerned, the models studies here are english only and to adapt squadshifts qa evaluation in other languages, we need qa datasets to train and evaluate the qag model in those languages. the focus on this paper was on evaluating the quality of generated question-answer pairs. as such, we do not attempt to achieve the best qa model possible, but rather use question answering as an extrinsic evaluation. this extrinsic evaluation could be further enhanced with an intrinsic manual evaluation that we did not perform in this paper. finally, given computational constraints, our qa evaluation is based on a single model only. again, the goal here was not to achieve the best qa performance, but we acknowledge than using different models could lead to different results."
974,"limitations the representation dimension (default is 32) is important but limited by our gpu resources. with the support of large gpu, a large dimension (e.g., 512) may achieve better performance. we also attempt to expand the inductive setting (relational patterns are same in the source and target kgs) to the independent setting (the source kg is independent from the target kg), but experimental performance is not good. that is, if the two kgs are irrelevant, it may be impossible to transfer information."
975,"limitations we discuss the limitations of pluglm as follows: (1) despite the strong performance achieved by our approach with dpm, it results in a reduced inference efficiency at the same time due to the mips search. for example, pluglm is about two times slower than pure transformer-based models in glue. this would be more crucial when the external memory is much larger. potential solutions to this issue include (1) constructing the memory using a coarser granularity (borgeaud et al., 2022); (2) compressing dpm by semantic clustering as in tay et al. (2022) or knowledge summarization as in xu et al. (2022). (2) in this paper, we choose wikipedia for dpm construction and pluglm pre-training. while wikipedia is the most commonly used data source for language model pre-training (devlin et al., 2019; liu et al., 2019), there are also many other types of knowledge not covered in wikipedia, and how to integrate different types of knowledge (e.g., factual, commonsense, syntactic and semantic knowledge) into our framework remains under-explored. (3) although this paper proposes a general architecture that is applicable to plms of all kinds and sizes including bidirectional (devlin et al., 2019; liu et al., 2019; yang et al., 2019), unidirectional (radford et al., 2018, 2019; brown et al., 2020) and encoder-decoder-based plm (lewis et al., 2020b; raffel et al., 2020; song et al., 2019), we only experiment with bidirectional models in moderate size. in particular, we believe this architectural design would be greatly beneficial for llm (smith et al., 2022; chowdhery et al., 2022; ouyang et al., 2022) for the following reasons: (1) the parameters of llm could not be easily updated once the pre-training is done due to the unaffordable training cost. (2) the additional latency cost by mips retrieval is negligible compared with that of the whole llm."
