,Text
0,"First, our work essentially relies upon a generative language model to understand the relationships between the sentiment elements in contrast to discriminative/extractive models which make structured predictions by design. As a result, our model is susceptible to usual anomalies suffered by generative models e.g., malformed outputs. We recover the quadruples from the model’s output sequence using regular expression based matching with fixed templates, as a result, an end-user will never receive any irrelevant text generated by the model. However, the accuracy will still be impacted in such cases nevertheless. Second, input sequences in user-generated content can be arbitrarily long and that might result in increased decoding time because of the underlying generative model. Last but not the least, all the instruction templates we provide in this work are designed solely for English. It would be interesting to explore systematic ways to be more language inclusive for instruction tuning based ABSA."
1,"Due to the high-impact nature of the solved task, we review the ethical considerations made during this research project. Additionally, we outline further steps to ensure safety and transparency beyond publication, as well as recommendations for buildup work.
First, let us focus on the presence of biases in the data. We put extensive procedures in place even at the very start of the project. By inviting media researchers into our core team, we wanted to minimize misunderstandings and mistakes that scientists from the field of computational linguistics could easily make when assembling the methodology for the task of trustworthiness assessment due to their limited knowledge of the current literature and theory in the area of journalism. Prior to the data annotation, we invited scholars in media studies and journalists from the industry to a series of workshops, where we asked them to submit feedback and discuss the methodology. Based on the assembled comments, we kept updating it until a general consensus was reached. In terms of the annotation process itself, multiple safeguards have been employed to prevent annotators’ bias towards specific sources or authors (that may affect the classification).
Second, let us shift towards the ethics of using any technology built around this data in the wild. We want to stress that anyone using this dataset for the purposes of creating a trustworthiness classification system should provide transparent information to the users that this process is automatic and hence faulty to a certain extent. We must note that it still needs to be determined how models trained on this data generalize for future articles (i.e., news about topics and events they have not encountered in the training set) and news sources not included in the training set. A study into these should be conducted prior to making this technology available
unrestrictedly to the public. Despite bearing these safety questions in mind is crucial, such systems can eventually be great assistive tools for people reading news stories online. The potential benefits of such technology should support initiatives to safeguard it first and establish public and academic trust."
2,"Given the subjective nature of our proposed task, this work does have some limitations and challenges. Firstly, the notion of harm or potential
to do harm is seldom an objective factor and is also difficult to measure or quantify. Our experiments on inter-annotator agreement use a small dataset, so this study could be expanded with collaboration with social science researchers to better qualify how people perceive the agenda in different articles. Our work is also grounded in the United States, so it may have limited applications to the news in other countries (discussed more in Section 8). Secondly, our data and framework can be used to build and train a system to perform posthoc detection of harmful agendas in news articles. However, in a real-world system, this identification would likely need to happen on the fly, so as to make readers aware of these agendas as they are exposed to the articles. Finally, another aspect that we have not addressed in this study is the effect that a platform or community may have on the perceived harm in an article. For example, on dedicated social media channels hosting discussions on alternate theories and contentious topics (such as the efficacy of COVID-19 vaccines), a junk science article with dubious claims may not be as “harmful"" as opposed to the same article being posted on an open forum where readers may perceive it as scientific fact, thereby making the article more “harmful"". The context in which news articles are disseminated may have a profound impact on this perceived harm and this may be an interesting direction for future exploration."
3,"Our study has several limitations that should be acknowledged. First, the study was conducted on a relatively small test dataset. Future work is needed to assess whether our results are generalizable to larger datasets. Second, while we employed a rigorous methodology for evaluating ChatGPT’s performance, we have not measured other safety criteria, such as biases or privacy issues in using
14https://open-assistant.io/ 15https://lmsys.org/blog/
2023-03-30-vicuna/
this model. Third, our study focused only on the initial step of suicide risk assessment and did not explore the use of ChatGPT in ongoing monitoring or intervention. Fourth, we are unsure if the UMD dataset has been used in the training of ChatGPT in any capacity since the specifics of the training data of ChatGPT are not disclosed to the public. Future work should focus on creating new datasets to assess the performance of ChatGPT on fully unknown test sets.
It is important to note that despite these limitations, our work represents an important first step in understanding the potential for ChatGPT in suicide risk assessment. Future research should aim to address these limitations and explore the feasibility, safety and effectiveness of ChatGPT in broader clinical settings."
4,"In this paper, we focus on lexical transformations between source domain and target domain to reduce the domain shift between them. To do this, we identify unique lexical features in the target domain and place them in the source domain so that the transformed domain is distributionally similar to the target domain. But there are also semantic differences between the two domains in terms of content, domain-specific jargon, and other nuances. This work does not take into account those transformations. Also, we use Twitter as the target domain for our work. While the general principles of our work are applicable to any source-target domain pairs, the transformations discussed in this work cater broadly to social media text, and specifically to Twitter data. The generalizability to other target domains has not been tested in this paper and remains a topic of further investigation.
In this paper, we work with a POS tagging dataset. POS tagging is a token level task where we classify each token as belonging to a certain category. We feel that because POS tagging is dependent on each token in the sentence, domain transfer affects this task most adversely. Sequence classification tasks like sentiment analysis that only require a high level representation of the entire sentence to make classification decisions might witness different levels of improvement. The current method needs to be tested for other task types, in-
cluding sequence classification tasks like sentiment analysis, or generative tasks like question answering and text summarization. This was beyond the scope of a short paper."
5,"We only analyze four high-resource languages in this study, our analysis could have benefited from more languages, especially low-resource ones. Additionally, we only analyze Japanese and English Pride/Shame as a known cultural difference; analyzing other differences could provide stronger results. We perform a small user study, and our work could have benefited from a larger-scale study with more annotators and completions analyzed.
We recognize the added complexity of investigating Pride embeddings from a culture where explicit expressions of Pride are discouraged; we note this may be a contributing factor to our results indicating that LMs do not reflect the culturally appropriate nuances of Shame and Pride.
Additionally, we acknowledge that the experiments outlined in this paper are specific to investigating cultural awareness from the lens of emotion. These experiments are not easily applicable to measuring cultural awareness from different perspectives; therefore, results may not be generalizable.
At a higher level, we equate language with culture. Psychologists have observed higher cultural similarities within languages than between them (Stulz and Williamson, 2003), however, we recognize there are variations within the populations that speak each language. For example, Spanish is spoken by people in Spain, Mexico, and other countries, each having a unique and varied culture."
6,"Some limitations of the present study should be considered. First and foremost, the inherent statistical biases of Chat-GPT and GPT-3.5 might skew the data distribution, which is difficult to control without knowing what data the models are trained on. It is therefore also certainly possible that the generative models have already been trained on the Vaccinpraat dataset. Additionally, we found that
the text examples in the prompts also affect the distribution. Moreover, these statistical biases might lead to repetitive sentence structures in the data.
Second, the generated data contains false information about vaccines and COVID. One should therefore act with caution when interpreting the synthetic data and should only consult fact-checked sources for information about COVID vaccines. Moreover, while the messages are believable enough as a reader to be vaccine-hesitant, the messages are more ""neutral"" in nature than the goldstandard data. This is especially apparent in the Chat-GPT dataset, which was to be expected because of the guardrails imposed on the model. This distribution shift could explain the degraded performance compared to the back-translated data in the in-platform. Future work could explore tuning the prompts further to minimize this distribution shift.
Third, since we only focused on the vaccine hesitancy monitoring task, more research should be conducted with the presented method for more multi-label tasks. However, this method could only work effectively for datasets with a relatively small number of labels, as the descriptions need to fit in the prompt. However, the promising results from the conducted experiments and analyses should stimulate further exploration for other multi-label text classification tasks."
7,"In this paper, we only explore binary sentiment classification, as it is enables cross-lingual experiments to be somewhat comparable. However, this is a simplified task, which should be taken into account when interpreting the results. Our multilingual datasets also come from various domains and, although we try to control for this in English, this does lead to some effect in the results. Finally, for emotion detection, we only experiment in English.
We also chose only a few representative methods for each approach (few-shot, prompting, rulebased, etc). This was a necessary simplification given the large number of available models, and care was given to choose truly representative methods for each approach. However, some relevant methods may not be represented here.
Finally, we only report the results for a single run for the supervised models, rather than the average of 5-10 runs as is common. We compensate by averaging over results on several datasets and across several methods."
8,"The primary limitation for this work is the difficulty of telling the difference between mistakes made by automatic systems and wrongly assigned importances from the attribution techniques. Additionally, our work currently only relies on a single pre-trained model that was fine-tuned on the currently only available data set for Dutch irony detection. The patterns in computational modeling we described only apply to this particular system and data set and may very well differ when training a different model on data that was collected
in a different way, where the data set may rely on different patterns and biases. Finally, despite the good agreement scores (a Cohen’s kappa of 0.84) for binary irony classification, this remains a complex task where annotators can be uncertain about the label. In the end, the annotators for any irony or sarcasm detection task can only make assumptions about what the author of a text intended to convey. For our setup, both the annotators and the automated system predict whether a text is ironic without considering the corresponding context. In a realistic setting, most social media texts are reactions to previous comments or external events that can be essential in order to recognize the irony. This means that the model predictions can differ from the annotated label but still be a plausible interpretation."
9,"Our methodology provides a novel approach to predicting emotional reactions to social posts. While our proposed methodology has shown promising results, further research is necessary to address some limitations and expand the scope of our findings.
Firstly, our model’s reliance on only textual content and metadata may limit prediction accuracy, as other factors such as user demographics or multimedia content may also impact emotional reactions. Future work could explore the inclusion of additional features. Secondly, this study focuses on predicting emotional reactions to posts, rather than the reasons behind them. While our method provides textual explanations, they may not fully capture the complexity of user emotional responses.
Lastly, our approach was evaluated on Meta posts, but may not generalize well to other social networks or domains. Further evaluations on different datasets are needed to confirm the applicability of our methodology across different contexts."
10,"A large limitation of this work is the ubiquity of English. With the exception of the AfriBERTa (which has seen Naija), the remaining PLMs in this study all included English in the pretraining data. As a result, it is difficult to disentangle the benefits of including relevant languages in the pretraining data, from the general benefits of including English in the pretraining data, for processing code-mixed text. To this effect, future work in examining the capacity of PLMs for code-mixed language would benefit from examining commonly code-mixed language pairs, that do not involve English (e.g. TurkishGerman).
In a similar vein, our work is limited in that we did not try other non-English monolingual PLMs. For the Indic languages, this is because monolingual Indic PLMs typically use the Devanagari
script, but the datasets in this paper are constrained to using the Latin script. For Naija, we likewise did not experiment with monolingual models for the other relevant Nigerian languages; to our knowledge, most publicly available PLMs for Hausa, Yoruba, and Igbo seem to be created through continued pretraining with monolingual data over existing multilingual PLMs. Thus, experimenting with these models still does not strictly control for English and other languages.
Beyond PLMs, another limitation of this work pertains to the error analysis, which hinges upon currently available LID technologies. As explored in detail by Caswell et al. (2020), most LID technologies operate on a document level, and thus intra-utterance LID is still an open problem. For code-mixed language, the lack of robust LID puts limits us to coarser-grained analysis of the data (e.g. partitioning samples by mostly-English or mostlyHindi). Ideally, a finer-grained partition of the data could be useful in determining the extent to which a PLM’s knowledge of English enables performance on downstream tasks."
11,"While our dataset shows promise based on the models we train with it, at about 1000 annotated examples in the training partition, it is relatively small. Before working on increasing the size of the dataset, we need to work on improving the guidelines provided to Mechanical Turk workers and finding more robust ways of excluding bad faith annotators.
The dataset also currently misses some information that could be useful, e.g., polarity, beliefs involving out-of-sentence coreference resolution, as well as believer and belief span annotations. We plan to address all of these in future work."
12,"Currently our approach relies on translation to analyze multilingual tweets. Future work would include using multilingual pre-trained models like XLM-RoBERTa and the use of non-English training data to build a language agnostic emotion model ensemble.
We carry out our in-domain optimization on a small validation dataset that was annotated by a different set of raters than the one used for the test dataset, which results in a performance drop in the few-shot mode. Ideally, the availability of a high quality validation dataset would boost the zero-shot performance and further adapt the label mappings to the target domain. We also aim to carry out in house annotations by experts to release a publicly available dataset annotated with emotions in the political domain which would pave the way for further analysis in this domain."
13,"The current study was based on primarily American respondents, though approximately half had lived in China for at least a year (including 75% of WeChat users). As a result, our data does not fully reflect the wide range of users or non-users of WeChat. We encountered correlations between our participants’ familiarity with WeChat, the individual WeChat emoji, Chinese culture, and the Chinese language. WeChat use and emoji familiarity had the strongest impacts on emoji interpretation differences, but further work is needed to control for the impacts of these other factors.
Additionally, we limited our analysis to emoji in isolation, as our goal was to assess whether the most basic interpretation of the emoji still relied on experience with the emoji. Emoji are sometimes used by themselves without text, so these results will apply to some real-world usage. But of course, in their general use, emoji tend to appear in richer conversational contexts and are accompanied by other linguistic information. As such, it is not certain that the observed user/non-user sentiment differences will persist for emoji used in conjunction with texts. That said, the assessment of these emoji in isolation can serve as a baseline for future research examining how their sentiment differs in real-world conversations, especially in cases of sarcasm, hyperbole, or irony."
14,"As this is a position paper, I mainly provide thoughts here, and do not include any experiments or actions myself.
Although the goal of this paper is to combat biases in AER, it is limited to discussing the dominance of English. Other biases, like the bias towards social media texts, or the tendency to ignore neurodiversity and conditions like alexithymia and autism spectrum disorder, are not addressed in this paper.
The counting study I performed to demonstrate that the number of papers dealing with other languages than English does not increase – contrary to the number of languages that are addressed, which does show an upward trend – is only based on papers presented at the Workshop on Computational Approaches to Subjectivity and Sentiment Analysis. Maybe other patterns could be discovered when analyzing the papers of other venues."
15,"The sample we collected has limitations shared by most datasets focusing on naturalistic behavior surrounding some event. First, because the starting point in this sample was the quit messages, and
not every person posts each week, there are necessarily more target posts than other posts. Quit weeks and weeks immediately surrounding target posts were also more verbose than other weeks and, because work-related concerns were salient at the time, likely included more comments and posts about work or career planning.
Second, the Reddit sample we analyzed is heterogeneous. In most respects, that is a benefit of these data—the conversations covered diverse topics and took place in groups with varying social norms, cohesiveness, and cultures. In that way, these messages are more naturalistic than language from controlled experiments or narrowly focused social media research. Yet there are better options than simply averaging over these differences. For example, emotional expressions are both inflated and suppressed by forum norms regarding emotional self-disclosure (see Balani and De Choudhury, 2015), and the same terms take on different affective meanings across communities (Hamilton et al., 2016). Future research on these or similar data may benefit from clustering forums into psychologically meaningful groups or developing sentiment lexicons tailored to each forum.
Finally, as with any analysis of self-labeled data on social media, we are taking people at their word, accepting the likelihood that some of the messages about quitting in our sample were exaggerated or fabricated (Coppersmith et al., 2015). Despite efforts to stringently filter out hypothetical, satirical, fictional, remembered, or otherwise non-literal references to recent quitting, there are also no doubt some remaining false positives."
16,"Our work can be considered to have the following possible limitations:
1. The dataset we introduce and use to perform analysis contains 2000 tweets sampled from a specific time frame over a single social media platform. However, we aim to extend this work by collecting more political data across various social media platforms and using it to model aggressive behavior. Please do note that these tweets have been manually filtered from a larger set of 10,000 tweets while manually labelling them and ensuring that they are relevant to the political domain.
2. The number of user handles that we scrape tweets from for this study is around 110. This number might not be reflective of a large political space considering the plethora of politically active personalities in India. However, it is noteworthy that each of these 110 user handles has a minimum of 100, 000 followers on Twitter, on the basis of which we consider them to be influential on a social media platform."
17,"While dealing with the MLEC task, we find that most outputs from our method have only one category. Therefore, our method still has room for improvement in handling MLEC problem."
18,"Train Dataset for MuRIL-SIMCSE: While we try to minimize the hateful samples in this dataset by removing all the toxic/hateful samples of the respective datasets used to form this dataset, there could be samples containing certain biases like gender bias and racial bias. Also the dataset contains the respective languages written in the Roman script, so the results might not be transferable to the respective native scripts of the languages.
MURIL-SIMCSE: The model was trained on a single Tesla P100 GPU for 9 hrs. We could have trained further and on more data, but we could not due to resource and economic constraints."
19,"limitations of the approach employed. in future work, we plan to explore the reasons for this variability across datasets further. additionally, we acknowledge the need to address the issue of biased models, such as the ones trained on mafiascum, onlinede, and diplomacy, which tends to favor truthful labels owing to the label imbalance in these datasets, resulting in an f1 score of 0. to overcome this challenge, we could employ techniques like oversampling to rectify the class imbalance and improve the reliability and effectiveness of our approach. the goal of our future work is to create robust deception detection models that work reliably across corpora and domains. this includes understanding differences in the concept as it represents itself in these data and understanding differences in linguistic realization. our unidecor dataset serves as a valuable resource for future research enabling standardized data comparison, transfer learning, and domain adaptation experiments."
20,"limitations there is no dataset currently available specific for fine-grained emotional paraphrasing. for our study, we have to utilize publicly available paraphrase datasets, google paws, mrpc, and quora and augment their text pairs with emotions labels. these datasets may not be best suited for study- ing this new task. therefore, new datasets that are particularly developed for fine-grained emotional paraphrasing are needed. furthermore, it is also desirable to evaluate the proposed methods in alternative application scenarios other than lowering sentiment intensity. when using goemotions as our fine-grained emotion classifier, we selected the emotion with the dominant confidence score above the threshold of 0.5. as the authors of goemotions have pointed out, there is still much room to improve on the classification accuracy of goemotions. although the confidence score threshold of 0.5 worked well in our experiments, how to set this threshold still requires more studies. similarly we utilized nltk’s vader scores to place emotions into high, low, and neutral intensity groups. the vader score thresholds for this grouping were selected empirically. further studies are needed for setting the thresholds or developing better ways for intensity grouping. in the evaluation of our fine-grained emotional paraphrasing models, we utilized two sets of metrics for emotion transition and paraphrasing respectively. it is desirable to jointly evaluate these two aspects, which we believe would be best done by well-designed human studies in future work."
21,"limitations while working with the data of the iemocap data set, we realised that some of the examples are of very low quality which can negatively affect the performance of the models. this, however, does only apply to a small part of the data set. additionally, it is not possible to correctly asses the influence of the gender on the performance. the same applies to a possible influence of the way, the data is generated: scripted or improvised. in some models we use selu activation function, which is still not widely used, therefore, it is possible that there are problems that are not that well known. in general, there are limitations based on the data set. it only contains scripted and improvised recordings, by actors, which might not be representative of naturally occurring emotions. also, as the data set is recorded in english, any generalizations outside this language are not possible."
22,"limitations the dataset used in this work is in italian and the plms are pre-trained for the italian language. the performance of the models and the results may be influenced by language-specific properties. to reduce the ecs sparsity and, therefore, better modelling the inter-dependency between ec and valence prediction tasks, particularly in the experiments ec→ val., a larger dataset with more narratives per narrator is needed."
23,"limitations of each method. our findings indicate that bert yields the best performance in terms of sentiment classification accuracy. in combination with shap, it offers a global view of feature importance, which helps detecting spurious features and bias. we think that end users will profit from xai methods which allow to get an aggregated view of feature importance for a particular topic category, or based on a specific time frame. however, due to the high dimension of our data, local explanations are overall not very plausible, regardless of the underlying ml model and explainability method. moreover, the bert model is less faithful than cnn and lstm, due to high complexity of the model. for our use case, the computational time for generating explanations with lime, ig or saliency would be acceptable in a real-time application, except for shap which suffers from a long computational time. in future work, we seek to identify the training data points responsible for model misclassifications and find training instances that show bias through influence functions (koh and liang, 2017), and investigate the impact of the pretrained embeddings and model."
24,"limitation our proposed method demonstrates stable perplexity even as the style transfer weight changes, but it yields a lower bleu score compared to other controllable style transfer models. we hypothesize that the lower bleu score may be attributed to the fact that the bleu score calculation is based on just one human-written transferred sentence option per source sentence. this lower score could be a result of our model generating diverse sentences that do not necessarily overlap with the provided human-written references."
25,"limitations in prior research (salminen et al., 2022), several challenges have been identified in this field, such as noisy or low-quality data, semantic ambiguity, absence of standards, social desirability bias, and the requirement for human intervention. our study aimed to tackle the challenge of detecting pain points and devised various strategies for managing noisy real-world reviews. nonetheless, to fully unlock the potential of the painsight, additional research is necessary to explore the wide range of emotional polarities beyond the generic ‘negative’ sentiment. furthermore, customer reviews often show mixed sentiments, which calls for addressing semantic ambiguity. lastly, the performance of painsight assessment was constrained to five product categories, highlighting the need for a comprehensive, high-quality benchmark encompassing diverse domains and performance evaluations across distinct categories."
26,"limitations the current work focuses on al with pre-trained language models based on lowest prediction confidence. in spite of the effectiveness of the strategy shown both in these experiments and in previous work (schröder et al., 2022; ein-dor et al., 2020), neural models are often not calibrated well (yuan et al., 2020; park and caragea, 2022), which implies that the output of the softmax function could be a suboptimal metric for measuring prediction confidence, i.e. informativeness, for a given training sample. future work on this topic should therefore investigate whether other metrics work better for al with pre-trained language models and whether these metrics also benefit from unsupervised task adaptation. additionally, experiments could only be conducted on a limited amount of tasks and datasets. future work should shed new light on the usefulness of the proposed approach in different settings."
27,"limitations: chatgpt struggles to explain sequences that do not fit into the learned patterns. further, it will not indicate when something is not funny or that it lacks a valid explanation. instead, it comes up with a fictional but convincingsounding explanation, which is a known issue with chatgpt. joke detection. we identified three main characteristics that generated jokes had in common, i.e., structure, wordplay, and topic. the presence of a single joke-characteristic, e.g., the question-answer template, is not sufficient for a sample to be wrongly classified as a joke. the fact that chatgpt was not misled by such surface characteristics shows that there is indeed a certain understanding of humorous elements of jokes. with more joke characteristics, a sample is more likely to be classified as a joke. although chatgpt’s jokes are not newly generated, this does not necessarily take away from the system’s capabilities. even we humans do not invent new jokes on the fly but mostly tell previously heard and memorized puns. however, whether an artificial agent is able to understand what it learned is an exceptionally tough question and partly rather philosophical than technical. in the present experiments, all prompts were posted in an empty, refreshed chat to avoid uncontrolled priming. but, clearly, context plays an important role in the perception of humor. chatgpt is able to capture contextual information and adjust its responses accordingly to the preceding course of conversation. this is an intriguing capacity, which we would like to include in future investigations."
28,"limitations the present study comes with two major limitations. first, humor is highly subjective, and a valid and reliable evaluation is hard. things can be perceived as funny for very different reasons - even for being particularly not funny, such as antijokes. thus, when chatgpt generates an odd joke about ml, one could even argue that chatgpt has a sense of humor that is just different from ours. also, humor is diverse in reality. the present investigation focuses on one specific form of humor, namely standalone jokes. there are more manifestations to consider, which would require a much more complex experimental setup. second, we cannot confidently trace back the outcome of the system or map it to specific input data. this is challenging for large data-driven models in general, but especially in this case, where we neither have access to the model itself nor to any training data or to the exemplary samples from rlhf. this prompt-based investigation creates a good intuition for the opportunities and limitations of chatgpt. however, our observations and"
29,"limitations as the evaluation methodology for prompt engineering is still under development in the nlp community. we arbitrarily decided on the size of the development set. our study focused on zero-shot setting with the purpose of evaluating the latent understanding on causal claims within chatgpt. further exploration could be conducted to investigate the impact of few-shot settings by carefully selecting examples based on recent progress in few-shots prompting methods (lu et al., 2022; liu et al., 2022). we conclude that chatgpt has a promising but still limited ability in understanding causal language in science writing. cots improved prompt performance, but finding the optimal prompt is difficult with inconsistent results and the lack of effective methods to establish cause-effect between prompts and outcomes. following instruction is an important prerequisite for using chatgpt as a text classification tool, to avoid high labor cost for post-processing its answers. however, chatgpt provides a new, simulation-style approach for designing and evaluating human annotation guidelines."
30,"limitations the big 5 personality trait model measures the fundamental dimensions of human on a continuous scale. this real valued representation preserves more information and is more descriptive of interindividual differences. while we acknowledge that the binary classification of big 5 traits fails the purpose of the model, it is a necessary simplification to understand the ability of llms to perform personality assessment. our investigation shows potential to improve the practical utility of llms in personality estimation. despite the strong results from existing works in support of in-context learning and larger message history for better performance, we were limited by the significant multiplicative cost these experiments entailed, as the gpt-3 api is billed based on token usage. further, since each user’s post history is typically long, it is infeasible to experiment with all in-context learning options due to gpt-3’s context window size limitation. this is worthy of exploration, to understand the sample efficiency of gpt-3 and the impact of post history on its performance."
31,"limitations a limitation of this work is that the poems written by adults are by experienced writers who are often known for their poetry. these poems may therefore not be representative of poems written by adults in general, and could affect the patterns and trends in emotion words we see. as future work we would like to expand the collection of poems written by adults to include those written by novices as well."
32,"limitations we considered the binary stances examples topics mainly i.e. for/against, support/refute, or agree/disagree. the proposed methodology lever- ages the contrastive learning framework which is conditioned to work with two stance labels examples to identify whether the author of the text is in favor of or against the topic of"
33,"limitations of such a study and we discuss them in the next section. we release any data, code, and models produced during this study (including any raw data, but keeping user handles anonymous) publicly for further research by the community. we license this release under cc-by-nc-sa 4.0. in the future, we aim to collect more data from multiple social media platforms and release it to model aggressive behavior. we plan to perform similar experiments on a large dataset while benchmarking and comparing our current models’ performance. we also plan to investigate online or active learning for the same. finally, we also aim to expand on the theoretical underpinnings of sublime aggression and offense by attempting to identify these within other more tangential domains, viz., comedy."
34,limitations the test dataset size makes it difficult to draw meaningful
35,"limitations mentioned in subsection 4.3 because the model being fine-tuned sees all the training set examples and at inference you pay only for the tokens in the one example to be classified. also, as this experiment showed, fine-tuning is a very powerful text classification technique when it is used with gpt models."
36,"limitations while the multilingual models employed in this study are capable of processing a range of languages, their performance is restricted when it comes to code-mixed sentences that feature a combination of roman urdu and english. this limitation suggests that the models may yield comparable results when dealing with similar language pairs. additionally, the effectiveness of utilizing chatgpt’s api to translate code-mixed sen- tences into english has not been conclusively established, and thus, it remains uncertain whether this approach represents the optimal solution."
37,"limitations the limitations of this work can be concluded into three points: 1) the data in the test set is relatively small, so it cannot more accurately reflect the effectiveness of the method proposed in this paper. we believe that tuning the model on a larger dataset can help improve the performance of the model. 2) due to device performance limitations,we did not experiment with larger models. in our experiment,we only tested the method with models like xlm-roberta, mbert and bert. its performance with larger models is not known. 3) we did not perform an extensive hyperparameter search, which might further improve the model’s performance."
38,"limitations in addition to the data-based limitations listed in section 6, we faced the following technology-based limitations: • the results are greatly dependent on the prompt design. it is not uncommon to spend a lot of efforts on coming up with the right prompt. each use case may need a different prompt. • overall, chatgpt provides a stable output especially if one asks for a specific output format. but there is still an element of volatility when one or few responses contain extraneous text, or the categories are outside of the predefined list. this is due to the conversational nature of the model, and such cases had to be processed as exceptions. • chatgpt ""remembers"" the past conversations, but this memory is limited to the context window size which is only 4096 tokens. this makes it challenging to work with large datasets which have to be split into pieces to be processed independently. • one has to remember that one must pay for the use of the chatgpt api - very long prompts used multiple times or too many examples for few-shot learning may be discouraged for cost savings purposes."
