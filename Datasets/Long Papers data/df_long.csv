,Text
0,"Our analysis and conclusions have been based only on a single translation direction (German to English), a single dataset, and a single transformerbased model. The generalization to other languages, data and models is yet to be verified.
Even in this setup, we have seen that some of the proposed methods are very good at detecting fully detached hallucinations. However, none of them were able to well separate strongly detached hallucinations (when only a part of the generated translation is unrelated to the source) from correct translations. Perhaps, such partial hallucinations should be detected on the level of individual tokens instead of the whole sentence.
One of the metrics that we propose, average ALTI source contribution, has an advantage of not requiring any external models except the translation model itself. However, the two best detection metrics (based on LaBSE and on XNLI model) re-
quire additional encoders trained on the source and target languages, which limits their applicability for lower-resourced languages or in the settings with limited computational resources.
Being an internal method is an advantage of ALTI, but it is also a limitation: this method is suitable only for transformer-based translation models. In principle, it can be adapted to other neural architectures, but not to non-neural approaches, such as statistical machine translation."
1,Section 7 (after conclusions)
2,"Despite the promising results obtained in our model, there are still several areas for improvement. Firstly, when dealing with a large corpus, the online retrieval function becomes challenging as
it requires a significant amount of computational resources and time. Additionally, creating a vectorized corpus dynamically every time becomes difficult. Secondly, the process of collecting a large number of reviews from users raises privacy concerns. The collection of data, especially from private and non-public sources, may pose difficulties."
3,"We conduct experiments on public datasets of finite sentence length, while generalizability to extremely long sequences or even streaming data has not been verified. Furthermore, the generalizability of the proposed quantization method to other tasks, including computer vision or speech recognition, remains to be tested. In addition, binarization and ternarization require bit-packing to have actual memory savings and dedicated hardware support for real-time acceleration, which is more of a hardware implementation aspect and not studied in this paper."
4,"We discussed the limitations of our work in the unnumbered limitations section.
7 A2. Did you discuss any potential risks of your work? We only used publically available datasets that are commonly used in research on dialogue systems. We believe there are no significant risks associated with our work."
5,"We discuss limitations of our work that hopefully could inspire future research in this avenue. Task Coverage in ARCADE ARCADE consists of realistic data wrangling and EDA tasks for a variety of ML datasets. In particular, we focus on problems that can be solved using pandas because of its popularity in data science — 90% of Kaggle notebooks use pandas. Still, our annotated problems may not cover all the types of tasks in these two categories. As an example, data visualization is an important part of EDA. Our dataset also includes 59 natural language to plotting problems, which are not used in this paper due to challenges in automated evaluation (Chen et al., 2021b). Future work might consider evaluation of plotting tasks using unit tests (Lai et al., 2022). Additionally, some of the existing datasets in Tab. 1 usually contain broader types of problems other than the wrangling and EDA tasks considered in this paper (e.g., fitting ML models, §7). We leave expanding the task spectrum as important future work. Session-level Evaluation ARCADE features multiple contextually dependent problems in computational notebooks. As the first step towards evaluating code LMs in this interactive program synthesis paradigm, we report turn-level accuracy, and generate notebook context for prompting using ground-truth solutions for the prior turns of a problem (§5.1), following the common evaluation protocol in task-oriented dialogue (Hosseini-Asl et al., 2020; Andreas et al., 2020). Future work could consider a more realistic scenario of session-level evaluation where history contexts consist of model-predicted code instead of the reference (Yang et al., 2020; Nijkamp et al., 2022). However, this evaluation setting is still not ideal without modeling the user (e.g., asking follow-up questions to correct a model’s predictions in a turn before proceeding to the next round, see Austin et al., 2021), which often requires building specialized simulators (Cheng et al., 2022a). Reliance on Large Language Models Our experiments are based on public and in-house large code LMs (PACHINCO), which require adequate computational resources9 and create carbon emissions (Patterson et al., 2021). Their predictions could also be subject to known issues such as misalignment with user intents; for a discussion
9FLOPs usage of fine-tuning PACHINCO is 3.6× 1022.
of these and other risks of code language models, see Chen et al. (2021a, Appendices E-H) and Chowdhery et al. (2022, Section 6.4). To reduce the amount of computational resources required, our initial prompting experiments (§5.2) and error analysis (Appendix J) suggest that leveraging program execution information (e.g., schema descriptions) could be a promising direction to improve sample efficiency and reduce the size of code LMs (Nye et al., 2021), while explicit modeling of code-intent correspondence (Zhang et al., 2022) could be a viable path to mitigate alignment issues in model predictions. In addition, as generative AI coding tools are becoming more available to developers, more efforts are required to understand the potential limitations of those systems and the risks they may pose, such as producing insecure code and over-reliance on model predictions (Chen et al., 2021a). We leave addressing those issues as important future work."
6,"Although our NAR approach can generate fluent and meaningful text, it inevitably suffers from the typical generation problems like in the AR fashion: (1) off-prompt: the provided prompt is very short, which causes the model can not focus on meaningful content and generate reasonable text. Besides, the model usually simply copy prompt text to generate results instead of planning reasonable content, such as the case 3 as shown in Table 13 in Appendix D. (2) incoherent between sentences: When the model is initialized, it does not consider the logical order between sentences, so it can only rely on the training data to learn automatically. We will consider how to generate a suitable initialization to help the model generate coherence results. Our paper’s primary concern focuses on accelerating the generation speed, and we will put how to solve these problems in future work.
Ethics Statement
Our method heavily relies on the pre-trained language models, e.g., RoBERTa, which may inherit the problematic biases (Radford et al.). We have
attempted to mitigate these issues by conducting experiments on comparatively innocuous story generation and opinion generation tasks. Furthermore, we have replaced all the names in those corpora with special placeholders. Although some measures are taken to mitigate the problematic biases, such issues cannot be solved completely. Thus, we urge the users to carefully examine the generation results and cautiously apply our method in real-world applications. Additionally, it is worth noting that all the corpora used in our experiments are only for scientific research.
As for the human evaluation process, we resort to open source web library Django|| to build our own human evaluation interface. Before releasing the human evaluation cases, we carefully check that there is no private information or other problematic biases in the cases. Besides, we did not collect personal information or ask the annotators about their private information during the annotation process. We hired three annotators and paid each of them $0.29 for each case comparison. The payment is reasonable since there are only 100 cases for annotation, and it would cost average 4 hours for one to finish all the comparisons."
7,"Firstly, the training process of our proposed model is dependent on supervised data, thus precluding its application to languages without a supervised dataset for CCG.
Also, the span-based parsing algorithm proposed in this study is implemented in Python and may take a considerable amount of time to parse extremely long sentences (more than 100 words) due to a lack of optimization for implementation."
8,"Section Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
9,Section 9
10,"The high computational complexity is one of the biggest disadvantages of the path aggregation. The time consumption and GPU memory used for multiple operations are expensive. So it is very desirable to use only one time of path aggregation due to attributes of the ABSA task in our APARN.
Another limitation of this work is that the performance of the model is still somewhat affected by the quality of the AMR parsing results. The good news is that the research on AMR parsing is continuing to make progress. In the future, APARN with higher quality AMRs is expected to further improve the level of the ABSA task.
Besides, this model is flawed in dealing with implicit and ambiguous sentiments in sentences. Implicit sentiment lacks corresponding opinion words, and ambiguous sentiment is subtle and not apparent. An example of this is the sentence ""There was only one [waiter] for the whole restaurant upstairs,"" which has an ambiguous sentiment associated with the aspect word ""waiter"". The golden label is ""Neutral"", but our model predicts it as ""Negative"".
Finally, generalization to other ABSA tasks such
as end-to-end ABSA or ASTE is another restriction. Considering the complexity of the task, we only apply our motivation to sentiment classification in this paper. We will further generalize it to more complex sentiment analysis tasks in the future work."
11,"Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
12,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
13,"In this section, we discuss some of the limitations of the Rule by Example method."
14,"Section ""Limitations"" (after conclusion)
7 A2. Did you discuss any potential risks of your work? We analyze the current state of identity inclusion in MT. Thus, our work points to risks of such systems."
15,7: Limitations
16,"In this paper, we explore incorporating multiple constraints to simile generation and attempt to interpret the simile comparisons from the aspect of Cognitive Linguistics. However, the creativity of simile is one kind of subjective feeling and is difficult to be accurately judged, which is also a big challenge for other kinds of creative writing tasks. We hope this task and dataset could provide novel insight into user-oriented text generation, and give the interactive and collaborative generation a closer and more detailed exploration."
17,"There are two limitations of this study that could be addressed in future research. First, this study focuses solely on the ED task. In the future, we seek to extend it to the overall event extraction (EE) task, which also includes the event argument extraction task, where a complete annotation is more challenging than in ED. Second, our study models the partially labeled training data instead of annotators. Indeed, the annotators produce the data, so building a model for annotators may be an essential way to address the partial learning problem. For example, an annotator may be more careless than others and generate more noisy data. Consequently, a robust model for the task should give a lower belief in the data of this annotator to improve learning. Lastly, our research raises no ethical issues because it focuses solely on the technical aspects of a normal information extraction problem."
18,Section 8
19,"In this work, we limit ourselves to object-centric grounding, which ignored that language can ground events, attributes, manners, mental states, etc. The grounded meaning of some groundable words, especially ADVs, NUMs, VERBs, and PRONs, cannot be fully captured by the bounding boxes alone. Future work should explore better task formulations to study the acquisition of their grounded meanings. An exciting future work along this line is to extend the setting from images to videos and physical interactions with the environment, and to incorporate the rich temporal dynamics of the world for language acquisition. In addition, we ignored the social aspects of language learning, where children infer the referents of words from their caregivers through communication (Carpenter et al., 1998; Bloom, 2000). Future work could also investigate grounded word acquisition from natural dialogue."
20,"Section ""Limitations""."
21,"Our implementation of proxy models applies those models after the whole data is generated. Due to this, in the resulting dataset, the number of instances can often be unbalanced between labels. Such a limitation might be addressable by training proxy models from intermediate datasets with a smaller number of instances, and using those models while generating the rest of the dataset. As the data become unbalanced during the generation,
the generation pipeline can try to generate more instances with labels that are a minority in the intermediate dataset. However, when we piloted this approach, we identified potential problems. First, intermediately trained proxy models could perform worse than those trained after all data are generated, due to the lower diversity in intermediate data used to train proxy models. Second, if many data points generated with a specific label (label a) actually belong to another label (label b), there can be cases where most instances of label b come from the prompt with label a. It can skew the linguistic patterns of instances within the dataset, as only a small number of texts for label b might have been from the prompt with label b. Advanced approaches to address these issues can be future work directions.
Our implementation of efficient OOSF was not effective in increasing model accuracy. It might be due to the negative impact of removing instances, such as filtering instances on the decision boundary. As our study of OOSF was not complete, future work is necessary. Applying OOSF to the entire generated dataset and seeing the impact of their removal would be the first step. With a comprehensible understanding of OOSF, we would be able to design better OOSF strategies, such as filtering instances with various criteria.
In this work, we only examined the text-davinci-002 model of GPT-3. Although we believe that the overall trends of results would be similar for other models, examining other models with our approaches is a necessary future work. We also examined only one prompt (Prompt A), while there may be other options. In Appendix F, we present partial results on using another prompt, showing that our approach is generalizable to other prompts. Combining human interventions with automatic annotation error detection (Klie et al., 2023) can be another future direction."
22,Section 8. Limitations
23,"Like all unstructured pruning methods, SMP is hard to achieve inference speedup compared to structured pruning methods. Since SMP prunes model without fine-tuning, this also limits the extension of SMP to structured pruning methods. However, we find that most rows of the sparsity matrices in SMP are completely pruned at high sparsity level. This allows us to directly compress the size of matrices, resulting in faster inference. For example, the 3% remaining weights model of MNLI can be compressed to 47.43% of the model actual size (resulting in around 1.37× inference speedup) without retraining or performance loss. By removing rows of matrices that contain less than 10 remaining weights, we can further compress it to 25.19% actual size (1.76× inference speedup) with 0.9 accuracy drop. We expect that a carefully designed loss function during training could result in even smaller actual model size and faster inference speedup, which we leave it in the future."
24,"Last section in page 9 (unnumber)
7 A2. Did you discuss any potential risks of your work? Work doesn’t have immediate ethical risk"
25,"While Sec. 3 characterizes the effect of general Lhomomorphisms, LEXSYM specifically produces single-token swaps. In images represented as discrete symbol sequences, if a single symbol simultaneously encodes multiple visual features (e.g. color and texture), these features will remain entangled in synthesized examples. It will not exchange substructures larger than a single token, and thus will not synthesize examples longer than those already present in the training set (Lake et al., 2019). This is because LEXSYM targets compositionality but not recursion, which is also required to model the full range of human-like generalizations in sequence learning problems.
LEXSYM is also sensitive to the nature of the tokenization scheme itself. In morphologically rich languages, for example, LEXSYM may need to be applied not on top of words or segments, but instead canonicalized morphemes produced by learned morphological analyzers (Narasimhan et al., 2015; Bergmanis and Goldwater, 2017; Cotterell and Schütze, 2018) (analogous to the use of learned image patch representations rather than pixels in our VQA experiments).
Finally, LEXSYM does not induce some of the generalizations obtained other methods for improv-
ing compositional generalization, especially those that exploit extra structure (e.g. tree-shaped inputs and outputs) in the semantic parsing domain (e.g. Liu et al., 2021a). It might serve as a platform for future versions of those methods that offer greater generality and formal guarantees."
26,8 (Limitations)
27,The limitation section is after the conclusion part of the thesis.
28,"The New Yorker Cartoon Caption Contest represents a narrow slice of humor, deriving from a particular language, region, history, culture, style, and set of conventions. Hence, the results of this study do not represent or cover all types of humor.
Our framing of the quality ranking task could be interpreted as seemingly prescriptive (i.e., that joke A is “objectively” better than joke B), but New Yorker editorial selections should not be taken as ground truth for funniness; disagreement about what is funny is expected and valid. Our tasks operationalize the prediction of only average preferences (rather than individual ones), and these preferences may include a partiality or bias towards items that conform to the characteristics of prior contest winners or published New Yorker cartoons.
Finally, the explanations in our annotated corpus were largely written by a single author of this paper. While a larger pool of the crowdworkers judged these explanations to be of higher quality in comparison to machine generations, future work would be well-suited to compare the person-toperson variance in explaining why particular jokes are funny.
16Or never. Is never good for you?"
29,"6, Limitations"
30,"Lexical variation is not our focus because it is not well-described by systematic, scalable, and generalizable rules. One can derive lexical distributions from data, but many low-resource dialects lack corpora on which to base these insights. This is an important problem for future research.
Multi-VALUE’s strength is its extensive coverage of English morphosyntacic patterns that have been documented in eWAVE by over 80 linguists. Such comprehensive resources are not available for other languages, but we encourage continued collaborations between computer scientists and linguists to build these resources for dialect-robust NLP systems across languages. As it stands, the current iteration of Multi-VALUE provides global value by serving a global contact language, English, and its 50 most documented varieties.
Despite the scope and precision of eWAVE for
English, its catalog ultimately derives from linguists’ oral interviews with native speakers, and here we can identify some additional limitations. First, the orthographic conventions that linguists use to encode spoken dialect may not always align with the speakers’ own writing conventions and usage. Second, our approach can only cover the variation that linguists observe frequently enough to document, and in canonical forms in which they are documented. This means we may not fully capture variation within each feature.
Finally, dialects should not be treated like deterministic speech patterns, but rather like a range of grammatical options or switches that may be turned on and off and adjusted for frequency in various social and personal contexts. Dialects do not always fit into nicely prescribed categories."
31,Section 8
32,"Perhaps the most important limitation regarding ColD Fusion is its deployment. This paper presents a method for multitasking, not a platform. In that sense it solves both multitask learning goals under the constraints resulting from collaboration. However, using ColD Fusion in practice might require much more effort – It would require a place to host the models, a way to make sure no malicious or erroneous model was sent, and other aspects of a platform to support this training.
This is the first method to tackle collaborative multitasking and we scaled it to 35 datasets. However, future methods may be found more efficient or scale better with the amount of data and computation.
ColD Fusion with many iterations and models might require more computational effort for a given amount of data (§6) than regular multitask learning. As a result, while our bottom line performance is encouraging, ColD Fusion might not be the preferred way under every possible scenario. Still, some of the costs may be alleviated by future work – for example the additional iterations when fusing many models, might be reduced by aligning models’ weights before fusing (Ainsworth et al., 2022).
While this paper studied the impact of various ColD Fusion parameters, it is unclear how finetuning or even pretraining parameters affect results. However, we do have a reason to believe the method is relatively robust to these refactors through our initial results and the replication on another architecture (App. §D).
Another limitation is the assumption that the weights of the model change. Some adaptation methods assume the model is frozen and only its inputs change. In those cases, the model would
not be improved by use. Still, even in such cases, multitask learning (Wang et al., 2023) might be applied on the inputs, or the same model might be used in different ways, where some also adapt parts of it (Hu et al.; Jang et al., 2023; Qin et al., 2022; Yadav et al., 2023). In those cases, the method might still prove useful, even if it benefits only from some of the contributions.
As mentioned before, another concern is a possible harmful update done by a contributor. Handling it would require monitoring the updates by regularly evaluating the model, or measuring the updates diff to identify noisy models (too large diff / random weights)."
33,"In this work, we present a general masking scheme for multilingual MLM pre-training on multiple monolingual corpora. Experiments show that our method can work for similar languages (including low-resource and high-resource ones) and dissimilar languages. However, we only experiment with dissimilar language Ne. More experiments are required for dissimilar and distant languages.
When computing [C]x for more than 3 languages, to avoid cross-lingual bias, we adapt our method to a pivoting-based framework, using En as a pivot or anchor point. Although we show this framework can work for cross-lingual classification tasks, this could be a potential problem for further adaptation to other multilingual tasks, which requires further
experiments. Intuitively, we can compute [C]x in random languages instead of only in En with a balanced sample strategy.
Our method provides a general framework to leverage cross-lingual prototypes for multilingual MLM pre-training, but the scope of the study is limited. We believe there are some other solutions. For instance, we can leverage linguistic varieties for masking, but the question is how to obtain linguistic varieties without using parallel corpora. Perhaps, we can consider word frequencies because Zipf’s law indicates that words appear with different frequencies, and one may suggest similar meaning words appear with relatively similar frequencies in a pair of languages. Most importantly, solutions should further consider morphological variations, since in this paper we prove morphological variations are significantly beneficial."
34,Section 7: Limitation after the Section 6: Conclusion and future work
35,"We discuss four limitations of our work: the inclusion of unsafe content, potential biases in data
sources, a limited measure of image quality and generalizability to different generative models.
• Inclusion of unsafe images and prompts. We collect images and their prompts from the Stable Diffusion Discord server (§ 2). The Discord server has rules against users generating or sharing harmful or NSFW (not suitable for work, such as sexual and violent content) images. The Stable Diffusion model used in the server also has an NSFW filter that blurs the generated images if it detects NSFW content. However, we observe that DIFFUSIONDB includes some NSFW images that were not detected by the NSFW filter or removed by the server moderators. To mitigate the potential harm, we compute and share the likelihood of an image or a prompt containing unsafe content using the state-of-theart NSFW detectors (§ 2.3). In addition, we provide a Google Form on the DIFFUSIONDB website where users can report harmful or inappropriate images and prompts. We will closely monitor this form and remove reported images and prompts from DIFFUSIONDB.
• Potential biases of the data source. The 14 million images in DIFFUSIONDB have diverse styles and categories. However, Discord can be a biased data source. Our images come from channels where early users could use a bot to use Stable Diffusion before release. As these users had started using Stable Diffusion before the model was public, we hypothesize that they are AI art enthusiasts and are likely to have experience with other text-to-image generative models. Therefore, the prompting style in DIFFUSIONDB might not represent novice users. Similarly, the prompts in DIFFUSIONDB might not generalize to domains that require specific knowledge, such as medical images (Chambon et al., 2022).
• Limited measure of image quality. We use joint text-image CLIP embeddings between prompts and images to detect generation misalignments (§ 3.5). While the CLIP embedding distance can indicate the degree of alignment between the prompts and generated images, it does not provide a measure of the overall image quality. When constructing our dataset, we have considered including image properties such as entropy, variance, and the most common colors to help users gauge image qualities. However, these metrics do not provide a good measure of the overall image quality as well. To better mea-
sure image quality, future researchers can recruit annotators to rate images in DIFFUSIONDB.
• Generalizability. Previous research has shown a prompt that works well on one generative model might not give the optimal result when used in other models (Borji, 2022). Therefore, different models can need users to write different prompts. For example, many Stable Diffusion prompts use commas to separate keywords, while this pattern is less seen in prompts for DALL-E 2 (Ramesh et al., 2022) or Midjourney (Holz, 2022). Thus, we caution researchers that some research findings from DIFFUSIONDB might not be generalizable to other text-to-image generative models."
36,"The last section (unnumbered), immediately following the conclusion
7 A2. Did you discuss any potential risks of your work? We carefully reviewed the guidelines and could not think of potential risks worth mentioning in the paper."
37,"Models are often developed with specific datasets in mind. Some papers introducing new models also introduce new training sets such as CopyAttention (Cui et al., 2018), SpanOIE (Zhan and Zhao, 2020), and IMoJIE (Kolluru et al., 2020b) which may influence model assumptions. SpanOIE also introduces its own manually annotated benchmark, which may have informed the assumptions SpanOIE makes. The lack of consensus on how to label OpenIE makes it difficult to perform apples-to-apples comparisons because certain models can not extract some relations due to the assumptions they make.
OpenIE has also largely been limited to English. MILIE makes assumptions that allow for different extraction methods depending on the language, but other OpenIE models that support multilingual extraction largely treat extraction from other languages the same as extraction from English. Multilingual OpenIE remains an open field of study."
38,"We have Limitations Section at the end of the paper after Conclusion
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
39,"Although our model achieves good performance in solving the compositional and zero-shot generalization problems, there is still room for improvement on the i.i.d datasets. The fine-grained module in our framework cannot take advantage of explicit composition information when the component compositions in the testing set and training set significantly overlapp. For example, in Freebase, ""Who is the coach of FC Barcelona?"" is answered by the join of relation “sports.sports_team.coaches” and “sports.sports_team_coach_tenure.coach”. Our fine-grained extractor may fail to recall “sports.sports_team_coach_tenure.coach” and instead select “base.american_football.football_coac -h.coach” as the candidate since ‘football coach” is more relevant to the question than “coach tenure” in semantics. The only coarse-grained model, however, can directly memorize the pattern because such composition appears frequently in the training data. Therefore, compared to conventional models that completely memorize composition patterns, our model may only have minor advantages.
Another limitation is that we cannot guarantee the generalization on other KBs such as WikiData because gaps between KBs may bring negative impact. For example, relations in Freebase are often more specific
(ice_hockey.hockey_player.hockey_position, soccer.football_player.position_s), while relations in Wikidata are more general (position_played_on_team). We consider it as a direction for our future work."
40,Section 7
41,
42,
43,"ALERT aims to encompass a wide range of reasoning skills, but some reasoning skills are missing, specifically in regards to symbolic reasoning (last letter concatenation task and coin flip (Wei et al., 2022)) and compositionality reasoning (SCAN (Lake and Baroni, 2018), COGS (Kim and Linzen, 2020) and CFQ (Keysers et al., 2019)). These reasoning skills should be included in future work.
In terms of computing power, we have experimented with models that were accessible to us. We acknowledge that there are larger models that we were not able to train due to the limitations of our computational budget.
During our analysis, we discovered that some datasets contain noise, where even human experts are unable to provide accurate answers for certain instances. While it is important to address this issue, it is a time-consuming process to carefully review and clean each instance in the dataset. We plan to address this in future work."
44,section ’Limitation’
45,"Limitations Section.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
46,"Considerations and Limitations section
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
47,"Our model may have several limitations: (1) As a memory-based model, our model consumes additional space to store typical samples and static prototypes, which causes the performance to be influenced by the storage capacity. (2) Although we propose memory-insensitive relation prototypes and memory augmentation, our model still relies on the selection of typical samples. The selected samples of low quality may harm the performance of our model. (3) The recent progress in large language models may alleviate catastrophic forgetting and overfitting, which has not been explored in this paper yet."
48,"We discussed the limitations of work in section 7 of the paper.
7 A2. Did you discuss any potential risks of your work? Our work does not have any immediate risks as it is related to improving pretraining techniques for code-switched NLU."
49,"Our theory currently assumes that input speech features are quantized into discrete units, as in (Chen et al., 2019), while preserving all the linguistic information in the speech. As a result, our theory does not account for the loss of linguistic information during the quantization process, as often occurred in realistic speech datasets. Further, more recent works (Baevski et al., 2021; Liu et al., 2022) have shown that continuous features, with the help of additional regularization losses, can achieve almost perfect ASR-U. Such phenomena is beyond explanations based on our current theory and require generalizing our theory to continuous speech features. Further, our model assumes that sufficiently reliable phoneme boundaries are fed to the ASR-U system, and kept fixed during training. It will be interesting to extend our framework to systems with trainable phoneme boundaries, such as the wav2vec-U systems, to better understand its effect on training stability."
50,"Our proposed ThinkSum has demonstrated strong performance on thirteen challenging BIG-bench tasks. However, it is important to acknowledge certain limitations of the system.
Firstly, as the number of objects or facts that are reasoned over increases, the computation cost will also rise. However, increasing the number of objects will also make the task harder, and direct prompting may cease to work at all (as we indeed observe in BIG-bench results, such as LOGICAL DEDUCTION with more than five objects), while ThinkSum offers a generalizable methodology, as the atomic Think operations do not increase in complexity as the number of objects grows.
Secondly, when solving a new task, it is necessary to expend human effort to select specific operations in each step, as outlined in §2. This limitation is shared with prompt engineering of all kinds, including direct or chain-of-thought prompting: finding a prompt for a new task requires an often-cumbersome prompt engineering procedure. We have described ThinkSum as a general twostage paradigm, with an external inference step. This generality aims to facilitate the adaptation of ThinkSum to new tasks, with minimal modifications to the Think and Sum steps. Work on automating the prompt engineering procedure (Zhou et al., 2022b) is a promising path towards overcoming this limitation. An alternative to prompt engineering that does not require such human effort is tuning (i.e., differentiable end-to-end learning) of prompts or model parameters; however, this remains impractical for GPT-3-scale models, and attempts to tune models directly on symbolic reasoning chains have met with limited success (Kassner et al., 2020).
Last but not least, ThinkSum has mainly been evaluated with GPT-3 (davinci) and InstructGPT (text-davinci-002) models. To further improve performance, it may be beneficial to apply ThinkSum to more recent instruction-tuned models such as Flan-PaLM (Chowdhery et al., 2022; Chung et al., 2022), text-davinci-003, ChatGPT, and GPT-4, which seem more capable of robustly performing Think steps."
51,"Robustness to perturbations Our empirical study does not explore the connection between the discriminative power of automatic metrics based on the proposed metric preference checklist and their robustness to simple perturbations or other natural language phenomena that may occur in texts or NLG use cases.
Metric Fairness (Social Bias) Our study does not include an investigation of metric fairness or social bias issues that may be introduced by Language Model-based NLG evaluation Metrics.
Single-aspect vs. Multi-aspect Our current empirical experiments mainly explore the discriminative power of evaluation metrics in single-aspect experiment setup (section §5.2). It may also be interesting to inspect to what extend the metrics can identify multi-aspect levels of quality, particularly when there exists disagreement between human evaluation aspects. For example, instead of disjointly splitting samples into {low Engagingness, moderate Engagingness, high Coherence}, samples can be divided based on the joint aspects, such as {low Engagingness and low Coherence}.
Universal input-output structure Our experiments are mainly carried on publicly available author-annotated human evaluation benchmark datasets. Thus, we do not guarantee the universal input-output structure and a uniform naming system across datasets or tasks. For example, UniEval - Topical Chat data (UniEval-TC) (Zhong et al., 2022) and USR - Topical Chat (USR-TC) (Mehri and Eskenazi, 2020) use a different naming system for human evaluation aspects, yet the aspects refer to the same dimension of human-like qualities.
Dependency of NLG Systems When comparing outputs from two different NLG systems, the systems are presumably independent. However, in many NLG use cases, this assumption is not fully accurate. For example, in Controlled Generation task, the systems originate from one pretrained Language Model as an encoder model. In inference or decoding stage, the encoder’s probability outputs are used as inputs for multiple decoding schemes, such as the use of Log-Likelihood ranking, distance scoring as filter, etc (Dathathri et al., 2020), yielding n-systems to compare with. As a result of this setup, the generation outputs from these n-systems are often less diverse and less distinguishable than
the outputs from two independent systems that do not share the same encoding scheme or training objective."
52,"Following instructions, we add Limitations after Conclusion."
53,"Due to the massive combination of relations and times on TKGs, balancing the model performance and efficiency is challenging. Our model TECHS performs well as Section 5.2 and 5.4 discussed. However, there is also a limitation. TECHS is a
two-step approach that can be further improved if we can fuse logical reasoning in the graph encoder like ConGLR (Lin et al., 2022). The model will be more efficient for computational space and time."
54,"Through extensive empirical analyses, we demonstrated that our proposed method can produce highutility synthetic text with strong privacy protection. However, we acknowledge there are limitations.
Our method captures general statistical properties of the original text but is not able to perfectly replicate all details. DP protects the privacy of individual samples in the original training text, but this means that DP also limits the model in learning the tail of the training distribution (Suriyakumar et al., 2021). Overall, strong DP guarantees render the generation of rare patterns in the original data unlikely. This means that the synthetic text generated from a DP-trained model may potentially miss valuable information conveyed in the outliers of the training text.
We observed in our conditional generation studies that DP disproportionally affects classes (corresponding to control codes) with different sample sizes. In particular, tight DP guarantees most negatively impact learning the distribution of small-size classes. Future work may study approaches that mitigate this negative impact for minority populations in private synthetic data generation.
We selected values for privacy parameters ϵ = 4 and δ = 1/(N · logN) based on prior privacyutility trade-off studies for text classification and table-to-text generation (Li et al., 2022b; Yu et al., 2021b). We leave it to future work for a more extensive privacy-utility trade-off analysis for general synthetic text generation.
Our canary extraction experiments demonstrated that strong DP guarantees lead to strong empirical privacy even for “private” information (the subject) that appears across multiple training instances. However, we note that DP guarantees generally translate into strong empirical privacy guarantees only when individual samples have low or no correlation (Kifer and Machanavajjhala, 2011). It is therefore crucial that DP machine learning be applied in conjunction with other modes of privacypreserving techniques (e.g., data deduplication and redaction (Zhao et al., 2022)) for optimal protection. For deployments of DP synthetic text generation, one should also consider meaningful example boundaries."
55,"Training Data Our pre-training data is sourced from 19 existing dialogue datasets. However, it’s important to note that these datasets may contain noise, such as harmful content, irrelevant file names, and URL links. Despite utilizing multiple automatic tools to filter out this content during preprocessing, there is still a chance that some noise may be present in our pre-training data. This could potentially impact the performance of DIONYSUS, making it important to monitor and improve the pre-processing steps continuously.
We also know the potential drawbacks of constructing pseudo summaries using the GSG method, which may lead to unnatural summaries for dialogue data. To mitigate this, we introduced the Summary Helper in Section 2.1, which is specifically trained on two dialogue summarization datasets containing natural summaries. This approach enables more realistic pseudo-summaries and enhances zero-shot performance. Although we employ top-m turns as an additional source of pseudo summaries, Figure 4 illustrates that GSG+ contributes a minor portion of the pseudo summary, with a 0.7 to 0.3 ratio between generated and topm turns. Our method thus minimizes referent and pronoun confusion, ensuring better coherence than solely employing the standard GSG technique.
Training Resource To improve our model’s performance, we employ the “Better ROUGE” strategy, which calculates the ROUGE score for both candidates and selects the best one as the final training objective. This data pre-processing process can be pretty time-consuming, taking approximately one day to complete for our pre-training data when utilizing 100 threads. Additionally, we utilize 16 Nvidia V100 GPUs to train our models, which may not be accessible or reproducible for all researchers. This could present a significant obstacle for those looking to replicate or build upon our work.
Test Data Another potential concern is the test datasets used to evaluate DIONYSUS. The test set size is relatively small, which may not fully represent the breadth of dialogue types that a general dialogue summarization model should be able to handle. This could lead to the model performing well on the test set but not generalizing to other unseen dialogue types. Further, our analysis did not include the assessment of long dialogue summarization, such as lengthy meetings (Carletta et al., 2005;
Zhong et al., 2021; Janin et al., 2003) or screenplays (Chen et al., 2022). However, our study’s approach has the potential to handle these scenarios, even though it was not specifically designed for them. By incorporating LongT5 (Guo et al., 2022) or DialogLM (Zhong et al., 2022), which are known for their ability to process extended input sequences, we expect that they could efficiently tackle this task."
56,Section 8
57,"The limitation of this paper are twofold. First, our method does not provide a recipe for data imbalancement in NLVL task. Thus, our method does not guarantee the effectiveness on edge cases. Second, the choice of feature extractor is considered relatively outdated. Our model does not benefit from the recent development of pre-trained visionlanguage models. On the other hand, using pretrained vision-language models remains in its early stage in NLVL tasks. Not using pre-trained features makes a fair comparison between our model with existing baselines. As a part of future work, we will explore the potential of using more powerful feature extractors in our model."
58,"Section 7
7 A2. Did you discuss any potential risks of your work? Our paper uses public benchmarks and does not have any risk of ethics or infringement."
59,Limitations
60,"Despite the demonstrated effectiveness of selfadaptive ICL, this new paradigm suffers from the following limitations. (I) As we discussed in § 6.4, due to the large search space, we need to trade efficiency for effectiveness. So how to balance the efficiency-effectiveness trade-off is an important decision choice to make when deploying selfadaptive ICL methods. (II) As shown in § 6.1, the gains of our method shrink when the size of the retrieval set gets smaller. To maximize performance, we require a high-quality retrieval set, which might not always be available when dealing with unseen tasks in practice. We also note that both limitations can be alleviated with better selection and ranking algorithms.
The remarkable performance of our method should partially attribute to the powerful TopK selection method, so we also discuss the limitation of TopK here. Despite its popularity, our analysis (§ 6.2) reveals that TopK’s effectiveness is limited to simple NLU tasks with limited label space, and it does not work well with tasks with large or even infinite label space (QA, multi-choice, and NLG). This limitation signals a new direction for ICL research: we need better selection methods to adapt ICL methods to more tasks."
61,section 8
62,Section Limitations
63,Section 7
64,"Section after Conclusion
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
65,"Rather than a complete, systematic probing of the stereotypes and biases related to each demographic group that may occur in the open-ended outputs, our study offers insight into the patterns in the stereotypes that the widespread use of LLMs may propagate. It is limited in scope, as we only evaluate models available through the OpenAI API.
Stereotypes vary across cultures. While our approach can be generalized to other contexts, our lexicon and qualitative analysis draw only upon American stereotypes, and we perform the analysis only on English. Beyond the five race/ethnicity and three gender groups we evaluate, there are many other demographic categories and identity markers that we do not yet explore.
Another limitation of our method is that it currently requires defining which identities are (un)marked a priori, rather than finding the default/unmarked class in an unsupervised manner. The prompts are marked with the desired demographic attribute, and every persona is produced with an explicit group label. Given these explicit labels, we then compare and analyze the results for marked vs. unmarked groups.
A potential risk of our paper is that by studying harms to particular demographic groups, we reify these socially constructed categories. Also, by focusing our research on OpenAI’s models, we contribute to their dominance and widespread use."
66,"Section Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 7 Did you use or create scientific artifacts? Left blank.
B1. Did you cite the creators of artifacts you used? No response.
B2. Did you discuss the license or terms for use and / or distribution of any artifacts? No response.
B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? No response.
B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? No response.
B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? No response.
B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. No response.
C 3 Did you run computational experiments? 5"
67,"The main limitations of our study also provide motivation for future work. First, while we have provided an extensive ablation study for GEC-DePenD, there are many more low-level optimizations that can be done to further improve the results. In a real life application, one would be encouraged to investigate these optimizations.
Second, obviously, non-autoregressive models, including GEC-DePenD, still lose to state of the art autoregressive models. While the existence of this gap may be inevitable, we believe that it can be significantly reduced in further work."
68,"Section 7
7 A2. Did you discuss any potential risks of your work? Our work deals with improving grammatical error correction and does not seem to have potential risks beyond the usual ecological concerns related to using large language models; we do note the model size and training time."
69,After the conclusion section and before the reference section
70,"section Limitations after the conclusion
7 A2. Did you discuss any potential risks of your work? As our model does not generate its own outputs, when used with trustworthy sources, we do not see high societal risks. However, we admit that those biases from the training datasets can be amplified. For example, regardless of improvements, our model can not fully address the deficiency of dense retrieval on rare entities, which can compromise the fairness of retrieval."
71,"Although our approach produces promising results on two datasets, there are certain limitations. In the future, we will continue to dig into these concerns.
Firstly, we evaluate the DaMSTF on two classification tasks. We do not conduct experiments on other NLP tasks, such as machine translation (Yang et al., 2018) or named entity recognition (Jia et al., 2019). Nonetheless, as text classification is a fundamental task, other NLP applications can be specified as a case of classification. For example, named entity recognition can be formulated as a wordword relation classification task (Li et al., 2022).
Secondly, the meta-learning module carries out extra computation overhead. As the bi-level hyperparameters optimization involves a second-order derivate on the model’s parameters, their computation overhead is quadratic to the model’s parameters. In DaMSTF, we use the approximation techniques in WIND to compute the derivate, which is linear to the model’s parameters. In the future, we
will investigate other techniques to accelerate the DaMSTF."
72,"Even the premise of parsing questions to Wikidata queries leads to linguistic and cultural bias, as Wikidata is biased towards English-speaking cultures (Amaral et al., 2021). As Cui et al. (2022) argue, speakers of other languages may care about entities and relations that are not represented in Englishcentric data (Liu et al., 2021b; Hershcovich et al., 2022a). For this reason and for the linguistic reasons we demonstrated in this paper, creating CG benchmarks natively in typologically diverse languages is essential for multilingual information access and its evaluation.
As we mentioned in §4.2, our translation system fails to deal with ambiguities beyond grammar and thus generates wrong translations for a few samples (less than 0.31%). Moreover, although the dataset can be potentially augmented with low-resource languages and in general other languages through the translation framework, adequate knowledge will be required to expand rules for the specific target languages.
With limited computational resources, we are not able to further investigate the impact of parameters and model sizes of multilingual PLM as our preliminary results show significant performance gaps between PLMs.
Broader Impact
A general concern regarding language resource and data collection is the potential (cultural) bias that may occur when annotators lack representativeness. Our released data largely avoid such issue due to the synthetic and cultural-invariant questions based on knowledge base. Assessment by native speakers ensures its grammatical correction. However, we are aware that bias may still exist occasionally. For this purpose, we release the toolkit and grammar used for generation, which allows further investigation and potentially generating branches for other languages, especially low-resource ones.
In response to the appeal for greater environmental awareness as highlighted by Hershcovich et al. (2022b), a climate performance model card for mT5-small is reported in Table 7. By providing access to the pre-trained models, we aim to support future endeavors while minimizing the need for redundant training efforts."
73,"The Limitations section follows the Conclusion section.
7 A2. Did you discuss any potential risks of your work? Our work only provides a benchmark to evaluate semantic parsing models and not an application that can be used for potentially risky purposes."
74,"While our approach effectively mitigates query latency through a cascade ranking paradigm, it necessitates additional computational resources during training due to the need for attention score calculation and alignment in the optimization process. Additionally, our model incorporates passage-level relevance scores into the ranker, generating a cooperative matching representation during document ranking, which could marginally augment the inference time. In our future endeavors, we aim to explore more efficient methodologies that can further improve ranking efficiency. Furthermore, it
is worth noting that our approach has been tested using specific backbone models. To fully evaluate the effectiveness of our method, it is essential to conduct experiments with a diverse range of backbone models, which remains an avenue for further exploration."
75,"Limitation section
7 A2. Did you discuss any potential risks of your work? The topic of the paper deals only with document retrieval"
76,Section Limitations.
77,"Obviously, the work presented in this paper is limited to transcripts of spontaneous conversations in English. Since we are investigating the problem of named entity recognition, we have to point out that there are practically no datasets of human conversations (both audio and transcripts) annotated with entity spans apart from SWNE, OntoNotes and Earnings-21, the three datasets used in our paper. These datasets are relatively small, and the distribution of the frequency of appearance of entity classes is extremely skewed, with several entity classes represented by a handful of examples.
Another significant limitation of the results reported in this paper is the choice of metric. Following the common practice in the NLP community, we have chosen the F1 score as the primary metric of entity recognition. However, this metric is questionable in the context of NER recognition in ASR transcripts because it is highly dependent on two factors: the WER produced by the ASR and the definition of span alignment. Consider a gold transcript annotation ""JohnB-PERSON F.I-PERSON KennedyI-PERSON"" and the ASR output with ""F."" transcribed as ""eh"" annotated as follows: ""JohnB-PERSON eh KennedyB-PERSON."" Should this annotation be considered correct? The original person entity starting at ""John"" is only partially matched, and a new person entity starting at ""Kennedy"" is introduced in the ASR output. Consider another gold annotation of the following transcript: ""secondB-DATE quarterI-DATE twentyI-DATE twentyI-DATE,"" which the NER model tags as follows: ""secondB-DATE quarterI-DATE twentyB-CARDINAL twentyI-CARDINAL"" (NER model trained on written language does not recognize ""twenty twenty"" as a valid date). Again, how should this scenario be scored by an accuracy metric? Unfortunately, the traditional definition
of the F1 score is too restrictive to produce a robust score that could paint a reliable picture of the model’s performance. The design and implementation of a metric that could compute the alignment of entity spans in the presence of ASR errors would be a significant step in the direction of producing more robust NER models for spoken conversations.
We conduct experiments with the ASR on audio files from the Earnings-21 dataset. These files are recorded at 11 kHz-44 kHz, while typical call center conversations are recorded at 8 kHz-16 kHz. Unfortunately, training datasets with recording characteristics resembling real-world usage scenarios are unavailable. We also do not address the problem of racial, gender, and age disparity (Koenecke et al., 2020) due to the lack of availability of sufficiently representative and inclusive datasets. It is, however, to be expected that the performance of the ASR deteriorates for the recordings of speakers other than male speakers of General American."
78,7. Limitation
79,Section 6
80,"We investigated CSJ with SELECT, SIMPLIFY and REWRITE. We adopted HIPORANK as SELECT be-
cause it is a lightweight, unsupervised model that extracts a summary in a discourse-aware manner. However, when we replaced it with other extractive models during the component analysis, we found no significant difference in overall performance. We adopted KEEP-IT-SIMPLE for SIMPLIFY because it facilitates paragraph simplification. We found the model is quite heavy, making it slow during training. To the best of our knowledge, there is no paragraph-based simplification model we could explore in component replacement. The choice among various pre-trained models for REWRITE was quite challenging, as all these models are variations of transformer-based architectures. So we adopted the latest three SOTA models, which are efficient and effective summarization models. We also trained the vanilla sequenceto-sequence model, pointer-generator model and transformer as our baselines to provide sufficient variations of SOTA models. We found mBART is more promising performance-wise in our experiments. However, its training time is also slow for our datasets due to longer inputs."
81,"Section Limitation
7 A2. Did you discuss any potential risks of your work? There is no potential risk."
82,"Section 8
7 A2. Did you discuss any potential risks of your work? This work examines research integrity issues related to model evaluation and does not feature new datasets or models. It is possible the findings of this work will have negative consequences for past and future research, which is a point we discuss in the text. However, because this work does not involve releasing data or model artifacts, it is unlikely that any outcome of this work will be misused with malicious or unintended effects or deployed in any context that is risky, harmful, or negatively impacts privacy, security, or fairness."
83,"Since our primary goal is to study the phenomenon of instruction induction under lab conditions, we focus on tasks that have simple instructions. Future work may extend instruction induction research by including tasks with more complex instructions. These tasks are expected to pose a greater evaluation challenge, especially when considering reference-based methods. Evaluating through execution accuracy, however, may mitigate some of that challenge. Additionally, only one model showed instruction induction abilities, i.e., textdavinci-002. The exact implementation details of the model and its training data are not publicly available, thus we are unable to investigate the reason behind the emergence of this ability. However, we note that our goal is to present the phenomenon of instruction induction and to raise the ambitious possibility of instruction induction as a learning paradigm. Thus, our goal is not to focus on specific models but rather to shed light on this unexplored phenomenon. Finally, we point to a limitation of the execution accuracy metric, namely assuming the existence of a good-enough instruction-tuned model. Due to recent interest and progress in instruction tuning, we believe this to be a reasonable assumption."
84,"In its current formulation, REV might reward a rationale for an incorrect prediction as long as the rationale supports the prediction with relevant additional information. Additionally, our metric does not consider the factuality of rationales. Future work might explore evaluation that penalizes rationales which support incorrect predictions, thus bridging together predictive performance with interpretability metrics. We considered a single declarative construction for baseline rationales and leave analyzing how different baseline construction impacts our metric to future work. Another limitation is that the utility of REV depends on the quality of crowd-sourced rationales used to train the evaluator. Building a good automatic metric REV requires high-quality rationales that provide sufficient new information (e.g., commonsense knowledge) to explain the corresponding labels. The architecture of evaluation models also has an impact on REV evaluation. Using different evaluator architectures may result in varying REV scores, as discussed in Appendix B.3."
85,"We conclude the limitations of our schema into two aspects. Firstly, our method benefits from the assumption that there exists similar semantics between the seen data and unseen samples. However, our work might not own obvious advantages in the case where the correlation among domains is weak, such as medical assistant and movie service. But notably, in such cases, most zero-shot
learning methods will also fail to show well generalization. Secondly, we propose to train semanticindependent DST experts, which is ideal but we believe advanced components could move towards this goal, such as using advanced clustering algorithms and pretrained language models."
86,"After Section 6
3 A2. Did you discuss any potential risks of your work? 6
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 5
3 B1. Did you cite the creators of artifacts you used? 5
3 B2. Did you discuss the license or terms for use and / or distribution of any artifacts? 1,5,6"
87,In the last section
88,"Our proposed KALM has two limitations:
• KALM relies on existing knowledge graphs to facilitate knowledge-aware long document understanding. While knowledge graphs are effective and prevalent tools for modeling real-world symbolic knowledge, they are often sparse and hardly exhaustive (Tan et al., 2022; Pujara et al., 2017). In addition, external knowledge is not only limited to knowledge graphs but also exists in textual, visual, and other symbolic forms. We leave it to future work on how to jointly leverage multiple forms and sources of external knowledge in document understanding.
• KALM leverages TagMe (Ferragina and Scaiella, 2011) to identify entity mentions and build the three knowledge-aware contexts. While TagMe and other entity identification tools are effective, they are not 100% correct, resulting in potentially omitted entities and external knowledge. In addition, running TagMe on hundreds of thousands of long documents is time-consuming and resource-consuming even if processed in parallel. We leave it to future work on how to leverage
knowledge graphs for long document understanding without using entity linking tools."
89,right after the main paper on page 9
90,"Adapting PLMs to our proposed model does not go as smoothly as expected, because there are three different forms of tokenization: the PLM tokenizer, the multilingual tokenizer implemented in our proposed model, and the special annotations of numerical values/entity mentions/long-winded attribute values in the attribute extraction datasets, which are difficult to reconcile simultaneously. Although our model without PLM has outperformed PLMbased ones, this does impose a limitation for future explorations.
Although Re-CNShipNet, one of the datasets used in our experiments, is more accurate with our careful re-annotating, the size of which is still so small that would produce randomness bias during the model training and may affect the final experimental results.
Besides, due to the limitation of computational resources, we did not conduct experiments on large
language models such as T5 (Raffel et al., 2020), LLaMA (Touvron et al., 2023), etc., which may lead to insufficiency of the experiment.
Ethics Statement
This work uses three publicly available datasets, and we respect and adhere to their user agreements and licenses. The content of pre-existing datasets does not reflect our perspectives. We, the in-house authors, re-annotate one of these datasets, i.e., Re-CHShipNet; the purpose of re-annotation is mainly to correct errors and re-balance the ratio of CWA/OWA labels. The annotation may introduce personal judgment and bias, which may bring potential risks. Further, the potential downstream applications of this work include knowledge graph construction, search engine, e-Commerce, recommendation system, etc.; we caution that our proposed method may cause misextraction or false information, and may fail in the case of out-ofdistribution and domain shift, which may harm those applications."
91,Section 8 (Limitations).
92,the limitation section on page 9.
93,"Due to the high computational costs of the method, we tested it only on a very small set of sentences and larger-scale experiments are needed to confirm the results.
Many parameters of the GA algorithm were left unexplored – the results could be improved by grid search over the values for mutation and crossover ratios, using a better list of mutation candidates (for example based on k-NN search), experimenting with different selection methods, combining more metrics in the fitness function or using multiobjective GA like NSGA-II (Deb et al., 2002).
In the experiments concerning held-out metrics, we assumed weaknesses of the held-out metrics are not correlated to the weaknesses of the optimization metrics, which is probably not true, due to similar model architectures and training datasets. This means that held-out metrics are not strictly independent, but we believe combining multiple different held-out metrics should mitigate this issue."
94,"section Limitations
7 A2. Did you discuss any potential risks of your work? All data used in our work comes from public datasets, which ensures that there are no privacy issues involved in our work, so there is no potential risk in our work."
95,Limitations section
96,Left blank.
97,"In this study, we mainly utilised the BERT family of models for Chinese text classification tasks. Given the similarity with respect to transformer language models and pre-training paradigms, as well as the preliminary results on English datasets as discussed in Section 6.3, we may be able to extrapolate the results to other architectures/tasks/languages.
For example, Perplection can be seamlessly apply to decoder-only models (e.g., GLM (Du et al., 2022), LLaMA (Touvron et al., 2023)) to see whether it can boost the performance for those NLG tasks. But further investigation is needed to verify the utility of findings on other model architectures, tasks, and languages. In the future, we expect to see Perplection applied to different NLG tasks such as seq2seq information extraction (Lu et al., 2022b), question answering, arithmetic reasoning, machine translation or even multi-modality tasks.
Also, utilising Perplection may exacerbate the inherent limitations of pre-trained language models. We suspect that, in instances where the model has not been exposed to certain texts or concepts during pre-training, reliance on perplexity for template selection may result in subpar performance. In the future, we expect to explore whether we can alleviate this problem by certain annotation-free methods, such as continuous self-supervised training with downstream data, or extend our method in a few-shot setting where limited label information is available.
Besides, the use of perplexity as a metric has the drawback of favoring long texts, which forces us to design templates of the same length. Therefore, a length-agnostic metric can be considered as an alternative."
98,"Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 4, 5
3 B1. Did you cite the creators of artifacts you used? 4, 5
B2. Did you discuss the license or terms for use and / or distribution of any artifacts? Not applicable. Left blank.
B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Not applicable. Left blank.
B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.
B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Not applicable. Left blank.
3 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Left blank.
C 3 Did you run computational experiments? 4, 5"
99,"The data set presented is still quite small for machine-learning models, as is the number of annotators (and thus the demographic diversity). Since the annotation required a lot of human effort, we chose fewer, but experienced, student assistants as annotators to ensure a high quality of the annotations.
The agreement for effectiveness and argumentative function is low. To address this weakness we used the following strategies: a) An examination of the confusion matrices reveals that the annotation scheme is not exclusive, that is, a story can take on multiple argumentative functions. We therefore include different, aggregated versions of our dataset that include this annotation layer as a multi-label layer (see Section 4). b) We address the subjectivity of the two annotation layers in a regression analysis (Section 6). The interactions between each annotator and certain annotated properties show annotator-specific differences, which should also not be ignored in the modeling.
A crowd-sourcing study could build on the initial findings and collect more annotations for effectiveness to investigate perspectivism in this context. Finally, we lacked sufficient space to analyze the existing annotations of the sub-corpora of our resource (e.g. testimony in CMV and Regulation Room) and discuss them with our new annotations. We see this as an opportunity for future work."
100,"There are several important limitations to this work that can be split into two categories: (1) method applicability to other domains and (2) method scalability to much larger models.
Method applicability to other domains. Utilization rate computation and regularization are possible when there is some external knowledge that can be used to infer which tokens are “important.” In particular, our highest-performing model uses token semantic type to compute utilization rates. This limits our approach to sub-domains where there is an external knowledge source that can inform us about important tokens and give us higher-order semantic information about how to group the important tokens. For example, our approach will likely not be very helpful for open-domain conversations.
Method scalability to much larger models. We have evaluated our approach for models on the scale of O(108) parameters. However, modern state-of-the-art models often involve O(1011) parameters, three orders of magnitude larger than models in our experiments. Large language models (LLMs) often still suffer from the under-generation of rare tokens, but our study is insufficient to determine if our approach would still work. We suppose that utilization-rate-based regularization is most likely to be beneficial in the fine-tuning step of LLMs, but verification of this is left for future work."
101,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
102,"Section 8
7 A2. Did you discuss any potential risks of your work? No potential risks"
103,"No section number, after Section 5 Conclusion
7 A2. Did you discuss any potential risks of your work? We do not see significant risks in our work"
104,After section 8 and before the references - as requested.
105,"In this work, we only focus on designing strategies for PLMs with the MLM-style pre-training objective, and do not account for other types of pre-trained language models such as discriminative PLMs (Clark et al., 2020; Shen et al., 2021). However, as there are recent works that aim to design prompts for discriminative PLMs (Yao et al., 2022; Xia et al., 2022), PATRON can be potentially combined with them to improve the data efficiency.
We are also aware that there exists advanced fewshot fine-tuning techniques for PLMs recently (Hu et al., 2022; Tam et al., 2021; Zhang et al., 2022b, inter alia). We argue that PATRON does not rely on a specific fine-tuning method, and can be combined with them to further improve the performance. Lastly, as prompting methods have been widely adopted to other tasks such as natural language inference (Gao et al., 2021a) and relation extraction (Han et al., 2021), it is possible to extend our method to these tasks."
106,"In our paper, we presented existing and novel training-free NAS metrics for RNNs and transformers. Benchmarks are required to evaluate the effectiveness of these metrics on various architectures. While there exists a robust benchmark for RNN architectures (NAS-Bench-NLP), there is none for transformer models. Thus, we had to create our own NAS benchmark. For our work, we were limited by the computational resources available to us, so we were only able to pretrain and finetune 500 models for our NAS BERT benchmark. A larger sample size would give a more accurate evaluation of the training-free NAS metrics. Furthermore, we only investigated the FlexiBERT search space. While FlexiBERT has a diverse search space, having heterogeneous layers and alternative attention operators, the variation between possible architectures is limited and still dependent on the linear paradigm of BERT. Alternative transformer search spaces using cell-based methods, such as those presented in “Primer” (So et al., 2021) and “AutoBERT-ZERO” (Gao et al., 2022), do not have this limitation. We were ultimately unable to investigate the performance of training-free NAS metrics on this type of search space, as there are no available benchmarks for these search spaces, and their greater variability necessitates a copiously large sample size that is well outside our computational capabilities.
Another limitation is that we only evaluated the effectiveness of the presented metrics on encoderonly transformer architectures, and not encoderdecoder or decoder-only architectures. Furthermore, while the training-free NAS metrics are dataagnostic, the benchmarks they were evaluated on were only trained and evaluated on English datasets and tasks."
107,"Algorithmic Limitations: The current approach assumes each phoneme / grapheme corruption is independent of the surrounding phonemes / graphemes, which can be relaxed to get further insights and model any contextual phonetic shifts. The relative importance between grapheme and phoneme corruptions could also be explored as a hyperparameter to personalize more to the type of errors of a community. Other Limitations (with respect to available data and existing resources): Our coverage analysis is conservative since it does not cover the user generated data from various social media where such
L1-L2 phonetic misspellings are bound to be more common. The coverage analysis also relies on the context not being corrupted. However, this might not necessarily hold and the analysis could benefit from a careful formulation of a relaxed matching criteria that also considers cases with corrupted contexts. With transliteration playing a major role in our solution, it is difficult to immediately extend the work to low-resource languages that do not have models or appropriate datasets to build transliteration modules."
108,"There are two limitations to this work. First, it takes effort to design the prompt to guide the LLMs to generate correct reasoning steps. The GPT-3 models are sensitive to the expressions in prompts. Thus we need to carefully design the prompts. Second, the proposed plan-and-solve prompting can help address the calculation errors and missing-reasoningstep errors, but the semantic misunderstanding errors still remain. We will explore how to address semantic misunderstanding errors by prompting instead of upgrading LLMs in the future."
109,"Section 6
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
110,"Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
111,Limitations section (and throughout)
112,"Our findings are primarily based on ROUGE score, which is a noisy, unstable metric with well-studied limitations (Schluter, 2017). To address this, however, we conduct a human evaluation to support our findings. In both automatic and human annotation settings, we base our evaluations on naturally occurring references, which have been shown to be silver-standard (Gehrmann et al., 2022; Wan and Bansal, 2022; Adams et al., 2022). We hope that our work on PGA–a method to generate high-quality diverse candidates–can be applied to new domains (e.g., (Gliwa et al., 2019; Adams et al., 2021; DeYoung et al., 2021)) and reference-free learning objectives (e.g., RLHF and calibration). Also, our candidate generation method requires two models, which is less elegant and computationally efficient than an end to end solution combining planning and surface realization.
Lastly, PGA treats all content plans as equally likely (each plan is given one abstractive beam). Yet, there is an unexplored trade-off between exploration and exploitation. Should higher-confidence content plans receive more candidates? Future work should explore a generating diverse abstracts from a dynamic nucleus of extracts, which would allow for the generation of many abstracts from only a few extracts when confident (e.g. short documents), while exploring more diverse content when the extractive generator is less confident. We sketch out such a potential system in Figure 5 with a made-up nucleus probability of 0.9."
113,"In this section, we outline the key limitations of our research. Our findings on the ACQ models are not as advanced as the current state-of-the-art, but they serve as a benchmark for others to compare with when using similar datasets. Additionally, to conduct more extensive experiments on larger datasets and more advanced models, we require additional computational resources. Specifically, generating clarification questions is a demanding task as it requires the use of powerful language models."
114,"Experiments on other types of reasoning tasks. In addition to the two representative reasoning tasks (arithmetic reasoning and multi-hop question answering) that we experiment on, there are also other tasks where CoT prompting brings significant improvements over standard prompting shown by previous work, many of which are symbolic reasoning tasks such as Last letter concatenation, Coin flip from Wei et al. (2022) and Temporal Sequences, Tracking Shuffled Objects from BIG-Bench (Srivastava et al., 2022; Suzgun et al., 2022). However, most (if not all) tasks there are highly templatebased and hence the reasoning steps have little variations, both within each example and across different examples. This makes it difficult for us to conduct our ablation studies on these tasks. Take the example of Last letter concatenation, a task about concatenating the last letters of a given sequence of words (e.g., “Amy Brown” → “yn”). Here, every step in the rationale except the last is in the form “The last letter of X is Y” where X is some word in the given sequence and Y is the last letter of X. Hence, the language templates are the same and there is no sense of order among the steps (the order is completely characterized by the given sequence instead), and our ablation settings will not apply well. Extending our ablation designs to these “reduced” cases is one of the items we want to explore in the future. A more systematic treatment of “invalid reasoning”. We manually write rationales with invalid reasoning for the experiments in §4 since automatically synthesizing such rationales turns out to be
challenging, mostly due to the informal nature of the tasks we experiment on (relatedly, the original CoT rationales are also human-written). We intend to give a more systematic treatment of the invalid reasoning setting in the future, e.g., following the categorizations of informal logical fallacies (Copi et al., 2016). Improvements on intrinsic evaluation. Our intrinsic evaluation of the generated rationales is based on the correctness of bridging objects, which, even though is a good indicator of the quality of language templates (Appendix A.2) in our experiments, may not be a good metric in general cases. It also relies on ground truth bridging objects, which are usually not available and costly to annotate. Toward this end, one direction we want to explore further is to develop ways to conduct more comprehensive and reference-free intrinsic evaluations. Recent papers such as Golovneva et al. (2023) have also done promising work along this line."
115,"Other ways of reducing the amount of required supervision could be attempted, but we do not expect that these would change the outcomes significantly. Self-supervised learning via masking / denoising objectives, either in the form of an auxiliary task or via the use of pretrained models, is one such approach. This however generally underperforms backtranslation, which can utilise the same monolingual data to more effect (NLLB Team et al., 2022), as we see in the experiments of Appendix E. Iterative backtranslation might offer an additional boost for data-scarce settings, but is very computationally intensive, complex, and any gains would almost certainly apply to models trained with the addition of seed data too.
The seed datasets that we release bring about large translation performance gains for a number of low-resource languages. We note that, due to budgetary and complexity constraints, the source data we used was sourced from English Wikipedia only. This is likely to have two effects. First, translating English-original data leads to so-called translationese effects one the low-resource side (Volansky et al., 2015), leading to decreased effectiveness for directions that target low-resource languages. Second, the data is unlikely to adequately cover diverse content from multiple cultures. An interesting avenue for future research would therefore involve studying the effects of seed parallel data that is originally translated from low-resource languages."
116,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
117,"Sec. Limitations.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
118,"The section after Conclusion.
7 A2. Did you discuss any potential risks of your work? This work presents a general compression method, which is not tied to particular applications."
119,"The techniques in SpanSub are constructed on the basis prior works of extracting span alignments and clustering words in the training data according to their syntactic role. There is no generic solution for these problem applicable for all of the datasets (this is mainly because the output formats and structures are diverse) at present, which requires users to spend efforts looking for preprocessing techniques applicable for their own datasets. However, the methodology of the proposed SpanSub is rather general to many different datasets and tasks (e.g., Semantic Parsing and Machine Translation). Besides, although we define eligible spans to try to alleviate additionally introducing noisy augmented data, our experiment result on GeoQuery (i.i.d. split) shows that SpanSub can still slightly hurt generalization performance (in comparison with other state-of-the-art methods). Hence we regard that relieving the potentially negative influence of noisy augmentation is important to further improve this work."
120,The Limitation Section on page 9.
121,"Yes, we provide the limitations of our work in Section 7 (conclusion) and Limitation Section.
7 A2. Did you discuss any potential risks of your work? There seem to be no potential risks."
122,"We recognize that our annotation and analysis methods can require considerable human labor, that can limit the amount of annotated data we can collect. Also, despite cycle training being generally accepted as a model-agnostic approach, we were not able to test a wide variety of backbone models due to resource constraints. In addition, though we relaxed the entity constraints and made cycle training for data-to-text generation end-to-end, the nondifferentiability problem remains unsolved. The intermediate outputs generated by the first model of each cycle are assumed to be correct. This is a weak assumption that may propagate misleading training signals to the second model of each cycle, particularly in the early stage of the training.
To address these limitations, future work may focus on the following directions: 1) building differentiable cycle training models; 2) exploring au-
tomated error detection methods and building models that may utilize such signals; and 3) assessing different backbone models, including large language models like GPT-X, with the cycle training approach."
123,"In this paper, we employ an information entropyguided algorithm for purifying the induced biased features. For each dimension of the biased features, the component with less information entropy is priorly regarded as the component corresponding to semantic information, and excluded when deriving the purified biased features. However, there is still the risk that the discarded component still account for part of the dataset biases. This would lead to a decrease in the effectiveness of the debiasing process. Hence, although the prior-knowledge free nature endows our proposed biased features purification algorithm with strong generality, in cases when resources indicating the distribution of dataset biases are available, incorporating these resources would further enhance the purification of the biased features."
124,"Although DEER has shown excellent performance on multiple datasets and tasks, we still found some limitations affecting its usability and efficiency: (1) The latent alignment model (such as CTC) cannot deal with the multi-modality problem in the largescale dataset, which also leads DEER to underfitting the multiple latent alignment targets that need to be aligned. (3) Although DEER does not need to perform length prediction, it relies on the assumption that the input length is large than the output, which causes the model to lose flexibility in length control. (3) We compared sequence-tosequence models such as BART and ProphetNet in the experimental part of this work. In fact, BART only through six layers on each forward pass, while the BERT family model needs to go through 12 layers, leading the inefficient inference due to latency accumulation of multiple iteration steps."
125,"We provide the limitations in Section 8.
7 A2. Did you discuss any potential risks of your work? We think our general training method will not lead to any negative societal impact."
126,"While our theoretical work is broadly applicable to any protected attribute and any dialogue task, our empirical study has primarily tested gender bias on the GuessWhat?! task. Continued experimental study on a wider range of protected attributes and tasks can better support our mathematical findings. Also, users of our theory should verify the assumptions of our theory when using it to draw insights on new datasets. Specifically, as the type of data bias changes, it is possible the assumptions of Thm. 3.2 may no longer be met. Users of our theory should take care in ensuring context-awareness and context-preservation, for example, are reasonable assumptions on new data, prior to applying the insights of § 3.3. Lastly, while all of our gender annotations come from human annotators, only a smaller subset come from annotators primed to
14https://github.com/anthonysicilia/equitable-dialogueACL2023
judge correctness/equity of gender reference. So, more in-depth human evaluation can better support our theoretical results as well."
127,"Legislators show political support in multiple ways. In this work, we operationalised political support as Active and Passive cosponsorship. Active and
Passive cosponsorship represent a strong signal of support between legislators that has been widely accepted in the political science literature (Kessler and Krehbiel, 1996; Wilson and Young, 1997; Browne, 1985; Woon, 2008; Sciarini et al., 2021; Dockendorff, 2021; Fowler, 2006; Kirkland, 2011; Kirkland and Gross, 2014; Lee et al., 2017). However, other forms of political support, e.g., endorsement of public posts on social media, could be considered. Future research might explore the extent to which these forms of support might reveal additional insights about the cooperation between legislators.
Our second limitation relates to the estimation of legislator’s ideology. Ideology is a latent concept. This means that it cannot be directly measured and no ground-truth data exists. Therefore, to validate that our legislator representations encode ideology, we need to prove their performance in a variety of tasks in which the political science literature suggests ideology is important. In our work, we studied three tasks: (i) active/passive cosponsorship prediction, (ii) party affiliation recovery, and (iii) voting prediction. We argue that this is a representative set of tasks. However, legislators are involved in additional ideology-driven tasks, e.g., the release of public statements. Showing that our representations are also predictive of these additional tasks might be considered an even more robust and convincing validation of our results.
Third, in its current form, our model cannot compute predictions for newly elected legislators. This is due to no data being available—newly elected legislators have not given any speeches, or (co)sponsored any bills. We argue that by applying our model as an online predictor, new information on legislators could be incorporated as soon as it becomes available. However, a full exploration of our model’s potential for this application was outside the scope of this work.
Our final limitation concerns how our model can be extended to other data. In our work, we studied four different U.S. Congresses. For these, we obtained consistent and high performance. Therefore, we expect this performance to extend to other Congresses. However, having focused exclusively on the U.S., we cannot make any statements about the applicability of our framework to other legislative systems. Addressing this limitation could contribute to proving the generalizability of our results.
Future Work Our work can impact studies on t latent factors (e.g., ideology) in other domains. For instance, recent works on radicalization (Russo et al., 2022b,a) can take a similar approach to study the relation between ideology and radicalization. Similarly, studies on international relations can benefit (Stoehr et al., 2023b) from this approach in order to study latent states between nations such as “ally”, “neutral”, and “enemy”.
ACL 2023 Responsible NLP Checklist"
128,"The construction of the reasoning tree may be affected by the KG quality since the connection operations are variant with the KG structure. Hence the unsolved problem in Knowledge Graph such as incompleteness or noise could disturb the reasoning process. In the future, we will explore a solution to alleviate the influence of the side information."
129,Section Limitations
130,"Dataset Representativeness Our dataset covers a range of topics of public interest (COVID-19, climate change, abortion, migration, the RussoUkrainian war, and local elections) as well as media from all sides of the political spectrum. However, it should not be seen as representative of the media in any country, nor should it be seen as perfectly balanced in any specific way.
Biases Human data annotation involves some degree of subjectivity. To mitigate this, we created a comprehensive 60-page guidelines document (Piskorski et al., 2023a), which we updated from time to time to clarify newly arising important cases during the annotation process. We further had quality control steps in the data annotation process, and we have been excluding low-performing annotators. Despite all this, we are aware that some degree of intrinsic subjectivity will inevitably be present in the dataset and will eventually be learned by models trained on it.
Baseline Models The reported experiments can be seen as strong baselines as they include fairly small encoder-only transformer architectures. We leave for future work the exploration of other architectures and modeling techniques that are known to improve the efficiency and to reduce the computational requirements of the used models, e.g., fewshot and zero-shot in-context learning, instructionbased evaluation, multitask learning, etc.
Model biases We did not explore whether and to what extent our dataset contains unwanted biases.
5https://propaganda.math.unipd.it/ semeval2023task3/"
131,"We hereby discuss the current limitations of our work: (1) As mentioned in Section 3.1, although our annotated dataset enables the possibility of learning an extractive model that can be trained to predict the span of the text segments of interest from scratch, we focus on the more essential actioncondition dependency linkage inference task as we find that the SRL extraction heuristic currently applied sufficiently reliable. In the future, we look forward to actualizing such an extractive module and other relevant works that can either further refine the SRL-spans or directly propose the text segments we require. More specifically, the extractive module can be supervised and/or evaluated against with our human annotations on the text segment start-end positions of an article. (2) The current system is only trained on unimodal (text-only) and English instruction resources. Multilingual and multimodal versions of our work could be as well an interesting future endeavors to make. (3) In this work, we mostly consider instructions from physical works. While certain conditions and actions can still be defined within more social domain of data (e.g. a precondition to being a good person might be cultivating good habits). As a result, we do not really guarantee the performance of our models when applied to data from these less physicaloriented domains."
132,Limitation is section 6 after conclusion
133,"In this section, we discuss the limitations of this work. First, this study is limited to Englishlanguage tasks, due to English being the common language of the annotators. It is possible that some conclusions from this work may not extend to task definitions written in other languages; we hope that future work can extend this analysis to a multilingual context. Further, the datasets and models used may contain biases reflecting the culture of the English-speaking population, as well as biases relating to gender, race, age, and other socioeconomic factors.
Second, in Section 5, we propose a common structured format to organize the key information for a task. We rewrite the original natural language definitions into triplets after extracting key information in it and observe improved performance. However, a complementary perspective is to write such a triplet from scratch, by filling in the blanks in triplet templates and seeing whether the improvements still hold. This directly reflects whether such
an organizing method works. Our approach serves as a starting point to demonstrate the effectiveness of using a structured and condensed definition.
Third, larger language models can be tested. The largest model we adopt is a T5 model with 3B parameters. As we observe variant behavior as model size grows, later work can further extend our analysis to larger models. Also, new emergent ability of LMs might be discovered with larger models, like mathematical reasoning with larger models following instructions. That is beyond the scope of this paper.
Last, some observations cannot be easily explained in this paper. For example, we saw that removing label information for classification tasks during training eventually also affects the model performance on generation tasks, which can be counter-intuitive and requires further exploration. Later work can pick a few points in the paper and provide deeper analysis on them."
134,Section 9.
135,"Limitation section
7 A2. Did you discuss any potential risks of your work? The topic of the paper deals only with dialogue retrieval"
136,"The study of language model in their alignment to linguistic theories are interdisciplinary and hence usually hard to find explicit connection between language model and theories. In this paper we claim that a generative model, ciwGAN, can model both phonetic and phonology features. However, the two features are learned by two ciwGAN instances from disjoint training data sets. Our finding couldn’t support or deny the following statements that are of researchers’ concern:
1. Generic GAN model can learn phonology features like ciwGAN.
2. CiwGAN can model phonetic and phonology features simultaneously from a single dataset."
137,Limitations
138,"There are two main limitations to this work. First, we focus on the “filtering” approach to controlled generation. While this formulation clarifies what a distribution is, it can be computationally expensive to do rejection sampling in practice. A promising area of future research is the application of these invariance principles to the design of large language models. Second, achieving true invariance, i.e., generalizing to any arbitrary distribution of text, is a challenging open problem. The purpose of this paper is not to solve this problem. Rather, we illustrate that controlled generation is an important application area for invariance methods. An exciting area of future work is to use prompted language models to construct well-defined distribution shift benchmarks for domain generalization methods.
Controlled text generation has the potential to have large impacts on society, both positive and negative. One potential source of risk is misuse. Although we focus on the detection and removal of toxicity, the method we developed can also be applied to the generation of dangerous and toxic content. In addition, this paper does not address other biases (such as gender or social bias) that may already be present in language models. The use of a toxicity filter may compound the problem of decreased diversity in generated text if there is a correlation between social biases and toxicity."
139,Section 7 (Limitations and Potential Risks)
140,"We acknowledge that our dataset is not huge compared to other sentence-level relation extraction datasets. However, HistRED is the first bilingual RE dataset at the document level on the historical corpus. In addition, we constructed 5,816 data instances, and our bilingual model trained on HistRED achieved an F1 score of 63.48 percent when SL is 2. This reveals that our dataset is sufficient for finetuning the pretrained language models. Also, because Yeonhaengnok is a collection of travel records, the domain is not as expansive as other Joseon dynasty records. Additional research on massive corpora covering a broader domain is required in future studies."
141,"We study a limited scope of long-form answers. The questions are either drawn from search queries or from community forums. In the real world, we will encounter many more diverse forms of long form question answering, such as answering questions in education or commercial settings. We only cover the English language, and thus our questions are topically limited to English-speaking culture.
Our evaluation of long-form answers is stationary. Annotators are provided a pre-generated output from the model without being able to interact with the model over multiple rounds. A more interactive evaluation (Lee et al., 2022) of models is a great direction for future work."
142,"Collapsed fine-tuning runs mostly occur in the low resource scenario where PLMs may easily overfit to the small data. The improvement with the proposed technique becomes marginal when the amount of training data scales up, as shown in Table 2. The other limitation is that HyPe introduces two new hyper-parameters: The noise distribution form and the scale of variance. To achieve the best performance, we may need to search for different combinations of hyper-parameters."
143,"Personalized news headline generation has the potential to improve the way users consume and understand the news. However, it is important to be aware of its limitations. The performance of any natural language generation model, including those used for personalized news headlines, is dependent on the quality and consistency of the data used to train it. Similar to personalized recommendation systems, personalized headlines have the potential to create echo chambers. If the model is trained on a biased or unrepresentative dataset, it may generate outputs that are incomplete, inaccurate, or misleading. Therefore, it is crucial to be aware of the limitations of the model and to ensure that it is trained on high-quality data to generate accurate and personalized headlines."
144,"Our current framework does not explicitly consider the temporal order via which word senses have emerged. In particular, in the data collection step, we construct source-target token pairs for each word type by randomly sampling a target sense from its sense inventory. An alternative and more realistic approach would be to sort all senses of a word chronologically by their times of emergence in history, and use the model to incrementally predict each sense of a word based on usages of its older senses. However, we found that it is infeasible to find accurate timestamps of senses in natural corpora at a comprehensive scale. Another approach is to have human annotators evaluate the plausibility of each ground-truth source-target token pairs that are automatically created in our data collection pipeline, which is a potential area for future consideration."
145,"Although our work can effectively model the variability issue in dialogue, we acknowledge some limitations of our study. Firstly, our study can work well on the approaches based on RNN, but cannot be employed to sequence models based on Transformer, which limits the generality of our approach. The reasons we analyze are as follows.
Transformer is not a good architecture for finegrained diversity. The diversity of dialogue includes three granularities of discourse level, utterance level and word level. To model diversity, models will be required to utilize the representation at time t and the relationship between the representation at time t and time t+1 to determine the representation at time t+1. Relationships are computed step by step. If we only consider discourse-level diversity, our approach and variational mechanisms are easily transferable to Transformer architectures. Because we can use the Transformer model to encode the entire historical dialogue sequence. Latent variables or summarizing variables only exist between the entire historical sequence and the responses. This will not destroy the parallel structure of the Transformer. if we employ a Transformer to model diversity at the utterance and word granularity, this will seriously damage the parallelism of the Transformer.
There are great limitations in the variational transformer models. The transformer and variational thinking is not a good match, which leads to less relevant research. The Transformer baselines we compared in the manuscript (i.e. SVT, GVT, PLATO and DialogVED) cover most of the current transformer models that combine variations. Although SVT, GVT, PLATO and DialogVED incorporate variational ideas, these models connect
all the dialogue history utterances into a consecutive sequence. It is inadvisable to model the finegrained diversity relationship in a parallel structure.
Secondly, although our methods can improve the diversity and relevence of responses, there are still gaps in fluency compared with other baselines."
146,"Because decoding symbolism is a challenging new problem, our approach and experimental results have some limitations. First, our work builds on available resources, which may have a bias toward an English/Euro-centric perspective. Second, the evaluative datasets that we curated have a limited coverage of possible symbols even within the English literary tradition. Third, as mentioned in Section 3.1, our study on situated symbolism is limited to symbolic pairs that can be found in static visual advertisements rather than longer form text or videos. Finally, while we have proposed one debiasing method based on re-ranking with PMI, which worked well for our experimental setting, there may be other methods and metrics more suited to different settings. We believe that despite these limitations, our proposed evaluative framework and
methodology offers a good starting point for further exploration."
147,"sec 6
7 A2. Did you discuss any potential risks of your work? No user; no ethic concern"
148,"Since MPCHAT sources the data from Reddit, it has the limitation that it may not be representative of the general population. First, all subreddits of MPCHAT are primarily written in English, and a significant percentage of Reddit users are from English-speaking countries. The four countries with the highest desktop traffic on Reddit are the US, UK, New Zealand, and Australia, accounting for 66% of the total user (Clement, 2022). Moreover, compared to the average US population, Barthel et al. (2016) reported that Reddit users are more likely to be male (67% vs. 49%), young (64% 18-29 years old vs. 22%), college-educated (42% vs. 28%), and politically liberal (43% vs. 24%). Therefore, MPCHAT may reflect such somewhat narrow interests, and the demographic group represented by our model may be biased toward personal conversations suitable for it."
149,In the limitations section
150,"Our work has two limitations. The first is that it may not work well for some specific types of PET. For example Prompt-tuning, which is only added on the input layer. We cannot use CLNorm but only ILProj. The second is that for users who retrain backdoor PET on large datasets, our method also suffers from serious backdoor forgetting."
151,"In section Limitations
7 A2. Did you discuss any potential risks of your work? This work was conducted in accordance with ethical principles. We use the publicly available dataset for the experiments and have no potential risks about credentials or data privacy. No human participants are involved in our experiment. Therefore, we don’t foresee any potential risk of this work."
152,"In our work, we rely on a single Mixture-of-Experts NMT model which is NLLB-200. There is a risk that our conclusions may only hold for this particular model and are specific to the way this model was trained. We believe that our findings still can be of interest to any person willing to use the NLLB200 model because: (1) It was the only publiclyavailable MoE NMT model at the time of submission; (2) It is the only model covering 202 languages and reaching SoTA results for most of those languages.
Moreover, we did not try to finetune the pruned model, which could potentially improve the results (but requires a large number of GPUs) and therefore change some of our conclusions.
This work has similar risks as the original NLLB200 models regarding the misuse of potentially wrong translations. Note that, as observed by Mohammadshahi et al. (2022b), pruning could amplify the biases already present in the full model."
153,"This work only carries out experiments using English as the base training language for domain adversarial transfer. It is possible that domain adversarial transfer has a variable effect depending on the training language from which labeled data is used. Additionally, while typologically and regionally diverse, all but one language used in our evaluation is of Indo-European origin."
154,Section 8
155,"We would like to highlight a few limitations of our work. First, we would like to point out that GENEVA is designed to evaluate the generalizability of EAE models. Although the dataset contains event type and event trigger annotations, it can only be viewed as a partially-annotated dataset if end-to-end event extraction is considered. Second, GENEVA is derived from an existing dataset FrameNet. Despite human validation efforts, there is no guarantee that all possible events in the sentence are exhaustively annotated."
156,"Orthogonal to the speed-ups discussed in this work, Earley (1970) described an extension that we do not include here, which filters deduction items using k words of lookahead. (However, we do treat 1-word lookahead and left-corner parsing in App. G.2.)
While our deduction system runs in time proportional to the grammar size |G|, this size is measured only after unary and nullary productions have been eliminated from the grammar—which can increase the grammar size as discussed in Apps. E and F.
We described how to compute prefix weights only for EarleyFast, and we gave a prioritized execution scheme (App. H.3) only for EarleyFast. The versions for EarleyFSA should be similar.
Computing sentence weights (2) and prefix weights (3) involves a sum over infinitely many trees. In arbitrary semirings, there is no guarantee that such sums can be computed. Computing them requires summing geometric series and— more generally—finding minimal solutions to systems of polynomial equations. See discussion in App. A and App. F. Non-commutative semirings also present special challenges; see App. K."
157,"The theoretical results and the algorithm should be applicable for other knowledge integration models which encode target sentences and associated textual knowledge descriptions in mini-batches. However, this paper does not extensively apply the proposed method to various knowledge integration models to explore its efficiency and effectiveness."
158,"Sec 5
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
159,Limitations Section
160,Limitations
161,Limitation Section
162,section 7
163,"Application to Other Benchmarks A central limitation of our work is that the main experiments are based on a single task-oriented dialogue benchmark. While there are multiple other natural language understanding benchmarks like XNLI, XQUAD, MLQA, and PAWS-X (Conneau et al., 2018; Artetxe et al., 2020; Lewis et al., 2020; Yang et al., 2019) that can also be used to back up our claims, we argue that this is outside the scope of this paper. The main objectives of this paper are
to first come up with a new definition of a crosslingual continual learning challenge and then to give an example using a comprehensive and realistic benchmark like task-oriented dialogue to catalyze more research in that direction.
Choice of Realistic Permutations For more realistic setups of continual learning, we need to come up with an approach to define continual learning annotation scenarios of languages. Rather than using brute force with all possible ways the languages could be annotated at different stages, a principled way would be more desired. Since it is hard to tell if there is any logic or pattern in the annotation process itself and given the sheer amount of realistic scenarios, we chose one scenario experienced by some of the users: a model is built for a user, then the user reveals that more languages are desired. We test in our work the plausibility of continual learning approaches where the sequence moves from one language to another without repetition of the same language. Working on scenarios where the data from different languages are integrated as soon as they are annotated, implying different languages for different hops, is out of the scope of this paper.
Data and Model Size Analysis In this paper, we pick certain model expansion approach variations to analyze the effect of model components (one aspect of model size) and two data distribution scenarios. However, analyzing extensively the effect of the scale of data and model size is beyond the scope of our work. We agree that different data sizes can be used and it is interesting to analyze different supervision levels such as using different proportions of the data for each language and simulating few-shot scenarios. We believe that for lowresource scenarios we need to investigate specific approaches to continual learning like meta-learning. We plan to investigate that in future work.
Application to Other Transformers Another possible limitation of our work is the restriction of the evaluation to a base model on top of M-BERT Transformers. With the advent of Transformer-based encoders as strong pillars for transfer-learning, several Transformers such as XLM-R have been proposed more recently. Although those models have been shown to outperform M-BERT on numerous downstream applications especially on low-resource languages (Conneau et al., 2020), M-BERT is still largely used due
to its reduced number of parameters. In our specific continual learning challenge, efficiency is a top concern as we are training in multiple hops and benchmarking on different models. So, M-BERT has been feasible in our use case. We leave experimenting with other Transformer-based encoders to future work."
164,"We discuss the limitations in the limitations section after the conclusion. There are no strong assumptions, claims or biases we are aware of that are not stated in the paper. We state only claims that are supported by evidence and experimental setup that we design and describe clearly in the main paper and in the supplemental material (appendix and code) (more details on the experimental setup can be found in Appendix C).
A2. Did you discuss any potential risks of your work? Not applicable. To the best of our knowledge, there is no potential risk or harm of any kind that could result from this work. Our research is just an analysis paper about cross-lingual continuous learning where we share our experiments on pre-existing approaches."
165,"The ""Limitations"" section comes after Section 9 (we did not number it).
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
166,Section 8
167,"Although we believe that controlling the forgetting in the fine-tuning phase to avoid forgetting cross-lingual/general knowledge and to reduce the negative interference from misaligned pre-training tasks and downstream tasks can benefit other finetuning settings (e.g. Multi-task setting), we have not yet investigated these settings. In the future, we will try to propose a more general method for fine-tuning a large pre-trained model across various settings."
168,Limitation
169,Section 9 (Limitations)
170,"While we unify diverse IE tasks into token-pair classification tasks and propose a simple but useful architecture to help token pairs interact with each other in an effective way, there are still several limitations that are worth discussing. Firstly, all modules in our UTC-IE are based on pre-trained language models, and experiments proves that different PLMs may influence the performance on the same dataset. Hence, our model relies on the capability of the PLM, which need a lot of GPU resources to complete the experiments. Additionally, although incorporating PlusAttention instead of self-attention can effectively reduce the memory and computational complexity from O(L4) to O(2L3), it still require a little large computation. Future work can leverage the backbone of our unification and model, and focus on the acceleration on each module."
171,"The word embeddings and language models used in this work are trained on contemporary English language, and our social contexts overly contain explicit stereotypes encoded in English. Stereotypes for a specific group can be quite different depending on the language and culture. Although out of the scope of the present work, cross-societal differences in human stereotyping have been shown to be explainable using the SCM framework (Cuddy et al., 2009). Thus, it is fair to posit that our SCMbased framework generalizes to social group biases beyond those in English. Future research is encouraged to replicate our study in non-English languages.
Furthermore, we would like to point out that there exists a catalogue of bias measurements for
word embeddings and language models in the field. However, the current catalogue is far from comprehensive in covering social groups even in the contemporary English/American context, with few resources for the intersectionality of groups and attributes (Subramanian et al., 2021; Dhamala et al., 2021). Additionally, some of these measures have been shown to fail robustness checks. Although our current work uses some of the most recently developed ECT and EQT, we believe that few, if any, of these measurements are completely sound nor complete. In our experiments for language models, we tried to measure bias for the same social group or attribute using multiple benchmarks but still found some substantial differences in results across benchmarks. Therefore, we caution against interpreting low bias measurements as evidence of complete bias removal. While developing a new bias measurement scale is not within the scope of this work, we are optimistic that the social psychological theory in which our approach is grounded provides the bedrock for the current evidence of SCM efficacy to hold on future benchmarks.
Unlike bias mitigation methods for static word embeddings, such as partial projection, the post hoc methods of debiasing for large language models can’t be trivially applied to mitigate biases for multiple social attributes simultaneously. For DPCE, the formulation allows for mitigating biases on multiple social attributes, but collecting enough sentences from each attribute that do not include any words from other attributes or neutral words (i.e. mutually exclusive sentences) was not possible with the corpora we experimented with. This problem is exacerbated as the number of social attributes grow due to the mutual exclusivity condition. For ADEPT on the other hand, the formulation did not trivially handle multiple dimensions. Hence, we employed a coordinate-descent modification in our experiments to apply ADEPT to SCM (more info in §B.4). We encourage future work to devise data-efficient methods that can mitigate biases on multiple dimensions at the same time."
172,"Biases may be present in the data annotator as well as in the data the models were pretrained on. Furthermore, we only include English-language data in our benchmark and analysis. Recent work has noted that language models may be susceptible to learning such data biases (Lucy and Bamman, 2021), thus we request that the users be aware of potential issues in downstream use cases.
As described in Appendix D.1, we take measures to ensure a high quality benchmark. There will inevitably be noise in the dataset collection process, either in the ACU writing or matching step, and high agreement of annotations does not necessarily coincide with correctness. However, we believe that the steps taken to spot check ACU writing and filter workers for ACU matching allow us to curate a high-quality benchmark. Furthermore, we encourage the community to analyze and improve RoSE in the spirit of evolving, living benchmarks (Gehrmann et al., 2021).
For reference-based evaluation, questions about reference quality arise naturally. We also note that the original Pyramid protocol was designed for multi-reference evaluation and weighting of semantic content units, while we do not weight ACUs during aggregation. As discussed above, we argue that our benchmark and analysis are still valuable given the purpose of studying conditional generation and evaluating automatic metrics for semantic overlap in targeted evaluation. We view the collection of high-quality reference summaries as a valuable, orthogonal direction to this work, and we plan to explore ACU weighting in future work."
173,Section 8.
174,"Dungeons & Dragons is a very complex game to capture completely, and there are certain aspects that FIREBALL does not take into account. For example, FIREBALL’s scenarios are recorded independently of the overarching narrative context they take place in, do not record players’ inventory, and do not account for any movement or placement on a map. Our models are not able to play D&D autonomously - but doing so is not the goal. Instead, D&D models can be used to assist and inspire the humans playing.
Our models do not take into account the gener-
ation of profanity or sensitive topics; these were filtered out post-hoc. D&D is a game played by players of all ages that often contains violent or profane descriptions, and unfiltered generations may be unsuitable for young players. There are previous instances of roleplaying games that incorporate language models being used to generate sexual content3 that would require age restrictions and content warnings.
GPT-3 may be prohibitively expensive for everyday use; in our experiments, we were unable to use the full set of data we had available for fine-tuning due to budget constraints."
175,"We note several methodological limitations with our experiments. First, since the evaluation materials were manually crafted, there is a rather small number of items (compared to the size of automatically generated NLP benchmarks). Small evaluation sets can introduce issues of statistical power (Card et al., 2020) and introduce bias based on lexical items. We feel this is not a major concern, because (1) our materials are validated by expert researchers; (2) models can be directly compared to humans in Floyd et al.’s experiments; and (3) in practice, there is enough signal to distinguish between the tested models.
Second, we only evaluate models on Englishlanguage materials, and some of the tasks were designed based on norms of communication and social interaction in Western cultures. As pragmatics can vary widely across language and cultures (Li, 2012; Rubio-Fernandez and Jara-Ettinger, 2020; Floyd, 2021; Brown et al., 2021; Dideriksen et al., 2022), an important direction for future work is to evaluate pragmatics beyond English (Ameka and Terkourafi, 2019; Blasi et al., 2022).
Third, aside from the OpenAI API models, we were only able to test models with ≤11B parameters due to limited computational resources. Models with parameter sizes between 11B and the size of text-davinci-002 could exhibit qualitatively different behaviors.
Finally, we emphasize that it is impossible to predict how models will respond to an arbitrary input. Therefore, we caution against extrapolating from our results and expecting that models will behave “pragmatically” in downstream applications. This is especially true for models behind the OpenAI API,
and text-davinci-002 in particular, for which very little is publicly known about the training protocol."
176,"""Limitations"" section after Section 6"
177,Section Limitations
178,Limitations
179,"There are several limitations to the work presented in this paper that need to be acknowledged.
First, the SREDFM and REDFM datasets are based on Wikipedia and Wikidata, which means they may not cover all possible relation types or entities. In addition, the quality of the annotations in these datasets may be influenced by the biases and limitations of these sources.
Second, the Triplet Critic is trained on a small subset of the SREDFM dataset, which may limit its ability to generalize to other relation types or languages. Additionally, the performance of the Triplet Critic may be affected by the quality of the annotations used to train it.
Third, the authors of this work are native speakers of some of the languages tackled in this work and external native speakers created the annotation guidelines. However, for some of the automaticallyannotated languages, there were no native speakers involved. Additionally, the qualitative error analysis does not include Arabic or Chinese examples, as neither of the authors of the paper is proficient in those languages.
Finally, the mREBEL system is based on a Transformer architecture, which may not be optimal for all relation extraction tasks. It is possible that other types of model, such as graph neural networks or rule-based systems, could outperform mREBEL on certain relation types or languages.
Overall, the results presented in this paper should be interpreted in the context of these limitations. Further research is needed to address these limitations and to improve the performance of multilingual relation extraction systems."
180,"Aside from the still-improvable performance of the classification models we evaluated, our work is limited in two ways: the nature of what is considered appropriate as well as the difficulties that arise during corpus creation in NLP in general.
We point to the subjectivity in perception regarding appropriateness, which is also displayed and discussed in the paper by the inter-annotator agreement. Many sociocultural factors can influence this perception within cultures, such as age, gender, education, or ethnicity. We sought to account at least for gender by including both male and female annotators for all arguments. However, we encourage further studies that focus on other factors, as we expect appropriateness to be seen differently, primarily across cultures with varying styles of debates. Since our corpus contains only arguments written in English and is annotated by native English speakers, it may also be insufficient to generalize across languages.
Moreover, appropriateness perception is likely subject to change over time. Although we collected arguments from different years, we see long-time limitations to our corpus. In general, it also depends on the expectations of the discussion participants, which are to some extent predetermined by the context (e.g., a sales pitch vs. a discussion with friends). In that regard, the context of our corpus is solely that of discussing controversial issues with strangers on the web. Finally, the size of the created corpus we propose in the paper may limit the generalizability of approaches that build on it and should be investigated further in future work."
181,"While our method demonstrates strong performance in our experimental setups, potential issues may arise when the characteristics of the available unlabeled dataset drastically change. For one example, if the scale of the available dataset is too small, the effectiveness of our clustering-based data filtering may fall drastically, leading to poor performance. Or, if the dataset is highly unbalanced, our model cannot acquire information about several specific classes. One way to compensate for this shortcoming is to use an externally imported corpus or dataset, similar to other ZSL or WSL methods. Another drawback of CELDA is that the final performance is highly dependent on the performance of the initial pseudo label, as shown in ablation. Nevertheless, as demonstrated in our ablation studies, we can remedy this issue by labeling a few samples, like active learning."
182,"In this work, we conduct research on event commonsense of open-domain dialogue systems for the first time. While achieving higher correlations with human judgments than existing baselines, ACCENT has some limitations:
First, the ACCENT framework is based on a fixed set of event relations and the commonsense knowledge in ATOMIC2020 which may fail to cover some potential event commonsense aspects. We believe augmenting the current framework with more commonsense resources is a worthwhile direction for the further improvement of ACCENT.
Second, the event-relation extractor in ACCENT framework is a T5 model fine-tuned in a low resource setting. Although the current model can yield fairly strong performance, it is an important research direction to improve the joint event-relation extraction component because the extracted tuples serve as the symbolic representation for commonsense reasoning in ACCENT framework. Since human extracted tuples are very costly to collect, we hope to explore whether we can improve this component through high-quality synthetic data construction or transfer learning in the future."
183,"Sections 6.2, 6.4 and Appendix H include the error analysis of the ACCENT framework. A separate section of “Limitations” is also included in Appendix A.
7 A2. Did you discuss any potential risks of your work? We do not include potential risks of our work, since we believe that none of the components in our model and the dataset by itself can produce offensive results. The responses generated and augmented to DECO dataset are coming from previously proposed state-of-the-art models which are trained on datasets without profanity and inappropriate utterances. Other than that, our work is pertinent to evaluation and has less feasibility of potential risks."
184,"We notice a few key limitations of our approach. Similar to what was shown by previous interpretability studies (Camburu et al., 2018, i.a.), incorporating explanations comes with some penalty on in-distribution accuracy when there is no spurious cue. This penalty decreases as model size increases, potentially because it is less challenging for larger models to generate good explanations. The second limitation is that our artificially constructed training set may not reflect the strength of the studied spurious cues in the real world. In our main experiments, we focus on the case where one spurious cue is perfectly correlated with the target label. For further exploration, we can study the alternative setting where there are multiple weak spurious cues instead of a single strong one. Finally, our work here is limited by the scope of the experiments. We only experiment with generative LMs and binary classification tasks. Also, because of resource constraints, we only consider four datasets and eight types of spurious cues (including datasetindependent and dataset-specific ones). Additional experiments using a wider variety of spurious cues and datasets would help to shed light on how our method generalizes to other scenarios."
185,"Despite the success of our CAME optimizer in training large language models with memory efficiency, there are still some limitations that need to be addressed in the future.
Our proposed memory-efficient optimizer introduces additional computation costs for the nonnegative matrix factorization of the instability matrix in comparison with Adafactor. We observe, however, that the training time of CAME increases only slightly in our experiments. Beyond that, CAME exhibits minor performance degradation in large-batch training of the BERT-Large model versus LAMB, which allows for further improvement in the future. Meanwhile, it is possible to conduct further experiments on other models in other fields, such as Computer Vision and Reinforcement Learning, thereby exploring the effectiveness of CAME
training under more application scenarios. As a final point, it would be much more helpful to provide an in-depth theoretical analysis of CAME to improve comprehensiveness of the paper."
186,"Systematically exploring more prompts Our work uses CoT prompting structure inspired by Kojima et al. (2022). However, small variations to the prompt structure yield dramatically different results. We also do not explore how different CoT prompts affect stereotypes, focusing only on the SOTA “let’s think step by step.” While we qualitatively observe that ""faster"" prompts (think quickly, think fast, not think step by step) are less toxic,
comprehensive work on understanding and evaluating different zero-shot CoT’s for socially relevant tasks is an avenue for future work. For example, priming CoT generation with “Let’s think about how to answer the question in a way that avoids bias or stereotyping” may reduce biased outputs (Ganguli et al., 2023). We also do not explore bias in few-shot settings. Models are very sensitive to few-shot exemplars (Zhao et al., 2021; Perez et al., 2021); furthermore, exemplars trivialize intrinsic bias benchmarks, and are similar to finetuning (Akyürek et al., 2022). Carefully measuring bias in few-shot CoT with respect to these confounds is an avenue already explored by future work (Turpin et al., 2023).
Limitations of Bias Benchmarks Prior work has shown flaws in existing fairness benchmarks; measuring fairness is itself an open problem. Benchmarks often-time have differing conceptualizations of bias (Blodgett et al., 2021), leading to contradictory results (Delobelle et al., 2022; Cao et al., 2022; Goldfarb-Tarrant et al., 2021). We ran our analysis across 3 separate benchmarks, including an extrinsic evaluation of bias in question answering (Parrish et al., 2022). We also conduct a manual, qualitative analysis of failures to tie our quantitative findings to examples of representational harm against protected groups. We believe the general agreement across our analyses mitigates the flaws of each individual benchmark, but the limitations and stated goals of each should be carefully considered when interpreting results."
187,Section 8
188,"Section 4.4, Section 5, Limitations"
189,"Our method cannot discover entities that are mentioned without disappearing contexts since we utilize such disappearance signals.
Although our experiments focused solely on Wikipedia entities, they did not sufficiently cover certain entities, such as stores and food products, for which disappearance is important for us. Therefore, it is necessary to collect and conduct experiments specifically for these entities."
190,"the section following the conclusions.
7 A2. Did you discuss any potential risks of your work? not any known risks."
191,"We discuss the limitations in the ""Limitations"" Section before ""Ethics Statement"" Section."
192,Limitations
193,"One limitation of our study is that we only evaluated our method on the T5 architecture. Further experiments on other architectures could be useful to determine the generalizability of our findings. Additionally, as in previous SOTA, our model also did not produce better results for the hotel domain, even though it did improve performance in general. We have attempted to explain why this domain is more difficult, but more research is needed to fully understand the reasons for this variability and to create methods that can improve performance across all domains."
194,"Limitations (Section 8)
7 A2. Did you discuss any potential risks of your work? It is unlikely that research on zero-shot dialogue state tracking poses any foreseeable risks."
195,"More Complex Domains. We choose the BW for its intuitive and simplistic nature (with one kind of object, three types of actions, and three predicates). Although the generalization experiments suffice currently to challenge transformers, realworld situations are more complicated. With the improvement of the algorithms, the need for a better arrangement of actions domains is emerging. In time, it could be beneficial to include several domains with various levels of complexities.
Balance Between Rigor and Natural. For now, the synthesized English sentences are generated using a fixed template. Whilst being accurate without ambiguity, the resulting text is still quite formal. It would be valuable to add variety in the expressions without losing precision.
Better Solvers. As our demonstrations suggest, current LMs still fall short on the generalization tests. We hope that our work will pique interests in the community towards reasoning about actions and change, and challenge approaches to undertake the fundamental reasoning tasks."
196,"the ""limitations"" section after ""conclusion"" section"
197,"Keeping narration coherent within a movie is crucial for visually impaired people to enjoy the movie. In this work, we move a step forward for this target by setting the ground-truth texts in the Movie Clip Narrating task as narration paragraphs and providing longer video clips as inputs. However, how to ensure description coherence across different clips within a movie has not been studied in this work. This requires a higher-level comprehending ability of models to process the whole movie and connect different plots. We leave this to our future investigation."
198,"Limitations are presented in a standalone Limitation section (after the Conclusion and before the References).
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
199,In Section 6
200,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
201,"""Limitations"" section.
A2. Did you discuss any potential risks of your work? Not applicable. This paper aims to identify risks."
202,"Although our methodology is agnostic to the specific sequence-processing models used as encoder and decoder, its main purpose is to be used together with LPLM. This clearly limits its use to institutions and users with access to computing facilities able to handle such models. Indeed, although the encoder of HSN can successfully be trained while keeping the weights of BERT (or any other LPLM) frozen, a feature which reduces training time, the GPT-based decoder does need to be fine-tuned to learn how to interpret the inferred symbols. This last points further limits the use of our methodology to “not-so-large” decoder models. That being said, fine-tuning HSN decoders via LoRA (Hu et al., 2022a) is an exciting option that can possibly solve (or help with) this last issue. We shall explore using LoRA with HSN in the near future."
203,Limitations
204,"The Goldstein scale is a widely-used measure of the conflictual versus cooperative nature of interactions between countries (Goldstein, 1992). The scale was created by a panel of international relations experts who ranked descriptions of interactions. It was initially created to score action categories in the WEIS event ontology (McClelland, 1984) and was later adapted to CAMEO (Schrodt, 2012).
The Goldstein scale applies only to the action category of an event (e.g., “fight” or “trade”). Thus, two different “fight” events, which might involve two different pairs of actors, occur at different times, or differ dramatically with respect to the number of associated fatalities, will still be assigned the same Goldstein value.The Goldstein scale is thus a poor measure of a conflict’s perceived “intensity”, as it ignores much of the information that contributes to that perception. Recent work in conflict studies, for instance, operationalizes “intensity” primarily using casualty counts (Chaudoin et al., 2017; Zhong et al., 2023), which the Goldstein scale ignores entirely.
In Tab. 1, we show the empirical distribution of assigned Goldstein values alongside the empirical distribution of casualty counts in a dataset of conflict events. The Goldstein scale is very coarsegrained; while it ostensibly ranges between −10.0 and +10.0, only a small number of discrete values ever occur, with many action categories assigned the same value. For the purpose of measuring conflict intensity, a finer-grained and more contextual scale is desirable."
205,Limitations
206,"This study is restricted to the Americas. Therefore the results from this paper can not be generalized, as different indigenous communities or nations might have different pasts. Also, all opinions expressed by the interviewed people are exclusively personal and in should not be interpreted as the general stand of the communities. As discussed in the paper, the main aim of this work is
not to provide a normative for MT researchers. We rather provide a set of questions and open topics that should be considered when performing MT work with indigenous languages. Nevertheless, we also provide general and broad non-normative recommendations that should be carefully applied to the concrete case of each community."
207,"This benchmark compiled and analyzed existing resources collected from diverse methods and domains. Although we demonstrated how careful use of these resources could transfer well to other resources, along with a manual analysis of a varied set of corpora, we cannot guarantee the quality of each resource or validate the methods that the original authors used to create them. We explore each dataset’s linguistic properties in Appendix B. However, we encourage a deeper exploration of the quality of individual resources by researchers that speak the 12 languages included in this benchmark and corresponding data loaders.
Additionally, the human evaluation performed in this study was limited in scope and served primarily to validate the findings by automatic metrics. A more extensive evaluation with more annotators evaluating more sentences would be beneficial in order to draw further conclusions.
Furthermore, some of the resources discussed in this paper were automatically aligned. Although Neural CRF models in English have been shown to yield high-quality alignments (Jiang et al., 2020), other alignment algorithms such as TF-IDF scoring (Nelken and Shieber, 2006) have been shown to result in a high number of false positives (Xu et al., 2015). Future work could include realigning automatically aligned corpora using an embeddingbased sentence alignment model trained on manually annotated alignment data (Jiang et al., 2020). We will continue updating this benchmark as updates are made to the underlying datasets, and new
multilingual resources are released."
208,"Section 9; Page 9
A2. Did you discuss any potential risks of your work? Not applicable. The primary contribution of our work is a benchmark for multilingual text simplification consisting of parallel complex-simple sentences. We provide a collection of existing resources for text simplification and empirical analysis of their utility in fine-tuning and few-shot experiments. We do not release any models, and our work is a collection of existing datasets. In effect, we are not releasing any potentially harmful models or resources that merit a discussion of risk."
209,"The section after conclusion, following the instruction in the Latex file.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
210,"DP training of large models is compute-intensive, requiring per-example gradients and large batch sizes (Li et al., 2021; Subramani et al., 2021). This renders the training of such models difficult and not easily accessible to everyone.
DP-SGD takes records to be single training examples, which in this paper’s experiments
9
correspond to single user utterances. That setup prevents the trained model from revealing much information about any given single utterance, but it may still allow information to leak that is repeated across multiple utterances (Brown et al., 2022).
For both the baseline method and our two-stage method, we trained our model to approximately match the true distribution of private user utterances, ppriv(x), to the extent that this was possible under a differential privacy guarantee. Of course, there are many ways to measure the quality of an approximation, and different approximations are appropriate for different tasks where it might be important to preserve different properties of ppriv(x). The one-stage baseline approach implicitly aims to achieve a low cross-entropy, by applying DP-SGD to the log-likelihood function. In contrast, our two-stage approach aims to encourage an approximation that also roughly preserves the marginal distribution over semantic function types. We did not investigate more direct ways of encouraging such an approximation, for example, one-stage DP-SGD with a modified objective function that explicitly evaluates the marginal distribution in addition to the log-likelihood.
Finally, we trained an approximate model of ppriv from which we can draw utterances to inspect and annotate. But we must acknowledge that ppriv is not the ideal distribution to approximate. Even if we were able to actually use private utterances to improve the system, we would not necessarily want to draw them directly from ppriv. Rather, we would want to select them by active learning—selecting the private user utterances that would be most useful to inspect or to include in the annotated training data. Thus, when training our model by DP-SGD (using either the one-stage or two-stage procedure), we could upweight or upselect the private utterances that appear useful in this way—resulting in a differentially private model that generates useful synthetic utterances. Specifically, traditional active learning by uncertainty sampling (Settles, 2012) would select utterances where the semantic parser was uncertain what to do. We would also want to select utterances where the system suspected for other reasons that it did not do the right thing—because it classified the user’s request as a functionality that the system did not yet support, or because the user objected in some way to the system’s response. We
have left experiments on this setup to future work."
211,In Limitaion Section
212,"In this paper, we propose a continued pre-training method to inject knowledge into large pre-trained language models. There are eight V100 GPUs involved in each pre-training experiment and the whole pre-training process takes 5 days for the base-size model and 13 days for the large-size model, in primary settings. These numbers in data upscaling settings are significantly greater (30 days for the large-size model). Despite its advantage in reducing resource need in inference time, KILM is both time-consuming and computationally resource-consuming during training time.
Similar to any model-based generation system, KILM could be prone to generating factually incorrect statements with regard to entities. These statements might also be prone to be biased based on ethnicity, race, and sexual orientation."
213,"We point out potential limitations and ethical concerns of this work. Limitation: Data and Modeling The dialogues
in our dataset are made by playwright, which are slightly different from daily chat. Second, the automatic evaluation metrics for the response generation task can not perfectly reflect the interactiveness of dialogue system. Lastly, Our autoregressive generative model simply add the segment embedding to the inputs. Similar to the position encoding in transformer, our coarse method does not make good use of the segmentation, and lacks interpretability. Ethics: Copyright and Licensing The data source are publicly available TV series. Its collection and annotation procedure is designed for videogrouned dialogue understanding and generation purpose, and does not involve privacy issues. Following LSMDC (Rohrbach et al., 2016) and MovieNet (Huang et al., 2020), we will polish an agreement and release TV shows content under very strict conditions but will open-source all the scrawling code, pretrained features and sampled images."
214,"While we hope that our approach to data collection can serve as a benchmark for future NLP studies beyond peer review, we deem it equally important to explicitly outline the potential risks and limitations of NLPEER and NLP for peer review in general. Our discussion below encourages future research in ethics and applied NLP for peer review; many of our considerations are not specific to peer review and are equally relevant to the applications of NLP in general.
From the data perspective, we deem it important to clearly state what NLPEER is not meant for. Our data collection campaigns for ARR-22 and COLING-20 included an explicit disclaimer on the risks of author profiling on the peer reviewing data; we stress that such applications violate the intended use of NLPEER. Furthermore, NLPEER enables a wide range of new NLP assistance tasks for peer review. Yet, we encourage future studies of NLP for peer review to reflect carefully about the potential risks and benefits of new task defi-
13https://intertext.ukp-lab.de/
nitions atop of peer reviewing data in general and NLPEER specifically. For instance, the full automation of peer review, i.e. the generation of review reports given a paper, bears risks and dual uses.
Considering diversity in NLP datasets, we stress that even NLPEER only covers a fraction of peer reviewing across all fields of science, and more data needs to be collected to enable fully representative NLP-based study of peer review. Due to the genre standards of scientific publishing our dataset only covers papers and reviews in English language. Multilingual scholarly document processing is overall poorly represented in NLP, and constitutes a promising avenue for future research. While our resource contains data from a wide range of domains, research in arts and humanities is under-represented due to the poor data availability. The trend towards open science and the adoption of responsible data collection practices (Dycke et al., 2022) might bring reviewing data from previously unexplored domains and languages into NLP. We stress that any direct comparison based on our corpus would need to take into account reviewing practices and guidelines adopted by the respective communities. Specifically, potential biases resulting from the donation-based collection for ARR-22 and COLING-20 should be taken into account.
From the task side, we highlight that implementations and resulting models presented here are meant to exemplify the proposed tasks, determine their technical feasibility, and serve as a starting point for developing future NLP for peer review assistance systems. As such, the provided implementations have limitations: for example, sentence-level pragmatic labels derived from structure-based ARR forms might contain noise since ARR forms group text on section level; guided skimming does not make use of implicit links, and explicit links are mostly based on line numbers and quotes, limiting the recall. Since we did not perform extensive hyperparameter search and tuning of the models, our results should not be interpreted as a claim towards superiority of a particular model, approach or reviewing system.
We highlight that high intrinsic task performance does not necessarily translate into the extrinsic utility of NLP support in real-world reviewing environments. We thus deem it crucial to study the factors that affect the success of NLP assistance for peer reviewing. This includes the study of the humanmachine interaction dynamics and its desiderata;
for example, review score recommendations should be accompanied by explanations. We encourage extensive research on risks of biases and errors in NLP assistance models; for instance, a review score prediction model might learn undesirable biases against certain types of papers. Review writing assistance implemented in a real reviewing system should always be accompanied by carefully designed guidelines and policies.
Finally, we invite the community to reflect on the potential societal consequences of the individual NLP assistance tasks, even if NLP models accomplish them well. To provide an example, our newly introduced guided skimming task assists during the effortful and time-intensive, yet crucial step of reading the paper under review. Although the guided skimming for peer review task models an intermediate step during reading and is intended to serve as an additional point of reference during the iterated skimming steps of peer review, such a technology might encourage reviewers to read only the paragraphs suggested by the model. We argue that this risk of ""lazy reading"" is independent of the technology at hand; a reviewer that is institutionally incentivized to perform reviews as quick as possible, may read a paper superficially and settle with heuristics for their assessment (Rogers and Augenstein, 2020) regardless of assistance. A greater risk, however, may be imposed by potential biases and errors of a guided skimming model, which could distract less experienced reviewers. While recent work on skimming assistance in scholarly articles (Fok et al., 2023) suggests a mature and reflected interaction of users with highlight recommendations and possible errors, this needs a specific investigation for the use case during peer review. On the other hand, a critical reading model may serve as a useful point of reference to guide reviewers to employ more scrutiny on the parts of the paper appropriate for this specific paper type, which ultimately may improve reviewing quality. We assess that the opportunities provided by the introduced review assistance tasks outweigh the potential risks in general, yet highlight that a targeted study is necessary to substantiate this assessment.
References 2008–2023. Grobid. https://github.com/ kermitt2/grobid.
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: A pretrained language model for scientific text. In
Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615–3620.
Lutz Bornmann. 2011. Scientific peer review. Annual Review of Information Science and Technology, 45(1):197–245.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.
Liying Cheng, Lidong Bing, Qian Yu, Wei Lu, and Luo Si. 2020. APE: Argument pair extraction from peer review and rebuttal via multi-task learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7000–7011, Online. Association for Computational Linguistics.
Alexandra Chronopoulou, Matthew Peters, and Jesse Dodge. 2022. Efficient hierarchical domain adaptation for pretrained language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1336–1351, Seattle, United States. Association for Computational Linguistics.
Nils Dycke, Ilia Kuznetsov, and Iryna Gurevych. 2022. Yes-yes-yes: Proactive data collection for ACL rolling review and beyond. In Findings of the Association for Computational Linguistics: Empirical Methods in Natural Language Processing 2022, pages 300–318, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
Nils Dycke, Edwin Simpson, Ilia Kuznetsov, and Iryna Gurevych. 2021. Assisting decision making in scholarly peer review: A preference learning perspective. arXiv:2109.01190.
Raymond Fok, Hita Kambhamettu, Luca Soldaini, Jonathan Bragg, Kyle Lo, Marti Hearst, Andrew Head, and Daniel S Weld. 2023. Scim: Intelligent skimming support for scientific papers. In Proceedings of the 28th International Conference on Intelligent User Interfaces, IUI ’23, page 476–490, New York, NY, USA. Association for Computing Machinery.
Tirthankar Ghosal, Sandeep Kumar, Prabhat Kumar Bharti, and Asif Ekbal. 2022a. Peer review analyze: A novel benchmark resource for computational analysis of peer reviews. PLOS ONE, 17(1):1–29.
Tirthankar Ghosal, Kamal Kaushik Varanasi, and Valia Kordoni. 2022b. HedgePeer: A dataset for uncertainty detection in peer reviews. In Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries, JCDL ’22, pages 1–5, New York, NY, USA. Association for Computing Machinery.
Tirthankar Ghosal, Rajeev Verma, Asif Ekbal, and Pushpak Bhattacharyya. 2019. DeepSentiPeer: Harnessing Sentiment in Review Texts to Recommend Peer Review Decisions. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1120–1130, Florence, Italy. Association for Computational Linguistics.
Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer. 2022. DEMix layers: Disentangling domains for modular language modeling. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5557–5576, Seattle, United States. Association for Computational Linguistics.
Xinyu Hua, Mitko Nikolov, Nikhil Badugu, and Lu Wang. 2019. Argument mining for understanding peer reviews. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2131–2137, Minneapolis, Minnesota. Association for Computational Linguistics.
Rob Johnson, Anthony Watkinson, and Michael Mabe. 2018. The STM Report: An overview of scientific and scholarly publishing. International Association of Scientific, Technical and Medical Publishers, The Hague, Netherlands.
Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine van Zuylen, Sebastian Kohlmeier, Eduard Hovy, and Roy Schwartz. 2018. A dataset of peer reviews (PeerRead): Collection, insights and NLP applications. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1647–1661, New Orleans, Louisiana. Association for Computational Linguistics.
Neha Kennard, Tim O’Gorman, Rajarshi Das, Akshay Sharma, Chhandak Bagchi, Matthew Clinton, Pranay Kumar Yelugam, Hamed Zamani, and Andrew McCallum. 2022. DISAPERE: A dataset for discourse structure in peer review discussions. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1234–1249, Seattle, United States. Association for Computational Linguistics.
Ilia Kuznetsov, Jan Buchmann, Max Eichler, and Iryna Gurevych. 2022. Revise and Resubmit: An Intertextual Model of Text-based Collaboration in Peer Review. Computational Linguistics, 48(4):949–986.
Carole J. Lee, Cassidy R. Sugimoto, Guo Zhang, and Blaise Cronin. 2013. Bias in peer review. Journal of the American Society for Information Science and Technology, 64(1):2–17.
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.
2020. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234–1240.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.
Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. 2021. On the stability of fine-tuning BERT: misconceptions, explanations, and strong baselines. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
Barbara Plank. 2016. What to do about non-standard (or non-canonical) language in NLP. arXiv:1608.07836.
Simon Price and Peter A Flach. 2017. Computational support for academic peer review: A perspective from artificial intelligence. Communications of the Association for Computing Machinery (ACM), 60(3):70–79.
Chenglei Qin and Chengzhi Zhang. 2022. Which structure of academic articles do referees pay more attention to?: perspective of peer review and full-text of academic articles. CoRR, abs/2209.01841.
Anna Rogers and Isabelle Augenstein. 2020. What can we do to improve peer review in NLP? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1256–1262, Online. ACL.
Nihar B Shah. 2019. Principled Methods to Improve Peer Review. Available online at http://www. cs.cmu.edu/~nihars/publications/ survey_peerreview_niharshah.pdf (accessed on 22.01.2022).
Chenhui Shen, Liying Cheng, Ran Zhou, Lidong Bing, Yang You, and Luo Si. 2022. MReD: A meta-review dataset for structure-controllable text generation. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2521–2535, Dublin, Ireland. Association for Computational Linguistics.
Lukas Stappen, Georgios Rizos, Madina Hasan, Thomas Hain, and Björn W. Schuller. 2020. Uncertaintyaware machine support for paper reviewing on the interspeech 2019 submission corpus. In Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020, pages 1808– 1812. ISCA.
Ivan Stelmakh, Nihar B Shah, Aarti Singh, and Hal Daumé III. 2021. Prior and prejudice: The novice reviewers’ bias against resubmissions in conference peer review. Proceedings of the Association for Computing Machinery (ACM) on Human-Computer Interaction, 5:1–17.
Andrew Tomkins, Min Zhang, and William D. Heavlin. 2017. Reviewer bias in single- versus double-blind peer review. Proceedings of the National Academy of Sciences, 114(48):12708–12713.
Richard Walker and Pascal Rocha da Silva. 2015. Emerging trends in peer review—a survey. Frontiers in Neuroscience, 9.
Jingyan Wang and Nihar B. Shah. 2019. Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’19, Montreal, QC, Canada, May 13-17, 2019, pages 864–872. International Foundation for Autonomous Agents and Multiagent Systems.
Weizhe Yuan, Pengfei Liu, and Graham Neubig. 2022. Can we automate scientific reviewing? Journal of Artificial Intelligence Research, 75:171–212.
Rongsheng Zhang, Yinhe Zheng, Xiaoxi Mao, and Minlie Huang. 2021. Unsupervised domain adaptation with adapter. arXiv:2111.00667."
215,"In this paper, we introduce Z-Code++, a robust pre-trained model tailored for summarization tasks. However, it should be noted that there are certain limitations to our model. Firstly, the model is not versatile enough as it is specifically designed for summarization. It is unclear whether it performs well on other natural language tasks. Secondly, while FiE can handle document summarization, there are still significant potential for improving cost efficiency. Lastly, the evaluation of multilingual summarization is not thorough enough due to the limitations of available datasets. We intend to address these limitations in our future work."
216,"Refer to Section ""Limitations"" at the end of paper.
7 A2. Did you discuss any potential risks of your work? This paper mainly focuses on improving performance on downstream tasks with external knowledge. We have not found risks of the potential use cases."
217,"A major component of RSMI has been developed with the concept of randomized smoothing which is known to be certifiably robust within a radius of a ball around an input point. Though we have proved the robustness for the perturbed samples within this given ball, there is no theoretical guarantee that a perturbed sample will always lie within the ball. Accordingly, our study is limited to empirical validation of the effectiveness of RSMI, although it has theoretical robustness within a L2 norm ball as shown in §2. Nevertheless, certified robustness is a critical research direction for robust and reliable deployment of NLP systems to address undiscovered attacks. In our future work, we will explore the theoretical understanding of the certified-robustness of NLP systems and textual adversarial examples in-depth."
218,"Despite introducing meta learning for domain adaptive few-shot misinformation detection, we have not discussed the setting of cross-domain adaptation with multiple source datasets to further improve the performance for identifying early-stage misinformation. Due to the lack of early-stage misinformation data, we limit our choice of the target domain to COVID-19, which may hinder the generalization of the proposed method to other domains. Additionally, the proposed method does not leverage efficient approximation or first-order meta learning methods to reduce the computational costs in training MetaAdapt. As such, we plan to explore multi-source few-shot misinformation detection via efficient meta learning as future work."
219,7 Limitations
220,Section 6
221,"In the Section Limitations, we discuss the limitations of our work."
222,"Although our approach, MIME is empirically observed to outperform several other competitive baselines, we do observe some limitations in the modeling capacity towards MEMEX. As depicted in Table 6, there are three possible scenarios of ineffective detection – (a) no predictions, (b) partial match, and (c) incorrect predictions. The key challenges stem from the limitations in modeling the complex level of abstractions that a meme exhibits. These are primarily encountered in either of the following potential scenarios: • A critical, yet a cryptic piece of information
within memes, comes from the visuals, which typically requires some systematic integration of factual knowledge, that currently lacks in MIME.
• Insufficient textual cues pose challenges for MIME, for learning the required contextual associativity. • Potentially spurious pieces of evidence being picked up due to the lexical biasing within the related context."
223,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
224,"Computing Resources. Despite the surprising performance it achieves, our framework needs to be applied to large language models like GPT3 or PaLM. Inference with these models costs more time and budgets than fine-tuning models like RoBERTa (Liu et al., 2019).
Faithfulness. Although DIVERSE can significantly improve the accuracy of final answers, we still cannot guarantee that the reasoning paths produced by the language models are 100 percent faithful. This is the key challenge and future direction for this line of research (chain-of-thought reasoning).
More Training Data. DIVERSE needs more labeled data with well-annotated reasoning paths to construct diverse prompts, and it also needs a training dataset for supervising the verifier. However, from another point of view, this limitation can also be regarded as a contribution that studies how chain-of-thought reasoning can be further improved if we have more training data than just a few exemplars.
Human Evaluation of Reasoning Steps. We use human evaluation to measure the quality of the intermediate steps in reasoning paths since few current works provide reliable frameworks to evaluate the quality of reasoning steps."
225,"Section 9
7 A2. Did you discuss any potential risks of your work? Left blank."
226,"In this study, we provide a survey of reasoning with language model prompting. We discuss the related surveys in Appendix A.1 and will continue adding more related approaches with more detailed analysis. Despite our best efforts, there may be still some limitations that remain in this paper.
References & Methods. Due to the page limit, we may miss some important references and cannot afford all the technical details. We mainly review the cutting-edge methods within two years (mostly in 2022) in §3, mainly from the ACL, EMNLP, NAACL, NeurIPS, ICLR, arXiv, etc., and we will continue to pay attention to and supplement the latest works.
Benchmarks. Most of the reasoning benchmarks mentioned in §5 are gathered and categorized from the experimental part of mainstream works. The definition and boundary of each task may not be accurate enough. Besides, our work may miss some kind of reasoning tasks such as reasoning with generics (Allaway et al., 2022), default inheritance reasoning (Brewka, 1987), non-monotonic reasoning (Ginsberg, 1987) in NLP, and will try our best to fulfill this gap.
Empirical Conclusions. We give detailed comparisons and discussions of language models and prompts in §4, and list some promising future directions in §6. All the conclusions are proposed and further speculated upon empirical analysis of existing works which may not be macroscopic enough. As the field evolves faster, we will update the latest opinions timely."
227,"Section 8 (Limitations)
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
228,"Last section (""Limitations"", no number)
A2. Did you discuss any potential risks of your work? Not applicable. Left blank.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 4
3 B1. Did you cite the creators of artifacts you used? 4
3 B2. Did you discuss the license or terms for use and / or distribution of any artifacts? 4"
229,"Section 6 Limitations
A2. Did you discuss any potential risks of your work? Not applicable. We currently do not identify any potential risks inherently associated with our work."
230,"Though CACR shows significant gains in compositional performance, results are limited in their exploration of only one pre-trained model and compositionality dataset. A significant risk of models is their tendency to be biased by distributions in their training data; vision-language models are
not free from this flaw, but we see our work as teaching VLMs to learn better structured representations rather than memorizing spurious correlations in data. We remain far from solving the vision-language compositionality problem, so biases must continue to be actively mitigated."
231,"limitations, 4.1 Datasets and 4.4 Evaluations."
232,"The experimental methodology employed in this study for both contrastive explanations and NMT is not directly extensible to languages other than English, due to the scarcity of resources such as models and annotations.
The datasets employed in this study to evaluate contrastive explanations across various linguistic paradigms are restricted to sentences that possess a well-defined structure. As a result, it is possible that the conclusions drawn may not be generalizable to the broader distribution of sentences.
Lastly, it should be noted that the method proposed in this study should not be used as a definitive explanation of model predictions in any other context. It is recommended to use the method as a debugging tool and should be employed in conjunction with other methods to gain a comprehensive understanding of model predictions."
233,"While we have argued that our approach to collecting counterfactual data via DISCO is agnostic to the particular task and language, we emphasize that the experiments we report are limited to English and the task of NLI. Given that English is a high-resource language, there could be additional challenges (e.g., finding the tools needed for making span selection) in re-creating our pipeline for other languages. We also emphasize that our data generation experiments were carried out using only a single LLM, namely the publicly available GPT3
model first reported in Brown et al. (2020). As with the related studies we cite (e.g., Liu et al. (2022)), given the high costs associated with largescale prompting, we are unable to ablate all parts of our data generation pipeline (e.g., the effect of systematically alternating prompting styles at scale, alternative span extraction techniques). Similar to virtually all experiments involving LLM prompting, such differences could affect the results and quality of the resulting augmentation datasets. Similarly, given the high costs of human annotation, we have limited our human evaluation to around 500 random instances (each involving 3 annotators), which follows other related studies."
234,
235,"Section ""Limitations"""
236,"The present study has certain limitations that should be acknowledged. Firstly, the RST parsing task itself is known to be highly complex and challenging, and achieving high accuracy in this task is not guaranteed. Although we have utilized the most high-performing parser, there is still room for further improvement in the RST parsing performance, which could potentially enhance the downstream summarization task.
Another limitation pertains to the size of the data used for human evaluation. Due to the nature of long document summarization and the length of
the original texts (often spanning several pages), scaling up the evaluation process, such as through crowd-sourcing, becomes difficult. Consequently, we are only able to evaluate a limited number of 10 documents, which may not be fully representative of the entire dataset.
Furthermore, another potential risk in our study is the limitation in obtaining an unlimited number of training samples. The data samples investigated are often small subsets of real-world data or may exhibit certain biases, which may not accurately reflect the distribution of real-world data. Although we have verified the effectiveness of our model using highly diverse and heterogeneous datasets from different domains, it is important to note that the model’s performance on the specific dataset of interest may not be as robust as its performance on unseen real-world data.
Finally, both training and evaluating the models require significant computational resources. Despite our attempts to optimize the computation by replacing the original attention calculation with the RST attention tensor (as demonstrated in the ablation experiment), we have not achieved satisfactory results. The high computational costs pose a challenge, as they result in increased human and material resources required for the model."
237,Section 7
238,"In an unnumbered section after Section 7
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
239,"1. Computational cost: For our experiments, we used almost 850h of GPUs. In future research, we could try to lower this cost by experimenting with prompting for LRC task in few-shot scenarios, which would also help when conducting the task for low-researched languages.
2. Language: Our experiments were conducted just for the English language. Thus, and with the advantage derived from minimal prompting of being language independent, in further research we would like to expand our experiments to multilingual datasets such as the ones from (Wachowiak et al., 2020).
3. Original dataset limitations: In line with (Lang et al., 2021), we found some misleading annotations in CogALexV dataset. This not only decrease the performance of the model but can also lead to hard-to-detect biases. Once again, few-shot tuning would decrease the annotation cost, making it possible to train with, although less, better-annotated examples. Additionally, synonymy remains the most difficult relation to capture, a more fine-graded annotation of the different kinds of synonyms could improve their classification.
4. Domain dependence: The limitation spotted by (Necsulescu et al., 2015) is persistent in our model. A richer domain annotation would be advised to better research domain bias in the LRC task.
Aknowledgements
Supported by the Spanish project PID2020113903RB-I00 (AEI/FEDER, UE), by DGA/FEDER, by the Agencia Estatal de Investigación of the Spanish Ministry of Economy and Competitiveness and the European Social Fund through the “Ramón y Cajal” program (RYC2019-028112-I), and by the EU research and innovation program HORIZON Europe 2021 through the “4D PICTURE"" project under grant agreement 101057332."
240,page 9 Section Limitations
241,"Although our approach has demonstrated advantages in producing faithful factual error corrections, we recognize that our approach is not capable of correcting all errors, particularly those that require domain-specific knowledge, as illustrated in Table 3. Therefore, it is important to exercise caution when applying this framework in user-facing settings. For instance, end users should be made aware that not all factual errors may be corrected.
In addition, our approach assumes evidence is given. Although this assumption is also true for applying our method to summarization tasks since the source document is treated as evidence, it does not hold for automatic textual knowledge base updates. When updating these knowledge bases, it is often required to retrieve relevant evidence from external sources. Hence, a reliable retrieval system is required when applying our method to this task."
242,Section 8.
243,"The event schemas generated by our model are not directly comparable to those generated by previous work that utilized a close-domain ontology. As a result, we were unable to adopt the same metrics and
evaluate our schemas on type-level event prediction tasks as in (Li et al., 2021; Jin et al., 2022; Dror et al., 2022). Grounding the events generated by the LLM into one of the types in the ontology could be added as a post-processing step to our model, but this would require some ontology-specific training data, which goes against our principles of designing an open-domain, portable framework.
Our event schema does not explicitly represent entity coreference, entity relations, and entity attributes. The current schemas that we produce focus on events and their relations, with entity information captured implicitly through the event descriptions. For instance, the See Medical Professional event is described as “The patient is seen by a doctor or other medical professional” and the proceeding Obtain Medical History event is described as “The medical professional obtains a medical history from the patient”. The “medical professional” and “patient” are implied to be coreferential entities in this case, but not explicitly connected in the schema graph.
Our approach is also quite distinct from prior work (Rudinger et al., 2015b; Wang et al., 2017; Li et al., 2021; Jin et al., 2022) that consider a probabilistic model as an implicit schema where the schema graph, or event narrative chain can be sampled from. Probabilistic schema models have the advantage of being adaptive and can be conditioned on partially observed event sequences, but are hard to interpret. We make the conscious design decision to generate explicit, human-readable schema graphs instead of black-box schema models.
Finally, our model relies on the usage of LMs, which have been observed to sometimes show inconsistent behavior between different runs or when using different prompts with the same meaning (Elazar et al., 2021; Zhou et al., 2022a). However, quantification of consistency has only been done for factual probing tasks while schema generation is a more open-ended task. For example, in our experiments on everyday scenarios, we observe that the model could generate distinct schemas for Buying a (computer) mouse based on whether the purchase was done online or in person. This variance is often benign and we leave it to future work to take advantage of such variance and possibly aggregate results over multiple runs."
244,"Section 7
7 A2. Did you discuss any potential risks of your work? Our model produces human-readable graphs that describe typical sequences of events, which is a form of event-related commonsense knowledge."
245,"The primary weakness of ‘fairly’ averaging model weights for XLT is that sensible checkpoints need to be averaged. This manifests, for instance, in hyperparameter ablation for ZS-XLT on TyDiQAGoldP. TyDiQA-GoldP is a complex task with merely 3,696 training instances that observes unusual training dynamics. On such a dataset, the early checkpoints often underperform models that (nearly) have converged, especially if training utilizes low learning rates with schedulers. Here, SRCDEV could be used to weed out underperforming checkpoints, such that CA then always exceeds the baseline that performs model selection on sourcelanguage validation data. Whenever the English training portion is sizable – like in our other tasks – checkpoint averaging is consistently beneficial. Our experiments also demonstrate that XLT behaves differently by task. Averaging checkpoints consequently might affect other tasks differently like, for instance, document classification that reason about long contexts or retrieval tasks like Tatoeba that jointly require sequence- and word-level semantics. Another dimension we did not explore further due to a limited compute budget is how to ensure best that monolingual models are aligned for run averaging. For instance, it may not be required or even desirable to keep classifiers frozen throughout the second step of our proposed training curriculum (§3), as we would ideally also want to average out idiosyncratic noise of the original classifier."
246,section 6
247,"after conclusion selection, page 9
7 A2. Did you discuss any potential risks of your work? we do not see any ethical implications or risks of our work"
248,"A general limitation of this line of work is that most of the results are primarily confined to artificial datasets. Although such formal languages provide us with a controlled setting and clarity regarding the precise nature of the problem, the relation to practical tasks remains unclear. Hence, while our results highlight the contrast in the performance between the two types of architectures, its precise implications on real-world tasks remain unclear.
There are two negative results that do not support our hypothesis. (a) All the experiments discussed in the main paper are on strings of fixed lengths. We conducted some experiments on tasks with variable length sequences which in some sense have low sensitivity. The tasks can be seen as a variable length extension of sparse parities and sparse majorities. Unlike the fixed length setting, we found both LSTMs and Transformers perform similarly on those tasks. See Section E.1 in the Appendix for more details. (b) Although we found Transformers to consistently converge to low sensitivity functions in the case of Boolean functions, we did not find similar behaviour on sentiment classification datasets such as SST and IMDB (see Section C).
A caveat with empirical studies such as this is that the results depend on the hyperparameters and other aspects of the experimental setup. While we have tried to be as thorough as possible with hyperparameter tuning, there is always a chance that the results or behaviour could differ for some hyperparameter."
249,"The current work marks the first step towards intentconditioned counterspeech generation, and as we noted, even though our model excels in fluency, a larger and more diverse dataset paired with knowledge grounding is necessary to improve and ensure factual correctness. Although the annotators kept the quality of counterspeech as high as possible, it is possible that this data is not at par with other datasets that are annotated by more skilled NGO operators, as is the case with the Multi-Target CONAN dataset (Fanton et al., 2021). A more large-scale annotation of our dataset with higher instances for under-represented target communities would hence be beneficial to learn more accurate distributions of every counterspeech class. Another limitation of the current work is that it exhibits a slightly higher-degree of toxicity compared to the baseline. It, therefore, pertains to accounting for lowering the amount of toxicity present in the generated counterspeeches as future research. Lastly, humor in counterspeech is a very subjective topic, and inspite of including only a few datapoints from that class as compared to the others in our dataset, it is likely that QUARC could generate vague and/or offensive text under the pretext of humor. We intend on keeping the dataset private and only provide access for research and educational purposes."
250,"Similar to the limitations of existing selection methods, our method needs a reasonable feature embedding for accent representation in order to effectively target accents. MFCC features are not the best choice to represent accent information. Some accents may be more difficult to represent than others. This also lowers fairness scores for such accents. For instance, in one of our experiments where Manipuri accent was paired with Rajasthani
or Assamese accents, we observe that acquiring a fair subset using any selection strategy is challenging (see Tab. 5). Although, FLMI was able to achieve a higher TF score than others, it was relatively lower than other accent pairs (see Tab. 3 and Tab. 4). This is due to the fact that the pairwise similarity scores of utterances within the Manipuri accent are lower than other accents. The lower pairwise similarity scores lead to lower marginal gains during greedy maximization and are a consequence of poor feature representations due to insufficient information being encoded about the Manipuri accent. On another note, a risk associated with the targeting ability of DITTO is that it could be misused to create models that are unfair to certain populations. For future work, evaluating the performance of DITTO on larger datasets and other diverse settings (e.g. out-of-distribution accents) will be interesting."
251,Section 8
252,"There are a few limitations to the current framework. Firstly, Verify-and-Edit works the best for open-domain question-answering tasks that require complex reasoning. Less complex datasets or commonsense datasets that do not require knowledge retrieval may not result in high improvements. Secondly, it is most ideal to edit a group of mostly incorrect samples, which we try to select by using consistency. Thus, our method is reliant on the consistency method’s performance and its abilities to separate correct and incorrect predictions. Most often, it can demonstrate a larger improvement with a more challenging set of examples.
To address these limitations, we plan to work on reducing the noise brought in the rationale-editing stage and utilize more knowledge resources, such as knowledge bases, as a follow-up."
253,"• Our work focuses on data from one platform, Kialo, which contains cleaner and higher quality arguments from a diverse range of topics and domains. How our approach performs on data from other platforms or more specialized domains (e.g. deliberations about policy) has to be investigated in the future.
• The vast majority of data available is English which makes conducting and evaluating multilingual experiments not feasible even with language transfer (see Appendix Section A.1).
• The dataset used in the training and evaluation has only one correct position although there might be multiple suitable parents. Given the large scale of the data and the huge number of nodes per tree, annotating all suitable parents would’ve require a very-large-scale unfeasible annotation. This could be investigated in future-work with the support of our models.
• The design of our annotation study does not take into consideration the structure of the tree. This might have made the task more challenging for the annotators. Reconstructing or representing the tree structure without revealing the actual parent (since the majority of the candidates are close relatives) is challenging when limiting the candidate parents to 10. Further refinement of the annotation study is left for future work along with the inclusion of the structure in the modeling.
• Although small models are shown to perform relatively well and are recommended to use when computation resources are limited, the models that perform, in our experiments, on par with humans are large models that are costly to train. Employing parameter efficient fine-tuning methods might be of interest here. • We use only manually designed templates as a simple approach that required no extra training or engineering. How the results compare to using automated template/prompt engineering methods is also left for future work. Including prompt-based fine-tuning might be also of interest to investigate in combination with contrastive training although language modeling training would require more computational resources. • Our task definition excludes the prediction of pro/con relation as less important, but the pro/con template information might be useful for this. More evaluation and analysis is needed to verify that. • Extra analysis that was out-of-scope to include in this paper might be of interest: e.g. the effect of topic, the degree of a node, and semantic similarity to siblings on model or human performance."
254,"One potential limitation of this review is our selection bias which may affect the representativeness of the included papers. Another limitation is the potential differences in methodologies across the
papers we reviewed, which makes it difficult to draw generalizable conclusions. Different studies use different experimental settings and methods for measuring feature importance, which could also impact the comparability of the findings across the included studies. Furthermore, we acknowledge the potential publication bias which might lead to an overestimation of the impact of different factors, as studies with statistically significant results may be more likely to be published than those with non-significant results."
255,"Limitations section
7 A2. Did you discuss any potential risks of your work? During literature review no potential risks could be identified. Additionally, our review focuses on the potential benefits of different factors rather than on the risks."
256,"Unnumbered section after conclusions.
7 A2. Did you discuss any potential risks of your work? Left blank."
257,"In the ”Limitations” section.
7 A2. Did you discuss any potential risks of your work? We only adopt publicly open resources including data and packages. Those resources are commonly used in corresponding domains. Our work does not introduce additional risk."
258,"The limitations of this work are: (1) In this work, we expect to consider more realistic and more applicable settings for class-incremental NER. Therefore, we consider the Unlabeled Entity Problem and provide a more realistic benchmark based on 66 fine-grained entity types. However, there remain some more serious situations unsolved in this work. First, the entity classes in each step might not be disjoint. For example, a new entity type ""Director"" might be included in an old entity type ""Person"". This problem is referred to as the coarse-to-fine problem existing in emerging types of NER. Second, the amount of data or labeled data introduced in each step can also be
limited, referring to the few-shot class-incremental problem. Therefore, the proposed method can be further improved to solve these problems. Third, the current version of the proposed method cannot handle the nested NER or contiguous NER problems. In the current version, we simply followed typical works in NER and adopted the sequence labeling scheme to model the NER task, which is not suitable for more complicated NER tasks. Nonetheless, as the proposed representation learning and re-labeling methods are agnostic to the formation of representations, we believe our method can also be adapted to a span-level version, which might be future works. (2) The proposed method is a rehearsal-based method that requires keeping exemplar sets for each class. Although the number of exemplars for each class is really small, we believe there can be more data-efficient solutions that totally avoid the need of memorizing data and also achieve good results. (3) The proposed method includes several hyper-parameters such as the entity threshold Tentity, relabeling threshold T hNN and T hproto. Although we have shown that the choice of thresholds is relatively robust (Sec.5.4), it still requires efforts to explore the most suitable thresholds when applied to other datasets or situations. There can be further work to improve this problem by formulating an automatic threshold searching strategy."
259,"""Limitations"" section.
A2. Did you discuss any potential risks of your work? Not applicable. We study cross-lingual NER task on public datasets, our work doesn’t have potential risks."
260,Limitations Section
261,"In principle our method is applicable in many domains, for example, one could use a biomedical knowledge graph instead of ConceptNet in a relevant domain. However, in this paper we only evaluate the quality of our approach in argumentative tasks which require commonsense knowledge. Our approach is unsupervised, but its performance depends on the quality of the used knowledge graph and SBERT model.
Similarly, we only evaluate CCKGs for English data, although our approach is not limited to English if one uses multilingual SBERT models (Reimers and Gurevych, 2020) or a multilingual knowledge graph.
Finally, our approach is purely extractive and hence, is limited by the coverage and quality of knowledge graphs. However, improving knowledge graphs is an active field of research and hence, high-quality and high-coverage knowledge graphs are to be expected. Furthermore, our extracted CCKGs could be augmented with generative models if coverage in the knowledge graph is not sufficient. However, that would reduce the interpretability that our approach provides."
262,The proposed AMI component is effective in the low-data regime but cannot be generalized to all cases. Future work will investigate the role of syntax in the structural regularization of attention and the extension of the proposed approach to discrete augmentation.
263,"Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
264,"A limitation of this study is that it is based solely on papers published in the ACL Anthology, which primarily represents the international Englishlanguage NLP conference community. While the ACL Anthology is a reputable source of NLP research, it should be acknowledged that a significant amount of research is also published in other venues such as AAAI, ICLR, ICML, and WWW. Additionally, there are also vibrant local NLP communities and venues, often publishing in non-English languages, that are not represented in the ACL Anthology. As a result, the conclusions drawn from our experiments may not fully capture the global landscape of NLP research and further work is needed to explore the diversity of sub-communities and venues across the world.
This work focuses on the aggregate trends of citing older work in NLP, but does not investigate the reasons for lower citation of certain older papers. There may be various factors that contribute to this, such as the accessibility to these older papers, the large number of recent papers, the applicability of these old works, and the technical relevance of the older work. Determining the relative impact of each reason is a challenging task. Therefore, more research is needed to fully understand the underlying mechanisms that influence the citation of older NLP papers.
This study aims to investigate the factors that contribute to the citation of older works in the field of NLP. We have analyzed different factors such as the mean age of citation, diversity in the age of citations, venue of publication, and subfield of research. Our results indicate that these factors are associated with the citation of older works, but it should be noted that these associations do not establish any causal relationship between them.
Lastly, it is important to note that citations can
be heterogeneous and can be categorized in different ways. For example, some classifications of citations include background, method, and result citations. However, certain citations may be more important than others, as shown by previous research such as ""Identifying Meaningful Citations"" by (Valenzuela-Escarcega et al., 2015)."
265,Section 9
266,We discuss the limitations of our work in section 8 Limitations.
267,"We hereby discuss the current limitations of our work: (1) The SIMMC-VR dataset, similar to the SIMMC 2.0 version, focuses on shopping scenarios (clothing and furniture purchasing domains), one of the most common everyday activities that virtual reality could enable users to do from anywhere, anytime. We have not tested whether the models would generalize to domains outside of the shopping experiences, thus we cannot speak to the transferability of our results to environments with very different visual properties than what our virtual environments provide. (2) In this dataset, we hand-design several possible dialog acts that we assume are common for human buyers, as well as their associated scenarios. This may not exhaust all the possible interactions a shopper can do with the assistant. However, we emphasize that the coverage should be sufficient for common shopping experiences. Additionally, although most of our proposed subtasks should be modeling generic user-assistant multimodal dialogue interaction and thus could be transferred well to other domains, the (our) domain specific MM-DST may not generalize as much. Nevertheless, they should still be transferable to similar (shopping) environments. (3) The audio of the SIMMC-VR videos are generated by automatic TTS, which may fall short to represent the natural human speech. However, we do not foresee this causing problems for multimodal dialog modeling, which this work mostly focuses on."
268,"A limitation of this approach is the trade-off between completeness and noise in the training data. While our method using keywords to extract text from Wikipedia is effective, IMPLICATION likely contains redundant sentences that cannot improve the model’s logical reasoning capability. A better rule-based or neural model might be able to extract a better corpus with potentially higher computational costs. Additionally, using POS tagging limits the application of this approach to languages with well-defined POS taggers. Switching to a more universal semantic tagging system (Abzianidze and Bos, 2017) can potentially alleviate this."
269,"Our synthetic pre-training dataset was automatically generated from manual templates, which inspite of dataset creation scalability and low cost,
may limit the diversity of the generated SQL queries. Our model, MultiTabQA, requires improvement in numeracy understanding and numeric operations. Real numbers are especially challenging and the model may not be able to correctly generate all the digits of the number correctly rending the generated cell incorrect. Furthermore, large input tables pose a challenge as the input sequence may get truncated beyond the model’s maximum sequence length. This has practical limitation in the size and number of input tables which the model can accommodate before truncation which leads to incorrect answers."
270,Section Limitations
271,"We presented Parallel Context Windows (PCW), a simple approach that alleviates context window restrictions for any off-the-shelf LLM, without ad-
ditional training. We showed the potential of this method on a variety of models and datasets. With that, our method does have some limitations.
The number of context windows has a limit, and needs to be predetermined. Similarly to vanilla in-context learning, the number of examples to include in the prompt must be selected beforehand. For PCW, it is also required to select the number of context windows, B. In this paper, most of the results are for B = 3. We experiment in Appendix C with the choice of B. The results are task dependent, but at a high level we find that there are diminishing returns around B in the range of 5 to 7. We leave further investigation of how to effectively benefit from more windows for future work.
Not effective for all types of tasks. As discussed in Section 3, PCW shows impressive gains in ICL for tasks such as multi-class tasks classification as well as information extraction. However, for some tasks, PCW does not improve performance. This might indicate that some tasks are not suited for parallel processing. Section 4.2 demonstrated that PCW is more suitable for cases where the input text could be divided into few independent inputs, but it remains an open question as to whether tasks, such as long text generation, would benefit from PCW."
272,"Linguistic variation Our results are highly dependent on the target language and its morphology. For example, word boundaries might seem like an obvious choice for dynamic segmentation, and in fact they achieve the best performance in English and Vietnamese. However, for some languages like agglutinative Finnish, whitespaces are less frequent, which is detrimental to model performance. Explicit word boundaries are not available for all scripts. For example, in Chinese characters, or in modalities other than text like speech or vision, there is no obvious equivalent to whitespaces. However, segmentation based on stochastic re-parameterisation, subword tokenizers and spikes in conditional entropy overcomes these limitations.
Contiguous segments In its current formulation, dynamic pooling only allows for merging contiguous segments of tokens in a sequence. However, this is not ideal for morphology types like Hebrew where morphemes are discontinuous: vowels are interspersed between consonant roots for inflection. Moreover, future works should consider higher levels of linguistic structure than words, such as dependency trees, for pooling. In this case, discontinuous segments may be necessary to handle non-projective syntactic dependencies.
Independent boundary decisions The decision to emit a boundary at time step t depends on previous boundaries only indirectly through the hidden representation of the first Transformer block, as this preserves the efficiency of the boundary predictor. Instead, a recurrent model could be explicitly conditioned on previous boundary decisions, which however would negatively affect the time complexity of the language model.
Work contribution of authors
The idea of training the models with pooling of variable-length segments was discussed among the authors while Jan Chorowski was at the University of Wrocław. Experiments were performed by Piotr Nawrot while he was employed in a research grant at the University of Wrocław, under the supervision of Adrian Łańcucki and Edoardo M. Ponti. The manuscript was written by Piotr Nawrot, Adrian Łańcucki and Edoardo M. Ponti."
273,"In this paper, we propose DocREDHWE and introduce a new metric to select the most robust and trustworthy model from those well-performed ones in DocRE. However, all data in DocRED are sampled from Wikipedia and Wikidata, which indicates that training and test data in DocRED can be identically and independently distributed (i.i.d. assumption). The i.i.d. assumption impedes our demonstration of the intuition: A model with a higher MAP will obtain a higher F1 score on the test set. Due to the i.i.d. assumption, models can succeed in obtaining a higher F1 score by greedily absorbing all correlations (including spurious correlations) in the training data. To strictly demonstrate the intuition, we need a test set that exhibits different and unknown testing distributions. In addition, expanding the research scope to a cleaner Re-DocRED and analyzing the role of unobservable wrong labels are also crucial and interesting ideas. We leave them as our future work."
274,Limitations section after the conclusion section.
275,In Section Limitations.
276,"We identify some limitations and risks with our current work that can be addressed in future work. • The current work is mainly applicable to log-
ical entailment problems, where one needs to solve a classification problem of whether a goal can be proved, disproved, or neither proved nor disproved based on a theory. Future work can extend LAMBADA to non-classification cases, e.g., where one needs to apply logical reasoning to answer questions such as “What color is Fiona?”. • The current work assumes all the rules are given as input and the rule set is small enough to be included in the prompt. Future work can extend LAMBADA to the cases where not all the rules are provided as input and part of the knowledge has to come from the LM itself, as well as the case where not all the rules can be included in the prompt due to the limitation in the prompt length. • The current work is limited to deductive reasoning with the modus ponens rule; future work can expand the applicability of LAMBADA on datasets with other types of rules such as proof by contradiction, disjunction elimination, etc. • The calls made to the LM modules in LAMBADA are dependent on the value from the previous call. That is, we need to wait for the results from one call before we decide what the next call must be. Since making batch calls to the LMs is typically easier and faster, future work can find ways to implement LAMBADA with batch LM calls. • While we showed that LAMBADA is more efficient than SI in terms of the number of inference calls it makes to the LM, it still requires many more calls to the LM compared to approaches such as CoT, hence increasing the required computation and cost."
277,Limitations section on p9
278,After the Section 6 Conclusion
279,Section Limitations
280,"Our automatic error type system, KAGAS, has room for improvements. Although we got high human acceptance rate for the error type classification results of KAGAS, Our coverage of error types is about 80% to 90% (Table 4). Currently, our system rely on the Kkma POS tagger for Korean. We believe that the improvement of a POS tagger will enable KAGAS to define a more detailed error type classification with high coverage and reliability. Also, there could be other ways (or more efficient ways) to define and classify Korean grammatical edits. However, we would like KAGAS to be viewed as the first step towards the effort of making an automatic annotation tool for Korean GEC, which, though not perfect, have meaningful contributions to the field in its current form.
Future Directions. Currently, the 14 error types of KAGAS is focused to be as specifc as possible, while respecting both statistical characteristics of Korean language and incorporation into a reliable, deterministic system with high agreement of human evaluation. However, definment of a more richer error type classfication system derived from KAGAS such as differentiating between the typographical and phonetic errors would be an important future direction for our research, as both are defined as SPELL errors on our current system. It would require solving additional challenges of accurately disambiguating a writer’s intention behind errors on a grammatical aspect. Another possible future direction would be applying data augmentation techniques on our datasets to boost the size of the training examples and obtain evaluation metric accuracy gains."
281,"Section 8.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
282,"The neuro-symbolic rule learning presented in the paper can handle most generic text-based games. Only in a few specific use cases, additional training of the AMR parser would be required. Since AMR is used for symbolic representation for text-based games, the vocabulary of the extracted triples is limited by the vocabulary of PropBank semantic roles. For applications in a very specific kind of domain where the predicates and entities do not match with this pre-defined vocabulary (for example, specific financial, legal domains, etc.), the AMR semantic parsing engine needs to be retrained first on such specific data before using it for rule learning. However, even in the cases where the testing environment requires additional rules, NESTA allows human-in-the-loop debugging to conveniently add them making it adaptable to generic environments."
283,Section 7 is the limitations section
284,"In this paper, we developed an approach for evaluating how large language models encode social attitudes about gender, and we applied that approach to evaluate BERT-base-uncased. Because the goal
of this paper was ethical in nature, limitations on the generalizability of our approach and findings entail ethical risks. With this in mind, we discuss both limitations and risks in this section. We first discuss limitations related to data, and then discuss those related to models and tasks. For both data and models/tasks, we consider general limitations of our approach, as well as more specific limitations of how we applied the approach here."
285,"Just as it is not possible to create a single benchmark for all language understanding (Raji et al., 2021), it is not possible to create a single, definitive dataset that relates language choices to social attitudes. Human experimental data is always limited by practical considerations and cannot test every condition of theoretical interest; e.g., in the role nouns dataset, there were no conditions with gender neutral names, while in the singular they dataset, there was no comparison to neopronouns (e.g., xe/xem). Additionally, because past work has found that model preferences may vary across similar linguistic contexts (Delobelle et al., 2022), it may be the case that BERT’s predictions would correlate differently with human responses on other variations on the stimuli. Relating model preferences to human behaviour will always be limited by the amount of human data that can be obtained.
Moreover, datasets are always situated in a perspective, emphasizing some people or views over others (e.g., Barrowman, 2018; Chasalow and Levy, 2021). For example, both datasets we consider focus on first language English speakers from the United States, and the specific relationship between social attitudes and linguistic choices captured by those datasets may not generalize outside that context. Languages other than English may have extensive grammatical gender systems, or classification systems that include social roles, among other linguistic devices, which interact to yield rich mechanisms for expressing social attitudes around gender. Even within English speakers in the US, how language signals social attitudes about gender may vary across groups and social contexts. (In fact, Papineau et al. (2022) found that Republicans with progressive social attitudes about gender did not use more gender neutral forms the way Democrats did; other, more fine-grained differences likely also exist.)
Additionally, relating social attitudes about gen-
der to linguistic choices requires some method for measuring social attitudes. Since conceptions of gender are so diverse and culturally variable, no single measurement would be appropriate for all contexts. For example, in one of the datasets we used, a survey for measuring social attitudes about gender asks participants to evaluate statements about stereotypical social roles of men and women, which are likely culturally specific (e.g., “A father’s major responsibility is to provide financially for his children”) (Baber and Tucker, 2006).
In evaluating language technology, a focus on associations between linguistic choices and social attitudes limited to particular linguistic and cultural contexts risks prioritizing the social knowledge from those communities, and imposing that in other communities when language technology is deployed. To support the creation of inclusive technology, the research community will need to prioritize generation of datasets like the two we drew on here – i.e., ones explicitly connecting linguistic choices to social views – across more languages and cultural contexts."
286,"There are also several limitations related to the models and tasks considered. First, we evaluated only one model (BERT-base-uncased), and more work is needed to understand if and how our specific results generalize to other masked language models. This is especially important given that past findings comparing gender bias in masked language models with different architectures and model sizes are mixed (e.g., Sharma et al., 2020; Jentzsch and Turan, 2022; Tal et al., 2022).
Additionally, we only considered the task of masked language modeling. We made this choice because psycholinguistic datasets that pair linguistic choices with results of social attitude surveys are rare, and those available to us used language tasks that were most appropriate for evaluation on the task of masked language modeling. However, given that bias on the intrinsic task of masked language modeling may not relate to (extrinsic) bias on downstream tasks (Delobelle et al., 2022), our results (such as BERT’s language communicating conservative attitudes) may or may not carry over to downstream tasks. In the future, our approach for relating task predictions to social attitudes could be used to evaluate downstream tasks (such as coreference resolution), once appropriate human data is
available.
Another limitation has to do with differences in the information considered by language models, as opposed to humans, in choosing to use gendered vs. gender neutral language. In both tasks we study, participants and language models evaluate the appropriateness of gender neutral forms based only on contextual cues to the subject’s gender, especially gender associations of names. However, when deciding what to say, people can also take into account the referential gender(s) (e.g., the pronouns someone uses, Cao and Daumé III, 2020) of people being referred to. For example, if a person knows that someone named Michael uses feminine referential gender, they would likely refer to her with gender neutral or feminine forms (e.g., congressperson, congresswoman) but probably not masculine forms (e.g., congressman). Focusing on evaluation tasks (and language models) which do not consider information about referential gender risks encouraging the development of language technology that performs worse on data from (binary and nonbinary) trans people, and contributing to their erasure. Note that in the Michael example there are still linguistic choices (i.e., between congressperson and congresswoman), which may reflect social attitudes. Future work should study the relationship between linguistic choices and social attitudes in models which can take referential gender into account, while also recognizing the social implications of language choices that respect referential gender.
Finally, while this work developed an approach for evaluating the social attitudes about gender communicated by language models, it does not propose any approaches for improving language models or adjusting the attitudes they communicate. Past work in NLP has discussed different approaches for how pronouns might be handled in language technology (Lauscher et al., 2022), and has developed gender neutral re-writing tasks (Sun et al., 2021; Vanmassenhove et al., 2021), which replace gendered pronouns and words like fireman/firewoman with gender neutral variants. Contrasting with standard fairness approaches in NLP that remove information about gender from language technology, work in feminist HCI has discussed approaches for the treatment of gender in language generation which are intended to challenge existing norms and stereotypes, and bring about social change (Strengers et al., 2020). Additionally, work on lan-
guage reform has discussed the challenges involved in working towards gender-inclusive language, including how explicitly gendered and gender neutral variants can often take on different meanings (Ehrlich and King, 1992; Zimman, 2017). Future work in NLP should consider each of these lines of research, discussing when and how it may be desirable for models to use or represent language that signals gender, and what attitudes those language choices communicate."
287,ENDERANKER is only tested on DIASUMFACT. Further tests on more datasets are required to establish its general applicability.
288,section Limitations between conclusion and References
289,"First, our work applies hand-crafted action labels as operation hints, which leads to some limitations to represent more complex operation steps. For the future work, we can use a generator instead of a classifier to generate a more flexible set of operation prompts, making them more representative and meaningful Secondly, due to the high controllable generation of our approach, if our approach yields a wrong operation step prediction, it would further mislead the intermediate step generation. To eliminate the drawback where inaccurately generated operation prompts would mislead the next step, we can apply a verifier (Cobbe et al., 2021) to evaluate the reliability of the generated operation prompts. When the reliability is low, we ditch the operation prompt to prevent it from guiding the model into an incorrect path."
290,Section 6
291,"Section 7
7 A2. Did you discuss any potential risks of your work? The data we used are publicly available and do not contain this issue."
292,Limitations section (unnumbered) after the conclusion
293,Limitations Section
294,"4.4, 4.5, Limitation
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
295,Line 587 - 620
296,"Limitations
7 A2. Did you discuss any potential risks of your work? We use the open data to pre-training our model."
297,"This work utilizes generative models trained on large volumes of data, to generate supplemental training data for named entity recognition systems. We do not address any biases, or filter generations of the underlying paraphrasers when using their generated data. This can bias the fine tuned models towards underlying biases of the generative system.
While we do not test or correct the paraphrasing systems for biases, we do not find any evidence for the models deviating unfairly from the underlying training data in any of our human evaluations of the paraphrases.
We recommend human review, and automatic filtering of the generations when applying techniques based on generative models to critical applications, to ensure the black box paraphrasing does not introduce, or exacerbate the biases in existing training datasets."
298,"Estimating human utility is expensive. The core of our work is built on conducting extensive human evaluations, to understand how well lay humans can solve tasks with rationales. In order to replicate these findings to other tasks, one would require the same scale of human evaluations, which are expensive and tedious. These tasks are also difficult to explain to lay crowdworkers, because of which several rounds of turking are required to reach good annotator agreements. Given these shortcomings of human evaluation, a reliable metric that estimates human utility is necessary.
Generating generalization questions is not completely automated. Even though we prompt GPT-3 with varied demonstrations to generate generalization questions of each type, we still have to manually filter them (via crowdsourcing) to obtain a cleaner set of questions. Furthermore, in order to obtain gold answers of these questions, we generate answers by prompting GPT-3 again, which also requires further validation. A completely automated method of generating these questions would lead LM updates to be independent of human involvement.
Even though GEN-U has a better correlation with human utility, the correlation is still low. To train models to produce free-text rationales with more human-utility through Quark (Lu et al., 2022), it is first necessary to have an accurate metric that can serve as a reward function/scoring metric for human utility. In this work, we found that human generalization is good indicator of human-utility. However, given that Quark requires frequent reward scoring, it is infeasible to use human annotations for the same. Our proposed automatic metric GEN-U that simulates human generalization has a good correlation with human utility (better than task accuracy, or BERTScore), but overall, it still has a low correlation with human utility of rationales. Developing a score with better correlation with human utility (perhaps even a stronger version of GEN-U) will decrease the effect of this limitation and lead to training that further increases human utility of generated rationales."
299,"The current framework bears several limitations. First, although a common strategy in the related literature (Brunner et al., 2020; Ek and Wirén, 2019; Jannidis et al., 2018) which we also adopted, the binary annotation at the token-level is limiting. With this schema, the focus is not on speakers’ utterances or turns, but on DS sequences. A subsequent issue is that consecutive turns by different characters are considered as one DS sequence if there is no ""O"" labeled tokens between them. One solution could have been to mark the start and end of a DS turn while paying attention to handle imbricated narration (ie. incises). However, this would have required significant more re-annotation efforts, which we left for a future research cycle within the proposed framework.
Second, because of copyright issues the corpus contains excerpts exclusively from a specific period, 1830-1937. Thus, the models were trained and tested on a specific type of literature and may not generalize well to other forms of narratives, in particular modern and contemporary. In this direction, the curation of the test corpus could benefit from more literary insights considering that the evaluation showed high variance of the performance over chapters. This could help to better determine the application scope of the models, and which kind of narratives require further work.
With regard to the deep neural network baselines, we did not perform an extensive parameter search and model optimisation. This could have further improved the results. However, performances on recognizing full DS spans were clearly lower than token-level metrics, which had most likely other causes. Regarding the evaluation, although we adopted ZME scores from page segmentation to have more qualitative insights, there are still other aspects we have not quantified and could be particularly relevant. For instance, does the model tend to miss the beginning, the end or some other specific parts of a DS sequence? We tried to capture some of these phenomena through our manual analysis, but it is challenging to apply it at scale
without introducing methods to automatically compute metrics."
300,"the Limitations section
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
301,Section 6: Limitations
302,In the Broader Impact Statement.
303,"Despite achieving high translation performance on various language pairs, LOBEF has some limitations, coming from the nature of processing UTF-8 byte sequences.
Speed: As shown in Table 8 in the appendix, the inference times for byte-based models are higher when compared to subword-based models. It is also worth noting that we use the same amounts of model parameters for a total of 6 Transformer encoder layers and 6 Transformer decoder layers for all models in comparison. As shown in Table 7, byte-based models can effectively reduce the amounts of parameters for the embedding layers comparing to the subword-based models, leading to faster training time as shown in Table 8. However, as indicated by Xue et al. (2021), by adding more encoder layers, we can construct byte-based models with comparable amounts of parameters as subword-based models, and these larger byte-based models still require much longer time for training than subword-based models.
Extremely Low-resource Languages: The performance of byte-based models on extremely lowresource languages (e.g., 2.7K training data for Lao-English) is still lower than subword models especially in the multilingual setting. We suspect that byte-based methods require a relatively larger number of training data in order to aggregate information from a combination of byte tokens, comparing to subword-based models that explicitly maintain a subword vocabulary.
Extra Preprocessing: The Byte-WSF model requires an extra preprocessing step that precomputes the attention mask corresponding to the words in each sentence. This adds a slight overhead before training, while training the Byte-WSF model is as fast as the Byte model, as both model use the same Transformer architecture. However,
for languages (e.g., Chinese) that do not have whitespaces to indicate the word boundary, we may rely on an off-the-shell word segmentation tool to preprocess the text."
304,"Limitations section towards the end, unnumbered"
305,"""Limitations"""
306,"Section ""Limitations"""
307,"A well-known shortcoming of transformers is the computational complexity in self-attention lay-
ers (Vaswani et al., 2017). Since the number of required calculations grows quadratically with the length of the input, transformers become prohibitively slow on very long sequences. An unfortunate side effect of processing inputs at the characterlevel is that internal sequences become much longer, so token-free transformers run into these efficiency problems much earlier than subword-based models. Figure 7 illustrates this problem by contrasting the runtime of all poetry generation systems when generating a single quatrain. Even ByGPT5 (small), the smallest model in terms of number of parameters (cf. Table 1) and the fastest token-free transformer, is only marginally faster than GPT-2 (medium), which is almost five times larger. Tay et al. (2022) propose a solution to this problem for transformer encoder blocks by applying a neural pooling operation over input embeddings before feeding them into the model, which could be extended to decoder blocks in future work. Alternatively, Libovický et al. (2022) propose a two-stage decoding architecture in which the transformer decoder operates on character blocks that an additional LSTM model (Hochreiter and Schmidhuber, 1997) decodes into individual characters.
Another shortcoming is that our poetry generation systems can only generate a single poetic form, i.e., quatrains. In general, poetry is a very diverse form of language and stanzas can be of arbitrary length, so this is a serious limitation. In future work, we thus plan to extend our implementation
of style-conditioning to variable length poems. In particular, one could encode a rhyme scheme not as a single special token, but as an arbitrary series of letters indicating which verses rhyme with each other. Alternatively, our current systems could be used to generate longer stanzas through a sliding window approach, i.e., generating one verse at a time with the last three verse as context.
Further, our human evaluation has limitations due to its relatively small scope. We only have a limited number of annotators and only consider a subset of all style combinations. Nevertheless, we have achieved moderately high to high agreement on all tasks, and we have an additional human evaluation of German poetry in Appendix B, which points to the same conclusion.
Lastly, QuaTrain is limited in that it consists of pseudo-quatrains, which are not real quatrains and often have missing contexts. Nonetheless, as can be seen in Appendix D, models trained on QuaTrain are still able to generate meaningful poetry. In future work, we plan to improve the quality of our dataset by obtaining real quatrains from additional sources such as the EighteenthCentury Poetry Archive (Huber, 2022)."
308,"Following instructions, we add Limitations after Conclusion."
309,"Limitations section
7 A2. Did you discuss any potential risks of your work? Our work does not involve any sensitive data or tasks, and there is no potential risk.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 4
3 B1. Did you cite the creators of artifacts you used? 4
7 B2. Did you discuss the license or terms for use and / or distribution of any artifacts? We will discuss the license at GitHub upon publication."
310,"7: Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
311,"In the Limitations section, after the conclusion.
7 A2. Did you discuss any potential risks of your work? Because we are extending existing work on existing datasets with new methods, our work introduces no additional risk beyond existing models for solving those problems. These problems are also specifically far from production uses, so risk is generally low."
312,
313,"Professional coders are trained to find sufficient, as opposed to exhaustive, evidence for each code. Our Profee coders were instructed to find all the evidence for each code. However, given the large number of notes in some MIMIC encounters, they might only manage to annotate most of the evidence. For Inpatient, there might be more bias among coders towards finding sufficient evidence: namely, there were many cases in which one coder found evidence that another had not, but during the adjudication process, both coders agreed it should be included. Thus, although we have opened the door to automatic evaluation of evidence extraction systems, some metrics, such as recall on our dataset, might underestimate the true recall of a system.
We observed inconsistencies and human errors while cleaning up the data. Coders sometimes only annotated partial evidence, leaving out modifiers like ""acute"", ""moderate"" and ""bilateral"". For example, we consider ""bilateral pleural effusions"" as the correct evidence but only ""effusions"" was highlighted, and for ""weakness in his lower extremities"", only ""weakness"" was highlighted. Another source of error is due to the limitation of the annotation tool which does not support highlighting and linking discontinuous spans of text as a single evidence for a code. As a result, some evidence may contain extra tokens between the correct evidence tokens and others may miss part of the evidence when the supporting text spans are far apart. We tried our best to fix these issues, but some errors likely remain in the dataset."
314,"Although our masks in different layers are binary and require significantly less storage compared to other PETL networks, we still need the underlying 32-bit scores for each mask during the training process. Therefore, PROPETL consumes slightly more memory in the training time than existing PETL methods. To fine-tune PROPETL, it takes a similar training time to conventional PETL modules, which means our method will normally take a longer time to converge compared to the fully fine-tuned model."
315,Limitation section
316,"RL4F is primarily targeted at improving final performance. While we have found that the critiques learned by RL4F remain natural, we do not introduce any explicit restraints preventing semantic drift. As though it may raise end-task performance, semantic drift would also hinder interpretability. Future work might study datasets that are not covered by this dataset and quantify semantic drift along with proposing measures to prevent it, as necessary. Moreover, this work does not provide an explicit mechanism to incorporate new critique labels that might become available in the future nor it identifies a framework that could combine critiques from multiple experts such humans and other machines. Lastly, we limit our analysis to GPT-3 and focus on a scenario where it is inefficient or impossible to train the task model while this may be a conservative assumption for other settings."
317,Limitations
318,"At its core, NORMBANK is a collection of logical operations on unique constraints. Consequently, one practical limitation stems from the issue that some situations cannot be reasonably expressed as a set of constraints. While theoretically all logic can be decomposed into AND and OR operations, the logic may be too challenging for an individual to formulate, or the set of constraints themselves might be too large and unwieldy. The latter is problematic, because language models have a finite input token capacity, and for the set of constraints to be digestible, they must fit within that capacity. Relatedly, if the logic to encode constraints become more sophisticated, ensuring that logic is not unnecessarily duplicated will pose a greater challenge. Additionally, certain properties of NORMBANK like the role and behaviors may be challenging to succinctly describe. Further work will be needed to ascertain how these can be incorporated or to more clearly define situations that are out of scope.
Due to limitations on time and computational resources, we have not exhaustively evaluated all downstream applications of NORMBANK, and in future work, we will test additional transfer tasks beyond the moral and social classification tasks considered in this work. Since NORMBANK is the first to encode non-monotonic situational norms, there was no other available benchmark that is directly analogous to ours. Instead, our primary evidence for NormBank’s utility is in Table 3, where human evaluators confirm that models trained on NORMBANK can reliably learn to make new inferences about non-monotonic situational norms.
Other follow up studies should consider training larger normative reasoning models, and/or engineering better prompts for expanding NORMBANK. Relatedly, we have no data to speculate about the long-term evolution of real-world norms relative to this resource, nor the rate of decay in the reliability of NORMBANK. Future work should also expand this resource with perspectives from cultures other than our available annotator pool. The pool was not representative of all cultures and people groups, as we discuss further in the Ethics section."
319,Section 7 (Page 9)
320,"Despite showing impressive performance, our graph-based approach still has several limitations. The first one is related to the construction of the sentence graph. At present, we consider two sentences to be semantically related if they share similar nouns. But coherence can be achieved not only by describing similar entities but also by discourse (rhetorical) relations (Jurafsky and Martin, 2021). So it will be an exciting direction to incorporate discourse relations into the construction of a graph. The second one is that we implemented our method using only a plain GCN. Recent work has pointed out that the original GCN can be further improved with more advanced aggregation functions (Xu et al., 2019a) or attention mechanisms (Velickovic et al., 2018). So another interesting direction is to explore the benefits of more powerful graph neural networks for our method, which we leave for future study."
321,"In Section 6.
7 A2. Did you discuss any potential risks of your work? This is an entirely technical paper. We don’t think it has any risk of bias or otherwise."
322,"The proposed CKL can automatically produce the scores for each utterance in the context and each sentence in the knowledge. However, it is constrained by the total length of the input sequence. CKL takes BART as its foundation, thus the bottleneck of BART limits the upper ability of CKL. BART requests the input length to be 1,024, which means the CKL can receive at most 1,024 tokens at a time. For some samples, the concatenation of the context and knowledge contains far more than 1,024 but is truncated to fit with the length requirement. In those cases, the CKL cannot get enough information, resulting in the sub-optimal performance of the CKL."
323,Section 7 Limitations
324,"While we have endeavored to include a good crosssection of selective prediction techniques in our empirical study, clearly it is not comprehensive of all work in this space. Moreover, this work does not address confidence calibration, nor does it address the behavior of selective predictive techniques under domain shift. Finally, our focus is on selective classification – we do not address confidence estimation for regression or generation tasks."
325,Section Limitations
326,"While the proposed MetaEvent achieves significant improvements in both zero- and few-shot event detection, MetaEvent requires additional computational resources due to the layer- and step-adaptive learning rates and the outer-loop optimization, which may cause increased computational costs for training MetaEvent. Moreover, we have not investigated the benefits of task scheduling techniques and similarity-based meta learning in MetaEvent to fully explore the training event types. Consequently, we plan to study efficient meta learning with advanced training task scheduling for further improvements in zero- and few-shot event detection as future work."
327,7 Limitations
328,"TST BT is simple and straightforward, which brings great improvements against BT baselines. However, comparing with standard BT, TST BT requires an additional style transfer model and additional time to process generated BT data."
329,"This work has one major risk. As the main idea proposed in this work heavily relies on the external 3D scene extractor, the quality of extractor on our used VSD images largely influences the task performance. However, we reveal in analysis that although suffering from the domain shift issue by the out-of-domain 3D scene extractor, our method still improves the VSD task. We show that when handling the in-domain VSD images as used for training the 3D scene extractor, the VSD performance has been boosted remarkedly. Thus, with better a 3D scene extractor, it can be expected that our system will exhibit much stronger capability and advance the VSD task more significantly."
330,"There are some limitations that have yet to be addressed. Since we use the predicted probability distributions of the model output as a medium for continual KD for NMT, the vocabulary of multiple models needs to be consistent. Overcoming it allows continual KD for NMT to be extended to models with different language pairs and different modalities. Also, although our approach is robust to malicious models, there are more diverse and sophisticated attacks in real-world that require more research on defense. In addition, the teacher and student models must be trained on the same language pair. Further studies can consider more general scenarios without the above limitations. There are other approaches worth exploring in order to address the transfer of knowledge from models rather than their training data besides sequential manner. For example, it is also possible to explore various distillation methods like organizing teacher models into batches or pipelines."
331,"There are two major limitations in this work. Firstly, while we showed that query refinement prompts improve the ability of LLMs to generate long-form answers in a closed book and few-shot settings, open-book systems still perform better even when using a lot less parameters. Doing open-book long-form question answering in LLMs is currently not trivial due to their input token length limit and the need to use longer prompts when context passages are given.
The other major limitation is that human annotators only used the gold-standard answers to check for correctness (i.e., VS GOLD in Table 6). As explained in Section 4.5, there can be many ways to disambiguate questions, and therefore the systems can obtain a longform answer that is not different from the gold answers but still should be considered correct. We tried asking annotators to use the Internet to check for correctness, however they found it difficult to do so even for a single example."
332,"As we mentioned, we only focus on event arguments with the assumption that event type is already provided. However, this is not always true for many applications in real life scenarios. But it would be out of the scope of this work to combine them together, so we leave it for future work. Besides, considering the long input of document-level extraction, the computing memory consumption significantly increase to tens of times compared with its sentence-level counterpart. We only consider the 1/2-Doc cases, although in reality more docs are possible. We believe finding a solution for decreasing the memory requirements would be of great impact for future research in this direction."
333,"Please see the ""Limitations"" section."
334,"Although MITQA achieves the best results for TextTableQA benchmarks to date, it still has some limitations, owing to its design, and the type of training data it can access. Design policy: We have designed MITQA as a collection of trainable modules, which are used in a specific sequence. This design has helped us to focus our innovations in specific modules such as multi-row training for RowRetriever, multi-span training for AnswerExtractor, etc., with an eye to boost overall accuracy. However, the modular design also means that MITQA is not fully end-to-end trainable. Therefore MITQA is, in principle, susceptible to compounding error propagation across modules. We view this as an acceptable trade-off while working on HybridQA and OTT-QA, but other data sets may force us to revisit this decision. Types of queries: TextTableQA, being a relatively new task, has only two major benchmarks available (HybridQA and OTT-QA), where OTT-QA is an open domain extension of HybridQA. Therefore, the types of queries to which MITQA during training are limited to effectively a single large benchmark (HybridQA). HybridQA — and consequently OTT-QA — corpora are similar to Wikipedia articles, not confined to any specific domain. Further experiments in specific verticals, such as Finance, Retail, and Health are needed to check if MITQA affords practical cross-domain adaptation.
Moreover, only a small fraction of queries in HybridQA and OTT-QA need aggregation. Due to their rareness, we have not considered handling aggregation queries through MITQA, which needs additional work in future."
335,"Section ""Limitations""."
336,"InsMed is built based on the large-scale pretraining model BART, which requires high computing resources. Besides, the data currently only covers four departments, limiting the usage scenarios of the data."
337,"Firstly, following the work of T0 (Sanh et al., 2022), we mainly focus on NLP tasks that can be formulated as rank classification. This covers classification and multiple-choice tasks, but not other task categories such as generation or regression. We hope to extend our training and evaluation to encompass a wider range of task categories, and hope the research community will collaborate in creating resources for such study.
Secondly, though we showed that FiD-ICL outperforms Concat-ICL, we still lack clear understanding on the source of such improvement. We hypothesized that FiD enables the model to learn from in-context examples more effectively, yet our perturbation experiments show that FiD-ICL mod-
els still learn little from input-label mapping (§6.2). Much more work is needed to further understand of the working mechanism of ICL models.
Thirdly, given the complexity of our study, we limit the scope to encoder-decoder models. We made this decision due to the superior performance of encoder-decoder models in task-level generalization (Wang et al., 2022a) and their compatibility with fusion-in-decoder method. Also, our important baselines, T0 (Sanh et al., 2022) and T-Few (Liu et al., 2022), are implemented with the T5 model family. We include more discussion in §A.4."
338,Limitation section after conclusion.
339,"We empirically conclude two limitations for S2ynRE in the hope of inspiring more future research. On one hand, its advantages are less significant when a large amount of annotated data is available. For example, TACRED training set has 68,142 annotated instances. Under this setting, even if we add another 100,000 synthetic samples, the improvement is only +0.98 compared to +22.02 under 1% training set. This means that the quality of synthetic data, although superior to distant ones, is still not as good as golden ones. Thus they can hardly provide identical utility the same as 100,000 golden data. Nevertheless, with the development of LLMs and their powerful generation ability, we look forward to accessing higher-quality synthetic data.
On the other hand, when training data are limited to a few samples (for example, 1% setting for SemEval only includes 73 training instances), even strong LLMs like GPT-2 can not perfectly fit the structure of relational statements within a few steps of finetuning (See Appendix G for illustration of cases). Therefore, many generated sentences may not contain correct subject or object entity markers as requested and have to be discarded. In general, although the formation of marked natural language sequence proposed in this work made such structured synthesis feasible, we look forward to further improving the synthesis efficacy in future works.
4https://www.wikidata.org/wiki/Property_talk: P609
ACL 2023 Responsible NLP Checklist"
340,Sec. Limitations
341,"In the Limitations section at the end of the paper (Section 7)
7 A2. Did you discuss any potential risks of your work? In our paper, we simply evaluate the performance of models to address the task of Entity Linking in simulated scenarios with noisy data. We cannot think of any potential risks of our work."
342,"Our analysis on temporal drift (§ 5.2) was limited by the fact that the developer of many models in our study did not release the exact time period of the pre-training corpora used. Additionally, models such as BERT and RoBERTa were pre-trained on corpora that could be potentially be temporally close to the CoNLL++ test set.
In the section on test reuse (§ 5.1.2), due to a limited compute budget, we were only able to conduct this experiment on a single new train/dev/test split, so it is possible that the new split happens to be easier than the mean of the distribution. However, our experiments still provide additional evidence models are not overfitting the original CoNLL-2003 test set.
It is worth noting that when using older models trained on the CoNLL-2003 dataset, one additional reason for the performance degradation, especially
in real-world deployment, is that the data used to evaluate the models can be out-of-domain. In our experiments, we attemped to control the domain of the test data on which the models were evaluated to assess other factors for performance degradation. However, we acknowledge that in reality, model performance can be affected by factors such as the emerging text types (e.g. Twitter did not exist when CoNLL-2003 NER task was created), which leads to changes in domain, and therefore affects the generalizability of the models.
We acknowledge that having CoNLL++ will not resolve the problem of generalization to modern data. As new data keep emerging, there will always be the question of how well NER models generalize to that new data. We hope that our paper will encourage researchers in the NLP community to continuously annotate new test set to study this problem, so that we ensure the robustness and generalizability of models."
343,"Section 8
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
344,Last section
345,"""Limitations"" section provided in the template"
346,Limitations
347,"Until now HERMES has only been evaluated on the formula prediction task, but we believe that the three-stage decoding pipeline and the sampling strategy for multi-level expansion in HERMES can further be extended to other forms of code such as SQL and Linux commands. Another limitation is how to unify the experiences of hierarchical expanding and other formula writing orders, because we think that the human reasoning order in writing formulas may be a mixture of top-down, bottomup, left-to-right, etc. HERMES proposed new ideas on variable high-level expansion, but it is desirable to be further integrated with our experiences.
ACL 2023 Responsible NLP Checklist"
348,Limitations Line 666
349,"Section 8
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
350,Section 8 Limitations.
351,"after conclusion selection, page 9
7 A2. Did you discuss any potential risks of your work? we do not see any ethical implications or risks of our work"
352,"The main limitation to our work lies in the handling of unordered n-ary relations. We hypothesize that the bottom-up paradigm performs best when there is one unambiguous logical form to generate for a particular question. While this is quite often true for semantic parsing, in our experience, unordered n-ary relations (e.g., WHERE) can quickly cause this not to be the case. With such relations, there tends to be a large number of correct logical forms for a particular question. In these situations, having so many candidate logical forms can cause significant issues in terms of runtime. A second limitation of our work is that it assumes the logical form will be given as a graph. Thus, there is an annotation burden on the users of this system that
would not be present in systems that treat semantic parsing as a text-to-text problem (e.g., fine-tuned large language models)."
353,Section Limitations
354,The Limitations section on p9
355,Limitations section at end
356,"One limitation of our approach is that we aggregated IC and GC measurements over clusters during the automatic subgroup discovery process, but we did not fully consider the relationships between clusters. A more comprehensive strategy for utilizing beneficial relationships and a more precise approach to potential conflicts between clusters could lead to further improvements in overall performance. Additionally, our MNLI experiments were conducted on large dataset that had multiple clusters with errors. We chose to focus on the top10 clusters with the most errors due to limitations in resources for running a user study. While TDG on top-K clusters has demonstrated effectiveness in improving performance, there is still the potential for further improvements by working on a larger number of clusters. At the same time, we emphasize that TDG should be used as the last step to improve performance in low-performing groups (clusters with high errors). If these groups are numerous, it means the model is likely under-trained, and other techniques (e.g. better data/modeling) should be applied first."
357,"Our work has the following limitations.
Comparisons with more recent models. In §3, we conducted a principled comparison between BERT, ELMo, and GPT-1 under comparable experimental conditions. This comparison notably excludes more recent models that benefit from more parameters, larger training data, or different loss functions, such as RoBERTa, Electra, and T5. Due to the even-higher cost of pre-training these more recent models, we leave a principled comparison that includes these models to future work, although we identified the development of more comprehensive PLM scaling laws as a promising future research direction that would allow us to extrapolate how our findings would generalize to different pretraining data sizes, objective functions, etc. (§4).
Interaction between different factors. In §3, we have conducted a principled comparison by varying only the pre-training objective function and the length of model training, whilst keeping all the other variables constant. In practice, however, the exact choice of these different control variables (e.g., what positional encodings to use, how we pre-process the data, etc.) can interact and affect the findings in a material way. It is conceivable — and rather likely — that our findings on the performance gap between BERT, ELMo, and GPT-1 may change under different experimental settings.
Simulated efficient learning scenario. Our efficient learning scenario in §3 constitutes a simulated one, where we artificially limit the number of updates to 200,000 steps (as opposed to 1M steps in the full setting). We leave the extension to more realistic efficient learning scenarios, such as in languages where there is only a limited number of monolingual data, or where there is a hard limit on what pre-training computational resources we can use (e.g., 1 GPU for 3 days), to future work.
Extension to multi-lingual settings. Our experiments are thus far conducted only in English. We leave the extension to other languages — including low-resource languages with only a limited amount of monolingual data as a realistic and necessary benchmark of efficient learning — to future work.
The increasing prevalence of closed-source / proprietary PLMs. Despite our recommendations and calls for change, we acknowledge the fact that
recent PLM trends have shifted more towards proprietary and closed-source models — a development we attribute to the rapidly increasing commercialization potential of this technology. Under this trend, very little is known about how each PLM is developed, as the vast majority of the technical details (e.g., the amount and source of the pre-training data, the data filtering strategy, the size and hyper-parameters of the model, how the model is implemented, etc.) are kept proprietary. While these trends may mean that our recommendations are more unlikely to be adopted by proprietary PLMs, we argue that our position paper and recommendations are still important (if not even more so) for two reasons. First, open-sourced community models, such as BLOOM (Scao et al., 2022), OPT (Zhang et al., 2022), and Alpaca (Taori et al., 2023), are gaining traction, and have rapidly narrowed the gap with proprietary models. This progress reflects the community’s strong desire to have open-sourced models that can rival proprietary ones in terms of model quality. The rise of these open-sourced models thus gives rise to the question: How can these community-driven models help the community pay off our scientific debt? To that end, our recommendations provide concrete and actionable steps in this direction. For instance, our recommendations call for standardizing the pretraining dataset, which has not yet been done thus far, even though there are plausible, open-sourced datasets that can be used for doing so. Furthermore, we also encourage the community to release the full evaluation results of their models, alongside the relevant hyper-parameter information, etc., such that we can collectively build a more comprehensive scaling law through crowd-sourcing (§4). Second, prior work that conducts extensive ablation studies and rigorous experiments (Raffel et al., 2020; Sun and Iyyer, 2021, inter alia) remains the exception, rather than the rule. Our position paper includes a call for change that will make it easier to pay off this scientific debt going forward, which is ever-more important in light of impressive progress from both proprietary and open-sourced PLMs.
Ethical Considerations
Our experiments replicate prior work under comparable experimental conditions. For this reason, we do not expect our work to introduce any novel ethical issues, although our experiments may inherit a similar set of issues concerning PLM (especially
large-scale ones), as outlined by various prior work (Gehman et al., 2020; Bender et al., 2021; Rae et al., 2021; Dinan et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021, inter alia). We remark, however, that conducting these principled comparisons across different models — which requires a degree of hyper-parameter tuning for each model (both at pre-training and fine-tuning stages) in order to enable a fair comparison — requires a large number of computational resources, which may contribute to increased carbon emissions (Strubell et al., 2019; Patterson et al., 2021)."
358,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
359,"As mentioned in our experimental setup, we provide results of AV-S2ST in LRS3-T with synthesized target speech, similar to the pioneer literature (Jia et al., 2022b) in S2ST. One of our future directions is to develop a better benchmark dataset (e.g., mined or human-annotated data) to improve translation performance.
As mentioned in our results analysis, the BLEU scores heavily depend on the ASR quality, which may not accurately reflect the speech translation performance. Future directions could be improving ASR quality or exploring other evaluation metrics without reliance on ASR models.
AV-TranSpeech lowers the requirements for audio-visual speech-to-speech translation, which may cause unemployment for people with related occupations such as interpreter and translator. In addition, there is the potential for harm from nonconsensual voice generation or fake media. The voices of the speakers in the recordings might be overused than they expect."
360,See Section 6.
361,"Assumptions: This work applies a lower-case transformation to the vendor names during the pre-processing step and assumes vendor accounts ""agentq"" and ""AgentQ"" to be from the same entity. However, in reality, these entities can refer to two different vendors. Additionally, we train our classifier in a multi-class classification setting, assuming that ads correspond to only one individual vendor account. However, our experiments uncover the existence of copycats on Darknet markets. In reality, it is always possible for multiple vendors to co-exist with similar vendor names; hence, any supervised approach will only generate skew results. In the future, we plan to look toward contrastive learning approaches (Pan et al., 2021; Zhou et al., 2021b; Wegmann et al., 2022) to avoid these assumptions.
Architectural limitations: This research establishes a BERT-base-cased classifier to verify migrating vendors across existing and emerging Darknet markets. While we acknowledge that using a bigger BERT model with a sliding window may improve our classification’s performance, given the resources at our disposal, we decided against it. Moreover, as mentioned earlier, most of the ads used in this research are in English, with a few exceptions where the vendors use multiple languages. Therefore, applying a multilingual transformerbased model to the classification task (Wang and
Banko, 2021) can improve our approach’s performance.
Unsupervised and HR settings: As described in the assumptions, the core of our approach lies in the availability of gold labels. VendorLink utilizes the supervised pre-training step to perform knowledge transfer and text-similarity tasks. Therefore, our approach suffers a significant limitation in the absence of these ground labels / unsupervised settings. Furthermore, as described in A.1.3, our approach could not scale well to verify vendor migrants in HR emerging datasets. In the future, we plan to expose VendorLink to contrastive learning approaches to learn universal representations and overcome the problem.
Diverse Advertisements: In the semi-supervised task, we compute the likelihood of two vendor accounts being from the same entity by calculating the similarity between the advertisements of the two vendors. Since one of the novelties of this research lies in the direction of End-to-End training, we have avoided using handcrafted labels for applying content control to generate content-independent style representation. However, as explained in section 4.2, an advertisement from the drug category can be very different from that of the weapon category. Therefore, in the future, we plan to train another classifier to classify Darknet advertisements into different trade categories before performing the vendor-verification task.
XAI limitations: eXplainaible Artificial Intelligence (XAI) is integral in promoting trust and understanding amongst the end-users. From LEA’s perspective, its absence can be viewed as arguably negligent and unreliable. While we acknowledge that our approach currently lacks an XAI feature, in the future, we plan to build upon our experiments in A.1.5 and establish a reliable approach for understanding and explaining our model’s decision."
362,"In terms of the test sets, due to time, labor, and financial limitations, we are unable to construct large-scale test sets of the same size as the original, so the domain balance in the test sets is not fully considered, but the uniformity of writing style might have slightly alleviated this issue. In terms of the method, we empirically explore the possibility of chain-of-thought application in text generation. However, due to the stronger openness of generative tasks compared to pure reasoning tasks, generated summaries might be more sensitive to the form of chain-of-thought, which is a key point worth further optimization."
363,Limitations
364,"Section 8
7 A2. Did you discuss any potential risks of your work? We use the common public datasets, models, and scripts provided by Huggingface to investigate a more efficient computation method for Shapley Values."
365,"Section Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
366,"We acknowledge a range of limitations of our work. As discussed in Sections 3 and 6, overall annotator agreement ranged from fair (frame annotations) to low (entity role annotations). We do not view this as a limitation per se, again pointing to the recent literature on the value of human label variance pointing at a potential loss of valuable information if we overly focus on arriving at a single gold label per instance, with high confidence (Plank, 2022; Pavlick and Kwiatkowski, 2019). Future modeling work involving entity labels should, however, carefully inspect the role label variation, and potentially remove or aggregate selected annotations, before incorporating the labels as signal into predictive models. We explicitly refrained from training model in this paper to avoid the risk of training a predictor on an unfavorable noise-to-signal ratio.
Our data set focuses on English-language news reports, sampled from 2017 to 2019 in mainstream media outlets in the US and UK, and as such focuses on cultures and communities which are already well-resourced and well studied. With climate change being a global challenge, broadening data sets, annotations and models to more languages is an important direction for future work. We explicitly caution against projecting annotations across languages without careful validation as we expect the manifestation of framing, views
on entities (or sheer set of dominant entities) to vary widely across countries and communities.
Even within our English study, we acknowledge that the size of annotated data is small for NLP scales, and an extension in the future is desirable. A related current limitation is the focus on just a single issue (climate change) and validation of our narrative framing framework for other issues is an important direction for the future. Finally, the annotation process was slow and costly, relying on trained, highly educated annotators with constant monitoring, rendering larger scale annotations challenging, on the one hand. On the other hand, we will release upon acceptance our annotation procedure including the full codebook with instructions, which have been optimized over several rounds of annotations and we hope can support more efficient annotation in the future.
Ethics statement
This study was approved by the University of Melbourne ethics board (Human Ethics Committee LNR 3B), Reference Number 2023-22109-37029- 4, and data acquisition and analysis has been taken out to the according ethical standards. We hired four local annotators who were paid an hourly rate of $53 AU in line with the casual research assistant hourly rates set up in the University of Melbourne collective agreement.
We will release the Narrative Framing Corpus comprising of 428 news articles annotated with frame labels, entities, their narrative roles and stakeholder categories. We also publish the raw (nonaggregated) annotations. Our data set builds on news articles from the NELA corpora 2017-2019, which were released to the public domain (license CC0 1.0).14 We release our code and Narrative Frames Corpus under a MIT license."
367,Section 7
368,"Section ""Limitations"""
369,"Though DuNST works well, it has four kinds of limitations as follows:
• Decelerated training process. As with all other Self-training methods, DuNST also needs to reproduce pseudo labels and pseudo text at each ST iteration. Since the pseudo text (both hard and soft) is generated in an autoregressive manner, which is hard to be done parallelly, leading to longer training time.
• Reliance of unlabeled in-domain text. As we discussed in Sec. 4, though our soft pseudo text brings non-trivial improvement, the overall performance of all ST methods still relies on pseudo labels from unlabeled text. When unlabeled text is extremely inadequate or even unavailable (e.g., low-resource scenarios), how to better utilize pseudo text for further improvement is an open challenge.
• Efforts of tuning noise level. As we discussed in Sec. 4.8, the noise level τ is essential for a balanced performance, which should be carefully tuned for each downstream task.
• Task generalization and scalability. We mainly investigate controllable NLG in this work, while it is still unknown whether our method works for other NLG tasks, like NMT and Text Summarization. Besides, as we analyzed in Sec. 3.4, ST actually acts as a kind of regularization and smoothing. How to apply this paradigm to super large PLMs (e.g., GPT2XL and GPT3), where the supervision signals from limited labeled data become extremely weak, is also an open question."
370,"Linguistic studies have shown that respective readings are not necessary to have two coordinate structures in the same sentence (Dalrymple and Kehler, 1995). Both WikiResNLI and NatResNLI have only one sentence in the premise and do not exhaust all possible and complicated realizations of respective readings. However, we are able to discuss and investigate LMs’ generalizability with “respectively” with three constructions, i.e., 1S1O,
1S2O and 2S1O. Our experiments are English-specific and are limited to LMs that can be run with an academic budget. However, our conclusions about the generalizability towards respective readings should be viewed as language-agnostic given there are linguistic constructions under-discussed in many other languages and it is worth researchers’ attention to study them."
371,"Section 7
7 A2. Did you discuss any potential risks of your work? We provide a NLI dataset and we do not see a risk for that"
372,"One limitation of this study is its scope, which covers two downstream tasks and two types of demographics (race and gender). The binary gender definition we used excludes other genders that do not fall under male or female. In the case of race, we explored only African American race (proxied
by African American English), which excludes biases related to other races, and is a US-centric view of racial bias. We did not investigate other types of bias, such as religious bias. Furthermore, our method was tested on datasets with short texts, and it is unclear how it will perform on longer texts. The experiments were conducted on datasets in English, and it is unclear how our method will work on languages that are morphologically rich."
373,"Unnumbered section named ""Limitations"""
374,"Section 4.1 (explicit list of controlled covariates in selecting datasets), Section 5: Results (discussion of both the cases where our objectives improved compared to baselines, and where they did not). Section Limitations (enumerating a list of potential uncontrolled covariates of our experiments)."
375,"The proposed CRINGE loss can be used to mitigate some of the identified problems of large language models, for example, the use of toxic language (Dinan et al., 2019a; Wulczyn et al., 2017; Xu et al., 2021b) or contradictory statements (Roller et al., 2021; Nie et al., 2021). Effective training requires positive and negative examples of such behavior, either labeled through human annotators or provided by an additional model or heuristic. The quality of the data bounds the success of the training approach. In our experiments, we assume non-adversarial label annotation. In real-world interactions with a chatbot, it is likely to experience at least some “trolls” that provide wrong feedback on purpose (Ju et al., 2022). Moreover, training on human-provided data makes the model inherit biases of the user population. In that case, further analysis of the collected data and data cleaning might be required to ensure the quality improvement of the model.
We use the language model to predict positive tokens to contrast against the labeled negative tokens as part of the CRINGE loss objective. Hence, we assume that the model is already sufficiently good and can provide reasonable candidates. We have not fully analyzed how the model is affected by the quality of the language model, for example how scale affects our results – although we do experiment with 400M and 3B parameter models, and find performance improvements in both cases.
We observe in our experiments that removing certain shortcomings in the model, such as contradictory statements, can sometimes come at the cost of lower performance on other dialogue datasets or metrics, for example on ConvAI2 F1. This tradeoff can be controlled by the α-value of the CRINGE loss, or the number of iterations performed."
376,"Limitations
7 A2. Did you discuss any potential risks of your work? Except for the limitations, we do not yet find any other risks the proposed model would have."
377,"This paper aims to make advancements toward automatically identifying fine-grained depressive symptoms from memes shared on social media. Although we used only those memes shared by users who self-declared themselves as depressive, we did not conduct any further clinical assessment to judge whether the user was depressive or not, nor we clinically evaluated their depression severity. Therefore, deploying this system without expert advice could compromise patient safety and lead to undesirable outcomes. We further acknowledge that determining the depression symptoms based on the visual and textual cues present in the meme can be subjective, and therefore the created gold-standard dataset may contain explicit and demographic biases. In this study, we focused on training the models using only the social media data, leaving their performance unchecked if tested on other medical data sources. Finally, our study is not indented to provide any diagnosis; instead, we envision the methods we provide being used as aids by healthcare professionals."
378,"limitations section in page 9.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
379,After Concluding Remarks; before References Pg 9. Section* number unmarked.
380,"Limitations section (unnumbered).
7 A2. Did you discuss any potential risks of your work? Our work analyzes existing models (RoBERTa) on existing datasets, predicting their performance for out-of-domain data. Rather than introducing new models and risks, we hope that these results can be used to reduce the potential risks of applying existing models in cases where they might perform poorly."
381,"The human evaluation shows that our pipeline performs worse than humans in the process of information retrieval and synthesis 67.5% of the time, which still leaves room for improvement (see appendix G for future works)."
382,"Although our proposed method achieves promising performance in the novel direction of ZeroCQG, it still has the following limitations: (1) retrieval-based conversation synthesis is limited to predefined question-answer pairs and may introduce repeated question-answer pairs with small differences (discussed in Appendix B.1). Future work may include exploring generative-based approaches to generate new and diverse questionanswer pairs for better conversation synthesis. (2) Existing question transformation only explore one of the most common conversational characteristics, anaphora. However, other different characteristics, such as ellipsis, should also be considered in the future. (3) The conversation prompting has limitations when the domain gap becomes large (discussed in Sec. 4.4). More robust prompt learning should be explored in the future."
383,"Our work follows the general assumption that the training and test set contain the same list of predefined entities. Without additional or necessary modifications, the few-shot or zero-shot capability of the model is expected to be limited. Future work includes exploring prompt-based architectures to unify pre-training and fine-tuning into the same query-based procedure."
384,Sec 6
385,"Section Limitations
7 A2. Did you discuss any potential risks of your work? There are no potential risks in our paper."
386,"section of its own name
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
387,"Our study has several limitations. First, demographics may not be the best construct for positionality, as there may be variability of beliefs within demographic groups. Assuming that there is homogeneity within demographic groups is reductionist and limited. Rather, capturing an individual’s attitudes
or beliefs may be a more reliable way to capture one’s positionality that future work can investigate.
Study annotators could also purposefully answer untruthfully, producing low-quality annotations. We address this risk by using LabintheWild. LabintheWild has been shown to produce highquality data because participants are intrinsically motivated to participate by learning something about themselves (Reinecke and Gajos, 2015). However, as is the case for all online recruiting methods, our sample of participants is not representative of the world’s population due to the necessity of having access to the Internet. In addition, there is likely a selection bias in who decides to participate in a LabintheWild study.
Pearson’s r may not fully capture alignment as it does not consider interaction effects between different demographics (i.e., intersectionality). Thus, there may be additional mediating or moderating variables that may explain the results that our analysis does not consider. We also took the average of the annotations per group, which could mask individual variations (Talat et al., 2022). Also, having a low number of participants from specific demographic groups may limit how well the results generalize to the entire group; further, it may risk tokenizing already marginalized communities.
As part of our study, we apply NLPositionality to only two tasks which have relatively straightforward annotation schemes. It may be difficult to generalize to other NLP tasks which have harder annotation schemes, especially ones that require a lot of explanation to the annotators, for example, natural language inference (NLI) tasks.
Our approach is evaluated and works the best for classification tasks and classifiers. Generation tasks would need more careful annotator training which is difficult to achieve on a voluntary platform without adequate incentives. Having annotators use one Likert scale to rate the social acceptability and toxicity of a situation or text may not be a sufficient measure to represent these complex social phenomena. To reduce this threat, we provide detailed instructions that describe how to provide annotations and followed the original annotation setup as closely as possible."
388,"There is a fundamental uncertainty in whether Backpack language models will continue to scale with parameters and data and be viable alternatives to Transformers at larger model scales. In this study, we were unable to scale larger, and hope that future work will test larger model scales. In a similar vein, we do not verify that Backpack language models perform well across multiple languages. We also do not consider, e.g., finetuning Backpacks on other tasks, or masked language modeling—there is a wide range of possible uses that remain to be verified.
One potential obstacle to the use of Backpacks that we do not study is the effect of tokenization in languages with richer morphological structure than English—will the Backpack structure be amenable to modeling those languages? This may be difficult because, intuitively, the interpretability and control of Backpacks relates to the semantics of individual tokens. Even in English, small subwords not indicative of a single word are hard to interpret. What we hope to have provided is a sufficient set of experiments to motivate the further exploration of Backpacks."
389,Section 6
390,"We report our limitations in the last section.
7 A2. Did you discuss any potential risks of your work? We do not think there are any potential risks of our work."
391,"In the Limitations section
7 A2. Did you discuss any potential risks of your work? Our work does not involve any sensitive data or sensitive tasks."
392,The Limitations section.
393,"In this work, we have presented results that help inform us what tasks, methods and metrics are best suited for monitoring as well as methodologies and empirical information about the current set of models. We provide detailed information of how these results can be reproduced, to the extend that research have access to the PLMs in question, but these results have limitations, in order to reduce costs, many languages were not evaluated which might have left unforeseen patterns not discussed in this work. Moreover, few-shot learning, in particular, could exhibit large variance if different prompts were chosen, or a different set of exemplars chosen. Because of the high costs involved our work does not explore the performance difference when multiple sets of hyper-parameters were chosen. On the conceptual level, we make the assumption that system-level improvements on our tasks translate to downstream usefulness. While prior work suggests that this is the case, tools like chatGPT have significantly expanded the possible application space beyond the realm of “typical” NLP tasks, and we don’t know how well our findings generalize to this space of tasks."
394,"lilGym uses synthetic visual stimuli, which does not reflect the complexity or characteristics of realistic visual observations. This is critical for our ability to control the environment and provide a lightweight and accessible RL benchmark. Our goal is not to provide a resource for the development of methods that aim to handle realistic visual input, and lilGym is not suitable for this purpose. The limited number of colors, shapes, and sizes used limits the visual and lexical complexity of the data. The synthetic nature of the data and the
modular library of functions we use allow to relatively easily extend the environment (e.g., with new colors). This will require collecting additional natural language data. In this work, we opted to rely on the NLVR data without further expanding it. Some annotators of the original NLVR data adopted annotation strategies that led to repetition of some common phrases (e.g., starting statements with there is). While this creates some implicit patterns in the data, Suhr et al. (2017) showed that NLVR demonstrates high semantic diversity and compositionality. Finally, lilGym includes English data only. Expanding this data to other language is an important direction for future work. Translating the data is a feasible low-cost solution, because the program annotations will not require updating.
Ethics Statement
We paid U.S. standard market wage to our programmers (Appendix B). The rate was determined by the workers. The lilGym environment and data as is are intended to be used for research, including algorithm development and evaluation, and not for development of models to be deployed.
Commented for anonymous submission"
395,"Although we have proven that our work can significantly alleviate concept bias and extract highquality and new concepts, it also has some limitations. In this section, we analyze three limitations and hope to advance future work.
Model Novelty Although KPCE can effectively mitigate the spurious co-occurrence correlations between entities and biased concepts, the proposed framework is not entirely novel. The novelty of our work is to conduct the first thorough causal analysis that shows the spurious correlations between entities and biased concepts in the concept extraction task. After defining the problem and SCM of concept extraction in § 3.1, we propose a promptbased approach to implement the interventions toward the SCM to elicit the unbiased knowledge from PLMs. Previous work in language prompting mostly guides the PLMs with prompts but is unaware of the cause-effect relations in its task, which may hinder the effectiveness of prompts. We hope our work can inspire future work to utilize language prompting from a causal perspective.
Topic Classification Although the topics obtained by clustering are mostly mutually exclusive, there are still cases where an entity can be classified into multiple topics. Therefore, considering only one topic for the entity excludes the correct concepts.
Threshold Selection We only reserve concepts with confidence scores bigger than the selection threshold (§ 4.2), which can hardly achieve a satisfactory balance of precision and recall. If we select a relatively big threshold, we can get more accurate concepts but may lose some correct ones. If the recall is preferred, precision might be hurt.
We suggest that future work consider these three limitations to achieve better performance in the CE task."
396,Limitations
397,"First, our method needs to check all spans in the given sentence and build a table for each sentiment polarity, and is therefore difficult to handle too long sentences. Another limitation of our work is that for the different aspects in the same sentence, we need to rebuild the tables."
398,"In this work, we experiment with GPT3, T5, and DeBERTa. Other large pretrained LMs, such as PaLM (Chowdhery et al., 2022), is not covered in this work. We do not experiment with methods such as fine-tuning GPT3 due to the computation cost. The main purpose of this work is to uncover and analyze the fundamental limitations of LMs on symbolic and arithmetic induction instead of improving their performance of reasoning tasks, so we do not directly compare the mitigation methods with the previous work such as scratchpad (Nye et al., 2021) and (Wei et al., 2022) in our experiments. We leave more advanced methods for future work."
399,"Right after Section 10
7 A2. Did you discuss any potential risks of your work? This work is improving the quality of text generation systems. We believe that the risks of these methods are essentially the same as the risks of broader text generation systems, which have been documented at length in other publications."
400,"There are majorly two limitations: Firstly, in this work, we only consider the current-sentence text context-related prosody. In future work, we will focus on improving the inter-sentence prosody to achieve coherent, expressive TTS for long-form text. Secondly, other variables are not considered during the contrastive pre-training. One can explore similar approaches that connect prosody to other conditions such as speaker, emotion, etc."
401,6.Limitations
402,"This paper takes into account the temporal semantic variations of words and proposes a method to learn dynamic contextualised word embeddings by timeadapting an MLM using prompt-based fine-tuning methods. In this section, we highlight some of the important limitations of this work. We hope this will be useful when extending our work in the future by addressing these limitations.
The learned dynamic contextualised word embeddings are limited to the English language, which is a morphologically limited language. Therefore, the findings reported in this work might not generalise to other languages. However, there are already numerous multilingual MLMs such as mBERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al., 2020), to name a few. Extending our work to multilingual dynamic contextualised word embeddings will be a natural line of future work.
Dynamic contextualised word embeddings represent words as a function of extralinguistic context (Hofmann et al., 2021), which consider both time and social aspects of words. However, in this paper we focused solely on the temporal aspect and ignored the social aspect. Extending our work to take into account the social semantic variations of a word is an important future direction.
Due to the costs involved when fine-tuning largescale MLMs, we keep the number of manuallywritten and automatically learnt templates to a manageable small number as shown in Table 1 in §3.4. However, it remains to be evaluated the impact of increasing the number of templates on the performance of the proposed method."
403,"Section Limitations
7 A2. Did you discuss any potential risks of your work? We utilize existing PLMs and datasets for improving text generation, without creating new models or datasets."
404,"Our methods are currently trained and tested on two SVHR datasets. Gender biases and unethical hashtags could exist in the datasets, which may cause the model trained on these datasets to generate these biases. Besides, although our methods are not language-specific, we only choose the English dataset due to its rich resource. Furthermore, we regard the user tags as the hashtags for SFVD2 in our experiments and there are small differences between the user tags and hashtags. Experiments on more diverse languages and datasets are needed in the future.
As an initial work for SVHR, in our task formulation, our model only take the video and its description as input to predict hashtags and ignore user preference in hashtag recommendations. Exploring user preference is also a promising direction for future work.
We used up to eight A100 GPUs per experiment and it took more than one day to run experiments on SFVD2. More efficient models are needed for real-world applications. Besides, we hope our experimental results can be used as benchmarks for comparison in future work to avoid repeating training."
405,"Section 7 ""Limitation"""
406,"One of the limitations is the selection of hyperparameters. At present, we determine the optimal hyperparameters based on the performance of the selection methods on an existing bilingual dataset. For example, to identify the appropriate utterances to be translated from English to German, we would adjust the hyperparameters based on the performance of the methods on existing datasets in English and Thai. However, this approach may not always be feasible as such a dataset is not always available, and different languages possess distinct characteristics. As a result, the process of tuning hyperparameters on English-Thai datasets may not guarantee optimal performance on English-German datasets. As a future direction, we intend to investigate and develop more effective methods for hyperparameter tuning to address this limitation."
407,Limitations Section
408,"One limitation of our method is that when there are rules of different lengths, the final result is decided by ensemble, not by building a model to generate a single rule with the best length. The second way is more natural and important because figuring out the length of the rule is also a key part of symbolic reasoning. However, it requires more parameterization (for example, the length of the rule could be a parameter) and a more advanced way to optimize. The investigation of the above method is left for future works."
409,"The last section.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
410,"Comparison with GPT-3: There are growing concerns in the research community about the lack of open availability of GPT-3. There are several versions of the model and the details of the training data used for each version are largely unavailable. Direct comparison with GPT-3 is, therefore, becoming increasingly challenging. In this work, we compare against the ‘text-davinci-001’ version of the GPT-3 model and note that newer models might do better. However, extracting the best performance from GPT-3 is beside the point of our work. We believe that as a community, we must investigate alternative approaches that do not only rely on scale.
Undesirable Generations: Language models, large and small, have been shown to be prone to generating toxic text (Gehman et al., 2020). I2D2 relies on GPT-2 XL could also potentially generate toxic statements. While the trained critic model is able to filter out most toxic generations, we estimate the proportion of undesirable generations using the Delphi (Jiang et al., 2021) model. We find that ∼ 1.3% of the generations may not be morally acceptable, either because the statements are not accurate, not verifiable, too restrictive, or they are potentially toxic.
Self-Imitation Iterations: In this work, we only try two iterations of self-imitation due to resource constraints. Exploring the effects of more selfimitation iterations is left for future work. But, based on the performance improvements we observed after two iterations, we hypothesize that the improvements could diminish with each future iteration.
Runtime Efficiency A batch of 32 generations from I2D2 takes 3mins on a single RTX A6000 GPU. NeuroLogic Decoding is the most computationally expensive component. As constrained decoding methods become more efficient, the runtime of I2D2 will also improve. Our focus in this work is to study the quality of generations and we leave runtime efficiency improvements to future work."
411,"7 (after the section of conclusions)
7 A2. Did you discuss any potential risks of your work? Our work cannot produce new contents. Our main goal is to build a state-of-the-art evaluation metric for text generation which shows great generalization ability and interpretability.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 4
3 B1. Did you cite the creators of artifacts you used? 4"
412,"This paper mainly focuses on neural code search models. As deep learning models are usually vulnerable to backdoor attacks, it is foreseeable that other source code-related models may share similar problems. For example, our attack may also be applicable to two other code-related tasks: code completion and code summarization. Code completion recommends next code tokens based on existing code. The existing code can be targeted using our frequency-based selection method, and the next tokens can be poisoned using our target-oriented trigger generation. Code summarization generates comments for code. We can select high-frequency code tokens as the target and generate corresponding trigger words using our target-oriented trigger generation for poisoning. It is unclear how our attack performs empirically in these tasks. We leave the expeirmental exploration to future work."
413,"Our discussion in this paper leaves out the consideration of computability of measures over languages. Specifically, we note that there exist works on computable measure theory developed in the context of theoretical computer science (de Leeuw et al., 1956) and probabilistic programming languages (Roy, 2011). Additional machinery needs to be developed in order for a proper treatment and we leave this for future work.
Another notable limitation is that we exclusively focused on the autoregressive production of language. Importantly, our formalism might not be compatible with other models of language production such as those induced by a PCFG.
Finally, our proofs of Thm. 5.9 and Prop. 5.10 exploit the strictly positive property of the softmax function. Importantly, they do not apply to models with sparse distributions (Martins and Astudillo, 2016; Peters et al., 2019; Martins, 2021)."
414,"Section Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
415,"We discuss the limitations after the conclusion, and before the references.
7 A2. Did you discuss any potential risks of your work? This paper discusses keyphrase extraction, which basically does not bring risks."
416,Section 7: Limitations
417,"In this paper, we propose a new framework that extracts the various aspect of information about given data, relying on the existing model-driven metainformation from the trained models. Hence, if there are some flaws within the used models, such as biased prediction (Sun et al., 2019) or learning of spurious correlation (Liu et al., 2021), then our framework can be directly affected and may have a risk of inheritance or amplification of such problematic behaviors. However, as our framework is not limited to any specific models and metainformation, one can prevent this problem by using the robustly trained models (Sagawa et al., 2020) or introducing more specialized meta-information (Lee et al., 2021a) for these problems. In addition, despite the empirical gains we find, our subset selection method is not theoretically guaranteed to be (or tightly bound to) the optimal set of max informativeness, which remains an interesting direction. A further study is necessary showing that selected samples from infoVerse could lead to low inter-annotator agreement in manual annotation but provide more accurate information than pseudolabels. Abnormality detection using infoVerse, like noisy labels, out-of-distribution, or annotation artifacts, could be interesting future directions.
Broader Impact and Ethical Implications
Our work aims to quantify the data informativeness with multi-perspective for capturing properties that can not be revealed by a single perspective. Especially, infoVerse lends some insight into data by models what we have. Thus, infoVerse has the potential for guiding the construction of high-quality datasets, e.g., removing mis-labeled samples. From these points, it is possible to develop a system or general platform for effectively collecting data like Dynabench6 and Snorkle7. We anticipate that the general platform of infoVerse could be contributing to human-involved machine learning systems.
Although our work empirically demonstrates the improvement over various real-world problems, the current version of infoVerse has a potential risk to be vulnerable to sample undesirable properties (e.g., gender bias (Bordia and Bowman, 2019)) in a dataset, as we construct infoVerse with metainformation measure do not consider such properties. However, it can be easily alleviated by
6https://dynabench.org/ 7https://www.snorkel.org/
adding various measurements which represent ’fairness’ thanks to the extensibility of our framework. Hence, we believe that our proposed method can be personalized to the purpose of data collection."
418,"Although, we uncover and collate a broad-range of stereotypes, it is not without limitations. Firstly, we generate stereotypes using seeds which influence and skew the output stereotypes retrieved. Our coverage could thus be greatly affected and potentially increased with different or more seed stereotypes. Secondly, stereotypes are inherently subjective in nature and even though we do get 6 annotations from annotators residing in different regions, they have a limited world view and might not be aware of all the existing stereotypes. Additionally, certain stereotypes make sense only in context. For example the stereotype (Asians, hardworking) is not offensive by itself but becomes problematic when we compare or rank Asians with other social groups. Moreover, the stereotype (Asians, socially awkward) exists in tandem with the former stereotype which is offensive. Although we do capture regional sensitivity of stereotypes, our work does not capture the contextual information around these stereotypes. For capturing in-region vs out-region stereotypes, we only select annotators from North America but the out-region annotators can belong to any of the other regions as well. That is outside the scope of this work. Additionally, we emphasise that this work is not a replacement to the
more participatory work done directly with different communities to understand the societal context and the associated stereotypes. The complementary usage of our method with more community engaged methods can lead to broader coverage of evaluations of harm (Dev et al., 2023)."
419,"Limitations section, and parts of 8. Discussion"
420,Limitations
421,"First, in this work we limit the experimentation to vertical relational web-tables only, following the format of benchmarks used in TableQA, i.e., WikiSQL and WikiTQ. While we believe that ITR can easily be extended to horizontal entity web-tables, e.g., tables from Wikipedia, we do not expect our algorithm to transparently work on other types of tables that we do not consider, e.g., matrix tables from scientific papers and/or spreadsheets (Wang et al., 2021), where table items can be represented differently. However, this is not a limitation of the algorithm itself and adjusting our assumptions to certain scenarios and type of data can be feasible in the future. Second, ITR selects the relevant table elements by using a question as query. Therefore, it can only be applied for tasks with table-text joint input such as TableQA we showcase in the paper, or also table entailment tasks, e.g., table fact verification. Unfortunately, ITR cannot be used for tasks where table is the only input, e.g., table-to-text task. Finally, while ITR is beneficial for questions that do not rely on table completeness, its effectiveness is limited when, for example, all table cells are required to be predicted. Consider a question that requires cell counting, and the gold cells satisfying the query can be more than what we can feed a model with, e.g., “how many championship did Player A get?” and Player A has won 500 champions. However, this limitation does not arise from our approach and is rather inherited by existing TableQA models in the literature. Indeed, it can be a potential future direction of our work, which requires model innovation and table transformation that focuses on representing the information in a compact form."
422,"(1) In this paper, we tackle the problem of document-level simplification. This consists in simultaneous summarization and simplification. Applying the same model to sentence-level simplification needs to be further evaluated, as sentences naturally due to their shorter length may not require summarization.
(2) In addition, we did not explore various model sizes although we do conduct a fair comparison and show that even with a base-model size SIMSUM performs superior to baselines."
423,Section Limitations
424,"They are in the Limitation Section
7 A2. Did you discuss any potential risks of your work? We do not envision there are potential risks."
425,"We conducted our randomized field study on a single platform (Mental Health America) and in a single language (English). However, MHA is a particularly popular source for mental health resources
with over ten million yearly visitors. In addition, we note that a range of socio-cultural factors might influence how negative thoughts should be reframed and how LMs assisting this process should be developed. Conducting studies on specific communities, including underrepresented communities and minorities, was beyond the scope of this research. Ensuring equitable access of these tools and adapting them to various socio-cultural contexts requires further investigation.
Not all cognitive reframing implementations elicit situations, but we believed it was essential for making the reframe personally relatable. In the future, when an individual uses the system for multiple situations and thoughts, it would be interesting to study how their context can be learned more effectively over time. Due to privacy concerns, we presently do not gather information to link multiple sessions. However, with appropriate ethical considerations and user consent, this approach may be beneficial.
Our focus in this paper was primarily on creating an intervention that is effective in-the-moment. This was motivated by recent clinical psychology research that suggests that such single-session, inthe-moment interventions can lead to significant positive long-term mental health outcomes (Schleider et al., 2022). To integrate a partial longer-term perspective, we assessed the memorability of a reframe, which may be essential for future utility. Nevertheless, evaluating long-term outcomes is critical and forms an important future research direction. Finally, we emphasize that our study does not investigate short-term or long-term clinical outcomes."
426,"Our experimental analysis proved that text regression is a considerably reliable and accurate tool in dating nonliterary papyri. Limitations and challenges stem mainly from the composition of our dataset, which is balanced as far as the dates of the papyri included are concerned, both at the level of the century (approx. 40 records per century) and at the level of the quarter of the century (albeit less strictly and with the exception of the 7th CE). Furthermore, although we retained a substantial text sample of each papyrus, in approximately 1/4 of the records some text was eliminated.
Biases Despite our effort to balance the dataset in terms of dates, biases are present. Since our main concern in collecting the data was for the date distribution, no deliberate selection was made on the basis of the document types. Some types are thus over or underrepresented (e.g. private letters that do not usually bear a date; §6.2). Each type of document has however distinctive linguistic characteristics, such as the level of formality or unusual constructions (e.g. accounts). This uneven typological representation probably affects the performance of the models. Other possible biases in the dataset concern the
15We sampled randomly 100 regressors.
provenance of papyri, the length of their text, and the state of conservation (sizeable portions of missing text or entirely missing parts of the documents).
Chronological analysis of words Chronological analysis of word occurrence is possible if we detect and collect terms only attested in the papyrological material during a limited period. The word ‘denarius’ only appears after the 2nd CE and before the 5th CE, its presence in a text thus means that the text must have been written during this timespan. Likewise a text containing the word ‘indiction’ cannot have been written before the 4th CE. The investigation should also regard the possibility that the models make a prediction for a papyrus based on typical dating formulas present in the text like the name of the ruling emperor. Although our investigation of explanations did not yield any major concerns, a bigger sample of test cases should be created and more explainability methods should be employed (Ribeiro et al., 2016) to make conclusive remarks on this front.
Transcription of papyri is not optional Transcription of the papyri is required (at least partial, but substantial) to reach this high degree of accuracy with our method. Thus, while there are transcriptions available for most already published
papyri, it is less practical for dating unpublished papyri that have not been yet transcribed to a relatively high standard. In that case, image classification on the scripts can provide a less accurate prediction of the date as starting point."
427,"Yes, see the Limitations section after the Conclusion section."
428,"We discuss the following limitations of our work. First, the counterfactual data augmentation procedure we used can only be employed for questions whose answers are named entities. This restricts the applicability of the method as knowledge conflicts can arise for other types of questions, such as Boolean questions (Clark et al., 2019). Extending our framework to other question types will require a new counterfactual data augmentation method.
Second, we conduct our experiments using gold passages – i.e., an oracle retriever. Using retrieved passages, which is often required in real-world applications, may introduce additional challenges when considering knowledge disentanglement. Furthermore, the answerabilty approach presented in section 2.3 mainly serves as a proof-of-concept. It is quite simplistic, because the random context is unrelated to the question in terms of topic and participating entities. The focus of this work is on showing that unanswerable questions significantly boost the disentanglement capabilities of a QA model, and that even a simple approach like the one we took improves the model capability. Future creation of unanswerable examples would include more distracting contexts, that at first glance seem very relevant, but still do not contain the answer.
We note another minor limitation, implied by the high accuracy in the counterfactual case relative to the factual accuracy (see §4.5). This might stem from the model’s ability to identify that the text in the counterfactual examples is somewhat unnatural. It is therefore an indication of a potential limitation of the data augmentation methodology, albeit not a major one, judging by the small magnitude of the differences between the counterfactual and factual examples.
Finally, while our results indicate that models can learn to disentangle contextual and parametric knowledge, it remains unclear what characterizes easy vs. difficult cases for disentanglement. One such attribute, for example, can be the frequency of a given fact in the pretraining data. We view this as an important research question, which we plan to address in future work.
Due to the size of the models, we do not perform multiple trials of training from different initializations to test for significance. However, we do find similar trends across model sizes, which lends further support to the results presented."
429,Limitations section in Appendix A
430,"We present a novel (Target, Stance) pair Extraction task (TSE) for understanding the stance of interesting topics in the wild. There are two potential limitations to our work. First, the mapping module requires a predefined list of targets. Without the predefined list of targets, it is very difficult to understand the correctness of stance labels for the predicted targets in the absence of gold labels. On the other hand, the predefined list of targets makes the entire system end-to-end and automatically evaluable. Second, the process of mapping might become too slow if the number of targets of interest grows bigger. Future works include solving the given limitations and extracting (target, stance) pairs in a unified setting. However, the primary contribution of the work is not to present a fully robust pipeline model but to present a novel, interesting, and challenging task to the community working in stance detection."
431,"Left blank.
3 A2. Did you discuss any potential risks of your work? 9"
432,"Section Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
433,"Unnumbered ""Limitations"" section"
434,"Section 7 Limitations.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
435,"In this paper, we propose a novel SD method, entitled SD-APRR, which expresses negative situations of sarcasm by the potential results and human reactions of the associated events. We employ the COMET to estimate the results and human reactions, and form event-augmented samples with them. We employ those augmented samples as the whole sarcastic texts from the direct access view. We suggest a masked graph-based encoder, enabling to generate discriminative sample embeddings. Experimental results demonstrate that our SD-APRR can achieve competitive performance compared with the existing baseline methods.
We demonstrate two limitations: (1) The datasets used in this work are mostly collected from social media. In the future, we plan to collect sarcastic texts from various sources, such as the literature and films, and conduct more experiments with them. (2) Our exploration of sarcasm theories still has some space to improve. Though the incongruity theory is the mainstream in the community, there are other theories worthy to investigate in the future."
436,Limitations
437,"Our primary limitation is the size of our collected dataset; we have collected a quality dataset which we demonstrated is useful for analysis, but which is too small for training large-scale neural models. However, Section 5 indicates that a training-size dataset may not be necessary, as our question generation model is capable of capturing answer groups
without explicit supervision. Another limitation on our dataset is the relative subjectivity of the task; in completing the annotation, we found that identifying ambiguity and isolating the different underlying questions often involves a Gestalt shift. Once an interpretation of the question is chosen, it becomes increasingly hard to see any other. This makes the annotation task subjective; where one annotator might see ambiguity leading to multiple valid answers, another might see one correct answer group and a number of invalid ones. Thus, the annotations in our dataset represent a high precision subset (rather than a high-recall subset) of all the possible ambiguous datapoints. This subjectivity also risks introducing annotator bias (including the author’s own biases) into the data; we acknowledge that the vetting steps by the authors may have compounded this further. We are also limited by the quality of the underlying data. Our dataset builds on the VQAv2 dataset (Goyal et al., 2017) and the annotations from Bhattacharya et al. (2019), both of which were large-scale annotation efforts intended for training. Due to their scale, individual datapoint quality is often quite low; this was one factor contributing to the need for post-hoc cleaning in the annotation process."
438,"Please see Section ""Limitations"" after the text.
7 A2. Did you discuss any potential risks of your work? There is no potential risks mentioned in ""Responsible NLP Research checklist guidelines - A2"" in our work."
439,"While the LAIT framework can significantly reduce the computation required for large-scale sentencelevel reasoning and classification tasks, we do foresee some limitations in its use. Caching pertoken representations for large numbers of text segments leads to a dramatic increase in memory requirements, which could be prohibitive for extremely low-compute end users. We also note that LAIT can further exacerbate segment-level bias in datasets. While we believe that careful data curation approaches ameliorate this issue, the risk of bias is not always known to downstream users and as such corrective datasets may not always be available. Finally, LAIT can increase the cost of training because the optimal degree of independence is not known until all LAIT-p models are evaluated, though in practical settings (1) it is possible to perform a binary search of LAIT configurations because performance generally decreases monotonically as p increases; (2) even a naive rule of setting p to a quarter of the model’s depth seems to provide some immediate gains while preserving 99% of the accuracy in all our evaluated tasks; and (3) inference-time cost improvements will far outweigh training costs."
440,"We discuss limitations in section 5.2, as well as an un-numbered final ""Limitations"" section."
441,"Although based on the Transformer model, our methods also apply to various DNN modules, including CNNs, Poolings, and their compositions.
The applications of the proposed method in computer vision are left for future work.
An obvious limitation of this work is that we only verify our algorithm on models activated by ReLU. This issue can be alleviated because our algorithm is theoretically compatible with any piecewise linear activation function. For other functions in the ReLU family, such as the GELU (Hendrycks and Gimpel, 2016) used by BERT (Devlin et al., 2019; Liu et al., 2019), we replace the activations with ReLU, then fine-tune on downstream tasks and pretrain tasks (Appendix E). Our algorithms bog down on more complex nonlinear functions (e.g., sigmoid and tanh). It’s intuitive to fit these nonlinear functions with ReLU-activated FNNs. However, this leads to additional computational and space complexity, which degrades performance after fitting."
442,We discuss the limitations in Lines 552 - 599.
443,"Our dataset creation process - introducing unanswerability into a dataset of answerable KB questions by deleting KB elements - limits the nature of unanswerable questions. All of these become answerable by completing the provided KB. However, other kinds of unanswerability exists. Questions may involve false premise, for example, C. Manning works at which European University?, or may not even be relevant for the given KB. We will explore these in future work.
Complete training and inference for each model with our dataset size takes 50-60 hours. As a result, generating multiple results for the same models in the same setting was not possible and our results are based on single runs. However, using multiple runs with smaller dataset sizes we have seen that the variance is quite small. Also, the dataset creation involves sampling KB elements for deletion and as such the generated dataset is one sample dataset with unanswerability. This is unfortunately unavoidable when creating one benchmark dataset."
444,"As this is the first large-scale analysis of client reactions in online mental health counseling, there is huge room for future improvement. Here we only list a few problems that we would like to address in the short future. First, although our annotation framework is comprehensive, the data labeled is quite imbalanced. In some rare classes, there are fewer than 50 instances, making it difficult to conduct an in-depth analysis, let alone train an accurate classifier. Therefore, our analysis mostly focuses on the Extending and Defending behaviors. We will label more data so that rare cases can be better understood and classified more accurately. The accuracy of a classifier is important for real-life applications because it has the potential to mislead counselors. Second, we only have one short post-survey, which limits our coarse-scale analysis. We are adding more and richer post-surveys. Third, while we hope that the lessons learned can be applied to everyday conversations, our analysis has only been limited to psycho-counseling. The lessons learned will be tested against a wider range of use cases. It is important, however, not to overgeneralize our findings as this may harm the naturalness of our daily conversations. After all, the psycho-counseling process is a very special type of conversation."
445,Section 7
446,Limitations section
447,LimitationSection discusses the limitation of our work
448,"Like existing short text clustering methods, we assume the real cluster number is known. In the future, we would like to explore a short text clustering method with an unknown number of clusters. Moreover, the time complexity of self-adaptive optimal transport is O(n2), we are going to seek a new computation to reduce the complexity."
449,"In section Limitations.
7 A2. Did you discuss any potential risks of your work? Our work is a very basic and general research work, and is generally risk-free."
450,"As we cannot control for all confounding variables when examining the correlates of the most effective contrast sets, we only claim to identify trends, not causality, between calibration set characteristics and downstream performance. For instance, the top beams, on average, have higher relevance. As such, for each strategy, we record all key set characteristics and focus our analysis on observing trends between set characteristic values and downstream performance across all experiments, not simply within each Selection Type."
451,"Left blank.
3 A2. Did you discuss any potential risks of your work? 8
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 4
3 B1. Did you cite the creators of artifacts you used? 4
3 B2. Did you discuss the license or terms for use and / or distribution of any artifacts? 4"
452,"Annotation speed for mention detection and coreference is dependent on many variables like annotation interface, domain expertise of annotators, annotation style, document length distribution. So, while our finding that coreference resolution is approximately 2X slower to annotate than mention detection held for two domains (i2b2, CN), there are many other variables that we do not experiment with.
We also experiment with transfer between domains with varying semantic similarity and annotation style similarity. But, our notion of annotation style is narrowly focused on types of mentions that are annotated (i.e. singletons, domain applicationspecific mentions). However, since our method is focused on mention detection, our findings may not hold for transfer to annotation styles with different notions of coreference linking (i.e. split-antecedent
anaphoric reference (Yu et al., 2021)). We also focus on one common coreference architecture Lee et al. (2018) with encoder SpanBERT. However, there have been more recent architectures surpassing the performance of Lee et al. (2018) over benchmark ON (Dobrovolskii, 2021; Kirstain et al., 2021). Our key finding that transferring the mention detector component can still be adopted."
453,"Even though our generalized UD can get comparable performance on some generative tasks, generalized UD may not handle certain complex generation tasks very well (e.g., summarization) We leave expanding UD to solve a broader range of generative tasks and achieve greater performance advantage as our future work."
454,"The contribution of this paper is mainly theoretical. Like most of the POS identification algorithms, the optimization of a criterion among the space of all partitions requires the use of heuristics, and finding the optimum is never guaranteed. Additional work is required before a generalization model that is efficient in practice can be obtained."
455,"section 7
7 A2. Did you discuss any potential risks of your work? We cannot imagine any risk
3 A3. Do the abstract and introduction summarize the paper’s main claims? 3
7 A4. Have you used AI writing assistants when working on this paper? 7
B 3 Did you use or create scientific artifacts? section 5"
456,"Limitation part.
7 A2. Did you discuss any potential risks of your work? There is no potential risk in our work."
457,"Section ""Limitations"" (7th Section)"
458,"SWIPE focuses on the English language. Although it is possible that some aspects of the work – such as the edit categorization – might transfer to the study of text simplification in other languages,
we focus on the English language. As of the writing of this paper, there is no equivalent of Simple English Wikipedia for other languages on Wikipedia, and creating similar resources for other languages would require finding other resources.
Difficulty in Retracing Original Editing. By matching revisions of Wikipedia pages that are factually aligned, and working with SEW editors to annotate the edits, we attempted to match the process used to create the resource. It is however not possible to recruit all 5,000+ SEW editors and for some page pairs the annotations are another editor’s best attempt to reconstruct the intended edits by the original editor.
Improving Annotation Reproducibility. The analysis we conduct in Section 4.2 reveals that our annotators achieve moderate agreement on samples repeatedly annotated. More detailed analysis reveals that agreement is generally strong from common edit categories such as Lexical Edits, semantic deletions, or sentence splitting, but is lower for more infrequent categories. Better training of annotators on tail categories could therefore likely improve annotation. We also found that discussion amongst annotators of a sample often led to eventual consensus. Therefore collecting multiple annotations per sample, and allowing for discussion when multiple interpretations occur could help improve annotation quality, but at an increased cost."
459,"In this paper, we conducted investigation on MPNN-based KGC models. MPNN-based models learn the node representations through aggregating from the local neighborhood, which differ from some recent path-based works that learn pair-wise representations by integrating the path information between the node pair. Moreover, we mainly focus on the KGC task which is based on knowledge graph, and thus other types of graph (e.g., homogeneous graph) are not considered. Therefore, our findings and observations might not be applicable for other non-MPNN-based models and non-KGC task."
460,"Long documents, intuitively, have more possible translations than short documents, so a dynamic number of generated translations may be a better choice when augmenting the data, which balances the training cost and the performance gain. Another potential solution is to sample a few translations and force the MT model to match the dynamic distribution of the DA model using these translations as decoder input, similar to Khayrallah et al. (2020). Such dynamic sampling and matching could potentially be used to increase training efficiency. We do not investigate the solution in this paper and leave the exploration of this topic to future work.
Target-side augmentation can potentially be applied to other seq2seq tasks, where the data sparsity is a problem. Due to the limitation of space in a conference submission, we will leave investigations on other tasks for future work."
461,"The Limitations section
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
462,limitations
463,"Our unsupervised speech recognition approach requires tools to phonemize text for the language of interest. Phonemizers are not available for all languages and this presents a bottleneck. To address this, future work may develop phonemizers for more languages, explore phonemization approaches that generalize across languages, or wav2vec-U 2.0 model training with graphemic text units such as letters.
We train bilingual unsupervised machine translation models with 2.1B English sentences and at least 46M sentences for the non-English language. For extremely low-resource languages, collecting millions of sentences for model training can be challenging. The feasibility of mBART-based online back-translation approach in this setup remains to be validated."
464,"A limitation of our work is that our Graph Traversal Algorithm (Section 3.1) is a heuristic and unlearned algorithm. This leads to a number of nodes after being selected by this algorithm are not suitable for the model to generate conversational questions, and are eventually filtered out by other modules. Future works can focus on more advanced techniques to guide the model to select the nodes such as Graph Neural Networks (Wu et al., 2020). Furthermore, our algorithm to select the relevant turns in the conversational history to generate the conversational questions is a heuristic of selecting a maximum of three previous turns. This heuristic may not be optimal for the model to gather necessary information from history to generate conversational questions in the next turns, as discussed by Do et al. (2022)."
465,"Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
466,"Our f -DISTILL variants are less efficient to train than SeqKD and ENGINE, as we require the teacher’s soft probabilities instead of hard, sampled sequences. However, our methods achieve a significant performance improvement, and more importantly, the additional training time does not affect inference when the model is deployed. This follows the spirit of knowledge distillation in general, i.e., to obtain a small and efficient model for deployment.
Another potential threat to validity is that we have not reported multi-run statistics. In our preliminary experiments, we ran our approach multiple
times and found results were generally consistent. Due to our excessive experimentation (estimated at 2000 GPU hours), it is not possible to run each model multiple times. We instead adopted a wide range of established automatic metrics, consistently showing the effectiveness of our approach. We further conducted in-depth analyses to better understand our proposed framework. We deem multi-run statistics not crucial to this paper, as this paper does not purely focus on empirical analysis. Rather, our main contributions lie in the novel machine learning framework, f -DISTILL, and the theoretical connections between step-wise and sequence-level f - divergence functions."
467,"The limitation section is right after the conclusion in Section 5.
7 A2. Did you discuss any potential risks of your work? Our work focuses on knowledge distillation for small models. It does not impose more risk than other machine learning/NLP research."
468,"Please see section Limitations.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
469,The limitations section after conclusion
470,Limitations
471,"Some Language families in Africa not covered For example, Khoisan and Austronesian (like Malagasy). We performed extensive analysis and experiments on Niger-Congo languages but we only covered one language each in the Afro-asiatic (Hausa) and Nilo-Saharan (Dholuo) families.
News domain Our annotated dataset belong to the news domain, which is a popular domain in UD. However, the POS dataset and models may not
generalize to other domains like speech transcript, conversation data etc.
Transfer results may not generalize to all NLP tasks We have only experimented with POS task, the best transfer language e.g for non-Bantu NigerCongo languages i.e Wolof, may not be the same for other NLP tasks."
472,"In the limitations section.
7 A2. Did you discuss any potential risks of your work? Left blank."
473,"In this section, we make a clear discussion of the limitation of our work. Our work mainly leverages a pre-training scheme to enhance the encoding of speech for video grounding. However, the adopted audio data (i.e. Libri Speech) for pre-training are different from the one in the grounding dataset (i.e. ActivityNet Speech). This could lead to performance degradation due to the domain gap. The findings could inspire the researchers to explore a better pre-training strategy to learn domain-invariant and effective speech representations for grounding."
474,Section Limitations (Line 558-570)
475,"Although the proposed method has verified the feasibility of the idea that constrains both naturallanguage and cross-modality spaces together, it is still necessary to explore more ways to better combine the output of two encoders. Third, our method involves multiple offline knowledge retrieval processes, such as retrieving relevant Wikipedia passages, which will make it difficult to deploy our model as an online model."
476,Limitations Section
477,"This paper has three main limitations worth noting. First and foremost, while our paper aims to model the social context in which a message is said, the current context is limited to only the parties’ relationship. In practice, the social context encompasses a wide variety of other factors, such as the sociodemographics of the parties, the culture and setting of the conversation, and the history of the parties. Even relationships themselves are often much more nuanced and the appropriateness may vary widely based on setting, e.g., statements said between spouses may vary in appropriateness when made in public versus private settings. These contextual factors are likely necessary for a full account of the effect of social context on how messages should be perceived. Our work provides an initial step in this direction by making the relationship explicit, but more work remains to be done. Future work may examine how to incorporate these aspects, such as by directly inputting the situation’s social network as context using graph embedding techniques (Kulkarni et al., 2021), where the network is labeled with relationships (Choi et al., 2021), or by modeling relationships particular types of settings such as in-person, phone, texting, or other online communication, which each have different norms.
Second, our data includes annotations on a finite set of relationships, while many more unique relationships are possible in practice, e.g., customer or pastor. Our initial set was developed based on discussions among annotators and aimed at high but not complete coverage due to the increasing complexity of the annotation task as more relationships were added. Our results in Section 5 suggest that our best model could be able to generalize to new types of relationships in some settings and zeroshot results on two new relationship types not seen in training (a fellow church member and a commercial relationship) match expectations of context sensitivity, (cf. Figure 5 . However, performance is likely limited for less-common relationships without additional training data to describe the norms of appropriateness in this context; and, based on the error analysis in Section 4, models are currently unlikely to generalize to unseen relationships that
have complex sensitivity norms. In addition, new settings such as online spaces may require additional definitions of relationships as individuals interact with each other anonymously.
Third, our judgments of appropriateness were drawn from five annotators total, each of whom had different views of appropriateness based on their values and life experience. While our analysis of agreement with the Adjudicated data (Section 3.2) suggests that when annotators can reach a consensus on a message’s meaning, they are highly likely to agree on appropriateness, we nonetheless view that our annotations are likely to primarily reflect the values of the annotators and may not generalize to other social or cultural contexts where the norms of relationships differ. Future work is needed to explore how these norms differ through additional annotation, and we hope that our dataset will provide a reference for comparison to these judgments. For example, future work may make use of annotation schemes that explicitly model disagreements (Fornaciari et al., 2021) or personalized judgments (Plepi et al., 2022); such approaches may be able to better represent common factors influencing appropriateness judgments."
478,"Our approach is based on meta-learning and is designed for constrained situations where computing resources are limited, such as on-device settings. Therefore, using large and complex feature encoders like LLM may pose scalability challenges. In addition, if the task involves a significant number of new classes, the model may not scale effectively. Lastly, our method is primarily suitable for text classification, such as news category or product review classification. It is not appropriate for text generation tasks."
479,"GPU resources. This work utilizes extremely large language models and thus has a high cost on GPU resources. Concretely, experiments are conducted on the 8 x NVIDIA A100 GPU station. The maximum inference time on each version of COFE (containing 4,785 test cases) is ∼ 8 hours. The maximum estimation of costed computing resources in this study is ∼ 500 x 8 GPU hours.
Synthetic data. As in most previous work on compositional generalization (Lake and Baroni, 2018; Keysers et al., 2019; Kim and Linzen, 2020), the COFE dataset is constructed using synthetic data rather than natural one. The source-side sentences in COFE are from COGS, which account for 70–80% of naturally-occurring English sentences (Kim and Linzen, 2020; Roland et al., 2007). Thus, this synthetic test suite could be close to the real-world application scenarios.
Single run. Due to the high cost on computing resources, we do not take multiple runs with different sets of examples, nor did we take multiple samples with temperature > 0. Observations under different prompt orders (in Appendix 4.4) imply that with desired factors in selecting in-context examples, there could be low variance in experiments."
480,Limitations
481,"While our work displays many strengths, we highlight some important limitations in our analysis. Namely, we pretrain our STAMP models on a range of sources containing structured knowledge, however our analysis is limited to text-to-SQL tasks and does not demonstrate if such pretraining helps more generally in structured information tasks. For instance, STAMP pretrains on tables with (1) masked column recovery as a way to learn the structure of a table using the rows and natural language statement as context, and (2) a context-to-output objective that always includes the table in the context (when available) — since this matches the format of textto-SQL tasks. It is unclear if our objective choices for pretraining on tables perform equally well on the range of structured knowledge tasks, such as table question-answering, table summarization, datato-text, fact verification, and others explored in Xie et al. (2022). Second, we acknowledge that significant GPU resources are required for pretraining, even in continued pretraining approaches like ours which limit the breadth of ablations studies. Conversely, our work explores pretraining at smaller scales where certain phenomena like strong zeroshot performance is unlikely. Pretraining specifically on structured knowledge has an unknown value at larger scales with models having tens or hundreds of billions of parameters."
482,"The ’Limitation’ section.
3 A2. Did you discuss any potential risks of your work? 5
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 3,4"
483,Limitation section
484,"Here we discuss several limitations of our work and point to potential future work directions. First, we focus on single teacher and single student setup to study guidance generation whereas in real life there often are multiple teachers and students. We plan to extend to multi-party goal-driven communication and D&D also provides a proper testbed to study this problem.
Second, there are more nuances in guidance: railroading direct guidance (“make a persuasion check”) and subtle indirect guidance (“the guards seem to be a bit shaken”). We did include them in our human labeling and evaluation interface but did not specifically distinguish them during modeling.
Third, due to the constraints on input sizes for most LMs, we have to set a context window to study dialogue generation in D&D. However, both DM and players have a long-term memory about the comprehensive story progression which might influence how they communicate. As a next step, we plan to use summarization models and adventure books as narrative backgrounds to ground our
G4C task with a larger world setting. We include answers to other Frequently Asked Questions (FAQ) in Appendix A."
485,The limitations are discussed in the first section after the conclusion.
486,"Our work is subject to certain limitations, one of which pertains to financial constraints that hindered the ability to conduct large-scale experimentation with the data annotation methods proposed. As a result, the findings of this study may not be fully representative of larger datasets or populations. Additionally, the utilization of GPT-3 as a model presents challenges in terms of interpretability, as it operates as a ""black box"" system. To further investigate this subject, it would be bene-
ficial to conduct larger-scale experiments and to compare the performances of GPT-3, ChatGPT9, and GPT-4 (OpenAI, 2023) and the open-sourced LLMs like LLaMA (Touvron et al., 2023)."
487,"We compare 12 representative methods, present a unified view on existing prototype-based methods, and propose a competitive unified baseline by combining the advantageous modules of these methods. We test all methods, including the unified baseline, on three commonly-used English datasets using various experimental settings and achieve consistent results. However we acknowledge the potential disproportionality of our experiments in terms of language, domain, schema type and data scarcity extent. Therefore, for future work, we aim to conduct our empirical studies on more diverse event-detection (ED) datasets.
We are fortunate to witness the rapid development of Large Language Models (LLMs Brown et al. 2020b; Ouyang et al. 2022; Chung et al. 2022) in recent times. In our work, we set incontext learning as a baseline and evaluate the performance of LLMs on few-shot ED tasks. We find current LLMs still face challenges in dealing with Information Extraction (IE) tasks that require structured outputs (Qin et al., 2023; Josifoski et al., 2023). However, we acknowledge the ICL approach adopted here is relatively simple. We do not work hard to find the optimal prompt format, demonstration selection strategy, etc., to reach the upper bounds of LLMs’ performance. We view how to leverage the power of LLMs on ED tasks as an open problem and leave it for future work.
In this work, we focus more on the model aspect of few-shot ED tasks rather than data aspect. In other words, we assume having and only having access to a small set of labeled instances. In the future, we plan to explore how to utilize annotation guidelines, unlabeled corpus and external structured knowledge to improve few-shot ED tasks."
488,Limitations and Ethics Statement
489,"Yes, in the Limitations section"
490,"Interpretability. Although ALIGNSCORE shows high correlation with human judgments, it is hard to interpret the reasoning behind its predictions. Therefore, an interesting future research direction is to develop interpretable factual consistency metrics that can accurately identify words or spans in the input that contain factual consistency errors and (or) produce human readable explanations justifying its predictions.
Synthetic data. Our alignment training data contains datasets augmented with synthetic data. While ablation studies show that synthetic data helps improve metric performance, our rule-based method for generating synthetic data could generate noisy data that may not accurately model the error types and distributions produced by real world generative systems. Thus, analyzing the quality of synthetic data and developing more effective ways to generate synthetic data is an interesting research topic.
Language coverage. While we show ALIGNSCORE generalize well to unseen data, it only covers a single language, English. Undoubtedly, factual consistency evaluation is also important for more resource-constrained languages or in a multilingual setting. Consequently, future research could focus on extending the Align metric to multiple languages, including resource-constrained languages."
491,In the Limitations section
492,"In this paper, we introduce the concept of multigranularity temporal question answering and construct a benchmark dataset MULTITQ, which features ample relevant facts and multiple temporal granularities. We also propose a multi-granularity temporal question Answering model MultiQA, serving as a strong baseline for follow-up research.
Limitation. The main drawback of our data creation protocol is that the question/answer pairs were generated automatically, leading the question distribution to be artificial from a semantic perspective. In addition, the KG adopted in the research focuses on a single event domain, and extending the dataset to multiple domains is planned as future work."
493,Section 6 Conclusion and Limitation
494,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
495,"Our proposed approach is based on the premise that faithfulness errors observed in generation systems are due to noise in the dataset. While there is substantial evidence for this from prior work, and our method outperforms existing approaches on the datasets we used, it’s possible the the utility of our approach could drop in cases where we have clean, curated datasets. It’s possible that certain generation errors made by the model could be due to spurious patterns learned by the model that do not generalize well. In such cases, it’s unclear whether using our error attribution approach to remove training instances would alleviate the problem. However, as most large-scale datasets in natural language generation tend to be sourced from the internet, it’s inevitable that these datasets will likely contain at least a few erroneous examples that could lead to undesirable model generations. Therefore, we believe that our approach to using error attribution to clean datasets is still a valuable method to improve generation systems."
496,"In this work, we evaluated neural networks on the task of back-translating mathematical formulae from the PL LATEX to semantic CLs. For this purpose, we explored various types of neural networks and found that convolutional neural networks per-
3For example, LATEXML denotes the binomial as \genfrac{(}{)}{0pt}{0}{n}{k} instead of \binom{n}{k}
form best. Moreover, we observed that the perplexity of the translation of mathematical formulae behaves differently from the perplexity of the translation between natural languages.
Our evaluation shows that our model outperforms the Mathematica software on the task of interpreting LATEX produced by Mathematica while inferring the semantic information from the context within the formula.
A general limitation of neural networks is that trained models inherit biases from training data. For a successful formula translation, this means that the set of symbols, as well as the style in which the formulae are written, has to be present in the training data. Mathematica exports into a very common flavor / convention of LATEX, while semantic LATEX, translated by LATEXML, yields many unconventional LATEX expressions. In both cases, however, the flavor / conventions of LATEX are constant and do not allow variation as it is produced by a rule-based translator. Because of the limited vocabularies as well as limited set of LATEX conventions in the data sets, the translation of mathematical LATEX expressions of different flavors is not possible. In addition, we can see that a shift to a more difficult domain, such as special functions in the DLMF, produces a drop in performance but still generates very promising results. In future work, the translator could be improved by augmenting the data set such that it uses more and different ways to express the same content in the source language. As an example, a random choice between multiple ways to express a Mathematica expression in LATEX could be added. For semantic LATEX, the performance on real-world data could be improved by using multiple macro definitions for each macro. Ideal would be a data set of hand-written equivalents between the PLs and CLs. An addition could be multilingual translation (Johnson et al., 2017; Blackwood et al., 2018). This could allow learning translations and tokens that are not present in the training data for the respective language pair. Further, mathematical language-independent concepts could support a shared internal representation.
Another limitation is that data sets of mathematical formulae are not publicly available due to copyright and licensing. We will attempt to mitigate this issue by providing the data sets to interested researchers.
Note that this work does not use information from the context around a formula. Integrating
such context information would aid the translation as it can solve ambiguities. For example, for interpreting the expression (x)n, information about the specific field of mathematics is essential. Further, context information can include custom mathematical definitions. In real-world applications, building on such additional information could be important for reliable translations."
497,"We utilize safety classifier modules, such as the BAD classifier and Perspective API, as the red team classifier to automatically identify offensive output from the victim model following the practice in Perez et al. (2022). However, automatic classification of offensive outputs can be subject to inaccuracies, which may lead to the identification of false positive test cases (Gehman et al., 2020). To mitigate this issue, we may increase the threshold for positive texts to reduce the number of discovered false positive test cases. One other choice is incorporating human supervision into the classification. For example, we may assume the human-in-theloop scenario that has access to the offensiveness scores evaluated by human annotators within a limited number of queries to the annotators. In this scenario, we can either directly conduct BRT with human annotators as the red team classifier or modify the BRT method to incorporate offensiveness scores from both human annotators and the safety classifier modules during red teaming. Further exploration of these possibilities is left as future work."
498,"On Page 9, the first unnumbered section, ""Limitation"""
499,"after conclusion
7 A2. Did you discuss any potential risks of your work? currently don’t find any risks"
500,"The main novelty of our proposed MIR-GAN is refining frame-level modality-invariant representations via adversarial learning. It is promising to combine this approach with the popular selfsupervised pre-training to learn unified multimodal representations. In this work, we only load pretrained AV-HuBERT for the front-ends and speech recognition model, while the proposed modules (i.e., encoders, generator, discriminator) are still trained from scratch. In future, we may include the entire MIR-GAN into self-supervised learning scheme, together with the adversarial learning to refine better multimodal representations."
501,"While HUQ outperforms individual aleatoric and epistemic UE methods for most datasets considered, for some, the effects are negligible. To understand this pattern, we analyze the difference between the training and test sets. We generate latent representations of instances in the datasets
using a fine-tuned ELECTRA model and fit a logistic regression model to discriminate between train and test sets using these representations as features. Good performance of the discriminator indicates a covariance shift between the training and test data, while bad performance indicates that instances come from the same distribution.
Table 4 presents F1 scores for this task aligned with the performance gains of HUQ-DDU in percentages over the best method from the pair <SR, DDU>. As we can see, high F1 scores often correspond to low values of performance gains (the Spearman rank correlation = 0.8). This means that HUQ is unlikely to provide improvements to the base methods for the tasks with big covariate shifts. In our analysis, this is due to prediction mistakes primarily arising from OOD instances, which are well-handled by epistemic UE methods.
Visualizing the differences between the datasets using a t-SNE decomposition of the latent representations (see Figure 4), we can see that for IMPLICITHATE and TWITTER, where HUQ does not provide improvements, some regions of the test data are not covered by the training set. For PARADETOX and TOXIGEN, on the other hand, the training dataset completely overlays all regions of
the test data, and using HUQ improves AUC-RC on the base methods."
502,
503,"Yes. Section 6.
7 A2. Did you discuss any potential risks of your work? We did not find our work present major risks."
504,"Our experiments reveal that simultaneously incorporating more rules into the loss produces better
performance in the task of interest (Table 4). These results indicate that rules working in tandem significantly complement supervision coming from both sources of direct annotation under a fully declarative loss. Nevertheless, controlling the influence of each term in the loss is crucial for training stability. We found that the system has different sensitivities to each term in the loss, requiring a full search over the λ hyper-parameters (15). From this perspective, the possible benefits of increasing the number of rules in the loss come at the cost of more difficult learning.
Due to hardware limitations of the protected environment server that stores the datasets we use, RoBERTa-base was the best model that could fit in the available GPUs. Although other pre-trained embeddings could provide better performance, we argue that this is orthogonal to our contribution of incorporating indirect supervision under a fully declarative learning framework. Moreover, integrating logic-driven frameworks and prompt-based models like T5 is an interesting future line of work.
Choosing RoBERTa as the underlying embedding foundation of our system introduces all the inherent limitations of large language models (Bender et al., 2021). From this standpoint, we envision the application of these sorts of systems as a humanguided tool used only for counselor training and quality assurance, and never for real counseling sessions."
505,"Our conceptualization of the core task has some important limitations. We highlight three main limitations here. First, in order to tie a character to a place, we require that both the character and the place are explicitly mentioned in the text. This simplyfying approach helps annotation and modeling but is inadequate against the general setting of grounding any character at any time in the story.
Another limitation with our current approach is the assumption that the location of a character is independent at every instance in the story. It is because of this assumption that we can label every character and location co-mention without considering any other labels. In reality, however, location of a character at some time is highly dependent on the location of the character at a previous time.
Finally, the spatial relationship categories are designed to be coarse. This is helpful in setting up the task as a classification task but collapses information that can be useful. For example, if a character is described to be standing outside the southern gate of a building, our current approach will assign the NEAR label retaining only the aspect of distance and not the spatial orientation."
506,right after the main paper on page 9
507,"Given that many special properties of ChildDirected Speech are not present in text, we would have liked to work on a multimodal dataset, where both visual and speech information would be present. More specifically, we would have liked to test the effect of the following:
• Grounding the language models in vision to test the effect of joint attention (Rowe, 2012; Akhtar and Gernsbacher, 2007). Joint attention refers to the phenomena where the caregiver’s and the child’s coordinated attention to each other to a third object or an event.
• Child-Directed Speech is known to have special prosodic properties such as higher variability in pitch (Fernald et al., 1989; McRoberts and Best, 1997; Papousek et al., 1991), lengthening of vowels and pauses (Albin and Echols, 1996; Ratner, 1986; Fernald et al., 1989), context-specific intonational contours (Katz et al., 1996; Papousek et al., 1991; Stern et al., 1982). These properties have been suggested by many researchers to serve as a mechanism for getting the infants attention (Cruttenden, 1994; Ferguson, 1977; Fernald, 1989). This attentive role may be considered to be beneficial for language development in children (Garnica, 1977). As our models only take text as the input, we were unable to test the relationship the between these properties and language acquisition in neural network based models have.
• Caregivers give a lot of feedback when young children are first producing and acquiring language (Soderstrom, 2007). Our current mainstream language models are not interactive.
Therefore, it is difficult to incorporate the feedback loop and the test the effect of the same in models’ language acquisition.
As it is, our findings suggest that many of the most important facilitative features of ChildDirected Speech are relevant to precisely those formal and conceptual aspects of language acquisition that are not captured by text-based language models.
In this paper, we have tested the effect of native CDS in L2 acquisition with 5 typologically diverse languages. However, there is enormous scope to test the effect of the same with many more different languages, which may lead to more pointed implications and conclusions than the findings offered here."
508,"Despite the fact that we demonstrate strong OSSC performance with low generation quotas in Appendix 5.3, CoNAL still is slightly more computationally expensive than vanilla training. It also requires access to a pretrained LLM with which to generate novel examples. To achieve optimal performance, usage of the OpenAI API is required, which poses some concerns around transparency, as details around GPT-3 training and data are not publicly released. Finally, performance varies across datasets, suggesting that types of outliers that are unexpected to LLMs might still confuse a CoNALtrained model."
509,"Although comprehensive, our study of MPT in this work has couple of limitations: • As mentioned in §5.3, because of infeasiblity
to search for optimal hyperparameters for each of the meta learning methods in each of the ten settings, we choose to use the R→R setting as our main representative setting. This could be one of the reasons for MPT underperforming MTL in some non-classification tasks (noted in §6-Q1).
• We mainly focus on how upstream meta learning can improve the performance on target tasks. However, meta learning also enables faster convergence. We leave how it could help reduce the convergence time of PT as future work. Aside from that, meta prompt tuning (MPT) as a
method has a limitation that it is Memory-intensive. Optimization-based meta learning methods, especially MAML, are memory-intensive, which limits the tuning of the inner batch size and inner update steps (§5.3). One potential solution is to build more memory-efficient meta learning libraries."
510,"section 6 and limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
511,"Our work is a comprehensive empirical study of a popular large language model’s capacity to perform in-context learning, relying on both task-specific (via a wide variety of challenging and practically relevant downstream tasks) and task-agnostic (via looking for induction heads) analyses and connecting the two via correlation/overlap investigations. We do not claim a causal link, i.e., we do not claim that an attention head that acquires the capacity to be an induction head will become capable of more sophisticated in-context learning associated with our downstream tasks. Making this claim will require a more deeper investigation that is outside the scope of this paper. We also do not fully understand why most attention heads seem to be unimportant for in-context learning and why there is an overlap in (un)important attention heads across tasks and shots, which warrant further investigation. Other more obvious limitations to our work include our use of only up to 5 in-context examples, random selection of in-context examples for a query input and our choice of all monolingual downstream tasks."
512,"Section 7, after the conclusion section."
513,Limitations section
514,"The limitation of CITADEL mainly shows in two aspects. First, at the beginning of training, the model needs to route each token vector to multiple activated keys for token interaction, which increases the computation cost compared to COIL and ColBERT. This results in slower training speed but it gets better when training approaches the end as more tokens are pruned by the ℓ1 regularization. Another drawback lies in the implementation of CITADEL, or more generally speaking, most multivector retrieval methods. The token-level retrieval and aggregation make them not compatible with established search libraries such as FAISS or Pyserini. Moreover, for time and space efficiency, multi-vector retrieval also requires more engineering efforts and low-level optimization. Recently, XTR (Lee et al., 2023) provides a solution that constrains the document-level retrieval to be consistent with the token-level retrieval during training, which can be used for streamlining CITADEL."
515,Please see Section Limitations
516,"We use RoBERTa-base models trained on a single 12GB memory GPU (we used a NVIDIA Titan XP graphics card) for our experiments. Obtaining annotations for cognitive dissonance are limited by the availability of annotators and is not easily scalable in crowdsourcing platforms due to the required training and expertise in identifying dissonance. Due to this limitation, only two iterations of the AL loop for each setting were feasible for experiments. The transfer learning experiments in this paper were limited to two similar tasks, but there might be other tasks that could further improve or exceed the zero-shot performance of the models to cold start the active learning.
We focus on fine-tuning and active learning selection strategies to improve performance of rareclass classification for a specific task: dissonance detection across discourse units. Therefore, fur-
ther work would be necessary to determine if the findings extend to other tasks. Additionally, the results may be different for other languages or time intervals of data collection. The performance of the neural parser on splitting tweets into discourse units can produce parses that are imperfect but the annotators and our systems worked off its output regardless to keep the process consistent. An improved discourse parser may also lead to improved annotator agreement and/or classifier accuracy. The dataset that we release from this paper, which contains labels of expressions of some cognitive states, was constructed using criteria that may not be fully objective."
517,"Our work is limited by several factors. First, we conduct our work primarily using popular, publicly available dementia detection datasets, all of which are in English. Thus, it is unclear whether our findings generalize to other languages, especially with richer morphology where different predictive patterns may emerge. Second, due to the emphasis on feature-based models in most dementia detection work, we study only feature-based and instance-based DA approaches. Neural DA approaches may yield different findings, although they are less relevant for many current dementia detection approaches. Finally, we only study two backbone classification algorithms in our experiments. These classifiers are among the most common in prior work with our selected datasets; however, it may be the case that with a wider scope, other classification algorithms may yield different results. Collectively, these limitations present intriguing avenues for follow-up work."
518,"While our approach inherits the linear runtime complexity of the backpropagation algorithm, runtime concerns should not be fully neglected. Firstly, the linear runtime is only an analytical result, not an empirical measure. This means that the actual runtime of the backpropagation and thus our algorithm depend heavily on their implementation. For instance, some deep learning frameworks do a better job at reusing and parallelizing computations than others (Goodfellow et al., 2016). Indeed, our code is optimized for good readability and extensibility at the expense of speed, which hints at another limitation of our approach: Our approach requires deep integration with the framework as it needs access to all model weights and the computation graph. For this reason, our approach cannot be easily packaged and wrapped around any existing model or framework and we instead developed our own JAX-based reverse-mode autodifferentiation library, based on the numpy-based Brunoflow library (Ritchie, 2020). While we release our library to enable other researchers to analyze models through their gradient graphs, it faces some computational and memory constraints. In our experiments, running the three semirings together on a single sentence can take several minutes (depending on sentence length) using google/bert_uncased_L-6_H-512_A-8, the 6-layered pretrained BERT from Huggingface (Wolf et al., 2020), totaling our experimentation time on our datasets at about 10 CPU-hours. For improved adoption of this method, we encourage the direct integration of semiring implementations into the most popular deep learning frameworks. Our final point pertains not only to our study but to most interpretability approaches: One has to be careful when drawing conclusions from gradient paths. Cognitive biases, wrong expectations, and omitted confounds may lead to misinterpretation of results.
Ethics statement
We foresee no ethical concerns with this work. Our work aims to make the inner workings of neural network models more interpretable. On this account, we hope to contribute to reducing biases inherent in model architectures, pre-trained model weights, and tasks by increasing overall transparency."
519,Section Limitation at the end of the paper.
520,"Some of our experiments, specifically those in the ablations with large batch sizes, required significant computational resources. We trained these models on Google Cloud TPUv3 Pod slice with 128 chips for a few days. This experiment is important, as otherwise there would be questions on how the models compare at large batch sizes where contrastive models are known to work better. Due to training costs and in the interest of open research, we will open source our code and model checkpoints for the community to use and build upon.
Secondly, VMSST and BITRANSLATION require decoding which which means they need more memory for the decoder and are slower during training. However one advantage of these models is that they can be trained with gradient checkpointing greatly reducing their memory requirements, which cannot be used for the contrastive models as that would reduce the effective batch size for finding negative examples. Moreover, during inference, there is no difference in the memory or speed requirements in CONTRASTIVE, BITRANSLATION, or VMSST as only a single encoder is used in inference and there is no decoding."
521,Section Limitation
522,"Limitations section
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
523,"It should be noted that our contributions are limited to fanfiction documents. Models trained on our datasets might not transfer to other online content like news articles, websites, or social media posts. Particularly social-media texts are shorter and contain fewer descriptions and more verbal expressions, which is a substantial-enough shift to warrant models explicitly trained in the genre. Similarly, the conclusions of our experiments are limited by the models we used, as well as the genre of the text. Furthermore, the trigger warning scheme we used is a simple structure. Further research should investigate more detailed trigger (warning) typologies with a more rich semantics."
524,"Section Limitation
7 A2. Did you discuss any potential risks of your work? Since the number of pages in the text is limited and our model does not have significant potential risks, we do not discuss this."
525,the last section
526,"This section discusses the potential limitations of our work. This paper’s analysis of model effects mainly focuses on common benchmarks for adversarial defence, which may introduce confounding factors that affect the stability of our framework. Therefore, our model’s performance on more tasks, e.g., the MRPC dataset for semantic matching tasks, is worth further exploring. In addition, the present work proposes to conduct adversarial training from the perspective of estimating the overall adversarial loss. We expect a more profound exploration of improving the accuracy and efficiency of such estimation. We are also aware of the necessity to study whether the properties of traditional methods, such as the robust overfitting problem, will also arise in DSRM-based adversarial training. We leave these problems to further work."
527,"section 7
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
528,"Yes, in the ""Limitation"" section"
529,"Due to limited availability of compute resources, we were unable to scale up the model architecture to the large sizes becoming increasingly mainstream today. Similarly, the upstream corpus we used (BookWiki) is 16GB in size, and while it is large enough such that it was used to pretrain BERT (Devlin et al., 2019), much larger pretraining datasets are in use today such as the Colossal Common Crawl Corpus (Raffel et al., 2020). The relative performance achieved by using self-pretraining vs pretraining on upstream corpus can likely vary with the size of the model and upstream corpus, and more compute-heavy large scale experiments are needed to characterize it."
530,"The scope of our approach is intended for our specific task setting, which is proposed as a practical solution to mine open-world attributes without heavy supervision, and has not been studied previously. Our approach does require an external dependency of a POS tagger, and assumes high POS tagging quality on English. Thankfully, there are POS tools publicly available with high performance, and are quite robust against domain shift, mostly fulfilling the assumption.
Our current candidate generation that utilizes syntax-oriented patterns does not check the semantics, which can be another limitation. It introduces noisy spans in the process, such as “supports joint
health & overall” (in Table 15). Future works could consider combining syntax with semantics to alleviate noisy spans."
531,"In this paper, we present a bidirectional generative framework for cross-domain ABSA that has achieved outstanding results on four cross-domain ABSA tasks. Although there is only one stage during inference, our method involves multiple training stages, including text-to-label, label-totext, and final training. These additional training stages not only lengthen the training time but also require additional computational resources, which may hinder scalability for large-scale data and result in a burden for the environment."
532,"As with any natural language understanding task, there are practical limitations and related ethical aspects that must be considered before deploying a system. In particular, our corpus and modeling approach assume that the user-provided REs always refer to one of the two options. If this is not the case, or if the RE is particularly contrived, undesirable or unexpected behavior may occur: For any expression, including for instance one made with arbitrary derisive language, the model would attempt to resolve this to one of the alternative entities. One approach system designers may consider could be to pre-classify any user-provided REs to avoid interpreting those that are off topic or phrased in a negative manner.
A second consideration is that of corpus representativeness. In our case, as this is a first corpus for this task, we have limited ourselves to English Wikipedia, native English speaking annotators, and particular item sampling strategies for practical reasons. However, if used for training a deployed system, the examples present may bias any model to understand specific types of references but not others. Similarly, the items in our corpus are sufficiently popular to have a relatively long Wikipedia entry, whereas items not present in Wikipedia, or with only minimal information, may exhibit different characteristics."
533,Limitations section
534,"The description of limitations has been placed at the end of the paper, please see Limitation section for more details."
535,"Sec. Limitations after Sec. Conclusion.
7 A2. Did you discuss any potential risks of your work? We do not see a substantial risk of our work. Although the output could contain toxic or biased content, we posit that it does not attribute to the search algorithm we propose."
536,Limitations (after the conclusion)
537,"The TAED model has slightly more parameters than the corresponding Transducer model due to the attention modules to connect the speech encoder and AED decoder. They have similar training time for the offline models. However, the optimization of the streaming model would require more GPU memory and computation time due to the chunk-based RNN-T synchronization scheme described in §2.1. In our experiments, the streaming TAED model takes about three times more training time than the offline model on the 16 A100 GPU cards, each having 40GB of GPU memory.
In this work, we evaluate our streaming ST al-
gorithms on two translation directions: EN→ES and EN→DE. The word ordering for English and Spanish languages are based on Subject-VerbObject (SVO) while German is Subject-ObjectVerb (SOV). The experiments validate the streaming algorithms on both different word ordering pair and similar word ordering pair. Our future work will extend to other source languages besides English and more language directions."
538,section 8
539,"In this paper, we have unearthed a variety of problems present in current evaluation benchmarks that favor systems over humans, or that simply make such comparisons unfair. We conclude that there is no real evidence to claim that today’s language models possess superhuman performance. However, without empirical results obtained under the right setups, we cannot even claim the opposite, namely that humans are still better than systems. We leave such demonstrations for future work.
Additionally, while a good portion of the NLP research effort is devoted to natural language generation (NLG) tasks (which includes MT), here we provide only some pointers to NLG/MT. Indeed, as discussed in Section 4.3, these problems exist in the NLG universe as well, but, due to space constraints, we limit our analysis to NLU tasks."
540,"Section 8
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
541,the limitation section
542,"We have limited ourselves to experimenting with only five languages due to lack of data for both the pretrained models and the DeepSpin tokenizer models. Although there are annotated data for some low-resource polysynthetic languages such as Nahuatl, Raramuri, Wixarika, Shipibo-Konibo (Mager et al., 2020) and Kunwinjku (Pimentel et al., 2021), the available data was below 1M and therefore not enough to create pretrained models for our experiments.
Regarding the aforementioned limitation, DeepSpin which has proven to be a good option to mitigate the problem of high TTR languages in closed vocabulary environments is a supervised method that requires the availability of training data. As
can be seen in Table 2, to achieve 90% to better accuracy DeepSpin requires around 350K annotated words. This can be a major drawback for low-resource languages, although the results with less annotated data are still competitive. We have not studied another source of differences in the vocabulary size that could be due to the texts used in pretraining. Ortiz Suárez et al. (2019) found that, in general, the OSCAR samples contain more vocabulary words than the Wikipedia ones. Additionally, the Quechua corpus we have used also consists of educational and legal texts that can increase the number of different types, compared to Wikipedia texts.
On the other hand, we believe it is important to mention that for the Quechua language the training, evaluation, and testing data for NER and POS tasks were obtained from the same corpus used for training the language model. Note that, due to the scarcity of available digital and physical texts in that language, it is difficult to do it otherwise. The limited availability of texts leads to the use of the same corpus for multiple tasks, which could have implications on the evaluation of the obtained results. For instance, if the training corpus contains an unequal proportion of certain types of grammatical structures, it might negatively affect the performance of POS classifiers. Furthermore, if the corpus does not adequately reflect the linguistic variability and diversity of Quechua, the resulting models are likely to be less accurate and less generalizable.
Ethical Considerations
The datasets used in this paper for the training and evaluations of the pre-trained models, DeepSpin models, and fine-tuned models have been extracted from various previous articles and open-access repositories, therefore, we abide by the ethical rules by citing the original authors of each dataset. On the other hand, the annotated Turkish and Quechua data that were constructed by us for the development of the DeepSpin models will be presented in a forthcoming paper for public use. In addition, we encourage authors who use the resources in this article to cite the original sources. Finally, we would like to note that one of the authors of this paper has a long history of working with resource-poor synthetic languages, especially Quechua, which allows us to better understand the problems and concerns of the Quechua-speaking communities."
543,"Limitations have been described as separate section after the Conclusions, as required by the ACL instructions."
544,"In this section, we summarize the limitations of our work as follows:
• There is still a lot more to explore in terms of event co-occurrence for EAE (e.g., iterative extraction, course learning, etc.). We are unable to cover all in this work and will explore further in the future.
• As demonstrated by our ablation study, the high performance of our model greatly relies on the manual prompts. This limits the application of our model to the scenes where high-quality prompts are unavailable and difficult to construct. To address this, we should look into the area of automatic prompt construction.
• Our work ignores the phenomenon of entity coreference commonly existing in narrative documents. This limits the model’s ability to figure out the underlying relation between entities, which is crucial for the task of EAE. And we will take entity co-references into account in our future works."
545,"Section 7.
7 A2. Did you discuss any potential risks of your work? No potential risk is forseen."
546,"Due to the limited ability of the policy model, most theorem still failed to find the proof because of poorly suggested proof steps. Predicting the proof step from the proof state requires substantial reasoning ability. It’s observed in the experiment that the language model tends to produce the same proof step in training data, and is unsatisfactory in generalizing for new states. Another limitation resides in the proof-level value functions. Although the performance of the proof-level value functions shows promising improvement in the value function test set. The end-to-end pass rate diminishes the performance gap. This accounts for two major reasons: 1) weak policy model fails to produce correct action even if our value function correctly located the state to expand. 2) Our value function’s performance is still behind the performance threshold where the value really helps the search drastically. One future direction is to enhance the language model for better reasoning ability by using a larger language model or adding symbolic reasoning into the language model to produce more reasonable proof steps and better evaluate states’ value."
547,"No section number but directly following conclusion.
7 A2. Did you discuss any potential risks of your work? We do not see that our work introduces any new risks over the already published and publicly available previous work. In fact, we believe that better rationale plausibility improves interpretability and fairness, thereby reducing the risk that black-box models pose to the general public, especially to the historically disadvantaged groups.
3 A3. Do the abstract and introduction summarize the paper’s main claims? 1
7 A4. Have you used AI writing assistants when working on this paper? Left blank.
B 3 Did you use or create scientific artifacts? 5, 8"
548,Limitations section after conclusion
549,"Downstream Tasks In this work, we focused on long-document summarization as we believe it is the task where controllable summarization is most needed. Future work could investigate the effect of SOCRATIC pretraining on other downstream applications beyond those studied here. To handle long document input we could not use the BART model with SOCRATIC pretraining adaptation directly. Instead, we applied the SegEnc architecture on top of BART. This adaptation of the pretrained model may have dampened some of the few-shot performance of SOCRATIC pretraining. We thus believe that tasks with shorter input documents for which the SegEnc architecture is not necessary would see even greater benefits in the low-resource setting.
Base Model Throughout this work, we restricted our analysis to one model architecture the SegEnc architecture with the BART base model. Previous work extensively studied the impact of different architectures for long-document query-focused summarization (Vig et al., 2022). These primarily differ in how they model long documents. The authors found SegEnc, a simple sliding window adaptation of BART, to perform best on QMSum. While the results presented here are specific to SegEnc and BART, our approach is agnostic to the underlying model architecture and is orthogonal to longdocument modeling. We leave it to future work to investigate the effect SOCRATIC pretraining has on other architectures.
Evaluation Metrics As discussed in prior work (Fabbri et al., 2021b; Pagnoni et al., 2021; Gehrmann et al., 2021), there are limitations with the current automated evaluation metrics which do not strongly correlate with human judgments. Our results from these metrics should therefore be interpreted with caution and in combination with the human evaluation we performed to support them. One area in which automated metrics have been reported to perform poorly is factuality. Moreover, current factuality metrics have been designed and tested in the news domain and their performance in the out-of-domain setting (long documents and dialog data) was not systematically evaluated and is hard to interpret (Agarwal et al., 2022). In this work, we therefore choose not to report any factuality metric results.
QG Efficiency We did not optimize the efficiency of the QG component of SOCRATIC pretraining and, consequently, it is computationally expensive. Currently, given equal amounts of resources for QG and pretraining, it takes us about the same time to perform the QG phase and pretraining phase on the same amount of data. We note, however, that in low-resource scenarios, the additional compute can lead to significant benefits, as shown in our results. In addition, we did not experiment with efficient sampling strategies, and believe that improving the efficiency of the QG model inference, for example through model distillation (Hinton et al., 2015), could lead to significant efficiency gains.
Dataset Biases The datasets for pretraining and finetuning used in this work are in English and thus mainly represent the culture of the Englishspeaking populace. Political or gender biases may also exist in the dataset, and models trained on these datasets may propagate these biases. Additionally, the pretrained BART model carries biases from the data it was pretrained on. We did not stress test these models for biases and request that the users be aware of these potential issues in applying the models presented.
Misuse Potential and Failure Mode When properly used, the summarization models described in this paper can be time-saving. However, the current model outputs may be factually inconsistent with the input documents, and in such a case could contribute to misinformation on the internet. This issue is present among all current abstractive summarization models and is an area of active research.
12745"
550,Section 9
551,After the conclusion section (§6)
552,"More generators bring significant benefits to the model performance to our MGR, but the training cost is also increased with the number of generators growing. Although we have verified that we only need to keep one generator during test, there is no denying that the training cost is still an important problem. In the future, we will explore some methods like multi-task learning and model fusion, to reduce the model complexity."
553,In the Limitations section
554,Limitations
555,"The main limitation of this paper is the need for human-labeled reference responses. We will explore automated or human-machine collaboration methods to reduce the cost of annotation in the next stage. Another limitation is that we need to explore whether other auxiliary tasks can also enhance the performance of score prediction. In the future, we also plan to reproduce the proposed method for other, less resource-rich languages."
556,"Our method does not apply to VL models where the cross-modal encoder layers are relatively lightweight. For example, the vision encoder is much more computationally expensive than the cross-modal encoder for VL models like ALBEF (Li et al., 2021) and X-VLM (Zeng et al., 2021), therefore, the end to end inference speed improvement is marginal. Reducing the image tokens inside the vision encoder could further improve the model efficiency, we leave this exploration to future work."
557,"Please see the ""Limitations"" section."
558,We discussed the limitation of our work in a separate section after the main paper
559,"The ""Limitations"" section."
560,"The Conceptualizer we propose consists of two core steps, i.e., forward pass and backward pass. The forward pass identifies the most associated target-language strings for a focal concept. However, due to possible data sparsity of PBC in some low-resource languages and some cases of verselevel misalignment, χ2 scores of the real translations can be indistinguishable compared with some other rare words that also occur in the same verses. Under such rare cases, Conceptualizer will not work well enough. In addition, the genre of PBC is limited to religion and therefore the diversity of the concepts across languages is largely influenced. Nevertheless, PBC, as far as we know, provides texts in the largest number of low-resource languages. PBC is thus a good fit for our goal.
In this work, we select 83 concepts, including the Swadesh32 and Bible51, representing a wide range of interesting crosslingual concepts. The runtime for computing the results for one concept in all languages is around 10 hours on average. The relatively long runtime, however, can prevent us from exploring more interesting concepts.
We find that the concreteness of a focal concept can be a contributor to the stability measure. As we use English as the source language for representing the focal concepts, we naturally resort to concreteness scores from English language ratings only. In addition, the analysis is carried out from an English perspective. Nevertheless, as we want to compare different languages, we have to use a unified source language. Theoretically, we can use any language as the source language and represent the concepts in that language. We therefore plan to use other languages, e.g., Chinese, or some low-resource languages, as the source language in future research."
561,"This work only explores the multilingual VLP model for the image-text retrieval task. We leave the exploration of other multilingual vision-andlanguage downstream tasks such as visual question answering as future work. At the same time, our proposed method relies on a well-pretrained vision Transformer and a multilingual text encoder. Its performance is heavily influenced by the performance of the visual and textual backbones. This hinders the mCLIP from further improvements with the given backbones."
562,"Although we conducted extensive experiments, the exploration scope of this work has some limitations: (1) All data is from one of the largest MOOC websites in China, so the dataset is in the Chinese language, which limits the linguistic features covered in our analyses. We will add comprehensive corpora from other MOOC platforms with various languages such as English, Japanese, French, and so on to enhance the availability and coverage of our dataset. (2) We present two models with highprecision and high-recall behaviors. The severe noisy and incomplete issues could not be coped with simply by combining two technical methods (i.e. co-training and PUL). A more robust training method should be proposed to jointly achieve better overall performance. We encourage future works to address these limitations and get more comprehensive analysis results."
563,"As seen in Section 4.2, sometimes the metrics are unnecessarily penalised due to errors made by the end task models. Filtering these cases would require checking every example in every task manually. We hope our results can provide conclusive trends to the metric developers focusing on segment-level MT evaluation.
We included three tasks to cover different types of errors in machine translations and different types of contexts in which an online MT metric is required. Naturally, this regime can be extended to other datasets, other tasks, and other languages (Ruder et al., 2021; Doddapaneni et al., 2022). Further, our tasks used stricter evaluation metrics such as exact match. Incorporating information from partially correct outputs is not trivial and will be hopefully addressed in the future. We have covered 37 language pairs across the tasks which majorly use English as one of the languages. Most of the language pairs in this study are high-resource languages. Similarly, the examples in multilingual datasets are likely to exhibit translationese - unnatural artefacts from the task language present in the test language during manual translation; which tend to overestimate the performance of the various tasks (Majewska et al., 2023; Freitag et al., 2020). We hope to explore the effect of translationese on MT evaluation (Graham et al., 2020) and extrinsic tasks in future. The choice of metrics in this work is not exhaustive and is dependent on the availability and ease of use of the metric provided by the authors."
564,Section IV
565,"Yes, in the limitation section on Page 10."
566,
567,"In addition to the technical limitations discussed in the previous two subsections, our research has some higher-level limitations. Importantly, the dichotomy between university and industry is not black and white. We captured industry presence by looking at author affiliation and research grants. However, we did not look at university or department funding, which any individual researcher does not receive. Many universities receive (or have received) funding to sponsor their departments and, consequently, their faculty and research. A researcher may not be directly affiliated with a company nor receive funding from any company, yet feel some pressure if their department or university is funded by industry.
This analysis is a snapshot of industry presence up until 2022. Currently, there is no automatic tool to analyze future research and industry presence or interactively set filters and generate sub-views of this study. We invite researchers to use our opensource data and code to create such an interactive system in the future.
Furthermore, we did not consider the effect of governmental or military funding on the research done at universities. Both government and military, like industry, have vested interests and can influence research (Kistiakowsky, 1989; Barker, 2017; Goldfarb, 2008). Exploration of their effect and presence over time is an area we leave to future work.
Although we quantified industry presence at multiple conferences, we did not quantify the amount of industry funding present at each conference as
sponsors. Previous work, conference websites, and personal past experiences make us confident that most large conferences are funded, in part, by industry.
Our analysis did not stratify interaction academic-industry interactions by ethnicity, sex, or many other sensitive attributes. While we believe in the importance of such an analysis, the data to enable this analysis was not accessible to us: it is often not listed on websites, and the information gathered by ACL is unavailable to researchers.
Another aspect that our analysis did not touch on is that many universities are private and also require regular funding to maintain their research work. While student registration fees cover most of the operative business, research funds typically come from state grants, federal government grants, private institutions, and industry. We plan to trace such funding tracks of large private universities and research institutions in the future."
568,"While we conducted an extensive set of experiments to gain a broad picture of whether modeling improvements hold between benchmarks, it is always possible to investigate more settings. While our study covers a representative set of 20 nonpretrained and pre-trained modeling approaches, it is conceivable that evaluating more modeling approaches (or a different set of modeling approaches) on additional benchmarks (or a different set of benchmarks) would have led to different results.
Furthermore, although we evaluate each modeling approach on each benchmark with the same training hyperparameters used for SQuAD, as well as 5 additional randomly sampled hyperparameter settings (20 × 32 × 6 = 3840 experiments in total), it is possible that the SQuAD hyperparameters for some modeling approaches happen to be more general than other modeling approaches. Ideally, each modeling approach would be individually tuned to maximize performance on every benchmark, but doing so requires prohibitive amounts of compute and researcher effort—we believe that our experiments have enough coverage with respect to hyperparameter optimization."
569,"We mainly summarize three limitations of our work. First, the translator only generates a representation, not an actual instruction, making the model less interpretable. Second, we do not include more advanced vision representations such as ViT and CLIP to train the navigation agent. Although only using ResNet, we already surpass prior methods using those visual representations (e.g., HAMT (Chen et al., 2021)), it would be interesting to experiment with those different visual representations. Third, this navigation agent is trained in a simulated environment, and a more realistic setting will be more challenging."
570,"At the end of the paper.
7 A2. Did you discuss any potential risks of your work? Our work studies general information extraction techniques and does not have ethical issues."
571,"Traditional methods for the completion of ATOMIC proposed to score all candidate tail events given the head event and the relation. The GCN for encoding graph embeddings of events induced two shortcomings: 1) it is difficult for a GCN to propagate information due to the sparse graph structure of ATOMIC (Malaviya et al., 2020); 2) it cannot sufficiently utilize semantic information of events."
572,"In Limitations section.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
573,"Section 7.
7 A2. Did you discuss any potential risks of your work? We propose a method for non-autoregressive speech translation, which does not have any risks."
574,"We make use of MS-MARCO, a resource that provides large-scale relevance annotations. However, as with most retrieval datasets, this dataset could contain annotation biases. Given the vast number of documents in the corpus supplied by the dataset, relevance annotations are sparsely distributed, with all other documents assumed to be non-relevant. Consequently, some relevant documents may be inaccurately labeled as non-relevant, leading to false negatives. A notable annotation bias in MSMARCO is that the relevant label correlates highly with the exact matching term (Xiong et al., 2020). This bias poses a limitation during the training or evaluation stages. To appropriately address this annotation bias, we might need to reorganize the labeling process using either a human or a neural annotator, or we could aim to design and train a model that is resilient to such bias. We reserve this task for future research efforts."
575,Limitation Section after the Conclusion.
576,The second to last section.
577,"The last setction
7 A2. Did you discuss any potential risks of your work? This paper does not have such risk since it is a multi-choice question answering setting."
578,"Left blank.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
579,Left blank.
580,"Here, we discuss some limitations of this work to inspire future research in this direction.
Tail phenomena. SELF-INSTRUCT depends on LMs, and it will inherit all the limitations that carry over with LMs. As recent studies have shown (Razeghi et al., 2022; Kandpal et al., 2022), tail phenomena pose a serious challenge to the success of LMs. In other words, LMs’ largest gains correspond to the frequent uses of languages (head of the language use distribution), and there might be minimal gains in the low-frequency contexts. Similarly, in the context of this work, it would not be surprising if the majority of the gains by SELFINSTRUCT are skewed toward tasks or instructions that present more frequently in the pretraining corpus. As a consequence, the approach might show brittleness with respect to uncommon and creative instructions.
Dependence on large models. Because of SELFINSTRUCT’s dependence on the inductive biases extracted from LMs, it might work best for larger models. If true, this may create barriers to access for those who may not have large computing resources. We hope future studies will carefully study the gains as a function of model size or various other parameters. It is worthwhile to
note that instruction-tuning with human annotation also suffers from a similar limitation: gains of instruction-tuning are higher for larger models (Wei et al., 2022).
Reinforcing LM biases. A point of concern for the authors is the unintended consequences of this iterative algorithm, such as the amplification of problematic social biases (stereotypes or slurs about gender, race, etc.). Relatedly, one observed challenge in this process is the algorithm’s difficulty in producing balanced labels, which reflected models’ prior biases. We hope future work will lead to better understanding of the pros and cons of the approach."
581,Limitations section
582,"Although the bias metrics and debiasing methods we study work well, they certainly have limitations. Limitations of this paper are given below:
(i) We are aware that defining a bias in terms of target-attribute pairs can be incomplete and somewhat subjective. Future work could look for a more objective and thoughtful way to define different bias categories or a way that does not require defining bias in advance with some item sets.
(ii) Our dataset contains multiple bias categories, but they are still defined in advance and limited. It is feasible to explicitly define the different bias categories separately, but this also means that we need to use the corresponding subsets of the dataset when studying the different biases. Therefore, a mechanism that can automatically classify biases is necessary."
583,"All of our experiments have taken place by deploying conversational agents on Amazon Mechanical Turk with crowdworkers2, using English-language responses written by workers located in the United States. While these workers are reasonably diverse (Moss et al., 2020), this is quite different to a public deployment with organic users, who are using the system not because they are being paid but because they are genuinely engaged. In that case, collecting feedback will have different tradeoffs which we could not factor into the current work. For example, asking to provide detailed feedback might dissuade users from wanting to interact with the system, lowering engagement and hence the amount of collected data. We believe either more natural free-form or lightweight feedback might be best in that case, which is why we study and compare feedback methods in this work to evaluate their relative impact.
In public deployments with organic users, safety issues also become a much more important factor – in particular dealing with noisy or adversarial inputs and feedback. In the worst case this could mean human conversationalists could teach the model erroneous reasoning, misinformation, toxic or other undesirable behavior. We note that steps to address this issue are studied elsewhere, for example Ju et al. (2022).
2Our crowdsourcing tasks pay workers well above minimum wage. The tasks do not request any personal information from workers."
584,"In the section after the conclusion, without a section number.
7 A2. Did you discuss any potential risks of your work? We didn’t discuss potienal risks, because to the best of our knowledge, the research topic does not introduce additional risks."
585,Limitations
586,Section: Limitations
587,"One unsatisfying aspect of proposed task is that it accounts for distributive coordination structures, but is not able to handle sentences with collective reading where the main predicate applies to the plurality of conjuncts as a whole. In our data collection these account for about 4.9% of the verbal omission cases, and such sentences are left “non-rewritable”. In future work, we would like a solution that allows to resolve also such sentences in a consistent yet easy-to-annotate manner.
Additionally, in the GPT prompting experiment we experimented with a few different prompts, but did not do exhaustive prompt engineering, and it is possible that with more aggressive prompt engineering GPT can perform better on the task than our results indicate. Similarly for the fine-tuning experiments with T5-large, in which we did some hyperparameter tuning, but not aggressively so."
588,"This work relied on previously published datasets to source personas on which to anchor the generated unhelpful thoughts, and thus shares the limitations of those datasets. In particular, they use English-language responses, written by workers located in the United States.10. While these workers are reasonably diverse (Moss et al., 2020), the examples generated may not reflect the thought patterns and personas across cultures and diverse populations. This data is also generated by people who are being paid, as opposed to people genuinely engaging about situations that matter to them. Besides the substance of the thoughts themselves, a more direct limitation is that the models generate only English, so would not be directly usable for speakers of other languages.
In addition, the data collected reflects the understanding of lay people, rather than trained clinical psychologists. While this makes the material more immediately relatable to other lay people, it is possible that the data do not capture what clinical psychologists would consider adequate illustrations of unhelpful patterns. Our data has been spot-checked by a CBT-trained clinical psychologist and found generally sound, but the entire material should undergo further validation.
Another limitation is that the models that we have tested are resource-intensive. In particular, the
9https://github.com/facebookresearch/ ParlAI/tree/main/projects/reframe_ thoughts
10Our crowdsourcing tasks pay workers well above minimum wage.
best-performing model, GPT3.5, is only available through a paid API."
589,"In the 7-th Section
7 A2. Did you discuss any potential risks of your work? The data used for pre-training is based on publicly and widely used wikidata and wikipedia."
590,"First, AGATE can handle DTDNs but does not handle CTDNs. In addition, it currently handles neither labeled nor directed edges. In the future, we intend to extend AGATE to cover a more comprehensive collection of graph types. Second, we assumed that most new nodes have similar attributes to those of new nodes at the previous time step, which were observed in our preliminary experiments. We plan to propose new strategies for the new node appearance task. Third, we tuned the hyper-parameters independently of models close to the input, so it may not be the optimal combination of methods. We plan to employ auto-ML techniques to enhance performance and mitigate the learning process."
591,"Limitation
7 A2. Did you discuss any potential risks of your work? We do regular NLP task and use standard NLP datasets"
592,"We discuss the limitations of the work as follows:
• One major limitation of our work is that we analyze language models pre-trained with the same data, similar training procedures, and the same autoregressive language modeling objective. Our findings may support model families trained in this restricted setting. When comparing models trained with different corpora, such as Neo GPT NEO (Black et al., 2021) and BLOOM (Scao et al., 2022a), different architectures and objectives, such as retrievalbased language models (Khandelwal et al., 2020; Zhong et al., 2022; Borgeaud et al., 2021) and sparse models (Fedus et al., 2022; Artetxe et al., 2022a), the relationship between validation perplexity and downstream task performance could be more obscure.
• For downstream task evaluation, we only evaluate on multiple-choice tasks, where the evaluation protocol is the most similar to the pretraining objective. Evaluating on generationbased tasks is more messy and hard to scale up, and we will leave it as future work. Another risk is that as we always take aggregated measurements over tasks, it might conceal important patterns of individual tasks.
• We do not provide a concrete explanation for the double-descent behavior that consistently occurs during pre-training, nor do we know if it is an artifact of the data, the objective or the optimization process. We consider it an interesting phenomenon and will look more closely into it in future works."
593,Limitation Section.
594,"We highlight two main limitations of our work.
Firstly, instead of focusing on more recent NMT models that use large pretrained language models as their backbone, our experiments were based on transformer base models. That is because we used the NMT models that produced the translations in the datasets we analyze, i.e, the models that actually hallucinate for the source sequences in the dataset. Nevertheless, research on hallucinations for larger NMT models makes for an exciting line of future work and would be valuable to assess the broad validity of our claims.
Secondly, although our method does not require any training data or human annotations, it relies on access to a pre-existing database of source mass distributions. This can be easily obtained offline by running the model on monolingual data to obtain the distributions. Nevertheless, these datastores need not be costly in terms of memory. In fact, in Appendix J, we validate our detectors for datastores that contain less than 100k distributions."
595,Limitations
596,The section after conclusion
597,"ReCode benchmark has several limitations: (1) It contains perturbed datasets based on HumanEval and MBPP which focuses on Python function completion use cases. Therefore, we only perform
evaluation on Python language and not be able to capture robustness in a wide variety of code completion use cases. However, our transformations are generalizable and could be easily extended to other languages and also other coderelated datasets (Athiwaratkun et al., 2023). We encourage researchers to apply and extend ReCode benchmark to additional languages and other coderelated tasks; (2) ReCode benchmark is designed for robustness evaluation and cannot mitigate the lack of robustness. Given that our benchmark can be used to generate comprehensive collection of perturbed data, we believe that it can be used for training data augmentation to enhance model robustness. We will consider corresponding robust training strategy design and evaluation in the future work."
598,"Section titled ""Limitations"" after Section 6 (Conclusion).
7 A2. Did you discuss any potential risks of your work? We do not foresee any potential risks involved in the use of the resource."
599,"the last section
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
600,"The collection and verification of this work has required help from over 250 annotators. This makes the dataset difficult to replicate, as is the case with many dataset papers. We have selected annotators carefully, considering relevant experience and using techniques to determine annotator quality to minimise the subjective variance. We have tried to cover the arguments involved in debating by talking to experts and people from debate circuits across the world, with different experiences and expertise. However, due to the nature of this activity, it is possible that there are arguments and experiences have not been covered in the dataset. These could be experiences of marginalized communities, underrepresented debate circuits, etc. Moreover, some debate motions used are relevant to the time period in which the motion was the most prominent (for example, motions about Trump and his actions, certain policy decisions, wars and their outcomes, etc). Our dataset does not account for the changes that might have taken place pertinent to that issue after the generation of arguments."
601,"SYMBOLICTOM assumes stories are written chronologically, which may not hold for some human-written stories. This may be alleviated using time-stamping models like Faghihi and Kordjamshidi (2021). Furthermore, since we use off-theshelf models (WANLI (Liu et al., 2022) and OpenIE (Stanovsky et al., 2018)) to create and update the graphs, the presented approach may propagate errors as revealed in the linguistic diversity experiments. However, these issues can be largely alle-
5As a part of out-of-domain testing, we also create a more challenging version of the available ToM datasets, available at https://github.com/msclar/symbolictom along with a corrected version of ToMi.
viated by using more sophisticated models, even the LLMs like GPT3 themselves. We do not experiment with them due to budgetary restrictions.
Currently, all NLP datasets available for theory of mind reasoning describe Sally-Anne tests. In these datasets, the concept of large distances is absent, meaning that anyone specified to be in a location is assumed to be a witness of the actions that occur there. This assumption can be violated in realistic settings. For example, “Anne is in the USA” does not imply she is a witness to every action happening in the USA. In future work, this approach can be improved by refining the witnesses detection algorithm to incorporate physical commonsense reasoning. We could also refine the witness detection algorithm by sampling paths between the inserted edge and each node referring to a person, to query an LM directly on that substory by asking if the person witnessed the action. To be able to test both of these ideas, we would need to obtain new theory of mind datasets with significantly more types of interactions and physical commonsense in the stories."
602,"This work is subject to two limitations. First, our experiments were restricted to text classification tasks and we did not evaluate if our methods can effectively defend against adversarial attacks for other tasks like QA, etc. (Jia and Liang, 2017). It therefore remains unexplored if our conclusions transfer beyond the text classification tasks.
Second, the primary contribution of our work, ATINTER relies on using a language model like T5, which is trained on large amount of text in English. It is possible that our approach is not as effective for languages where such a model is not freely available. Additionally, in this work, we did not explore the impact of large language model pretraining on our results."
603,"Section 7, Section 5.6"
604,"Naturalness. Since our dataset relies on the Wikipedia category names and semi-automatically generated compositions, it does not represent an unbiased sample from a natural distribution of real search queries that contain implicit set operations. Further, we limit attention to non-ambiguous queries and do not address the additional challenges that arise due to ambiguity in real search scenarios. However, the queries in our dataset were judged to plausibly correspond to real user search needs and system improvements measured on QUEST should correlate with improvements on at least a fraction of natural search engine queries with set operations.
Recall. We also note that because Wikipedia categories have imperfect recall of all relevant entities (that contain sufficient evidence in their documents), systems may be incorrectly penalised for predicted relevant entities assessed as false positive. We quantify this in section 5. We have also limited the trusted source for an entity to its Wikipedia document but entities with insufficient textual evidence in their documents may still be relevant. Ideally, multiple trusted sources could be taken into account and evidence could be aggregated to make relevance decisions. RomQA (Zhong et al., 2022) takes a step in this latter direction although the evidence attribution is not manually verified.
Answer Set Sizes. To ensure that relevance labels are correct and verifiable, we seek the help of crowdworkers. However, this meant that we needed to restrict the answer set sizes to 20 for the queries in our dataset, to make annotation feasible. On one hand, this is realistic for a search scenario because users may only be interested in a limited set of results. On the other hand, our dataset does not model a scenario where the answer set sizes are much larger."
605,In Section 7 Limitations
606,"Recruiting human subjects for annotation limits the reproducibility of human evaluation. In addition, we have only tested the performance of the proposed framework on the fixed dataset, ArgKP2021, that we described above, and not on a wider range of data. This is because ArgKP-2021 was the only dataset available for use in this task. Finally, we did not filter the arguments in the original corpus, with the result that potentially offensive arguments may come into the framework as input and generate key points which some readers might find offensive. It is worth noting, however, that the identification of offensive language is not the aim of this work."
607,Left blank.
608,"Section Limitations.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
609,"There is a growing interest in investigating human morality in text (Russell et al., 2015; Gabriel, 2020). However, like most technologies, morality classification can be misused, especially targeting sensitive features including ethnicity and political orientation (Kalimeri et al., 2019a; Talat et al., 2022). For instance, authorities in non-liberal countries could use Tomea to identify repressed minorities by detecting moral language that diverges from the expected moral rhetoric. Ongoing research is investigating such issues, e.g., by creating methods that mitigate bias and unfairness by design (Dinan et al., 2020; Vargas and Cotterell, 2020).
We discuss three main limitations of our analyses related to the corpus we use (MFTC). First, MFTC is composed of English tweets, and we employ a version of BERT that was pre-trained on large-scale English data. Our experiments show that Tomea produces insightful results under these conditions. However, the performance of Tomea with models pre-trained on smaller datasets, e.g., datasets for morphologically richer languages, remains to be investigated. Further, the scalability of Tomea to longer text formats (e.g., news articles) and different mediums of communication (e.g., surveys) is yet to be explored.
Second, the tweets in the MFTC were collected using the Twitter API, which only yields public
posts. Thus, following Twitter’s Terms of Service, deleted content will not be available (limiting the reproducibility of any Twitter-based study). Further, the demographic and cultural distribution of Twitter users may not be representative of the general population, In addition, we required the crowd workers involved in the evaluation to be fluent in English, and their demographic distribution (Appendix B.3) is skewed towards Europe. These factors could possibly lead to the perpetuation of Western values and biases (Mehrabi et al., 2021) in our analyses. Additional experiments are needed to investigate whether Tomea would produce insightful results when applied on a dataset collected on a more extensive slice of the population, with a broader set of linguistical expressions.
Third, the MFTC is focused on US-centric topics. However, when recruiting annotators for our crowd evaluation, we did not require familiarity with such topics. Even though the annotators were not exposed to the original tweets but to a processed version of the dataset (i.e., the output of Tomea, see Section 4.4.1), the potential lack of familiarity may have influenced the evaluation results.
Finally, we remind that Tomea’s d-distances measure how (dis-)similar two domains are, and are thus not a (binary) judgment of (dis-)similarity. Further, two corpora collected in the same domain (e.g., two datasets on BLM protests) will likely not have a d-distance of 0. It is left to the user to judge the similarity of the two corpora, supported by Tomea’s quantitative and qualitative metrics."
610,Section 7
611,We discuss the limitations of our work in the Limitation section.
612,Section 7
613,Limitations
614,"In this work, we propose a structure-based pseudolabel generation method for zero-shot video sentence localization and propose a noise-resistant method to reduce the effect of pseudo-label noise. The limitations of our work are: (1) although we generate free-form natural language queries, the distribution of generated queries may still differ from the distribution of queries in the dataset (e.g. queries on the Charades-STA dataset usually start with ‘person’), which may degrade the performance during testing; (2) our pseudo label refinement can correct the noisy event labels, but there is no mechanism to correct noisy queries. These can be studied as future works."
615,"The approach to collect our dataset is expensive and laborious. This along with the dependence on expert annotators makes the transfer of such an approach challenging for other low-resource languages. We however, find this a necessary endeavor to develop initial resources that can help provide a starting point to extend access to more languages and iteratively improve research, technologies and services across languages."
616,"Limitations
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
617,"The omission problem is critical in dialogue summarization, but even if this problem is solved, we still cannot guarantee a candidate is appropriate because it might bring hallucination content that is not presented by the source dialogue. Previous works (Tang et al., 2022; Maynez et al., 2020) also concluded that factual inconsistency is a critical
problem in dialogue summarization, and it is not easy to distinguish. How to mitigate the omission problem while avoiding the occurrence of new errors is not discussed in this paper, and we hope to address this issue in future work."
618,"No section number, ""limitations"" section (page 8 and 9)"
619,"We identify crucial financial signals in reports which can help financial practitioners to digest long financial documents efficiently. However, factors such as macroeconomics, stock prices, and public policies may affect how a financial practitioner views financial reports in practice. Confidential intelligence or social media may greatly affect the analysis results. Therefore, we limit our task to the scenario in which the content in the reports is the sole information available to users. Accordingly, to prevent bias in the annotation process, we acquire annotations from annotators under similar scenarios (graduate students majoring in accounting or other related fields) rather than from financial professionals. In addition, language partially constrains our methods since the data we used in stage S2 is in English; adding a machine translation module may have sub-optimal effectiveness of financial signal highlighting. This is mainly because the financial signals highly depend on many languagespecific knowledge or country regulations."
620,"Since the minimax objective requires using two separately trained models, i.e. the learner and the auxiliary, the design of the latter plays a crucial role in the overall stability of the training process. In particular, while having a very capable auxiliary model will naturally result in a more accurate and robust example weight distribution, it will also potentially lead to overfitting to certain training instances with high-losses. Another potential limitation of minimax training is that the existence of noise in the labels may cause the auxiliary to generate erroneous example weights due to high-loss noisy instances co-existing with the “hard” examples containing meaningful patterns that contradict the shortcuts. Furthermore, we explore shortcut mitigation only for NLI in English, and thus our method might not transfer to other tasks and/or languages. Finally, the datasets we consider are well-used and -discussed in the literature, and consequently their shortcuts (and how they are adopted by the models) are well-known. Further testing is needed to establish whether our approach would transfer to datasets containing different shortcuts."
621,"Limitations section on Page 9
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
622,"Despite the progress we made, there still exist limitations in our work. On the one hand, we only investigated some classic dynamic networks and found that the proposed method contribute to the best performance in selected criteria. However, other advanced partition methods that further improve the performance and efficiency may exist, which deserve exploration in future work. On the other hand, since we only consider MoE and DY-Conv in limited tasks, it would be valuable to consider other architectures (e.g., Switch Transformer (Fedus et al., 2021)), machine learning methods (e.g., reinforcement learning (Li et al., 2022)) and tasks (e.g., machine translation (Ding et al., 2020, 2021)).
Ethics Statement
We take ethical considerations seriously and strictly adhere to the ACL Ethics Policy. This paper focuses on the higher efficiency of dynamic networks, e.g., the mixture of experts. Both the datasets and models used in this paper are publicly available and have been widely adopted by researchers. We ensure that the findings and conclusions of this paper are reported accurately and objectively."
623,"We point at some directions for future improvements in automatic instruction generation.
First, as shown in §3, Unnatural Instructions contains noisy examples, in which either the instruction, input, or output are invalid. Future work may focus on developing better filters for such examples - e.g., by annotating a subset of examples as either valid or not and training a classifier for determining the correctness of generated instances (West et al., 2022; Liu et al., 2022a).
Second, future work may employ a human-inthe-loop approach, where humans should recognize challenging patterns, encouraging models to generate more complex examples (Liu et al., 2022a). In another human-in-the-loop scenario, models trained on Unnatural Instructions can be queried by humans to find examples on which these models fail, thus collecting harder examples (Nie et al., 2020).
Finally, language models are known to sometimes reflect undesirable biases present in their training data. Automatically generated data may therefore contain such content. We note that during our manual analysis, we did not notice any harmful examples. Still, future work may consider applying a filtering mechanism to reduce the risk of having biased content."
624,"This work focuses on English QA datasets only. Similar techniques should apply in other languages as well; however, we did not evaluate them. The augmentations generated are difficult to validate for yes/no questions for the few-shot method. Moreover, it can be challenging to generate these augmentations if access to large LM is unavailable. However, under those scenarios, data in the target domain should be annotated, which ideally would perform better than the few-shot setting. Our models also suffer from similar problems as LLMs, like hallucinations, misinformation, etc."
625,"It seems to be difficult to evaluate the efficiency of ODQA models fairly and impartially due to multiple factors that should be considered and need to be traded off. On the one hand, it is not enough to only use accuracy, memory, and processing time to evaluate effectiveness. It is also important to establish what resource, e.g., money, power consumption, carbon emissions, etc., one attempt to constrain (Treviso et al., 2022). On the other hand, how to deploy models and what tools model implementation relies on contributes to inequity growth (Blodgett et al., 2020). It is extremely challenging to unify the deployment of all models and the tools they rely on and to achieve a truly fair and unbiased effectiveness comparison."
626,"Section 5
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
627,"Limitations, at the end of the paper
7 A2. Did you discuss any potential risks of your work? No apparent societal risks."
628,"In this work, we evaluate the proposed models for NLP tasks only. However, tasks in other fields such as Computer Vision may present a very different input inductive bias, thus affecting the performance. Moreover, our models are trained from scratch, hence it is unknown whether the same divide-andconquer strategy works for pre-trained models. We will study these limitations in the future to give a more extensive exploration."
629,"May extend to other domains In this paper, we present a generic framework and evaluate the effectiveness of our proposed model Jointprop on three public datasets. We may further extend the framework to various datasets in different domains. For example, ACE05 (Walker et al., 2006) in social networks, journalism, and broadcasting, as well as GENIA corpus (Ohta et al., 2002) in biomedical research.
May extend to other NLP tasks Our proposed model focus on two tasks, namely NER and RE. We may extend our framework to include more information extraction tasks, such as coreference resolution and event extraction. Moreover, we may contract knowledge graphs from extracted structural information."
630,"Currently, RoHT framework is restricted to incorporating KBs and text. However, since RoHT retrieves answers from each knowledge source in a separate way, it could in principle utilize knowledge from more heterogeneous sources such as tables, and we will study this in future work. In addition, a device with large storage space and memory is needed for the storage and usage of Wikipeida and Wikidata."
631,"To understand the gap between our automatic data generation method and fake news written by humans, we expanded PN-SILVER to different sizes and compared the performance of ROBERTALARGE when trained on these generated datasets and the human-written fake news dataset, SNOPES. Note that since the TIMELINE17 dataset only contains around 4K samples, we additionally crawled New York Times news articles as an input to our generator for the “5 times” to “10 times” experiments. The results are shown in Figure 3. Although the detector performance at first improves as we add more silver training data, it reaches a plateau after the size is increased five-fold. This illustrates that while our approach is more effective compared to baseline generation methods, there is still a clear gap between our generated articles and human-crafted fake news, likely in aspects such as style (as discussed in §5.2), intent (i.e., limited modeling of propaganda techniques), and falsehood (i.e., the generated content is 100% false).
Despite the advantages of our generation approach, as compared to previous methods, it is uncapable of generating other propaganda techniques covered in (Da San Martino et al., 2019), such as straw man. Thus, our method is not generic enough to handle all types of propaganda techniques within a unified framework. Moreover, our approach is limited to generating English-only news articles, and cannot be applied to other languages."
632,Section 8.
633,"In the final part after conclusion
A2. Did you discuss any potential risks of your work? Not applicable. It is fundamental research and not tied to particular applications."
634,Limitations Section on page 10.
635,"Using a medium size fine-tuned teacher. With recent advances in huge LM such as GPT4 and their extraordinary generation capabilities, one may wonder about the relevance of this work which mainly focuses on a medium size fine-tuned teacher. Although we show the distillation of a huge LM (GPT-4), it is often infeasible.
First, when the data cannot be sent to external servers because of privacy constraints or when the domain is unique or specific (e.g., in national security settings or human conversations), huge LMs that cannot be fine-tuned may be less effective.
Second, we have distinguished between two types of costs: computational and financial. While training a student model with a medium-size finetuned teacher may take a few days, the entire process is feasible since training time is typically not a limited resource. In contrast, generating PTs with a huge LM like GPT-4 can easily cost (many) dozens of thousands of dollars. This financial cost is often prohibitive, particularly when training a general high-quality student or several domain-specific ones. While it is possible to utilize a huge LM to obtain a limited number of labeled examples, relying on it for generating PTs for abundant unlabeled data is not feasible. Therefore, a medium size teacher is needed.
Furthermore, research suggests that using mediator/assistant teachers aids the distillation process (Mirzadeh et al., 2020; Wang et al., 2020), as might be the case in distillation from a huge LM to a medium size fine-tuned teacher, and finally to a small student. Considering the aforementioned reasons, our study holds significant relevance as it emphasizes the importance of the distillation process with a medium size teacher, regardless of whether the data is generated manually or by a huge LM.
The scope of our realistic setup. While our results demonstrate the effectiveness of KD for various English-to-English NLG tasks, for the tasks that were part of the study, the output length is relatively short compared to the input (e.g., Summarization and Question Generation) or has a similar length (Abductive Reasoning, Style Transfer and Simplification). The results may differ for tasks with much longer output lengths or for non-English-to-English tasks such as NMT, data-to-text (e.g., table-to-text), multilingual, or multi-modality tasks.
In addition, the results are applicable to our realistic task-specific setups, and some findings may
vary in high-resource scenarios or when unlabeled data is unavailable. Although these scenarios may be less relevant to NLP application developers, they are commonly studied in academic research.
Computational training costs. Another limitation of our research is that we did not consider the computational costs of the KD stages. The training time comparison between the methods was therefore overlooked. This is because we assumed that one-time resource usage for training could be neglected compared to the accumulated inference cost of a deployed model.
However, it is worth noting that generating PTs with the teacher for all the training and unlabeled examples is computationally expensive (it could take one to a few days, depending on the number of unlabeled examples). Furthermore, Joint-Teaching can also be computationally heavier than other KD methods, as the student generates PTs during the training process (although the student is fast).
In addition, different training objectives also have different costs, with some methods being more computationally intensive than others (e.g., Attention-Relation is more costly than Logits KD). Finally, the distillation process can be long, and multiple epochs are required until the student converges - in some setups, we trained the student for more than a few days.
Utilizing huge LMs. Certain limitations arise in our extreme setup, which involves the costly utilization of huge LMs (GPT-4) provided by external companies like OpenAI. First, the comparison with the Joint-Teaching method is not conducted due to the need for repeated costly querying of the teacher model to extract its logits every time a PT is generated with the student. Nevertheless, extracting the logits of the teacher PTs (for Logits KD) and generating multiple PTs is approximately equivalent to generating a single PT. This is because the prompt, consisting of many tokens, is processed only once, and the marginal cost of generating multiple (relatively short) PTs is low.
Another limitation arises from relying on external companies to enable logit extraction (for Logits KD) and there is no assurance that this feature will be supported. For instance, in the chat versions: ChatGPT and GPT-4, logits are not accessible. In this work, we rely on an internal version of GPT-4, which allows us to extract its logits. Fortunately, we demonstrate that even without Logits KD, achieving a strong student model is possible."
636,"In this paper, we evaluate the quality of humanannotated natural language explanations towards the models’ prediction performance on multiple datasets. Although it is a natural step that our evaluation metric could be generalized to evaluate the helpfulness of model-generated explanations, we would like to caution that: our metric and evaluation experiment requires the models to generate explanations for the train split data, then use the data with generated explanations to fine-tune the second model with the Infusion setting, which may not be suitable for those systems that are trained on train split data. In addition, we acknowledge that the human-annotated explanations are very expensive to collect, thus, a better mechanism (e.g., Active-Learning approaches (Yao et al., 2023)) is needed to improve human annotators’ performance."
637,"This paper covers only four NLP tasks. Certain other tasks requiring more background knowledge may show different results. We suggest recruiting language learners when native speakers are not available, but recruiting learners may also be difficult for languages that are not popular for learners. Our results are based on a relatively low number of participants, as we chose to cover three different languages to show generalizability across languages. Many factors that may contribute to the results remain, such as the order of the batch of annotation questions with respect to the question difficulty level."
638,"The focus of the TEMPREASON dataset is to examine language models’ temporal reasoning capability. However, the temporal expressions of TEMPREASON are only in the form of month in textual form and year in numeric form. One limitation of the TEMPREASON benchmark is the lack of adversarial attacks in other temporal formats, such as
all numeric dates and months. The robustness of temporal reasoning is also important in real-world applications. Since the scope of this paper only focuses on the reasoning aspect, the robustness of TEMPREASON will be left for future research. Besides, the knowledge triples of TEMPREASON are from the crowd-sourced Wikidata KB, and these triples are used to construct the question-answer pairs in this paper. Hence, it is possible that errors in the Wikidata KB propagate to the answers in TEMPREASON. However, such errors have minimal effect in the ReasonQA setting, for this task only asks the models to infer from factual knowledge in the Wikidata KB."
639,"This work focuses on the effects of adaptive inference in a low-resource setting, specifically when training data is limited. Our experiments (Section 5) suggest that the negative impact of conflicting gradients may be less prominent when larger amount of training data is available.
Our experiments were conducted using relatively small pre-trained language models (≤ 350M parameters) due to computational constraints, and we defer the replication of our findings with larger, more powerful models to future work. Nonetheless, our results have important implications for the growing trend of increasingly large language models. We hope this work inspires further research on methods to reduce the computational cost of NLP.
This work concentrates on evaluating the speedaccuracy trade-off of Multi-Model and Early-Exit at inference time. We recognize that there are additional factors, such as memory usage, batch processing, and training duration, that could be considered when comparing these methods.
Finally, we experimented with seven text classification tasks in English. We recognize that results may vary for other tasks and languages."
640,"Section 8
7 A2. Did you discuss any potential risks of your work? Our work does not present any risks."
641,
642,"The main limitation of our method is that it heavily depends on the quality of the label description. If a label description does not precisely describe the meaning of the label, our method cannot work. For some classification tasks such as microaggression detection, their labels have abstract meaning that is difficult to be understood by pre-trained language models. Similarly, our method cannot work on the domain that is not covered by the pre-training corpora of language models, such as the medical domain.
Another limitation of our method is that PLCT loss cannot handle short texts. If a text consists of only one sentence, PLCT loss will no longer work because LCT requires a document to be more than one sentence. In this case, PCL loss can still be used for self-training."
643,"As previously mentioned, our study is focused on the generator component of a QA pipeline and ignores the retrieval task. In the experiments presented in the paper, we have used gold facts to report the results. For certain datasets such as HiTab and MULTIHIERTT which were designed for complex tabular structures, this might simplify the end-to-end challenge. In future studies, we hope to explore whether CounterComp can enhance the performance of retrievers.
The datasets used in our experiments were curated using enterprise documents such as financial reports or other corporate disclosures. Quantitative QA over these reports often involves multi-step reasoning that is limited to linear arithmetic operations such as addition, division, averaging, etc. A completely open-domain QA engine might need to cover more complex operators.
Lastly, we designed CounterComp to leverage existing data by sampling from the training set. Nevertheless, combining CounterComp with augmentation-focused methods such as CAD might lead to more robust models."
644,"We identify a few limitations of the current work. Our approach still suffers from biases in the training data and may produce incorrect output or lead to an inaccurate understanding of multi-modal content. And a large-scale audio-visual pre-trained model is a promising direction toward more advanced and cheaper approaches for transfer learning, which we leave for future study."
645,"Three main limitations with regards to certain aspects of this paper are the comparison against very large models, the distribution of the Original set, and the restriction of the output length.
Firstly, due to the lack of computational resources and availability of some models, we were unable to make a rigorous comparison of our finetuned models’ as described in Section 5.2 against very large models like Minerva (Lewkowycz et al., 2022) or even Codex (Chen et al., 2021). However, these larger models can still be evaluated as FERMAT is made publicly available.
Secondly, another limitation of FERMAT is its use of Illinois and CommonCore which have highly skewed distributions of numbers (see Section 3.1.2)
and their answers are mainly integers which is not representative of the real-world. This undesired effect is mirrored in the number types that use the same numbers as Original. However, this was part of our design for FERMAT as the alternative would have been to combined all the ranges of numbers used with the representation, creating too many aspects but mainly conflicting with non-independent analyses between representation and range of numbers. Therefore, we chose to use the same numbers as Original, and since the templates will be openly accessible, they can be used to generate more combinations for wider aspects.
Lastly, when generating training questions, despite our best intentions, we had to limit the length of the output to an arbitrary length of 12 digits, therefore some number combination were not possible, for example 1÷3 = 0.3333... . This practical implication could have been avoided with the use of fractions or rounding. But we judged that it would have added an extra layer of difficulty for the models and decided to restrict the output length instead."
646,"Section 7 - Limitations
7 A2. Did you discuss any potential risks of your work? We do not believe our work to have potential risks, instead we aim to reduce environmental impact by looking at alternative to large models."
647,"There are several characteristics of the presented analyses that limit the scope of conclusions that can be drawn. We discuss how each of these limitations affect the takeaways of our results below.
Number of Chatbots The generalizability of our metric analysis results (Section 7) is constrained by
the fact that we were only able to include conversations from 4 chatbots in our analyses. We did our best to choose chatbots representative of the field and seem to have selected a fairly diverse group of models (Section 8). However, it is possible that not all results we found in our metric analyses will generalize when evaluating other chat models. One possible example is the number of partner contradictions we observed among our 4 chatbots (Figure 4), which may be similar by coincidence. If other chatbot models indeed differ more substantially in partner contradiction rates, our sensitivity metric analysis may have underestimated the sensitivity of our partner contradiction metric (Section 7.3). In general, including a larger number of chatbots in a metric analysis will improve the chance that its results will apply to new chatbot models. Future work that performs metric analyses like those we presented, but with different chatbots than the 4 selected in this work, would aid further analysis of our results’ generalizability.
Use of Surgers as Evaluators We perform our analyses using only a single evaluator group (Surgers). This choice of evaluator group does not harm the replicability of our methods, as other researchers have access to use of SurgeHQ or similar third-party annotation companies. However, several other evaluator groups are more popularly used for chat model evaluation, such as university students and Amazon Mechanical Turkers (MTurkers). We attempted to carry out our study with three evaluator groups (see Appendix E for details), but were unable to proceed with student and MTurker evaluator groups due to time constraints. Consequently, it is unclear to what extent our metric analysis results will generalize to other choices of evaluator.
Number of Collected Conversations As with any study involving a sampling procedure, resource constraints limit the number of collected samples, which in turn limits the statistical power of the study’s analyses. Our study included 400 conversations, which provided more than adequate statistical power for most of our analyses. For example, our investigation of each metric’s predictive validity (Section 7.2) relied on a simple linear regression analyses. At a significance level of α=0.05, our 400 conversation samples would yield a statistical power of 1-β=0.80 to detect effect sizes of f2=0.142 by F-test for each metric’s regression. However, our analyses with the weakest statistical
power are our dialogue-level analyses that compare bots with only 100 samples per bot. At 100 samples per bot, and assuming a standard deviation of 1.0 Likert points,13 a two-tailed t-test of mean Dialogue Likert rating would have a statistical power of 1-β=0.80 to detect differences of an effect size of Cohen’s d=0.40. This is still a reasonable amount of statistical power, but leaves room for our study to produce inconclusive results when the true differences between chatbots are small."
648,Section 10
649,"In the last section
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
650,"Data diversity: As a template-generated dataset, KITMUS does not reflect the full diversity of natural data. However, we do not attempt to emulate the diversity of natural datasets. Using templates over natural data for diagnostic purposes has a few advantages. Templates facilitate control over the source of a certain type of knowledge, which may not be possible to do with more natural datasets like Ontonotes. This allows us to isolate the model behavior we want to probe. We also take several steps to add diversity, like using multiple templates, sampling from large resource pools, random shuffling of entities, addition of noise sentences, and canonical data splits with non-overlapping templates and resources. To prevent spurious factors at lexical level, the templates are hand-crafted to remove surface cues and validated in a study with human participants. Background Knowledge Assumption in LMs: The results of our work is based on the assumption that pretrained LMs have access to background knowledge about real occupations. To verify that the pretrained LMs evaluated in this work contain background knowledge mapping occupations to situations, we ran a LAMA probe (Petroni et al., 2020) on BERT and ELMo. Given the template “The work of a [MASK] is [SITUATION].”, we compared the probabilities the LMs assigned to all single-token occupation names used in KITMUS (probing for multi-token words is not supported by LAMA). BERT assigned higher probabilities to the correct occupation than to any other occupation for 90% of occupations. ELMo assigned the highest probability to the correct occupation for only 45% occupations, which might contribute to explaining why the ELMo-based model C2F generally performs worse than BERT4Coref on the BACKGROUND-PRETRAIN variant KITMUS, which requires such knowledge about occupations. Root Word Overlap: One potential limitation of testing for non-fictional background knowledge like “firefighters put out fires” is that the natural occurrence of the root word “fire” in both occupation and situation might enable models to solve the task without having access to background knowledge. An analysis of trigram overlaps in all occupationsituation pairs shows that 45% of non-fictional occupations have at least one overlapping root word. However, a comparison of performances on those samples with and without root word overlap
showed neither systematic increase nor decrease for any model, indicating that models do not rely on the root word mappings. Results split up by root word overlap can be found in Table 10.
Train Set Size: The size of the train set for KITMUS, 2000, was chosen to mirror that of GAP (Webster et al., 2018). To evaluate whether the failure of models to learn the task is due to the relatively small number of samples observed during training, we re-generated all variants with 5000 train examples and repeated all experiments. We observe an increase in the magnitude of performance both in BERT4Coref and C2F on those variants where performance was higher than random performance with 2000 examples, but not on those that were equal to or below random performance. Consistent with previous results, BERT4Coref performs well on BACKGROUNDPRETRAIN and BACKGROUND-BOTH, but not on all fictional BACKGROUND-INFERENCE variants (Tables 7 and 13). We release the KITMUS generation code to enable experimentation with other train set sizes in future work."
651,Limitations section
652,Final section (9)
653,Section 9
654,"This work represents an initial push to bring dogwhistles to the forefront of NLP and computational social science research, and as such, has many limitations. Our glossary is the most comprehensive resource to date (to the best of our knowledge) but aims to document a moving target, as dogwhistles continuously emerge or fall out of use due to outgroup awareness. We aim to make this resource a “living glossary” and encourage others to submit new entries or examples. We further encourage future research to develop models to automatically detect the emergence of new dogwhistles.
Another major limitation in this work is that we identify as out-group members for nearly all dogwhistles in the glossary and have an adversarial relationship with many of the communities studied (e.g. white supremacists). Although our work would ideally be validated by members of the ingroups, they have very little incentive to share this information, as that would damage the dogwhistle’s utility as a tool for covert in-group communication.
This work, like most prior work, is limited in that we operationalize dogwhistles as a static binary; we assume each term either does or does not have a dogwhistle interpretation and is categorically included or excluded from our glossary and analyses. In reality, dogwhistles are far more complicated constructs. For example, Lee and Kosse (2020) characterize dogwhistles along two dimensions: the size of their in-group and the degree to which their usage is conventionalized. Other axes of variation may include the level of out-group awareness, and the social and political risks of backlash to the communicator if the dogwhistle interpretation is exposed. It is even possible that audience members who hear a dogwhistle further recirculate it even if they themselves do not recognize the covert meaning (Saul, 2018). We hope future work will consider multifaceted and continuous measures of “dogwhistleness"" that account for such nuances.
Finally, the current work is limited in the scope of dogwhistles considered: they are all in English with the vast majority coming from the U.S. political and cultural contexts. However, dogwhistles are prominent across cultures (Pal et al., 2018; Åkerlund, 2021) and we hope that future work will consider other languages and cultures, especially involving researchers who have high awareness of or expertise in non-U.S political environments."
655,"While we aim for a comprehensive analysis of existing methods (such as lemmatization) and model types for Ancient Greek and other Classical languages, there are limits to exhaustively exploring the full space of variations and rigorously evaluating their impact on model performance. For example, we could not comprehensively evaluate the effects of (i) the pre-training corpora, as we did not re-train a BERT model for Ancient Greek, to pin down the exact difference between prior BERT models (which were trained on smaller data before) and our own models, which are based on inherently stronger model types; similarly, we did not induce Latin ROBERTA and T5 models, to confirm the differences between mono- and multilingual models for language-specific Latin tasks. (ii) In a similar vein, we did not compare different model sizes. However, we studied prior work and scaling laws and believe that the base model is appropriate for the size of our training data. Further factors of this type concern (iii) hyperparameter settings and (iv) other factors in isolation.
Not only do we miss sufficient computational
resources to perform such manifold ablations and comparative assessments, we also considered the carbon footprint that such experiments cause and which does not stand up to the insights that could possibly be gained from more experiments.
For these reasons, we focused on two selected dimensions of variants that we believe to be valuable for a community interested in Classical languages:
(i) We tried to answer questions as to when multilingual models can be profitably used, and (ii) aimed to showcase various potential advantages of encoder-decoder models, which by now have not been considered in studies on Classical languages.
Another clear limitation lies in the size of the demonstrated semantic and knowledge probing tasks. (i) They are of small size, and we cannot, therefore, draw firm conclusions as to, e.g., the effect of multilinguality. Also, the synonym/antonym disambiguation task is presumably the most difficult one. As a counter-balance, we used a more tangible task for knowledge probing, by choosing family relationships, which we expect to be frequently found in the pre-training corpora.
(ii) A further limitation we find for the knowledge probing tasks resides in the size of our trained models and the underlying pretraining data. This limitation could be one that is not easy to overcome. But we still encourage the community to create similar probing task datasets. Future work may find appropriate ways of data augmentation, or transfer learning methods that are applicable to historical languages so that further progress and insight will be possible."
656,in Section Limitations
657,"We state two points of limitations and future work in this section. First, the UniVPM combines both restored clean audio and original input audio for downstream speech recognition, while without any trade-off to weight them. For example, under extremely noisy conditions the restored clean audio plays a more important role, while in less noisy scenarios the original audio may provide more valid information. Some weighting strategies to select the most effective audio information could benefit the downstream speech recognition. Second, the proposed clustering and viseme-phoneme mapping are actually unsupervised schemes, so that it could be promising to extend our UniVPM to the popular self-supervised learning framework, in order to make full use of the abundant unlabeled data."
658,"The paper has only focused on graphs with multitype relations (knowledge graphs). When MAGNN shows improvement over baselines, someone may doubt if MA-GNN will do well on single-type relation graphs. The limitations of the representational power of the MA-GNN model should be discussed more deeply."
659,"In this work, we attempt to extend an existing MNMT model to support new language pairs with an acceptable expense. In addition to the advantages, our method has the following limitations:
(1) Additional introduced parameters. We utilize the parameter-isolation based method to support new language pairs. The total parameters of the MNMT model have been increased by pluggable modules to achieve better performance than prior studies. In the future, we will compress the number of parameters to the same size of original models meanwhile preserve the performance on all translation directions.
(2) The gap between our scenario and the realworld scenario. Our proposed method is a whitebox service in incremental learning. Thus, we train
a powerful MNMT model as the original model instead of directly utilizing existing models from the Internet. And we only consider eight incremental language pairs due to the limitation of computation resources. We try our best to simulate the realworld scenario and we will apply our proposed method for large-scale pre-trained MNMT models (e.g., NLLB 54.5B and M2M 12B) to validate the effectiveness in industrial scenarios."
660,We add a limitation section after the conclusions.
661,"TERTiUS is a pilot dataset. In particular, its test set can support segment-level metrics, but is not large enough to support reliable dialogue-level evaluation metrics. Due to resource constraints, we also do not report inter-annotator agreement measurements. While we made effort to make our interface low-friction, the demonstration setting still differs from the test-time scenario it is meant to emulate, and such a mismatch may also result in undesired data biases. Because our dialogues were collected before having a trained interpretation model, trajectories always follow gold interpretations. Because of this, the main sources of errors are ASR misdetections or user speech errors. In particular, TERTiUS contains data on: 1. misdetections and speech errors in transcription, and how to fix them through commands, 2. misdetections and speech errors in edits, and what intent they correspond to. We leave to future work the task of addressing semantic errors and ambiguities which result from incorrect interpretation of user intent. Some of these limitations can be addressed by incorporating trained models into the demonstration interface, which will allow faster demonstration, and capture trajectories that include actual system (non-gold) interpretations.
Though the trained system runs, we have not done user studies with it because it is not production-ready. The T5-base models are efficient enough, but the prompted GPT3 model is too slow for a responsive interactive experience. Neither model is accurate enough at interpretation. We welcome more research on this task!
When a human dictates to another human, interleaved corrections and commands are often marked prosodically (by pitch melody, intensity, and timing). Our current system examines only the textual ASR output; we have given no account of how to incorporate prosody, a problem that we leave to future work. We also haven’t considered how to make use of speech lattices or n-best lists, but they could be very useful if the user is correcting our mistranscription—both to figure out what text the user is referring to, and to fix it."
662,"section ’limitations"""
663,"Even though XY-LENT paves the way towards better general-purpose multilingual representation foundation models, in this section, we highlight the limitations associated with this work. We first expound upon the limitations associated with selfsupervised learning on large web extracted corpora. Then we show that while XY-LENT achieves strong performance on multiple multilingual benchmarks, when the downstream task involves unseen (during pretraining) languages, the performance drops by a substantial margin. Finally, we show the potential limitation associated with a common methodology used for domain adaptation associated with leveraging these multilingual foundation models, illustrating how catastrophic forgetting exacerbates certail issues pertaining to low resource language performance.
Training Data
XY-LENT uses CC-100 which a static multilingual corpus extracted from Common Crawl for 100 languages. As noted by Wenzek et al. (2020), several data filtering strategies have been applied to remove duplicated documents, paragraphs with high ratio of punctuations, digits and profanities, the resultant data may still result in many potential biases requiring further analysis. Additionally, these issues might be aggravated for models that leverage bitext data, since the bitexts themselves are mined from web crawls, and thus potentially have all the associated biases, stereotypes and other associated harms. Furthermore, the raw data was compiled from static Common Crawl snapshots from January, 2020 to December, 2020 and hence may not include information about some of the recent events such as COVID-19.
Performance on Unseen Languages
Given the performance improvements observed with scaling, we investigate how it impacts extremely low resource languages which are not present in the pre-training data. In order to do so, we consider our model’s performance on the AmericasNLI dataset (Ebrahimi et al., 2022) which extends the XNLI dataset to 10 Indigenous languages of the Americas.
Table 5 presents the results on the AmericasNLI
dataset. As can be seen, XY-LENT does outperform XLM-R, indicating that better representation learning also benefits these extremely low resource languages. However, we do not see an increase in performance while scaling our models. Specifically, the performance of XY-LENTBase and XYLENTXL model is nearly the same, and substantially worse that the performance observed on the XNLI dataset. This indicates that, while parameter scaling can help improve performance on languages that the model has seen during pre-training, it does not automatically improve performance in the extremely low-resource regime 6. Thus, while model scaling allows for improvements across numerous dimensions, it is far from a panacea, especially if not done in conjunction with data scaling efforts. To be able to improve performance for unseen languages, an intervention would need to be made at the data collection efforts during pretraining, which we aim to assess in future works.
Continued Training for Domain Adaptation in Pre-Trained Encoders
In recent years, continued training on domain specific corpora has been considered a viable approach for domain adaptation of MLM style pre-trained models (Gururangan et al., 2020; Yao et al., 2021) where the core idea is to continue train the pretrained model on domain specific corpora with the goal of improving in-domain downstream evaluation.
We first show that this phenomenon can be extended to models pretrained with an ELECTRA style training objective. Concretely, we apply domain adaptation in the biomedical domain where we continue to train our XY-LENTBase as well as XY-LENTMLM + TLM model on the PubMed data presented in Yao et al. (2021), and evaluate it on the ChemProt task (which aims at extracting relations between chemicals and proteins) presented in Gururangan et al. (2020) as the in-domain downstream task.
We observe that the continued training approach presented in Gururangan et al. (2020) for the ELECTRA style models, using the same peak learning rate as used during pre-training, results in divergence. Interestingly, this neither happens for the generator of the ELECTRA model nor for the
6Note that since the tokenizer is a sentencepiece tokenzier, there are extremely few UNK words in the low-resource languages. Consequently, the poor performance is not explained by excessive occurrences of UNK tokens
MLM style pre-trained model. Thus, for an ELECTRA style continued training setup, we posit reducing the peak learning rate to be a crucial change. Table 7 shows the performance on the downstream task post the continued training approach and unsurprisingly it helps with improving in-domain performance.
However, given the multilingual nature of such models, we test the multilinguality of these models
before and after continued training; using crosslingual zero-shot XNLI as a proxy for multilingual model quality. Table 6 shows the drop in performance across all languages pre and post continued training. We first note that this drop in performance is present for both MLM and ELECTRA style of models, and thus is not an artifact of the pre-training objective. We observe that the drop in performance is not uniform across all languages and the drop is worse for MLM style models (with using the same peak learning rate suffering more from this issue; Table 7). While we expect the drop in English performance to be relatively less, we do see that the drop is substantially more for the mid and low resource languages (especially Hindi, Turkish, Urdu and Swahili; see Fig. 5). While this can potentially be ameliorated by using techniques like Adapters (Houlsby et al., 2019) etc., we would like to draw attention towards the fact that general purpose continued training does suffer from this issue."
664,Limitations section after section 5
665,In Section: Limitations
666,Unnumbered Limitations section immediately after Conclusion.
667,Section Limitations
668,"Yes. Section Limitation.
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
669,
670,Limitation
671,"Supporting more tasks. In this paper, we only consider attacking classification tasks (i.e., sentiment analysis, toxic detection, and natural language inference). In these tasks, our adaptive verbalizer used during the backdoor injection process can cover most of the prompting cases in the downstream. Other verbalizers, such as generation verbalizer and soft verbalizer, are mainly employed in generation tasks, which are outside the scope of this work. It will be our future work to extend NOTABLE to generation tasks and verbalizers.
Extension to more domains. Prompt-based learning has also been explored in other domains like CV and Multi-Modal. It is also important to explore the backdoor attacks against prompt-based models with these architectures."
672,"There are additional limitations and potential risks of LLM evaluations that should be noted, and these limitations are actually well-known problems of pre-trained language models. As listed on the Open AI blog for ChatGPT, ChatGPT sometimes generates answers that sound right and plausible but are totally nonsense. OpenAI also admits that the model’s response may be sensitive to the prompt used to query the model. While in Section 3.3.2, we find that the overall results among different instructions are not significantly different, we cannot guarantee that this is the case for all kinds of modification on the task instructions.
Other than the limitations listed on the OpenAI blog, there are still other limitations. For example, LLMs may not have emotions. Whether AI models have emotion is a more philosophical question and is controversial, so the results of using such models for evaluating emotion-related tasks may be strongly challenged and may even violate research ethics. As we find during our experiments, ChatGPT often replies ""I am an AI system and I do not have emotions like a human"" when asked to rate the likability of a story.
Another important limitation of LLM evaluation is that LLMs lack the ability to process visual cues in task instructions, unlike human evaluation. Human evaluators can use formattings such as special fonts or text styles to focus on important parts of the instructions. Additionally, the way instructions and questions are formatted can influence how human evaluators approach the task. While using special HTML syntax can serve as an alternative for visual cues, such tags are not used in human evaluation, so we do not use those HTML tags in LLM evaluation to incorporate visual cues in the inputs to the LLMs. However, LLMs can only process raw text input and are unable to take in visual cues."
673,"Many limitations of our study are already discussed in Section 5.2, however, we repeat and add to them explicitly here.
Small resource scenario Our study investigates MLP-based architectures for text classification tasks and finds competitive performance with vanilla Transformers while having lower cost in terms of the Green AI equation. However, the scope of our findings is naturally limited to the testing scenario, which is low-resource: Our models are relatively small, not pretrained on large generalpurpose corpora, and trained on datasets with fewer than 1 million examples. We may not say with certainty that our results will also hold on larger scale. For the sake of hypothesis-driven research we consider it more valuable to run many controlled small-scale experiments rather than few large-scale experiments. Nonetheless, scaling up should certainly be part of future research directions, as this is essential for optimal task performance.
Limitation to English pairwise sentence classification tasks Since token mixing is the independent variable in our study, we put our main focus on English sentence-pair classification tasks with textual input only, which we presume (and provide some evidence for) to be most useful to assess differences between token mixing models. Of course, vanilla Transformers are very flexible in the sense that, over the course of many studies, they have been shown to be very effective for a wide range of tasks, languages and data modalities. Whether or not the proposed HyperMixer model possesses similar flexibility cannot be answered in this study. The HyperMixer encoder arguably possesses similar inductive biases as Transformers. We thus expect it to be straight-forward to apply to tasks that are also solved well by Transformer encoders (e.g., span classification). For tasks such as language modeling, which involve a Transformer decoder, significant modeling advancements are required to obtain a HyperMixer equivalent. We consider this a very promising direction for future work.
Limitation to MLP-based baselines Similar to a trend in the computer vision community, our study investigates the suitability of MLP-based architectures for NLP. Due to their conceptual simplicity, these models promise to be easier to train, potentially leading to reduced Green AI costs. To this
end we compare our proposed HyperMixer model to a range of other MLP-based models, and Transformers. Apart from FNet and Linear Transformers, which are efficient Transformer alternatives, we do not attempt an exhaustive comparison to non-MLP-based efficient NLP models. Hence, the scope of our claims does not extend to all efficient Transformer models. However, these models are of course very relevant to this study, as they are targeted towards one of the factors of Green AI cost (single forward pass complexity). Therefore, we regard a comprehensive comparison as valuable future work."
674,"Since two-pass direct S2ST models require linguistic units as the target for the first-pass decoder, they cannot be used when the target language is unwritten. Compared to cascaded S2ST systems, direct S2ST systems require more data preparation steps, including training a HuBERT model, synthesizing target speech with a TTS model, extracting discrete units with the HuBERT model, and training a unit-based vocoder, etc. Moreover, the target audio quality of direct speech-to-unit systems relies on the quality of discrete units generated by selfsupervised discrete models. It further depends on the availability of speech data to train HuBERT models for the target languages.
Because S2ST systems could generate speech that does not necessarily represent the source speech’s content, there is a potential risk of conveying wrong information."
675,"""Limitations” section"
676,"Despite achieving good performance, there are some limitations in our study. The first is how to handle ambiguous instances in the corpus. 3.45% of the implicit data in PDTB 2.0 and 5% in PDTB 3.0 contains more than one label. Currently, we follow previous work and simply use the first label for training. But there might be a better solution to handle those cases. Another is the required time for training. To mimic the annotation process of PDTB, our model needs to pass through the embedding layer and transformers twice, so it takes more
time to train than the RoBERTa baseline. However, our training time is shorter than Pipeline and Adversarial due to those two models’ pipeline setup and adversarial training strategy. Also, note that our method has a similar number of parameters to the RoBERTa baseline since we share embedding layers and transformers between the connection generation and relation classification modules in our approach. Therefore, the memory required to train our model is not much different from that required to train the RoBERTa baseline."
677,"In Section 6.
7 A2. Did you discuss any potential risks of your work? Our paper is an entirely technical work. We don’t think it has any risk of bias or otherwise."
678,"Section Limitations
7 A2. Did you discuss any potential risks of your work? The model is designed and evaluated on established tasks and datasets, which should not cause severe risks."
679,"Due to the restrictions of the adopted benchmarks and resources, our evaluation bears the following limitations: (i) We only focus on social biases in the English language and North American cultures. This is due to the fact that both CrowSPairs and StereoSet are generated by crowd workers from North America. Future work can extend our analysis to other languages and cultures with
7See Appendix B for more details.
BERT/GPT-2 are highlighted in green .
the corresponding resources such as the French CrowS-Pairs (Névéol et al., 2022) and multilingual WEAT (Lauscher and Glavaš, 2019). (ii) Our evaluation has a limited coverage over different kinds of harms according to Blodgett et al. (2020). CrowS-Pairs, StereoSet, and WinoBias all focus on stereotyping, a kind of representational harm, while others like allocational harms are untouched. Developing methods to measure these harms generally requires in-depth interactions between technologists and customers. Blodgett et al. (2021) also point out several conceputalization and operationalization pitfalls in the above three bias benchmarks, which limits the validity of the results evaluated on them. (iii) Due to the incomplete bias attribute word lists, our CDA-based debiasing method is by no means fair enough to cover all the minority groups (e.g., groups with non-binary genders). Therefore the current debiasing method in this paper can only be used to mitigate bias among the demographic groups mentioned in Appendix A. We
recommend more complete resources such as the gender-inclusive word list in (Cao and Daumé III, 2021) for real-world scenarios."
680,"section 8 ""Limitations"""
681,"The primary motivation behind this paper was to provide a comprehensive benchmarking study that explores the impact of model compression techniques on bias in large language models. While our work is among the first efforts to address fairness in compressed language models across multiple com-
pression methods, including exploring multilingual settings, we are aware of the inherent limitations associated with our benchmarking study. Some of the limitations and potential directions for future work that builds on our study include the following:
• Our study primarily focused on benchmarking pre-trained models and evaluating their performance in the downstream text classification task. Expanding our investigation to encompass other tasks, particularly those involving generative models or large language models (LLMs), would be a valuable contribution to the research community. Examining the impact of model compression techniques on fairness in these domains would provide further insights and contribute to a more comprehensive understanding of bias in different types of language models.
• While our work includes a multilingual evaluation component, we acknowledge that there is room for further improvement and comprehensiveness in our benchmarking study, particularly with regard to quantization and pruning techniques. Apart from this, we did not provide a comparative analysis of monolingual and multilingual models using the same extrinsic data, which could provide valuable insights into the disparate impact of compression on the bias across languages. These are potential areas for future research that could contribute to a more thorough understanding of bias in compressed language models.
• Despite showing results for state-of-the-art pruning methods, further benchmarking is necessary to observe how bias varies across different pruning techniques. Similarly, whilst our method serves as a proxy to estimate bias trends in quantized models, a thorough quantization-specific study is needed.
• Different compression strategies yield varied benefits in terms of latency, memory, and so forth. Investigating the tradeoffs between these elements and fairness and accuracy would yield valuable insights for obtaining realistic estimations in real-world scenarios. Additionally, conducting case-study analyses would give practitioners in the field a deeper understanding of the potential harm these methods may introduce."
682,"Yes, in the final section of the paper.
7 A2. Did you discuss any potential risks of your work? No, as our work deals with pointing out the potential risks of certain strategies used to optimize for efficiency that could inadvertently cause models to make more biased decision. We have extensively discussed the limitations and other potential implications of our findings, however."
683,"This work has been studied on the Wikipedia corpus, following the standard experimental setting used in previous unsupervised sentence representation learning studies. We expect to see many important findings by investigating sentence representation learning on various corpora in different domains such as Bookcorpus (Zhu et al., 2015) and the C4 corpus (Raffel et al., 2019)."
684,"A limitation of our work is that we cannot directly apply our methods to the few existing revisionbased corpora from other domains (Yang et al., 2017; Afrin and Litman, 2018; Anthonio et al., 2020) for multiple reasons: On the one hand, those corpora do not contain histories with more than one revision but only before-after sentence pairs). Some also consist of less than 1000 sentence pairs, rendering the quantitative experiments considered in this paper pointless. On the other hand, additional metadata useful for our analysis (e.g., revision types and contextual information) is either not available at all or only for a limited number of instances that is insufficient for training models.
Furthermore, the methods we evaluated utilize distantly supervised labels based on the assumption that each revision improves the quality of the claim and additional annotations provided by human editors. These annotations suffer from being coarse-grained, consisting of mainly three classes. However, each of the improvement types can be represented by several more fine-grained revision intentions. A point that we did not consider as part of this work is whether certain revisions can affect or inform future revisions within the same debate, for example, rephrasing of arguments to avoid repetition or ensuring that all claims use the same wording for the main concepts. Often, such relationships are implicit and cannot be derived without additional information provided by the user performing the revision. We believe that collecting datasets and developing approaches, which enable distinguishing more fine-grained types of edits and implicit relationships, could not only enable deeper analysis and training more fine-grained improvement suggestion models, but also allow for better explanations to end users.
However, it should be noted that some of the considered methods rely on deep learning and have certain limitations when it comes to underrepresented classes, where the number of available training instances is very low. This is especially important when considering the task of claim improvement suggestion. We also point out in this regard that we only use the base versions of the BERT, ELECTRA, and DeBERTa models due to resource constraints.
The results may vary, if larger models are used. While common types of improvements likely differ across other domains and communities, we stress that our approaches are entirely data-driven, and are not tied to any specific quality definition. Therefore, we expect our data processing and filtering methods as well as the considered approaches to be applicable to other domains, where historical collaborative editing data similar to ours is available. When it comes to practice, several issues require further investigation, such as how to integrate recommendations in collaborative editing and educational environments, whether the recommended improvements will be accepted by users, and how they may impact the users’ behavior. We leave these questions for future work."
685,"While our approach does require domain-specific information extraction models to extract structured representations of novel misinformation claims for easy aggregation and review, there is significant prior work on event extraction that can be adapted to extract check-worthy claims (Ritter et al., 2012; Luan et al., 2019; Du and Cardie, 2020). Furthermore, we argue content moderators or factcheckers are likely to be more effective when focusing on one claim type at a time (e.g. COVID-19 treatments, election integrity, vaccine effectiveness, etc.), rather than reviewing a mixture of claims on multiple topics.
Our COVID-19 case study also makes use of “mock” content moderators, rather than employees or contractors working for social media companies or fact-checking websites. However, we believe this methodology still provides valuable insight that would not be publicly available otherwise, as social media companies do not currently publish extensive details about their content moderation processes6 and fact-checking websites vary widely in policy and have been shown to provide inconsistent claim classification (Marietta et al., 2015). Some prior user studies (Nguyen et al., 2018; Pennycook and Rand, 2019; Shabani et al., 2021) have also shown laypeople (e.g., Amazon Mechanical Turk workers) can be good at judging the veracity of claims or reliability of news articles.
As of late November 2022, Twitter has suspended enforcement of its COVID-19 misleading information policies such as the one we target in this paper.7 However, per the Associated Press article, one of the possible reasons for the suspension was that Twitter has “struggled to respond to a torrent of misinformation about the virus” with many “bogus claims about home remedies” still on the site despite the previous enforcement of policies. While we do not have details about the internal automated systems Twitter has in place to assist with content moderation, an end-to-end early detection system might have helped stem the spread of misinformation on the platform. Additionally, despite the lack of official policy enforcement, our system can still be used by third-party fact-checking websites or researchers to measure and report misinformation
6https://www.nytimes.com/2022/05/19/b usiness/twitter-content-moderation.html
7https://apnews.com/article/twitter-e nds-covid-misinformation-policy-cc232c9 ce0f193c505bbc63bf57ecad6
on Twitter. Finally, the main goal of our work is not to create a system for COVID-19 misinformation detection but rather to propose a framework that allows for a fair and realistic evaluation of early misinformation detection systems in any domain."
686,Section 6
687,"There are several limitations in this work. First, we have not explored how to make use of compositionbased augmentations in the supervised setting. A second limitation is a lack of theoretical grounding in the impact of our latent space composition. Finally, we have not explored interoperability with other training objectives."
688,We provided a negative result in Section 4. We also dedicated a section to this end: Section 8.
689,"One limitation of this work is the focus on Englishto-many and many-to-English settings, while previous studies also went beyond English-centric translation (Freitag and Firat, 2020; Fan et al., 2022). Second, we experiment with a WMT based benchmark that has a total of 15 languages and 200M training examples, when translation models were also trained on larger datasets (Aharoni et al., 2019; Arivazhagan et al., 2019; NLLB Team et al., 2022). We leave questions about the amount of scale that will be required to effectively mitigate interference in massively (many-to-many, billions of parallel sequences) multilingual settings for future work.
Additionally, the data collected from high resource languages may be of higher quality compared to that collected from low resource languages. Further research is needed to determine the impact of low quality training data on interference and synergy. Finally, while we explore trends when scaling models width, deeper models (Ghorbani et al., 2022) might help mitigating interference even further."
690,"Section 7
7 A2. Did you discuss any potential risks of your work? Our work does not add new risks involving translation models"
691,Limitations
692,Limitation section after Conclusion
693,"While we cover a wide range of different factors of cross-lingual semantic parsing (e.g., tasks, datasets, natural languages, meaning representations, domains), we cannot include all possible dimensions along with these aspects. Furthermore, we focus on the linguistic generalization ability for semantic parsing because the questions are translated from the English datasets. In the future, we will explore questions raised by native speakers in each language to study the model ability under variations in cultural backgrounds and information-seeking needs."
694,"Despite promising results, we also observe that refreshing and querying the datastore during training
is time-consuming. Our proposed training framework usually takes 3× ∼ 4× training time. In future work, we will explore methods to improve training efficiency. We include a training loop to dynamically use the latest datastore to inject knowledge into neural networks. However, we still find that the kNN knowledge still helps the inference even after our training loops, demonstrating that there still remains space to improve the effectiveness of knowledge injection."
695,"Limitations and bias of pre-trained models: Our work uses detected objects and their attributes in the images to introduce novel insertions in the corresponding text. To this end, it is important to address the limitations of the state-of-the-art object and attribute detection methods. The undesired artifacts of these methods could be categorized as inaccurate or biased. The detected objects could be incorrect, but since we only consider objects that are also mentioned in the text, the effect of incorrect object detections is non-existent in our augmentations. However, we notice that some of the detected attributes in images and BERT predictions reflect stereotypical associations and have been documented in prior works (Li and Xu, 2021;
Kaneko and Bollegala, 2022). We acknowledge that the current state of deep learning research is limited, and the consequential shortcomings are reflected in our augmentations to some extent. Broader social impact: The authors do not foresee any negative social impacts of this work. We believe our cross-modal augmentations will enable an exhaustive evaluation of the robustness of visionand-language models, leading to more reliable multimodal systems. We release the code for our experiments to aid reproducibility and enable future research on this topic. Annotations, IRB approval, and datasets: The annotators for evaluations done in this study were recruited via Amazon Mechanical Turk. We specifically recruited ‘Master’ annotators located in the United States; and paid them at an hourly rate of 12 USD for their annotations. The human evaluation experiments were approved by the Institutional Review Board (IRB) at the authors’ institution. The datasets used in this study are publicly available and were curated by previous research. We abide by their terms of use."
696,"We highlight several limitations of our work:
Unnatural prompting format The choice to separate inputs and targets using a space character has proven effective to multitask finetune our decoder-only models. Nonetheless, poorly formatted prompts may result in undesirable behavior. For example, given the following prompt: “Translate to English: Je t’aime"", the model may continue the input with additional French content before starting to solve the task, i.e. translating the input from French to English. This can be mitigated by improving the prompts with a trailing full stop or a newline symbol. Encoder-decoder models, such as our mT0, do not suffer from this problem, as inputs and targets are fed into different parts of the model.
Limited languages in xP3 The pretraining corpus of mT0 contains more than 101 languages (Xue et al., 2020), however, we finetune on only 46 languages. Likely, finetuning on the full 101 languages mT0 has seen during pretraining would lead to better performance. However, we decided to use only the languages of BLOOM in order to study language generalization (§4.2). Similarly, one could likely attain better performance by enhancing xP3 with more datasets, such as via BIG-Bench (Srivastava et al., 2022; Suzgun et al., 2022), or more
prompts, such as via NL-Augmenter (Dhole et al., 2021). We have released an extended version of xP3 dubbed xP3x that covers 277 languages and is around ten times larger than xP3, but are yet to finetune models on it.
Performance While our models show strong capabilities of performing tasks zero-shot, there remain numerous failure modes that are common in large language models (Rae et al., 2021; Bommasani et al., 2021; Zhang et al., 2022; Smith et al., 2022; Ouyang et al., 2022; Taylor et al., 2022; Chowdhery et al., 2022; Biderman et al., 2023; Allal et al., 2023; Li et al., 2023). In Figure 16 of Appendix §F, BLOOMZ fails to understand the moral of a fable resulting in an undesirable generation. Similarly, in Figure 15, mT0-13B is asked to provide an explanation, but answers with a question. We have made several modifications to the multitask finetuning recipe, such as loss weighting, mixing in long tasks, and various multilingual aspects, leading to the strong zero-shot performance of our models. However, there are many other changes to the multitask finetuning procedure that are worth exploring to get better models (Honovich et al., 2022; Wang et al., 2022b; Longpre et al., 2023a; Liu et al., 2023; Dettmers et al., 2023; Yin et al., 2023). Further, the pre-trained models we use, BLOOM and mT5, are suboptimal in many aspects such as compute allocation (Hoffmann et al., 2022; Muennighoff et al., 2023), pretraining datasets (Longpre et al., 2023b; Touvron et al., 2023; Chung et al., 2023), pre-training objective (Tay et al., 2022b) and possibly model architecture (Komatsuzaki et al., 2022; Shen et al., 2023). Future work should investigate multitask finetuning better base models.
Learning new languages during finetuning While we have investigated generalization to languages only seen during pretraining, we did not investigate generalization to languages only seen during finetuning. Our mT0 models are finetuned on several new languages not seen in pretraining (see Figure 2). Out of those, we only evaluated on code (HumanEval), where mT0 performed at the random baseline (0.00 in Table 10). We point to follow-up work that has investigated the question of teaching BLOOMZ new languages (Yong et al., 2022; Cahyawijaya et al., 2023) and work investigating adaptation of BLOOM (Ennen et al., 2023; Yong and Nikoulina, 2022)."
697,
698,"Our work has a few limitations that we care to highlight. First, it focuses on interpreting models through the vocabulary lens. While we have shown evidence for this, it does not preclude other factors from being involved. Second, we used E′ = ET, but future research may find variants of E that improve performance. Additionally, most of the work focused on GPT-2. This is due to shortcomings in the current state of our framework, as well as for clear presentation. We believe nonlinearities in language modeling are resolvable, as is indicated in the experiment with BERT.
In terms of potential bias in the framework, some parameters might consider terms related to each due to stereotypes learned from the corpus."
699,"In the section ""Limitations"" after Section 5."
700,"It is important to mention some limitations of our work. Firstly, it would be wise to evaluate the impact of the tokenizer on the performance of the models to ensure that this is not the main reason for the observed performance gains.
Furthermore, we can not affirm in this study whether the medical domain transfer observed from English to French using continual pre-training on PubMedBERT can be generalized to other languages or other domains.
Finally, it is possible that training a ChuBERT model with more diverse private clinical data and in a larger quantity could have brought notable performance gains on private tasks.
A considerable amount of computational resources was used to conduct this study, since approximately 18,000 hours of GPU computation were used to create the 7 models presented here, as well as about 7,500 hours of GPU for debugging due to technical issues related to model configurations and poor performance, for a total of 25,500 hours. The total environmental cost, according to the Jean Zay supercomputer documentation4 is equivalent to 6,604,500 Wh or 376.45 kg CO2eq based on the carbon intensity of the energy grid mention by BLOOM environmental cost study also made on Jean Zay (Luccioni et al., 2022). This makes the present study difficult to reproduce and to transpose to other languages when limited material resources are available.
4http://www.idris.fr/media/jean-zay/jean-zay-consoheure-calcul.pdf"
701,Section 9
702,"The limitations of SENDIR include the following two points: (1) It has not extended to documentlevel entity-centric relations tasks. Our work is event-centric, and future work extends it with entity-centric cases. Document-level entity-centric RE needs to consider multiple mentions of an entity and different relations in different directions of the
same entity pair. (2) It does not bring in external commonsense knowledge. Knowledge can be used to enrich events and improve the accurate ERE."
703,"In the section on ""limitations""."
704,"Limitations. The HuBERT model quality is critical to speech-to-speech translation performance, as its extracted units are used by both speech-tounit model and vocoder. We have not explored the optimal strategy of multilingual HuBERT training. One research question is how to choose a group of languages so that a multilingual HuBERT model could be well trained. For example, it is arguable whether Lithuanian (lt) should be included in Slavic or Uralic family. Other questions could be whether a larger HuBERT with more model capacity should be used and how we should deal with language imbalance in multilingual training.
We provide benchmark results of bilingual speech translation with mined data selected by heuristics. One of our future directions is to come up with a better strategy of mined data selection to improve translation performance and training efficiency.
As mentioned in our results analysis, the reported BLEU scores are heavily dependent on the ASR quality, which may not reflect the speech translation performance accurately. Future directions could be improving ASR quality or exploring other evaluation metrics without reliance on ASR models.
Potential Risks. As a technology used for speech generation, the presented speech translation models or the translation models that will be trained with SpeechMatrix dataset might have systemic bias or produce inappropriate outputs."
705,Limitations of this work is discussed in Section 7.
706,"This paper are based on the assumption that Universal Information Extraction (UIE) models have limitations, particularly with regards to over-reliance on label span boundaries and inflexible attention span length. Therefore, the proposed framework may be computationally and spatially expensive as it requires a more complex attention mechanism and additional computing power for training. Nevertheless, this limitation of the span-based UIE model can be overlooked in comparison to that of the generative UIE model, which uses a stronger language model. Additionally, the probability density functions explored in FSL are limited; thus, further research is needed to develop a more targeted strategy for adjusting the correct information distribution."
707,"Section #Limitation
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
708,"While we discuss limitations to our survey design and results in §3 and §6, we expand on the issues here."
709,Limitations section
710,"We present a new dataset for meeting summarization that has the potential to improve the efficiency and effectiveness of meetings. However, we note that the dataset is limited to city council meetings
from U.S. cities over the past decade and licensing issues have restricted our ability to include certain city council meetings in the dataset. For example, we contacted the City Council of San Francisco and were informed that they do not allow the redistribution of meeting minutes. Moreover, our dataset does not include non-verbal cues such as eye gazes, gestures and facial expressions, which may make it less suitable for developing summarization systems that rely on these cues. Despite these limitations, we believe that the dataset is of high quality and will be a valuable resource for the development of meeting summarization systems."
711,"Section 9
A2. Did you discuss any potential risks of your work? Not applicable. Left blank."
712,"In this paper, our main contribution is an effective and efficient framework for universal IE. We aim to introduce a new unified IE paradigm with extractive structures and triaffine attention mechanism, which can achieve better performance in a variety of tasks and scenarios with more efficient inferencespeed. However, it is non-trivial to decide whether a sophisticated and artificial prompt is required for complex datasets and large label sets. In addition, we only compare with limited baselines with specific datasets configurations when analyzing the performance of the UniEX in supervised, few-shot and zero-shot settings. In experiments, we implement only a few comparative experiments between BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) due to the limit of computational resources."
713,limitations
714,"Limitations of our task definition Depending on the application, attribution and preservation may not deserve equal weight. For instance, if there are multiple acceptable options for the output, such as in a dialog system, we might trade-off preservation for attribution, similar to how LaMDA behaves in our experiments.
Our evaluation metrics also do not measure all aspects of attribution. For instance, some sentences are self-evident and do not require attribution (e.g., “I agree.”) but would be penalized in our evaluation. It is also necessary to note that linguistic assertions have varying scope: for example, there is a difference between “Frozen is a scary movie” and “I got scared watching Frozen” — while expressing a similar sentiment, the former makes a more general statement that many would disagree with, while the latter is scoped to the speaker’s own experience. In some applications, one could even argue that the latter case does not require attribution, since the speaker is their own source-of-truth. In addition to varying scope, utterances can also make assertions with varying levels of directness. For example, according to standard linguistics, “John ate some of the cookies” yields the implicature that John did not eat all of the cookies, even though it is not logically entailed. This raises the question of which implicatures or implied assertions should be detected and attributed, which should be explored in future work. For more nuances, we refer to Rashkin et al. (2021).
For preservation, we wish to explore other properties that should be preserved, such as discourse or logical coherence. Additionally, if the input text passage is completely misguided or flawed, it can be difficult to revise the text without significant changes, which would be heavily penalized by the current metrics.
Limitations of our model While we aspire to improve attribution for arbitrary text, it is clear that RARR is not yet fully general. For example, the current implementation of RARR would not be well-prepared to edit poetry (where preserving rhyme matters) or long documents, primarily because we do not provide examples of such inputs in our few-shot LLM prompts. However, we do believe that future developers may be able to quickly adapt RARR to such tasks by simply changing the prompts. Second, RARR tends to preserve rather
than delete claims that it cannot attribute. Some of these claims genuinely do not require attribution, but others are hallucination and should be removed. Judging whether a claim requires attribution can be subjective and challenging. Finally, our model is computationally costly, since it is based on prompting a large language model. One potential solution is to leverage recent synthetic data generation recipes to train a smaller model (Lee et al., 2021; Schick et al., 2022)."
715,"limitations for the dataset, although we adopt several methods to assure a high quality of the dataset, mislabeled data still exist due to the subjectivity of the annotators. for example, annotators may have different opinions on whether to regard 屁民(shitizen) as unsafe because 屁民(shitizen) is a rare word in chinese and could be both derogatory and selfdeprecating humorously in most cases. moreover, our dataset is in chinese. directly translating safeconv to other languages with translation tools may induce erroneous labels due to syntactic and cultural differences between languages. we call for endeavors to fix it, such as annotating similar datasets in other languages or improving translation strategies. for the experiments, firstly, in section 6, we evaluate the performance of the rewriter based on chatbots of restricted sizes. however, there are large chatbots that we do not include in the evaluation due to the limitation of computing resources, such as eva-xlarge with up to 2.8b parameters, on which the detoxifying results will lead to more comprehensive results. secondly, as shown in table 8, the overall contextual coherence and informativeness of the responses from current state-of-the-art chatbots in chinese are still not satisfying. evaluating safeconv on more powerful chatbots based on large language models is worth exploring in the future."
716,"limitations one trade-off of limiting the prediction space using an extractive pointer module is that it does not support prediction of multiple slot values which is necessary for some dialogues in the multiwoz 2.3 and 2.4 datasets. to keep the architecture simple we do not consider cases in which slots take multiple values in this work, but we can effectively adapt our model for this setting by introducing sequential query tokens for each slot. another limitation is that the span representation requires a computation of o(n · lans) complexity where n and lans represent the length of context and answer span, respectively. for very long answers this might occur significant computational costs compared to exist- ing span prediction approaches which have o(n) complexity. however, this can be alleviated by adding a simple sampling and filtering step during training and prediction. we plan to further study and address these limitations in future work."
717,"limitations first, due to the lack of datasets to evaluate the mpdrg task, we perform our experiments only on the ubuntu irc benchmark and pre-train our model only on the domain of ubuntu chats. however, the potential of our approach goes far beyond that since it is applicable to any open-domain multi-party dialogue dataset. in the future work, we will consider applying our method in more open-domain conversational datasets, such as the transcripts of tv series or movies. additionally, the pre-training process solely relies on the addressee information of individual turns, disregarding the reply-to relations within the dialogue history. this oversight prevents the model from benefiting from valuable contextual cues necessary for a comprehensive understanding of the multi-party dialogue. in our future work, we will explore the integration of discourse-level reply-to relations into the pre-training process to further enrich the capabilities of the model."
718,"limitations we list down some potential limitations of aclm: 1) plms are restricted by their knowledge to generate entirely new complex entities due to their syntactically ambiguous nature. adding to this, substituting complex nes in existing sentences leads to context-entity mismatch. thus, as part of future work, we would like to explore if integrating external knowledge into aclm can help generate sentences with new complex entities in diverse contexts. 2) we do not conduct experiments in the language farsi from the multiconer dataset as neither mbart-50-large nor xlm-roberta-large was pre-trained on this language. 3) the use of mbart-50-large for generation also restricts aclm from being transferred to code-switched settings, and we would like to explore this as part of future work."
719,"limitations this study focuses only on improving the speed of knn-mt during decoding; other problems with knn-mt remain. for example, it still demands large amounts of memory and disk space for the target token datastore. in addition, our subset knn-mt requires to construct a sentence datastore; therefore, the memory and disk requirements are increased. for example, the quantized target token datastore has 52gb (|m| = 862,648,422) and our sentence datastore has 2gb (|s| = 29,540,337) in the experiment of wmt’19 de-en (section 4.2). although subset knn-mt is faster than the original knn-mt in inference, datastore construction is still time-consuming. the decoding latency of our subset knn-mt is still several times slower than base mt for large batch sizes. the experiments reported in this paper evaluated the inference speed of the proposed method on a single computer and single run only; the amount of speed improvement may differ when different computer architectures are used. ethical consideration we construct both knn-mt and subset knn-mt datastores from open datasets; therefore, if their datasets have toxic text, knn-mt and our subset knn-mt may have the risk of generating toxic contents."
720,"limitations we report the following limitations of mildecoding. mil model still suffers from the tradeoff between detoxification effectiveness and lan- guage model quality (wang et al., 2022). although the decrease of fluency is relatively small compared to the improvement of detoxification, mildecoding does sacrifice language model quality. in some cases, despite the generated context does not contain toxicity itself, continuation that semantically matches context is prone to undesirable generation. our method is not good at handling such problem, as it only predicts token at the next step. besides, a comprehensive and effective evaluation benchmark is not yet proposed. in most cases, toxicity is measured with a trained classifier. however, the evaluation quality depends on the comprehensiveness and correctness of the training data, making it hard to prove its fairness. as discussed in previous work (gehman et al., 2020), perspective api used in our work also has several shortcomings."
721,"limitations of the work given that the training data for most pre-trained models has not been released, further investigation of the frequency effects of control verbs in the corpora, or for that matter, of any other critical word in the sentence (names, adjectives, etc.) is not feasible. this is a shortcoming of our work because word frequency during training is known to be an important factor for model performance (wei et al., 2021). nonetheless, in order to approximate this issue, we run preliminary comparisons of the models’ performance depending on whether the control verb appears or not in the vocabulary (and therefore, assuming that it had enough frequency in the training corpus). very similar results were obtained for both sentences with known and unknown verbs in the main clause. besides, detailed comparisons between models have been left out for reasons of space and scope, since the objective of the research was not to compare model performance, although it is a relevant and interesting issue in itself (for instance, the fact that the lms based on the roberta architecture performed better across tasks, or that the high performance of xlm-roberta contrasts with that of mbert). in relation to this, the comparison of models with different architectures and training objectives (e.g. generative models) was also left for further research. finally, it is worth noting that the two languages evaluated in this study (spanish and galician) are very similar, so that it could be interesting to expand the research to non-romance languages."
722,"limitations of mlms along with the iterative nar inference for open-ended text generation and observed that mlms would collapse for open-ltg. through extensive study and analysis, we found the reason is the inappropriate attention mechanism and inference strategies, and introduced two simple strategies to alleviate such a problem, i.e., dynamic sliding window attention and linear temperature decay. experiments demonstrate that our model achieves competitive performance and significant speedup. we hope our research can make pre-trained mlms as new candidates for the open-ltg community."
723,"limitations one limitation of our study is that interpretable feature spaces are at times only semi-interpretable. we infer from patterns of model behavior that buchanan features such as ‘human’, ‘child’, and ‘animal’ can be signals for animacy more broadly construed. the need to conjecture about what a feature means points to a weakness in our approach. some interpretation will always be necessary, and with a more heavy-handed probing method like ours, it can’t be certain what effects are coming from the model and which are coming from the probe. one way to get around this need for subjective interpretation is to train a separate classifier for animacy more broadly understood, and then use the feature prediction model to examine what features are most relevant to the classifier (chersoni et al., 2021). however, this method is not foolproof either. the classification distinction is wholly determined by the labeled data used to train the animacy probe, and the judgments are subjective. even for a seemingly straightforward feature, the correct label is not always clear. is a clock that sings the hour animate? what about a stony face? subjective interpretation is an important and unavoidable component of both linguistic and neural language model analysis. the goal of datadriven research is to extend the sphere of concern beyond self-reflexive subjective judgments of the researcher to the shared subjectivities of a language community. information about animacy reflected in an annotated dataset still reflects subjectivities, but shared ones. it is important to always be clear about where interpretation is happening, whose interpretations are taken into account, and how they affect what"
724,"limitations though our method does not require iterative retraining, pruning, and rewinding process, one question still remains under-explored: how to selfadaptively find the optimal sparsity instead of trial training, which can boost the training efficiency. also, we plan to further investigate the effectiveness of lottery prompt tuning in other scenarios, including the multi-task learning (he et al., 2022b), prompt ensembling (lester et al., 2021b), etc. furthermore, the proposed learning method should be compatible with other parameter-efficient finetuning methods, such as adapter tuning (houlsby et al., 2019) and lora (hu et al., 2021). we leave these for future research."
725,"limitations we discuss the limitations of our research as follows: • firstly, since the t5-large model has many parameters and our task is document level, one training process will occupy four nvidia v100 32gb gpus; • our paper mainly studies document-level eae task. although we believe our approach is compatible with all document-level extraction tasks, how to adapt it to those tasks still remains an open question."
726,"limitations hyper-parameters selection some hyperparameters still acquire careful selection for wecheck, e.g. p+, p−. also, using different set of hyper-parameters for different tasks and datasets will further boost performance. thus, we need to train the model several time and select the best performing parameters based on validation. end-to-end training wecheck applies the weak annotation and noise-aware fine-tuning twostep pipeline, where the noises in the first step will greatly affect the performance of the second step. by modifying the overall framework into end-toend training will solve this problem."
727,"limitations in this paper, we discuss an important topic in the nlp field, the defense against adversarial attacks in nlp applications. we provide a strong defense strategy against the most widely used word substitution attacks in the nlp field, which is limited in several directions. • we are testing defense strategies using downstream task models such as bert and roberta, and the purification tool is a model with a mask-filling ability such as bert. such a process can be further improved with strong models such as large language models. • we study the concept of adversarial purification in the adversarial attack scenarios with word-substitution attacks on small fine-tuned models. the concept of adversarial purification can be further expanded to various nlp applications. for instance, the purification of natural language can be used in malicious text purification which is more suitable in applications with large language models."
728,"limitations although speech performs well on event-centric structured prediction tasks in this paper, it still has some limitations. the first limitation relates to efficiency. as speech involves many tasks and requires complex calculation, the training process is not very prompt. the second limitation relates to robustness. as seen in the experimental analysis in § 4.5, speech seems not always robust to unevenly-distributed data. the third limitation relates to universality. not all eventcentric structured prediction tasks can simultaneously achieve the best performance at the same settings of speech."
729,"limitations naturally, our work comes with a number of limitations: for instance, we restrict ourselves to testing eight pronoun sets out of the rich plethora of existing options. to ensure diversity, we resort to one or two sets per pronoun group—we hope that individuals feel represented by our choices. similarly, we only translate single sentences and don’t investigate translations of larger and possibly more complex texts and we only translate to a number of languages none of which is resource-lean. our study demonstrates that simpler and shorter texts already exhibit fundamental problems in their translations, even to resource-rich languages."
730,"limitations this work introduces a pseudo-entity recognition (per) task to supervise the model learning overlap knowledge. since no additional entity annotation is available, we manually create a mapping functionm(r), which maps each argument role r to an entity type. with the help of the mapping functionm(r), the eae label can be converted to the per label. however, because the annotation of the eae task is complicated, it is hard to avoid a few exceptional samples in the prior mapping function. some entity words may be attached to impertinent entity types. for example, there is a triple of argument role, event type, and argument in rams’s movement.transportartifact.preventexit event: {artifact, object, two pilots}. the ""artifact"" argument is mapped to ""object"" inm(r), but we expect ""two pilots"" can be mapped to ""person or organization"". we tolerate such exceptional samples, and the occasional noise has not affected the training of ape."
731,"limitations as we tentatively give a successful implementation of leveraging soft-prompt-based manner to benefit both single and multi-attribute ctg, such a paradigm deserves a closer and more detailed exploration. first, we explore multi-attribute ctg in the scenario of two-attribute composition, yet combining more attributes when generating a completion is more challenging and thrilling, and still in its fledgeless stage. besides, while extensive experiments demonstrate that tailor consistently improves attribute-based ctg, applying our approach on a wider variety of plms will evaluate the effectiveness of tailor in a more generally way."
732,"limitations although our datasets are publicly available and gathered from participants in different countries, they cannot entirely represent the moral norms from all the individuals in different cultures over the world or predict how moral norms might change into the future (bloom, 2010; bicchieri, 2005). additionally, we examine a limited set of moral issues for each country, therefore the current experiments should not be regarded as comprehensive of the space of moral issues that people might encounter in different countries. moreover, taking the average of moral ratings for each culture is a limitation of our work and reduces the natural distribution of moral values in a culture to a single point (talat et al., 2021). implementing a framework that incorporates both within-country variation and temporal moral variation (xie et al., 2019) is a potential future research direction. currently, it is not clear whether the difference between eplms’ estimated moral norms and the empirical moral ratings is due to the lack of cultural moral norms in the pre-training data, or that the cultural moral norms mentioned in the pre-training data represent the perspective of an english-speaking person of another country. for example, a person from the united states could write about the moral norms in another country from a western perspective. a person from a nonwestern country could also write about their own moral views using english. these two cases have different implications and introduce different moral biases into the system. potential risks we believe that the language models should not be used to prescribe"
733,"limitations the current system may require the user to have some music knowledge to compose the word boundary prompt from music. hence, more efforts need to be made to fulfill this gap before such a system can operate fully automatically without the human user providing word boundary prompt themselves. we use the back-translation of mono-lingual data to augment the parallel training data, but the quality, especially the text style of back-translations has room to improve. although we have tried using iterative bt to gradually refine the backward direction mt model to adapt its outputs to lyric style, we found some errors gradually accumulated in the back-translated data, which finally made our model perform unsatisfactorily for negative sentences, together with the decrease of controlling effectiveness. further exploration is needed in this aspect. similar to chat text, lyrics are usually composed in short sentences. sometimes it would be challenging to guarantee the consistency of style and meaning for different sentences, if the current sentencelevel translation system are adopted. hence, for building future lyric translation systems, it would be a better option to translate the lyrics directly at the paragraph level or document level."
734,"limitations of only considering the comparator when judging a simile. meanwhile, ppl shows a higher correlation than the other two metrics in evaluating fluency, yet having a remarkable gap with the human score. to furtherly explore the concerns of human when evaluating a simile, we also compute the internal correlation of human scores. as shown in appendix table 11, there is a strong correlation between creativity and consistency. it means that having ground is also important in generating a creative simile, illustrating the necessity of interpretably retrieving tenor-vehivle pair in the vehicle-unknown setup."
735,"limitations while the proposed single-frame training approach shows strong performance on various videolanguage datasets, it does not work well on true temporal tasks like the new ssv2 tasks. compared to multi-frame models, our single-frame model also has a higher demand for pre-training data."
736,"limitations a key limitation in our work is that llms might have seen these math problems. our work theoretically assumes this is not the case. another limitation is that for the sake of simplicity, our work makes some assumptions. for example, we assume all numbers in the range of integers 0 to c = 300. this would not cover every mwp out there. and future work is needed to generalize our framework to other forms of mwps. in this work, we are also constrained by the limitations of the openai policy on the gpt-3 api. this limits the number of perturbations we consider in this work as well as the accuracy with which we can estimate our causal distributions. finally, our work is restricted to english, and extending it to other languages will require us to create an mwp dataset in that language."
737,"limitations although our proposed method performs well in evaluating the open-domain dialogue systems, it also has some limitations. our method identifies the dependencies between context and response. however, according to howcroft et al. (2020), human-evaluated metrics can contain a variety of attributes whilst we only identify the large-scale dependencies of semantics and do not disentangle the texts into the attributes of human-evaluated metrics. in the future, we will conduct disentanglement studies to disentangle the text into various attributes to optimise our model and further improve the interpretability of text evaluation methods based on these disentangled attributes."
738,limitations and how these approaches would impact the data bias and the resulting model performance would need to be further researched.
739,"limitations while muda relies on set of hand-crafted rules for tagging specific phenomena, these rules might involve the use of other error-prone systems (such as coreference resolution and alignment models) and these errors might be susceptible to problems (such as lack of out-of-domain generalization) that could limit the applicability of our tagger. however, this could be fixed by extending muda to use newer and better versions of these systems. the use of f-1 per tag with surface-form matching between reference/translation can also lead to penalizing translations that use context correctly but choose other equivalent words. nevertheless, this should also be mitigable by extending the scoring method to, for example, match synonyms. finally, the benchmarking of context-aware models might not apply to newer, state-of-the-art translation models, especially if these leverage large language models that were trained on long-context data."
740,"limitations when applying causal intervention to remove psycholinguistic bias, we utilize the liwc dictionary to construct the confounder dictionary du. we argue that the debiasing performance could be affected by the quality of the constructed confounder dictionary. in the future, we could try to improve the confounder dictionary with external knowledge."
741,"limitations the limitations of this work are mainly twofold. 1. different modalities are trained with the same optimizer setting, which might cause imbalance across modalities. 2. no theoretical analysis is established to provide insight of the balance between modality independence and dependence."
742,"limitations in this work, we propose to use the denoising score matching function to estimate the gradient of logdensity distribution, then describe the differences between the adversarial and normal samples by the denoising process of langevin dynamics. although our method achieves very good detection performance (nearly 100% under various settings), the actual denoising process requires multi-step iterative updates, resulting in a very slow inference speed compared to previous methods. in addition, the trained score network is highly correlated with the domain data, which makes it difficult to achieve good generalization across multiple domains at the same time."
743,"limitations while we show a clear benefit of data augmentation when the amount of available training data is limited, the performance gain seems to be lower when a larger quantity of manually transcribed speech data is available. whether data augmentation is always beneficial is an open question. we did not measure the effect of sociolinguistic variables on the performance of the models. a risk might be that especially for the models for gronings, which were developed on the basis of speech data from only a few speakers, results might be negatively affected by differences in language background (such as speaking a different variety of gronings, or being from a different social group). we likewise did not measure the effect of nonlinguistic variation (e.g., use of different micro- phones) on the performance of the models. while bartelds et al. (2022) showed that wav2vec 2.0 representations are relatively unaffected by nonlinguistic variation, we aim to further explore this in future work. finally, we evaluated the effect of training data size and data augmentation on four different minority languages or language variants, each using a single test set. of course, using a different test set might have affected the results. however, given that the pattern of results was similar across a range of language varieties we do not expect this difference to be large."
744,"limitations our scheduling strategy only re-arranges the training examples after each training epoch, limiting the flexibility of scheduling them compared with re-arranging the examples after each training step. therefore, the order of the training examples will still be fixed within each training epoch. besides, our method finds it challenging to transfer from the task of idiom usage recognition to that of metaphor detection. therefore, more advanced methods for learning the broad nature of non-compositionality, including those of idioms and those of metaphors are needed. we leave this to a future study."
745,"limitations the methodology and experimental approach presented in this paper have certain limitations concerning their practical application and the availability of language resources. the proposed method estimates uncertainty using monte carlo dropout with k iterations and subsequently performing adaptation j times. these additional computations result in increased inference time in real-world applications. empirical evidence suggests that larger values of j lead to a linear increase in time costs in practical scenarios. although the number of j on the wmt21 benchmark was limited in our experiments, the exact cost associated with achieving successful adaptation for new models or datasets remains uncertain. in terms of language resources, the majority of mt metric benchmarks still focus on the news domain, leaving a dearth of multidomain mqm benchmarks for conducting more meta-evaluation experiments during the preparation of this paper. to address these limitations, it is imperative to explore the performance of the proposed methodology on a wider range of out-ofdistribution benchmarks in the future. furthermore, as highlighted by the reviewer, it is also important to note that the proposed methodology does not consistently exhibit performance improvements on certain specific test sets. one possible explanation for this observation could be attributed to our investigation of the optimal learning rate using the wmt20 dataset. the divergence in scoring perspectives between the conventional wmt score and the mqm score might lead to discrepancies in improvement trends."
746,"limitations and broader impacts of diffusiondb in its data sheet (gebru et al., 2020) (‡ a)."
747,"limitations key point hierarchies may be valuable for summarizing opinions and views in multiple domains, including reviews, survey responses, customer feedback, political debates etc. however, in this work, we only demonstrated their value for business and product reviews, leaving other types of data to future work. also, we only attempted to create kphs for english reviews, for which an abundance of resources is available, including a huge number of written reviews and high-quality trained models, e.g. for nli and key point matching. applying these methods to low-resource languages is expected to be far more challenging. finally, the quality of the resulting kphs depends on the quality of the extracted key points provided as input, which may vary across different domains. to alleviate this problem in thinkp, we manually filtered out problematic key points from the dataset (§4.2)."
748,"limitations although this work aims to be as comprehensive as possible, there are several limitations to this paper. our comparisons only consider neural openie models despite rule-based methods being very popular among downstream applications. this is because of the lack of recent surveys on neural openie methods and the difficulties we personally encountered when trying to determine which openie method was state-of-the-art. we acknowledge that there are many cases where rule-based methods may be preferable to neural models due to being faster or more tailor-made for a specific application. however, we feel that focusing on neural openie methods is not a detriment because we are interested in which methods work best ""out of the box"". based on the results reported in these neural openie papers, we believe they are currently the best out-of-the-box openie models using the metrics we report in this paper on the test sets covered in this paper. the corpora we chose are all limited to english. as a result, our results are not generalizable to any downstream task that relies on different languages. in our experiments, we do not report results for the benchie test set or using the benchie metric. this is because the benchie test set uniquely can only be evaluated using the benchie metric, and the benchie metric can only be applied to the benchie test set. we do not feel that its exclusion hurts our final"
749,"limitations evaluation: we evaluate work as a single-label learning problem (accuracy) and a probability distribution (kl). these metrics do not fully capture the nuances of the crowd (inel et al., 2014). we hope to build on this work by moving beyond general population-level predictions to predictions on subpopulations of interest, such as vulnerable communities. we hope to develop better methods for evaluating and assessing the performance of population-level learning. the range of mixing (w =) of the language features and labels in our experiments could be further delved into. our experiments cover weights ranging from 0 to 100 in quartiles, but this parameter, as a hyperparameter, could benefit from additional experiments in finer ranges. datasets: our experimental datasets have been primarily in english. in addressing the ability to generalize, we hope to explore other offensive or hate speech-related datasets from other languages. the challenge of evaluating our models with other languages is acquiring a dataset with annotatorlevel labels, a rare resource for english datasets and challenging for other languages. finally, we hope our methods open the"
750,"limitations the proposed post-abstention methods require additional computation and storage. despite this additional requirement, we note that this is not a serious concern as current devices have high storage capacity and computation hardware. furthermore, additional computation for training auxiliary model in retop is required only once and just an inference is required at evaluation time which has a much lower computation cost. moreover, the risk mitigation that comes with the post-abstention methods weighs much more than the computational or storage overhead in terms of importance. secondly, human-intervention techniques require a human to be a participant and contribute in the answering process. however, these approaches do not expect the participating human to be an expert in the task. like other empirical research, it is difficult to exactly predict the magnitude of improvement a post-abstention method can bring. our idea of exploring sequential application of multiple postabstention methods addresses this concern and can be used based on the application requirements."
751,"limitations the limitations of our work include: 1) in our work, the structure of lyrics is the chorus and verse parts of songs, and it is learned in a data-driven manner, which highly relies on data quality. 2) the settings of the lyric-to-beat model will limit the effect of our model. for this work, we make an assumption that all songs are with 4/4 time signatures for the lyric-to-beat model. if the time signature is not 4/4, we need to re-train the lyricto-beat model.3) better ”beat construction” can be investigated, such as using a language model to generate the beat sequence. we only explore the simple method and achieve satisfying results. 4) the model trained from scratch may not achieve satisfying results. and a gpu with at least 20g memory may be needed to use the pre-trained language model (mt5) to reproduce our work."
752,"limitations the approach of identifying source domains relies on having a contextual sentence but also a target domain available. the datasets available for evaluation do not always provide precise target domains. for example, the lcc dataset provides the target domain gun ownership for the sentence i just don’t know what it will take for people in this country to embrace gun safety, or the target domain climate change for the sentence the event is billed as the largest meeting of influential figures within the renewable energy field. this mismatch often makes it difficult to provide precise source domains. a similar problem also exists when wanting to use our source domain prediction approach in the wild as we have to somehow provide the model with a target domain. while we can provide a target domain by selecting sentences based on seed-word lists designed for specific domains, we do not know how precisely this matches the target domain occurring in the sentence. in a multilingual setting, the issue becomes more pressing since there are very few multilingual metaphor datasets and for semiautomated approaches the seed-word lists would have to be provided for each language. another challenge is connected to the fact that the model output requires time-consuming manual evaluation to obtain a precise accuracy score. however, deciding what counts as a correct source domain can be difficult and might change depending on how strictly the annotators apply certain rules. for instance, whether an annotator sees a pre- dicted source domain as too general or too specific is a matter of degree. overall, this makes it hard to benchmark different approaches across papers, which is why further investigation of automated metrics, as presented in this paper, is crucial. lastly, there are issues regarding the accessibility of large neural language models, such as gpt-3, and the transparency of openai’s api as described in the"
753,"limitations our analysis is limited to non-autoregressive transformer-based models, fine-tuned with the same set of hyperparameters. hyperparameter optimization would undoubtedly lead to better performance for some models, but we fine-tuned each model with standard hyperparameter values for solving sentiment analysis tasks (deyoung et al., 2019) to reduce resource consumption. this should not affect the"
754,"limitations (1) we did not perform any comprehensive hyperparameter search, which would have further consolidated our results. this decision was made due to the high cost of training multiple models. (2) compared to current very large models, glot500-m is comparatively small. (3) although we have tried to minimize the amount of noise in our data, some noise is still present."
755,"limitations in this paper, we conduct experiments only on the chinese benchmark dataset due to the lack of english datasets and comparisons of related methods. moreover, the model is based on bertbase-chinese, so the maximum input length is constrained to less than 512. however, the numbers of words in some long documents exceed the limit, so we use a sliding window to deal with the problem. otherwise, some documents having too many clauses require large gpu resources after aligning and padding. limited by the memory capacity, we have to set a small batch size."
756,"limitations the pretrained bidirectional distillation transfers language knowledge through the nmt training process, a limitation of this method is that a computational overhead is introduced during training. specifically, there is an extra language model forward pass to generate the pretrained bidirectional distillation objectives. although we significantly reduce the computational overhead by designing a self-distilled language model, the overhead cannot be completely avoided. fortunately, most computations stem from back-propagation when model training, and the introduced computational overhead only affects training time. once the training is completed, the nmt has an identical inference cost as regular translation models."
757,"limitations lmrec is trained on user behavior text data that are collected from diverse service applications. these datasets are preprocessed to users’ behavior sequences as detailed in figure 1 and section 3.1. however, in order to improve the quality of user representations, choosing the item information differently for each application may improve the effectiveness. as such, we can consider domain-specific information for each service rather than using general item information. for example, we may leverage additional domain-specific information such as news topics or categories, names of the press agency, and keywords for the news content rather than using only news titles for the news dataset. this issue is a promising extension for practitioners to successfully apply lmrec to real-world applications. the types of task-agnostic data will largely affect the performance gains of lmrec+agnostic and lmrectl+agnostic. we fully utilize four types of taskagnostic data, i.e., search, e-comm., sns, and news, and achieve state-of-the-art results. however, this paper does not thoroughly explore their optimized combination or mixing ratio of the corpus due to the heavy computational costs, which most large lm studies suffer from. while prior work shows how the pretraining corpus sources and their combination affect diverse downstream tasks (raffel et al., 2020; gururangan et al., 2020; lee et al., 2020; krishna et al., 2022; shin et al., 2022), there still remain limitations in finding the generic relation between downstream performance and corpus properties; measuring the effect of the pretraining corpus on the downstream task is still underexplored. we point out that more careful study is left for future research. regarding reproducibility, it is difficult to open our in-house data due to legal issues caused by privacy and user agreement. therefore, we tried our best to validate the efficacy of our lmrec with the experiments on benchmark datasets in addition to in-house data."
758,"limitations our current freqmlm techniques tend to fail on lid predictions when the linguistic differences between languages are small. for example, english and spanish are quite close: (1) they are written in the same script, (2) english and spanish share a lot of common vocabulary. this can confound freqmlm. the strategy to select the best layer for drawing residual connections in resbert is quite tedious. for a 12-layer mbert, we train 10 resbert models with residual connections from some intermediate layer x ∈ {1, · · · , 10} and choose the best layer based on validation performance. this is quite computationally prohibitive. we are considering parameterizing the layer choice using gating functions so that it can be learned without having to resort to a tedious grid search. if the embedded language in a code-switched sentence has a very low occurrence, we will have very few switch-points. this might reduce the number of maskable tokens to a point where even masking all the maskable tokens will not satisfy the overall 15% masking requirement. however, we never faced this issue. in our experiments, we compensate by masking around 25%-35% of the maskable tokens (calculated based on the switch-points in the dataset)."
759,"limitations similar to other augmentation methods, dialogps demands high requirements for computing resources. the training is performed on up to 8 v100 gpus. on dailydialog: a vanilla transformer only needs 50 minutes while a non-pretrained dialogps takes about 80 minutes when k = 1. other baselines take about the same amount of time as dialogps k = 1. but when dialogps achieves its performance peak (k = 16), the training takes 4 hours. most of time cost comes from sampling which is difficult to be accelerated by gpus."
760,"limitations for representation consistency, we apply the regularization to all the tokens and do not distinguish between the different roles the tokens play. adaptive determination of which tokens or chunks require to be consistent in the representation space is an intriguing research question, which we leave as future work. more effective data sampling strategies can also be explored."
761,"limitations although our proposed nuwa-xl improves the quality of long video generation and accelerates the inference speed, there are still several limitations: first, due to the unavailability of open-domain long videos (such as movies, and tv shows), we only validate the effectiveness of nuwa-xl on public available cartoon flintstones. we are actively building an open-domain long video dataset and have achieved some phased results, we plan to extend nuwa-xl to open-domain in future work. second, direct training on long videos reduces the training-inference gap but poses a great challenge to data. third, although nuwa-xl can accelerate the inference speed, this part of the gain requires reasonable gpu resources to support parallel inference."
762,"limitations and future work we identify two limitations in our work that necessitate further investigation and improvement. first, only empirical results are presented in our work. a theoretical understanding of plms calibration is still lacking. going forward, we are motivated to investigate this problem from the standpoint of feature learning. we see great potential in unifying several problems in ai safety (houben et al., 2021) from a feature-learning perspective, including spurious correlations (gu et al., 2019; wang et al., 2022), robustness (yuan et al., 2021; zhang et al., 2022), backdoor learning (sheng et al., 2022; cui et al., 2022), and calibration (ulmer et al., 2022). second, we propose three simple extended calibration methods based on existing ones. in our experiments, we evaluate the calibration performance of existing and our calibration methods. we make an assumption that we have a large held-out validation set that can be employed as the training dataset for the calibration task. we demonstrate the effectiveness of learnable calibration methods in this ideal situation. however, in practice, we need to make the decision about how to allocate the data for the main task and the calibration task given limited training samples."
763,"limitations demonstration selection methods we assume that diversity can be obtained by choosing demonstrations with different program structures. this is based on previous work that demonstrated the importance of diversifying program structures in semantic parsing tasks (oren et al., 2021; bogin et al., 2022; gupta et al., 2022). we also try to diversify utterance words or program symbols but do not consider more complex utterance features that could be applied to a wider range of language understating tasks. we also assume that recall matters more than precision when designing cover-ls algorithm. that means we aim to choose a set of demonstrations that covers every predicted local structure in sỹtest , since it has the potential to be a correct one. we do not predict whether a specific structure should be covered. furthermore, our approach for increasing gold structure coverage by using additional beam candidates could be improved by employing search methods specifically targeted for diversity (meister et al., 2021; narayan et al., 2022). retrievers we used different retrievers for noft and ft setups based on the retriever that worked best on the development set. future research should be conducted to understand why different retrievers are preferred in different setups. a potential method could be to consider both input utterances and programs for retrieval, as suggested in zemlyanskiy et al. (2022)."
764,"limitations despite obtaining promising results, our proposed approach still has the following limitations. first, although our da2lm approach can generate a large amount of target-domain data with high diversity, the generated words are still limited by the source-domain labeled data and target-domain unlabeled data. how to make the model generate novel target-domain words is a challenging problem to explore in the future. second, our da2lm model is primarily proposed for the absa and ae tasks, which are not directly applicable for the other information extraction tasks with more than two elements, such as aspect sentiment triplet extraction (aste). therefore, cross-domain data augmentation for multiple-element information extraction tasks may be a promising followup direction."
765,"limitations our work on compo is subject to multiple limitations. the first limitation is around its scope when probing compositional operations. we only explored compositional substitution for topical snippets in conversations as an initial effort. however, there are many other types of conversation structures that can be leveraged such as conversation stages or specific discourse acts. second, we used a set of external tools to process the conversations for augmentation, such as the use of c99 for topic split and action extraction. although we choose to select widely-used tools with high precision, error cascades are inevitable. furthermore, our approach may not be applicable to low-resourced languages since these pre-processing tools may not be available even in the first place for these low-resourced contexts. we urge future work to further work on this line of compositional data augmentation without any dependencies on external software."
766,"limitations our approach achieves promising results in crossprompt aes by enhancing the consistency between source and target prompts. we believe that this idea can also be used to other cross-domain or domain adaptation tasks. in addition, as can be seen from table 1, our approach fails to perform well in some cases. we think that forcing the representations of two prompts to be closer during model training may result in more errors when the prompts’ grading rubrics, writing genres, and writing requirements are quite different. therefore, there are two possible directions can be explored for future research: 1) more fine-grained shared features can be extracted to improve scoring performance. 2) scoreaware information can be integrated into model to improve source and target prompts consistency."
767,"limitations we consider the current work has the following two limitations: • we design our lightweight ood detection framework based on the prefix-tuning paradigm. nevertheless, there may be other techniques to achieve this goal, which requires further exploration. • for pto + label, each label focuses on its own prefixes, suffering from prefix redundancy problem. one can design share prefixes across different labels to trigger label-invariant sentence features."
768,"limitations our work focuses on assessing recent english vlms on tasks which require fine-grained understanding. here, we outline limitations that we believe are important considerations for future work. first, we only examined a limited number of models. these include (i) strong coarse-grained models, such as albef, clip, flamingo and blip-2, and (ii) two strong fine-grained models, pevl and x-vlm, that build on albef. while we believe our selection of models is representative of strong components in pretrained vlms (such as dual-encoder and cross-modal interactions), we could not easily evaluate different approaches towards fine-grained understanding (e.g., yao et al., 2022a; li et al., 2022a) as the corresponding models and code are not open-source. we hence hope our study will motivate future work to report zeroshot performance on fine-grained benchmarks. second, we evaluate our models in a zero–shot setting using image–text matching. future work could consider how fine-grained understanding improves when fine-tuning for specific tasks. as opposed to relying on image–text matching scores, alternative methods like input ablations, visualising attention or activations could also be used to gain an understanding of potential failure modes. third, though we note specific areas where model performance fluctuates a lot during pretraining, we look forward to future research that improves performance for various such areas, like existence and counting. finally, some datasets we use are quite small. for example, winoground only has 1,600 data points. we hope that our analysis sheds light on the kinds of skills models struggle with and encourages more and larger datasets that test for these skills."
769,"limitations our work has the following limitations. first, we only used one evaluation data, namely se23, because it is the only data suitable for the vwsd setting, especially for the oov examples. in addition, our methodology relies entirely on wordnet. therefore, this may be limited the model’s ability when the target word is a proper noun such as a named entity. finally, we depend on the results of gpt-3 definition generation to handle oov words. since the generated definitions may contain errors, as revealed in the qualitative analyses, the errors led to incorrect predictions. ethical consideration the generated definitions were annotated by two annotators. both annotators were fully paid by complying with local minimum wage regulation. in addition, in the sampled definition generations, the authors could not find any statements violating the acl anti-harassment policy. however, generated definitions that authors have not vetted are still at risk of containing toxic or hates contents (e.g. racism, insulting or xenophobic)."
770,"limitations we identify the following limitations of our work. our current cos’s reranking expert only learns to rerank single-step results. thus it can not model the interaction between documents in case of multipassage evidence chains, which might lead to suboptimal performance, e.g., when we need to rerank the full evidence path for hotpotqa. at the same time, we hypothesize that the capacity of the small model used in our experiments is insufficient for modeling evidence chain reranking. we leave the exploration of learning a full path reranker for future work. also, our current pretraining setup only includes the three bi-encoder tasks, and thus we can not use the pretrained model out-of-box to solve tasks like end-to-end entity linking. consequently, the learned skills from self-supervision can not be chained together to perform configurable zero-shot retrieval. it would be interesting to also include the entity span proposal skill in the pretraining stage, which could unleash the full potential of the chain-of-skills inference for zero-shot scenarios."
771,"limitation of elabor is lack of exploration beyond gpt-3. we consider investigating this problem as our future work. limitations given the ability of elabor to generate free-text elaborations for commonsense question answering, we still observe some cases where the modelgenerated elaborations are not factually correct, or irrelevant to the question, distracting the answer predictor towards incorrect answers. this reflects a limitation of elabor on the controllability of its generations, which is also commonly discovered when using language models for text generation. we consider this as a possible future direction which aims at verifying the factuality and relevancy of model-generated texts before incorporating them for final inference or as a controlling mechanism during generation."
772,"limitation, ethical considerations, and social impacts of this paper are in appendix f and g."
773,"limitations despite the insights obtained through our analysis, certain limitations persist, which we outline in this section. with respect to the re-parameterization of parameters as presented in eq. (3), we adopted the layer-wise setting as proposed by aghajanyan et al. (2021) in order to alleviate memory and computational burdens. nonetheless, such a setting restricts us to only identifying local subspaces, rather than discovering global subspaces within the entire parameter space of a pre-trained language model. the existence of a task-specific global subspace is yet to be ascertained. if such a subspace does exist, the correlation between this global subspace and the identified local subspaces needs to be explored in future research. in terms of experimental settings, the evaluation tasks are limited to natural language understanding tasks, with a lack of natural language generation tasks. on model architecture, decoder-only (e.g., gpt) and encoder-decoder (e.g., t5) models are not included. on model scale, we use basicsize models rather than large ones due to limited computational resources. consequently, the"
774,"limitations we discuss three limitations of this work as follows. the first one is the instability of reinforcement learning. reward-driven policy learning is an essential advantage of this work because it is better equipped with the positive emotion-driven process of esc than existing works and can model flexible esc expression beyond the training data. however, this flexibility also suffers from instability, which calls for additional knowledge or strategies to refine the learning process. the second one is the need for further reference to psychological theory. an advantage of our work is to learn posterior esc patterns integrating the dialogue context and future feedback in the form of rewards. however, there is still other valuable prior knowledge to be referred from psychology studies, e.g., the cbt (cognitive-behavioral therapy) methods. this kind of prior knowledge can be used as additional knowledge to refine the learning process as mentioned in the first limitation. the third one is that the reward design can be further optimized. the ideal case is to construct a high-quality dataset with human-feedback labels for training reward model (e.g., the constructed example of chatgpt). at the same time, the larger parameter of the reward model, the more conducive it is to learn a robust policy and avoid it overfitting to the reward function. however, such optimizations need a trade-off with cost. ethical considerations in this paper, the esconv dataset used in our experiments is a publicly-available benchmark for emotional support conversation, which does not contain sensitive and personal information as well as unethical language. our work builds on this dataset to study positive emotion elicitation to improve the user’s mental state. therefore, we focus on constructing a dialogue system to provide emotional support from families and friends in the daily scenarios limited by this dataset rather than profes- sional psychological counseling or psychological treatment. for risky non-daily scenarios such as self-harm or suicide-related conversations, we do not claim that the dialogue system we built has a treatment or improvement effect on them. additionally, we also ensure the anonymity of our interactive human evaluation. we believe our work meets acl’s code of"
775,"limitations as shown in table 2, our approach underperforms the state-of-the-art supervised model on the smd dataset, where the supervised sota labels a search instruction for each sample. in addition, the lemonpicked example in table 8 demonstrates that some- times it is challenging for the query generator to learn complicated expressions automatically. despite our model’s superiority over all unsupervised methods, these gaps reveal some improvement room of qkconv. in appendix d, we try to bridge the gaps by incorporating a few query annotations. another limitation is that our approach suffers from the time-consuming off-the-shelf knowledge selection when given a large dataset and knowledge base. it takes half of the training hours in knowledge selection since it involves heavy computation of retrieval from a large-scale knowledge base and reranking with a cross-encoder."
776,"limitations our hyde method relies on real-time generation from llms and therefore may not be suitable for tasks that demand high throughput or low latency. however, over the years we have seen the cost of hardware decrease and model compression techniques advance, which may help improve the efficiency of llm inference. meanwhile, as we describe in the"
777,"limitations mutation. we propose a simple but effective gradient-based mutation strategy. more complex mutation methods can be integrated into our framework to further improve attacking effectiveness. black-box attack. dgslow is based on a whitebox setting to craft samples with fewer query times, but it can be easily adapted to black-box scenarios by using a non-gradient search algorithm, e.g., define word saliency based on our fitness function and do greedy substitutions. adversarial defense. we do not consider defense methods in this work. some defense methods, e.g., adversarial training and input denoising, may be able to defend our proposed dgslow. note that our goal is to pose potential threats by adversarial attacks and reveal the vulnerability of dg models, thus motivating the research of model robustness."
778,"limitations despite parla being intended for general-purpose linguistic rule learning, we only tested it on arabic and only to learn morphophonology rules. we also recognize the state of the data and the task being on out-of-context standalone tokens and not continuous utterances which is the nature of spoken languages. this is something we plan to investigate in the immediate future."
779,"limitations imposed by selecting a single curriculum strategy, and instead, focuses on finding and analyzing different curricula that work equally-well for a given model and dataset. in addition, the discovered curricula provide insight into how different portions of the dataset contribute toward learning at different stages of training a model, which, in turn, provide knowledge about the learning dynamics of different models. the task of curriculum discovery could be costly on large datasets, in particular, when the goal is to find optimal curricula for different models and datasets. to mitigate the computational cost, we show that it is possible to rapidly discover a curriculum on a small subset of the dataset (or a smaller version of the model with significantly less number of parameters) and apply the resulting curriculum to the full dataset. there are several promising areas for future work. these include approaches for learning new difficulty indicators from data (e.g., linguistic difficulty including lexical, syntactic and semantic difficulty), prioritizing medium level instances and those with greatest progress during training, and developing challenge datasets that contain diverse data samples with different levels of difficulty. finally, investigating diverse curricula that are suitable for general use and across datasets through curriculum discovery and generalization is a promising area for research. limitations the present work investigates the use of two sample difficulty scoring functions, human-induced annotation entropy and model-induced loss, for nlp models and datasets. the former requires the availability of multiple annotations per sample and the latter requires training an auxiliary model to compute sample instantaneous loss during the course of training. our work does not provide a general solution to the choice or availability of good difficulty scoring functions. however, once such a function is available, our work presents solutions to the problem of finding high-performing curricula in curriculum space. our approach, although effective at finding such curricula, requires a bayesian search of its hyperparameters. we reduce these costs by finding curricula on smaller datasets and smaller models that can then be applied to corresponding larger datasets and models. finally, the proposed method lacks theoretical analysis of the dynamic interactions between data, downstream models, and discovered curricula."
780,"limitation in comparison to other transfer learning methods of nmt, knn-tl incurs extra time costs and more processes to transfer knowledge from the parent model. this is a result of the requirement to construct a high-resource datastore utilizing large-scale parent data and retrieve it. on the other hand, knntl requires a substantial amount of storage capacity due to the storage of a datastore containing millions of entries. we employ the output representation layer for the alignment and the intermediate representation layer for the retrieval. this method justification is mainly supported by the results of model validation (table 4), which might deserve further investigation."
781,"limitations common everyday things change over the years. while we try to choose ones that are in children’s vocabulary, over decades, devices evolve and humans change in which things they interact with more frequently, affecting which relationships would be more prominent in an average person’s mental model. so the parts mental models in such a dataset may not stay constant over time (e.g. some entities may be less familiar and certain relations may be less salient to annotators of the future). it would be interesting to use our parrot dataset as a point of comparison when studying mental models of everyday things in the future to reveal interesting insights on how humans’ mental models of everyday things evolve over time. other important future directions include to explore how more coherent mental models can help in complex reasoning tasks about everyday things, combine these parts mental models with mental models along other dimensions e.g. gu et al. (2022a,b), as well as using our dataset of commonsense queries about everyday things as a source of follow-up questions for existing qa tasks e.g., piqa (bisk et al., 2020) and csqa (talmor et al., 2019). this paper only focuses on relationships (spatial orientation, connectivity, and functional dependency) between parts of everyday things. however, our approach parrot-con is easily extensible to other applications such as: • spatial relations in other domains e.g. for geographical distances, we can similarly impose constraints on inverse relations like closer and further • temporal relations e.g. on a timeline, if event a occurred before event b, then event b cannot have occurred before event a (before is asymmetric) we leave the demonstration of the generalizability of our approach to future works."
782,"limitations notes on key research challenges and decisions that affect the findings of this work. inclusion criteria • venue selection. our systematic review is restricted to papers from major machine learning venues. in order to download and search entire papers, we restrict our review to open-access venues only and exclude all closed-access research. • peer-review focus. we only review peer-reviewed papers, and exclude preprints, technical reports, and other informal articles from our review, even though rouge evaluation frequently occurs in these non-reviewed manuscripts. • archival publications. for completeness, we include all archival acl anthology papers including workshop papers. how- ever, due to technical limitations, we only include the main conference proceedings for non-acl venues. • post-publication changes. historical versions of papers and codebases may contain additional reproducibility information, but we only review current versions (as of january 1, 2023). • external materials. we only review main paper text, appendices, and code linked in papers. we do not review external materials such as websites, slides, videos, or codebases with no link appearing in papers. appendices and supplemental manuscripts distributed separately from the main paper manuscript are not included in our review. • underlying biases. the distribution of papers we review directly reflects the underlying authorship, identity, and content biases (e.g., geography, nationality, gender, language, affiliation, etc.) in papers accepted to machine learning venues. paper annotation • automated annotation. our first paper annotation stage uses automated regular expression pattern matching of paper text. although these patterns are validated and refined through a human-in-the-loop development process, automated pattern matching cannot entirely replace expert human judgement and may incorrectly annotate papers. automated patterns cannot match text in bitmap image figures and tables due to limitations in pdf text extraction. • human annotation. we use a second stage of manual paper review for all papers to identify and correct annotation errors introduced by automated pattern matching. manual review sometimes involves human inference and judgement in challenging cases. (for example, papers that cite “rouge-1.5.5” sometimes use a nonstandard rouge-1.5.5 wrapper instead.) • preliminary search. we perform a preliminary case-insensitive search for “rouge” in all papers. matching papers receive full automated annotation, manual review, and codebase review. however, we are aware of several papers that compute and report rouge scores without specifically naming the metric. they are labeled as non-rouge papers and receive no manual review. • non-english annotation. most reviewed papers are written in english. due to human annotator language limitations and english-oriented automated pattern matching, non-english papers may receive less accurate labels than english papers. • author clarification. contacting authors for clarification may help resolve paper reproducibility questions (for example, see: errington et al., 2021). however, evaluating this aspect of reproducibility is infeasible at the scale of our work. • non-evaluation metrics. some papers use rouge for reasons other than evaluation, such as feature generation or for internal training validation. we do not make any distinction between evaluation and non-evaluation rouge during our review. • assumed correctness. our annotation protocol assumes all papers that use rouge-1.5.5 directly (rather than using a wrapper or reimplementation) report correct rouge scores. however, many of these papers may run rouge-1.5.5 via custom ad hoc wrapper code that (like many wrapper packages) is implemented incorrectly and introduces scoring errors. codebase annotation • codebase linking. we use the papers with code dataset to link papers with codebases. however, this dataset does not cover all papers in our review, which limits our ability to assess their codebase reproducibility. • package inference. many codebases are missing explicit dependency specification, making identifying exact rouge pack- ages challenging. in these cases, function signatures are used to identify the most likely rouge package. • vendored dependencies. in some codebases, rouge package code is “vendored” (copied and pasted into the project code). it is more challenging to accurately identify the source of vendored rouge packages, particularly if the code has been modified. • package aliasing. codebases frequently import very similar versions of rouge packages distributed under different names (examples: ms/rouge and gl/rougescore). we attempt to resolve these packages to a single canonical package for our evaluation. however, slight differences may exist between package aliases that affect our correctness assessment. • multiple packages. when a codebase contain multiple rouge packages, we attempt to identify which packages are used to compute rouge scores reported in the paper. if this is unclear, we list all rouge packages used in the codebase. evaluation experiments • specimen task/model. we choose a single specimen task (cnn / daily mail) and model (lead-3) for measuring rouge scoring discrepancies due to configurations and packages. scoring discrepancies differ for other tasks and models. • summarization focus. although rouge evaluation is used for many different tasks and datasets, our experiments only focus on a single popular task (single-document summarization) and dataset (cnn / daily mail). • english evaluation. rouge was designed for english language evaluation and we perform experiments on the english language cnn / daily mail dataset. while there are rouge packages designed for other languages, there is no universal standard for them like rouge-1.5.5. therefore, we do not cover non-english rouge evaluation in our experiments. • score variants. we only examine three common rouge score variants (rouge-1, rouge-2, rouge-l). we exclude uncommon variants (e.g., rouge-w, rouge-s, rouge-su) rare in papers and often unimplemented in packages. • multiple references. we do not perform any experiments involving multiple reference evaluation, which is not supported by our specimen task (cnn / daily mail) and is not implemented in many nonstandard rouge packages. • bootstrap sampling. bootstrapping is built into rouge-1.5.5 and is often unimplemented or incorrectly implemented in reimplementations. our package experiments operate on individual model outputs and cannot detect bootstrapping errors. • custom implementations. our code review identified several instances of custom rouge implementations, but because we only evaluate packages used by more than one author, it is unknown how correct these custom implementations are. • package versions. many nonstandard rouge implementations change over time (for example: section 5.3). package changes likely affect comparability between papers. however, our evaluation only considers the most recent version of each package (as of january 1, 2023) and does not study these between-version scoring differences."
783,"limitations perception and reasoning in text-based raven. in this work, one limitation is that we do not attempt to solve the perception problem of analogymaking in rpm, rather we apply perfect perception in solving the reasoning part, and assume the perception problem is simple. by doing so, we find that plms may be a strong solution to the reasoning problem here, which may better direct future efforts toward ai and analogy. obviously, the perception problem for idealized domains is a lot different than more natural domains, and identifying key features across many domains that can facilitate a mapping is still a challenging unsolved problem. we hope that our work sparks more interest in this problem. meanwhile, one may argue that our decomposition abstractions are too strong, and actually contribute to the reasoning problem in rpm, as they make an independence assumption about which features of the task can be teased apart. making such an assumption requires an understanding of the problem that cannot be inferred by only seeing one instance. however, we decomposed the task based on very intuitive and common attributes, e.g., shapes, colors, sizes, and counts of items. we believe that the strength of such an abstraction, which could be applied in many problems, should not be understated. nonetheless, we include decomposition-free forms of results as much as possible throughout the paper to help compare the contributions of decomposition versus naming abstractions, which is more clearly only providing perceptual information. in fact, we find that without any decomposition, plms still achieve very strong performance in many cases, and performance gains from decomposition are not always large. human performance. lastly, we note some limitations in the human performance measurements used as reference points. in zhang et al. (2019a), human performance on raven was measured by giving subjects some task-specific training, then evaluating them on the original visual form of the task. this differs from our results in two ways. first, plms had no task-specific training for raven, given that experiments were zero-shot and the text data we generate is new and thus impossible to appear directly in plm pre-training. this may give humans an advantage. second, the task is presented to plms in text form, not visually. while the essential information from the task is preserved by our conversion, it is possible that this conversion would affect the difficulty of the task for humans (making it easier or harder). as such, it becomes unclear how to contextualize our results with these past human results. future work may carry out systematic human studies to compare the analogical reasoning capabilities of humans and plms in different settings. ethical considerations this work does not use any human subjects or human-generated data. our work deals with abstract visual features that are described with numerical symbols, thus not strongly targeting any language. a possible ethical concern for this work is the amount of computational resources used in evaluating plms. to reduce unnecessary computation in our study, we chose to apply plms to only a subset of 500 testing examples from each sub-task of the raven dataset, while the full testing set is four times as large."
784,"limitations while our work tries to focus around reasoning over both fine- and coarse-grained cross-document relationships, qamden, the resulted pre-trained model, might still suffer from factual consistency errors while generating information given a query, and there is no guarantee that it will always generate factual and reasonable content without any further fine-tuning. the qasem question generation model that we used may also have been a source of these problems. there is a possibility that qasem produces inadequate questions that could harm the pre-training process of the model. an attempt was made to filter out noise using a question model, but the results were inferior to non-filtering. consequently, if the model is not fine-tuned, inconsistency (hallucinations) may occur more frequently. in addition, by using the newshead corpus as the pre-training data source, we assume that it is comprised of high quality documents. we also take into account the fact that newshead is limited to documents in the news domain, while some of the benchmarks used for evaluating qamden include another topics of interest. future work may further assess the quality of the documents, such as checking for duplications or wrong statements, and diversify the corpus domains. this is crucial for productizing models like qamden in interactive multi-text applications (chatbots) and semantic search applications which are gaining attraction nowadays (hirsch et al., 2021; eirew et al., 2022). finally, the resulted model qamden was pretrained on sets of related documents, by answering questions that matched their content. as in an out-of-domain scenario, qamden’s use over sets of documents that are not related, or over single documents, might be unexpected. such settings may be the subject of another research direction in the future."
785,"limitations although lgtm has demonstrated superior performance in task-specific knowledge distillation, it is worth investigating the potential benefits of combining lgtm with pre-training knowledge distillation (jiao et al., 2020; wang et al., 2020). additionally, while our experiments have been limited to text classification tasks, which are relatively simple for current pre-trained language models, future work should explore the application of lgtm to more complex text generation tasks."
786,"limitations one limitation of our dataset, elqa, is that the corpus only contains questions in english and about english. however, stack exchange has sites with questions about other languages and our main data extraction scripts are general enough that they can be used to create corpora for other sites on stack exchange. of course, language-specific processing steps, quality assurance and analysis must be applied before releasing such data. most importantly, the models we have presented here are intended only as baselines for future research, not for deployment. potential biases reflecting the demographics of authors represented in the training data (in terms of native language, level of english proficiency, etc.) also need to be considered if models are deployed for different target populations. moreover, many of these types of questions are found on the web, and a lot of the same topics are brought up by many users, so a model’s ability to generate correct answers cannot necessarily be attributed to abstract reasoning."
787,"limitation by collecting an addendum to big-c using images taken locally in zambia. in addition, we plan to further expand to other zambian languages such as tonga, tumbuka, chewa, or lozi, by translating the existing dataset (creating an n-way parallel corpus for zambian languages) and by direct data collection. further down the roan we plan to study the dialectal varieties of bemba and the other languages, by collecting contrastive datasets from different regions of the country. 7note that zambia is a land-locked country. limitations we observe the following limitations with the dataset: • language diversity: in terms of number of languages, the presented dataset only covers two languages; bemba and english. • image diversity all the images used in this dataset were obtained from flickr30k image dataset. therefore, in terms image composition, our dataset is limited to the image diversity in the flickr30k dataset. it mostly lacks images that could be considered as ""culturally relevant"" ones for the zambian or generally sub-saharan african context. we plan to mitigate this in future work."
788,"limitations our approach builds on a task schema that characterizes a task-oriented dialogue system’s domain. for example, the schema captures various attributes of the task. for some domains, when a schema is not pre-defined, it first needs to be extracted, e.g., from a corpus of dialogues. in this paper, we used bert as our lm to be comparable with related work, but more advanced models could further improve the performance. a limitation of our task attribute importance scoring method is that it currently produces a static set of weights, reflecting the domain. in the future, the importance weights may be personalized to the current user’s needs instead."
789,"limitations despite its robustness, our method has subpar results on the automatic semantic metrics compared to the most recent work. this may be a natural consequence of the perceptibility vs. robustness trade-off (tao et al., 2014; de vleeschouwer et al., 2002): a stronger watermark tends to interfere with the original content. nonetheless, by using some technical tricks (e.g. neural infill model, nli-sorted ordering) our method is able to be superior to all the other methods including two traditional ones and a neural network-based method. techniques from adversarial attack were employed to simulate possible corruptions in our work. however, these automatic attacks does not always lead to imperceptible modifications of the original texts (morris et al., 2020a). thus, the corruptions used in our work may be a rough estimate of what true adversaries might do to evade watermarking. in addition, our method is not tested against paraphrasing, which may substantially change the syntactic component of the text. one realistic reason that deterred us from experimenting on paraphrasebased attacks was their lack of controllability compared to other attacks that have fine-grained control over the number of corrupted words. likewise, for text resources like novels that value subtle nuances, the aforementioned property may discourage the adversary from using it to destroy watermarking."
790,"limitations our work still has some limitations: 1) due to the lack of research about the bias mitigation of qe, there is only one directly related work in this area, which serves as the main baseline in our experiments. since the bias of qe is a conspicuous problem, we hope there will be more related work in the future. 2) although our experiments are on wmt qe datasets, we do not implement the complicated data augmentation or model ensemble techniques as described in specia et al. (2021) and zerva et al. (2022), therefore our results can not compete with the best results of the wmt qe evaluation tasks. 3) also, our method requires reference as the positive sample. although most qe data includes reference, there are still chances that the qe data is annotated without the absence of reference, and our method would be hard to apply to such cases."
791,"limitations first, limited by the category of video multimodal fusion tasks, we do not perform experiments on more tasks to better validate the effectiveness of our method, and we hope to extend our model to more various and complete benchmarks in future work. secondly, as shown in section 4.3, our model achieves relatively slight performance improvement on sentiment analysis task. for reasons, our model may be dependent on the scale of datasets to learn noise and redundancy patterns in video, which needs to be further improved and studied."
792,"limitations one limitation of simlm is that it can not be used as a zero-shot dense retriever, since the pre-training framework does not have any contrastive objective. fine-tuning on labeled data is necessary to get a high-quality model. on the other hand, although simlm pre-training is quite efficient thanks to the replaced language modeling objective, it still requires extra computational resources to train the model. ethical considerations if the retrieval corpus contains some offensive or biased texts, they could be exposed to users under certain queries through our dense retriever. to deal with such risks, we need to introduce toxic text classifiers or manual inspection to exclude such texts from the corpus."
793,"limitations we train a ufet model and then fine-tune it for target fet tasks. in our approach, the ufet training data is the main source of limitations. first, the large size ufet training data are automatically generated, and thus may contain errors. such errors can propagate to the fine-tuned fet models. another problem is that, for some entity types, there are not many training examples. moreover, some types useful in specific domains (e.g., adverse drug reaction for the biomedical domain) are not included in the ufet type vocabulary at all. as a result, the ufet model will not be as helpful when applied to fet data that contain such types."
794,"limitations one major shortcoming of feag method is the dependency on creation of counterfactual inputs. if there is an error in counterfactual generation, we might get a wrong feature effect estimate. thus, for simplicity, our evaluation considered tokens as features. the parallel development of counterfactual input generation methods (wu et al., 2021; howard et al., 2022) would hopefully ease this issue and allow feag to be used reliably for spurious correlations on more complex features too."
795,"limitation extension to multi-sentence tasks. our experiments are limited to single-sentence tasks, as we only retrieve single-sentence nearest neighbors to a test input. multi-sentence tasks such as natural language inference would require constructing pseudo-demonstrations that consists of multiple sentences, which we leave for future work. beyond classification. our experiments are limited to classification. extensions to multi-choice tasks or generation tasks requires going beyond a fixed set of options shared between inputs in the demonstrations and the test input. we leave extensions to non-classification tasks for future work. better construction of pseudo-demonstrations. we think future work can explore better constructing the pseudo-demonstrations. for instance, this paper uses manually chosen synonym labels (see appendix b for more detail). we hypothesize that better pseudo-demonstrations can improve performance, which we leave for future work."
796,"limitations in this paper, we build the optimal translation policy under all latency by simply setting the search interval, achieving high performance. however, we think that the performance of our method can be further improved by exploring more interval settings. additionally, although we train the agent using a simple architecture and achieve good performance, there exists a performance gap between the learned policy and the searched optimal policy under low latency. exploring more powerful models of the agent may help improve the performance and we leave it for future work."
797,"limitations our monotonic kd approach requires searching for a hyper-parameter k to strike a balance between monotonicity and translation quality for generating pseudo-targets. the current process requires substantial computational resources to determine the optimal value, which may be different depending on the dataset. more studies are needed to establish an efficient method."
798,"limitation of sequence length on a single device. we have shown that sequence parallelism can handle longer sequence and is more memory-efficient than sota. in particular, sequence parallelism achieves 3.0× maximum sequence length and 13.7× maximum batch size than tensor parallelism when scaling up to 64 gpus. unlike both tensor and pipeline parallelism, sequence parallelism is not limited by the smaller hyper-parameters (e.g., number of attention heads, number of layers). therefore, our sequence parallelism can be adapted as long as the sequence length is divisible by sequence parallel size. with efficient attention, sequence parallelism can handle sequence with over 114k tokens, which is over 27× longer than existing efficient attention works holding the whole sequence on a single device. we used a language model (i.e., bert) to evaluate our system, but it can also be adapted to vision tasks. this work paves the way to process large images (hou et al., 2019) by vit (dosovitskiy et al., 2020) as a larger image means more patches or longer sequences. limitations in order to perform communication between subsequences during training, the use of sequence parallelism can result in increased communication costs, which in turn can slow down the training process. however, by combining sequence parallelism with pipeline parallelism, this issue can be alleviated and the communication cost can be made comparable to advanced forms of model parallelism such as tensor parallelism. nonetheless, sequence parallelism still incurs higher communication costs than vanilla data parallelism. while sequence parallelism is effective for training of unidirectional attention models as well as training and inference of bidirectional attention models, it poses a challenge for unidirectional at- tention models inference due to the autoregressive decoding process. this means that different devices cannot compute in parallel, resulting in reduced throughput and decreased gpu utilization."
799,"limitations there are two limitations of the current must model. first, although pre-trained language models can potentially boost the performance in web information extraction, pre-train a must on web documents has its unique challenges. there are several possibilities for our future exploration. for example, we plan to pretrain a must model by incorporating html-specific tasks, such as masking dom nodes and predicting the relations between dom nodes. second, our model focuses on web pages with single-object, where each target field only has exactly one answer. for a multi-object page, e.g. a movie listing page, there are different movie names corresponding to different movies on the page. however, methods like repeated patterns (adelfio and samet, 2013) can be applied."
800,"limitations due to the limitation of computational resources, we have not evaluated the flan-t5xxl whose number of parameters is 11b, and the opt whose number of parameters is greater than 1.3b. since opt and gpt-neo perform poorly in the zero-shot setting and separating attention scores of each document in the input is tedious for decoderonly models, we choose not to use them as source lms. however, we prove that taking the encoderdecoder model flan-t5base as our source lm is also robust to augment decoder-only models. we will explore new methods to annotate lm-preferred documents of decoder-only models based on their inherent signals."
801,"limitations in the case of complextable, where table images are generated using an auto html table creator that utilizes a web browser engine for rendering, applying tablevlm directly to recognize the structure of handwritten tables without fine-tuning poses a challenge. this is particularly evident when dealing with handwritten tables found in ancient documents. moreover, the process of annotating the structural information of tables in handwritten documents is both time-consuming and laborious. as a result, there is ample room for further exploration and improvement in enhancing the accuracy of table structure recognition for handwritten tables."
802,"limitations our work has some limitations. the design of the co-attention in our method can be improved. currently the design of co-attention in our method is limited to four types, which affects its adaptability. in addition, due to the fact that there is only one publicly available dataset in multimodal sarcasm detection, we conduct our experiments based on it. this has limited the evaluation of the generalization of our method."
803,"limitations we point to several limitations of our work. first, our work considers a popular family of models referred to as “dense retrievers”, but other approaches for retrieval include sparse retrievers (robertson and zaragoza, 2009; bai et al., 2020; formal et al., 2021), generative retrievers (tay et al., 2022; bevilacqua et al., 2022), late-interaction models (khattab and zaharia, 2020), inter alia. while our work draws interesting connections between dense and sparse retrieval, our main focus is on understanding and improving dense models. second, all three dense models we analyze are bidirectional and were trained in a contrastive fashion. while most dense retrievers indeed satisfy these properties, there are works that suggested other approaches, both in terms of other architectures (muennighoff, 2022; neelakantan et al., 2022; ni et al., 2022) and other training frameworks (lewis et al., 2020; izacard et al., 2022b). last, while our work introduces new ways to interpret and analyze dense retrieval models, we believe our work is the tip of the iceberg, and there is still much work to be done in order to gain a full understanding of these models."
804,"limitations present in them. however, the"
805,"limitations our getmtl needs to compute the gi for each task i at each iteration and requires a backwardpropagation procedure over the model parameters. every iteration requires one forward-propagation followed by t backward-propagation procedure and computation of backward-propagation is typically more expensive than the forward-propagation. here, we define the time of one forward pass and one backward pass as ef and eb, respectively. the time of optimization process is defined as eo. therefore, the total time e of getmtl is defined, e = ef + teb + eo ≈ teb + eo for few-task learning scenario (t < 100), usually eo eb and getmtl still works fine. however, for large-scale task set (like t 100), usually eo eb or eo teb. consequently, our getmtl may get stuck in the optimization and backward-propagation process at each iteration. therefore, the major limitation of our work is that it can not be applied to scenarios with large-scale task sets."
806,"limitations in this work, we take the sufficient advantages of the external semantic and syntactic structure knowledge to improve our focused problem. but this could be a double-edged sword to use such features. specifically, our paper has the following two potential limitations. first of all, our method closely relies on the availability of the resources of scene graph structures and syntax structures. while most of the languages come with these structure annotations to train good-performing structure parsers (for example, the syntax structure annotations of penn treebank cover most of the existing languages), some minor languages may not have structure resources. that being said, our idea still works well even in the absence of the targetside structure annotations. with only the structure annotations at pivot-side (resource-rich) language (in this case, the cross-modal semantic&syntactic structure aligning learning are canceled), we can still achieve much better performances than those baselines without using the structural features. besides, our method will be subject to the quality of the external structure parsers. when the parsed structures of scene graphs and syntax trees are with much noise, the helpfulness of our methods will be hurt. fortunately, the existing external semantic and syntactic structure parsers have already achieved satisfactory performances, which can meet our demands."
807,"limitations although dupmae is to learn representation instead of generative models, it performs pre-training on open web data. therefore, it is also subject to potential ethical and social risks, like bias, discrimination, and toxicity. besides, dupmae is pre-trained with comparatively limited amount of data due to the constraint on computation resources. despite that it already achieves a promising retrieval performance at present, it remains to explore whether the performance can be further improved with the scaling up of pre-training data, by leveraging more high-quality datasets like c4 and openwebtext."
808,"limitations decompx is an explanation method for decomposing output tokens based on input tokens of a transformer model. although the theory is applicable to other use cases, since our work is focused on english text classification tasks, extra care and evaluation experiments may be required to be used safely in other languages and settings. due to limited resources, evaluation of large language models such as gpt-2 (radford et al., 2019) and t5 (raffel et al., 2022) was not viable."
809,"limitations several limitations of our study include: 1. only english-language chain-of-thoughts/tasks considered; 2. reliance on gpt-3, which is a closed-source product with an unknown training set (which could itself include some explanations); and 3. focusing only on a single type of student model, opt. more broadly, learning from and with explanations carries some specific risks related to automation bias. while a model might rationalize its predictions using a seemingly coherent string of natural language steps, even if it eventually gets the prediction correct, there’s no guarantee that the eventually predicted output actually results from a process represented by the rationalization. a user might assign excessive confidence to that system based on the chain-of-thought. we observed many cases where the chain of thought seemed promising only to result in models ultimately making incorrect predictions in the final few tokens. caution should be taken when displaying chain-of-thoughts to users."
810,"limitations that increase the difficulty of developing largescale models for generating or ranking clarification questions. it remains challenging to collect and build large amounts of data. in the near future, researchers should optimize the process of acqs based on the current retrieval technologies (see (trippas et al., 2018) for a description of collecting such datasets). (4) multi-modal acqs dataset. recently multi-modal conversational information seeking has received attention in conversational systems (deldjoo et al., 2021). amazon alexa4 organised the first conversational system challenge to incorporate multi-modal (voice and vision) customer experience. however, there is a lack of existing datasets containing multi-modal information for acqs."
811,"limitations in this section, we discuss limitations of rmlm with integrity and attempt to provide valuable directions to further improve our method. there are some potential limitations as follows: 1) rmlm does not perform well on the sst-2 dataset, indicating it may not be applicable to phrase-level datasets with data scarcity. and in some extreme cases of short text, rmlm may often give incorrect predictions. we recommend doing more mlm pre-training using our wordlevel transformation if resources are available. 2) the mitigation is mainly contributed by the transformation and the bert defender. however, there is a lack of exploration of different types of them in this paper. it is worth exploring different transformation schemes (e.g., span masking) and a lightweight model (e.g., albert (lan et al., 2020)) as a defender to reduce the computation overhead. 3) the adopted evaluation is for testing the performance of defense against word-level adversarial attacks. rmlm may expose flaws in mitigating character-level or sentence-level attacks. the applicability of the proposed approach needs more investigation."
812,"limitations inference speed at the same model size, the latencies of grain on different tasks are relatively large compared to the methods like cofi and tinybert. this is because grain generates models with different head size, and the computation of these heads are not parallelized. thus the resulting models are slower than the models with uniform attention structures. this problem could be relieved by introducing model structure regularization at a higher level or by some engineering techniques, such as merging heads with the same or similar size into a large matrix to increase parallelism. backbone models grain is designed for transformer-based models. although the transformer is one of the most popular building blocks of nlp models, there are many other promising structures. the effectiveness of grain on model compression is possibly correlated with hardware lottery or software lottery (hooker, 2020). in addition, we have only tested our method with the standard multi-head attention mechanism. transplanting grain to other attention mechanisms is possible, but the effectiveness has yet to be tested."
813,"limitations of our work have been mentioned in section 6. here, we propose some attempts to overcome these limitations. control signals. in the acquisition of control signals, there are two main constraints for performance, including (1) the accuracy of control signals and (2) the suitability of retrieval results in the testing step. with regard to (1), the results of the oracle setting demonstrate that our framework has a high ceiling when ground-true control signals are given. therefore, we have tried to enhance robustness by noising the control factors. noising methods contain adding, removing, and replacing random control tokens. however, experimental results show that noising methods compromise the success rate of control, which is contrary to the motivation of this work. in the future, this approach can be tried to further improve language quality in scenarios where the demand for controllability is weak. with respect to (2), we focus on the performance of the retrieval model in the inference stage. the control signals straightforwardly come from the retrieved responses. in this paper, we have proposed a task-specific design that combines semantic and emotional similarity to retrieve but it is still simple compared to those sota dialogue response selection models. in future work, it is meaningful to replace our retrieval model with more powerful response selection methods. as an advantage of diffusemp, both the annotating taggers and the retrieval model are orthogonal to empathetic response generation. it is easy for followers to employ higher-performance response selection models and attribute annotating taggers to empower the diffusemp. diffusion models. finally, the diffusion model requires a lot of gpu computational resources and is slow when inference, which limits its application. there are many attempts to reduce the computational resources (rombach et al., 2021a) required by the diffusion model as well as to speed up the process (vahdat et al., 2021) and inference (song et al., 2021; bao et al., 2022). theoretically, the relevant improvements would also have an enhancing effect on our framework and would be helpful for spreading the diffusion model to the nlp community."
814,"limitations our method shows impressive performance but relies entirely on beam search during inference. however, it is well known that beam search is a computationally expensive algorithm. with the beam size of 50, the latency increases from 3.6 times (t5/seq-full) to 7 times (t5/seq) compared to greedy decoding. in addition, the re-ranking process causes another latency (about 12ms in our experiments). therefore, it may not be suitable for real-world dst scenarios. we leave this issue for future work. potential directions may include reducing the current two-step pipeline to an efficient one-step process by employing a novel objective function, using data augmentation, or changing the sequential decoding process to a nonautoregressive approach that can be applied in a parallel manner."
815,"limitations during deployment. our approach has achieved impressive results on multiple natural language processing tasks, including the glge benchmark and three machine translation datasets. furthermore, we have observed that the issue of length prediction consistently limits the performance of the model, especially when dealing with raw datasets. the model struggles to accurately determine the length of the target data, which somewhat affects the model evaluation. in our future work, we will prioritize addressing the challenge of length prediction, aiming to make it more convenient and applicable to a wider range of tasks and scenarios."
816,"limitations since the appearance of large pre-trained models such as gpt-3 (brown et al., 2020), there has been a wave of using large models without fine-tuning to do in-context learning directly to complete various nlp tasks, or to freeze the parameters of large models and then only optimize task-oriented parameters. the proposed hierverb is a lightweight method especially suitable for the case of insufficient labeled training data, but it is difficult to directly extend to a large-scale language model (i.e, >=175b) because large language models are hard to fine-tune in many situations. in future work, we plan to study our method on a larger scale language model in which only parts of parameters specific to downstream htc tasks need to be learned and further, extend our model to the zero-shot learning scenario."
817,"limitations although we show that our sov-mas outperforms the vg-mt5 model under different setups, there are some limitations worth considering to study in future work: (1) in this study, we only provide 44 languages and conduct experiments on them, and future work could extend our method to more languages; (2) the used mas model is based on the generative pre-trained language model, i.e., mt5 (xue et al., 2021). the large-scale model size can bring promising performance while it also consumes more training time (all mt5-based models in this work cost about five days under the multilingual training setting) and releases more carbon dioxide, which may be inconsistent with the theme of green ai. therefore, the work related to model compression (e.g., knowledge distillation) may be possibly future work for the multilingual mas task."
818,"limitations this work presents cats, a large-scale and highquality chinese answer-to-sequence dataset. it is a free and open dataset. one of most important motivations for presenting this dataset is that most of the existing datasets are built for english, which leads to advanced work on d2t generation primarily focusing on english and leaving other languages underexplored. however, cats only alleviates the dataset language bias rather than solving it. and it is limited to the study of chinese methods. regarding methodology, the proposed ugt converts the answer-to-sequence task to a graph-to-text problem to bridge the gap between two heterogeneous input data (sql and table). however, ugt works only for answer-to-sequence task rather than graph-totext task. additionally, though the proposed nse can help the graph-to-text model better preserve the original structural information, the contribution may be limited to the graph-to-text task."
819,"limitations the purpose of our work is to evaluate the ontological knowledge of plms. however, a sea of classes and properties exist in the real world and we only cover a selective part of them. consequently, the scope of our dataset for the experimental analysis is limited. the findings from our experiments demonstrate an imperfect knowledge and understanding obtained by the models, indicating a tangible room for enhancement in both ontological knowledge memorization and understanding and a need for a better ability to address paraphrasing. these observations lead us to contemplate refining the existing pretraining methods to help language models achieve better performance in related tasks."
820,"limitation (i) training computation overheads: although having the same inference complexity as any other two-stage retrieval-based dialogue system, our approach requires more computation resources during training as it needs to optimize the two modules in the meantime. (ii) static negatives: we train both modules with a fixed number of random negative samples for a fair comparison with baselines. actually, more effective negatives can be dynamically sampled by the fast retriever to the smart reranker to further improve its performance. ethical statement our paper primarily aims to enhance the training method for constructing retrieval-based dialogue systems that exhibit improved effectiveness. the training corpora we utilize, such as the ubuntu corpus and the response selection track of the dialog system technology challenge, are openly accessible and do not give rise to any privacy concerns. furthermore, the algorithm we propose is designed to be free from ethical or social bias, ensuring fairness and unbiased performance."
821,"limitations data in this work is limited to the english diachronic word usage graphs (dwugs). our methods themselves are language-agnostic and we do not anticipate serious problems with adapting them to dwugs in other languages (which already exist). at the same time, although flan-t5 is a multilingual lm, we did not thoroughly evaluate its ability to generate definitions in languages other than english. again, definition datasets in other languages do exist and technically it is trivial to fine-tune flan-t5 on some or all of them. generated definitions and mappings between definitions and word senses can contain all sorts of biases and stereotypes, stemming from the underlying language model. filtering inappropriate character strings from the definitions can only help as much, and further research is needed to estimate possible threats. in our experiments with flan-t5, the aim was to investigate the principal possibility of using this lm for definition modelling. although we did evaluate several different flan-t5 variants, we leave it for the future work to investigate the impact of model size and other experimental variables (such as decoding algorithms). the cases shown in §7 are hand-picked examples, demonstrating the potential of using generated definitions for explainable semantic change detection and improving lscd datasets. in the future, we plan to conduct a more rigorous evaluation of different ways to build sense dynamics map."
822,"limitations both the feedback simulator and the feedback evaluator in our work can be further improved. for example, while we simply fine-tuned a pre-trained t5 model as the feedback simulator, future work can design more specialized architectures for it, such as adding relation-aware attention (wang et al., 2020; elgohary et al., 2021) to augment the schema item linking among input components (e.g., question and template feedback in the tqes variant). alternatively, one can also leverage the feedback evaluator to steer the training of the feedback simulator (e.g., via reinforcement learning). as we briefly discussed, one could also extend our feedback simulator to imitate more fine-grained user behaviors, such as the agenda of how users would engage in the error correction process. finally, an intriguing research direction is whether one can leverage our feedback simulator for continually improving a semantic parser from nl feedback, drawing inspirations from clarke et al. (2010); iyer et al. (2017); yao et al. (2020). although our proposed approaches have not made any assumptions on the type of logical forms and can thus be applied to any of them, in experiments, we have only evaluated them in the task of text-to-sql semantic parsing. future research can further assess our proposed models in other semantic parsing settings such as knowledge base question answering (cai and yates, 2013; yih et al., 2016; gu et al., 2021; mo et al., 2022). on the other hand, as our simulator is primarily designed for interactive semantic parsing, it as- sumes meaning representations of both the groundtruth prediction and the model prediction. therefore, generalizing our methods to other nlp tasks may need additional effort. for example, if we apply our methods to a similar interaction scenario for retrieval-based qa (li et al., 2022), then we will additionally need to define logical forms to describe the ground-truth retrieval process and that of the qa model. for open-ended tasks such as keywordbased story generation (pascual et al., 2021), defining such logical forms will need non-trivial effort."
823,"limitations this work focuses on informative image captioning evaluation, including an overall score, vision recall, text precision and token-level scores. the effectiveness of our metric is validated on standard image captioning benchmarks. infometic in this work may not perform well in other captioning tasks due to domain gap, but we contend that our general framework can be adapted to other domains such as text-aware image captioning. for example, for textaware image captioning which focuses more on scene texts in images, we could further encode text regions besides the existing object regions for better comparison with captions. in the future, we will comprehensively explore how to adapt our metric to other captioning tasks, such as text-aware image captioning and video captioning."
824,"limitations of metrics trained directly from overall preference judgments. future work should look deeper into modelling frequent aspects mentioned by expert annotators, such as completeness and ease of understanding, perhaps by taking inspiration from evaluation methods that explicitly localize and categorize errors (freitag et al., 2021; goyal et al., 2022)."
825,"limitations and trends: 1. language bias. most works used chinese and japanese datasets as testbed for training zp models (song et al., 2020; ri et al., 2021). however, there were limited data available for other prodrop languages (e.g. portuguese and spanish), resulting that linguists mainly used them for corpus analysis (pereira, 2009; russo et al., 2012). however, zp phenomenon may vary across languages in terms of word form, occurrence frequency and category distribution, leading to learning bias on linguistic knowledge. thus, it is necessary to establish zp datasets for various languages (prasad, 7https://catalog.ldc.upenn.edu/ldc2013t21. 8https://zhidao.baidu.com. 2000; bacolini, 2017). 2. domain bias. most corpora were established in one single domain (e.g. news), which may not contain rich zp phenomena. because the frequencies and types of zps vary in different genres (yang et al., 2015). future works need more multi-domain datasets to better model behavior and quality for real-life use. 3. become an independent research problem. early works extracted zp information from closed annotations (e.g. ontonotes and treebanks) (yang and xue, 2010; chung and gildea, 2010), which were considered as a sub-problem of coreference or syntactic parsing. with further investigation on the problem, mt community payed more attention to it by manually or automatically constructing zp recovery and translation datasets (e.g. baiduknows and tvsub) (wang et al., 2018a; zhang et al., 2019). 4. coping with data scarcity. the scarcity of zpt data remains a core issue (currently only 2.2m ∼ 0.1k sentences) due to two challenges: (1) it requires experts for both source zp annotation and target translation (wang et al., 2016c, 2018a); (2) annotating the training data manually spends much time and money. nonetheless, it is still necessary to establish testing datasets for validating/analyzing the model performance. besides, pre-trained modes are already equipped with some capabilities on discourse (chen et al., 2019; koto et al., 2021). this highlights the importance of formulating the downstream task in a manner that can effectively leverage the capabilities of the pre-trained models."
826,"limitations in the models and the evaluation metrics. therefore, researchers need to pay more attention to mitigate these biases, such as using diverse data sets and debiasing techniques, to improve the accuracy and fairness of zpt methods."
827,"limitations and future directions the findings reported in this paper have to be seen in light of some limitations and, therefore, they just represent a first step. most of these limitations are related to the ellie dataset itself. first of all, though the predicate-argument combinations used in ellie come from the dtfit dataset and were rated by humans, still the elliptical sentences need human judgements,19 which is one of the future research direction. then, the dataset size is relatively small, especially comparing to other resources on ellipsis (e.g., the 1000 elliptical sentences of the blimp dataset). currently, ellie was mainly conceived as an evaluation dataset but it could be enlarged and become useful for models’ fine-tuning, or for carrying out few-shot learning experiments via prompting. moreover, we tested ellie only with two popular language models, but future works should include the comparison with other systems (e.g., roberta, xlnet, distilled transformer models, gpt-3, etc.) or even with specialized models for ellipsis resolution, to see to what extent our findings are generalizable. concerning the experiments, some changes could be made in the evaluation of task 3. first, we could test the prompts in (4) on the subsets for the other roles, and look for different prompt structures to see if this leads to performance changes. we could also adopt a softer evaluation for this task, by assessing the output in terms of similarity to the target answer. finally, another limitation is related to the strong dependence of our results to the language used for the analysis (i.e., english). from this point of view, a cross-linguistic study on the elliptical structures in ellie could contribute to improve our work from both a theoretical and practical perspective."
828,"limitations 7 and appendix a.3.4, authors and annotators of mpchat are primarily in the us, uk, new zealand, and australia. these demographic and geographic biases mean that mpchat may not equally represent all groups. meanwhile, wang et al. (2021); lee et al. (2022) reported that preprocessing data with clip can cause gender-bias issues. we use clip to measure image-text similarity in the pre-processing for data collection, so this problem may exist in our dataset. users of our dataset should be aware of these risks. to comply with the reddit api terms of use and to protect the privacy of reddit users, commer- cial and for-profit use of our data is limited. it must be available for academic purposes only."
829,"limitations although our method is efficient and scalable, we have not conducted pre-training on large-scale corpora due to limited computational resources. the quality and quantity of data are crucial factors for a pre-training model. as our model only covers 36 languages, it cannot provide services for many rare languages. this paper just proposes a new pretraining direction and does not use many training tricks. exploring dap’s full capability is left for future work. besides, rtl task is not the only possible tokenalignment task for our dap framework. other objectives based on token representations are also worth investigating. the best objective form is still under research."
830,"limitations since our model involves an additional step of ocr, it is less efficient than the end-to-end tit model, although it can achieve significantly better performance. besides, with the incorporation of image information, our model is still unable to completely address the issue of error propagation caused by ocr."
831,"limitations we summarized the limitations of fedlegal as follows: (1) although fedlegal includes a variety of legal tasks with natural language understanding, more useful legal generation tasks should be included, such as legal court debate, legal case summary, etc. however, the tasks in fedlegal are more commonly used in the legal domain com- pared to these tasks. on the other hand, the manual annotation cost is also a limited factor. we will expand more useful legal tasks and also welcome contributions of new datasets to keep fedlegal up-to-date. (2) we do not analyze the fl algorithm’s robustness attacks (i.e., poisoning attacks). we argue that it is impractical to have malicious court participants when multiple official courts perform federal learning. therefore that"
832,"limitations first, as we do not rely on specific corpora and avoid the shortcomings of extractive methods, we also lose their advantages. the typed egs generated by our tp-egg is strongly related to the seed predicates and training data of generation modules, while extractive egs can generate domainindependent egs from large corpora and do not require supervised training data to a considerable degree. second, the edge calculator w is time- consuming even we can control the scales of output egs, as the edge num |e(t1, t2)| will be relatively large for tp-egg to generate powerful egs. furthermore, how to effectively select seed predicates still remains a difficult problem which has not been discussed thoroughly in this work by using the validation datasets. we assume that this problem could be solved by carefully confirming how the seed predicates represent corresponding domain knowledge and we leave it to future work."
833,"limitations the main contributions of this paper are towards tackling over-smoothing issue for learning unsupervised sentence representation. the proposed approach is fairly basic and may simply be extended to improve the performance of other state-of-the-art models. more broadly, we anticipate that the central idea of this study will provide insights to other research communities seeking to improve sentence representation in an unsupervised setting. admittedly, the proposed strategies are restricted with unsupervised training, and biases in the training corpus also may influence the performance of the resulting model. these concerns warrant further research and consideration when utilizing this work to build unsupervised retrieval systems."
834,"limitations this work has two main limitations. first, we only consider baseline models with similar amount of parameters, and pre-trained on similar scale of text corpus for comparison. while we are aware of recent models including t5 (raffel et al., 2020) and palm (chowdhery et al., 2022), they either use huge corpus like c4 (745gb text) for pre-training or contain significantly more parameters than ours. in the future, we will try to find additional computational resources to scale up our model and pre-train on larger text corpus. second, we leverage spacy to segment sentences into words, which is rule-based using spaces, punctuations and other rules. this approach works well on english and many other common languages such as french, german and spanish. but for a few languages that do not use spaces to split words (e.g. chinese and japanese), it will be challenging to retrieve word boundaries. to address this issue, we consider either falling back to character splitting for these languages (similar to multilingual bert) or employing a more sophisticated word boundary detector in future work."
835,"limitations one of the primary limitations of nlp modeling in materials science, including this work, is the low quantity of available data as discussed in section 2. this analysis is affected by this limitation as well given that our evaluations were performed in a low-data setting within a dataset that was already limited in size. we believe that future work can improve upon this study by applying larger datasets, both in the number of samples and in the scope of tasks, to similar problem settings. the small nature of the datasets applied in this study also presents the danger that some of the models may have memorized certain answers instead of achieving a broader understanding, which could be mitigated by enlarging the datasets and making the tasks more complex. moreover, we did not study the generalization of nlp models beyond the materials science domain, including adjacent domains such as chemistry and physics. this targeted focus was intentional but imposes limitations on whether the proposed techniques and insights we gained from our analysis are transferable to other domains, including applying nlp models for scientific tasks outside of materials science. another limitation of our study is the fact that we focused on bert-based models exclusively and did not study autoregressive models, including large language models with billions of parameters highlighted in the introduction. the primary reason for focusing on bert-based models was the diversity of available models trained on different scientific text corpora. large autoregressive models, on the other hand, are mostly trained on general text corpora with some notable exceptions, such as galactica (taylor et al., 2022). we believe that future work analyzing a greater diversity of language models, including large autoregressive models pretrained on different kinds of text, would significantly strengthen the understanding surrounding the ability of nlp models to perform text-based tasks in materials science. while the results presented in this study indicate that domain-specific pretraining can lead to noticeable advantages in downstream performance on text-based materials science tasks, we would like to highlight the associated risks and costs of pretraining a larger set of customized language models for different domains. the heavy financial and environmental costs associated with these pretrain- ing procedures merit careful consideration of what conditions may warrant expensive pretraining and which ones may not. when possible, we encourage future researchers to build upon existing large models to mitigate the pretraining costs. broader impacts and"
836,"limitations in this work, our approach assumes event triggers and argument templates (i.e., ontology) are given. this limits our approach’s applicability, as it requires an event detection system to produce event triggers and event types before llms can be prompted to generate event arguments. we only explore hierarchical events with only 2 levels from the ace05-e ontology and data, which has limited coverage of real-world complex event hierarchy. similar to prior event argument extraction work, our approach relies on a human-curated hierarchical ontology. we leave automatically discover hierarchical ontology for future work. despite llms performing well on eae with few-shot data, compared to existing supervised approaches, their inference is relatively slow and costly11 since the llms we used are generally more than 100x larger in the number of parameters. prior work (zhao et al., 2021; lu et al., 2022) has demonstrated a strong relationship between performance and in-context demonstrations; however, for ease of comparison to supervised baselines, we use the same set of examples from the training set for in-context learning. we expect better selecting (liu et al., 2021) and ordering (lu et al., 2022) incontext examples can benefit code4struct performance, which we leave for future work. ethical considerations since event argument extraction only requires predicting arguments from the given text, the risk of generating toxic languages is relatively low as long as the given test is not toxic. this is because the prediction can be grounded in the input sentence, eliminating potential toxic tokens that did not appear in the original sentence. however, discrimination and bias are possible, as observed in the foundational llms we used (brown et al., 2020; chen et al., 2021; ouyang et al., 2022), which we refer to brown et al. (2020) for detailed"
837,"limitations there are several limitations to mathgpt, both practical and fundamental. first, the model depends on an external method for converting mathematical expressions to opts, currently being latexml. the conversion method is imperfect, which limits mathgpt’s capabilities as it will be presented with many distorted expressions during training and at test time. furthermore, the conversion process is slow and requires dataset-specific engineering to accommodate, making it difficult to deploy the model across many datasets. second, because mathgpt outputs trees rather than sequences, it is fundamentally difficult to evaluate and utilize in text-based settings without a highly accurate tree-to-text converter. the tree-to-text converter is yet another imperfect process in the pipeline, although it could be improved to a reasonable degree with significant engineering effort. third, because mathgpt has additional components and requires more information per token than gpt-2, it has higher space and time requirements that make training more expensive. finally, because mathgpt is pre-trained on highly formal and structured mathematical content, it may struggle to generalize to student-generated mathematical language, which is often error-prone and may exhibit very different patterns."
838,"limitations our method depends on a large-scale paraphrasing corpus. we only test our method on the english ls task. excluding english, other languages have large-scale paraphrasing datasets available, e.g., french, german, chinese, spanish, etc. our method can be easily extended to these languages. but, for some languages that cannot obtain enough paraphrasing datasets, our proposed method cannot be used. another limitation is that our method may struggle to generate substitutions for rare or unusual words and phrases, as they may not have encountered sufficient examples of these words in the training paraphrase data."
839,"limitations in this work, we adopt a data-driven approach to identifying latent relevancy. however, we believe that external knowledge such as knowledge graphs could also be of great help for this purpose, and is thus one of the directions of our future work."
840,"limitations to better inspire the follow-up work, we summarize the limitations of our method as follows: 1) from our experimental results the appendix d, we can see that the estimation of the number of intents in our proposed can be further improved. 2) we do not try more means to prevent knowledge from forgetting. we can probe into the intrinsic structure of unlabeled data in a more fine-grained way by improving the posterior estimation. 3) according to section 5.3, we have verified that both exploration and utilization are indispensable, but at the same time, we only empirically choose the specific proportion of both, without theoretical analysis of the most appropriate proportion for each dataset. we look forward to making progress in the follow-up research on the above limitations."
841,"limitations a key limitation of this work is the dependence on a machine translation system to get highquality translations and annotation projections of the dataset. depending on the availability of language resources and the mt model quality for a given language pair, the translations we use for training and evaluation may be inaccurate, or be affected by translationese, possibly leading to overly optimistic estimates of model performance. in addition, since the annotation projection for relation arguments is completely automatic, any alignment errors of the mt system will yield inaccurate instances. alignment is at the token-level, rendering it inadequate for e.g. compounding or highly inflectional languages. due to the significant resource requirements of constructing adequately-sized test sets, another limitation is the lack of evaluation on original-language test instances. while we manually validate and analyze sample translations in each target language (section 4.1) for an initial exploration of mt effects, these efforts should be extended to larger samples or the complete test sets. finally, we limited this work to a single dataset, which was constructed with a specific set of target relations (person- and organization-related), from news and web text sources. these text types and the corresponding relation expressions may be well reflected in the training data of current mt systems, and thus easier to translate than relation extraction datasets from other domains (e.g., biomedical), or other text types (e.g., social media). the translated examples also reflect the source language’s view of the world, not how the relations would necessarily be formulated in the target language (e.g., use of metaphors, or ignorance of cultural differences)."
842,"limitations our pareto-md doubles computational cost due to training two models simultaneously, which can be a limitation of our approach. however, paretomd obtains significant improvement that is hard to achieve for previous methods of training individual models, thus worthy. besides, our approach would not necessarily result in double training time because these two models can be trained in parallel as implemented by guo et al. (2020). moreover, pareto-md does not affect inference efficiency."
843,"limitations further research is needed to understand the robustness of our over-parameterization framework properly. the results given in this study are constrained by the natural language processing tasks and datasets used for evaluation. even though we employ standard classifications from the literature, the choice of downstream tasks and datasets is still subjective. furthermore, due to computing limitations, we could not investigate the scaling behavior of the large plms. additional study is needed in this area. in addition, as our approach is based on plms that may learn biased information from pretrained corpus, a potential risk is that our approach may also be affected by it and generates improper texts."
844,"limitations one limitation of this work is that we are only considering behavioral data which makes it difficult to 12this is also compatible with the observation by sap et al. (2022) that gpt-3.5 performs considerably better than gpt-3 in answering factual questions about object location changes. further, see madaan et al. (2022) for other tasks for which pretraining on code seems to be beneficial. establish a fully causal link between entity tracking capacities and high performance on our task. entity tracking is a high-level linguistic behavior and many other capacities are necessary for achieving high accuracy on our task. therefore, we cannot rule out that differences in some other capacity, such as interpreting sentences compositionally (see bogin 2022 and bogin et al. 2022 for evidence that gpt-3 and gpt-3.5 models differ in their compositional generalization behavior), are the main driver for the differences in behavior we see across models. a possible criticism of our setup is that it requires short-term memory capacities that exceed the memory capacities of most, if not all, humans. that is, if we presented humans with the same input as the model, we would not expect them to be able to keep track of the contents of all 7 boxes due to memory limitations. therefore we are potentially expecting models to do super-human entity tracking, a setup that has been criticized for model evaluations of other linguistic abilities (lampinen, 2022). we nevertheless believe that our task is justified given the architecture of the evaluated models. transformer-based models can look back to any token in the entire input sequence within their context window, so a proper comparison between humans and models would be to present humans with the full description in written form and let them re-read the description after being prompted to state the contents of a box. while we did not formally evaluate whether humans have this ability on a larger population, we personally did not have any trouble tracking the contents of boxes when we had access to the written description. relatedly, we designed our task such that the entire description fits within the context window of pretrained language models. however, as we mentioned in the introduction, entity tracking is an important ability for understanding long contexts and given the limited context window, our results do not apply to texts whose length exceeds a model’s context window, and likely different model architectures will be necessary to perform proper entity tracking for longer texts. further, while we found that gpt-3.5 models as well as finetuned t5 models can track entities in our task with higher accuracy than a strong random baseline, our results also indicate that this behavior is not very stable once several operations act on an entity. our results should therefore not be taken as justification for using these models for critical applications where high accuracy is needed. lastly, we only evaluated english models in this work. given that we showed that even without high lexical overlap between the training and evaluation examples, models can keep track of entities to some extent, it seems likely that our results also apply to other languages. however, whether this actually the case remains an open question."
845,"limitations data size: sugar is relatively small compared to recently published datasets. this is due to the complexity of our problem setting and annotation pipeline. we prioritized quality over quantity and performed multiple steps of manual intervention to reduce errors, false negatives, and annotation artifacts. these problems have been reported in various nlp tasks not limited to conversational tasks (gururangan et al., 2018; akama et al., 2020; elazar et al., 2020). nonetheless, our experiment has shown that pre-trained transformer models can be trained to outperform a tf-idf ranker by a clear margin, which is encouraging. in addition, we could automatically induce noisy but large-scale training instances from existing resources, for example, by harvesting event pairs that can be used as u and r from event knowledge bases such as atomic2020and generating situation statements using our generator (§3). representation of situation information: in sugar, situation information is represented in textual expressions. in real-world applications, such information could be collected via external apis (e.g., calendar and map) and sensors (e.g., camera) and stored in non-textual forms. our study is a proof-of-concept that shows the understanding of situational information is very important for response selection. future research should explore ways to process situation information that is expressed in other forms of data (e.g., structured texts, numbers, images). even if the value is structured or images, we could transform them into textual forms as done in data-to-text research (shen et al., 2020; miura et al., 2021). besides, we acknowledge that situational information is often under-specified in sugar because some information is considered to be common-sense (e.g., a room has a door) or presupposed (e.g., “please open the door” presupposes that the door is closed.), and such information was not explicitly stated by human annotators during data collection. therefore, response selection systems should be equipped with a mechanism to handle implicit knowledge to solve the task. ethical considerations undesired bias and abusive content: a multitude of sources have reported that data-driven conversational systems can (re)produce undesired bias or abusive language existing in language resources used for development. to minimize such a risk, we carefully curated conversation examples in sugar. our target task is response selection, where systems only produce language in a pre-compiled response list, and therefore, it is not likely that resulting systems yield harmful content. however, users of sugar should be cautious when it is used for developing generation systems in future work. human subjects: crowd workers in amazon mechanical turk (mturk) participated in our data collection pipeline. our annotation tasks were reviewed by the institutional review process before being published in mturk to avoid ethical issues. we did not collect any personally identifiable information of workers other than (anonymized) turker ids. task rewards were decided by several rounds of trials so that workers can receive at least $6.50 hourly. use of external data and tools: we used external datasets such as atomic2020and conceptnet and tools such as spacy and transformers library. we have confirmed that the use of these resources for our research does not violate usage restrictions."
846,"limitations we discuss here the limitations of the proposed diffusionner. first, as a latent generative model, diffusionner relies on sampling from a gaussian distribution to produce noisy spans, which leads to a random characteristic of entity generation. second, diffusionner converges slowly due to the denoising training and matching-based loss over a large noise timestep. finally, since discontinuous named entities often contain multiple fragments, diffusionner currently lacks the ability to generate such entities. we can design a simple classifier on top of diffusionner, which is used to combine entity fragments and thus solve the problem of discontinuous ner."
847,"limitations there are two main limitations in this work. first, instead of best st performance given full data, our cross-modal pre-training only aims to demonstrate the effectiveness of our method in the low-resource st setting. we realize that unified pre-training for both speech and text gradually becomes a dominant paradigm for st and our future work is to fuse waco into a joint pre-training framework. second, we note that tang et al. (2022) explores the possibility of pre-training mt models with phoneme tokenizations, though it is unclear if the phoneme-based mt model has an advantage over the bpe-based mt model. we follow the tradition of using the latter one and leave the comparison of them in future works."
848,"limitations despite outperforming previous best methods, our method still has several limitations and substantial room for future improvement. first, the variety of modules is limited in the current reasoning environment. it would be interesting to introduce a wider variety of modules (e.g., a numerical calculator) to make our method more general. second, our method currently retrieves facts from a fixed corpus. while this is efficient for the specific domain, it may not be sufficient for questions not covered by the fact corpus. it would be more powerful if we retrieve up-to-date information using a modern search engine as our retriever. third, in our experiments, we try our best to select the appropriate prompts to motivate gpt3 and chatgpt to generate reasoning steps and answers. with our prompts, gpt-3 and chatgpt can achieve high answer accuracy. but it is hard to guarantee that our prompts are the best ones to elicit the model’s capabilities completely. finally, although scaling up the size of the language models may lead to emergent abilities (wei et al., 2022a), in this paper, we do not experiment with larger language models (e.g., t5-11b) due to the computational constraints. to the best of our knowledge, our work is foundational research, and we do not find obvious risks related to malicious harmful effects, environmental impact, fairness considerations, or privacy considerations."
849,"limitations section. we will also apply ot-based alignment to problems related yet with different constraints and objectives, e.g., crosslingual word alignment and text matching. limitations in this study, we used standard and basic word embeddings to highlight the characteristics of the different ot problems on unbalanced word alignment. this limits the capability of phrasal alignment. similar to figure 3 (a), we binned all the test samples across datasets (under the ‘sure only’ setting) according to their phrase alignment ratios and evaluated the performance of the supervised ot-based alignment methods.13 specifically, we regarded one-to-many, many-to-one, and many-tomany alignment as phrase alignment. figure 5 shows the trend of the f1 score according to the phrase alignment ratios. obviously, the f1 score degrades as more phrase alignment exists in a sentence pair. one of the straightforward ways to improve the phrase alignment is exploring pre-trained language models enhanced for span representations (joshi et al., 2020) and sophisticated methods for phrase representation composition (yu and ettinger, 2020). in addition, phrase alignment can be addressed from the ot perspective, too, by conducting structure-aware (alvarez-melis et al., 2018) and order-aware (liu et al., 2018) optimal transport. these directions constitute our future work."
850,"limitations all samples used in this work are in english, thus to apply the model to other languages, it will require training data on the specified language or using multilingual language backbones. moreover, we are aware that it remains an open problem to mitigate biases in human stancetaking. of course, current models and laboratory experiments are always limited in this or similar ways. we do not foresee any unethical uses of our proposed methods or their underlying tools, but hope that it will contribute to reducing incorrect system outputs."
851,"limitations in this work, we propose a self-training method which requires unlabeled data in target languages. recall that we remove gold labels from readily available target-language training data from the same public ner dataset, and use them as unlabeled data in our experiments. however, this might not perfectly simulate a real-life application scenario. firstly, most free text in target languages might not contain any predefined named entities. this requires careful data cleaning and preprocessing to produce unlabeled data ready for use. secondly, there might be a domain shift between labeled source-language data and unlabeled targetlanguage data, which poses a question on the effectiveness of our method. furthermore, the ner datasets used in this work contain only a few entity types and different entity classes are relatively balanced. however, on datasets with a larger number of classes, each class will be underrepresented in a batch and a larger batch size might be required for contrastive selftraining to work satisfactorily. also, if the entity type distribution is long-tailed, prototypes for those rare entity types might be inaccurate, and this affects the efficacy of prototype-based pseudolabeling. lastly, as we observe slight drops of pseudo label quality at the end of training for some languages, the pseudo label update strategy can be refined for further improvement."
852,"limitations this work focused on assessing multimodal degree for recent english vl models. the following limitations can be relevant for future work. we only evaluated a limited number of models in a zero-shot setting using their image-sentence alignment and vqa heads. future work might be interested in assessing more models and tracking the evolution of mm-shap scores during model pretraining and finetuning. this work applied mm-shap to vl encoders. we leave it for future work to investigate autoregressive (decoder-only) vl models. in the time it took to review and publish this work, we already encountered efforts to apply shapley values for interpreting vl models in cafagna et al. (2023). we only applied ml-shap to vl models. future work might be interested in models working with other or additional modalities beyond vision and language. computing all possible coalitions between input tokens for shapley values is infeasible because their number is exponential in the number of tokens (2p). therefore we perform monte carlo approxi- mation by randomly sub-sampling 2p+1 coalitions. this results in approximate mm-shap scores per sample. we argue that as an alternative, one can simply increase the number of sampled coalitions for more exact measurements (as we did 10-fold for fig. 1 and the examples in appendix c) – at the cost of increasing the environmental footprint. but it is not necessary to increase the number of samples when estimating mm-shap at dataset level, because the number of coalitions has very little effect on a data-set wide range – given that approximation fluctuations average out. to compute mm-shap at data-set level, one needs to run models in inference mode 2p+1 times, where p is the number of tokens to mask (around 40 in average for mscoco-sized captions). on an nvidia titan x gpu, computing mm-shap for one image-caption pair can take 2 seconds for albef, 3 seconds for clip. lxmert is the most expensive and needs 15 seconds, because it computes image features with a cnn backbone for every masking configuration. ethical considerations this paper uses publicly available datasets and models and therefore could carry on their potential biases (meister et al., 2022; garcia et al., 2023) and imperfections. however, the method presented in this paper enables model and dataset interpretation and we hope that it can help future work locate harmful biases."
853,"limitations in this section, we analyze the limitations of this work: • as it is the first attempt to analyze the mixedinitiative interactions in emotional support conversations, the proposed metrics can be further improved for more robust evaluation. • since the knowledge retrieval is not the focus of this work, we did not spend much space on discussing the choice of different retrieval methods. as shown in table 5, there is still much room for improving the knowledge retrieval from a large scale knowledge graph. it is also worth studying more efficient retrieval methods for retrieving knowledge from a densely connected kg. • the proposed method requires an additional mental health related knowledge graph constructed by experts or knowledgeable workers, which is probably difficult to obtain in some applications. however, different from other knowledgeintensive tasks that can be benefited from opendomain knowledge (e.g., wikipedia), it attaches great importance in the professionals of the knowledge for building a helpful and safe esc system. ethical considerations the datasets adopted are publicly available and widely studied benchmarks collected from professionals or well-trained annotators. all personally identifiable and sensitive information, e.g., user and platform identifiers, in these dataset has been filtered out. we do not make any treatment recommendations or diagnostic claims. compared with existing methods for emotional support conversations, the proposed method can be regarded as one step further to a more safer esc system. the proposed method retrieves knowledge from a well-established mental health knowledge graph, which can be maintained by filtering out harmful information when applying into applications. then the knowledge-enhanced approach can alleviate the randomness during the response generation and provide the guidance towards more positive responses. in order to prevent the happening of unsafe cases, the analysis of emotion intensity prediction can also serve as an alarming mechanism that calls for handoffs to an actual psychologist."
854,limitation of our stimuli is that they were not specifically designed to require tom. new datasets that perform targeted manipulations of tom alongside tests of language comprehension could help reveal how linguistic experience and tom jointly support pragmatic behaviors.
855,"limitations our proposed method needs to construct counterfactual examples to estimate the natural direct effect of disconnected reasoning during the training phase, thus we need a little more gpu resources and computational time. however, the need of resource occupancy and time consumption of our approach does not increase during inference. another limitation is that we use the learnable parameters to approximate the yk1,k2,k∗ . in our future work, we will explore a better approach to model it."
856,"limitations we now discuss limitations of causal-debias. in consideration of the fairness, we follow the prior bias mitigation work (guo et al., 2022; cheng et al., 2021; he et al., 2022) and use human-collected lists of gender and racial pairs for counterfactual data augmentation and intervened distribution generation. it is obvious that the bias word lists are inadequate to cover all the bias-related demographic groups, while we believe the general list is exhaustive. we consider there is a possible model improvement that leverages the perturbation augmentation on bias-related sentences along multiple demographic axes (qian et al., 2022). another possible improvement would be to generate bias words by using prompts to probe the biases that may lead to a bad effect. moreover, we also considered the use of external corpora. the external corpora have been significantly investigated in prior works (liang et al., 2020; cheng et al., 2021) and are utilized as an intervention corpora. recently, he et al. (2022) used two natural language inference data (snli and mnli with gender terms) to produce generalpurpose debiased representations. there are several other corpora including news-commentary-v1 (kaneko and bollegala, 2021), wikipedia (zmigrod et al., 2019; webster et al., 2020), and wikitext-2 (guo et al., 2022). a possible future direction of debiasing is how to mitigate the biases without heavily relying on any corpora and just using internal knowledge. moreover, in the paper we primarily focus on studying gender and racial bias mitigation. it is also worth exploring intersectional biases mitigation (lalor et al., 2022) and domain-specific bias mitigation (chuang and yang, 2022; abbasi et al., 2021). we would also like to note that although causaldebias shows a satisfactory performance on seat tests and crows-pairs, these results should not be interpreted as a complete bias mitigation. interestingly, he et al. (2022) expressed the same opinion. the main metrics (like crows-pairs) are mainly against north american social biases and only reflect positive predictive power. they detect the presence of the biases but not their absence (meade et al., 2022). he et al. (2022) did not use the seat tests and evaluated their model on various metrics. from the perspective of different usage scenarios, we need a more general and reliable debias metric for comparison between different models. the lack of universality and agreement in existing evaluation frameworks is a fundamental challenge in this field."
857,"limitations we acknowledge the main limitation of this work is that we only evaluate our methods on some tasks from the glue and superglue benchmarks due to limited computation resources. and all tasks are not in a realistic few-shot setting, where the number of training samples is less than a few hundred and development sets are not offered. the benefit of peft methods could come from an exhaustive search of hyper-parameters for the development sets, while the realistic few-shot setting could solve this issue and shed more light on peft. it would be interesting to see how our methods and other baselines perform on a wide range of few-shot tasks. in addition, current frameworks are not friendly for sparse fine-tuning methods. most works (diff pruning, fish mask and our pafi) still need to calculate a full gradient of all parameters and selectively update the masked parameters, which makes it cost the same training time as full fine-tuning. last but not least, we only estimate our methods on one single complex task, i.e. wmt 2016 en-ro. one might not draw the same"
858,"limitations one limitation of our work is that manner only explicitly utilizes the memory to enhance the performance of the entity typing module in target domain. however, we argue that the memory could also implicitly enhances the span detection module through the shared pretrained language model with entity typing module. we leave how to explicitly leverage memory to enhance both entity typing and span detection modules as future work."
859,"limitations and ethical considerations there are several significant limitations of the massive dataset and of our modeling. starting with the dataset, the per-language data quantities are relatively small at 19.5k total records and 11.5k records for training. second, there are some lowquality utterances, both in the seed data and in the translations. for the most part, these are surfaced through the judgment scores we provide for each record, but if a user does filtering based on these judgments, then the data size decreases even further. third, the data were originally created through crowd-sourcing, not from a real virtual assistant, which introduces artificialities. relatedly, allowing the worker to decide on translation versus localization of slot entities added further noise to the dataset, although we try to store this decision in the metadata. fourth, our labeling schema is relatively simple when compared with hierarchical labeling schemata or flat schemata with more intent and slot options. fifth, our collection system did not have a robust method to preserving or denoting native tokenization practices—some languages do not separate with whitespace, while others do but there is no set practice. this results in potentially easier (larger chunks to predict slot labels) or harder (each character individually predicted) tasks. sixth, it’s possible, though unlikely, that some of our new crowd-sourced records may contain toxic or otherwise objectionable content. we performed analyses to check for such malicious activities and did not find any as such. regarding modeling, we have only investigated base-sized models in relatively standard setups, leaving room for much more sophisticated modeling. the risks associated with this dataset and work are relatively low, given that we have released a research dataset meant to promote better multilinguality in nlp systems."
860,"limitations the proposed method for improving llms is a post-hoc re-ranking approach, and we do not improve llms themselves due to the difficulty of fine-tuning llms. besides, we improve the ability of constrained language planning for smaller models from the perspective of building task-related datasets, but do not consider investigating the model itself, other than adopting retrieval augmentation. in addition, because automatic metrics for generated text are limited, the automatic evaluation of this paper may result in an overestimation or underestimation of the mentioned methods, though we attempt to mitigate this by incorporating a moderate amount of human evaluation. despite the advanced planning capabilities of newer language models, our work remains significantly valuable to the knowledge distillation of these llms into smaller and more cost-effective models. we also discover several limitations of the proposed coscript datasets. first, the specific goal explored in this work only inherits from an abstract one with one extra constraint. however, in real-life situations, complex planning may involve multiple constraints, which we do not investigate in this work. another limitation of coscript is that our dataset is generated from instructgpt, and thus the data distributions may be biased to favor causal language models. this is a common issue with machine-generated datasets, which we address by manually curating coscript’s validation and test sets. furthermore, there are still some incorrect samples (about 5%) in the training data without manual correction due to the limits of budget and time. last but not least, we only consider whether the script can be executed at the human level. the script execution for robots (huang et al., 2022; lu et al., 2022b) is unstudied in our work, and there still exist huge gaps in transferring complex human language to one that is understandable and executable by robots."
861,"limitations despite the state-of-the-art performances, our proposed methods still have some limitations for future directions. firstly, multi-view prompting creates overheads of training and inference proportional to the number of views. for efficiency in practice, according to figure 3, mvp with a relatively small number of views behaves decently (e.g., 5 or 7). secondly, we apply a simple yet effective aggregation strategy to combine the results of multiple views. more advanced strategies can be explored. lastly, experiments only verified the consistent improvement on absa tasks, while intuitively, the idea of mvp that leverages multiple views can be expanded to any structure prediction tasks, such as information extraction, emotion-cause pair extraction, and stance detection."
862,"limitation of large language models that can be widely and easily exploited by malicious end-users. however, we think the benefits of analyzing bias in reasoning prompts, along with possible methods to mitigate effects, may spur improvements in value-alignment work. because the content of our work is offensive, we include a warning at the start of the paper. we only use previously collected or synthetically generated benchmarks and rely on automated scoring, eliminating exposure of offensive text to human participants."
863,"limitations the outcome on multiple datasets verifies the powerful reasoning ability, which even works on models with only several billion parameters. however, our self-thinking procedure utilizes only one dataset, gsm8k, and the available training set size is only 7.5k. the main reason is the scarcity of high-quality datasets with rich reasoning paths. and, collecting such data incurs huge computation costs and expensive human resources. another limitation is that we have not conducted experiments on bigger language models, such as gpt-3 and palm, due to the expensive usage costs and the fact of no open-source codes. in a nutshell, in the future, we will focus on collecting more highquality labeled data and exploring our method on more powerful language models."
864,"limitations while we consider our approach more easily applicable to new languages than rule-based forward augmentation, it relies on the existence of sufficient original gender-fair text in the language of interest and it is currently unclear what the minimum amount of parallel data is to learn a genderfair rewriting model. additionally, our survey only targets affinity groups which limits the generalisability of our results to all german speakers. since people who choose to not use gender-fair language can simply not use a rewriting system, we do not think that this lack of generalisability is a problem in this case. another limitation is that we use a specific form of gender-fair german in our survey. we made participants aware of this in a disclaimer at the beginning of the survey. it should be stated that there are many different acceptable gender-fair forms in german (see section f). while using a different gender-fair form could affect the individual ratings in our survey, we do not expect that it would change our finding that rewriter outputs are rated more gender-fair than the original texts."
865,"limitations in this work, we demonstrate the effectiveness of the proposed diffusionbert. however, the sampling efficiency in unconditional generation still lags behind fine-tuned gpt and we observe a few sampled sentences lacking coherence when the preassigned length is large (e.g., 128). the issue of inference efficiency is more severe in constrained settings in that mbr decoding samples multiple sentences for one source text. though it brings significant improvement in bleu and rouge-l scores, the sampling time of one batch is several times that of unconditional generation."
866,"limitations the central limitation of minimoe is the increased memory footprint, which we could potentially address in the near future according to appendix g."
867,"limitations as we have shown, there is much room to improve the learning approach, which incur lower costs than increasing model’s parameters or elaborate data engineering. this paper is an exercise in guiding learning focus, and we argue that focusl is not perfect for the positioning method and the relevanceto-weight transformation method. for example, our positioning method may contain noise, and some words that are not important in given knowledge may be used as our learning focus. we will continue to explore better methods to guide the model’s learning focus. meanwhile, our method only experiments on the basic cross-entropy loss, and still needs to be explored for other learning approaches such as contrastive learning."
868,"limitations our work provides an effective solution to augment st when source transcripts are unavailable, which could benefit many unwritten languages. however, limited by the publicly available st datasets, we use english as an unwritten language for experiments, which may slightly differ from realworld unwritten languages. since we never use transcripts in our approach, we believe our work can shed some light on st for real-world unwritten languages. we are glad to explore this if there are available datasets in the future."
869,"limitations this paper aims to investigate a more efficient and effective framework to incorporate the heterogeneous features of both text and graph knowledge. the extensive experiments demonstrate our framework has a superior performance in capturing semantics of input knowledge, thus beating all sota models. however, due to the time and resource limit, we could not conduct further experimentation to compare with promising frameworks in similar areas. in fact, we have observed some other techniques (tang et al., 2022c; yu et al., 2022; wu et al., 2022) may be beneficial to our study, but when considering the difficulty in applying them here (due to additional annotation and knowledge being required), we have to leave them to future work. we also cannot exclude some other factors which may affect performance. for example, we select bart as the base language model in this paper. in practical use, the latest language models (e.g. chatgpt) may have better performance in this task. we have to leave the analysis of these factors to future study."
870,"limitations this work is currently limited to the action chain as the abstract summary of the complete explanation for the given limited observation. in the future, we will further upgrade this task, e.g., considering the progressive textual descriptions as the complete explanation. we hope our work can advance the reasoning ai system research community."
871,"limitations of transformers and, 2) serve as a challenge for generalization in rac over text. in the future, we expect to see more interesting work based on trac, such as better solvers with mechanisms to learn both preconditions and effects, and novel generalization tests that call for more specific reasoning abilities."
872,"limitations we illustrate this paper’s limitations from the following three aspects: 1) limited by the computational resources, we only train udr from the initialization of “bert base uncased” following epr (rubin et al., 2022). we regard explorations based on other competitive pre-trained models like roberta (liu et al., 2019) and deberta (he et al., 2021) as future work. 2) most of current dense demonstration retriev- ers, including udr, are black-box models. although they lead to significantly better performance than bm25, how they find informative demonstrations is still unknown. therefore, a better understanding of the principle of informative demonstration’s retrieval or an interpretable and transparent demonstration retriever may be the next stage of improving demonstration retrieval. xu et al. (2023) propose a more explainable method, beyond-context learning, which first uses the language model to get training data’s next word probability distribution, then assigns test instances with labels of their nearest neighbors with similar next word’s probability distribution. we leave demonstration retrieval with better explainability as future work. 3) in the training stage we use lm to score candidates separately but in the inference stage lm is provided with a sequence of demonstrations. although experimental results demonstrate udr’s effectiveness, we think it is a promising direction to model the dependence between different demonstrations and leave it to future work."
873,"limitations in this study, the limitations can be summarized into two major aspects: (1) the usage-based approach (dunn, 2017, 2019) being employed in our work has the ability to extract most of constructions, while a small portion of non-contiguous constructions (e.g., comparative correlative constructions) are neglected. these noncontiguous constructions are probably fragmented into multiple independent constructions. we will investigate the approaches to capture these noncontiguous constructions via incorporating more syntactic knowledge in future work for language representation enhancement. (2) as discussed in appendix h, the performances are not significant improved on the tasks that contain a large amount of colloquial expressions. since our constructions are mainly learned from the formal corpus, which has less colloquial expressions. it causes fewer constructions to be accessed in these tasks, which encourages us to learn constructions in more diverse corpus to enhance the language representation for natural language understanding tasks in the future."
874,"limitations this study has potential limitations. first, it only focuses on answering existential positive first-order logic queries but does not support the negation operation. we will later address this limitation by modeling the negation operation. second, we uti- lize bert as the backbone model for inductive generalization due to computing resource limits. we plan to investigate the use of more powerful pre-trained language models with stronger generalizability in future research to improve inductive logical reasoning over kgs."
875,"limitations our proposed dimongen task involves generating several diverse sentences to describe the relationships between concepts. however, it does not take into account the number of relationships between different concept pairs. this can lead to problems when applying the model trained on the dimongen dataset to other unseen concept pairs. for example, some concepts may have a small number of relationships, and asking the model to generate a greater number of diverse relationships may lead to hallucinations which can be misleading when using the generative model for educational purposes. we leave this as a future work for the research community. additionally, the performance of the moree model is heavily dependent on the quality of the external corpora used in the retrieval stage. if the corpora do not contain any relevant information for the input concepts, the moree model will perform similarly to a vanilla moe model. an alternative approach is to retrieve information from the web (huang et al., 2022b; lazaridou et al., 2022). last, it should be noted that the base models used in this study were relatively small. recent studies have demonstrated that large language models possess superior reasoning abilities compared to their smaller counterparts (wei et al., 2022; huang and chang, 2022a). future work on exploring the diversified generative commonsense reasoning ability of large language models is encouraged."
876,limitations this work focuses on binary and multi-class classification settings using data in english. benchmarking faithfulness metrics in sequence labeling tasks as well as in multi-lingual settings should be explored in future work.
877,"limitations first, off-policy algorithms like q learning are not explored in this"
878,"limitations we focus on augmenting the hidden representations of a plm. thus most of our baselines, such as dropout (srivastava et al., 2014) and variational information bottleneck methods (mahabadi et al., 2021), do not require unlabeled data. for a fair comparison, we assume that the unlabeled data is not available. therefore, only the limited labeled training set are used to train the autoencoders in our experiments. however, such unlabeled generalor in-domain data (e.g., wikipedia text) are easy to obtain in practice, and can be used to pre-train the autoencoders with unsupervised language modeling tasks, which may help further improve the performance. we leave it for future work. ethical impact deep learning has demonstrated encouraging performance on a wide range of tasks during the past few years. however, neural models are data hungry, which usually requires a large amount of training data to achieve reasonable performance. it is expensive and time consuming to annotate a large amount of data. pretrained language models (plms) (devlin et al., 2019; conneau and lample, 2019; liu et al., 2020) have been proven to be useful to transfer knowledge from massive unlabeled text to downstream tasks, but they are also prone to overfitting during fine-tuning due to overparameterization. in this work, we propose a novel method to help improve model robustness in the low-resource scenarios, which is part of the attempt to reduce neural model reliance on the labeled data, and hence reduce annotation cost. our method has also demonstrated promising performance improvement on cross-lingual nlp tasks, which is also an attempt to break the language barrier and allow a larger amount of population to benefit from the advance of nlp techniques."
879,"limitations the coco-crola benchmark generating procedure is intended to yield multilingual evaluations that can be scaled to even larger sets of concepts and languages without experienced annotators. in the interests of both concept and language quantity scale, we opted for an automated procedure which leverages machine translation systems, which can introduce translation errors. furthermore, variation in the nuance or normative meaning of concepts, particularly culturally contested ones such as “face,” (engelmann et al., 2022) “person,” or “man” will inevitably drive some variance in expected outputs by users across language communities. this cultural variation will place an unavoidable upper bound on the performance of inherently cross-cultural benchmarks such as coco-crola. additionally, typological variation between languages can introduce complications in applying our framework. for example, while simple template filling for prompting is straightforward in chinese, which requires no word-dependent articles, in english phonological properties of the word govern the preceding article, and in spanish and german grammatical gender do the same. hebrew has gendered nouns, adjectives, and verbs but not articles, on the other hand. overall, it appears that these have limited influence as grammaticality isn’t a crucial feature in the prediction of image tokens performed in t2i models, appendix b. while doing so aids in the scalability of the approach, using clip as a feature extractor for computing the metrics, particularly correctness xc and wc, potentially introduces biases due to the english-primary data that clip is pretrained on. future work could test this hypothesis by comparing the performance of coco-crola’s clip-based features with xc computed using inception features (as in fid) (chong and forsyth, 2020) or with dedicated concept-level purpose-trained classifiers."
880,"limitations one limitation of our paper is that the exact distribution of the intrinsic tasks in the original corpus and the constructed data is still unknown. knowing the distribution can offer a better interpretation of the effectiveness of picl, even of the strong performance of large language models. besides, although we can find many constructed instances that share obvious intrinsic tasks (see appendix f), there still exist some instances where the intrinsic tasks are hard to identify. how to better evaluate the contribution of these instances to the icl ability or designing better filtering approaches to select more informative data for icl is worth studying. our task-semantics encoder inevitably contains some bias because it is trained on downstream datasets, although we have tried to ensure a large number and diversity of the dataset collection. however, the final language model is pre-trained on the general corpus, and we add the full document loss, which eliminates the bias to some extent. regarding computing power, we acknowledge that our framework takes relatively large training resources in the retrieval and pre-training process. therefore, we did not conduct experiments based on extra-large language models."
881,"limitations and issues with certain systems (leidner and plachouras, 2017). nlp research can also be used as a political instrument of power, where we can observe mutual relationships between language, society, and the individual that “are also the source for the societal impact factors of nlp” (horváth et al., 2017). in this way, nlp translation can be applied as an instrument to changing the culture of minorities as in traditional translation (cf. section 3.2). so colonizers used translation as means of imperial control and expropriation (cheyfitz, 1997; niranjana, 1992). the asymmetry of power is the cause of domination, where subaltern cultures being flooded with “foreign materials and foreign language impositions” is a real danger for minority cultures (tymoczko, 2006). schwartz (2022) discuss the need to decolonize the scientific approach of the nlp community as a whole, expressing the need for researchers to be cognizant of the history and the cultural aspects of the communities which use the languages they are working with. additionally, he proposes that our research should have an obligation to provide some benefit from our studies to the communities, an obligation of accountability (and therefore be in direct contact with their governing organizations), and an obligation of non-maleficence. the fact that many translation systems nowadays are multilingual8 also result in more multi-cultural challenges (hershcovich et al., 2022). finally, we also want to highlight the importance of discussing mt systems in a text-to-text setup. the usage of text is constrained to certain topics and varies from community to community. for instance, wixarika and quechua, languages that are spoken across all generations, are used in a written fashion mostly in private messaging apps (like whatsapp) but also have a prolific meme and facebook publication generation9. even if a certain community does not widely adopt the written tradition, there are, at minimum legal obligations of the states towards indigenous languages. for example, some constitutions recognize indigenous languages as national languages (e.g., mexico and bolivia), 8multilingual systems refer in nlp to systems capable of translating a set of languages from and to english. in some cases, they are also able to translate between languages where english is not involved. 9for example, wixarika memes: https://www.fa cebook.com/memeswixarika2019, quechua speaking group: https://www.facebook.com/groups/ 711230846397383/ binding the state to the responsibility to translate all official pages, documents, laws, etc., to indigenous languages. this has not been implemented, and this case is a highly valuable application case for machine translation to assist human translation. however, our findings also apply to speech-to-text translation and speech-to-speech tasks that would cover all languages, even with no written tradition."
882,"limitations. it also sheds light on developing better neuro-symbolic systems in general. limitations despite the strong performance of pangu, we identify several limitations that call for further improvement. the first major limitation lies in efficiency. because pangu requires an lm to iteratively score candidate plans, it is resource-consuming in terms of both time and computing. compared with arcaneqa, which efficiently handles complex questions in kbqa, pangu is about twice as slow for both training and inference and consumes about twice as much gpu memory when using the same lm. concretely, to predict a plan of l tokens, generation-based methods involve using an lm to do l forward passes. for pangu, the number of forward passes is proportional to the number of candidate plans, which can range widely. in the future, algorithms with complexity better than o(n), n being the number of candidate plans, are desired to find the top-k candidates. that being said, we would like to note that both arcaneqa and pangu are more efficient than most existing methods due to their efficient dynamic search design. for example, pangu is 8 times faster than rng-kbqa, according to the numbers reported in gu and su (2022). nonetheless, we list efficiency as a limitation because there is clear potential for further improvement. second, though pangu has shown some promising results with codex, the true potential of enabling few-shot grounded language understanding with pangu has yet to be fully realized. we only experiment with a straightforward scoring function and have not experimented with different prompt designs systematically. in the future, we plan to try different prompt designs, retrievers, and scoring functions, including using latest techniques like chain-of-thought prompting (wei et al., 2022). third, though orthogonal to the general framework of our proposal, in our current instantiation, we assume gold plans for training. however, gold plans can be expensive to collect for some environments. exploring fine-tuning lms with weak supervision can be an interesting direction. in addition to proposing candidate plans to the lm, the agent may also respond to the lm with rewards based on its decisions (liang et al., 2017). finally, one important merit of pangu, controllability, is under-explored in this paper, because it is not very necessary for kbqa. while for tasks like text-to-sql parsing, controllability could be a highly desirable property. intruders may manipulate text-to-sql models to launch database attacks via sql injection (peng et al., 2022). with pangu, we can easily get rid of malicious sql operations in candidate enumeration. however, for generationbased methods, such controls are hard to achieve during generation because the decoding process can be shortsighted—it is difficult to tell whether the current predicted token would lead to a malicious operation several steps later. we leave exploration on pangu’s controllability to future work."
883,"limitations although the effectiveness of the iou-aware optimal transport mechanism and the assignmentguided multi-granularity loss has been verified by empirical results on three datasets, the proposed iot framework may still suffer from imprecise boundary identification and co-reference handling, as identified in our earlier"
884,"limitations considering that the golden labels of all instances have been given in the datasets, we directly use these labels as manual labels without performing the manual labeling process. the practice implicitly assumes that all the manual labels are correct. however, with the increase of labeling scale, problems such as (1) inconsistent labeling granularity across annotators and (2) noise in manual labels gradually emerge. how to effectively improve the labeling quality and the robustness of the clustering model are worthy of attention."
885,"limitations our work demonstrates the feasibility of combining query rewriting and query expansion to reformulate a conversational query for passage retrieval. within our proposed convgqr, the rewriting and expansion are based on two plms trained with different data, which introduce additional training load and model parameters for storage. thus, designing an integrated model that can simultaneously generate the query rewrite and the expanded terms would be a promising improvement to our method. another limitation is that the potential answer acting as expansion terms could be generated from more resources (e.g., pseudo-relevant feedback and knowledge graph) rather than only relying on the generative plms. besides, more alternative methods for knowledge infusion can be tested to connect query reformulation with the search task."
886,"limitations and ethical concerns of this work. limitation: data and modeling the dialogues in our dataset are made by playwright, which are slightly different from daily chat. second, the automatic evaluation metrics for the response generation task can not perfectly reflect the interactiveness of dialogue system. lastly, our autoregressive generative model simply add the segment embedding to the inputs. similar to the position encoding in transformer, our coarse method does not make good use of the segmentation, and lacks interpretability."
887,"limitations although mixda achieves promising results on domain adaptation compared with baseline models, there are certain limitations. mixda is a two-stage approach, which is not fully end-to-end. our approach requires training a domain adapter and task adapter, respectively. in the future, we will explore the unifying domain and task adapters by merging them into one."
888,"limitations one limitation of the proposed method, infinity, is that it is currently limited to fully unsupervised and not incorporating any parallel data which may lead to performance deterioration in uncommon scenarios. furthermore, it only works well with languages having limited morphology such as english and may not perform as well on languages with complex morphology. finally, the method may have low scalability to long text as it requires large gpu resources. these limitations inspire further investigation to improve the performance and applicability of the method."
889,"limitations one potential improvement to this work is the development of a method to evaluate the accuracy of the severity measure component. we have demonstrated the effectiveness of sescore2 with a severity measure through improved kendall correlations for various types of retrieval augmented synthesis in figure 4. however, there is currently no widely accepted way to quantitatively measure the accuracy of the severity labels. this is because there is no existing dataset that can be used to benchmark severity measures. while freitag et al. (2021a,b) have released mqm annotations with error spans for each segment, these annotations often include compositional errors that prevent the evaluation of individual severity labels without also considering other errors in the sentence. a potential future direction for this research would be to create a benchmark dataset that would allow direct assessment of individual severity estimation or explore alternative methods for evaluating the accuracy of severity measures. second, we have not been able to test sescore2 on low-resource languages due to the lack of mqm-annotated testing sets in these languages. however, we have demonstrated that sescore2 can still perform well without severity estimation by outperforming top unsupervised metrics such as bertscore, bartscore and prism as shown in figure 4. this suggests that sescore2 may be useful for low-resource languages since parallel corpora are not available for most low-resource language settings. to further verify this, a potential future direction would be to create testing sets with mqm labels for lowresource languages, to test the performance of sescore2 and other learned metrics in such scenarios. lastly, since sescore2 is based on proximity between reference and model output, its capabilities for open-ended text generation tasks have not yet been fully explored. this presents an opportunity for future research to investigate the potential of this method in such scenarios."
890,"limitations it is possible that there is a hidden effect caused by the language pair direction, model selection, or training data and its size. however, our results bear high statistical significance for cases where we desire high correlation and low statistical significance where we expect low correlation. assured by this and concerned by the large cost of training a large number of mt systems, we did not experiment with larger data or other language directions apart from limited additional experiments in tab. 2."
891,"limitations our method has some limitations that should be acknowledged and addressed in future research. one of the main limitations is the restriction of the plm type to masked lms. while this model type has been widely used in previous studies, it may not be the only option. with the ongoing advancements in pre-trained large language models, it is possible that our method could be applied to a wider range of plm types. furthermore, we have only considered three commonly used perturbation types in this study, future studies could investigate a broader range of perturbations and how they interact with each other in determining the constituents. these limitations provide an opportunity to further improve the method and its applicability in the field."
892,"limitations considering modality heterogeneity can promote many related multimodal applications, it is worth continually exploring. in this paper, we propose text-guided fusion (tgf) module equipped with sparse-attention to integrate different modalities in representation aspects, which is an implicit way to build the relations of fine-grained features, such as visual objects, and textual words. previous work (khademi, 2020; wang et al., 2020) has proven that graph convolutional network (gcn) (scarselli et al., 2008) shows advantages in modeling the relations among visual and textual elements. inspired by these works, we argue that explicitly introducing the relationship of fine-grained features via gcn can better guide the model to eliminate redundant features. thus it can further narrow the modalities gap and facilitate fusion for multimodal content understanding. in the future, we will bring gcn to learn multimodal relationships and boost the performance of the model."
893,"limitations, suggesting the possible scope of improvement."
894,"limitation & risks in this paper, we bridge the gap between discourse markers and the underlying relations. we use distributed discourse markers to express discourse more informatively. however, learning dmr requires large-scale data on markers. although it’s potentially unlimited in corpus, the distribution and types of markers may affect the performance of dmr. besides, the current solution proposed in this paper is limited to relations between adjacent sentences. our model can be potentially used for natural language commonsense inference and has the potential to be a component for large-scale commonsense acquisition in a new form. potential risks include a possible bias on collected commonsense due to the data it relies on, which may be alleviated by introducing a voting-based selection mechanism on large-scale data."
895,"limitations this paper evaluates language models for their ability to use gender-neutral pronouns and neopronouns using a template-based dataset, misgendered. while this approach is helpful in assessing bias, the measurements can be sensitive to the choice of templates (delobelle et al., 2022; seshadri et al., 2022; alnegheimish et al., 2022; selvam et al., 2022). consequently, our findings should not be considered as the definitive verdict on the phenomenon of misgendering by language models. there are other limitations to our work that should be considered as well. we also only conduct an upstream evaluation on language models and do not assess downstream applications. our evaluation is also limited to a western conception of gender and restricted to english only. we only consider names and genders assigned at birth in the united states. subsequent changes in names or genders are not taken into account in our analysis. furthermore, our work does not take into account individuals who use multiple sets of pronouns, such as she/they combinations (them, 2021), nor does it consider the full range of nonbinary pronouns as the list continues to expand (lauscher et al., 2022). however, additional names (rare, self-created, or non-western) and neo-pronouns can be directly used with our framework to further evaluate llms. we release our full code dataset to make this easier. lastly, there are larger models that were not evaluated due to limitations in our computational budget. further research needs to be done to address these limitations for the complete assessment of accurate preferred pronoun usage by language models."
896,"limitations in this work, we focused on en→{fr,de,cs} multimodal mt. at the time of writing, our method can only be applied for en→x mmt. it is indeed necessary to have access to a modulated object detector in the source language to extract the features and the image-text relationship exploited by our model. this type of modulated object detector is only available in english for the moment. we leave the extension of our method to non-english source languages to future work. moreover, our method requires large amount of captioning data to perform well. it is therefore computationally expensive."
897,"limitations we strived to make this work as accessible and applicable as possible. however, as with any other research effort, it suffers from several limitations stemming from preconceived assumptions. we believe that the most important limitation of our work is the assumption of the existence of a pre-trained multilingual language model, to be used as an encoder, that supports both the desired source and target languages. though most modern multilingual language models support over a hundred languages, with over 7000 spoken languages in the world, the vast majority of languages remain unsupported. that being said, language models are trained in an unsupervised manner, meaning that only unlabeled data is required for training purposes. as such, a suitable encoder could be trained provided there is access to enough unlabeled data. this leads to what we consider to be the second biggest limitation of our work: the assumption of the availability of unlabeled target-language data. in general, raw unlabeled data is easy to obtain for most languages. however, it can represent a challenge for extremely low-resource languages. in these special cases, training an effective encoder can be an impossibility which, in turn, limits the applicability of our approach. other limitations stem from our constrained time and computational resources. our method requires a gpu with a largeenough memory to fit the transformer-based encoder which is usually more than what a personal computer gpu provides. depending on the dataset and selected batch size, our model requires between 15 and 32 gb of gpu memory. we performed all our experiments on a tesla v100 gpu with 32gb. finally, additional experiments on a more diverse set of source/target language pairs could certainly provide a more comprehensive overview of our method’s strengths and weaknesses."
898,"limitations first, we find robustness deficiencies in metrics by comparing the evaluation differences among metrics. this applies to the case when there are metrics that do not have the same robustness flaws. if there are more latent common defects in the metrics, they cannot be identified by mrt. we leave this topic for future research. second, we use beam search to generate candidates during mrt training, but beam search is also known to have deficiencies. for example, beam search suffers from heuristic search biases and shifts statistics away from those of the data (eikema and aziz, 2020). different decoding methods may have an impact on the experiment results."
899,"limitations first, our model is a method of approximating clustering by contrastive learning, but due to the limitations of the model structure, we cannot directly explore the performance of past clustering algorithms on this task. secondly, due to the large scale of the experiment, our dialogue generator only considers gpt-2. although the ablation study proves the effectiveness of our model, it is a limitation. finally, this paper proposes a complete evaluation framework for personalized dialogue generation. it is very effective, but the specific indicators in it still need to be discussed and further studied. in addition, the model assumes that response and persona are independent gaussian distributions in cvae. although it performs well in the experiment, it does not conform to realistic cognition."
900,"limitations entity knowledge propagation focuses on updating lms’ knowledge about emerging entities. however, there might be cases where knowledge about existing entities needs to be updated (e.g., regime change, new champion, and renaming etc.). we intentionally exclude these cases since they can easily become intractable due to their complexity. for example, an organization changing its name could theoretically reflect a large number of entities that have relations to that organization. by investigating model behavior when a lm encounters new information which is completely unseen during pretraining, we can experiment in a controlled environment. we find ample challenges unaddressed by current research even in this setting. our experiments are conducted on english language models only. while we believe the results can generalize to multilingual models, it is conceivable that the internal representations of these models make them more or less amenable to the sorts of updating explored here. more work is needed to benchmark these techniques in broader settings such as with larger language models and newer parameter-tuning approaches."
901,"limitations compared to a standard knowledge distillation process, our method requires additional computation when preparing training data and training the student. first, our contrastive decoding needs to perform forward pass in the teacher model one time more than greedy decoding does to obtain the perturbed plausibility for each token generated (eq. 4). second, our kd process introduces additional training data for training the student with the counterfactual reasoning objective (eq.5). besides computation cost, this work focuses on improving faithfulness of the rationales rather than performance, which is complementary to prior works which leverages rationales for improving the performance only."
902,"limitations since our approach, tm-hgnn, aggregates every note during icu stays for patient representation learning, it is inappropriate for time-series prediction tasks (e.g. vital signs). we look forward to further study that adopts and applies our approach to time-series prediction tasks."
903,"limitations our main focus in this work is limited to factoid information-seeking questions that typically prompt short answers. however, lexical matching is adopted by more complicated forms of qa that require complex reasoning. more precisely, qa tasks such as multi-hop reasoning (yang et al., 2018), discrete reasoning (dua et al., 2019), and causal relations (lin et al., 2019) also warrant similar systematic analysis as studied in this paper."
904,"limitations our analysis has both methodological and technical limitations. while dependency parsers are the most robust semanto-syntactic tools available to us, we are limited both by the quality of the parser’s output and its paradigm. all automated tools make errors, and while our work uses short and simple phrases that are comparatively easy for these tools to handle, it is possible that even systematic errors could seep into the analysis. it is also possible that other semanto-syntactic tools would highlight different phenomena and improve (or worsen) the quality of the analysis. due to the dataset used, which we picked for quantitative comparison to prior art, there is an inherent bias towards concrete concepts, as they are derived from image captions. we are therefore limited in the understanding of how our method applies to more abstract concepts (say, “love” and “dignity”), potentially warranting further study. there are also concerns about the internal validity of attention maps as an interpretability tool. for example, serrano and smith (2019) argue, “[in many cases,] gradient-based rankings of attention weights better predict [models’] effects than their magnitudes.” however, for the analysis of diffusion models, gradient methods are intractable because a backpropagation pass is required for every pixel for all time steps, as stated in section 2.2. therefore, attention scores remain the most feasible method. lastly, we have consciously limited ourselves to purely making analytical observations regarding attribution and entanglement. this has arguably allowed us to cover a very wide range of phenomena and make a large number of observations, but this choice naturally limits us to not providing a method to resolve the issues we have observed with existing models, which is something we have left (and described in section 6) as future work."
905,"limitations the proposed method is tested for a binary labeling scenario where each instance can belong to one of the labels but not both. the scenario of overlapping labeling space is not tested, nor is the scenario for multi-class labeling space. since we aim to obtain high-quality prompts similar to the base prompt, if the base prompt is very restrictive, then the suggested prompt might be the same as the base prompt. the approach only applies to two moderately sized mlm models, and the extension to other larger models is not tested."
906,"limitations in this paper, we pre-train cclm with moderate multi-modal data, e.g. cc3m, to make a fair comparison with previous work such as m3p and uc2. we leverage large-scale vision language pretraining simply by utilizing the pre-trained weights of x2-vlm which has been pre-trained on billionscale image-text pairs in english. collecting more image-text pairs in different languages will very likely lead to further performance improvements. moreover, there exists larger public available multilingual datasets, such as multiun (ziemski et al., 2016) and opus (tiedemann, 2012). leveraging more multi-lingual datasets for pre-training should also yield a more powerful multi-lingual multi-modal model. as for social impact, multi-modal pre-trained models can be used in applications that help people with disability in one modality. our work makes these applications applicable to minority people speaking non-english, and potentially low-resource languages. in sum, our work potentially enables deep learning technology to benefit more people, and is unlikely to have direct negative social impact."
907,"limitations there are several limitations of our work. we tried training the tn-lcfrs on the discontinuous version of the english penn treebank (dptb, evang and kallmeyer, 2011) but failed to induce any meaningful discontinuous structures. this is possibly because discontinuous phenomena in english are much less common than in german and dutch. for example, while 5.67% of the gold constituents are discontinuous in negra, only 1.84% gold constituents are discontinuous in dptb (corro, 2020). the neural lcfrs was also quite sensitive to hyperparameters and parameterization. the instability of unsupervised structure induction is widely acknowledged and could potentially be mitigated by a large amount of training data, as suggested by liang and klein (2008) and pate and johnson (2016). due to this sensitivity, we rely on dev sets for some modeling choices (e.g., rank of the probability tensors). hence, our approach is arguably not fully unsupervised in the strictest sense of the term, although this is a common setup in unsupervised parsing due to the mismatch between the unsupervised learning objective and structure recovery. (however see shi et al. (2020) for a critical"
908,"limitations of our work in the previous section. we use two existing datasets, sst (socher et al., 2013) and imdb (maas et al., 2011), which are publicly available and commonly used in nlp research. we synthetically generate datasets of formal languages which does not require ethical consideration. we have discussed the experimental details and computational budget in detail in appendix g. the research presented in this paper focuses on analysing the inductive biases of transformers and lstms based on experiments on formal languages and subsequently we believe that our work does not raise any ethical concerns."
909,"limitations when using our method, we have to fine-tune the upstream nmt model to construct the downstream nmt model and then datastore for the reviser training. hence, compared with the current commonlyused knn-mt variant (zheng et al., 2021a), our method requires more time for training. nevertheless, it does not introduce additional parameters during inference."
910,"limitations limitations of bartscore++ are three-fold: • in §3.1, we propose explicit/ implicit errors to better distinguish different types of errors in generated texts. however, explicit errors only contain token-level errors that can be detected and corrected by error analysis, not involving all error types mentioned in mqm (e.g. severe fluency errors). we hope future studies can take these situations into account. • in §3.2 we can see that our proposed error analysis framework fully relies on the generation probabilities of bart to decide how to refine the hypothesis. still, we see that this framework may lead to false judgments due to unfaithful content. further research can explore how to calibrate the pre-trained models during error analysis. • in §3.3 we integrate the distance of explicit and implicit errors by simply computing their weighted sum. this can be improved by considering more factors, e.g. the overall quality of the generated text, refining iterations, and external signals. we will leave the exploration of combining these factors and designing better weighting schemes as future work."
911,"limitations in this section, we will point out the limitations of our work, which can be summarized in the following two aspects. firstly, in the step of answer mapping (section 3.1), we only select those connectives that are tokenized with a single token as answer words, since most masked plms predict only a single word. therefore, those connectives tokenized with multiple tokens will be replaced by the most frequent answer word with the same subtype-level sense tags. we believe that this approach will filter out several meaningful connectives as answer words. in the future, we will utilize the generative model to predict the connectives between argument pairs, which can decode multiple tokens at a single mask position. secondly, in section 5.1, we can observe that multi-prompt ensembling is effective for fusing multiple single-prompts for implicit discourse relation recognition. in the future, we will explore multi-teacher knowledge distillation method for the idrr task, here teacher models are trained with different templates. in this way, we can take advantage of the different prompt templates."
912,"limitations choice of languages our choice of languages for wikiann and ud probing evaluations were intended to strike a balance between being being typologically diverse and having data in our chosen benchmarks. however, there are major language families and geographical regions not represented in our languages (there is no indigenous language of the americas in any of our benchmarks, and no southern african language in ud or wikiann). while we expect the trends in our results to continue to hold for other languages, we believe that further investigation is necessary on more languages to confirm our hypothesis. choice of evaluation tasks one notable omission from our evaluation suite are sentence-level tasks, such xnli (conneau et al., 2018), xglue (liang et al., 2020) and crosslingual retrieval tasks. one reason is that previous work has shown that character-level models already perform well on these evaluations. in our work, we were particularly interested in situations where prior work showed character-level models underperforming subword-based models. in particular, canine underperformed at ner, especially in the high-resource conll 2003 ner dataset (tjong kim sang and de meulder, 2003). therefore, we chose to focus specifically on ner and extractive qa as typical use cases of encoder-only models. in future work, we will investigate more thoroughly the capabilities of character-level models on a wider range of tasks."
913,"limitations despite promising, the current work still has limitations. first, the current model mainly focuses on understanding problems. the generation ability of our model has not yet been investigated. it is unclear whether our weakly supervised framework also fits generative models and transfers strong generation capability across languages. secondly, the current work explores multilingual corpora and overlooks the domain gaps in existing image resources. as argued in (liu et al., 2021), the visual appearances of objects are diverse across cultures. bias naturally exists in the distribution of images in existing v-l corpora. to develop a truly generalized multilingual multimodal model, the gap between visual distributions in different cultures should be considered."
914,"limitations our framework relies on the availability of translation system and unlabeled data in the target language, which can not be applied to languages without any unlabeled text or translation text. the knowledge distillation step requires a certain amount of unlabeled text, while it may struggle in cases where only few hundreds of unlabeled sentences are available. it would be interesting to combine our label denoising framework with data augmentation techniques in such scenarios. besides, the boarder application to other low-resource languages, such as masakhaner 2.0 (adelani et al., 2022), and other cross-lingual sequence labeling tasks are left for exploration in future work."
915,"limitation, our work focuses on dialect robustness and only briefly evaluates dialect awareness. future works may extend the details and criteria of the dialect-aware nlg evaluation, and we hope our work can serve as a baseline in this new research direction. our encouraging preliminary results lead us to urge researchers to consider and improve the dialect diversity during pretraining. limitations besides the limited size of the evaluation corpora and a brevity of the exploration of dialect awareness that we point out as limitations in §8, we again acknowledge the data acquisition strategy as another limitation of our work. our data acquisition of dialects requires country codes, which exclude many dialects. there is some work on getting dialectal data without country codes: blodgett et al. (2016) build a dataset of tweets that are likely to include a high density of african-american english by linking geolocated twitter data with demographic data from the u.s. census. however, this approach is limited to dialects that have strong geographic associations within the united states and which correlate with census demographics like race. similarly, abdul-mageed et al. (2018) build a dataset of city-level arabic dialects, again relying on twitter geolocation. an alternative approach that does not rely on geolocation is to translate existing corpora into multiple dialects (e.g., faisal et al., 2021; ziems et al., 2022). however, this is labor intensive and therefore difficult to scale up to the amount of data needed for pretraining. we leave to future work the question of how to build largescale corpora for dialects that do not align with easily-identifiable geographical indicators such as national boundaries."
916,"limitations while our proposed method demonstrates high translation quality and constraint accuracy, it is important to acknowledge that the hard copy mechanism may not be suitable for certain morphologically complex languages, such as arabic. in arabic, phrases or terminologies often involve conjunctions or prepositions and exhibit varying morphological forms. unfortunately, our proposed method is not capable of effectively handling such cases, and addressing this challenge remains an open area for future research."
917,"limitations the prefixes we use are semantically independent from the test sentences, and also semantically implausible when chained together. this is the opposite of what we typically expect in natural language, where sentences follow from some pragmatically licit prior context. while our findings are theoretically relevant to any nlp task that leverages natural language inputs, we may see qualitatively different trends in more naturalistic settings. our results are currently limited to english. certain languages have grammatical features (such as case marking) that could strongly impact on language models’ acceptability judgments, and this could affect the trends we have observed. future work should investigate similar phenomena across languages to ensure that these findings suitably general."
918,"limitations our suggested approaches have two primary practical limitations: first, weighted sampling is restricted to languages with available running text sources for extracting frequencies. a project on extremely low-resource languages (e.g., liu et al., 2022) may be restricted to uniform and overlapaware sampling. second, as the number of seeds increases, so do requirements for training time and/or computing power. a shared task, for example, might limit itself to only a few seeds in order to assure on-time submissions. future work would benefit from a wider selection of model architectures, along with more sampling strategies, and of course a wider sample of typologically diverse languages. notably, this work reproduces the effect observed in the sigmorphon 2022 shared task (kodner et al., 2022), which found a substantial performance hit for featsnovel relative to featsattested, but not lemmanovel relative to lemmaattested. however, both this work and the shared task fail to replicate the effect observed in goldman et al. (2022), which reports a 95% performance hit on lemmanovel vs. lemmaattested. this may have something to do with differences in splitting algorithms, unmeasured feature overlap in goldman et al. (2022), or choice of model architectures."
919,"limitations in this work, we adopt a series of strategies for optimizing the generation models when corpus scaling up. although we successfully train tome on largescale corpora, there is still a performance gap compared to mainstream dense retrieval methods under this scenario. this is also one of the limitations of current model-based retrieval methods, because this retrieval paradigm requires the model to memorize the entire corpus, unlike dense retrievers that have strong generalization capability for different documents in a large corpus. in addition, effective training on large-scale corpus also requires largescale computing resources (up to 32 tesla a100 80g gpu) and long training time, which will indirectly generate risks of energy consumption and emissions."
920,"limitations a qualitative analysis of distractors generated via mt shows that this method can produce some inadequate candidates (and so do word2vec and bert-based methods). thus, a human-in-the-loop is needed to ensure the validity of the generated distractors. however, human-in-the-loop is standard practice, when producing language exercises and tests (attali et al., 2022). we therefore believe that the proposed approach does not need to be fully automatic to be useful, as it can still help speed up distractor generation to create advanced vocabulary exercises. the mt method can thus be of huge help to human test developers. the mt approach can be computationally more expensive than the methods proposed in prior work such as bert and word2vec. although we make use of pre-trained mt systems, the approach can be still costly, as it requires running two mt systems (forward and backward) with each pivot, and a bert-based word alignment model to align the carrier sentence with each of its 900 back-translations. in terms of cost comparison, it takes 1-2 hours in a single nvidia tesla a100 gpu to generate 900 translations and produce candidate distractors for a single pivot, versus 0.5 hour with bert and word2vec. however, the mt approach can potentially offer advantages that other methods cannot, such as producing a more diverse pool of distractors and, importantly, relating the native language of the learner to the pivot systems used to produce distractors. as our analyses show, each pivot system generates unique distractors. we stress that, while we show that using multiple pivots generates diverse distractors, we leave the question of whether using a pivot based on learner’s first language is useful, to future work. we do hypothesize, however, that using pivots tied to the first language might be useful, however, but verifying this claim is left for future work. this is because verifying whether tying the pivot to learner’s native language would be useful would require a human study with a relatively large group of learners of at least 20- 30 students (all of advanced level) that all share the same first language. in fact, we would need to have several groups of learners, such that students in each group have the same first language background. this would be a large-scale study that is out of the scope of the paper. note that the current work already presents a human study with 32 students that demonstrates that the automatically generated pivots are of the same difficulty as those created manually. we also note that the method requires relatively good mt systems for generating more difficult distractors. finally, our study is limited to cloze items that include single words as targets and does not consider fixed expressions, such as phrasal verbs and idioms. in the language testing community, such expressions are typically tested separately from the generic cloze items. the basic approach is to detect them before the carrier sentence is cleared to be used for cloze exercises. our current work is not focused on carrier sentence selection. but it makes sense to include this consideration in a larger suite of tools for cloze item generation."
921,"limitations in our study, we address the issue of linguistic forgetting via the injection of the strict non-linguistic skill of quantitative reasoning. although quantitative reasoning with llms is an active research area, as discussed above, further fine-grained studies are required to extrapolate this behavior to tasks that leverage synergies between aspects of both linguistics and non-linguistics - such as math word problems or data-to-text generation. further, investigations into the linguistic forgetting tendencies of different languages would lend an insight into the role of linguistic morphology in this behavior. the restrictions from our in-house gpu resources does not allow scaling this study to more recent models that exceed 100 billion parameters, although, due to the sharing of similar architectures, we forecast our findings to hold despite of model scaling."
922,"limitations our study leaves room for future work. first, we would like to highlight the difficulty of applying the validity assessment framework from measurement theory to instability measures. for example, in §5.1, our low convergent validity scores may have different interpretations because there are no well-established instability measures. further, in §5.2, because no previous studies have built theoretical foundations of factors that impact the prediction and representation instability, both our tests do not rigorously follow the concurrent validity definition: our first test of successful and failed runs is based on an assumption derived from observations of mosbach et al. (2021) rather than theory, and our second test of differences among test datasets examines the consistency between theoretically indistinguishable groups instead of the differences between theoretically distinguishable groups. second, we only experimented with a limited number of tasks, instability measures, plms, and validity types. future work can use our framework to further validate the generalizability of our observations. for example, to apply our validity testing framework to larger datasets, to include other measures (e.g. functional similarity measures, csiszárik et al., 2021 and jitter, liu et al., 2022), to study generative plms (e.g. t5, raffel et al., 2020 and opt, zhang et al., 2022), and to test other types and validity (e.g. discriminative and predictive validity). third, we focused on general text classification tasks in this paper. one promising direction is to investigate which measures to use for specific settings. for example, to extend our framework to more recent generative models (e.g. bart, lewis et al., 2020 and gpt-3, (brown et al., 2020)). however, in this case, because our prediction measures in §3 are only useful for classification, new prediction measures should be developed, and our tests should be adjusted accordingly."
923,"limitations of mitigation methods. in this section, we suggest possible analyses, along with illustrative case studies."
924,"limitations of existing datasets. in turn, fairprism provides a richer lens for diagnosing (1) the types of fairness-related harms that ai text generation systems cause, and (2) the potential limitations of mitigation methods. the process we followed to develop fairprism offers a recipe for building improved datasets for measuring and mitigating harms caused by ai systems. in addition, since we limited the scope of fairprism to stereotyping and demeaning harms relating to gender and sexuality, future work could create similar datasets for other demographic groups, such as those based on race, ethnicity, religion, age, national origin, or disability status. limitations fairprism is limited to fairness-related harms relating to gender and sexuality. it contains only english text, primarily represents varieties of english used in the u.s., and the annotators who labeled the examples were from the u.s. and canada. as a result, it is less well suited to measuring or mitigating harms relating to other demographic groups, harms specific to other countries, and harms in other languages. in addition, the social bias frames dataset, from which we obtained some of the human inputs, consists of text from social media sites, so it may not reflect typical interactions with ai text generation systems. some of the constructs we attempted to operationalize have competing definitions, which may affect the range of harms covered by fairprism. for example, our definitions of stereotyping and demeaning harms may have caused annotators to label some stereotypes, demeaning content, or forms of phrasing as harmful more easily than others. annotators may also have used implicit criteria when labeling examples (e.g., equating explicit language or particular language varieties with harmful text, despite our instructions to the contrary). in addition, our focus on stereotyping and demeaning harms excludes other types of harms. for example, allocation and quality-of-service harms are not covered by fairprism, nor are harms that stem from the use of ai text generation systems more broadly, such as questions of power and agency that relate to who is able to design or use these systems. unintended uses as a result of fairprism’s limitations, we do not intend it to be used for any of the purposes outlined below. access to fairprism is restricted as a preventative measure. to request access, please send an email to fairprism@microsoft.com detailing your desired use case for us to review. as training data for generating hate speech. illintentioned actors could train models on fairprism for the purpose of generating hate speech. as training data for mitigation methods. directly using fairprism to train classifiers for mitigating fairness-related harms prevents it from being useful as a measurement instrument. furthermore, fairprism is not sufficiently large or comprehensive to be effective for training mitigation methods. as a benchmark to be “beaten.” if ai systems are repeatedly trained to improve on any single aggregate metric calculated using fairprism, this will result in overfitting to the dataset, which will make the dataset less useful for measurement and may lead to a greater proliferation of harms that it does not cover due to a false sense of complete coverage. application mismatches. fairprism contains examples of text generated in both reply scenarios (e.g., autoreplies or chatbots) and continuation scenarios (e.g., writing emails or generating stories from a prompt). its efficacy will therefore lessen for applications that are further removed from these scenarios (e.g., it is not intended for measuring harms in human-authored text) and for applications that are highly specific (e.g., medical chatbots). fairprism is also less well suited to measuring or mitigating harms relating to demographic groups other than those based on gender and sexuality, harms specific to countries other than the u.s. and canada, and harms in languages other than english."
925,"limitations while our approach shows promising results in both automatic and human evaluation, it relies on two significant pillars: a strong entailment model and a strong initial summarization model. the nli model implicitly encodes the biases and other data regularities that were part of the nli training set into the generated summaries of our policy. this is well demonstrated by the gap between human attribution judgements and the automatic nli metric. our rl policies cannot improve on factual consistency errors if they are undetectable by the nli reward. hopefully, as nli capabilities get better, so will the efficacy of rlef and the abilities to automatically flag hallucinations and contradictions. secondly, a strong summarization model is essential for our method in two ways: as an initialized starting point for rl exploration and as an anchor point to a policy. while our rl training does not require any reference data and opens the possibility to use more un-summarized documents, it would probably not succeed as well without initializing from a high-quality supervised model. another limitation is that our experiments suggest that model size is important when using rlef (figure 4): both our summarization and nli models are 11b parameters models. we believe it is important to further understand how to make our approach more robust to smaller models, to increase its computational efficiency and availability."
926,"limitations since this work relies on the in-context learning ability of large language models, the challenges associated with computational resources to load an llm ensue. due to resource constraints, we could not use larger or commercially available llms to validate if the advantages of x-insta translate to those models as well. as we observed in section 3.5, the static nature of the aligners poses a limitation on x-insta. moreover, these aligners are manually designed. therefore, task-specific, trial-and-error style manual intervention is needed. we believe a better understanding of the pretraining distribution of the multilingual llms can pave the way toward better automated alignment methods. there are multiple shortcomings of monolingual icl that entail its cross-lingual counterpart and x-insta does not address them; issues like knowledge hallucination, limited common-sense reasoning, inconsistency in retrieving factual associations, etc."
927,"limitations although our proposal enjoys the advantages of validity and generality, there are still two major limitations. first, vlp cannot directly generalize to the inductive setting, since vlp is defined based on the score functions of transductive embedding models. one potential direction is to design an inductive reference selector for emerging entities. second, how to efficiently select more helpful references for prediction is still an open challenge. we expect future studies to mitigate these issues."
928,"limitations in this work, we have identified two key limitations of coad that can be further examined in future research. the first limitation is that coad only allows for the querying of one symptom at a time, making it unsuitable for scenarios where multiple symptoms are present. however, coad has superior performance in the main metrics for automatic diagnosis. to relieve this limitation, potential solutions include relaxing symptom feedback conditions and allowing the model to produce symptoms sequentially until a stop signal is encountered or querying the top k symptoms in a single turn. additionally, coad has some restrictions on input format, requiring standardized symptoms and values. to make it more applicable to end-to-end settings, an natural language understanding module (nlu) is required to parse plain text and obtain the input symptom sequence, and a natural language generation (nlg) module is needed to translate the predicted symptom or disease to text. the ultimate goal of automatic diagnosis is to support the dialogue between doctors and patients, after coad determines the symptom or disease, rulebased nlu and nlg modules can help to achieve the text to text communication."
929,"limitations we identify the major limitation of this work is its input modality. specifically, our model only considers textual inputs, ignoring question answering tasks in vision and audio. a multi-modal question answering model under realistic open longtailed scenario is worth further exploration. fortunately, through multi-modal pre-training models (xu et al., 2021; huo et al., 2021) and question answering methods (kim et al., 2020), we can equip our model with multi-modal question answering ability. for future work, learning multi-modal question answering in an open (including out of distribution data (lang et al., 2022, 2023a,b)) longtailed scenario still remains a challenge, and we will continue to work on it."
930,"limitations while our work displays many strengths, we highlight some limitations. first, we focus on python for programming language evaluation, which is one of the most widely used programming languages. however, we believe that our proposed approach, contraclm, would benefit code lms trained on any programming language. second, the empirical findings presented in this work are mainly based on the smaller versions of gpt-2 and codegen with 124m and 350m parameters, respectively. however, as shown in figure 1b, by continuing to train the pretrained models with our proposed objective, contraclm is able to address not only the isotropy and poor discrimination issue that both gpt2-small and codegen suffer from, but also improve the representation quality of gpt2-large which has a good starting point for both isotropy and discrimination. therefore, we believe the effectiveness of contraclm should be applicable to larger versions of these lms, regardless of whether they suffer from the anisotropy issue (e.g., large codegen models) or not (large scale gpt-2 models). we leave the explorations of larger models as future work."
931,"limitations unfortunately, we cannot access most sighan2008 bakeoff datasets, which were proprietary but used by many previous works. this makes the comparison in table 2 a little unfair. we argue that we replaced these non-accessible datasets with the ones publicly accessible (including ud, wtb, and zx). we note that huang et al. (2020b) faced the same limitation as us. thus they also replaced datasets just as we did, which makes them the only directly comparable work to ours."
932,"limitations although our model obtains satisfying results, it also exposes some limitations. first, for a fair comparison to other models, we mainly carry out relevant experiments on pdtb 2.0. due to the lack of baselines on pdtb 3.0, further analysis and comparison cannot be conducted. second, in our experiments, we can find out that the hlr method does not improve the top-level or bottom-level results effectively, indicating that with the increase of the level, the refining method is insufficient to continue to generalize the bottom-level labels and further improvement should be made according to the specific features of the idrr task. third, due to the limitation of space, this paper does not focus much on semantic weight for the refining of sub-labels. this is a very broad topic involving the rationality of the discourse relation annotation and the interpretability of the label embeddings. we will conduct a further study which may appear in our next work."
933,"limitations we explain model pathology from a classification perspective, but the pathological nature may exist in language models for performing various tasks, such as reading comprehension, textual entailment, and visual question answering. although our proposed regularization technique may be applicable to various tasks, we have only investigated its effectiveness in classification problems. further evaluations are expected to be conducted in future works. the proposed method also leads to more time-consuming training, primarily due to the generation of adversarial examples, while only a minimal amount of time is spent on generating out-of-distribution examples."
934,"limitations our analysis is primarily limited by the accuracy of underlying nlp models used in our character event extraction pipeline. for example, booknlp does not cluster nominal mentions of characters (""the girl"") with the corresponding character names (""cinderella""). this results in character event chains that do not account for all of the character’s actual events. using allennlp to extract all action verbs in a sentence as the event triggers meant that not all of our events were on the same dimension: some events were intended or thought of, while others actually happened. additionally, narrative events that are described in ways beyond just action verbs are not extracted. (for example, the event of a kidnapping might be described as two separate actions: a character picking up another character and running away.) our salient event identification algorithm might also filter out many events of analytic interest. both characters whose gender are not specified in the story or who are gender-less are classified as “unknown”. there is no explicit way to extract non-binary characters as models tend to label uses of the pronoun ""them"" as plural. thus, the current implementation is lim- ited to comparisons of female and male characters which perpetuates a gender binary. our use of bootstrapping to calculate confidence intervals and determine statistical significance is valid under the assumption that the original fairtytaleqa sample is representative of all fairy tales. as the sample was collected only from popular open-source stories, this assumption may not hold. lastly, bias exists beyond just gender groups and gender itself intersects with other social groups. we plan on expanding this component to include attributes such as race and ethnicity, age, and socioeconomic class. the cultural comparisons and overall analyses were too limited as the fairytaleqa dataset is very eurocentric with most fairy-tales coming from northern and western europe (table 4 in a.3. only some stories income from east asian, southern european, or indigenous north american cultures. meanwhile, almost no fairytales are included from south america, the middle east, africa, south asia, or south east asia. unfortunately, after considering the break down of event chains by gender and culture, the samples were too small to observe robust trends."
935,"limitations although futuretod achieves significant improvements over existing baselines, there are some directions to explore for future work: (1) in this paper, futuretod doesn’t use any data augmentation strategies to enhance representations. we believe existing augmentation methods will benefit further improving performance. (2) we design a simple technique of constructing the teacher. more complicated methods should be considered, such as multi-teacher and large teacher. (3) futuretod in this paper cares about dialogue understanding tasks like intent detection, dialogue state tracking, etc. we hope to extend the similar idea to the generative dialogue pre-trained models and larger tod corpus. besides, exploiting limited dialogue labels is also valuable to explore."
936,"limitations we acknowledge a few limitations in this work. first, peacok cannot be comprehensive. persona knowledge is very broad and our resource cannot cover all dimensions of personas, nor all attributes of these dimensions. we select five dimensions of personas that we found salient from background literature in human interaction, and we distill attributes for these dimensions from atomic2020, comet and instructgpt-3. these resources, while rich in knowledge, only represent a subset of possible background resources for the construction of peacok(among other kgs and pretrained language models). furthermore, the primary language of these three resources is english, making peacok a solely english resource. finally, in downstream narrative experiments, the usage of our augmented persona knowledge is constrained by the capacity of baseline model, which leaves for future work the exploration of downstream persona knowledge augmentation on a larger scale."
937,"limitations the simulated dialogues constructed by kidg are a powerful source of training data for retrieval-free knowledge-grounded dialogue systems. however, there is a clear style difference between the generated utterance and the original document sentences: one is the oral expression and the other is a more formal style. but as shown in table 5, the pdms trained on kidial appear to be more proactive and knowledgeable during conversations. the generated utterances serve as a type of prompt to help the model understand the knowledge. in the meanwhile, our kidg embeds the knowledge into different contexts, alleviating the one-to-many problem in some degree. although generating dialogues needs to cost gpu resources, it is still a cheaper and quicker way to acquire large-scale knowledge-intensive dialogues."
938,"limitations in this paper, we propose a novel concept of dense retrieval, the matching representation. based on this, we introduce a novel generalizable dense retrieval training method via training the balanced and extractable representation for matching (berm). despite the strong performance of our method in improving the generalization ability of dense retrieval models, more theoretical proof needs to be researched to gain the deeper understanding of generalization improvement. especially for matching representation, more theoretical analysis and implementation will be discussed in future work. we believe that the deeper study of matching representation will promote the development of dense retrieval, because it not only alleviates the problem that query and passage cannot interact in depth during training, but also describes the essence of retrieval task."
939,"limitations with gpt-3 we only perform a subset of our evaluations of structured prompting on gpt-3, due to the cost of running the models in the api; this also means we do not run comprehensive prompt ablations to better tailor the setup for these models. additionally, the results (i.e., lower performance than comparable gpt-neo models) are difficult to interpret due to the black box nature of the gpt-3 models – it may be due to pretraining data differences (as mentioned in the previous limitation), the lack of prompt engineering for the models, or some other discrepancy. english-only experiments the experiments in this paper focus on english sequence tagging tasks, and it is unclear how well the proposed method generalizes to other languages. we find evidence of task-relevant data in pretraining corpora in nonenglish languages, which suggests there is signal for the approach to work in other languages. however, prior work shows that plms behave much worse when prompted outside of english (lin et al., 2022; shi et al., 2022) but does not address the effect of pretraining data on this phenomenon."
940,"limitations in this article, we focus only on meeting minutes, speech, and press conference data. many other text datasets such as transcripts from congressional and senate testimonies, beige books, green books, etc can be incorporated to understand pre-fomc drift better. we don’t use audio or video features in constructing the measure, which might contain additional information. it can be an interesting future study to compare measures generated from fomc text with an alternate measure that can be constructed from the news or social media data. in dataset construction, while splitting sentences, we use a simple rule-based approach. we leave it as an open problem for future researchers to find better methods for splitting sentences with opposite tones. in our trading strategy construction, we do not include transaction fees as it involves low-frequency trading. in the future, one can use our model and data to construct a high-frequency trading strategy as well. in addition, a more comprehensive zeroshot and few-shot generative llm benchmark with open-source models can be performed to provide a better comparison."
941,"limitations elaborated relation descriptions are the foundation of the matching-based methods to achieve superior performance. although we have proposed some ways to enrich the entity information in the descriptions, it is still a promising direction to explore more diversified and effective ways to enrich relation description (e.g. ensemble of multiple descriptions). we leave this as our future work."
942,"limitations considering the wide spectrum of llms’ applications, not only defining social sensitivity on llm-based generation is not trivial and explicit but also completely addressing all the socially sensitive issues might not be feasible. therefore, our square mainly focuses on socially sensitive questions with three categories and their acceptable responses with six types for safer applications of llms, by in-depth"
943,"limitations our work focuses on building a two-stage framework for generating and learning from explanations. in our investigation, we are limited by the available computational resources, financial budgets, and datasets. gpt-3 and pet are performant few-shot learners that work well for our use case. however, gpt-3 is not free to use and partly for financial considerations, we did not experiment with gpt3 in-context learning initially. the performance difference between gpt-3 babbage and davinci are aligned with the emergent abilities of largescale language models (wei et al., 2022a; rae et al., 2022). therefore, in the era of research with private large-scale language models, it would be useful for the research community to collectively build knowledge about how large-scale language models work. it would be useful to experiment with other models such as google’s palm (540b) (chowdhery et al., 2022b) and deepmind’s gopher (280b) (rae et al., 2022). it is an important question for the research community to explore productive paths forward. often, prompt engineering requires either significant manual work to come up with good templates (brown et al., 2020; schick and schütze, 2020) or a big budget to run automatic prompt generation methods (lester et al., 2021; wu et al., 2022). in this work, we used a fixed prompt (see appendix c.1) for explanation generation, future work could also investigate from the angle of generating better prompts. we experimented with two natural language inference tasks, which tend to correlate with a certain form of explanations. one way to interpret the difference in our findings and chain-of-thought prompting is indeed that the reasoning in e-snli and e-hans are not the multi-step reasoning used in arithmetic reasoning. as tan (2022) argues, there are diverse types of explanations, which may lead to varying levels of effectiveness from a learning method. future work could investigate the effectiveness of our method on other tasks and different types of explanations. while our method demonstrates effectiveness against strong baselines, there is still a big gap from the upper bound performance and suggests potential for better use of the explanations in future work. for example, future work could incorporate careful example selection into learning with explanations. we picked examples randomly, but research has shown that calibration (zhao et al., 2021) reorder- ing (lu et al., 2022) and example selection (liu et al., 2021) changes gpt-3’s behavior. we also used human explanations to fine-tune the gpt-3 model for explanation generation, but human explanations may not always be high-quality or the best guide for machine learning models. additionally, we use roberta as our backbone model for the classifier used in both the non-gpt baselines and our flame framework. we manage to beat stronggpt-3 baselines that use explanations. while more powerful classifiers (e.g., deberta) could also be used in place of roberta, we believe we have demonstrated the effectiveness of our method by using a simpler classifier. we leave it to future work to investigate the effectiveness of our method with more powerful classifiers. finally, it is worth noting that we use a particular setup of k = 16 for our experiments. while we believe that this is a reasonable few-shot learning setup, results could differ for different k. we leave it to future work for examining the impact of examples, explanations, and number of samples. broader impacts we propose a framework to generate and learn from explanations and conduct in-depth analysis to understand the utility of explanations. our work has the potential to help people understand the behavior or usage of large-scale language models and improve their trustworthiness."
944,"limitations in this part, we show limitations of our work by categorizing wrong predictions outputed by our method clever into two groups. the first type of error is induced by the unconspicuous biased features of claims. for example, the claim scandinavia includes the remote norwegian islands of svalbard and jan mayen. does not contain obvious biases so that the output of claimonly model cannot represent the biased distribution. therefore, subtracting such output fails to mitigate biases but reduces the beneficial claim information instead. these errors may be avoided by employing different strategies for instances with distinct bias extents, which we leave as future work. the second type of error occurs when high-level reasoning is required, e.g., mathematical computation and multi-hop reasoning, which drops into the scope of model reasoning ability. this work mainly focuses on debiasing fact-checking models that make them concentrate on the intrinsic evidential information. after debiasing, how to enhance the reasoning ability over such information is a promising future direction."
945,"limitations we discuss the limitations of our framework as follows: (1) in this paper, we take an initial step on the robustness of the summarization system by focusing on word-level perturbations in the input document. however, in practice, the robustness of the summarization models is reflected in many other aspects. for example, the summarization performance towards sentence-level or document-level perturbations is also a kind of robustness. (2) although dasum greatly improves the generation quality compared with other augmentationbased models, it requires more computational resources with respect to the augmented dataset construction process. for large-scale datasets with long text (e.g., bigpatent (sharma et al., 2019)), it is worth considering the time complexity of transformer architecture."
946,"limitations the primary limitation of the proposed model is computational efficiency. specifically, during the training phase, the input size of the model is more than double that of traditional models, which is due to the inclusion of both predicted and gold templates. besides, the source sentences are transformed into longer sequences, resulting in an increased memory footprint and longer training time. additionally, both during the training and testing phase, an additional step of preparing detection labels for the data further contributes to the increased processing time. in future research, we aim to investigate methods for achieving comparable or superior performance while reducing the input size and addressing these limitations, building upon the foundation of our current work. additionally, templategec does not support the joint training of the seq2edit model. we will further explore how to jointly train the seq2edit model in future work, particularly focusing on the continuous modeling of detection labels based on an end-to-end model."
947,"limitations that are revealed in our work, such as multiple training procedures or hyperparameter tuning for each method (e.g., how much we allow accuracy drop during layer pruning). limitations although our method well estimates the ambiguity without additional resources as well as boosting model latency significantly, there are a few limitations. first, our method requires additional training procedures, such as training the internal classifiers and kd. for this, we may fine-tune the original model and internal classifiers simultaneously. another limitation is in setting the hyperparameters. we allow the drop of accuracy by 1% to determine the target layer for layer pruning and the value of λ for kd, but this could be subjective and differ depending on the researchers’ experience. finally, we validated our method with a limited number of benchmarks since most of datasets have been released with only aggregated gold labels (uma et al., 2021)."
948,"limitations the main limitation of this work is the quantity of annotated human reflections. overall, 15 human reflections are annotated, which are outnumbered more than 7:1 by gpt-2 reflections and 9:1 by gpt-3 reflections. if there were more human reflections annotated, we may be able to confirm, among other potential findings, that gpt-3 reflections were indeed significantly more often annotated as coherent compared to human reflections. we also note that the laypeople had a longer between-stage waiting period than the experts, because we could not enforce a similarly long waiting period for the experts due to practical reasons (appendix c). while an ideal setup would keep the same waiting period duration, appendices c and d show that the duration difference is not critical. furthermore, we adopted sequential annotation for reflections within a batch to make the interface easier to navigate for the human annotators, but this also means that the early samples in a batch might indirectly affect the annotation of the later samples. we leave more investigation on this to future work."
949,"limitations similar to many knowledge graph embedding models, our proposed method is yet to handle link prediction under inductive settings. one possible future extension is to leverage entity description information to generate textual features and use compounde as a decoder to handle unseen entities. also, the affine operators we use are limited to translation, rotation, and scaling and this may limit the number of different relation patterns we can handle. in the future, we can include all affine transformations and investigate their difference. also, because we use 2d givens rotation matrix, the embedding dimension setting needs to be a factor of 2. we can explore higher dimensional transformations such as 3d transformations and compare the modeling power."
950,"limitations as in-context learning with llm heavily depends on the selected exemplars in the prompt, the performance of kb-binder might vary from different subsets of randomly sampled examples, especially in a low-shot setting. but kb-binder still shows strong performance on thousands of data points on each testing dataset with randomly sampled exemplars, which verifies the robustness of our method to a degree. in the meantime, the performance of kb-binder is restricted with the one-time generated drafts from the perspective of the imaginary frame and schema items of the preliminary logical forms, which can be further improved with interactively generation and retrieval. moreover, we have not explored whether the performance can be further improved with explanation/instruction during the stage of draft generation. we will take these limitations into account and mitigate them in future work."
951,"limitations we identify two main limitations of programfc. first, despite being complex in their surface form, the claims in the hover and feverous datasets mostly require only explicit multi-step reasoning, i.e., the decomposition can be derived from the claim’s syntactic structure or how the claim is framed. this lowers the difficulty of generating reasoning programs. however, for many real-world complex claims, the reasoning is often implicit. for example, for the claim “aristotle couldn’t have used a laptop”, the reasoning program is: answer_1 = question(“when did aristotle live?”); answer_2 = question(“when was the laptop invented?”); fact_1 = verify(“answer_1 is before answer_2.”); label = predict(fact_1) generating reasoning programs for such implicit complex claims requires a deeper understanding of the claim and also access to world and commonsense knowledge. we conducted preliminary experiments on these types of claims, but we found that our codex-based generator struggled to produce a correct reasoning program. this highlights the gap in applying our programfc to fact-check real-world claims. addressing these challenges is an important direction for future work. second, programfc incurs a higher computational cost than baseline end-to-end fact-checking models. it requires calling large language models for program generation and further calling multiple sub-task models. this results in the actual computational time that is ∼4–5× higher than for an endto-end flan-t5 model. developing more efficient methods for program generation and execution is an important direction for future work."
952,"limitations in this paper, we simply prepend the retrieved prompt to the input embeddings before encoding. a well-designed method of combining prompts with the input embeddings, such as prefix tuning (li and liang, 2021), may result in additional enhancements. finally, as observed in section 4.2, prompt-based fine-tuning does not present obvious superiority over standard fine-tuning. exploring the prompt tuning on cross-lingual natural language understanding is a challenging task that has recently gained attention (qi et al., 2022; huang et al., 2022b), and we leave it as future work."
953,"limitation calculating complexity indices for large-scale graphs can be computationally expensive and time consuming. some of the complexity indices show longer turnaround time when computed for denser areas in the graphs. in addition, as we mentioned in the paper, although we made sure our framework and implementation allows adding any number of additional indices in a modular way, there might be other effective complexity indices that are not included in this investigation. furthermore, it should be noted that the model has been exclusively tested on graphs where nodes contain textual content, which may limit its application to more general graph types. finally, the model has not been applied to other graph-based tasks such as clustering and graph-level classification."
954,"limitations inappropriate initial user questions can negatively affect ner performance. if they are not proper, the qa model returns incorrect phrases, and the phrase embedding queries generated from them will also be erroneous. the absence of a component for controlling this error cascade in our framework should be addressed in future studies. in addition, our method is dependent on the phrase encoder of densephrases. because the phrase encoder is a general-purpose model trained on wikipedia-based datasets, its capability may be limited for domain-specific entities. in fewshot ner, the phrase encoder can be sensitive to the quality of given example sentences. future studies should thoroughly analyze the effect of the phrase encoder’s performance on the resulting ner datasets and ner performance."
955,"limitation, more results can be found in appendix b. accuracy-oriented methods can still hardly reduce the inference efficiency. yet, our proposed same effectively reduce the speedup ratio by 89%, which is comparable to 93% on base-size models. impact of modification rate: in our main results, we set the allowable modification rate ϵ as 10% of the input words. we further investigate whether same can reduce the inference efficiency under lower modification rate (imperceptible attack). the experiment results across glue benchmark on deebert-base and pabee-bert-base under are summarized in table 7. even constrained with a very low modification rate, e.g., 3%, both variants of same can still significantly reduce the model’s efficiency. in addition, with increasing modification rate, same leads to higher reduction in efficiency. ablation study: to understand the inner mechanism of same, we conduct ablation studies on each component. as shown in table 8, solely using heuristic loss can already lead to significant effi- ciency drop. in addition, using loss combination, and adding layer-wise importance weights can both further increase the high computation ratio. finally, same utilizes all the sub-components, which leads to the lowest inference efficiency. semantic similarity: while we constrain the modification rate in our experiments to keep the semantic meaning consistent, the semantic similarity between benign and adversarial examples is not explicitly constrained. therefore, we further investigate the sentence semantic similarity between original and adversarial examples on sst2 dataset. specifically, we first obtain the sentence representations of adversarial and original sample with a state-of-the-art st5-large embedding model (ni et al., 2022), and then compute their pairwise cosine similarity. with deebertbase and pabee-bert-base as the victim model, the same-word has an average cosine similarity of 0.89, and same-char has an average cosine similarity 0.96. the results suggest that both variants of same can well preserve the inputs’ semantic meaning, at the same time, reduce the efficiency of dynamic transformers. visualization: to illustrate the impact of efficiency-based v.s. correctness-based adversarial perturbations, we present a case study of adversarial samples produced from sst-2 dataset in table 9. for better explainability, we show examples with one-word only modification. due to space limitations, more adversarial samples generated using same can be found in appendix c. as shown in table 9, our efficiency-based method will perturb the word but to bujt, thereby altering the explicit turning relationship between two sentences. while humans can make the correct prediction even without the word but, it can be challenging for dynamic transformers to infer the turning relationship in the early stage. therefore, they fail to satisfy the exiting conditions, resulting in reduced inference efficiency. in contrast, correctnessbased approaches will keep the transition word and adversarially modify the word deeper, e.g., to deper with textbugger. with the transition word but, the model will emphasize more on the latter sentence, and easily get a high model confidence."
956,"limitations firstly, our proposed same is for the white-box attacking scenario only, which is less practical in real-world scenarios. however, experimental results on black-box transferability show that a blackbox efficiency-oriented attack is highly feasible. therefore, we leave the black box same as a future study. secondly, we mainly study multi-exit transformers for sentence classification tasks in this work. we notice that several recent works extend the idea of multi-exiting to other nlp tasks, e.g., sequence labelling (li et al., 2021), text generation (schuster et al., 2022). for classification tasks, same slowdowns the models by avoiding early exiting. while for text generation tasks, in addition to avoiding early exiting, ones can also slow down the model by forcing the model to produce a longer sequence. we leave the exploration of other dynamic models to future work. thirdly, as the first work that evaluates the efficiency robustness of dynamic transformers. we use a relatively simple permutation strategy. although these permutations can lead to severe performance degradation, they might not be imperceptible enough. yet, they could be easily replaced by other sophisticated permutations under same framework."
957,"limitations the creation of templates still requires native speaker expertise and an understanding of a language’s grammar. morphological inflection models are imperfect so morphological forms may need to be enumerated to ensure highquality tests. we leave model-in-the-loop template creation and improving morphological inflection models for future work. while we design representative templates with thousands of permutations for each capability, a larger set of templates and arguments may be necessary to ensure a comprehensive coverage."
958,"limitations wtp performs comparatively worse in some lowresource languages (e.g. welsh, nepalese, punjabi, pushto). this may be attributed to quality issues of mc4 in these languages (kreutzer et al., 2022). in addition, we find that the adapted wtppunct classifiers generally do not transfer well across languages and dataset collections (appendix d). finally, although bias is less obvious in segmentation tasks than e.g. generation, wtp may be biased by performing disproportionately well on text by communities which are overrepresented in the training data, while performing worse on text from underrepresented communities. we try to minimize this form of bias by sampling text from all languages uniformly."
959,"limitations due to the limited number of available code-related downstream tasks, we did not evaluate our attacks against other code-related tasks. there are several limitations to our designed attack. while the attack can be applied to any downstream seq2seq task for the generation task, compared to those attacks designed for a specific scenario or task (schuster et al., 2021), our backdoor threats are less harmful and can be manually checked to detect and remove bugs or faulty logic introduced by these attacks. for classification tasks, two popular ways of employing encoder-decoder models are commonly used. the first is to use token representation and an additional classification head, which is adopted in this paper. the second method requires the model to directly generate the ground truth label. if the victim users adopt this paradigm, the implanted backdoor will not be activated because the model doesn’t use the ’eos’ token representation for classification. ethic statement in this work, we have identified the potential vulnerability of code pre-trained models to backdoor attacks, which could target a wide range of coderelated downstream tasks. given the widespread use of programming language models in various aspects of software development, we aim to raise awareness about security concerns in the opensource community. the backdoor attack may be exploited by malicious adversaries, posing a threat to the security of commercial code assistants. for example, attackers may implant backdoors in programming assistance models (e.g., copilot), leading to code with vulnerabilities. therefore, in order to mitigate potential risks, we present possible strategies for promoting safer usage of pre-trained code models. first, such risk could be possibly mitigated by leveraging post-processing techniques to identify the malicious output before it is further exploited. detailed"
960,"limitations one limitation of this work is that both ptw masking and mrd are conducted only on bert due to limited resources, and mlms with other structures may have different reactions to the timevariant masking with different contents and ratios. another limitation is that although we propose mrd for the first time, the strategy of time-variant masking ratio is hard to design like learning rate decay. in fact, other decay methods and choices of starting and ending point are various, where better strategies may exist and further work can be done."
961,"limitations potential limitations to our work can mainly be attributed to two factors: 1) the fact that we run our experiments using the icelandic language, and 2) inherent biases in the corpora we use. icelandic is north germanic language (along with faroese, norwegian, danish and swedish). as such, it is both germanic and indo-european. while we are fairly confident that our results hold for these languages, different results may hold for other languages, particularly those not using latin script or those using logograms, such as chinese characters. the curated datasets we use only represent a fairly small proportion of all possible demographics and users of the icelandic language. in particular, annotations are performed by a handful of university students, bringing in their biases to the annotated data. even so, the data should serve well to compare the relative differences. the resources we use to develop the models consist of a few high-performing gpus. while these are powerful, this is a relatively low requirement compared to many industry or academic use cases. finally, it is worth re-iterating that the byt5 model we use is slow compared to subword-based models for texts of similar length. inference in our setting was around 2.3x slower on average than for mt5. as such, production use of these methods may be better suited to offline processing, particularly for longer documents."
962,"limitations. we evaluate the proposed method on multiple datasets and results show that the proposed method yields new state-of-the-art performance. we analyze why our approach attains superior performance by conducting ablation studies and sentence representation visualization. we further apply our model as an aigc detector to distinguish chatgpt-generated texts from those generated by human experts and the experimental results demonstrate that our model outperforms human evaluators in the setting of paired answers. limitations table 6 shows the results of our model and other methods on the agnews benchmark. interestingly, we notice that our approach reports a slightly inferior performance when compared with mdf+imlm (xu et al., 2021). we can see that methods using sentence representations based on token aggregation, e.g., fasttext9 or glove (pennington et al., 2014)-based isoforest, ocsvm, and cvdd (ruff et al., 2019), as well as bert based mdf + imlm (xu et al., 2021), perform especially well on agnews compared to their performance on other datasets. we conjecture that this is because agnews has a much larger variation of sequence length (36.6) than other datasets (around 7 or 8). a larger length variation will lead to more acute fluctuations in perplexities, especially when adopting an autoregressive language model with unidirectional context such as gpt-2-small in this paper, making it more difficult to distinguish between id and ood examples than in other datasets. in contrast, sentence representation based methods benefit from directly estimating the ood score using the information from the whole sentence, thus producing superior performance. fortunately, the limitation of auto-regressive modeling could be eliminated by leveraging transcormer (song et al., 2022) as the base model of our approach, where bidirectional context is used for estimating tokens at each position. we leave this for future work. 9https://github.com/facebookresearch/fasttext"
963,"limitations we attempted to develop a novel framework for explainable complaint identification in a multitask setting. but the proposed approach is having some limitations as enumerated below: (1) the proposed methodology has been validated on an english language complaint dataset; further training would be required to scale up to codemixed language datasets which are prevalent in multilingual countries. (2) users often post some images along with text while writing complaints. the current system is unable to handle such multi-modal forms of inputs. (3) in some cases, users use an implicit sarcastic tone while writing complaints. in the current setup, sarcasm detection is not considered as a separate task. thus the proposed system will not be capable of detecting complaints with implicit sarcasm."
964,"limitations besides its merits, this work still has limitations that could be further explored. on the one hand, we collect source data of mmdialog with only english language. thus the applicability to other languages would be restricted. on the other hand, we get rid of gifs and video-modality elements in mmdialog. in the future, we hope to extend mmdialog to multilingual scenarios and include more modalities such as audio and video."
965,"limitations since there are two transformer-based models in hdld, the major limitation of hdld is the training cost. it costs twice training time than most baselines. here is the training time comparison on opensubtitles: model minutes per epoch adalabel 7.13 d2gpo 7.16 regdg 7.83 ours 14.32 to address this limitation, a potential improvement direction is to fuse the inter-duality approach into one transformer, e.g., sharing parameters between the fxy and fyx. however, we find the significant performance drop when doing so. in the future, the trade-off between performance and cost needs more attention. besides, like most generative models, if there is malicious information in data, there is no guarantee to avoid bad responses to users, which is a potential risk that needs to pay attention."
966,"limitations although our dualgats simultaneously consider the complementarity of discourse structure and speaker-aware context for more accurate erc, it requires more computation and a longer training time. the performance of discourse parsing could be more satisfying in the current stage. moreover, we directly utilize pre-trained deep sequential models to parse dialogues in erc datasets, which does not address the domain gap problem well."
967,"limitations there are two limitations in this paper: (1) compared with existing memory-based methods, the proposed prototype memory may bring additional storage space overhead. but since we only require very little additional memory (only one vector per class), we did not discuss it; (2) although we noted that the distortion and forgetting of the prototype are highly correlated, we did not conduct a detailed analysis of the reasons for special prototypes that do not follow this pattern."
968,"limitations we have considered a variety of different llms in order to study attribution. however we have only considered a small sample of the different llm architectures and training strategies. this has been with a view to using a small but diverse set of llms. of these 10 base models, we tested our approach to attribution on a controlled set of fine-tuned models. while a study that considers a wider variety and larger scale of fine-tuned models would be beneficial to the problem of attribution, the computation resources limited our study. furthermore, in our assumptions in this work we consider that there is a one-to-one mapping between mf and mb. however, this is not necessarily the case. there could be an m-to-n mapping and also a model may be present in one set, but not the other. we believe there is rich space for further research in this area that can address these limitations, and further develop the problem of attribution."
969,"limitations in this paper, we thoroughly investigate the existing large language models for nl2code, and summarize them from diverse perspectives with our own thinking. however, as this field is evolving so rapidly, there may be aspects that we have overlooked, or some new works that we have not covered. to mitigate this issue, we have created a website to track the latest progress through crowdsourcing, hoping that it will continually contribute to the development of the field. besides, the existing llms possess their own characteristics in terms of model size, architecture, corpus, pre-processing, tokenizer, hyper-parameters, and training platforms. also, some of them are currently not publicly available, such as alphacode (li et al., 2022b) and palm-coder (chowdhery et al., 2022). therefore, it is almost impractical to conduct a completely fair comparison. we tried our best to show a kind of comparison on the popular humaneval and mbpp benchmarks, hoping that it can provide clues to the differences in performance of different llms. in addition, evaluating llms has a high cost in computational resources. we thus have made all files generated by the llms publicly available on https:/"
970,"limitations firstly, the transferability between different tasks within an mtl system is not well measured in current work. we also find such transferability is asymmetric and thus hard to quantify using symmetric measurements such as cosine similarity between task embeddings or gradients: for example, tsa positively transfers to sc, but sc negatively transfers to tsa (see the “only sentiment” row in table 3); fsrl positively transfer to all other tasks, but other tasks negatively affect fsrl. future work may consider exploring better indicators that address the asymmetry of task transferability (e.g., similar to inter-task affinity scores (fifty et al., 2021) in the cv domain). secondly, some of the"
971,"limitations the limitations of our work can be viewed from two perspectives. firstly, we have not thoroughly investigated seq2seq architectures for explainable gec. secondly, the current input of the explainable system is the gold correction during training, whereas, in practical applications, the input would be the output of a gec system. we have not yet explored methods to bridge this gap."
972,"limitations because we focus on off-the-shelf tools in this work, we are necessarily constrained by the availability of such tools in different languages and contexts. while dependency annotation tools are widely available for many languages through projects like the universal dependency project, semantic frameworks, let alone effective, accurate parsers for them are harder to find. in addition, we are constrained by the current state of the art for amr parsing and, more challengingly, alignment. amr parsing continues to improve, but alignment has only recently attracted interest again as a problem, such as in (cabot et al., 2022). additionally, this work, in evaluating six fewshot settings across six pairs of datasets and a number of seeds suffers from a combinatorial problem in terms of the necessary compute infrastructure. as discussed in the paper, our work consumed roughly a month of gpu time. combined with the size of the models, this limits the accessibility of this vein of research. more effort understanding how to narrow down the choice of datasets before studying transfer would go a long way towards alleviating this issue."
973,"limitations while we observe marked improvements in the proposed multilingual language transfer with adapters, we recognize that there are several limitations still in the experiments. the first limitation is that the entity translation remains difficult, which is especially severe in the generated responses in the e&e setting. we think that name-entity translation is itself a task to be explored in-depth for future works. on the other hand, we think that while knowledge of language is one aspect for the transfer, the structural information of the semantic representation is also another important aspect – models need to acquire the important semantic structural information on top of the language-specific syntactic information. we think that this would further improve the resulting performance."
974,"limitations we currently give separate definitions of the derivation trees of the four two-level formalisms. ideally, one could give a general definition of derivation tree in a multi-level formalism, and then derive each particular case by plugging in the definition of derivation trees of (ld-)cfgs and (ld-)pdas. we leave this generalization for future work. the d-strong equivalence results only hold for spinal tag and spinal paa. however, full tag and paa are not d-strongly equivalent to spinal tag and spinal paa, and therefore they are not d-strongly equivalent to cfg ⊲ cfg and cfg ⊲ pda, either."
975,"limitations while our proposed confede method has shown promising results in multimodal sentiment analysis, there are some limitations to consider. firstly, our method is designed for multimodal sentiment analysis that includes three modalities: vision, audio, and text. the performance of the model when one of these modalities is missing is not considered. additionally, as the number of training samples increases, our custom-designed sampling method may require more processing time. however, the similarity calculation can be pre-processed between the unimodal training stage and the multimodal training stage (as outlined in appendix c). therefore, it may not consume a significant amount of time."
976,"limitation of neupsl dsi. if the domain knowledge introduced is weak or noisy (as in the sgd setting), when the model is provided with more substantial evidence, this additional noisy supervision can, at times, hurt generalization. therefore, enabling models to perform weight learning, where the model adaptively weights the importance of symbolic rules as stronger evidence is introduced, is an interesting future direction (karamanolakis et al., 2021)."
977,"limitations in this paper, we present a novel backdoor-based watermarking method, embmarker, for protecting the copyright of eaas models. our experiments on four datasets demonstrate the effectiveness of our trigger selection algorithm. however, we have observed that the optimal trigger set is related to the statistics of the dataset used by a potential stealer. to address this issue, we plan to improve embmarker in the future by designing several candidate trigger sets, and adopting one based on the statistics of the stealer’s previously queried data. additionally, we discover that as trigger numbers in the backdoor texts increase, the difference between embeddings of benign and backdoor samples in the cos similarity to the target embedding increases linearly. the optimal result should be that the cosine similarity keeps normal unless the trigger numbers in the backdoor texts reach m. we plan to further investigate these areas in future work."
978,"limitations the limitations of this paper include the absence of experiments on large language models. previous studies have shown that using high-capacity pre-trained language models can significantly improve the accuracy of answers but also entails an increase in computational overhead. due to (academic) limitations of computational resources, this paper employs a low-capacity t5 model for experiments. our experiments have suggested that the proposed iterative prompting method that works with the low-capacity model can achieve comparable results with baseline methods equipping with large models. in future work, we would like to scale up the proposed model to improve the model’s performance. recent research on large language models (llms) has shown that they can learn from few examples and reason well. we believe that it is worth exploring ways to enhance the prompting of llms to improve their completeness when responding to ambiguous questions and reduce model hallucination in generation (openai, 2023; zhao et al., 2023; sun et al., 2023b,a). another direction worth exploring in the future is the application in lowresource scenarios, such as low-resource languages. low-resources in our study are characterized by limited multi-answer-qa annotations, which aims to examine how data size impacts model performance. other low-resource languages may behave differently with less training data and large models (xue et al., 2020; sun et al., 2021). besides, we would like to explore more effective prompting methods, such as chain-of-thought prompting (wei et al., 2022b)."
979,"limitations argscichat is the result of a pilot study concerning 31 invited nlp experts. in particular, argscichat contains dialogues about 20 scientific papers regarding a few nlp topics. thus, dialogues in argscichat are only a small sample of the set of possible dialogues grounded in scientific papers. in particular, several design choices have been considered in our data collection methodology: (a) the topic of a paper; (b) the common background of invited nlp experts; (c) the available content of a paper during a dialogue; and (d) the dialogue setting (e.g., in our implementation, dialogues had a time limit which restricted the number of interactions between subjects). we chose the nlp domain in our study since we (the authors) have expertise in this domain. this choice also facilitated the def- inition of a pool of nlp experts to participate in our study through our research network. furthermore, dialogues in argscichat are grounded in scientific papers. in particular, we limit the paper’s content to the abstract and introduction sections. this choice reduces subjects’ effort to act as e, while also providing enough information to sustain a dialogue. thus, we do not collect dialogues between subjects concerning other sections of a paper."
980,"limitations. while we try to perform multilingual lexical specialization on a set of typologically diverse languages, we are still restricting our analysis to a small fraction of all the languages of the world. in addition to this, our analysis investigates only two mmts – albeit arguably the two most widely used. due to hardware limitations, we experimented with xlm-r base: the results we report may be substantially different for xlm-r large (or other larger mmts like mt5), which possibly encodes more lexical knowledge. ethical considerations. we leverage lexical constraints from babelnet, a resource constructed semi-automatically. babelnet may contain lexical associations reflecting negative social biases (e.g., sexism or racism). biased constraints, when used as training data in our specialization, may strengthen societal biases present in mmts. acknowledgments tommaso green and simone ponzetto have been supported by the join-t 2 project of the deutsche forschungsgemeinschaft (dfg). goran glavaš has been supported by the euinaction grant funded by norface governance (462-19-010) through deutsche forschungsgemeinschaft (dfg; gl950/2-1). we additionally acknowledge support by the state of badenwürttemberg through bwhpc and the german research foundation (dfg) through grant inst 35/1597-1 fugg. we thank our colleague sotaro takeshita for insightful"
981,"limitations we identify several limitations in this work: (i) false negatives: our current automatic triple extraction pipeline is built using the ds approach followed by filtering using an nli model. however, wikidata is not complete (tan et al., 2022). while some triples may not be completely available in webie, we expect models trained on this dataset can still discover new triples that do not exist in wikidata. (ii) limited relations in annotation: the human annotation is only conducted on the most frequent 200 relations. (iii) limited languages in mwebie: as discussed in §2.3 and appendix c, the languages in mwebie are limited to official languages from geographical regions where there is a reasonable amount of mturk workers to accept the job. an alternative solution would be to use professional translators, especially for low-resource languages. (iv) fixed dataset: facts might change in the world (and wikidata). this can lead to a degraded real-world performance if a system relies exclusively on webie for evaluation when the dataset is not updated accordingly."
982,"limitation we discuss some limitations of our study. our proposed method needs a pre-trained model to obtain the attention score mentioned in section 3.3. while our method achieved successful attacks with 4∼5 times fewer queries compared to the baselines, the time spent on the attack was approximately half of the baselines. this is because preprocessing is necessary to calculate the score v in section 3.1 and d in section 3.2."
983,"limitations of existing approaches regarding htc. in particular, tending to minimize structural entropy, we design circa to construct coding trees for the label hierarchy. to further extract textual information, we propose hitin to update node embeddings of the coding tree iteratively. experimental results demonstrate that hitin could enhance text representations with only structural information of the label hierarchy. our model outperforms existing methods while greatly reducing memory increments. limitations for text classification tasks, the text encoder is more important than other components. due to the lack of label semantic information and simplified learning procedure, the robustness of text encoders directly affects the performance of our model. from table 2 and 3, we could observe that bert has already surpassed textrcnn by 4.52% and 6.43% on micro-f1 and macro-f1. besides, bert beats all the textrcnn-based methods on rcv1-v2 and nytimes. however, when applying bert as the text encoder, our model makes slight improvements to micro-f1, especially on wos. a probable reason is that bert was pre-trained on news corpus while wos consists of academic papers."
984,"limitations psgd is a straightforward constrained-decoding algorithm for the translation suggestion task. how- ever, the early-stopping mechanism involves extra time costs. though psgd is more efficient than vdba in the scene of ts, where only two constraints (prefix and suffix) appear, it could be slower than vdba if there were more short constraints. besides, even if we take both prefix and suffix constraints into consideration for emphasis on the whole translation generation, the decoding process is still auto-regressive from left to right. the algorithm could be improved if we made better use of the information of suffix constraints. for example, how to apply psgd on non-autoregressive models, such as (gu et al., 2019) and (yang et al., 2021b) will be our future work."
985,"limitations there are several limitations to the current study. firstly, as for now, the corpus used in this study only consists of a single language pair. secondly, the coherence of the mt systems was evaluated using a fine-tuned conference model, as no anno- tations were available for the mt outputs. however, as shown in tab. 8, the fine-tuned conference model is not perfect and may affect the quality of our coherence evaluation. thirdly, this paper focuses on using discourse annotations to reveal and analyze discourse phenomena and the challenges they present to machine translation. using the annotations to improve mt models is beyond the scope of this study and is left for future work. ethical considerations the annotators were paid a fair wage, and the annotation process did not solicit any sensitive information from the annotators. in regard to the copyright of our dataset, as stated in the paper, the crawling script that we plan to release will allow others to reproduce our dataset faithfully and will not be in breach of any copyright. in addition, the release of our annotated test set will not violate the doctrine of fair use (us/eu), as the purpose and character of the use is transformative. please refer to https://www.nolo.com/legal-encyclopedia/ fair-use-the-four-factors.html for relevant laws."
986,"limitations the cmot method for speech translation is based on the idea of cross-modal knowledge transfer and the paradigm of multi-task learning, so it requires the transcripts of speech during training, which may not be applicable to some languages without transcribed text. besides, this work mainly focuses on the finetuning of speech translation, and applications on the pretraining phase or on other tasks are in need of exploration in the future."
987,"limitations we analyze the limitations of this work, so as to further improve the performance of our model in future work. based on our empirical observation, we reveal several limitations, which can be divided into two primary categories. (1) first, our proposed spectra method relies on large-scale spoken dialog corpora with explicit word-level speech-text alignment annotation, such as spotify100k. this limits the generality of our model on more spoken dialog corpora. in the future, we would like to develop a semisupervised pre-training method to leverage both labelled and unlabeled datasets. (2) second, our method is mainly designed for speech-text understanding and has not been fully explored for generative tasks. we plan to devise dialog generation per-training objective to empower the model with better generation ability. (3) third, the work only involves speech and text modalities. we are interested in handling more modalities, such as images or videos, to enrich cross-modal information in joint representations."
988,"limitations since methods based on pre-trained language models on text style transfer requires larger gpu resources and are not mainstream methods, we have not yet tested the effectiveness of our method on pre-trained language models. moreover, since there is no multiple-attribute dataset in existing research, the applicability of our method on multipleattribute tst tasks has also not been verified."
989,"limitations currently, cone is mainly implemented for proposal-based models as they can generate ex- plicit moment proposals for the introduced interwindow contrastive learning. in contrast, proposalfree methods (e.g., vslnet (zhang et al., 2020a)) directly predict the start/end timestamps without explicit proposals. in the future, it is worthwhile to explore how to incorporate the coarse-to-fine alignment mechanism with proposal-free methods to enhance the generalization ability of cone. furthermore, cone falls short on the groundtruth moment case whose duration is longer than the adopted video window duration. to ease this issue, future work can explore adaptive-duration window slicing to ensure complete containment of scenes and events within windows or some rulebased proposal merging techniques."
990,"limitations although our method has achieved outstanding performance compared to current kd techniques, it is still a word-level kd method and also suffers from some limitations in vanilla word-level kd, e.g., the exposure bias as analyzed in appendix a. how to design a unified and more powerful kd method from the perspective of the connection between word- and sequence-level kd still remains unsolved. we will leave this for the future work. moreover, our study focuses on the mainstream kd techniques in nmt, which transfer knowledge through teachers’ predictions, while some other kd techniques, like directly distilling the hidden states (wu et al., 2020), are not within the scope of this study and thus not included."
991,"limitations for hkg one-position link prediction tasks, hahe shows the best performance in all three datasets. however, because hahe is based on hypergraph learning, it improves more on the wd50k high quality hyper-relational knowledge graph link prediction dataset, and less on the wikipeople dataset where triples are the majority, so hahe prefers the fact with more arity numbers. in the future, we will consider extending our approach to triples as a unified architecture. for hkg multi-position link prediction tasks, it can be seen that our model is effective when predicting multiple missing auxiliary information, which is a frequent situation in practical applications. however, the prediction accuracy of our model needs to be further improved in the case of missing primary relations."
992,"limitations there are several limitations to our framework. specifically, since observations are introduced as guiding information, our framework requires observation extraction tools to label the training set in advance. then, the nodes contained in the observation graph are mined from the training data. as a result, the mined n-grams could be biased when the overall size of the training set is small. in addition, our framework is a pipeline, and the report genera- tion performance highly relies on the performance of observation planning. thus, errors could accumulate through the pipeline, especially for small datasets. finally, our framework is designed for radiology report generation targeting chest x-ray images. however, there are other types of medical images (e.g., fundus fluorescein angiography images) that our framework needs to examine."
993,"limitations the main limitation of our work is the high memory and computation cost. as both our methods estimate the importance of training examples based on the prompt-performance statistics, we first need to run in-context learning on the dev set multiple times with different prompts in dicl. although icl does not require any parameter updates, llms still require a large amount of memory footprint during inference, especially when the model size is large and the average sequence length is long. for each setup, our dicl contains around 50,000 prompts (see table 8) and 50 dev examples per class, so the most expensive setup (running opt13b on boolq) costs more than 500 gpu hours on an rtxa6000 gpu. our preliminary study shows that the proposed methods need the statistics of at least 10,000 randomly sampled prompts to perform well. future work may use search algorithms instead of random sampling during data collection to reduce the number of prompts in dicl. we also release our collected data of every setup in https://github.com/terarachang/dataicl to support future studies on icl. in this paper, we only study classification tasks, for the sake of easy evaluation. future work may study the influence of in-context examples in generation tasks under different evaluation metrics. due to hardware constraints, we do not study llms of sizes over 13b, and we fix the number of shots and prompt templates for simplicity. in independent work, nguyen and wong (2023) complement these limitations of our paper, showing that similar approaches work well on larger models and a diverse number of shots for in-context example selection. still, the influence of in-context examples for gigantic llms larger than 100b parameters has not been studied. due to emergent abilities of llms (wei et al., 2022), it is unclear whether our methods and findings would still apply when prompting these gigantic llms."
994,"limitation the unstructured sparse patterns we introduce are not as hardware-friendly as the structured patterns, suggesting the speedup of using unstructured patterns maybe limited due to the implementation. the number of parameters of models we are studying are only at the level of 100 ∼ 300m, and the datasets are focus on glue, e2e, webnlg, and dart. we will generalize to wider choices of datasets in future works."
995,"limitations we discuss two limitations of this work as follows: one limitation of our work is the lack of taskspecific automatic metrics to evaluate the empathy of generated responses. therefore, the evaluation of empathy relies more on human evaluation. although human evaluation is a golden standard, automatic metrics help to conduct large-scale investigations. this is also a common limitation in current works on empathetic dialogue. the second limitation is the passive response to the user’s cognition and affection. in many scenarios, empathy is used as a strategy for emotional support by responding to the user’s cognition and affection. however, besides passive response, emotional support also requires active emotion elicitation, which can be studied in future work. ethical considerations in this paper, our experiments adopt the widely used empatheticdialogues benchmark, an open-source dataset collected from amazon mechanical turk (mturk) that does not contain personal information. we also ensure the anonymization of the human evaluation. we believe that this work honors the ethical code of acl."
996,"limitations we acknowledge that the randomness of our noise generation procedure may generate a new entity span that can be considered unnatural (for example, adding prepositions to a city name). such aspect of our method may have some impact on the performance levels measured, as distinct types of annotation mistakes can affect model performance differently. in this case, added noisy words that are improbable to be part of the entity may be more easily “ignored” by the model, while ambiguous additions can lead to a mistake. still, this approach can be used as a baseline and a more sophisticated performance assessment using more complex modeling or even real annotation inaccuracy could be done in future work. in addition, the constraint of expanding the mention boundary by a single token should be also taken into consideration. the reason for this design choice was not only based on narrowing the analysis spectrum by reducing the amount of data we had to investigate but also on time constraints, as the training procedure of multiple models on an even larger number of noisy dataset instances would escalate quickly. however, now conscious of the behavior relaxed annotation has on model behavior, it would be interesting to evaluate how this tendency is transformed by introducing even more unnecessary adjacent context into the annotations. a last limitation that can be pointed out is that we only evaluated the noise effect in a single dataset. there are other widely adopted benchmarks for el, such as msnbc (cucerzan, 2007) and clueweb (cao et al., 2007), which could be used in this work. however, we feel it would be more interesting to juxtapose with other textual domains, especially those with specific jargon and nes, such as the medical domain."
997,"limitations while we tried our best to maximize the diversity and coverage of our benchmark, it is practically impossible to cover all possible input noises. we acknowledge aspects that we did not get to cover, for example, the impact of different input devices (phones, tablets, as compared to keyboards used in our annotation). also, while we tried to re-construct the real-world input settings as much as possible, there may still be subtle differences between real-world input and our annotation process, for example, we posed speed limits during the keyboard input annotation and this may not capture exactly how users type in real applications. we encourage future work to consider how to increase the coverage of such benchmarks and also possible innovations in the data collection procedure to collect fully realistic user data."
998,"limitations of the dataset: (1) the handling of example complexity, (2) the coverage of the dataset in terms of domains and entity types, (3) the number of included languages, and (4) the quality of pretrained language models. these limitations are discussed in more details in the dedicated section below. limitations the analysis from subsection 4.6 shows that the number of entities per example has an important influence on results. for now, the handling of examples with more than one entity includes the detection of the mention, but does not consider other criteria. it would be useful to adapt the tsc methods in order to determine whether the sentiment about all entities is expressed by the same part of the example or not. if sentiment is expressed in different parts of the example, a splitting of the example into parts which express the sentiment about each entity would prove useful. despite a more diversified coverage of the political domain compared to newsmtsc, madtsc remains focused on politics. it would be interesting to include other major news domains, such as environment, business, culture, technology, sports, etc. equally important, all targets from mad-tsc are person names. it would be useful to also include sentiment expressed about other types of named entities (organizations, locations, events, etc.), as well as other polarization-prone concepts in each domain. such extensions of the dataset would provide a more complete view of tsc performance. ultimately, they would make the analysis of sentiments expressed in news articles more comprehensive and reliable. while mad-tsc is the first multilingual aligned dataset designed for tsc in news, it would benefit from the inclusion of more languages, including non-european ones. this limitation is due to the unavailability of massively multilingual and aligned news datasets which could be used to include more languages. a potential solution to overcome these limitations would be to enrich the dataset with manual translations in other languages. however, this solution is costly and is left for future work. finally, the comparison of results between languages is biased because the effectiveness of available pretrained language models is variable. this is a limitation which is shared by most nlp works which focus on multilingual datasets and reuse pretrained models, themselves trained on whatever datasets available for each language."
999,"limitations we only create a high-quality dataset for evaluation and a small-scale dataset for few-shot learning, since the lack of a large-scale chinese parallel ss corpus. the available research methods for chinese ss are limited to unsupervised learning and few-shot learning. we hope a large-scale chinese parallel ss corpus can be released in the future. then we can directly train more supervised models for chinese ss. furthermore, we only analyze whether the current standard metrics are suitable for the evaluation of chinese ss, and leave the work of proposing a new metric for future study. due to time constraints, we do not perform a human evaluation for the output of llms. we hope to conduct a more comprehensive evaluation for llms in the future."
1000,"limitations our work aims to uncover how the distribution of factual and contextual errors in referring expression generation varies based on the familiarity of entities. our proposed experiments uncover this effect using pragmatics-driven heuristics. we need a more general deep-dive into what models “know"" to estimate how language models handle known and unknown information differently, in a way that might even escape human scrutiny. further, our"
1001,"limitations as this work is mainly focused on weakly supervised vision-and-language pre-training, we do not fully explore the factors that may influence the performance of relative representations, such as the use of different unimodal encoders and the source of the anchors. besides, we only validate the effectiveness of relative representations in a weakly supervised setting, while it remains to be explored whether it is also useful for standard vlp and multimodal learning in other modalities (e.g., audio and video). we will further exploit the potential of relative representations and validate it in more cross-modal learning scenarios in the future."
1002,"limitations as depicted in table 4, there are scenarios where argu demonstrates a lack of understanding and instead paraphrases the input variables to generate an incorrect response. it seems likely that the model associates negation with con. however, in exam- ples 5 and 6, the model does not factor the word “stop” in variable 1, leading to arguments that contradict the intended stance. further, in examples 7 and 8, the argument decoder seems to modify the generated template, which changes the overall meaning of example 7. such scenarios might reduce the trust in the model, hurting its practical use. all experiments involving argspan, argspanscheme, and argu only pertain to abortion, minimum wage, nuclear energy, gun control, the death penalty and school uniform. the model performance on any other topics is unknown. although we test argspanscheme on out-of-domain test sets, it still confines the six topics. since argu is trained only on argument sentences with less than 150 tokens, it is more geared towards generating shorter arguments of less than 50 tokens. we further do not benchmark argu’s inference time for practical use."
1003,"limitations the main limitation of our methodology is that the training of generative question answering models requires the usage of large gpu resources, which may not be easily available to all researchers. regarding the performance of the model and quality of the generated answers, our approach can be affected by possible bias induced by the evaluation system we are using. for the experiments in this paper, we only consider datasets from the english language, however, we conjecture that our techniques should work similarly for languages with a similar morphology. automatic qa evaluation systems do not achieve perfect correlation with human annotations, which indicates a gap with respect to human evaluation. for safety critical applications, human evaluation of generated answers still remains the best means for evaluation."
1004,"limitations in this section, we discuss several limitations of our work that are worth future study. first, the performance of the hierarchical transformer retriever is limited since the utterance-level transformer is trained from scratch only on our small-scale dataset due to limited time and computational resources. with more resources, future work can focus on pre-training the utterance-level transformer on large-scale data such as the complete pushshift reddit data (baumgartner et al., 2020). pre-training can potentially improve the performance of the retriever and further improve 8412 the generation quality. second, the two types of retrieved responses in the recap-mixed model are encoded with the same encoder. however, intuitively, the two types of responses should contribute to generation in different ways, so treating them the same way might harm generation performance. this is also reflected in our results. even though the recap-mixed model shows improvement from both types of retrieved responses, the improvement is weaker than that on each separate model. in future work, designing a split encoder for different types of retrieved responses may help maximally preserve the performance boost from both types of retrieved responses."
1005,"limitations though autoregressive span selection effectively weakens the conditional independence assumptions imposed by current span-based parsing methods, this strategy imposes another arguably unreasonable inductive bias of forcing a predefined span selection order. we find using post order performs fairly well but this does not necessarily means it is the best order for span selection, and this work might leave other potentially better order unexplored. future works might consider using set prediction methods (sui et al., 2020; tan et al., 2021) 11for instance, if we want to extend the model of nguyen et al. (2021) to discontinuous parsing (assuming binarization), the number of splitting points is nondeterministic which can be difficult to handle: a continuous parent span could be split into two continuous subspans with one splitting point, or one continuous subspan and one discontinuous subspan with two splitting points, or two discontinuous subspans with at least three splitting points (depending on the fan-out of subspans). the cases of discontinuous parent spans are even more complicated. to eliminate such implausible inductive bias (of forcing orders) while still benefiting from explicit span correlation modeling. another issue is regarding time complexity. though our model needs only linear steps (in sentence length) for parsing, each step takes o(n2) time to select a single span over o(n2) total spans, making the overall time complexity cubic. we remark that for each step the o(n2) operation is parallelizable and—with full gpu parallelization— fairly fast in practice, but it would still be problematic when the sentence is extremely long."
1006,"limitations this work introduces the general idea of incorporating attribution into knowledge distillation, and there are three potential limitations. first, although ad-kd chooses integrated gradients for attribution, there are actually other attribution methods (janizek et al., 2021; sikdar et al., 2021) which can also be fitted in our framework. the question of whether these methods perform better than integrated gradients when combined with knowledge distillation is still unclear. second, we conduct experiments on bert of different scales and have not yet validated the effectiveness of ad-kd on other model structures. third, while we only perform task-specific knowledge distillation in our experiments, applying ad-kd to task-agnostic knowledge distillation is also worth investigating."
1007,"limitations the dataset we collected annotates questionable assumptions based on the current facts at the time of annotation. however, the status of questionable assumptions can change for some questions in the future. for instance, the question what episode does aidan appear in just like that contains an assumption that he appeared in some episode of the show. as of january 2023, this is invalid because the season that he has been confirmed to appear in has not aired yet, meaning that no episode of the show is public. but this assumption will become valid when the show airs. furthermore, annotation errors may arise due to the intrinsically challenging nature of verifying that something did not happen or does not exist. while the annotators tried to provide convincing evidence and used their best judgment to decide on the unverifiability of the assumptions, they could not check every relevant document available online. in theory, this would be necessary for perfect verification of nonexistence, unless there is an explicit statement of nonexistence. however, such statements are inherently rare due to reporting bias (e.g., people rarely explicitly state bananas are not rainbow colored). the annotators often used pragmatic inference, taking omission as supporting evidence that the event did not happen or the entity does not exist (e.g., taking statements like bananas can be yellow, green, brown, or red as supporting evidence for bananas are not rainbow colored). this is nonetheless heuristic and may be a source of annotation errors."
1008,"limitations since our approach identifies common opinions based on frequency of sentence encodings, we require a relatively large number of input sentences. we were not able to experiment with other popular datasets like amazon (he and mcauley, 2016), yelp (chu and liu, 2019) or rotten tomatoes (wang and ling, 2016) since these datasets only include a small number (usually 8) of input reviews. the abstractive summaries are generated solely based on the latent encoding, and our model does not include a copy mechanism or attend to the original inputs when decoding. it therefore does not always generalize well to new domains. however, this limitation is mitigated by not requiring any labelled data during training: hercules can easily be retrained on a new domain. generating output based only on latent encodings means that the model is also susceptible to hallucinating, since the output is less directly linked to the inputs. however, unlike other methods, hercules provides evidence sets alongside the generated summaries, making it easier to check whether the output is faithful. finally, hercules generates summary sentences independently, leading to summaries that are less coherent than approaches that model the summary as a single sequence. we welcome future work on combinining the relative strengths of each approach. we do not anticipate any significant risks resulting from this work."
1009,"limitations since our method constructs on the multimodal transformer, it cannot be migrated to the dualstream model. experiment results show that cfsum can achieve comparable performance with strong baselines. but it still cannot surpass the sota of some dual-stream large models."
1010,"limitations during the creation of the remuq dataset, we simply remove the words in the question that are duplicated in the image caption – in some cases, this may result in grammatical errors in the text query. we performed the experiments for studying optimal masking ratio on a subset of the pretraining data, due to resource constraints."
1011,"limitations in this paper, we explore label-semantic augmentation (la) for multi-label few-shot intent detection via appending label name after utterances, which is similar to instruction learning or prompt learning. however, we don’t further study the relationship between la and instruction learning due to space limitations. we believe that instruction learning integrated with labels will inspire further investigation for multi-label few-shot intent detection."
1012,"limitations in this paper, we mainly focus on developing an amortized model to efficiently achieve a reliable estimation of sv. though not experimented with in the paper, our method can be widely applied to other black-box post-hoc explanation methods including lime (ribeiro et al., 2016). also, due to the limited budget, we only run experiments on bert-based models. however, as we do not make any assumption for the model as other blackbox explanation methods, our amortized model can be easily applied to other large language models. we only need to collect the model output and our model can be trained offline with just thousands of examples as we show in our method and experiments. comparison and training with exact shapley values computing exact sv is computationally prohibitive for large language models (llms) on lengthy text inputs, as it necessitates the evaluation of llms on an exponential (in sequence length) number of perturbation samples per instance. as a result, we resort to using svs-25, which serves as a reliable approximation, for training our amortized models."
1013,"limitations in this section, we discuss the limitations of this work as follows: • peerda leverages labeled spans in the existing training set to conduct data augmentation. this means that peerda improves the semantics learning of existing labeled spans, but is ineffective to classify other spans outside the training set. therefore, it would be beneficial to engage outer source knowledge (e.g. wikipedia), where a variety of important entities and text spans can also be better learned with our peerda approach. • since peerda is designed in the mrc formulation on top of the encoder-only pre-trained language models (plms) (devlin et al., 2019; liu et al., 2019), it is not comparable with other methods built on encoder-decoder plms (yan et al., 2021b; chen et al., 2022; zhang et al., 2021b; yan et al., 2021a). it would be of great value to try peerda on encoder-decoder plms such as bart (lewis et al., 2020) and t5 (raffel et al., 2020), to see whether peerda is a general approach regardless of model architecture. • as shown in table 9, although peerda can significantly alleviate the missing predictions, the most prevailing error in the mrc model, peerda also introduces some new errors, i.e. multiple labels and incorrect label. it should be noted that those problematic spans are usually observed in different span sets, where they would learn different category semantics from their peers. therefore, we speculate that those spans tend to leverage the learned category semantics more than their context information to determine their categories. we hope such finding can shed light on future research to further improve peerda."
1014,"limitations the main limitation of the presented work is the need for significant computing resources to train multimodal models using dynamic uda. it should be noted that the proposed methods, mmbv and dynamic uda, require fewer computational resources than the original version of uda."
1015,"limitations tasks in bgglue the bgglue benchmark is comprised of nine challenging nlu tasks, including three token classification tasks, one ranking task and five text classification tasks. while we cover three different types of tasks in the benchmark, we are restricted by the available resources for bulgarian, and thus we could not include some other nlp tasks, such as language generation. we also consider only nlp tasks and we do not include tasks with other/multiple modalities. finally, some of the tasks are of similar nature, e.g., we include two datasets for ner and two for credibility/fake news classification (see section 2). domains in bgglue the tasks included in bgglue span over multiple domains such as social media posts, wikipedia, and news articles and can test both for short and long document understanding. however, each task is limited to one domain and the topics within the domain do not necessarily have full coverage of all possible topics. moreover, some of the tasks have overlapping domains, e.g., the documents in both cred.-n and fake-n are news articles. baseline models as described in section 5, the baseline models provided for bgglue include fairly small encoder-only transformer architectures. we leave for future work other modeling architectures and modeling techniques that are known for improving the efficiency and the computational requirements of the used models, e.g., few-shot and zero-shot in-context learning and instruction-based evaluation, multi-task learning, etc. model biases in this work, we did not explore whether the datasets in bgglue contain unwanted biases, which could also lead to potentially hazardous behavior of the baselines we trained in our experiments with the bgglue benchmark."
1016,"limitations • our study focuses on word replacement attacks. while these attacks are the most common in the literature, the human perception of attacks that rely on insertion or deletion can differ from our"
1017,"limitations we experiment with a range of adaptation domains that we draw systematically to capture the covariates enumerated in section 4.1. however, future work should acknowledge that these are not all the covariates responsible for the success of adaptation and the robustness of the final model. following is the non-exhaustive list of possible covariates that we do not control in this work. (i) the adapted model size, (ii) the size of pre-training data, (iii) pretraining configuration parameters, but also (iv) the broad variance of adapted language pair(s); (v) the variance of mutual similarity of languages within the pair, and hence (vi) the difficulty of training the translation model. the evaluation of our experiments did not consider the effect of randomness of the training process. despite the fact that our experiments were run with a fixed random seed and initial value, making our results deterministically reproducible, the variance of the results among the experiments of different random seeds was not investigated due to the related infrastructural costs. however, all our results are aggregated over a larger set of checkpoints and/or domains, ranging from 10 (ids in table 1) to 720 (oods in table 2), as described in section 4.2. the alignment scheme proposed in section 3.1 might have blind spots; for instance, in the cases utilizing decontextualized embeddings, where both the hypothesis and reference contain multiple occurrences of the same word, the alignment scheme will make the prediction of the same target token equally good, regardless of the position. in future work, this imperfection could be addressed by using the optimal transport algorithm (kusner et al., 2015) within the alignment, similarly to zhang et al. (2020a)."
1018,"limitations although our proposed method asap is able to outperform baseline estimators, an important factor it ignores is the subjectivity of user satisfaction. in practice, different users may have different degrees of satisfaction with the same dialogue. this implies that asap may be effective for some users, but it may also fail to predict true satisfaction for others. in order to adequately simulate a user, it is essential to take the issue of subjectivity into account. given this, we would like to extend asap for personalized satisfaction estimation by incorporating user profile information in the future."
1019,"limitation while dealing with memes, which will be considered in the future."
1020,"limitations one limitation of this work is the lack of human performance scores on the new tasks. although the baseline performance is far from perfect, and it seems quite likely that human performance is much better, this should be measured in future work. another limitation is that it is unknown how much each task should benefit from access to the audio in addition to text; this could be measured in principle for humans, but again we leave this to future work. broader impact and"
1021,"limitations our work has several limitations. first, our experiments are limited by the multi-domain datasets available for sequence classification tasks, limiting both our task coverage (sentiment classification and nli) and domain type coverage (product categories, temporal splits, and text source domains). future work can evaluate our drift metrics on token classification tasks or even sequence-to-sequence tasks by predicting sequence-level performance (e.g. proportions of correct tokens, or examplelevel bleu scores; papineni et al., 2002) from our example-level drift metrics. past work has already considered dataset-level drift metrics and performance predictions for token classification tasks such as named entity recognition (ner) and part-of-speech (pos) tagging (ramesh kashyap et al., 2021; rijhwani and preotiuc-pietro, 2020), and example-level drift metrics have been used in machine translation for training data example selection (axelrod et al., 2011; wang et al., 2017). we hope that future work will evaluate example-level drift metrics in their ability to predict nlp model performance on this wider variety of tasks. second, we only consider simple logistic regressions to predict whether individual examples will be predicted correctly by different models. more complex classifiers (e.g. xgboost; chen and guestrin, 2016) might improve performance predictions, particularly if more drift metrics are included as inputs, or if raw example features are included (e.g. sequence length; ye et al., 2021). our three dimensions of linguistic drift (vocabulary, structural, and semantic drift) represent just one way of decomposing linguistic dataset drift into distinct dimensions. we hope that future work will explore novel dimensions of linguistic drift, identifying new ways of integrating different drift metrics into nlp model performance predictions across tasks and domains."
1022,"limitations the performance of kpe is also related to the used pre-trained language model (plm), in addition to the proposed framework. kpe could suffer from unsatisfactory performance when the base plm is not strong enough. applying our proposed kpe to stronger plms, such as deberta, may lead to further improvements."
1023,"limitations we are evaluating s2st in an artificial setting given that we have to synthesize the text references. in fact, since there was no metric capable of evaluating the quality in speech, there was no motivation to build such benchmarks either (the chicken-and-egg problem). however, we expect that next benchmarks for the task will have speech references be- cause of the rise of end-to-end s2st systems and their quality increase. blaser paves the way so that we can take advantage of such benchmarks when they appear. our metric works at the sentence-level, by embedding the entire sentence into an intermediate space. we ignore how sensitive blaser is to the length of the sentence, which is a key aspect when we want to extend to the corpus-level metric in the future. moreover, we are aware that sometimes sentence embedding do not discriminate different numbers or words that belong to the same word family, which may disregard impactful errors such as the change of a number in the translation output. ethical considerations translation quality scores were provided by bilingual raters as mentioned in section 4. they were all paid a fair rate. we can not open-source the data form our experiments given that our sources are shared under no-derivative license. small human evaluation detailed in appendix d was done by volunteers."
1024,"limitation of assigning labels to people as being inherently reductionist. as mentioned in §7, using a single likert scale for social acceptability and toxicity is not sufficient in capturing the complexities in these phenomena, such as situational context. we note that quantifying positionality of existing systems is not an endorsement of the system. in addition to making sure that language technologies work for all populations, researchers should also continue to examine whether these systems should exist in the first place (denton and gebru, 2020; keyes et al., 2019). further, we note that understanding a dataset or model’s positionality does not preclude researchers from the responsibilities of adjusting it further. this study was undertaken following approval from the irb at the university of washington (study00014813). labinthewild annotators were not compensated financially. they were lay people from a wide range of ages (including minors) and diverse backgrounds. participants were asked for informed consent to the study procedures as well as the associated risks, such as being exposed to toxic or mature content, prior to beginning the study. research team positionality we discuss aspects of our positionality below that we believe are most relevant to this research. the research team is comprised of computer scientists who study human-computer interaction and nlp and have a bent for using quantitative methods. thus, we approach the topic from a perspective that assumes that positionality can be characterized, fixed, and quantified. the entire research team currently resides in the united states. in alphabetical order, the team members originate from belgium and switzerland, france, germany, india, and the united states; and identify as east asian, south asian, and white. these nationalities and ethnicities are overrepresented in the development of nlp technologies. thus, we acknowledge that our knowledge of how design biases in nlp datasets and models impact people is largely through research, rather than personal experience."
1025,"limitations community survey the winoqueer benchmark is necessarily an imperfect representation of the needs of the lgbtq+ community, because our sample of survey participants does not represent the entire queer community. crowdsourcing, or volunteer sampling, was used for recruiting survey participants in this study as it has its strength in situations where there is a limitation in availability or willingness to participate in research (e.g., recruiting hard-to-reach populations). however, this sampling method has a weakness in terms of generalizability due to selection bias and/or undercoverage bias. we limited our survey population to english-speakers, and the winoqueer benchmark is entirely in english. we also limited our survey population to adults (18 and older) to avoid requiring parental involvement, so queer youth are not represented in our sample. additionally, because we recruited participants online, younger community members are overrepresented, and queer elders are underrepresented. compared to the overall demographics of the us, black, hispanic/latino, and native american individuals are underrepresentend in our survey population. geographically, our respondents are mostly american, and the global south is heavily underrepresented. these shortcomings are important opportunities for growth and improvement in future participatory research. finetuning data collection in an effort to balance the amount of linguistic data retrieved from media cloud and twitter respectively, we had to use additional search terms for media cloud as it yielded significantly fewer results than twitter when using the same search terms. also, news articles from january to may 2022 are excluded from the news article dataset due to media cloud’s backend api issues. due to the size our datasets and the inexact nature of sampling based on hashtags, it is likely that there are at least some irrelevant and spam tweets in our sample. template creation our generated sentences have several limitations and areas for improvement. first, our nine identity subgroups are necessarily broad and may not represent all identities in the queer community. the winoqueer benchmark is limited to biases about gender and sexual orientation. it does not consider intersectional biases and the disparate effects of anti-lgbtq+ bias on individuals with multiple marginalized identities. the names used in templates are taken from the us census, so they are generally western european names common among middle-aged white americans. noneuropean names are not well-represented in the benchmark. additionally, the benchmark currently only includes he, she, and they personal pronouns; future versions should include a more diverse set of personal pronouns. finally, sentences are generated from a small set of templates, so they do not represent every possible stereotyping, offensive, or harmful statement about lgbtq+ individuals. a high winoqueer bias score is an indicator that a model encodes homophobic and transphobic stereotypes, but a low bias score does not indicate that these stereotypes are absent. evaluation and finetuning we used similar, but not identical, scoring functions to evaluate masked and autoregressive lan- guage models. it is possible that the metrics are not perfectly calibrated, and that one category of models may be evaluated more harshly than the other. additionally, some of our finetuned models scored below the ideal bias score of 50. this means that they are more likely to apply homophobic and transphobic stereotypes to heterosexual and cisgender people than to lgbtq+ people. many of these stereotypes are toxic and offensive regardless of the target, but others do not carry the same weight when applied to cis and straight individuals. currently, it is not well-defined what wq scores under 50 mean, in theory or in practice. this definition will need to be developed in consultation with researchers, end users, and the lgbtq+ community. this paper only includes results for a small fraction of available pretrained language models, and our results only represent comparatively small models. we present baseline results for models up to 7.1 billion parameters and finetuned results for models up to 1.5 billion parameters, but many of the models in use today have hundreds of billions of parameters. finally, our results are limited to opensource models and do not include closed-source or proprietary models."
1026,"limitations although we introduce a new gmner task and propose a number of baseline systems and an hindex framework, there are still some limitations in this work. first, our gmner task only requires identifying the visual regions that are correspondent to named entities mentioned in text. however, for each image, many visual regions may contain real-world entities that are not mentioned in text. therefore, it would be interesting to further annotate the entities that only occur in the image and explore a more complete mner task in the future. second, our work is a preliminary exploration of the gmner task, and the proposed approaches are primarily based on previous representative ner or mner methods. we hope this work can encourage more research to apply the recent advanced techniques from both the nlp and computer vision communities to improve its performance."
1027,"limitations there are three limitations on our method. first, we did not verify our method on more generic tasks, such as text classification, yet it is not limited to commonsense qa. extending our method to other downstream tasks is our future work. second, our method requires a longer training time and a larger gpu memory since the knns require forward and backward propagation additionally. third, we do not consider the ambiguity of gold answers, which may affect the quality of knns. for example, “apple” may refer to a kind of fruit or a technology company."
1028,"limitations first, we again emphasise that the lack of highquality non-english image-caption pairs is a primary obstacle to wider-scale multilingual and cross-lingual tti investigations. we hope that researchers in the future can construct and release more high-quality vision-language data for different languages, especially for low-resource ones. second, our work uses 512-dim ‘xlm-r large vit-b/32’ mclip22 and is based on the stylegan2 framework (karras et al., 2020b). since the main focus of our work is to realise multilingual and cross-lingual tti and enable fair comparisons across different models and approaches, we compare all proposed and baseline methods with the same mclip text encoder and the gan framework. however, for readers and potential users interested in ‘chasing’ stronger absolute fid scores, we speculate that the larger 640-dim ‘xlm-r large vitb/16+’ mclip text encoder and the more recent stylegan3 (karras et al., 2021) can be helpful. third, we notice that in addition to lafite, several state-of-the-art large diffusion models such as those from saharia et al. (2022) and rombach et al. (2022) also use clip to condition image generation on text input. this means that we could be able to derive multilingual diffusion models for mtti also by replacing clip with mclip and enhance the mtti performance with our proposed ensad (of course, we would need to redesign our loss functions). however, due to limited computational resources, we leave it to future work. fourth, the ensad boosts cross-lingual transfer for tti by combining the knowledge from multiple translations, which can mitigate potential translation errors. our work does not demonstrate if ensad is applicable and adaptable to downstream cross-lingual tasks besides tti. it is because 1) downstream tasks other than tti are out of the scope of this work and 2) adapting ensad to different tasks will require redesign of model structures and losses catering to the characteristics of each downstream task, making us believe it is not proper to expand the topic and include everything in a single piece of work. therefore, we also leave this to future work. 22https://github.com/freddefrallan/multilingual-clip"
1029,"limitations we discuss the limitations of our work. first of all, our model lyra is build upon pre-trained language models (ptlm) including bart (lewis et al., 2020) and gpt-2 (radford et al., 2019). although our method is much more data friendly than previous methods in that it does not require training on melody-lyric aligned data, our pipeline may not apply to low-resource languages which do not have ptlms. second, our current adoption of melody constraints is still simple and based on a strong assumption of syllable stress and note duration. we encourage future investigation about other alignments such as the tone or pitch variations. lastly, although we already have the music genre as an input feature, it remains an open question how to analyze or evaluate the generated lyrics with respect to a specific music genre."
1030,"limitations of pretrained lms on arithmetic reasoning and symbolic manipulation. we experiment with three simple symbolic manipulation tasks and show that improving the locating and induction capability of lms can be important for further improving their performance. our method that combines abstraction and finest-grained step-by-step tutoring demonstrates its potential to generalize correctly, shedding light on possible directions orthogonal to scaling up lms for future work in this area."
1031,"limitations while our approach is designed to be as broadly applicable as possible, an inherent limitation of our work is that it depends on the usage of causal tfrstyle models, which, though architecturally similar to many existing pre-trained models, require hyperparameter search and fine-tuning to replace non-tfrs on downstream tasks. while we find evidence that such models are as capable as other rerankers, and we believe tfrs can be a default design choice going forward with little loss, this extra requirement may still be a barrier for the broad adoption of our approach. more broadly, our approach is designed for settings where some reranker is available. if it is not possible to train a robust reranker, for example in a low data setting or a setting where evaluation relies entirely on human judgments that cannot be reproduced by models, our approach cannot be applied. however, we believe that the growth in learned functions of human judgments as part of rl from human feedback loops provides a promising avenue to roll out our approach to new settings. our experiments were carefully chosen to represent the capabilities of our models with several base tasks and several reranking objectives. we didn’t, however, explore certain domains involving attribute control such as formality or simplicity, choosing instead to explore more quality-based downstream exploration. we showed the applicability of our approach when reranking outputs on languages other than english, but further results on languages with different typological characteristics may show different trends. while our work already provides strong speedups both with candidate sets from lattice and beam search decoding, these speedups become even more valuable for approaches that combine multiple rerankers, which have been shown to potentially lead to further improvements in reranking (fernandes et al., 2022). while we explore this partially in the form of ensembled eel with model probabilities, more exploration on eel for multiple rerankers may be valuable."
1032,"limitations for cls, giving a more faithful solution. limitations the limitation of this paper can be stated from three perspectives. first, although using our cls annotation protocol can label more faithful data, the annotation cost is higher because annotators need to comprehend the full source text instead of only the source summary. second, convsumx only covers 3 typical languages, while languages from different language families and have different morphology and lexical-/syntactic rules require further investigation. third, although the proposed 2-step method is effective, we simply concatenate the source input text and mono-lingual summary at the token level as the model input but do not make further exploration. we believe that more smart and sophisticated designs to integrate features from source input text and mono-lingual summary can further improve the cls performance, which, however, we leave for future work."
1033,"limitations we view strong performance on our evaluation datasets as necessary but not sufficient to demonstrate human-like learning. thus, if models perform poorly on our datasets (as the models we evaluated did), then we have strong reason to conclude that models are not learning in human-like ways. if future models perform better, such results would be consistent with human-like learning but would not conclusively establish that models learn as humans do, as they might instead be using some shallow heuristic that is not controlled for in our datasets. in other words, a criterion that is necessary but not sufficient facilitates strong"
1034,"limitations our theoretical analysis targets a specific notion of information leakage, and it is likely that it does not apply to alternative ones. while the v-informationbased approach seems natural, future work should consider alternative extrinsic bias measures as well as alternative notions of guardedness. additionally, our focus is on the linear case, which is tractable and important—but limits the generality of our"
1035,"limitations our findings should be interpreted considering a series of problem definitions and design choices. first, our quantitative results on measuring incidental bilingualism at scale are subject to language identification, sentence splitting, and mining errors. our qualitative analysis for the english-french language pair revealed that those errors are reasonably small (see §3.2). however, we expect the accuracy of our tools to vary across languages and, crucially, exhibit unanticipated failure modes on web text and low-resource languages (caswell et al., 2020). second, our findings are restricted to quantifying bilingualism and translations within a limited set of language pairs and only paired with english. thus, by problem definition, we are limited to computing a lower-bound estimate on incidental bilingualism of palm. the above limitations should also be taken into consideration when interpreting our ablation results. although we attempted to remove most bilingual signals in our series of mt experiments, it is still possible that bilingualism slips through due to either model errors or due to bilin- gual signals beyond our focus set of languages. finally, any results and findings of our work are restricted to palm; the single llm studied in this work. however, our finer-grained analysis (see table 11 of appendix e) reveals that incidental bilingualism, including translation signals, is observed across various data sources (e.g., webpages, books, etc.) that are commonly included in the training data of other popular llms."
1036,"limitations we synthesize negative instances by substituting relational phrases with misleading tokens. however, the relational semantics in some instances may be expressed implicitly. that is, there are no key tokens that directly correspond to the target relation. therefore, we cannot synthesize negative instances based on these instances. additionally, we consider substitution ratio ϵ as a fixed hyperparameter. it may be a better choice to dynamically determine ϵ based on the input instance. we leave these limitations as our future work."
1037,"limitations while we evaluate our method on three distinct generation tasks, we acknowledge that we rely on a single language (english) and a single type of structural constraints (on top of syntactic dependencies). further work is required to verify if the proposed approach holds on other languages (e.g., it is unclear how much our method is impacted by low-resource languages where syntactic parsers may be of lower quality) and other types of structural constraints (e.g., semantic roles). this work focuses on relatively smaller langauge models and does not address the impact and modes of usage of structural constraints on larger language models such as gpt-3."
1038,"limitations since our eap builds its graph representation from social media data, our method may carry inductive biases rooted in this type of data. moreover, note that the scope of our study is limited to english social media posts and our approach does not consider inputs larger than 512 tokens. therefore using our approach in long document summarization may be challenging. finally, the general applicability of eap in a different domain is highly dependent on the existence of high-quality lexicons for the domain in question, which may not be available."
1039,"limitations despite the contributions of our work, there are also unavoidable limitations of it. first, our method is based on the setting that each utterance in the dialogue except the first one has exactly one addressee. this setting holds tightly in online forums such as twitter or reddit, yet has its limit in group chats or meetings, where an utterance can reply to multiple or no addressees. however, this scenario is relatively rare in multiparty conversations. considering this scenario is challenging and complicated since the one-to-many reply-to relations can cause the single-turn em algorithm intractable. for this part, we leave it to future works. second, the ubuntu irc benchmark of response generation task is extracted from the ubuntu chat corpus (lowe et al., 2015), where people discuss the technical issues on the ubuntu operating system. due to the lack of human annotators with knowledge of linux and ubuntu, we do not con- duct human evaluations on this dataset. however, we do provide the generated responses in our supplementary materials for those who are interested in the human evaluations."
1040,"limitations in this paper, we present a novel approach to remove the permutation invariance of the attention module. specifically, we propose a weight concatenation operation that exactly follows the word order of a sentence, leading to an increase in dimensionality and the introduction of affine transformations aimed at reducing it. hence, the effect of increased parameter counts cannot be well isolated. while our preliminary experiments show that an increase in the number of parameter counts does not necessarily enhance the experimental results, we acknowledge the increased complexity resulting from direct concatenation and, thus, have utilized the equivalent form of the proposed operation in practice. in the future, we aim to explore alternative operations that implicitly encode positional information based on word order, without resorting to affine transformations, to replace the weight sum operation of the attention module."
1041,"limitations due to the limitation of dataset resources, we evaluate our unified model only with tb-dense and matres. although the experiment results show that our approach can significantly outperform stateof-the-art methods, we still need to experiment on more datasets with various kinds of temporal relations to further prove the generalization capability and robustness of our framework."
1042,"limitations learning an ensemble of multiple source models is expensive, especially for large language models. hence, to adapt to the new target domain, we cast the problem as an iterative-decision making process. while our work reduces the model access frequency to 1 or 2 at each training step, continually updating the language model from a stream of test data is still costly. future work can explore better methods for efficient optimization for a single lm. besides, in some cases, the distribution of test data may change dynamically over the stream, but our work considers only the situation where the test data is from one specific distribution. more complex cases of test distribution can be studied in future work."
1043,"limitations this paper mainly focuses on the generalized intent discovery (gid) task in task-oriented dialogue systems. our proposed decoupled prototype learning (dpl) framework well decouple pseudo label disambiguation and representation learning through protopical contrastive learning and prototype-based label disambiguation, and achieves sota performance on three gid benchmark datasets. however, our work also have several limitations: (1) we only verified the effectiveness of our dpl framework on gid task, but the adaptability of dpl in more unsupervised / semi-supervised settings, such as unsupervised clustering and ood intent discovery, is worth further exploration. (2) we follow standard experiment settings as previous work, and assume that each ood sample must belong to a corresponding intent cluster. however, a more realistic scenario is that there may be noise samples in the ood data. these noise samples do not actually belong to any cluster/category and are some outliers. we leave the noise ood issue to the future work. (3) our experiments in appendix 6 find that the performance of the dpl method decreases significantly when the imbalance factor of unlabeled ood data increases. how to improve the performance of gid model on the long tailed unlabeled data is also a problem worthy of attention in the future."
1044,"limitations the limitation of our work includes the following aspects: 1) the instruction-style question which measures the quality of generated texts from different dimensions still needs manual design. although the questions in our experiment have already involved typical dimensions in text summarization, dialogue generation, and data-to-text generation, we admit that it is hard to cover all the dimensions in various nlg tasks. we believe that this is not a severe problem because we can refer to the definition and human annotation instructions (mehri and eskénazi, 2020) of each dimension, which are commonly formulated as questions. we leave the exploration of automatically constructing instruction-style questions for multiple dimensions of nlg evaluation as future work. 2) due to the limitation of computational resources, the largest base model used in our experiment is flan-t5-xl with 3b parameters. since the ability of instruction following is related to the model scale (wei et al., 2022), we leave the exploration of adopting larger instruction-tuned plms such as flan-t5-xxl and opt-iml (iyer et al., 2022) as future work."
1045,"limitations our study is limited in scope, studying only english question-answering data. we also acknowledge that the long-form answers we study are not always factually correct, as they can be outdated (zhang and choi, 2021) or incorrect as they are crawled from web forums (fan et al., 2019). further, our user study is limited in its scale, evaluating 175 instances, and does not carefully study potentially diverging interpretations from annotators of different demographics. we also do not extensively explore all summarization models, such as the extract-and-abstract approaches mentioned in related work."
1046,"limitations the limitations of our method are as follows: • we find that utilizing multi-view representations in the cross-encoder is an effective method for mvd, however, the ranking performance of the cross-encoder may slightly decrease. therefore, it is sub-optimal to directly use the cross-encoder model for entity ranking. • mention detection is the predecessor task of our retrieval model, so our retrieval model will be affected by the error of the mention detection. therefore, designing a joint model of mention detection and entity retrieval is an improvement direction of our method."
1047,"limitations due to the lack of theoretical support, it is challenging for us to formalize an annotation scheme for implicit persona attributes in the current stage, e.g., extracting an implicit triplet (i, like_animal, dogs) from a sentence “every day, i personally take my dogs out for a walk and lend a hand to my neighbors by occasionally taking their furry friends out for a stroll as well”, besides (i, have_pet, dogs). therefore, our personaext is not compatible with the implicit or multiple persona attribute triplet extraction tasks. additionally, our framework did not exploit complementary information from the context of the current utterance for paed. for an input with multiple dialogue utterances, it is hard for our model to match extracted persona triplets with the exact speaker because of the existence of pronouns and more than one speaker in dialogues."
1048,"limitations the core of promptrank lies in calculating the probability of generating the candidate with a designed prompt by the decoder, which is used to rank the candidates. our experiments have shown that the design of the prompt plays a crucial role in determining the performance of the method. while we have manually designed and selected some prompts to achieve state-of-the-art results, the process is time-consuming and may not guarantee an optimal result. to address this limitation, future research could focus on finding ways to automatically search for optimal prompts."
1049,"limitations of relying on lms’ parameters to memorize factual knowledge and to understand what factors affect factual knowledge memorization. our results show that memorization has a strong correlation with entity popularity and that scaling up models on long-tail distributions may only provide marginal improvements. we also demonstrate that non-parametric memories can greatly aid lms on these long-tail distributions, but can also mislead lms on questions about well-known entities, as powerful lms have already memorized them in their parameters. based on those findings, we devise simple-yet-effective adaptive retrieval, which only retrieves when necessary, using a heuristic based on entity popularity and relationship types. our experimental results show that this method is not only more powerful than lms or previous retrieval-augmented lms but also more efficient. limitations this work focuses on entity-centric factual knowledge and demonstrates that lms’ memorization is heavily affected by the popularity of the entities and the aspect of the entities being asked in the questions. it is important to emphasize that for running controlled experiments, we have relied on two synthetic datasets, and the extent to which our results apply to naturally occurring factual knowledge has not been firmly established. while we can be fairly confident about the relationship between scaling, retrieval, popularity, relationship type, and performance for the kinds of knowledge studied here, the effectiveness of adaptive retrieval will depend on many details of the question answering pipeline. moreover, our work depends on a definition of popularity that is time-dependent and may not perfectly reflect how frequently entities are discussed on the web. wikipedia page views are one possible definition of popularity for which we observe our results, and we invite others to improve upon it in future work. further research can expand upon this simple approach, perhaps drawing on insights from kadavath et al. (2022) to improve the effectiveness of adaptive retrieval. it is an open question if the same findings are applicable to other types of world knowledge such as commonsense. we conjecture that the concept of the subject topic (entity), as well as the aspect (relationship type), can be applied with some minor modifications, which future work can quantify memorization following our scheme. ethical considerations recent work (huang et al., 2022) shows that lms memorize personal information available on the web, which has significant security issues. our evaluation focuses on the memorization of general entity-centric knowledge, but our findings can be applicable to those areas. our findings suggest that lms are likely to have less reliable knowledge of minority groups. parrish et al. (2022) established that models often rely on stereotypes to answer in uncertain cases, so our results indicate that lms are likely to rely on stereotypes disproportionately for minority groups. future work could investigate whether retrieval augmentation reduces bias in these cases."
1050,"limitations though we include 6 systems in our annotation which reflect the current state-of-the-art, all of the models are transformer-based and fine-tuned on just the cochrane dataset, which may limit the diversity of our generated summaries. additionally, none of the systems are generating summaries that approach the accuracy of human-written summaries. as a consequence, though the summaries in our dataset span the spectrum of quality, they may have less coverage on the higher end of quality (summaries approaching the accuracy and utility of human-written review summaries). our analysis of evaluation metrics also assumes the existence of reference summaries. in many real-world summarization scenarios, reference summaries do not exist, and reference-free evaluation metrics are needed for assessment. we refer the reader to related work in reference-free summarization evaluation (vasilyev et al., 2020; gao et al., 2020; luo et al., 2022), which have been found in some settings by fabbri et al. (2021) to exhibit even lower correlation with human notions of summary quality; the performance of these metrics on mslr evaluation is unknown and is left to future work. our notions of summary quality also do not necessarily correspond to clinical utility. as with anything in the medical setting, it is of utmost importance to verify correctness and the quality of evidence before using any generated text to make or guide clinical decisions. ethical considerations as with other applications of nlp in the medical domain, results of mslr systems must be verified by domain experts before they should be considered for use in clinical guidance. we do not intend the system outputs included in our dataset and analysis to be used for such end applications, as this would be clearly premature given the low quality of generated summaries and our lack of ability to assess the prevalence of factuality errors in these summary texts. nonetheless, we believe that medical mds holds eventual promise, and it is of vital importance that we study its challenges and how to measure and detect quality issues in generated text."
1051,"limitations in this work, we generate diverse responses through large-scale sampling in the oversampling stage. although we use the compression and distillation models to speed up, the problem of generation speed still exists. thus, one of the limitations of this work is the additional time cost when generating large-scale candidate responses. in addition, we use existing dialogue models for dialogue generation, mainly used in short text generation and unsuitable for long text generation, which is another limitation of this work."
1052,"limitations our research focuses on the adversarial attack itself and provides a framework that can be potentially used in different adversarial training strategies. we limit ourselves on attacks in this work, but it would be interesting to investigate logic-based attacks in adversarial training. we will leave that as future work. the proposed attack approach is also limited by the limitations of natural logic, while the latter has been a classical logic mechanism. for example, our proposed framework has less deductive power than first-order logic. it cannot construct attacks building on inference rules like modus ponens, modus tollens, and disjunction elimination. as discussed in the paper, some components of the generation and quality control process can be further enhanced."
1053,"limitations exist (discussed in §7.3), compared to traditional approaches the models trained in this study are expected to reduce biases. their value is not limited to predicting dates for individual manuscripts, but they can be applied to any attribute of a group of papyri, e.g. the place of provenance or the text’s type. at the same time, easily accessible open-source metadata exist for most published papyri (§3.1)."
1054,"limitations ircot relies on the base lm to have a zero or few-shot cot-generation ability. while this is commonly available in large lms (over 100b), it’s not as common for small lms (under 20b), which to some extent limits ircot adoptability. given the recent surge of interest (tay et al., 2023; magister et al., 2022; ho et al., 2022), however, smaller lms will likely increasingly acquire such ability, making ircot compatible with many more lms. ircot also relies on the base lm to support long inputs as multiple retrieved paragraphs need to fit in the lm’s input, in addition to at least a few demonstrations of qa or cot with paragraphs. this was supported by the models we used as code-davinci-002 (gpt3) allows 8k tokens and flan-t5-* uses relative position embeddings making it as extensible as the gpu memory constraints allow. future work can explore strategies to rerank and select the retrieved paragraphs instead of passing all of them to the lm to alleviate the need for the lm to support long input. the performance gain of ircot retriever and qa (over oner and zeror baselines) come with an additional computational cost. this is because ircot makes a separate call to an (l)lm for each sentence of cot. future work can focus on, for instance, dynamically deciding when to retrieve more information and when to perform additional reasoning with the current information. lastly, a portion of our experiments was carried out using a commercial llm api from openai (code-davinci-002). this model was deprecated by openai after our submission making the reproduction of these experiments challenging despite our best efforts, just like any other work using such apis. the trends discussed in the paper (ircot > oner > nor), we believe, would still hold. additionally, all our experiments using flan-t5-*, which exhibit similar trends as that of gpt3, will remain reproducible, thanks to its publicly available model weights. ethical considerations language models are known to hallucinate incorrect and potentially biased information. this is especially problematic when the questions asked to it are of a sensitive nature. while retrievalaugmented approaches such as ours are expected to alleviate this issue to some extent by grounding generation in external text, this by no means solves the problem of generating biased or offensive statements. appropriate care should thus be taken if deploying such systems in user-facing applications. all the datasets and models used in this work are publicly available with permissible licenses. hotpotqa has cc by-sa 4.0 license15, 2wikimultihopqa has apache-2.0 license16, musique and iirc have cc by 4.0 license17, and flan-t5-* models have apache-2.0 license."
1055,"limitations of the conventional fact retrieval pipeline, usually consisting of entity mention detection, entity disambiguation and relation classification, which not only requires additional labels for training each subcomponent but also is vulnerable to the error propagation across submodules. to this end, we proposed the extremely simple direct fact retrieval (difar) framework. during training, it requires only pairs of input texts and relevant triplets, while, in inference, it directly retrieves relevant triplets based on their representational similarities to the given query. further, to calibrate the ranks of retrieved triplets, we proposed to use a reranker. we demonstrated that our difar outperforms existing fact retrieval baselines despite its great simplicity, but also ours with the reranking strategy significantly improves the performances; for the first time, we revealed that fact retrieval can be easily yet effectively done. we believe our work paves new avenues for fact retrieval, which leads to various follow-up work. limitations in this section, we faithfully discuss the current limitations and potential avenues for future research. first of all, while one advantage of our direct fact retrieval (difar) is its simplicity, this model architecture is arguably simple and might be less effective in handling very complex queries (sen et al., 2022). for example, as shown in figure 2, even though our difar framework can handle the input queries demanding multi-hop retrieval, the performances on such queries are far from perfect. therefore, future work may improve difar by including more advanced techniques, for example, further traversing over the kg based on the retrieved facts from our difar. also, while we use only the text-based similarities between queries and triplets with lms, it is interesting to model triplets over kgs based on their graph structures and blend their representations with representations from lms to generate more effective search space. also, we focus on retrieval datasets in english. here we would like to note that, in fact retrieval, most datasets are annotated in english, and, based on this, most existing work evaluates model performances on english samples. however, handling samples in various languages is an important yet challenging problem, and, as future work, one may extend our difar to multilingual settings."
1056,"limitations in this work, we have only analyzed the common errors of two models (i.e., gpt-j and chatgpt) in the instructional dialogue task. one open question is whether other gpt-based models or models with other architectures (e.g., encoder-decoder models) also have the same issue in this task. our work and dataset are also limited to the english language. ethical considerations to collect recipe-grounded conversations we hired crowd workers using the prolific platform.6 the study was conducted with the approval of our local irb. the compensation was derived based on prolific’s payment principles. we estimate the hourly pay for crowd workers was $15.49 (details in appendix d). crowd workers were strictly asked not to write any offensive content or personal information. 6https://www.prolific.co/"
1057,"limitations in our proposed model, we introduce a hyperparameter n as the number of event proxy nodes. the value of n needs to be pre-set. setting n to a value larger than the actual event number in a document would lead to computational redundancy as more proxy nodes would be mapped to the null event. however, setting n to a small value may miss some events in a document. we have experimented with automatically learning the value of n based on an input document in procnet. but we did not observe improved event extraction performance. as such, we simply set it to 16. in the chfinann dataset, 98% documents have less than 7 events annotated. this results in the learning of many redundant proxy nodes for such documents. it remains an open challenge on automatically learning a varying number of event proxy nodes based on an input document. reducing the number of redundant proxy nodes can reduce training time further. another shortcoming is the limited capability of procnet in capturing the long-term dependencies of sentences, as have been discussed in the perevent-type results in section 4.2 and 4.3. we observed a relatively worse performance of procnet in dealing with long documents with more than 40 sentences as it does not explicitly model the interrelations of sentences. one possible direction is to explore the use of a heterogeneous graph which additionally models the entity-entity, entity-sentence, and sentence-sentence relations. we will leave it as the future work to study the trade-off between event extraction performance and training efficiency."
1058,"limitation, we report the results of jddc/sr, jddc/d-sts, ic and senti."
1059,"limitations although the proposed method achieves exciting results, there are still some issues that need to be addressed in the future: (1) when designing the structure of hssa layers, we assume that humans tend to understand a dialogue from the local to global perspective, which supports the existence of inner- and inter-segment self-attention layers. (2) we use 2 public chinese corpora, jddc and edc, for post-training. though there are diverse topics in them, it is desired to introduce other corpora from different domains and languages. (3) ssl tasks are arranged in post-training via cmtl (sun et al., 2020) based on the intuitive understanding of their semantic levels and difficulties. therefore, to combine the power of each ssl task more effectively, new training strategies need to be explored."
1060,"limitation of detoxified language modeling, which cannot be avoided unless the provided prompts are rephrased into non-toxic prompts while maintaining their semantic meaning. in addition to developing a safe lms, it is essential to address the issue of lm hallucination, which refers to the generation of factually incorrect texts. while our paper does not focus on this aspect, ensuring both safety and factual valid generation of texts is vital for real-world applications of lms."
1061,"limitations: (1) the datasets used in this work are mostly collected from social media. in the future, we plan to collect sarcastic texts from various sources, such as the literature and films, and conduct more experiments with them. (2) our exploration of sarcasm theories still has some space to improve. though the incongruity theory is the mainstream in the community, there are other theories worthy to investigate in the future."
1062,"limitations we state the limitations of this work from the following aspects. first, we make an initial assumption about the dynamics between exercise difficulty, vocabulary, and student knowledge. while we believe our assumption is sensible in the domain of language learning, we acknowledge that we make some simplifications for the ease of modeling. for example, we measure difficulty using individual performance, whereas a better way could be combining it with inherent problem difficulty, e.g., text complexity. besides, we only consider vocabulary mastery in defining student knowledge and predicting their performance. exploring more dimensions of language knowledge (e.g., syntax) might lead to a finer-grained personalization. second, our model relies on student learning logs to estimate their realtime knowledge states. this model might face the cold start problem when dealing with insufficient history. though it is beyond the scope of this study, techniques like computerized adaptive testing can be used to combat this problem. lastly, due to the lack of a real learning environment, we discuss the educational promise of our model with simulation experiments. in the future, a user study can be incorporated to validate our"
1063,"limitations study scope. while our study only considers three papers, this is by design. as our goal is to study user experience, by fixing papers to be within a specific topic area and time requirement, and having people with different skill levels reproduce the same papers, this allows us to have sufficient samples to understand general behaviors. it also blocks other nuance factors (e.g., introduced by different papers) on beginners’ experience. each of our selected papers presented students with unique reproducibility barriers, and consequently resulted in a wealth of helpful insights. furthermore, finding reproducible nlp papers that satisfied our constraints (as laid out in section 3.1.2) was surprisingly difficult, with only 3 out of 24 considered papers found to be reproducible within our constraints. nevertheless, this study is still on the small scale. engaging a larger community in a large scale study may provide additional insight. related to this, our study only includes a population of mostly graduate students at our university. considering beginners from different educational backgrounds or regions could reveal more comprehensive insights, and we greatly encourage future efforts at a community level toward better understanding the needs of nlp beginners. gpu runtime calculation. it is also worth noting that it is difficult to consistently calculate runtime (as introduced in section 3.2.1) of code on gpu hardware, as fluctuations may occur due to a number of factors, including the specific gpu hardware allocated to a student,27 driver versions, and 26https://arc.umich.edu/ 27while experts used nvidia tesla v100 gpus with up to 16gb memory to reproduce results, nvidia a40 gpus with up to 48gb memory are also available within the cluster. file systems experiments were run with. to minimize the impact of such issues, we chose to reproduce experiments that used small models and had shorter expected runtimes. given that we observed runtimes up to several times larger than expert runtimes, we thus expect that trial and error in setting up experiments accounted for most fluctuation in observed runtimes."
1064,"limitations, we leave exploiting the rewriting model to future work. in table 2 and fig. 5 we demonstrated that our question rephrasing model works well for producing fluent questions that reduce ambiguity. furthermore, in table 3 we showed that the model’s representations contain information about the underlying question being asked, even though this information is not directly present in the training data and we do not include any supervision from our dataset. future work could examine utilizing the rephrasing model in a search-engine environment, where users are actively querying about images. given an ambiguous question identified and a set of answers to it from a vqa model, our model could be used to rephrase the question according to each answer. just as a presenter will often rephrase a question from the audience, the model might present the user with the rephrased question it is actually answering, which would result in better interpretability. this improved interpretability might teach users how to interact with the model."
1065,"limitations currently, to deal with spelling, missing, redundant character errors in chinese text, we jointly pre-train two sub-tasks based on a masked language model with task-specific attention mechanism and utilize re-tagging rules to reformulate the length of text during prediction. the proposed model might be less effective in more complex scenarios: word-level case according to the structure of our model, it could theoretically handle errors that are not limited to character-level, such as redundant or missing words. however, the currently used self-attention matrix works between tokens instead of spans of tokens. a novel attention mask strategy might be considered. if the problem is solved, then our model would be able to handle both chinese spelling correction (csc) and some kinds of grammatical error correction (gec) tasks at the same time. task-specific backbone case the backbone of the proposed model is bert that is not taskspecific, while some errors in sighan happened in entities that might need priori knowledge to solve. for example, the correct sentence is “我 要跟我的朋友去师大夜市” and the wrong sentence is “我要跟我的朋友去市大夜市”, where the mispelled character belonging to an entity “师 大” that is abbreviation of “师范大学” (means “normal university”). to improve the performance of our model in more complicated applications, backbones that learn more task-specific knowledge should be considered. languages mixture case in real world ocr or asr applications, a chinese character might be confused not only by another chinese character, but also by an english character due to their similar pronunciation or shape. for example, the chinese character “丁” is visually similar to the english capital letter “j”, while the chinese “喂” (means “hi”) is phoneticly similar to the english word “way”. furthermore, a same character of simplified chinese and traditional chinese might be visually different. high efficiency case industrial applications often require the prediction time in milliseconds-level under controlled usage of gpus, which would bring troubles to large models. distillation or truncating strategies might be a way to improve the proposed model."
1066,"limitations the primary limitations concern the dataset we created, which serves as the foundation of our findings. our dataset suffers from four key limitations: reliance on papers with code our system is trained and evaluated to retrieve datasets from papers with code datasets (pwc). unfortunately, pwc is not exhaustive. several queries in our test set corresponded to datasets that are not in pwc, such as iwslt 2014 (birch et al., 2014), pascal voc 2010 (everingham et al., 2010), and chime4 (vincent et al., 2017). papers with code datasets also skews the publication year of papers used in the datafinder dataset towards the present (the median years of papers in our train and test set are 2018 and 2017, respectively). for the most part, pwc only includes datasets used by another paper listed in papers with code, leading to the systematic omission of datasets seldom used today. popular dataset bias in the test set our test set is derived from the scirex corpus (jain et al., 2020). this corpus is biased towards popular or influential works: the median number of citations of a paper in scirex is 129, compared to 19 for any computer science paper in s2orc. the queries in our test set are therefore more likely to describe mainstream ideas in popular subields of ai. automatic tagging our training data is generated automatically using a list of canonical dataset names from papers with code. this tagger mislabels papers where a dataset is used but never referred to by one of these canonical names (e.g. nonstandard abbreviations or capitalizations). therefore, our training data is noisy and imperfect. queries in english only all queries in our training and test datasets were in english. therefore, these datasets only support the development of dataset recommendation systems for englishlanguage users. this is a serious limitation, as ai research is increasingly done in languages other english, such as chinese (chou, 2022)."
1067,"limitations we see four main limitations regarding our work. first, we have evaluated our models on a dataset containing events of one type only. it remains to be seen how applicable our formulation and methods are to other historical datasets and event types. second, given the nature of the historical question our dataset targets, it contains documents only from one language family. extending our methodology to languages from other language families might pose further challenges in terms of multilinguality. third, our method relies heavily on automatic translation tools, which are biased toward translating historical texts into modern language. this can negatively affect the performance of our models. lastly, in real-life cases, machine readable historical texts are often extremely noisy, suffering from high level of ocr errors and other text extraction mistakes. conversely, we have tested our methods on relatively clean datasets, with the unannotated dutch material as the only exception. we leave a more thorough study on how well our proposed methods are suitable for noisy text to future work. ethical considerations studying texts about the history of slavery poses ethical issues to historians and computer scientists alike since people of color still suffer consequences of this history in the present, not least because of lingering racist language (alim et al., 2016, 2020). as researchers, we know that an important ethical task is to develop sound nlp tools that can aid in the examination of historical texts containing racist language, while endeavoring at all costs not to reproduce or perpetuate such racist language through the very tools we develop. the enslaved people described in the newspapers adverts used in this study were alive centuries ago, so any immediate issues related to their privacy and personal data protection do not apply. nonetheless, the newspaper adverts studied here were posted by the oppressors of the people who tried to liberate themselves, and contain many examples of highly racist and demeaning language."
1068,"limitations our approach is only a small step towards mining more comprehensive, high-quality topic structures, and there are many more issues that need to be addressed in the future. for example, there are still limitations in the current assessment of the structure of topics mined by different models. examples include assessing the validity of topic hierarchical indicators by topic specialization and the validity of the symmetric structure of topics through clustering as we have demonstrated. all these assessment methods are only a sideways demonstration of the interpretability of the topic structure. besides, there is still a lot of a prior information available in the field of topic modelling, e.g. wordnet, and it may help researchers to explore further in the field of topic modelling if they can combine prior human knowledge and information on topic-words obtained from models to define quantitative metrics that are more consistent with human understanding."
1069,"limitation of token dropping in accelerating language model training. based on a series of preliminary analyses, we find that removing parts of tokens would lead to a semantic loss problem, which causes vulnerable and unstable training. furthermore, experiments show such a semantic loss will hinder the performance of token dropping in most semanticintense scenarios. to address this limitation, we improve token dropping with a novel semanticconsistent learning algorithm. it designs two semantic constraints to encourage models to preserve semantic information. experiments show that our approach consistently and significantly improves downstream performance across all task types and model architectures. in-depth analyses prove that our approach indeed alleviates the problem, and further improves training efficiency. in future work, we will explore the effectiveness of our method on more advanced discriminative language models (he et al., 2020; zhong et al., 2023b). also, it will be interesting to revisit and address the semantic loss problem in efficient training methods for generative language models (such as gpt3 (brown et al., 2020)). limitations our work has several potential limitations. first, given the limited computational budget, we only validate our sctd on the large and base sizes of bert models. it will be more convincing if scaling up to the larger model size and applying sctd to more cutting-edge model architectures. on the other hand, besides the downstream performance, we believe that there are still other properties, e.g., generalization and robustness, of mlms that can be improved by our sctd approach, which are not fully explored in this work."
1070,"limitations one of the primary limitations of this work is that this is essentially an empirical study. although we provide extensive experiments to show that the proposed approach demonstrates significantly better results in different settings, currently we do not provide any theoretical guarantees for this approach. second, many of our experiments would not be easily reproduced in languages other than english, that lack sufficient linguistic resources. during this study we used the gpt-2 and gpt-neo language models, which have been trained on large amounts of english text. finally, anecdotally we observed that this approach can also increase hallucination behaviors, which are a common issue with many text generation models. during application, one would have to take necessary measures to monitor the hallucinations produced by the model."
1071,"limitations this work applies to languages that have a modest amount of data in parallel with english and are represented in pre-trained language models. these are typically high to mid-resource languages. very low-resource languages might not have enough parallel corpora to extract sufficient ner training data. with limited parallel data and/or limited representation in pre-trained lms, it will be difficult to get high-quality word alignments for projection. we use span-based annotation projection to alleviate word alignment errors to some extent."
1072,"limitations inherent debatability in false presuppositions. as discussed earlier, the validity of presupposition is inherently debatable and largely depends on the background context, i.e., even experts in formal semantics and pragmatics observe a high disagreement rate (jeretic et al., 2020). our proposal in using the most upvoted comments partially address the issue, but not perfectly, as discussed extensively in section 5.2. one avenue for future work is to consider extra-linguistic context such as individuals background when judging the validity of presuppositions (zhang and choi, 2021). evaluating massive language models. massive language models such as gpt-3 (brown et al., 2020) have been shown impressive performance in open-ended question answering (nakano et al., 2021). our paper does not include large-scale, systematic evaluation of such models. instead, we conduct a small-scale case study with gpt-3 text-davinci-002. see appendix d for details. most generations are roughly on the right topic, but often contain information that is factually false and do not precisely answer the question. moreover, they rarely explicitly identify false presupposition and provide corrections, indicating that gpt-3 is far from solving our task. we think future work may explore larger-scale evaluation in a more systematic manner. false presuppositions beyond online forums. the domain of crepe is limited to online forums (reddit). while this choice was made due to the availability of large data and its general domain, we argue that false presuppositions are not specific to such domains. for instance, we find that a similar portion (25%) have false presuppositions on information-seeking questions on nlp research papers posed by nlp experts; see appendix e for details. we think future work can explore creating benchmarks on such domains, as well as studying false presuppositions on a broader set of domains that require domain expertise."
1073,"limitations as the experimental datasets are chinese and the word segmentation tool is employed, some parsing errors may exist. also, the token-token matrix is built on all tokens in each document, resulting in a large-scale matrix and the reduction of model training. all these are the limitations of this paper. nevertheless, if the corpus is english, the first limitation does not exist. also, the spatio-temporal efficiency in table 1 is acceptable. importantly, the experimental results obtained in this paper are based on the limitation, which indicates that it is effective to implement our model according to the segmentation results by syntactic tools."
1074,"limitations since the unified graph is very large, it will take more time to construct the subgraphs before the first training. but after saving these subgraphs, there is no need to rebuild the subgraphs in the subsequent training process. on the other hand, the aligned entities among different kgs is a necessary condition for our proposed framework and otherwise, our model can not conduct knowledge transfer among the given kgs without an alignment model or other techniques."
1075,"limitations in this paper, we focus on efficiently and accurately predicting missing links in kgs using low-dimensional features and binary classifiers. greenkgc can achieve impressive efficiency during the inference stage and can be applied to various platforms with memory constraints because of its superior performance in low-dimensional space. however, the whole training process of greenkgc still requires high-dimensional pre-trained embeddings as initial features. therefore, it may hinder greenkgc from being trained on resourceconstrained platforms from scratch. in addition, the current greenkgc model is proposed under a transductive setting, where we focus on a fixed entity and relation set. the generalizability of the few-shot learning capability on greenkgc is yet to be explored. the above-mentioned two limitations can be addressed by leveraging textual information in kgs. in recent years, text-based kgc models (wang et al., 2022a, 2021a,c), which take advantage of entities’ names and descriptions to obtain features, are more and more popular. we may extend greenkgc using word embeddings from pretrained language models as initial features to overcome the current limitations. in addition, continual learning on the classifiers (mai et al., 2021), which aims at learning new training samples without forgetting the old training samples, i.e. catastrophic forgetting, is also an active research topic. thus, greenkgc can incorporate such techniques to improve its generalizability to new data."
1076,"limitations one limitation of the proposed method is that it does not consider domain-specific information to evaluate informativeness. the phraseness module has access to domain-specific knowledge, which are the phrases that occur in similar contexts, i.e. the references. on the other hand, the informativeness module only employs a domain-general sentence embedding model to measure informativeness of phrases. therefore, the integration of both domain-specific and domain-general information for the evaluation of informativeness may be worth further investigation. another limitation of this work is that we only tested the proposed method on short texts. therefore, it is uncertain of the proposed framework’s performance on long text documents. handling long texts could be significantly more difficult than short text, as long texts contain much more information (can discuss a variety of topics). the final limitation of this work is the absence of experiments on using different sentence embedding models to construct the informativeness module. therefore, it might be useful to explore the impact of different sentence embedding models on keyphrase generation performance. we leave this for future work."
1077,"limitations the current dialogue system is mainly based on deep neural network, like transformer structure, which often requires a large number of data sets for training model. however, there are still some deficiencies in our dataset. we will further label and create more dataset to train model. in addition, in order to improve the quality of dialogue, our model parameters are relatively large, which affect the speed of dialogue generation to some extent. we will explore some methods, such as knowledge distillation, to reduce model parameters to improve the speed of dialogue generation on the premise of keeping the quality of dialogue generation unchanged."
1078,"limitations in this paper, we present a novel knowledge injection paradigm plug-and-play knowledge injection for plms. we show existing methods can not be well applied to the new paradigm and propose maptuning as a preliminary exploration of methods. the paradigm plug-and-play knowledge injection has a limitation in terms of its assumption. it assumes that a plm should be fine-tuned for downstream tasks. however, very large-scale plms can perform zero-shot learning or in-context learning on downstream tasks without being fine-tuned. future work may extend the definition of the proposed paradigm to make it meaningful in these scenes. the method map-tuning has three limitations in terms of its applicability. firstly, we did not evaluate map-tuning for plms pre-trained by other language modeling objectives (e.g., casual language modeling) besides mlm. as its spirit can be easily generalized to various language modeling objectives, we leave this evaluation as future work. secondly, we did not evaluate whether the plm can do complex reasoning (e.g., multi-hop reasoning) based on the knowledge injected by map-tuning. thirdly, map-tuning is designed to plug structural fact knowledge. it is also meaningful to plug other diverse knowledge bases, including text corpora, voice, images, and even other plms, which are not covered by our work."
1079,"limitations to further inspire the follow-up work, we summarize our limitations as follows: 1) we only preliminarily reveal the overthinking phenomenon in the open-world scenario, and explore how to mitigate and utilize it during inference. we do not continue to conduct more in-depth research on the broader forms of overthinking in the open-world scenario and do not explore whether there are differences in its performance in different models. in addition, whether it can be solved or alleviated by other ways, such as training methods. 2) from the results of sections 5.2, 5.3 and the corresponding appendix a, b, it seems that there is room for further improvement in the speedup of our method. we leave how to achieve the best accuracy-speed trade-offs to subsequent research. 3) we have preliminarily verified that our method can be compatible with more detection algorithms and models, and look forward to exploring more methods and models."
1080,"limitations an obvious limitation of our work is the considered search space. although we showed that it is well suited for the data used in practice by the nlp community, this may not hold in more general settings. moreover, we only experiment in english. we suspect that similar results would hold for morphologically-rich languages as we expect, in the latter case, that constituents are shorter (i.e. morphologically-rich languages heavily rely on morphological inflection, so we expect more mentions spanning a single word), see (haspelmath and sims, 2013, section 1.2 and table 1.1). however, this is not guaranteed and future work needs to explore the multilingual setting. finally, in this work we do not consider discontinuous mentions, which is an important setting in real world scenario."
1081,"limitations we have not tested all possible recent methods on lemon. we have used expensive gpu resources to speed up the training process on lemon, with 8 nvidia a100 sheets, but consistent results can also be obtained with 8 v100 sheets. our work focuses on chinese. other languages, such as japanese and korean, could benefit from the same technique, but have not been studied in this work."
1082,"limitations the proposed method has several limitations: 1) the current approach achieves hunky context reasoning performance in the cross-modal scene of a single text clue and image, but the context reasoning capability in the scene containing multiple textual and visual clues still needs to be further explored, such as video and long text. 2) from the experimental results, we observed that the visual prefix length greatly impacts the stability of language models infused with visual information. hence, we still need to explore effective and stable vision-aided language models for natural language processing and multi-modal scenarios. 3) we also hope this work could spark further research on improving the long context reasoning capability of pretrained vision-language models. acknowledge we thank the anonymous reviewers for their constructive comments, and gratefully acknowledge the support of natural science foundation of china (no.62006061, 61872107) and the stable support program for higher education institutions of shenzhen (no.gxwd2020123015542700320200824155011001)."
1083,"limitations although our modeling of event centrality is feasible and effective, there is still space for improvement. the performance of event centrality prediction could be higher by using more advanced encoding methods. besides, it is meaningful to further explore the interactions among various types of event relations. existing datasets only cover limited relation types at once, and many works focus on the identification of causal relations alone. in this paper, although we further consider the effect of coreference relations and perform joint classification, there are still some other relations that can be explored, such as temporal relations, subevent relations, etc."
1084,"limitations in this paper, we present a supervised adversarial contrastive learning (sacl) framework with contextual adversarial training to learn class-spread structured representations for context-dependent emotion classification. however, the framework is somewhat limited by the class imbalance issue, as illustrated in section 4. to more comprehensively evaluate the generalization of sacl, it is necessary to test its transferability in low-resource and out-of-distribution scenarios, and evaluate its performance across a wider range of tasks. additionally, it would be beneficial to explore the theoretical underpinnings and potential applications of the framework in greater depth. the aforementioned limitations will be left for future research."
1085,"limitations one major limitation of our work is that our experiments are only conducted on docred and re-docred that consist of documents from general domain. yet, information extraction has many broader applications in specific domains, e.g. biomedical data. we plan to adapt tag to some biomedical datasets, like cdr (li et al., 2016) and gda (wu et al., 2019), in the future. besides, since tag consists of a number of modules and use plm as encoder, the training process takes relatively more time and computational resources than dedicated docre model that only extract relations. we concern that it may affect the scalability with larger amount of either data or parameters."
1086,"limitations as discussed in appendix b, there is still a gap between the synthetic dialogues and the humanwritten dialogues in terms of quality. the synthetic dialogues sometimes do not express knowledge with sufficient accuracy. also, some of the synthetic dialogues are less coherent and diverse than the human-written ones. we believe that these issues can be mitigated in two aspects. first, similar to (zheng et al., 2022), employing larger lms can help generate utterances with higher quality. second, introducing knowledge graph and textual reasoning techniques to produce better dialogue flows. in addition, using large lms inevitably requires more computational resources. however, it is still a cheaper and promising alternative to hiring expensive labor."
1087,"limitations the limitations of this work can be concluded into two points: (1) to obtain the associations between semantic elements, semsin needs to transform the texts into the corresponding semantic graphs. existing methods can only transform single sentences into semantic graphs, and cannot parse texts containing multiple sentences. therefore, this method is not suitable for identifying causal relations between events in different sentences. (2) semsin only exploits the semantic structures of the texts and does not utilize external knowledge. external knowledge is also important for the eci task, and simultaneously exploiting semantic structures and external knowledge is a good direction for future studies."
1088,"limitations although our proposed knowledge transfer methods work well on wos in the few-shot setting, it is less effective on 5-datasets. moreover, all methods fail in the full-shot setting. based on our approach, a more general approach to knowledge transfer is expected in future works. in addition, our approach requires a well-trained language model for task identification and a transformer-based model (well-trained also) for parameter efficient tuning. therefore, it is challenging to cooperate our approach with a language model with random initialization or non-transformer architecture."
1089,"limitations although the proposed framework yields promising results on two fine-grained emotion datasets— goemotions and empathetic dialogues—there remain limitations, including: (1) to the best of our knowledge, there is no such fine-grained emotion dataset in other languages. although theoretically, our method should work fine on languages other than english, we can only show the results in english. (2) the proposed method works best when the label structure contains hierarchy, especially when the semantics of some labels are close and difficult to distinguish. when the label structure is flat and independent, our method may backoff to a conventional classification model."
1090,"limitations while imaginary concepts are encouraged in stylized visual storytelling task, it would be better if these literary imaginations are more related to visual contents. in order to improve semantic relevance, we could restrain models from generating visually unrelated descriptions, or make pseudo images more related to stylized stories. however, the former solution is likely to harm the style expression by decreasing stylistic imaginations. for the latter scheme, we have tried to generate pseudo visual inputs with pre-trained text2image model (ramesh et al., 2022), however, there is a domain gap between photos in vist and images generated with stylized sentences. it would be a challenging and interesting problem to be explored in the future."
1091,"limitations although wspalign successfully outperforms all existing baselines, it is still limited to the accessibility of low-resource language information. for example, the collection of pre-training data requires multi-lingual pos tagging tools to identify which words are common or not. it also requires a multilingual plm and wikipedia hyperlinks to make the alignments, which could be inaccessible for an exceptional minority language. but note that we showed wspalign’s cross-lingual ability in §5.1, which implies that this issue can potentially be addressed in the direction of pre-training on large-scale monolingual data with our future effort. besides, this paper lacks evaluation on real low-resource language benchmarks because there is no existing test set. we will try to collect and annotate low-resource word alignment data in our future work."
1092,"limitations this paper fundamentally considers a scenario in which practitioners rent cloud gpus. in the case of hosting gpus by themselves, the two strategies explored in this study would not be simply comparable. however, in practice, when training a large model (w/ 8 a100 gpus), we conjecture that renting gpus could be preferred in many cases as scaling compute powers is not trivial and prohibitively expensive (izsak et al., 2021; obandoceron and castro, 2021; minixhofer et al., 2022). it is also noteworthy that in the future, computational costs may become cheaper as new hardware advances, the pricing policy by cloud platform services changes, and more optimization techniques are applied. on the other hand, human annotation cost is likely to be the same at least or even more expensive. with cost changes in such a direction, the same"
1093,"limitations in this section, we would like to discuss two limitations of od-rte as follows: (1) in the current table-filling based rte methods including od-rte, the issue of sparse labels in the tables still exists. as cells of the table, the number of positive and negative token pairs is grossly unbalanced. in this work, although we alleviate the problem of unbalanced positive and negative relations by introducing the relation negative sampling strategy, the problem of unbalanced positive and negative token pairs still exists and needs to be addressed. we will try to mitigate the problem in our future work. (2) currently, od-rte can only be applied to the relational triple extraction task. in recent years, the table-filling-based approaches have been widely used for many information extraction tasks besides the rte task, such as opinion mining (wu et al., 2020) and named entity recognition (li et al., 2022). therefore, in future work, we will try to extend the object detection framework to other information extraction tasks to let the model make full use of the information of entity boundaries."
1094,"limitations in this work, we demonstrate the effectiveness of the proposed mp2 with the backbone ptm of cpt-large on a set of chinese nlp tasks. due to the expensive pre-training cost, we did not explore mp2 on other ptms with varying sizes, pretraining objectives and architectures. besides, it is also unknown how does the number of pre-training tasks affect the performance of mp2. for resourcerich languages such as english and chinese, it would be promising for mp2 to be well-performed since one can easily collect sufficient public upstream tasks. nevertheless, for low-resource languages or domains, the effect of mp2 is still underexplored."
1095,"limitations our system employs a modified sequence-tosequence architecture to implement the response generator. since the length of dialogue context increases as the dialogue continues, the generator needs to input multiple long dialogue contexts to the encoder simultaneously, each for a retrieved entity. this may cause redundancy in the input and lowers the proportion of kb-related information. we will explore more efficient architectures for the response generator in future work."
1096,"limitations our analyses are based on models with t5-like architectures and span denoising training objectives. thus, our findings may not generalize to other types of encoder-decoder models (e.g., bart), nor encoder-only and decoder-only models. we believe this is unlikely, given that similar findings have been shown for models with architectures and objectives that differ significantly from t5’s (huebner et al., 2021; warstadt and bowman, 2020). nonetheless, it cannot be ruled out. our analyses are also based entirely in english, and only leverage two syntactic transformations. it is possible that our findings will not generalize to other languages, given that certain grammatical features (e.g., more extensive case marking) induce more syntax-sensitive behavior given a similar amount of training data across languages (mueller et al., 2020; ravfogel et al., 2019); thus, perhaps less wikipedia or c4 data is needed in these languages for models to acquire hierarchical preferences. it is also possible that, within a language, a model could adopt a hierarchical inductive bias for one type of transformation, but not another— especially if one transformation is much more frequent than the other. indeed, the frequency of particular words positively correlates with syntactic evaluation accuracies (wei et al., 2021; newman et al., 2021), and it would be reasonable to expect a similar trend for the frequency of syntactic transformations. thus, future work should investigate more transformations in more languages to ensure that these findings are consistent."
1097,"limitations on western-centricity the majority of the crowdworkers producing the source data (δ-social and delphi) and δ-clarify were located in the united states. due to this, the predictions generated by clarifydelphi are currently limited to representing only the perspectives of western culture (particularly the united states). overcoming the western-centric bias is a compelling direction for future research. on defeasibility we rely upon delphi to produce acceptable judgments given a situation and the modifying context as a measure of defeasibility. we recognize that, however, delphi is not perfect and is characterized by a variety of limitations such as limited cultural awareness and inconsistent predictions (jiang et al., 2022). investigating improved methods for identifying answer divergences that will better capture defeasibility is a topic for future investigation."
1098,"limitations while promising, hint comes with several drawbacks related to its ease of use. first, hint takes advantage of the fact that (a) instructions are often long, and (b) often we want to perform inference over a larger (> 100) amount of examples with the same instruction. if either of these items are not true in a setup, then hint is unlikely to provide a large benefit over simply including the instruction with the input text. this can be seen in the smaller compute savings provided by hint for p3 in table 2. second, while hint is computeefficient at inference time, it is far more costly to train, as it effectively requires running the underlying model together with the hypernetwork for every batch. this means that while hint may be useful for practitioners with limited compute budgets, it may be difficult to train hint models with the same limited budget. finally, we train and test on english data only, and do not explore the generalisation of our approach to multilingual setups. considering the success of hypernetworks in multilingual settings (platanios et al., 2018; baziotis et al., 2022; ustun et al., 2022), we believe this is a promising direction for future research. as such, while promising, hint is limited by certain assumptions made about the length and format of instruction-augmented data, and we hope further improvements of the method work towards loosening these assumptions."
1099,"limitations model coverage. our study is targeted specifically at gpt-3 and it would be interesting to study feature bias patterns on other large language models such as opt (zhang et al., 2022) and bloom (scao et al., 2022); and it is possible that our intervention methods may have different effects on these language models trained with different data sources and scales. task coverage. apart from model coverage, our analysis is focused on only four common binary classification tasks. our main metric, h-accuracy, compares the predictions between a learned function f and a feature function h. for simplicity, we only study binary functions (consistent with prior work) to illustrate the main ideas, but the framework applies equally well if f and h are multi-class classifiers. for example, in the case of the threeway nli task, we might set h1 to predict on the basis of entailment / contradiction / neutral, and h2 to predict on the basis of the genres – e.g. fiction / government / telephone. future work could extend our framework to more tasks, and consider how to apply it to more complex tasks such as generation. feature coverage. our current experiments are limited to a set of hand-crafted features. one potential way to systematically scale our approach is to develop novel techniques for automatic feature discovery, for example, to cluster the data and treat each cluster as having a distinct feature. explaining feature biases. while our empirical findings shed light on the feature bias patterns of gpt-3, we do not yet have a"
1100,"limitations our proposed taxonomy is subject to extension, and we expect new phenomena to be included into its scope as the field progresses and as more document sources are considered. using a taxonomy as an organizational basis for the proposed schema is dictated by our aim to keep the schema simple. the design of future, formalized reporting schemata might adopt an onthology-based approach as it affords more flexibility, and take into account interoperability with the existing proposals in the linked open data community (hellmann et al., 2013). while source analysis is only one of our contributions and is thus limited in scope, we have observed that increasing the number of documents from the same source yields diminishing value: if a source uses a certain non-linguistic textual element, it does so consistently. this suggests that the future qualitative studies of document sources used in nlp should be conducted in a breadthfirst fashion, with few documents samples from many sources, unless quantitative measurement is desired (e.g. ""how often do wikipedia authors use text formatting"") or unless a source is known to accommodate a wide variety of document types with different publication and formatting standards. we do not provide specific details on documenting the text production environment, which represents a promising future research avenue. the study of how the texts in nlp are created is a critical research direction: due to the increased applied use of pre-trained generative language models, documenting the text form and origin is a pressing need. our"
1101,"limitations our work mainly suffers from two key limitations. 1) ignore that the text embedded in the image could also reflect the sarcastic intention. as mentioned previously, we found that our model performs better on non-ocr samples than the ocr samples. this may be due to the fact that our model ignores the text embedded in the image. nevertheless, such embedded text could also indicate the ironic intention, (see figure 3 (a)). we believe recognizing the text of the image can boost the performance of existing multimodal sarcasm explanation models. 2) ignore that different knowledge concepts may contribute differently to the sarcasm reasoning. as shown in figure 3 (b), the related concepts “disgusting” and “pleasant” should contribute more than the concept “night” in the sarcasm reasoning. currently, our model equally treats all the knowledge concepts."
1102,"limitations the limitations of the paper are twofold. first, we need to train the annotators to be familiar with another annotation paradigm: creating counterfactual samples for the labeled factual samples. it is an additional cost for active learning although our user study has shown that annotating counterfactual samples has similar costs to labeling factual samples. second, we require the annotators to manually find and edit the causal features, which is not effective enough. it can be improved by developing tools like generative models to automatically edit features for annotator judgment."
1103,"limitation. the main drawback of our data creation protocol is that the question/answer pairs were generated automatically, leading the question distribution to be artificial from a semantic perspective. in addition, the kg adopted in the research focuses on a single event domain, and extending the dataset to multiple domains is planned as future work."
1104,"limitations our work presents a new dataset based on text data scraped from the internet. hence, the quality of the text depends on the quality of the available websites. most of our data stems from the three websites apo, koe and mdr providing a rich vocabulary in our corpus. while this vocabulary covers a variety of mixed topics, we cannot rule out any negative side effects of data imbalance. moreover, our dataset can only represent topics that were considered relevant to be translated into simple german by the respective website. in section 6.2 we presented the different guis that we used to either manually align the sentence pairs or evaluate a sample of sentence alignments. one drawback of the tool for the second evaluation method is that it focuses solely on the matched sentences and presents them isolated from their contexts. one can argue that evaluators using the tool would have to see the context in which the sentences appear in order to correctly classify partial matches. also, providing more information to the annotators might enable them to also correctly classify additional explanatory sentences. future work and use cases our corpus comprises data in ls and es, two types of simple german. a higher granularity of language difficulty could be achieved by incorporating texts originally directed at language learners that are rated, e.g. according to the european reference system (council of europe, 2020). our work presents a parallel corpus for german and simple german and should be continuously expanded. not only to increase its size, but mainly to increase the number of topics covered in the corpus. yet, as there are no efforts to start a single big corpus like a simple german wikipedia, web scraping from various sources stays the method of choice for the future. an additional option is to compute sentence alignments for existing article aligned corpora to include them in the dataset (e.g. battisti et al., 2020). as for the sentence alignment algorithms, various extensions are imaginable. firstly, it might be interesting to allow one simple german sentence to be matched to multiple german sentences. also, the assumption of the mst-lis about the order of information is very strong, and recall might be improved by softening this assumption, e.g. by allowing matches that are at most n sentences away. other alignment algorithms that impose different biases on sentence order (barzilay and elhadad, 2003; jiang et al., 2020; zhang and lapata, 2017) are interesting for further extensions. our dataset can be used to train (or fine tune) automatic text simplification systems (e.g. xue et al., 2021) which then should produce text with properties of simple german. direct use cases for such simplification systems are support systems for human translators or browser plugins to simplify web pages. further research has shown that text simplification as a pre-processing step may increase performance in downstream natural language processing tasks such as information extraction (niklaus et al., 2016), relation extraction (van et al., 2021), or machine translation (stajner and popovic, 2016). it remains an interesting direction for future research if simple german can help to further increase performance on such tasks."
1105,"limitations the open information extraction methods may amplify the bias of the corpus by extracting any relation occurring in the data. the models with deep learning may learn the relation bias from the training corpus and extract those biased statements. to mitigate the effect of data bias, we try to balance the relations in constrained tuples and the ratio of constraints when constructing the cteb dataset. in addition, the utilization of external auxiliary information increases additional computation time. our ian model has still achieved superior performance when the external auxiliary information is removed."
1106,"limitations limitations of data collection our proposed dataset only targets english language tasks. future work should explore multimodal instruction tuning in a more diverse language setting and augment our multiinstruct with multi-multilingual tasks. in addition, our current dataset mainly focuses on vision-language tasks. datasets from more diverse modalities should be considered such as audio (panayotov et al., 2015; gemmeke et al., 2017; you et al., 2022) and video (soomro et al., 2012; ionescu et al., 2014). while we have built a novel multimodal instruction dataset containing 62 tasks, the number of tasks and associated instructions remains limited. to address this, future research could consider utilizing crowd-sourcing or automatic generation and augmentation techniques to increase the variety of instructions available. limitations of experiments and evaluation our work is the first to explore instruction tuning on multimodal tasks and shows improved performance compared to baseline methods. however, there is still room for improvement, specifically in utilizing text-only instruction datasets. future research could explore alternative architectures and stronger vision-language pre-trained models, or develop additional training loss functions to better utilize these unimodal instruction datasets. additionally, we only used ofa as the baseline model as it was the largest open-source multimodal pretrained model available when we conducted this research. as more and stronger multimodal pretrained models being publicly available, it would be interesting to conduct a thorough comparison between models with different sizes. finally, we take the first step to define sensitivity as a metric to evaluate the robustness of the models on understanding and following human-written instructions, which can be a potential standard metric for all the following instruction-tuning studies. however, it’s only based on the variation of model performance across different instructions for the same task. in the future, we will consider more broad factors, e.g., the model’s capability to understand different instructions for different tasks (inter-task sensitivity), to further improve the sensitivity metric for instruction tuning."
1107,"limitations we identify the following limitations of our work: longer output sequences while outputting the reasoning path as a single short sequence makes the model more interpretable, it increases the challenge of producing a long /coherent sequence when the question is complex (more than 3 hops). producing a longer sequence also increases the inference time. simplifying this output while not sacrificing interpretability is a good future direction entity identification our method needs wikipedia outlinks or a entity linker to construct a localized graph for every question. generalizing this step by pretraining the model to do entity linking (févry et al., 2020; sun et al., 2021; verga et al., 2020) might eliminate the need to use an external module."
1108,"limitations sample efficiency in ar-lms, an nll loss is computed at training time for every token in the sequence of length l (eq. 4). however, in ssdlm, each time a pretraining example is sampled, the loss is computed on only b tokens (eq. 9) leading to a lower sample efficiency than ar-lm. towards improving this efficiency, future work could explore model architectures dedicated to semiautoregressive diffusion rather than the vanilla transformer encoder we use in this work. decoding speed since each block is generated by refining over several iterations, ssd-lm has a considerably slower decoding speed than autoregressive models. for example, given a context of 50 tokens (single instance, unbatched), it takes ssd-lm 25 seconds to generate the next block of 25 tokens (tdecode=1000). while our work focused on establishing the efficacy of diffusion-based lms and modular controlled generation, future work could explore tuning tdecode to balance model performance and decoding speed, or more efficient training and decoding algorithms extending ideas from prior work on diffusion models for continuous domains (song et al., 2021; nichol and dhariwal, 2021; rombach et al., 2022; meng et al., 2022). decoding block size in this work, although we allow setups where btrain ̸= bdecode, the decoding block size bdecode remains the same across m decoding iterations, leaving space for a more flexible decoding schedule. future work can also explore learning bdecode (and btrain) rather than using constant pre-defined lengths. larger scale experiments with different kinds of controls and their combinations can be done, as well as more sophisticated ways to incorporate them (kumar et al., 2021). in addition, we plan to explore alternative methods to continuously represent and add noise to discrete text (bakosi and ristorcelli, 2013). this work experiments with pretraining data that is primarily in english. future work can also explore challenges and benefits of diffusion-based lms in a multilingual setup."
1109,"limitation one limitation of the mcce models is that the number of candidates during training and inference should be the same, otherwise, the performance drops severely. one simple potential solution is to divide or pad the candidates during inference to match the number of candidates during training. for example, divide 128 candidates into two sets with 64 candidates and apply twice forward passes of a filter model if it is trained on 64 candidates and required to filter 128 candidates during inference. we don’t fully explore the solutions to this limitation and leave it as future work."
1110,"limitations there are a few limitations of our work. first, we focus on evaluating state-of-the-art factuality metrics on english newswire datasets. this setting restricts us to english-language data, a formal style of text, and topics consisting of what is discussed in us and uk-centric news sources. moreover, other summarization domains such as dialogue summarization have different common error types such as wrong reference error (tang et al., 2022), which are not fully evaluated under current metrics. as settings like this are studied in future work, we believe that the kinds of analysis we do here can be extended to these settings as well. second, since our work is built on top of previous work, some analysis such as the error type mapping is limited by the quality and annotation agreement from previous work. we chose not to undertake large-scale reannotation to avoid causing confusion in the literature with multiple versions of datasets reflecting divergent annotator opinions. in spite of these limitations, we believe that our reevaluation of these metrics and the analysis of error types under newswire data can bring insights for future works in choosing, designing and evaluating factuality metrics."
1111,"limitations enabling dialogue agents to join multi-party conversations naturally is undoubtedly a crucial step towards building human-like conversational ai, especially as such technology becomes more affordable and portable. more crucially, research on multi-party conversations has the promising potential to improve the interactive experience between humans and machines. although the proposed method has shown great performance and generalization ability across various models and tasks, however, we never lose the sight of the other side of the coin. the proposed method requires full interactions among utterances in multi- head attention of transformers. therefore, computational complexity and inference latency may be worth considering when deploying to online dialogue systems. aside from the well-known difficulties in deployment, the proposed method was only evaluated on the domain-specific datasets, i.e., ubuntu irc, considering the constraints of dataset resources. in the future, we will try to search more open-domain datasets for multi-party conversations, and test if the proposed method can still show great performance on a more challenging open-domain setting."
1112,"limitations the political compass test in this work, we leveraged the political compass test as a test bed to probe the underlying political leaning of pretrained language models. while the political compass test is a widely adopted and straightforward toolkit, it is far from perfect and has several limitations: 1) in addition to a two-axis political spectrum on social and economic values (eysenck, 1957), there are numerous political science theories (blattberg, 2001; horrell, 2005; diamond and wolf, 2017) that support other ways of categorizing political ideologies. 2) the political compass test focuses heavily on the ideological issues and debates of the western world, while the political landscape is far from homogeneous around the globe. (hudson, 1978) 3) there are several criticisms of the political compass test: unclear scoring schema, libertarian bias, and vague statement formulation (utley, 2001; mitchell, 2007). however, we present a general methodology to probe the political leaning of lms that is compatible with any ideological theories, tests, and questionnaires. we encourage readers to use our approach along with other ideological theories and tests for a more well-rounded evaluation. probing language models for encoder-based language models, our approach of mask in-filling is widely adopted in numerous existing works (petroni et al., 2019; lin et al., 2022). for language generation models, we curate prompts, conduct prompted text generation, and employ a bartbased stance detector for response evaluation. an alternative approach would be to explicitly frame it as a multi-choice question in the prompt, forcing pretrained language models to choose from strong agree, agree, disagree, and strong disagree. these two approaches have their respective pros and cons: our approach is compatible with all lms that support text generation and is more interpretable, while the response mapping and the stance detector could be more subjective and rely on empirical hyperparameter settings; multichoice questions offer direct and unequivocal answers, while being less interpretable and does not work well with lms with fewer parameters such as gpt-2 (radford et al., 2019). fine-grained political leaning analysis in this work, we ""force"" each pretrained lm into its position on a two-dimensional space based on their responses to social and economic issues. however, political leaning could be more fine-grained than two numerical values: being liberal on one issue does not necessarily exclude the possibility of being conservative on another, and vice versa. we leave it to future work on how to achieve a more fine-grained understanding of lm political leaning in a topic- and issue-specific manner."
1113,"limitations there are two known limitations of the squadlike annotation approach we used in this work: (1) it can result in higher lexical-overlap between the context and question pairs. (2) it leads to proportionally fewer truly information-seeking questions (gururangan et al., 2018; kaushik and lipton, 2018). the main reason is that the annotators create questions after reading a paragraph, which can induce bias towards recycling words and phrases observed in the context. our annotation guidelines advise against this, but it is difficult to avoid entirely. several approaches have been proposed to mitigate this issue, such as natural questions (kwiatkowski et al., 2019) and tydiqa (clark et al., 2020). however, they tend to be expensive, and comparatively, the squad-like method is resource efficient and a more suitable starting point for low-resourced languages such as tigrinya. finally, the current dataset does not include adversarial examples to measure the capability of models to abstain from providing an answer when it does not exist in the context; this extension is left for future work."
1114,"limitations there are several limitations to this study that should be considered. first, a key limitation is the lack of a variety of language-specific jad. here, we have four different languages namely en, da, fr, and de. this means that our analysis is based on a limited subset of languages and may not be representative of jad data outside of these four languages. in turn, the second limitation is that the esco taxonomy used as pre-training data only covers europe and the datasets used in this work also covers mostly europe. the results may not be generalizable to other regions. however, we see a slight improvement in the bhola dataset, the data of which comes from singapore, which hints that it could generalize to other cultures. the esco relation prediction task aims for learning the relations between elements of the esco taxonomy. we acknowledge that we do not evaluate the effectiveness of the pre-training objective in relation-centered tasks. unfortunately, to the best of our knowledge, there is no job-related dataset containing relations between skill/occupation concepts to benchmark our model on. we consider this interesting future work. finally, we did not conduct an ablation study on the erp pre-training objective, i.e., which errors it makes. as the accuracy of the objective is 60%, we are unable to determine which sampling method is detrimental to this accuracy. however, we suspect that the linked sampling approach might be the hardest to predict correctly. for example, many occupations have a lot of necessary and optional skills, thus it is harder to determine if some skill truly belongs to a specific occupation. nevertheless, we see that adding the erp objective improves over regular mlm domain-adaptive pre-training. despite these limitations, we believe that this study provides valuable resources and insights into the use of escoxlm-r for analyzing jad and suggests directions for future research. future studies could address the limitations of this study by using a larger, more diverse datasets and by conducting ablation studies on the language model to better understand which parts contribute to the results."
1115,"limitations although the proposed multicapclip can generate multilingual zero-shot visual captions without any labeled vision-caption training pairs. we still need the independent set of text for training/translating, which may still be difficult to collect for some lowresource languages. this might be alleviated in the future with techniques such as knowledge distillation from publicly-available pre-trained models, e.g., bert (devlin et al., 2019). besides, our approach uses clip to measure text-text similarities for retrieving concept prompts and conducting input augmentation during training. considering that clip is optimized by image-text global contrast (radford et al., 2021) and intra-modal retrieval of such a model is not as well as its cross-modal retrieval (jia et al., 2021), an improvement direction of our approach is using a vision-language pre-trained model that measures intra-modal and inter-modal semantic similarities well (yang et al., 2022b)."
1116,"limitations one limitation of our approach is that in-sample curriculum learning methods (both tcl-sg and icl-sc) always incur extra overhead during training compared with the vanilla model shown in table 7. nevertheless, the inference time of different approaches is the same as the vanilla model. in a word, it’s worthwhile because (1) icl-sc can perform significantly better than both baselines without additional computational requirements during inference in real applications; (2) icl-sc doesn’t rely on task-specific expertise and has strong generalization ability. due to the limited computational resources, we were unable to do experiments on machine translation. according to the implementation details in liang et al. (2021), all of their machine translation experiments were done on 32g nvidia v100 gpus which are much more powerful than a single rtx 3090. even for the low resource setting with around 133k to 612k training samples, they used dynamic batching with 4096 maximum tokens and trained for 60 epochs. this will either lead to an out-of-memory error or take us several weeks or even months to get the results of a single run on our machine. instead, we tried our best to cover a range of representative natural language generation tasks and corresponding datasets with different characteristics, such as sizes and output lengths (table 1)."
1117,"limitations since product question answering (pqa) is actually a domain-specific application in general qa, the scope of the problem may be limited. however, in recent years, pqa has received increasing attention in both academy and industry. (1) from the research perspective, pqa exhibits some unique characteristics and thus brings some interesting research challenges as discussed in section 3. for example, some studies use pqa as an entrypoint to analyze the subjectivity in qa tasks. (2) from the application perspective, it has great commercial value. online shopping is playing an increasingly important role in everyone’s daily life, so that many high-tech companies develop ai conversational assistants for promptly solving customer’s online problems, including but not limited to amazon, ebay, alibaba, jd, etc. regarding the large amount of research efforts that have been made, there is not a systematic and comprehensive review about this research topic. similar to recent surveys of other domain-specific qa, such as biomedical qa (jin et al., 2023) and legal qa (gil, 2021), we hope that this paper can serve as a good reference for people working on pqa or beginning to work on pqa, as well as shed some light on future studies on pqa and raise more interests from the community for this topic."
1118,"limitations our work is the first step towards unified pretraining for political actor modeling and it is limited in two aspects. in terms of data, we focus on the typical political actors, i.e., the congress legislators, and their statements, without using a larger corpus like political news. but our method can be easily scaled to a larger corpus, where we can aggregate articles of different media and consider their structure information like page links for pre-training. in terms of method, in order to improve the retrieval efficiency in both pre-training and fine-tuning, we use simple methods rather than dynamic selection methods based on embeddings to query and aggregate statements, leaving much room for future exploration."
1119,"limitations and supervision biases. to address this issue, we propose to jointly train with today and its explanation annotations, resulting in improved performances on multiple temporal reasoning benchmarks, namely tracie (+7%), matres (+3%), and today (+10%). we also demonstrate that today can be used to distill gpt3.5 and automatically generate and filter incidental supervision instances with high-quality explanations, which further improves performances. despite these advances, the gap in performance on today still motivates future work toward generic temporal reasoning. limitations this work initially builds on human annotations, which are relatively expensive compared to simple model generations. due to such cost-related reasons, we do not include neutral contextual changes which are hard to annotate, and do not investigate the potential harms of annotated/generated language, e.g. harmful social biases. throughout this work, we only use rocstories as the source data, more diverse sources are reasonable for future work. we use t5 and gpt-3 architectures; however, there are more powerful architectures that could potentially improve our results. lastly, this work only focuses on generalizing temporal reasoning, which is a challenging yet relatively narrow task for large language models. through pilot experiments, we find that similar task formulation, annotation schemes, and model structures can be applied to other tasks, such as natural language inference (nli) and question answering (qa). a sample from the snli training set (bowman et al., 2015) using our formulation for explanation is shown in table 12 in the appendix."
1120,"limitations while our approach is able to optimise over the retrieved shortlist of replies, it does not improve the initial retrieval from the candidate pool, which still scores individual candidates, rather than reply sets, using the matching model. this is a limitation that is shared with prior baseline methods. a further limitation is that we only consider the monolingual setting, whereas many deployed sr applications have an international footprint. learning a multilingual matching model in sr is known to have additional challenges (deb et al., 2021). another limitation is that our model is only tested on public dialogue datasets, due to actual conversations on platforms using sr being proprietary. therefore, while our techniques should work well in the instant messaging setting, our methods have not been directly tested in the email setting. ethical considerations as neural dialogue models have grown in expressive capabilities and fluency, ethical considerations are an increasingly prominent issue. key considerations typically centre around model’s tendencies (1) to produce information that is factually inaccurate (shuster et al., 2021) or (2) to repeat toxic/biased behaviour from the training data (xu et al., 2020). compared to vanilla dialogue models, these risks are mitigated in sr: (1) sr is usually limited to short-form replies that express simple information, and is therefore less likely to lead to the kinds of hallucination seen in longer-form answers; (2) sr typically does not generate tokens sequentially, but retrieves responses from a pool of candidates, which can be vetted in advance. note however, this does not prevent replies that are contextually inappropriate when paired with a particular message, e.g. do you hate people? → yes, i do. the human-in-the-loop, who must ultimately choose and be accountable for whether or not to select one of the suggested replies, can be seen as a risk mitigant compared to vanilla chatbots. conversely however, wenker (2023) identify risks pertaining to a loss of human agency, such as due to a user selecting a sub-optimal reply to save time or being primed by the replies. this could lead to people being more trusting of an sr-generated reply versus receiving a reply from a chatbot, due to the belief that a human ultimately is behind it. we also only experimented with datasets that were released by previous studies, which are publicly available. these datasets (especially reddit) often contain toxic/biased behaviour which developers should bear in mind if using this system in a deployment context."
1121,"limitations we have primarily focused our analysis on similarity or log-probability based metrics for nlg. there are other important and interesting metrics that future work could examine. for example, deng et al. (2021) developed a family of interpretable metrics for various nlg tasks with the concept of information alignment. xu et al. (2022) recently proposed a metric based on stratified error synthesis. in addition, there are several task-specific metrics for paraphrase generation (shen et al., 2022), image captioning (hessel et al., 2021; kasai et al., 2022a), dialogue (mehri and eskenazi, 2020), controlled text generation (ke et al., 2022), etc., which would be interesting to evaluate. in §5.5, we design a number of fluency and consistency tests. it would be interesting to expand this set to be broader or more sophisticated (ng et al., 2014). also, there are other important aspects of text generation to consider, such as factuality (wang et al., 2020; pagnoni et al., 2021). all of our diagnostic data are synthetically created. while it provides valuable insights on the metric’s behavior, it does not have a good coverage of errors in real-world settings. expanding our analysis to real-world errors in a scalable way would be an important future direction. last but not least, we evaluate our proposed stress tests only on english texts. however, many language-specific properties can induce potential blind spots for metrics, especially for low-resource languages (haddow et al., 2022) where plms may provide poor text representations. an important future direction is expanding the tests to multilingual settings (thompson and post, 2020; pires et al., 2019)."
1122,"limitations semantic underspecification has been extensively studied in semantics, pragmatics, psycholinguistics, communication sciences, and cognitive sciences. in this position paper, we review this literature only superficially, although we are aware that a generalized and exhaustive understanding of the phenomenon necessarily requires knowledge of this previous work. we encourage the scholars working on this topic to embrace its complexity and depth. the paper focuses on approaches, tasks, and models within multimodal nlp. as such, it almost completely neglects a"
1123,"limitations in this paper, we limit the proposed whitenedcse for sentence embedding learning. conceptually, whitenedcse is potential to benefit contrastive learning on some other tasks, e.g., self-supervised image representation learning and self-supervised vision-language contrastive learning. however, we did not investigate the self-supervised image representation learning because this domain is currently dominated by masked image modeling. we will consider extending whitenedcse for visionlanguage contrastive learning when we have sufficient training resources for the extraordinary largescale text-image pairs."
1124,"limitations in this work, we address the heterogeneity challenge in the task of fl for semantic parsing, by leveraging the reduction of training loss signal. our work is motivated from the fl training procedure perspective to adjust the contribution of each client during the global model aggregation stage, but how each client’s data contribute to the final global model is still unclear. as the data of different clients contain different information, what kind of information of each client is helpful and can be more directly linked and utilized to facilitate the fl training is worth more efforts in future work. in addition, our proposed re-weighting mechanism is a universal technique for cross-silo fl. thus generalizing our proposed re-weighting mechanism to a broader range of tasks beyond semantic parsing, and further studying under what kind of conditions, lorar can make a huge difference for fl would be interesting future work to pursue."
1125,"limitations we discuss the limitations of our model as follows: 1. due to the natural uncertainty of financial forecast, although we have taken many methods to improve the generalization performance of the model (such as limiting the depth of memory layers and with the assistance of auxiliary data), creating a trustworthy application requires considering many other factors beyond the algorithmic level. we advise that users monitor the model’s performance over time and regularly update it to adapt to everchanging market conditions. 2. this paper uses granger causality based on transfer entropy to make a preliminary attempt to introduce causality between time series to model the similarity between stocks more accurately. but this description is junior and classical, and there are lots of more modern methods to measure precise causality in mathematics (like pc algorithm), which we believe would further improve the performance. 3. we only experiment the performance of model on the task of binary classification, leaving more complex tasks (such as regression task and returns prediction) and simulating actual investment to evaluate the capability and potential of the model comprehensively."
1126,"limitations it should be noted that, as our model and the baseline models in this study were trained using texts from social media and the experiments were conducted on online text, the results may not accurately reflect the performance in a clinical setting. a proper diagnosis by clinical experts necessitates a comprehensive analysis of various factors, including the number of manifested symptoms, the onset and history of symptoms, developmental background, lifestyle, and recent life changes, in order to gain a comprehensive understanding of the patient’s condition. however, it is still challenging to capture detailed information such as personal secrets through online text, as these texts are often composed of fragments of daily life, episodic experiences, and emotive expressions rather than providing a comprehensive view of an individual’s life. despite the domain-specific limitations imposed by the fragmentary text, we hope that our model may still serve as a valuable aid for clinical experts in their decision-making process. furthermore, future research should aim to move beyond predicting psychological symptoms and disorders solely based on linguistic styles and expressions, and instead seek to uncover the underlying features that contribute to these expressions as our model does."
1127,"limitations the main limitation of this work is that the pretraining model can not deal with numerical knowledge well. in future work, we will try to enhance the ability of the pre-training model to better deal with numerical knowledge."
1128,"limitations our proposed model has also some limitations. first, the requirement of large memory power of gpu (here, 40 gb) due to the use of gpt-2medium in the training of pal. further, weight optimization for each of the possible combinations of different rewards may lead to model training and validation time to months. hence, some heuristic is adopted to choose some sets of combinations of reward weights. in case of continuous, short and direct responses during interaction like ‘yes’, ‘i don’t know’, ‘no’, ‘2’, ‘yeah’, the system first tries to counsel client by inquiring about their issue but after three or four turns it starts deviating and may generate repetitive or inconsistent responses. this can be due to the fact that the datasets which are used to train the pal mostly consists of interactive dialogues with long utterances, hence model gets confused when treated with short and direct responses. lastly, it is also observed that sometimes, model asks too many questions to the user. this may dissatisfy the user. hence, the model should be forced to generate only relevant inquiries by discriminating the irrelevant inquiries. this opens up the door for future studies to build a counseling dialogue system."
1129,"limitations in this paper, we focus on open-ended text generation and demonstrate the effectiveness of contrastive decoding. we would like contrastive decoding to also work well for task-oriented generation settings such as summarization and machine translation. however, the idea of contrasting models across different scales (larger expert lm and smaller amateur lm) is not directly applicable, because the modes of both amateur lm and expert lm are of high quality. empirically, having a smaller summaization model (bart-small finetuned on summarization data) as the amateur lm yields lower rouge score than employing a uniform distribution as the amateur lm, which is equivalent to beam search based on log-probabilities. as future work, we aim to study the necessary properties of amateur lm to empower task-oriented generation (e.g. summarization, table-to-text)."
1130,"limitations mentioned in section 8 need to be considered and addressed carefully when using our dataset or models for evaluation or training of a deployed system. in addition, a biased corpus may lead to an evaluation that is unaware of re language forms used in other cultures and languages, or that refer to other types of items. we expect this consideration to be important in practical settings."
1131,"limitations the proposed algorithms allow to speed up an existing model out-of-the-box, without any modification or retraining. however, there are some considerations to bear in mind when using parallel decoding in order to have a speedup in terms of wall-clock time. firstly, as the name implies, the method executes the decoding phase in parallel. therefore, to appreciate the speedup one should be able to run computations in parallel. using parallel decoding without parallel resources or parallel-optimized software may increase wall-clock time due to overheads, leading to a waste of computation. this is further discussed in section 4.3 ""computational scaling"". the reported wall-clock time results are thus to be considered within the scope of the experimental setup proposed in this paper and they may vary depending on the underlying hardware and software. secondly, the method allows speedup of the decoding by scaling on parallel resources. this implies an additional computational cost during the inference phase to achieve a speedup. while using parallel decoding, one should consider a trade-off between the desired acceleration and the utilization of computational resources. thirdly, since our method performs the decoding in parallel, as for nat systems, it is difficult to combine it with beam search. beam search is inherently a dynamic programming algorithm and it is not possible to efficiently maximize the joint probability of the large search space without using sequential intermediate computations. we better explain this aspect in the next paragraph. beam search. beam search is widely employed to enhance the translation quality in mt (sutskever et al., 2014; bahdanau et al., 2015) as well as in other domains such as audio (reddy, 1977; postolache et al., 2023). however, it is an inherently sequential procedure that stores partial joint probabilities of the entire sequence (beams) while progressing with autoregressive decoding. determining the maximal joint probability of all sequences in parallel is a challenging task, equivalent to a full maximum a posteriori (map) estimation. this is an open research problem and it is also an issue for nat methods. nat methods patch up this limitation with sequence-level kd which has the advantage of ""not requiring any beam search at test-time"" (kim and rush, 2016) thanks to learning and distillation from large models. since our method is a decoding algorithm, we cannot use the same approach without learning. nevertheless, the quality guarantee allows our methods to have performance on par with greedy autoregressive and generally better than a nat model. we think of our method, not as a replacement for beam search, but rather as a way to obtain a speedup at inference time that is a middle ground between autoregressive greedy decoding (high quality, no requirements, no speed) and nats (quality compromises, increasing requirements with increasing speed). future works might address the quality gap with beam search by combining parallel decoding with alternative techniques like minimum bayes risk (eikema and aziz, 2020)."
1132,"limitation in hardpt, we focus on training specifically on hard samples while discarding misleading samples. however, it is worth acknowledging that these misleading samples may potentially contain valuable information. additionally, finding quantifiable and interpretable evaluation metrics to accurately assess the model’s ability to identify misleading and hard samples is a crucial challenge. in our future work, we plan to explore strategies for correcting mislabeled samples and develop evaluation metrics that accurately measure the accuracy of sample partitioning. our aim is to maximize the utilization of all available information from the original dataset."
1133,"limitations and ethical issues this work presents some limitations that will be addressed in future work. in particular, i) even if the model for biographical event detection obtained good results, more sophisticated approaches may be devised to increase its effectiveness (e.g., best performing lms, multi-task settings); ii) the intersectional analysis was performed on a specific sample of people, and thus limited to writers. taking into account people with other occupations may lead to different results; finally, iii) only wikipedia biographies were considered: biographies from other sources may differ in style and thus pose novel challenges to the biographical event detection task. the research involved the collection of documents from wikipedia, which are released under the creative commons attribution-sharealike 3.0 license. the annotation of the experiment was not crowdsourced. all the three annotators are member of the research team who carried out the research as well as authors of the present paper. they are all affiliated with the university of turin with whom they have a contract regulated by the italian laws. their annotation activity is part of their effort related to the development of the present work, which was economically recognized within their contracts with the university of turin. a data statement for the research can be accessed at the following url: https: //github.com/marcostranisci/ wikibio/blob/master/readme.md"
1134,"limitations in this work, we propose a decoding algorithm for text generation. we present the algorithm with comprehensive"
1135,"limitations to create a clean and diverse corpus, we have chosen to crawl news articles as our primary data sources. since all the articles are crawled from public domains, the data could potentially encompass the biases which propagate in public channels. currently, the models trained on such data sources could model the inherent biases present within the data. in the current work, we do not perform any debiasing techniques and leave that for future work. 8https://www.meity.gov.in/ 9https://www.bhashini.gov.in/ 10https://www.cdac.in/index.aspx?id=pune language identification (lid) tools are restricted to a limited number of languages and unavailable for some of the very low-resource languages like bodo, dogri, khasi, etc. we made our best effort to clean the corpus using unicode spans, but it is possible that the data sources could have some issues. we leave developing lid tools for low-resource languages as part of future work. from our ablation studies, we see that models are benefited by using in-language training and/or development sets. we call upon the community to work together to create more in-language data resources. finally, there is still work required in terms of building datasets for hundreds of extremely low-resource languages not represented in this work."
1136,"limitations there are two major limitations of the proposed tfsgc. the first one is that the effectiveness of tfsgc depends on the quality of the scene graph. since mscoco does not have sg annotations, we evaluate the parsers in visual genome: for butd/patch and vinvl, the recall@50 of relation/attribute are respectively 65.2/68.4 and 73.4/76.6. we use vinvl’s sgs in tfsgc(butd) and cider improves from 132.3 to 133.1, suggesting better sgs are beneficial. if the scene graph quality is poor, then tfsgc will not achieve good performance. when an incorrect node in the scene graph, it also affects the output of the caption. e.g., in fig. 4 (e), the correct object label should be ""surfboard"" instead of ""train"". in this paper, we use visual genome, which contains abundant and useful scene graph annotations for parsing effective scene graphs, but current performance is not the best, and we will improve the scene graph parser based on the latest scene graph parsing methods in the future. the second limitation of tfsgc is that if the visual features contain abundant attribute or relation knowledge, then the improvement of tfsgc compared with the classic transformer will be weakened. for example, compared with the butd feature case where the relative improvement of ciderd is 3.6 (tfsgc-base in table 1), the vinvl feature is more powerful since it is trained by much more data samples with more semantic labels, thus the relative improvement is lower, which is 2.2 (tfsgc-vinvl(transformer) in table 3)."
1137,"limitations a major limitation of our work may be that our disentangled representation learning framework adopts some heuristic assumptions and designs in data augmentation and counterfactual data construction, and it remains to be seen whether they are applicable to other datasets and other languages. in particular, for the data augmentation of cnaa strategy, we assume that more data can be synthesized by text concatenation and we heuristically decide the quality and content label of synthesized data by some random strategies. besides, for the counterfactual data generation, we mainly generate counterfactual samples and scores heuristically through our intuition and experience, rather than building a generation model based on counterfactual reasoning. considering that some researchers have already developed some counterfactual data generation models for nlp tasks such as neural dialogue generation (zhu et al., 2020), we are interested in whether it is possible and better to build a counterfactual data generation model for our method."
1138,limitations and design more solid and transparent benchmarks that will advance our scientific understanding of nlp systems and humans.
1139,"limitations we discuss here the limitations of the proposed promptner. first, although promptner performs well on flat and nested ner, it cannot recognize discontinuous entities. the discontinuous entity can be divided into multiple fragments, while each position slot of promptner can only fill one. a simple alternative is to expand the position slots in prompts to accommodate discontinuous entities. second, named entity recognition requires pretrained language models (plms) with the essential ability to sense the structure and semantics of entities, which can enhance entity locating and entity typing in lowresource scenarios. however, since plms prefer to learn semantic rather than structured information in the pre-training stage, promptner needs to be warmed up by wiki training when applied to low-resource scenarios. finally, since the number of prompts is determined during training, there is a limit to the number of entities that the model can recognize. if the number of entities in a sentence exceeds the pre-specified value when testing, promptner will perform poorly."
1140,"limitations • these methods have been performed on three sl datasets (section 4.1) as these were the only publicly available natural sl corpora found to contain gloss annotations. therefore, the generalization of these"
1141,"limitations we analyze the limitations of our work as follows. firstly, although applying a million-scale simile knowledge base or large-scale simile sentences as reference makes our designed metric significantly more correlated with humans than prior referencebased metrics (e.g. bleu, rouge, bertscore), our metrics are still reference-based and rely on the quality and scale of referenced data. we have discussed the effect of referenced dataset size in our paper and will design reference-free metrics to further complement our metrics in future work. additionally, since our metrics utilize a million-scale simile knowledge base or large-scale simile sentences as references, the efficiency of our method is slightly lower than the automatic metrics based on a few references. nevertheless, this limitation does not prevent our metrics from performing systematic and scalable comparisons between sg models. ethical considerations we provide details of our work to address potential ethical considerations. in our work, we propose holistic and automatic metrics for sg evaluation and construct an evaluation dataset to verify their effectiveness (sec. 4.1). all the data sources used in our evaluation dataset are publicly available. the details about human ratings, such as the instructions provided to raters, are provided in appx. a. in our case study (sec. 5), the human rankings are discussed by three raters. we protect the privacy rights of raters. all raters have been paid above the local minimum wage and consented to use the evaluation dataset for research purposes covered in our paper. our work does not raise any ethical considerations regarding potential risks and does not involve the research of human subjects."
1142,"limitations and potential risks the two limitations of dynainst are that it requires known task boundaries, and that it does not concern with corrupted or noisy training instances. in a realistic industry setting where the task definition is quite ambiguous, and a non-negligible amount of human bias and noise are introduced during the data collection process, these limitations of dynainst may degrade its performance. however, considering that this is the first time lifelong instruciton learning has been studied, these limitations can be considered interesting directions for future research. like any language model, the model trained with dynainst may output unfair and/or offensive predictions due to the bias embedded in the dataset. improving the fairness of instruction-tuned language models is beyond the scope of this paper; nonetheless, if these problems remain neglected, we will risk deploying language models that are heavily biased and discriminatory."
1143,"limitations our method requires balanced data because all attributes share the same normalizing flow. this means that when the training data for one attribute is much larger than others, we need additional training steps to make up such a gap to prevent the jacobian part of the normalizing flow from too much in favor of that attribute. in addition, although we can achieve good results on the data scale of 2.5k or 5k per attribute, our model does not fit well in few-shot scenarios. we can alleviate this problem by obtaining a sufficient amount of single-attribute labeled data from the style transfer tasks. in our experiments, each attribute is considered equally 8see more analyses in §d, f, and g. important, which may be different from the practical situation. fortunately, our control strategy is flexible and can be customized for different demands."
1144,"limitations in this section, we discuss the limitations of our model. specifically, the selection of k values in the lrl module necessitates human involvement. various types of data or entities may rely on distinct k values. while the majority of k values within a reasonable range lead to improvements in model performance, identifying the optimal value solely through human involvement poses challenges. moving forward, we will investigate the automatic optimization of k values to enhance the model’s capacity for acquiring latent relations."
1145,"limitations it is worth noting that the supportive pretraining data we investigated throughout the work is w.r.t. the current lm, such that a perturbative continued pretraining with the supportive data would improve the final lm checkpoint deployed to downstream tasks. it is possible that for some data which we did not determine as supportive, they had been supportive w.r.t. early checkpoints of the lm. with more computing resources, future work may investigate the trend of supportive patterns across multiple checkpoints of a lm throughout the pretraining process. additionally, another significant limitation of our work is the amount of involved computing resource. the orca-icl method is gradient-based that requires back-propagation. since we iterate through a large size of pretraining data, the cost of computation is similar to training a language model with a batch size of 1 on the considered pretraining data. on our 4 nodes each consists of 8 nvidia v100 gpus, finding the supportive pretraining data for each source task in our experiment would take about a week. one mitigating aspect of such computation is that the gradient calculation can be done asynchronously, therefore enabling the use of idle, leftover gpus scattered across a cluster of nodes. we plan to explore efficient computation of gradient similarity or move from a paradigm of extracting supportive data to generating supportive data in future work."
1146,"limitations and future work our approach makes it feasible to learn the discriminative ability of an expansion embedding for dense retrieval. however, it is unclear how it may be adapted for the single-representation dense retrieval prf model. in addition, in this work, we did not test the effect of the hard negative sampling and the number of negative samples for cwprf. finally, while we have focused on passage retrieval, longer document retrieval can be addressed through splitting documents into passages during indexing, retrieval and prf, and applying a max-passage aggregation (dai and callan, 2019) to obtain a document ranking. for future work, we will consider a hybrid approach to incorporate both the learned weights produced by cwprf and the statistical information in the expansion embedding identification process. while prf approaches typically increase query response time, they can also be used as teacher approaches to realise more effective and efficient student models (e.g., colbert-prf is applied as teacher by kim et al. (2022)). this means that improved prf approaches, such as cwprf, can also have downstream benefits to other retrieval approaches."
1147,"limitations to our dataset and experiments, which we discuss in a separate section following the"
1148,"limitations our study and findings are limited to the specific l1–l2 pair of chinese (mandarin and cantonese)–english. further, the experimental setting we draw our data from is highly controlled, with carefully-chosen lexical items and carefullydesigned (length- and distractor-matched) stimulus sentences. while this enables strong statistical"
1149,"limitations though we have injected math reasoning skills to matcha, error analysis shows that there is still room for improvement on queries requiring complex reasoning. besides, it remains debatable whether doing math calculation in weight space in a purely end-to-end manner is the most promising path forward.9 besides math reasoning, figure 2 shows that plot attributes is an area where matcha underperforms pali. we conjecture that it is due to matcha’s lack of massive scale grounded imagetext pretraining with rich semantics (which pali has using web-scale image-text pairs). while chartto-code pretraining provides certain level of plot attribute grounding, such plot features are mostly using default options in plotting packages but not explicitly written out in code. in terms of experimental setup, the reported number is result of a single run. pretraining is extremely costly especially when there exists more than twenty ablation setups and downstream evaluation tasks. we have collected pretraining and evaluation data points from multiple aspects on various scenarios to verify the robustness of matcha. however, we do acknowledge that the paper can benefit from reporting multiple runs given sufficient compute. last but not least, it is also worth noting that visual language is an umbrella term. there are other visual language systems beyond the ones discussed in this paper. as an example, comics/manga have their distinct visual lexicon or even grammars (cohn, 2013)."
1150,"limitations although bump is, to our knowledge, the first dataset on which to study the consistency of faithfulness metrics on human-written errors across different error types, there are some limitations regarding the"
1151,"limitations we provide a comprehensive study on the efficacy of leveraging pre-trained language models for zeroshot ood detection. our method is thus limited to the setting of abstaining from prediction on all ood data. this is more conservative than selective prediction, where the model must make predictions over as many id & ood points as possible while maintaining high accuracy. despite this, ood detection has lower risks to high-risk and safety-critical applications, where rare and anomalous data is more reasonably flagged to the expert. we believe our work provides new values and insights to the research community, especially on safe handling of distributional shifts when deploying pre-trained language models. as discussed in our ethical considerations, the ood detection problem is of significant use in high-risk settings, and should be incorporated into production-level pipelines. however, for the same reason, the ood detection models must be also reliable to avoid any risk to the downstream applications."
1152,"limitations the limitation of unisumm can be stated from three perspectives. first, the multi-task pre-training of unisumm can be time and cost consuming, which requires large gpu resources. second, the current framework uses prefixes of a fixed length for both multi-task training and few-shot prefixtuning. however, different summarization task may prefer different size of prefixes. third, in this work, we focus on summarization tasks in english. the performance of unisumm for languages that have a different morphology or syntactic structures from english needs further exploration."
1153,"limitation of our datasets, we concatenate one reference and model-generated response, which are then fed to the encoder. employing rade when the reference response is not available. considering the reference is not always available in real-world scenarios, we design two alternatives to enable rade, i.e., constructing a pseudo-reference via retrieval or generative method. we verify the two solutions on the fed dataset and the details can be found in appendix a.3."
1154,"limitations firstly, as analyzed in sec-4.3, our proposed method fails to make a significant improvement on span boundary identification. for one thing, the annotation inconsistency in the dataset hinders the model’s understanding. for another, our span proposal module leverages the contextual information alone with implicit training signals for span boundary information. we will consider enhancing the span proposal module with amr information in the future. secondly, though tara saves up to 56% inference time compared to the previous amr-guided work, its entire training requires more than 7h on 4 tesla t4 gpus. the bottleneck is the incongruity of pre-trained language models and non-pre-trained gnns. we leave the problem for future work. finally, arguments on wikievents and rams are still relatively close to its event trigger (e.g., rams limits the scope of arguments in a 5-sentence window), and thus connecting sentencelevel amr graphs is enough to model the longdistance dependency. otherwise, document-level amr graphs with coreference resolution are in demand."
1155,"limitations our model is trained in an end-to-end manner, resulting in more training time costs than featurebased methods. to eliminate the need for gloss annotations, the ccm process relies on a large amount of sign and translation pairs. the generalizability of the model is restrained by the number of such pairs available. the more ideal end-to-end framework should combine the visual backbone and visual2text encoder into one visual encoder that can be trained end-to-end. in addition, the selection of conceptual words is done according to manually-designed rules now and relies on external toolkits like nltk. we will investigate automatic conceptual word extraction methods in future work."
1156,"limitations this work focuses on pretraining large language models for zero-shot generalization. although our proposed method is more efficient than baselines, it still requires significant computational resources, specifically gpu resources. the gpu resources used and training time are detailed in appendix a.6. our study is also limited by the computational budget, preventing us from training models as large as gpt-3 or t011b. however, our large++ model (775m parameters) already rivals or outperforms previous state-of-the-art models."
1157,"limitations we identify four major limitations of our work. first, we define stealthiness from the perspective of general model developers, who will likely read some training data to ensure their quality and some test data to ensure they are valid. we therefore focus on producing natural-looking poisoned samples. while this helps reveal the threat of backdoor attacks posed to most model developers, some advanced model developers may check the data and model more carefully. for example, they may inspect the word distribution of the dataset (he et al., 2022), or employ backdoor detection methods (xu et al., 2021) to examine the trained model. our attack may not be stealthy under these settings. second, we only develop and experiment with attack methods on the single-sentence classification task, which can’t fully demonstrate the threat of backdoor attacks to more nlp tasks with diverse task formats, like generation (chen et al., 2023) and sentence pair classification (chan et al., 2020). the sentences in our experimented datasets are short. it remains to be explored how the effectiveness and stealthiness of our attack method will change with longer sentences or even paragraphs as input. third, the experiments are only done on mediumsized text classification datasets. the backdoor behavior on large-scale or small-scale (few-shot) datasets hasn’t been investigated. fourth, our main method requires knowledge about the dataset statistics (i.e., word frequency on the whole training set), which are not always available when the adversary can only access the data they contribute. the attack success rate drops without full access to the training set."
1158,"limitations based on internal review and user feedback, we summarized the following limitations to improve and iteratively update our system and framework in the future. problem modeling: new concepts appear yearly in the real world, but the current system cannot generate new concepts. generally, the emergence of new concepts often comes from the fusion of mature technologies. thus, we model the idea exploration as link prediction. note that it is not the only pathway to brew new ideas, but we have verified the effectiveness and rationality of this approach in the experiments. in addition, plm can be taken as an implicit knowledge graph (petroni et al., 2019; wang et al., 2020), which is capable of tackling uncovered concepts in the evolving concept graphs. we will continue exploring the potential of plm in knowledge discovery and innovation. logic, correctness, and concreteness: although the verbalized ideas can deceive many experts, they may still lack logic, correctness, and details, especially in natural and exact sciences. it is also a challenge for natural language generation. we plan to use more academic corpus and introduce constraint (zhang et al., 2020) to alleviate such problems. temporal information: in plm-lp, we simply take the year information as a token in the input sequence. we conduct additional experiments to show that the temporal information is not sensitive to plm-lp, which can be attributed to the negative sampling and the nature of the strictly evolving network. two birds one stone: the current system employs two different plms for link prediction and idea verbalization, respectively. the development of prompt learning (liu et al., 2021b) reveals that most nlp problems can be regarded as generation problems. in the future, we will introduce new training settings using a single plm to address link prediction and idea verbalization simultaneously."
1159,"limitation although experiments on two public datasets show the effectiveness of our proposed method compared with other state-of-the-art methods, we notice that our proposed model fails to distinguish similar emotions effectively going through the prediction results, as frustrated and anger, happy and excited (fig. 5(a)). moreover, our proposed model tends to misclassify samples of other emotions to neutral on meld due to the majority proportion of neutral samples in these datasets. we will address this issue in future work by integrating a component for capturing the fine-grained emotions."
1160,"limitations our framework manually sets thresholds t+ and t− in pseudo labeling by observations of data quality and hyperparameter searching. dynamic threshold tuning (xu et al., 2021) or meta pseudo labels (pham et al., 2021; li et al., 2021) can be implemented to better filter pseudo-labeled examples. and the thresholds for different tasks can be tuned separately to improve the models’ generalizability. recently, large generative language models such as gpt3.5 (brown et al., 2020) and chatgpt2 (ouyang et al., 2022; gao et al., 2022) have demonstrated their strong potential on various nlp tasks including probing abstract commonsense knowledge with in-context learning (brown et al., 2020; xie et al., 2022). due to our limited access, we did not conduct fully-scaled experiments in our paper. a short"
1161,"limitations we note several important limitations of this work. perhaps most importantly, our dataset is ""naturalistic,"" but not actually ""natural"" in the sense of independently occurring in the world. though the interactions between our participants are real, the task itself is ultimately artificially constructed. in a real-world negotiation over something as valuable and significant as a house, the negotiating parties will be much more invested in the outcome than our experimental participants, whose actions change their outcome to the order of a few dollars. this difference in turn could lead real-world negotiating parties to speak differently and possibly employ substantially different strategies than we observe. methodologically, our study has a few important limitations. firstly our analyses are based entirely on language that has been automatically transcribed (with some manual checks), and while this helps with expense and scale, these transcripts could be missing important subtleties that influence the outcome. koenecke et al. (2020) uncover an important limitation of these systems, finding significant racial disparities in the quality of asr transcriptions. the linguistic feature analysis we perform should be treated as largely exploratory, and provides suggestive and correlational rather than causal evidence for the relationship between language in the interactions and negotiation outcomes. lastly, there are further linguistic and interactional phenomena at play that we have not yet integrated into the analysis presented here. for one, we have access to the audio channel of participants’ actual speech, but we have not analyzed it in this work. there could very well be acoustic cues in participants’ speech that are as significant to the interactions as the textual features analyzed here, particularly speech prosody which has been shown to communicate social meanings that could be highly relevant to negotiation like friendliness (jurafsky et al., 2009). this particularly extends to more interactional questions of not simply who said what, but what was said in response to what and in what way. for instance, existing research has shown that acoustic entrainment in dialog (e.g., interlocutor adaptation to one another in terms of prosody) has important social associations with dialogue success (levitan et al., 2012). we leave a deeper investigation of these phenomena for future work. broader impacts this research, collectively with prior and future related work, has the potential to advance our understanding of negotiation, a ubiquitous human activity. our dataset can enable future research into the dynamics of human bargaining as well as interpersonal interactions more broadly. by employing the findings and insights gained from such research, individuals may be able to enhance their ability to negotiate effectively in various settings, such as salary negotiations, personal relationships, and community initiatives. meanwhile, we must acknowledge that while a better understanding of language as an instrument in social interaction can be empowering, it may also be used as a tool for manipulation."
1162,"limitations this study has two main limitations. the first limitation is its reliance on the assumption that teacher logits on augmented data follow a gaussian distribution. this assumption is used in the derivation of teacher logits in section 4.3. however, in practice, teacher logits may not strictly follow a gaussian distribution. it is challenging to estimate teacher logits under more realistic assumptions, which requires thorough investigations on the distribution of teacher logits and more complex computations for logits estimation. the second limitation is that our method still requires access to the training dataset of the downstream tasks. in this paper, we focus on kd when teacher plms only return decisions. however, our method is not capable of kd without publicly available training data, which is a more challenging scenario for decision-based kd. we believe training a data generation model (wang, 2021; zhang et al., 2022; sanyal et al., 2022) might be useful for such cases."
1163,"limitations the main limitation of mccl is the requirement of a sufficiently large batch size in training (32 documents in our experiments), leading to a need for large gpu memory. this is because mccl uses in-batch entity pairs for contrastive learning, and a small batch size does not provide enough instances to form multiple clusters. in addition, we need to store the entity pair embedding of the whole training set for knn-based inference, which is less memory-efficient than ce."
1164,"limitations one of the biggest concern people may have is whether approximate unlearning forget the information of the removal data. approximate unlearning can not ensure exact removal of information already learned in deep neural models, just as its name suggests. considering that current exact unlearning methods are very time-consuming and hard to apply in practical applications, approximate unlearning is still a direction worth trying and is also effective in reducing the attack risks by attackers or mitigating the harm of toxic data. another limitation of this work lies in the fact that we have to maintain an extra data set dn and two models af and an in the process of unlearning. though the extra cost of our kga method is trivial compared to the previous work (e.g., bourtoule et al. (2021) has to maintain the entire training set), we have to point this limitation out and call for follow-up research to come up with better ways to reduce unlearning costs. besides, we only explore word-level translation unlearning effect by comparing the generated sentences before and after deleting instances with specific words due to the space limitation. more interesting experiments with different granularity can be discussed in future work to explore how unlearning method works in different nlp tasks."
1165,"limitation the “narratives” dataset provides a valuable fmri resource, stimulated by language and obtained under naturalistic conditions. further research opportunities can be pursued with the availability of more detailed datasets. for instance, comparative studies between instances of stuttering and nonstuttering in text stimuli can be conducted, as our experiments demonstrate that the model tends to retain frequently-used filler words (such as “um” and “like,”) as a shortcut for higher accuracy. meanwhile, the evaluation strategy applied for current research of open-vocabulary brain decoding presents an idealized condition and and serves as a starting point from which further exploration of how existing methods might perform under more real-world scenarios can commence. although we use this setting for baseline comparison purposes and a testament to the feasibility of our fmri2text task, additional tests under more practical conditions could be an essential step in future work, further elucidating the applicability and robustness of the methods. furthermore, the structure of the snapshot encoder can be explored further, as exemplified by the use of transformer-based vision transformer (vit) in chen et al. (2022) for fmri encoding. ethical considerations in this work, we introduce a new nlp task related to fmri and a unified approach for decoding various types of cognitive signals into human language. we conduct our experiments on the public cognition datasets narratives and zuco1.0 with the authorization from the respective maintainers of the datasets. all experimental datasets involved have been de-identified by dataset providers and used for research only."
1166,"limitations our approach for constructing dense-atomic still has two limitations: 1) to keep dense-atomic simple, we only consider the most reasonable relation in this paper, while the relation between two events can be complex and diversified. we will release versions of dense-atomic with diversified relations later; 2) due to page limitation, we only evaluate dense-atomic on simple commonsense reasoning tasks, and we will further validate the multi-hop reasoning capacity of dense-atomic on more complex downstream tasks in the future."
1167,"limitations currently, the main goal of shrinke is to model inference patterns directly in the embedding space for hyper-relational kgs and we do not explore more advanced training strategies that have recently been proposed. for example, recent works (yu and yang, 2021; wang et al., 2021; shomer et al., 2022) have demonstrated that adding auxiliary training tasks, e.g., the task of predicting qualifier entities, can further improve the overall performance. we believe such auxiliary training tasks can also benefit shrinke and we leave it as future work. another limitation of shrinke, though rarely happens, is that when dealing with semantically opaque contexts, the monotonicity assumption might not hold. in that case, we need ad-hoc solutions. one simple way is to explicitly distinguish semantically transparent and semantically opaque contexts."
1168,"limitations although our ctc-nast model achieves excellent performance, there are still some underlying challenges that remain in the follow-up of our work. here are some limitations that we intend to resolve in the future: • the better designs of reordering augmentation and training strategy. although the proposed cla and clm approaches achieve good results by alleviating the monotonic assumption and relieving the modeling burden, combing them can not bring remarkable improvement. more importantly, these two methods fail to stable improvements in encode-decoder architecture. this drives us to investigate the interference of the optimizations between ctc and cross-entropy. • combination with the pre-training or multitask learning. although our methods bring remarkable gains on both ar and nar models, we do not explore the utilization of external data resources. although we can use the pre-trained models directly, we expect more effective methods in future work. theoretically, we need to design nar asr and mt models that share the same or similar architectures with the acoustic encoder and textual encoder, respectively. in this way, the nast model bridges the gap between pre-training and fine-tuning and has more potential for better performance. • the potential risk for unwritten languages. in our work, we assume that transcription is always available, which is consistent with almost previous studies. although some datasets have no transcription, we can use a well-trained asr model to generate pseudo labels. however, it is hard to handle speech translation from unwritten source speech. the supervision of source text is very important for our model. therefore, we need to develop better methods for stable training."
1169,"limitations our c-stance data is collected from social media, which may be seen as a limitation, as we may not cover all aspects of formal texts that could be used in essays or news comments. we will plan to extend this dataset with other types of text in the future. however, this is a limitation of any other datasets that focus on social media content. ethical statement our dataset does not provide any personally identifiable information. microblogs are collected using generic keywords instead of user information as queries, therefore our dataset does not have a large collection of microblogs from an individual user. thus our dataset complies with sina weibo’s information privacy policy."
1170,"limitations the potential limitations of this work are that wukong-reader has fixed sequence length that may prevent it from modeling long and multi-page documents. therefore it would be promising to handle varying-length inputs for wukong-reader by, for instance, equipping the model with relative positional embeddings of the model backbone. additionally, the pre-training objectives used in this work may not be applicable to all vdu tasks. for instance, it can be hardly applied in abstractive question answering or document summarization."
1171,"limitations to better analyze the limitations of pace, we carry out an analysis of the errors made by pace on the photochat and simmc2.0 test sets. we reveal several reasons for the errors, which can be divided into the following categories. first, since there are many similar images in the datasets, pace fail to distinguish some gold image from similar candidates. this may be because we do not design an explicit fine-grained reasoning module to capture the details of images and texts. for example, for the context mentions “i and my dad both have a camera”, our model can capture the entity “camera”, but fails to reason the fact that there should be two cameras. one possible solution is to introduce a deep reasoning and comprehension strategy to empower the model with excellent reasoning ability. second, due to the lack of fine-grained structural understanding of the images, the sentences generated by pace suffer from identifying the relative positions of entities. for example, pace may have difficulties recognizing the fact that the right side of a yellow shirt is black pants. this issue is particularly severe in simmc as there are many entities in the pictures and spatial descriptions of entities in the responses. one possible idea is to extract the relative positions of objects mentioned in the conversation as auxiliary data to guide the model’s generation."
1172,"limitation this paper presents the mvp-tuning framework, which combines multi-view knowledge retrieval with prompt tuning and incorporates retrieved knowledge in a simple kg-encoder-free paradigm. however, there are limitations to our approach. firstly, multi-view knowledge consists of self-view and consensus-view knowledge, which are one-hop triplets in the knowledge graph. however, not all question-choice pairs have one-hop triplets, leading to null knowledge being retrieved. additionally, excessive consensus-view knowledge can lead to noisy retrieved knowledge. therefore, our knowledge retrieval system needs further improvement to obtain sufficient, high-quality knowledge. secondly, we focus on the empirical study of prompt tuning in commonsense reasoning tasks. although we conduct extensive experiments, including initialization schemes and prefix token length, we do not fully understand the mechanism behind prompt tuning and sometimes experience unstable performance. although prompt tuning has been proven to be an efficient tuning paradigm for commonsense reasoning tasks, it requires further exploration."
1173,"limitations although peit is an end-to-end approach to image translation, in the current form, it needs to be pre-trained in two stages with mt and synthesized data and fine-tuned on the curated image translation data. the training procedure is longer than the standard mt task due to the lack of training data and the cross-modality challenge. for the created ecoit dataset, we used online mt to automatically generate translations and then manually post-edited translations via crowd-sourcing. this significantly reduces the cost of building a large-scale image translation dataset from scratch but may introduce translation noise and “machine translationese” (vanmassenhove et al., 2021) in comparison to professional human translation."
1174,"limitations our framework currently only supports english, thus not allowing us to complete a cross-lingual study. future work should focus on extending this study to a multilingual setup. our method is evaluated on a 16 dataset stance benchmark, where some domains bear similarities. the benchmark should be extended and analyzed further to find independent datasets with varying domains and minimal similarities, allowing for a more granular out-ofdomain evaluation."
1175,"limitations and outlook discomat is a pipelined solution trained component-wise. this raises a research question: can we train one end-to-end trained ml model that not only analyzes a wide variety of table structures but also combines the understanding of regular expressions, extraction of chemical compounds and scientific units, textual understanding and some mathematical processing? this defines a challenging ml research question and one that can have a direct impact on the scientific matsci community. indeed, automating parts of scientific discovery through such nlp-based approaches has the potential for biases and errors. note that wrong and biased results can lead to erroneous information about materials. to a great extent, this issue is addressed as we rely only on published literature. the issue could be further addressed by considering larger datasets covering a wider range of materials."
1176,"limitations the potential limitations of our model are threefold. first, the training process requires more computational cost as the model needs to conduct two forward passes for each sample in the self-distillation module. second, there is still room for improvement to reduce the model’s overcorrection of legal characters. third, the phonetics-aware sequence doubles the length of the original input, which demands extra computation cost at inference time."
1177,"limitation of not utilizing information beyond the training sequence length. fortunately, this is overcome by our new relative positional embedding, sandwich, which is simplified from the earliest proposed sinusoidal positional embedding. finally, sandwich demonstrates a log-decaying temporal bias pattern similar to that previously seen in the design of kerple and t5, and such pattern is likely to be the secret to successful length extrapolation. together these findings supports more effective design of future extrapolatable transformer language models. limitations although sandwich, kerple, and t5 use information beyond training sequence length, their receptive fields still highly favor the most recent tokens. while this recency bias is beneficial to the modeling of human-written text, it is problematic in other scenarios. let us consider the task of parity prediction: a model needs to predict whether a bit string has an even or odd number of ones. for example, the parity of [1, 1, 0, 1] is odd (or 1) and the parity of [1, 0, 1, 0] is even (or 0). unlike human-written text, every single bit is equally important. transformer language models with current rpes still struggle on this simple task (anil et al., 2022). its difficulty can be explained by the recency bias effect that we described. devising a new positional embedding or transformer model architecture that solves this problem is a promising direction for future work."
1178,"limitations in this work, we are the first to uncover the social bias problem in the text-to-sql task. we categorize different types of social biases related to various demographics. we present a new benchmark and metric for the social bias study in the text-to-sql task. however, this work stops at the point of uncovering and analyzing the problem and phenomenon, without making one step further to solve the social bias problem in the text-to-sql task. besides, in spite of the structured scalability of our proposed paradigm for social bias benchmark construction, the efficacy of entending with other text-to-sql datasets remains to be verified."
1179,"limitations this work focuses on mitigating the negative transfer and catastrophic forgetting issue in multi-task dialogue generation. all technologies built upon the large-scale plm more or less inherit their potential harms (bender et al., 2021). besides, we acknowledge some specific limitations within our methods: 1. the construction of pseudo labels requires dependency parsing with spacy, which is timeconsuming. but we only construct pseudo labels offline in the training processing and it causes no latency at inference. 2. we instantiate our modular framework using minilm (wang et al., 2020b) as the backbone of the reader within the programmer, and t5 (raffel et al., 2019) as the backbone for the content operators and linguistic operators. we did not try other instantiations although the modular framework does not depend on the specific initialization choice of modules. theoretically, any generative plm could be the backbone of these linguistic and content modules. 3. we aim at decomposing the response generation into relatively independent and composable operators. currently, the division of dialogue skills and module functions is in a heuristic way inspired by linguistics. thus it remains a future research question about how to design modular architecture in a more data-driven way."
1180,"limitations the intention classification task is not trivial even for humans, especially when the intention is implicit or disguised. the sample size of our study is small, which makes classification more challenging. currently, we are extending the dataset to include more samples in each category. we aimed to use this data as a proof of concept to shed light on using questions as a means to attack someone or disguise intention. future directions involve enlarging the dataset and including a variety of social interactions from different sources such as social media (e.g., twitter), forums (e.g., reddit), and spoken conversations to investigate other emerging categories based on context, topics, and events. moreover, the dataset is imbalanced. wikipedia editors should follow strict rules and avoid explicit hostility otherwise get blocked. the nature of wikipedia"
1181,"limitation of a lack of personalized and specific examples in existing datasets, when teaching cognitive techniques. future work will evaluate whether leveraging models to produce richer training material results in more robust learning and understanding of the types of unhelpful thought patterns in humans.this may serve as the basis for future psychological validation studies of the materials and support future studies of low-intensity self-help interventions."
1182,"limitations in-context learning is an useful ability, this paper only focuses on in-context named entity recognition, leaves the learning of other nlp tasks’ incontext learning abilities for future work. currently, we learn in-context learning via metafunction pre-training, by comparing an in-context extraction function and a fined-tuned surrogate extraction function at the representation level of their encoders. there are two approximation here: one is fined-tuned surrogate extraction function for approximating golden extraction function, and the difference between representations for approximating the divergence between functions. we think the above two approximations can be further improved for better and faster in-context learning."
1183,"limitations the limitation is that we separate node identification and node/edge labeling processes. because joint node identification and label classification should enumerate all possible spans in a sentence, which is too computationally expensive. most previous works also separate the two processes. but an obvious disadvantage of such a pipeline scheme is the error propagation problem. we take joint node identification and label classification with high-order inference as future work."
1184,"limitations our work reported in this paper has two limitations. first, because of the scarcity of treebanks and ner datasets for turkish, our pretrained spacy language models were tested on a limited amount of testsets. second, we trained our spacy models on generalpurpose datasets compiled from wikipedia data and formal written language resources. accordingly, our models may not be very effective in analyzing social media texts such as twitter data."
1185,"limitation while our pipeline is designed to be applicable to any financial text dataset, the evaluation dataset is transformed solely on earnings conference calls. we will expand the scope of experiments to include other financial text sources such as news articles and social media posts. finally, the current trading simulation does not take transaction costs into account. going forward it will be necessary to consider more sophisticated trading policies."
1186,"limitations in this section, we discuss the limitations of our work as follows. first, despite achieving promising results, our model needs to calculate pseudo ranking labels of the teacher which requires additional training time per epoch than the teacher. the training efficiency of rankcse and simcse can be seen in appendix d. second, we directly use simcsebase and simcselarge as a multi-teacher in our implementation and experiments. however, how to choose the best combination of the teacher models is worth further exploration. it could help researchers to better understand the upper bound of improvements. we plan to investigate more along this direction in the future."
1187,"limitations our method utilized pretrained entailed models and adapted them to other domains under zeroshot and self-training settings. there are two limitations that we would like to improve in future work. firstly, we use human-designed suppositions for each task, which is less automatic than a direct, zero-shot adaptation of the models. secondly, the self-training on some multi-class classification tasks is not as high as on binary nlu tasks, indicating the challenge of applying entailment models to multi-choice tasks. we would like to overcome this in the next step."
1188,"limitations while this work represents the first effort towards a perspectivist language resource for irony detection, it has to be noticed that the resource is monolingual (english). moreover, while we tried to maintain a fair balance in terms of demographic profile of the annotators, we limited the resource to five varieties of english tied to five countries, while leaving out other potential locations (e.g., new zealand or nigeria) or even more nuanced distinctions among language varieties. about the self-identified gender dimension, we are aware of the wider spectrum of genders. however, this information is provided by the annotators only in a binary form. another potential limitation is that, in the spirit of constructing a perspectivist corpus, we fully trusted the contributors. while the chosen crowdsourcing platform (prolific) is known for a high quality standard obtained e.g. by vetting its contributors, and we added a layer of checks through attention test questions, random noise in the annotation may still be present and undetected. while this paper mainly presents a new language resource, we also included the results of several analyses and validation experiments. in this direction, a number of dimensions are still unexplored, along which the data could be analysed. for instance, the genre difference between the sources of the data (reddit and twitter) and the distribution of different varieties of english were not yet explored."
1189,"limitation of the current static graph-based dialogue summarization methods and propose a static-dynamic graphbased dialogue summarization method (sdds). it contains two modules, a static graph module and a dynamic graph module. the former injects human prior into the summarization model and the latter encodes the implicit knowledge from a pretrained language model. by fusing these two kinds of graphs with a fine-grained 1×1 convolution, sdds could adaptively adjust the graph weight and learn the graph structure in an end-to-end learning fashion from the supervision of the summarization task. to validate the effectiveness of sdds, we conduct extensive experiments on three public dialogue summarization datasets (samsum, mediasum, and dialogsum) and observe significant improvement over strong baselines. we also carefully examine each key component and gives a detailed analysis of sdds for future research. limitations we discuss the limitations of sdds as follows: (1) although we propose a general framework for dialogue summarization by incorporating both static and dynamic graphs, we only adopt four static graphs to model the dialogue structure. since dialogue structure modeling is still an active research direction, we believe future advances would further benefit our framework. (2) despite the strong performance achieved by sdds across three dialogue summarization datasets, we use a pre-trained language model as the backbone of our proposed method, as a consequence, we can not go beyond the limitation of the maximum sequence length of the plm for the dialog summarization scenario like meeting summarization so it remains a future challenge for dialog summarization in the extremely long format. ethical consideration the dialogue data would inevitably contain private information about the interlocutors. we take careful consideration of this problem: (1) all data in our experiments are publicly available and anonymized by the original dataset provider. the license for samsum dataset is cc by-nc-nd 4.0 and for dialogsum mit license. for mediasum, it adheres to only-for-research-purpose guideline from the national public radio; (2) we do not use online user data to train our model and we would use an additional rule-based system to double-check whether our model output contains harmful and prejudicial discrimination when we use it for production. acknowlegement this work was supported by the national natural science foundation of china (nsfc grant no. t2293773 & no. 62122089 & no. 61876196), the national key research and development program of china (no. 2020aaa0106600), beijing outstanding young scientist program (no. bjjwzyjh012019100020098), and intelligent social governance platform, major innovation & planning interdisciplinary platform for the “double-first class” initiative, renmin university of china. rui yan is also supported by beijing academy of artificial intelligence (baai)."
1190,"limitations; the relevant cases are considered based on official citations as ground truth. however, there might be cases that were not mentioned by the judge (document writer) due to subjectivity involved in the common-law system; finding correct annotation for relevance is always a challenge for a domain like legal, where the number of documents is enormous."
1191,"limitations in this paper, we propose a simple model for prior case retrieval. as shown in experiments and results, the models could improve and score better. there is a big room for improvement. all the previously proposed approaches for pcr have calculated relevance as some form of lexical/semantic similarity between a case and its citations. however, cited case relevance may sometimes differ from lexical/semantic similarity. modeling the document in terms of events only partially addresses this. consequently, what is required is the inclusion of more legal information. we made an attempt towards that via experiments using rhetorical roles. similarly, one could use the information coming via statutes and laws since similar cases are likely to invoke similar statutes. another approach is learning representations using contrastive models that score relevant cases higher than non-relevant ones. in the future, we plan to investigate these approaches to improve the task of pcr. this paper considers a simple structure for an event as a tuple of predicates and arguments. however, more sophisticated formulations are possible, as outlined in the survey/tutorial by chen et al.. moreover, we are taking events in isolation and ignoring the sequential nature of events that help to form narratives. in the future, we would like to develop a model that captures a more sophisticated structure and sequential nature of events in the case. though we covered an extensive set of experiments for the proposed event-based matching technique, many more combinations can be experimented with to understand the role of events in legal documents. this unique finding of events missing from the legal literature would facilitate exploring new directions in the legal domain. in this paper, we evaluated only two datasets as we could not find any publicly available pcr datasets. however, in the future, if we can find more pcr datasets, we would like to evaluate them to see if the trends generalize over other legal corpora. ethical concerns this paper proposes a system for retrieving (recommending) relevant documents. the system is not involved in any decision-making process. the motivation for proposing the system is to augment legal experts rather than replace them. moreover, for training the system, we used publicly avail- able legal documents. we took steps to normalize documents concerning named entities to prevent a model from developing any known biases. to the best of our knowledge, we addressed any biases that the model might learn from the data."
1192,"limitations although our ulra outperforms all unsupervised baseline methods, there are still some limitations. the first limitation is that there is still a gap between the performance of our unsupervised method and that of some supervised methods. although our ulra can complete the aes task without label annotations, it is still worth exploring an unsupervised aes method whose performance is comparable to the state-of-the-art supervised method. the second limitation is that the essay encoder which adopted in our ulra (i.e., bert) is pretrained on the english-based corpora, and the essays for training is also written by english. thus, our ulra works mostly for english, which means a well-trained ulra model may fail to perform well on the essays written by other languages. an unsupervised aes system which supports multiple languages needs to be further explored. the third limitation is that it requires about 25g gpu memory for training, which may fail on devices with small gpu memory. a possible solution is to set a smaller batch size, but this may take longer time. however, the evaluation process only requires about 2g gpu memory, which can run in most of gpu devices, or even cpu devices."
1193,"limitations data and task limitation in this work, we analyze domain-label bias and apply our domaincontext calibration to english. we leave analysis and mitigation methods for multilingual tasks to future works. in experiments, we discuss calibration on classification tasks. the effect of domain-label bias could exist differently for open-end tasks like text generation. our analysis of domain-label bias also emphasizes more on the word-level bias. other types of biases associated with a domain, such as topics and genders, may also impact model prediction. we leave the diverse analysis to future works. due to budget limitations, we conduct experiments on a subset of the 24 reported datasets for gpt-3. one can evaluate all 24 datasets to get a complete picture with enough budget. model limitation for large language models, we only focus on the gpt models and only select roberta as the small-scale language model in experiments. future work could consider expanding to other model types, such as palm for large models and deberta for small models. access to the openai api for gpt-3 is also necessary for parts of our experiments. future work can consider experimenting with open-source llms like the opt-175b or bloom-176b model."
1194,"limitations in this section, we will analyze the limitations of our method. first, we introduce multiple knowledge sources to construct hkg, and encoding this knowledge through lm consumes more gpu resources. second, some useful knowledge may be removed when retrieving knowledge from key entities optimized by dictionary vocabulary. then, we experimentally demonstrate that the paraphrase descriptions are effective in improving the reasoning ability of the model, but due to resource constraints, we are unable to incorporate the paraphrases of all entities into hkg. finally, our method uses the simpler transe algorithm when optimizing the knowledge representation using krl due to gpu constraints, which may not be able to model the complex relationships in hkg well."
1195,"limitations firstly, due to the huge cost of large-scale plms, this paper only employs the t5-base as the backbone plm in our experiments, therefore only limited analysis on the effect of model scale is presented. however, we believe a larger model will benefit our method by providing better language understanding and generation abilities. secondly, the synthesized canonical utterances need manually designed synchronous grammars, which are used to guide raas with knowledge about semantic representation language. although most few-shot/zero-shot semantic parsing studies also rely on synchronous grammars, we leave how to model semantic representations without grammars as an open problem for future work. acknowledgments we sincerely thank the reviewers for their insightful comments and valuable suggestions. this research work is supported by the national natural science foundation of china under grants no. u1936207, 62122077 and 62106251. furthermore, this research was supported by meituan."
1196,"limitations despite the competitive performance, there are several limitations of this work: (1) as discussed in section 6.1, the generation performance relies on the parser performance, which is strong enough for english but still less satisfactory for other languages. dedicated methods need to be considered to compensate for the weak parser performance if we want to extend our method to more languages. (2) in this work, we consider two nlg tasks with semantic equivalence to testify if the proposed method can convey the source semantics accurately by following the target syntactic grammar. other tasks such as summarization and dialogue generation can also be tested, where the semantics are not equivalent between the source and target. (3) to train the neural decoder parallelly, we break down the source-target dataset into a triple set. however, the global dependency of the syntax parse tree is not considered, which can deteriorate generation performance. (4) due to the recursive encoding of the syntax contexts, our model’s inference speed is approximately half that of the seq2seq counterpart (appendix e). (5) future work should include experiments on large language models (brown et al., 2020; openai, 2023; zeng et al., 2022; touvron et al., 2023; taori et al., 2023). to further demonstrate the effectiveness of our method beyond pretrained language models."
1197,"limitation the grm model still has some limitations. even though our model brings some performance improvement to the contextual word embedding model (i.e., bert), this improvement is relatively small compared to the static model. in some cases, grm may hurt the performance of bert slightly, because the primary objective of context-based word embedding models is to infer word meaning from contexts. the approach set forward in our study enhances their initial input word embeddings through word formation, and the benefits brought by this method are modest. how to efficiently improve the performance of contextual word embedding models when faced with oov words remains to be explored."
1198,"limitations in this paper, we focused on english comics only because of their ease of availability. although we have not experimented with non-english text, we expect the proposed model to work well in multilingual settings if we replace gpt-2 decoder with other decoders like bloom (scao et al., 2022)."
1199,"limitations efficiency. to get the optimal performance from pairranker, one may need to call the model o(n2) times for getting the full matrix, thus resulting in a much less efficient solution. we attempted to resolve this limitation by proposing to use multiple rounds of bubble sort methods to reduce the number of inferences needed, and we find it works pretty well. we also want to argue that although the number of inferences can be large for obtaining the best performance with pairranker, those inferences can be executed in parallel because they are totally independent. human evaluation. we agree that automatic metrics have limitations. human evaluation could provide us with more reliable and comprehensive evaluation results. however, due to the number of models as well as the amounts of generation candidates, we cannot afford large-scale human evaluation. we argue that our use of chatgpt for evaluation is a good alternative, according to recent studies. also, we would like to highlight that we show the ground truths when using chatgpt to do pairwise comparisions, which is quite informative than the common practice. *ethical statement this work fully complies with the acl"
1200,"limitations although dcg achieves significant improvements compared with existing baselines, there are still avenues to be explored in future research. (1) dcg in this paper focuses on the compositional generalization for multi-attribute on controllable dialogue generation. we hope to extend the method to other generative tasks, including but not limited to dialogue summarization and story generation. (2) in this paper, we explored the control of coarsegrained discrete attributes and the control of finegrained ones separately, and we intend to study the combination of these two attributes in future research."
1201,"limitations we facilitate fair comparisons and realistic evaluations of recent wsl approaches. however, our study is not exhaustive and has the following limitations. first, it may be possible to perform model selection by utilizing prior knowledge about the dataset. for example, if the noise ratio (the proportion of incorrect labels in the training set) is known in advance, it can be used to determine (a subset of) hyperparameters (han et al., 2018; li et al., 2020). in this case, certain wsl approaches may still work without access to extra clean data. second, in this paper we concentrate on tasks in english where strong plms are available. as we have shown in section 6, training them on a small amount of data is sufficient for generalization. for low-resource languages where no plms are available, training may not be that effective, and wsl methods may achieve higher performance. third, we experiment with datasets from the established wrench benchmark, where the weak labels are frequently assigned by simple rules like as regular expressions (see appendix b for examples). however, in a broader context, weak supervision can have different forms. for example, smith et al. (2022) generates weak labels through large language models. zhou et al. (2022) use hyper-link information as weak labels for passage retrieval. we have not extended our research to more diverse types of weak labels. despite the above limitations, however, we identify the pitfalls in the existing evaluation of current wsl methods and demonstrate simple yet strong baselines through comprehensive experiments on a wide range of tasks."
1202,"limitations in this work, we focus on debiasing the gender bias for plms. in the future, we will try to mitigate social biases other than gender, such as race and religion. in addition, we also plan to extend our debiasing method to more language models, such as natural language generation (nlg) models."
1203,"limitations our method primarily focuses on operation-level specifications, while there are real-world use cases with other specifications. moreover, our method of creating cqas can only be scaled to all python codes that involve heavy api usage. however, if a similar code knowledge graph generator of another language is developed, our method can also be scaled to the corresponding language. our method is also limited in identifying specifications missing from the nld, suggesting potential future work to create cqs about specifications “mentioned but not specified enough” in the nld. ethical concerns one concern about the data is the issue of copyright. liu et al. (2021) have checked the data policy of all 20 kaggle competitions, in which none has copyright issues. furthermore, they have contacted kaggle’s administrator and have made sure that the dataset collection procedure did not violate the platform’s policy. we also check the license of open-source apis when collecting documentation and make sure that there is no concern about copyright issues. another concern about the data is that it might include privacy data. again, we think that our data has a minimum risk of leakage of data with privacy concerns since we only collect data from the 20 kaggle competitions where there is no concern of privacy data. the api documentation also has the minimum risk of containing data with privacy concerns."
1204,"limitations our approach has proven to be superior to previous methods on multiple public benchmark datasets. however, one major disadvantage of the table filling method is the increased training time and memory usage. the computational resources are required for the 2d table representation of word-pair relations for constructing and storing the table. in comparison, using a sequence representation as input could be generally more efficient. our approach also faces the computational challenge."
1205,"limitations we acknowledge that our benchmark dataset does not cover all the existing ambiguities and that ambiguities related to fairness do not cover all the possibilities. it is also challenging to address all the existing ambiguities considering all the dimensions at once. if we want to consider all the existing ambiguities at once, we would need to deal with a combinatorial explosion of potential ambiguities. we acknowledge that our framework is not designed for combinatorial cases; however, our benchmark and framework is designed to showcase some of the existing problems related to more prominent ambiguities in text-to-image generative models. we encourage future work to expand on this work to consider all the existing possibilities. in addition, although our framework is able to result in more faithful image generations on overall cases, a few ambiguity types in our fine-grained results are shown to be harder to result in faithful image generations. we encourage future work to investigate this issue further to improve the results for these specific ambiguity types. ethical considerations in this work, we study and propose solutions to resolve existing ambiguities in prompts given to text-to-image generative models. in addition to resolving ambiguities in prompts, this work not only frames and analyzes fairness from a new and different perspective, but it also results in more faithful image generations aligned with end user intention. these aspects can contribute to numerous positive impacts to the research community. not only one can generate more diverse images through disambiguating fairness type ambiguities, but our framework can also improve user satisfaction by generating aligned images to end user’s intention despite existing ambiguities in the provided prompts. resolving ambiguities can also avoid spread of misinformation and development of fallacies. despite the aforementioned positive impacts, we also acknowledge the limitations associated with this work. we acknowledge that our benchmark dataset is just a very small sample of different types of ambiguous prompts that can be provided to a system. in addition, for the fairness type ambiguities, we only consider gender (male vs female), skin color (dark vs light), and age (young vs old). we acknowledge that these are only a limited number of characteristics that can represent identity of an individual and that we do not cover all the cases possible. we agree that we do not cover all the cases possible; however, our intent is to showcase a few examples through our benchmark (tab) and highlight existing flaws associated with these systems encountering ambiguous prompts. in our experiments, we also utilize human annotators. we ensure to provide appropriate guidelines with a proper compensation to our workers (around 12$ per hour). we also utilize master workers based in the united states with proper expertise (completion of more than 1000 hits with an acceptance rate above 85%). in addition, we provide the workers the opportunity to raise any concerns about our task. based on the feedback, we believe that the task and the pay was satisfactory to the workers. we hope that our study can provide valuable insights to the research community with the positive implications out-weighting its limitations. we also open-source our benchmark dataset for the community to benefit from our work. as future work, researchers can investigate and propose better alternatives than our proposed framework for resolving ambiguities in text-to-image generative models along with extension of our work to semantic ambiguities in addition to the ones studied in this paper. our benchmark dataset can also serve as a valuable resource for research in commonsense reasoning studies in text-to-image generative models which is less explored in our current work. we provide information in our benchmark dataset (whether an interpretation is commonsensical or not) which can be accessible to interested researchers in this area."
1206,"limitations. in addition, we performed a quantitative analysis in terms of efficiency and offered certain suggestions about method selections of open domain question answering. finally, we discussed possible open challenges and potential future directions of efficient odqa models."
1207,"limitations one of the limitations of the current study is the lack of annotated data for all languages. this is also the case of machine translation for which data could only be found for kashmiri, sorani and sindhi, while other languages do not have much parallel data yet. on the other hand, the notion of noisy data is limited to the replacement of the missing characters in a script when compared to another one, i.e. that of the dominant language. as an ablation study, injecting other types of noise, beyond those discussed in this paper, may improve the performance of the models to tackle not only script normalization but several related tasks such as spelling error correction and may also increase the robustness of the models for morphologically rich languages or languages with versatile word boundaries using zwnj. although we did our best to filter out code-switched data in the corpora, our datasets may contain data in other languages (in perso-arabic scripts). in future work, we would like to apply our approach to other scripts and languages in bilingual communities. we also suggest evaluating the impact of script normalization on more downstream tasks, especially transliteration and tokenization."
1208,"limitations the conditional independence assumptions are a limitation for the applicability of our multiset tagging model. for example, the independence assumptions are too strong to apply it to natural language generation tasks such as summarization. from a technical point of view, the independence assumptions are important to be able to induce the latent assignment of output tokens to multisets efficiently. future work may design multiset tagging methods that make fewer independence assumptions. while our method for predicting permutations is comparatively fast and only has a memory requirement of o(n3), inference on long sequences, e.g. with more than 100 tokens, remains somewhat slow. in future work, we plan to investigate other approximate inference techniques like local search and dual decomposition. regarding the importance of trees for compositional generalization, our model has no explicit structural inductive bias towards trees. however, we do not exclude that the pretrained roberta model that we use as a component implicitly captures trees or tree-like structures to a certain degree."
1209,"limitations in this work, we propose managers that allow adaptive aggregation of uni-modal layer representations in each cross-modal layer. inevitably, aaue managers significantly improve performance which slightly increasing the computational budget, as we detailed discussed in appendix c. this needs to be further optimized in the future. analysis and optimization are also needed for the other types of managers as shown in appendix d. moreover, as shown in figure 5, the performance of managertower first increases gradually with the number of uni-modal representations, but then stops increasing and even decreases when the number of uni-modal representations exceeds 6. how to obtain better managertower performance using a lower computational budget while utilizing more insights of uni-modal experts, especially when scaling the model, e.g., 24-layer clip-vit l-224/16 and 24-layer robertalarge, is a question worth further exploration. for example, designing reasonable sparse activation functions for managers in managertower, instead of simple top-n or top-p sampling (which did not work well in our preliminary experiments)."
1210,"limitations in this work, we focus on causal language modeling. it needs additional efforts to integrate the proposed methods into bidirectional attention, such as masked language modeling (devlin et al., 2019). moreover, xpos introduces about 6% inference cost compared with absolute position embeddings, although it accelerates training convergence."
1211,"limitations one limitation of our survey work is that it is focused on the intersection of mathematical reasoning and deep learning over the past decade, which may not encompass the entire field and its history. additionally, our evaluation of existing benchmarks and methods is based on a curated set of papers and may not fully represent the state of the art in the field. furthermore, due to the fast-paced nature of the field, our survey may not reflect the latest developments and advancements which may have come out close to or after the survey was conducted. despite these limitations, our survey still provides a valuable overview of the current state and key trends in the field of mathematical reasoning and deep learning, and can serve as a valuable resource for researchers and practitioners working in this field. broader impact our survey paper on the intersection of mathematical reasoning and deep learning has the potential to significantly impact the field of artificial intelligence. by providing a comprehensive overview of the key tasks, datasets, and methods that have been developed in the past decade, we give researchers and practitioners a clear understanding of the current state-of-the-art and help them make informed decisions about their own research. additionally, by evaluating existing benchmarks and methods and discussing future research directions, we aim to identify gaps in the current state of the art and guide future research and development efforts towards more advanced and effective mathematical reasoning systems. overall, our survey has the potential to contribute to the advancement of mathematical reasoning and deep learning, and have a profound impact on machine learning and natural language processing."
1212,"limitation we verify our method mainly based on the recent robust vlp model albef (li et al., 2021). evaluating it more broadly by incorporating it into other vlp models can further highlight our contribution. given the solid theoretical foundation of our method, the main"
1213,"limitations the proposed t2d dataset has several limitations, which could be addressed in future work. first, it only considers and collects language instructions for the floor plan domain. future work could extend this language-guided design generation task to other design domains such as documents, mobile uis, etc. second, it is limited in the scope of languages where we only collect instructions written in english. future work could assess the generalizability of the t2d dataset to other languages. third, although generating floor plan designs from languages exhibit diversity, we do not consider improving generation diversity at this moment. future works could consider building frameworks that specifically aim at design diversity."
1214,"limitations although our proposed multiemo framework has achieved state-of-the-art performances on both iemocap and meld, there are some limitations with this work: • our proposed visual feature extractor visextnet does not distinguish between speakers and irrelevant people in the scene, which can be problematic in some scenarios. for instance, one scene in meld is the cafeteria, where a lot of background actors sit and drink coffee. the facial expressions of these background people have no impact on the emotion of the speaker since they do not participant in the conversation. however, visextnet captures visual features of everyone appeared in the cafeteria with no differentiation, which may lead to a wrong comprehension of the speaker’s emotional tendency due to the effects of facial expressions from irrelevant people. we plan to explore effective ways to distinguish between interlocutors and irrelevant people in the scene in our future work. • the effects of hyperparameters in the swfc loss (temperature parameter τ , sample-weight parameter α and focusing parameter γ) on model performances have not been fully studied, which will be thoroughly analyzed in our future research. • due to the class imbalanced issue with meld, the swfc loss requires a large batch size on meld to ensure that for each training sample there exists at least one positive pair in the batch, which can be computationally expensive. we will investigate effective approaches to tackle this challenge in our future research. • even though multiemo has achieved remarkable improvements in minority emotion categories, the performances of multiemo in minority emotions are still worse than majority classes. how to further improve performances in low-resource emotion classes will be explored in the future."
1215,"limitations in this work, we focused our exploration of lsls on the encoder. although we ran some initial explorations on the decoder side, further investigation is needed. another venue for research is how lsls affect language expansion. since our approach tries to limit the language-specific weights to just a few layers, in theory, it should be possible to add new languages by only expanding and training the lsls. however, blindly doing so might not work well and the interactions between languages from different families needs further studying. lastly, it is unclear whether our argmax approach to selecting where to place lsls is optimal, how dataset dependent it is, and if there exist alternative approaches that can lead to better results. the fact that it does not take model complexity (i.e., model size) into account can be a disadvantage in practice."
1216,"limitations our propose annotation strategy can be applied to labeling other mrc problems, no matter situated comprehension ones or not. however, when generalizing to other problems other than personality prediction we studied here, the accuracy of the user notes may vary with the difficulty of tasks. additional human verification on the correctness of notes like in our section 4.3 need to be conducted. our unsupervised training technique does not support the longformer reader with character history (char-hist longformer) yet. therefore, the improvement from unsupervised training for our this model is smaller. while longformer is common in benchmarking for long story understanding tasks. there are other families of models (rae et al., 2020; izacard and grave, 2021; ainslie et al., 2020; xiong et al., 2021; pang et al., 2022) handling long text encoding. we leave the comparison with these models to future work. potential risks like the other work that based on the similar set of books (bamman et al., 2019; bamman, 2020; vishnubhotla et al., 2022; thai et al., 2022), the classic literature may be limited by the time of writing, thus raise fairness considerations. however, please note that our dataset construction strategy is not limited to these books, but can work with any books on weread to create a sampled book set without such biases. the main reason we stick with the current list of books is for reproducibility since they are publicly available."
1217,"limitations in style transfer, content preservation and style transfer are adversarial. long texts have richer contents and more abstract stylistic features. we also notice that content preservation is the main disadvantage of storytrans in automatics evaluation results. case studies also indicate that storytrans can maintain some entities and the relations between entities. however, strong discourse-level style transfer ability endangered content preservation. in contrast, baselines such as style transformer have better content preservation but hardly transfer the style. we believe that storytrans is still a good starting point for this important and challenging task. during preliminary experiments, we also manually inspected multiple author styles besides shakespeare, such as mark twain. however, we found that their styles are not as obvious as shakespeare, as shown in the following example. therefore, we only selected authors with relatively distinct personal styles for our transfer experiments. in future work, we will expand our research and choose more authors with distinct styles for style transfer. for example, the style distinction between the following examples is not readily apparent. • everyday story in our datatset: ashley wanted to be a unicorn for halloween. she looked all over for a unicorn costume. she wasn’t able to find one. • ""a double barrelled detective story"" by mark twain: you will go and find him. i have known his hiding-place for eleven years; it cost me five years and more of inquiry."
1218,"limitations an important limitation of our approach vawi is the need for extracting visually-hungry words (vhwords) as the trigger to inject visual knowledge into plms. in real-world applications, it is hard to obtain the annotations of vh-words. therefore, we propose three vh-words extraction strategies. however, the three strategies may be not always proper for all nlp tasks, and we rely on the experimental results to select the best one among them. besides, we adopt the text encoder of clip as the vl-ptm for generating the visually-aligned representation. as a pre-trained model, clip also may contain biases learned from the pre-training corpus, which may result in improper biased prediction on some nlp tasks."
1219,"limitations due to the structure of meetingqa, the answers to questions asked by participants (if any) are present in the transcript itself, making it an extractive task. therefore, we do not extensively explore the use of generative models since the predictions do not stick to the sentences in the transcript and could possibly include hallucinations. however, we aim to mitigate hallucinations by using instruction-tuned generative models with suitably designed instructions and enforce a strict exact match criteria for filtering any possible hallucinations. future work can explore how to adapt or evaluate non-instruction-tuned generative models on this task and better identify hallucinations with a more relaxed filtering to improve performance. we also do not report zero-shot performance of instructgpt (ouyang et al., 2022) as these models are not freely accessible. additionally, we use a simple multi-span qa adaptation technique from segal et al. (2020), but predicting answer spans by classifying each token can be difficult to train leading to slightly lower performance (discussed in section 4.1). we hope our dataset provides additional motivation for future work on multi-span qa. finally, meetingqa only comprises of publicly available meeting transcripts in english, but our methodology of data collection and model training (using multilingual variants) should still be applicable for other languages in future work. ethical considerations the human participants in our work were recruited by an external crowd-sourcing company that ensured annotators provided informed consent, were given fair compensation, and no personally identifiable information (pii) was collected or released. we use existing publicly available meeting transcripts collected by the ami project (carletta et al., 2005) in controlled scenarios and filtered for offensive/toxic content. we also conducted manual inspection of a random sample from annotated transcripts and did not find any toxic content or pii. furthermore, the collected data and experiments are conducted in english and we do not claim generalization of our findings across languages. given the broad nature of meetings, the content can fall into a number of domains, of which only a few are represented in the ami corpus. therefore, we do not expect models trained on meetingqa to generalize to certain domains such as judicial, ethi- cal review, congressional proceedings, etc. which involve specific jargon and rules of engagement."
1220,"limitation dect explores how to adapt black-box ptms on downstream tasks. as we show in section 4.4, our method is not comparable to fine-tuning on hard tasks with increased data points. moreover, we only focus on classification tasks in this work and do not testify dect on free-form generation tasks. in the future, we will work toward more general maas adaptation strategies across tasks. ethical statement as large language models are getting more and more popular in nlp research and application, dect provides a cost-efficient way to adapt these large models. however, we need also to be cautious about the improper adaptation of large language models, such as generating toxic and biased speeches."
1221,limitations section for additional
1222,"limitations of existing methods and providing a more comprehensive view of a model’s predictions. limitations our work shows that crest is a suitable framework for generating high-quality counterfactuals and producing plausible rationales, and we hope that crest motivates new research to develop more robust and interpretable models. we note, however, two main limitations in our framework. first, our counterfactuals are the result of a large language model (t5), and as such, they may carry all the limitations within these models. therefore, caution should be exercised when making statements about the quality of counterfactuals beyond the metrics reported in this paper, especially if these statements might have societal impacts. second, crest relies on a rationalizer to produce highlights-based explanations, and therefore it is limited in its ability to answer interpretability questions that go beyond the tokens of the factual or counterfactual input."
1223,"limitations the annotation task we proposed in this work, i.e., detecting factual errors in summaries and providing human demonstrations and feedback for correcting the identified errors, can be complicated and timeconsuming. during our recruiting phase for mturk annotators, we found that the ratio of annotators who were qualified after finishing the qualification test was relatively low. therefore, it can be difficult to scale up the annotated dataset given the time and budget limitations. as a result, our dataset is of a relatively small scale and we only used one summarization dataset (xsum) and one base summarization model (pegasus). in this work, we view summary factual consistency as an example of user-expected quality to study leveraging natural language feedback for aligning system outputs with user preferences. however, user preferences can be diverse and personal and some user-expected output quality will be less well-defined and objective than summary factual consistency, which further increases the difficulty and ambiguity of data annotation and model evaluation. therefore, it can be challenging to directly apply the methods we proposed in this work to such subjective quality aspects, and we leave it for future work to explore generalizing our methods to more diverse user expectations and preferences."
1224,"limitations our method has the following limitations: datasets: in multi-modal pre-training, we rely on downstream datasets to evaluate the performance of pre-trained models. the commonly used entity extraction datasets are relatively small and lack diversity, so the proposed method may not generalize well to real word scenarios. lack of image modality: in layoutmask, we focus on text-layout interactions, leaving the image modality unexplored. however, documents in the real world contain many elements that can not be described by text and layout modalities, like figures and lines, so incorporating image modality is important in building a universal multi-modal pre-training model for document understanding."
1225,"limitations first, although our approach and existing methods for controllable text generation can improve the constraint accuracies, they are currently unable to achieve 100% accuracies in the vast majority of aspects (e.g., sentiment or topic). this makes them not yet applicable in scenarios with requirements of 100% control fulfillment. second, there is still a gap between the automatic and human evaluation of text generation, which makes there a trade-off between precision and efficiency in the evaluation of controllable text generation. third, although our approach reduces the mutual interference of plugins so that multiple plugins can be combined at a relatively small cost (a decrease in constraint accuracy), this cost will not be zero, which puts an upper limit on the number of plugins can be applied simultaneously. fortunately, for controllable text generation, the number of controls applied simultaneously is generally not too large (e.g., four or five aspects)."
1226,"limitations this work is motivated by the intuition that a mention is more likely to refer to an entity that occurs shortly earlier and refers to a high frequency entity but has not recently used. for the latter pattern, we show examples of topic switching to explain why these phenomena happened, but we have not found a rigorous linguistic explanation to support this finding. we provide empirical results on four benchmarks plus a book. however, the scarcity of long-doc cr benchmarks hinders us from verifying on a larger scale. our major contribution is a new cache design, but we also find the cache design becomes less matter when using a huge cache. nvidia a100 has 80g memory, which means it can handle a document of 100,000 words with a conventional cr model. as the gpu becomes larger and cheaper, the importance of studying cache design is weaker."
1227,"limitations this study aims to detect signs of anorexia, selfharm, and depression in users of social media environments through a double-domain adaptation of a language model. this study presents some limitations, mainly because these datasets are observational studies and we do not have access to the personal and medical information that is often considered in risk assessment studies. for example, we cannot discard that some users who publicly expressed that they have been diagnosed with anorexia are actually non-anorexia cases. however, the identification of positive users from selfexpressions of diagnosis is a common practice in this area (coppersmith et al., 2014), and the test collections built in this way are regarded as solid experimental benchmarks. there are also some limitations given by the nature of the data, as the users in these datasets might differ from users at risk who do not have exposure to social media (e.g., elderly people or individuals who do not have an online account or decided to not make their profiles public)."
1228,"limitations though our approach demonstrates better performances than the baseline models, how to design a good code-format prompt has not been fully inspected. besides, we mainly conduct experiments on the black-box gpt-3 and codex models but they are not open-sourced and querying the gpt-3 model cost the economic budget. and the use of llms may bring environmental pollution. another limitation of our approach is that the code-llms mainly trained on programming language datasets with english annotations. exploring our model on non-english datasets (like chinese datasets) is the future work."
1229,"limitations as shown in table 3, the results demonstrate the effectiveness of our proposed eft, but the performance of the unseen subset is still limited by comparing it with the performance of seen subset, which suggests plenty of room for improvement. data augmentation or generalization methods based on semi-supervised methods could be effective to solve the problem in the future."
1230,"limitations there exist some limitations in our work. livechat is a chinese-originated dataset involving unique cultures and abundant replying styles. however, this intensifies the difficulty of fully understand- ing the content of this dataset. fortunately, the same data construction pipeline can be applied to streaming platforms of other languages, like tiktok. and currently, our livechat is only sourced from 351 streamers on douyin, not sufficient to train a general chatbot. we believe that livechat helps get one’s foot in the door to the wonderful and diversified live scenarios and a dialogue model pre-trained on the considerable amount of videosourced dialogue data among cross-platforms is promising. besides, livechat contains some noisy spoken language segments that are not easy to read after transcribing from the asr tool. the upper bound data quality is limited by such third-party tools. the future work to concatenate such text segments to restore the content of the original expression by streamers is highly anticipated. as for the dialogue-matching method, we simply implement a combination of bow and bert for semantic matching, which needs further optimization. other limitations from the training perspective can also be highlighted. for example, contextual background information is not considered in our modeling. that includes history dialogues in multiturn settings and information from other modalities, like the streamer eating in front of the camera. in addition, we have not explored enough of our annotated basic profiles. in our primary experiments, we found that directly adding basic information such as age, gender, location, and other room information has limited influence on the model performance. we account for the fact that these basic profiles have limited connections with reply styles and contents in livechat. also, note that we remove the repetition part of a streamer’s response before training, while it is useful to maintain this pattern in practical application. ethical consideration this work presents livechat, a free and open chinese dataset for the research community to study personalized open-domain dialogue generation and addressee recognition. our dataset contains wellprocessed dialogues, and annotations (basic profiles and text profiles). data privacy the original live-streaming clips and streamers’ profiles of livechat are collected from douyin, one of the largest chinese livebroadcasting platforms. similar to previous dialogue data from reddit (mazaré et al., 2018) and weibo (qian et al., 2021), livechat is an open- domain dialogue dataset that crossover multiple topics and users. since all streamers must comply with platform rules during their online live streaming under the strict supervision of the chinese government, their topics do not contain any pornographic, violent, reactionary, or discriminatory statements. besides, due to the property of streaming, historically broadcast videos are no longer available when finished. therefore it is not traceable from livechat to the identity of real streamers. moreover, we clean the raw data with transformation, anonymization, and deletion to ensure there is no disclosure of private information and the identity of the streamers or audiences can not be inferred from it. thus, all the collected data (including persona profiles) is publicly available and does not contain any private information of streamers and audiences, such as emails, phone numbers, and real user names. although we collect the age and location information, in our basic profile, the age is expressed as an interval range that doesn’t represent the real age of the streamers, and the location only contains the province’s information. besides, all the attributes of our basic profiles are re-indexed as numbers in the final released dataset. thus, both our raw data and persona profiles do not create additional ethical risks. moreover, we are sure that all the collected data is consistent with the platform usage rules and protocols. livechat will only be allowed to be used for academic research. at last, our construction of livechat was approved by an internal review board (irb). annotators in terms of basic profile annotation and manual evaluation, all the annotators are chinese undergraduates specifically responsible for annotation work in our institution. they are informed of the ongoing research and well known the way the curated data will be used. all the annotated information and evaluation results do not contain any private information."
1231,"limitations as we use only a small number of language pairs, it is not clear how general our"
1232,"limitations the current method works with a large prompt search space t , which means a tremendous number of inference api calls are required. though figure 2 shows that the average cost of finding a lottery prompt for each instance is low, the searching process is highly randomized and there is no guarantee of the performance of searched prompts over the test dataset. therefore, finding strong prompts over the training set can still be laborious. how to use plm inference calls more efficiently and leverage the generalization ability of t ∗ within a reasonable cost is of future research interest. our acceleration strategy can be found in appendix b. another aspect is that not all strong prompts are interpretable as presented in 2. while recently emerged larger models like chatgpt demonstrate superb language understanding ability and can almost always answer yes or no questions correctly given a human-interpretable prompt. this gap observed between small plms like roberta and large language models like chatgpt is yet another interesting research topic. ethical considerations this work shows that with proper plain textual prompts, instance-level desired results can be prompted from plms. this inherent feature of plms means attacks can be launched to produce rude or discriminated words. on the other hand, however, we believe it can also be a technique used for debiasing a plm. overall, this effect depends on the intention of the users and the pre-training corpus of the corresponding plms. the analysis of this study can be used to facilitate the community to develop more specifications for the rational use of plms (especially the super-large ones), and more approaches to effectively prevent potential ethical issues. for example, we can use this technique to analyze which outputs that may have ethical issues are easily triggered by which contexts (prompts) and develop a set of intervention methods to make these tokens unavailable for output."
1233,"limitations our work has the following limitations. first, our proposed facialmmt approach is a two-stage framework that is not fully end-to-end. we plan to propose an end-to-end framework in the future, which integrates face sequence extraction and multimodal emotion recognition in a joint learning manner. second, this work primarily focuses on the visual modality, and has not yet delved into other aspects of the mermc task. therefore, in the future, we plan to leverage the extracted face sequences to explore better cross-modal alignment and multimodal fusion mechanisms to improve the performance of the mermc task."
1234,"limitations as previously mentioned, teast maps relations onto the corresponding archimedean spiral timeline and transforms the quadruples completion to 3th-order tensor factorization. it is required to store the values and this slightly increase the space requirement and training time in the embedding learning process. among all the baselines, tcomplex, tntcomplex and telm are all tensor factorization based models. table 4 compares training time and space requirement between our model and baselines on icews14. tcomplex is the smallest model and takes the minimum training time. compared with tcomplex, our model is about 4.6% bigger than tcomplex, and takes 21.4% more training time."
1235,"limitations as exciting as this work is, it does have several limitations and a lot of opportunities for future improvement. how to scale? we demonstrated our method in a highly constrained environment with very limited concepts, whereas humans are able to pick up new concepts in the noisy world with few shots. how could these representations learned in a clean environment be useful in real world? would comparative learning still be useful outside of the classroom? we followed the baby step of progressive alignment and hoping that establishing a set of clean base knowledge, can ease the acquisition of future more complex concepts through comparisons with existing knowledge, analogy and hierarchical abstraction. this hypothesis remains to be investigated in the future. what about other words? some concepts can be learned through just visual inputs, like color, whereas other concepts require grounding through different sensory types or modalities, like “hot”, “loud” and “fast”. even more concepts are built upon existing words through abstraction and generalization, e.g. “philosophy”, “momentum”. comparisons can still be used to ground these words, but input to these comparisons could vary from data modalities to computation methods, to abstract representations. we leave these for future work. how to put words into sentences? this work only focused on the grounding of individual words into visual representations, whereas sentence syntax, grammar, and article structure are yet to be learned. for future work, we could treat language as its own modality, and learn the structure through comparisons as well. just like in an elementary linguistic class, a teacher would list out several examples “i shower”/“you shower”/“he shower”. humans can learn grammar through what’s changing and what’s constant. this could be an interesting next step to look into. who can offer the supervision? as mentioned at the beginning, human language acquisition is a highly supervised learning process. babies are rarely inventing new words but learning how adults label objects through generations of conventions. a classroom setting with highly structured curriculum and clean dataset takes a lot of curriculum design and heavy annotation. this is the cost that humans are willing to spend in order to educate human children from kindergarten to college. maybe it is a fair price that we have to pay in order for artificial intelligence to learn what we want them to learn. about the current work itself, there are several constraints that we are limited to. first of all, due to limited computation resources and data size, we had to take a shortcut by using a pre-trained clip embedding as a starting point for our models. in theory, we could and would love to train our models from scratch, just like how a new born would learn their first language. a dataset like toys-200 (stojanov et al., 2019) could mimic the process of babies interacting with the objects, get a 360 view and help build 3d mental representations. second of all, like many other continual learning methods, an unbounded memory space is an unrealistic assumption. as more concepts are learned, the memory space would grow fast, so as the search time. an interesting next step could be to reorganize the memory according to the association distances and hierarchical structures. lastly, our work aims at proposing a novel language acquisition definition and the comparative continual learning method. we used somewhat simple model architecture and image generation models for proof-of-concept demonstration on the method. more sophisticated model architecture and training can be switched for different input modalities and applications. listed above are several major limitations and future directions based on current work. we are more than happy to take constructive suggestions and criticism to help improve this and future works."
1236,"limitations diversity of corpora while the newly introduced lexfiles corpus is significantly more diverse compared to the pile of law corpus of henderson* et al. (2022), it is still an english-only corpus covering only 6 legal systems (eu, uk, coe, us, india, canada). despite, the fact that we can train better models (lexlms) and evaluate these models across these corpora, in future work, we should extend our analysis to cover even more languages and legal systems, and a higher granularity in the labeling of legal fields within these systems. not only will this help support the inclusion of other legal traditions but also adding more linguistic and cultural diversity will help us better understand the robustness of existing methods. similarly, the newly introduced legallama benchmark consists of 8 sub-tasks targeting eu, echr, us, and canadian jurisdictions in a very controlled setting; where examples were automatically extracted. while on this benchmark, legaloriented plms has demonstrated a significant degree of “understanding"" of legal language and legal topics, this benchmark should be further expanded with more sub-tasks to evaluate the acquaintance of legal knowledge across more legal systems and topics, and possibly cleansed from both very easy and unsolvable examples. model considerations in this work, we consider encoder-only (bert-like) models up to approx. 350m parameters, while recent work on the development of large language models (llms) (kaplan et al., 2020; brown et al., 2020; hoffmann et al., 2022; chowdhery et al., 2022) is mainly targeting billion-parameter-sized models (10-100bs of parameters) that usually follow a decoder-only, e.g., gpt (radford and narasimhan, 2018), or encoder-decoder, e.g., t5 (raffel et al., 2020), architecture. moreover, new paradigms of training plms have been introduced, such as instructionbased finetuning (wei et al., 2021), and alignment via reinforcement learning from human feedback (rlhf) (stiennon et al., 2020; ouyang et al., 2022). latest gpt models (ouyang et al., 2022) have recently shown significant zero-shot progress on law-related tasks such as bar examination question answering (katz et al., 2023). thus, future work should follow the most recent advances by pre-training much larger auto-regressive gtp-like models that seem to lead to emergent zero-shot and few-shot capabilities. evaluation considerations in section 3, we present how we account for and evaluate multitoken expressions (terms) on the legallama benchmark; we are open to ideas on how we should possibly improve the current approach to provide a fairer and more robust evaluation framework across all models. similarly, in section 4.4, we fine-tune all examined plms for a single epoch to avoid extreme over-reparameterization and better estimate how model’s knowledge affects convergence and performance. nonetheless, there are possibly better approaches to control for these aspects, e.g., adapter-based (rücklé et al., 2021) finetuning, or other approaches, such as lora (hu et al., 2022). beyond performance while we consider a multi-facet analysis, we do not cover other interesting dimensions that should also be explored, especially since law is a very sensitive application domain; for instance trustworthiness-related topics, such as model interpretability (chalkidis et al., 2021b; malik et al., 2021), and fairness (chalkidis et al., 2022b). future work can build from the results reported herein to explore these important topics."
1237,"limitations we have demonstrated that across three standard re datasets, llms achieve sota results. in particular, gpt-3 yields such performance even given only 10s of training sample for in-context learning. we then showed that we can similarly achieve sota performance with the much smaller (and open-source) flan t5 (large) model, when trained using cot generations produced by gpt-3. we also highlighted key challenges for evaluation in this setting. but there are important limitations to these contributions. first, here we considered three standard re datasets with binary relations but— as we discussed—we excluded more complex re datasets. for example, we did not consider corpora containing n-ary relations between entities (taboureau et al., 2010). we were also unable to run experiments on datasets with lengthy texts and a large number of relations, such as docred (yao et al., 2021), due to the necessary prompt lengths for such inputs. second, while we found that cot-style explanations generated by gpt-3 can be fruitfully used as additional supervision to fine-tune smaller language models, we made no attempt to evaluate the quality of these generated explanations which may have an impact on the model performance. third, we did not fine-tune gpt-3 on the re datasets, mainly due to the cost of doing so. it is likely that a fine-tuned gpt-3 would yield performance superior to the results we achieved with flan t5 (which constitute current sota). but, in addition to the costs necessary for fine-tuning this model, the resultant weights would not be accessible to run locally in any case; one would have access to it only via the openai interface, which motivated our decision to fine-tune the smaller and open-source flan t5 instead. finally, we only experiment with datasets curated in the english language and therefore, we do not know that the issues we have highlighted could replicate in the same way in other languages."
1238,"limitations for those label names without semantic meanings, several keywords are still required for npprompt to work well. furthermore, this study focuses exclusively on the zero-shot setting. however, there are potential avenues for exploration in the few-shot scenario, which is prevalent in practical applications. the applicability of npprompt to other tasks, such as ranking and relation extraction, remains uncertain and warrants further investigation. designing a refinement method to jointly search for label words and templates can be a promising direction for future research."
1239,"limitations and ethical considerations of llm evaluation despite the promising results of llm evaluation shown in this paper, there are some limitations of this method. first, llm may possess incorrect factual knowledge (cao et al., 2021), so it is not suitable to use them in tasks that involve factual knowledge. next, llms trained to behave in a certain way can be biased toward certain responses. precisely, an llm that is trained to be safe and non-harmful can result in llms preferring to generate more positive and upbeat responses, which is observed throughout our interaction with chatgpt. additionally, even with researchers’ efforts to make llms safer (bai et al., 2022a,b), llms can still generate harmful and biased responses (ganguli et al., 2022; perez et al., 2022), which are violative of basic"
1240,"limitations of llm evaluation mentioned previously, there is a crucial ethical concern at the heart of llm evaluation. is it ethical to replace human evaluation with llm evaluation? some may question if this paper is suggesting that llms are now ready to replace humans and find this idea unsettling. as responsible and ethical nlp researchers, we understand these concerns but want to make it clear that this is not our intent. as our paper title suggests, we aim to offer an alternative option to human evaluation with the goal of enhancing the reproducibility of nlp research. human evaluation is still essential as the ultimate goal of nlp systems is to be used by human users, so it’s important to gather feedback from them. we highly enjoy the process of discussing the experiment settings and results with the english teachers we hired. we do not recommend that future researchers completely eliminate human evaluation; rather, we believe that human evaluation should be used in conjunction with llm evaluation. both methods have their own advantages and disadvantages, making them both necessary for evaluating nlp systems. we hope the positive results in this paper provide nlp researchers with an alternative method to evaluate systems and encourage further"
1241,"limitations the proposed approach (along with other methods for estimating uncertainty in inconsistent annotations) is only viable when the raw labels from different human annotators for each sentence are provided by the datasets. however, some multipleannotated datasets only released the majority vote or averaged label for each sentence ( i.e. poria et al., 2019). the proposed method made a gaussian assumption on the likelihood function for the analytic computation of the uncertainties. the results show that this modelling approach is effective. despite the effectiveness of the proposed method, other distributions could also be considered. data collection processes for aer datasets vary in terms of recording conditions, emotional elicitation scheme, and annotation procedure, etc. this work was tested on two typical datasets: iemocap and msp-podcast. the two datasets are both publicly available and differ in various aspects: • iemocap contains emotion acted by professional actors while msp-podcast contains natural emotion. • iemocap contains dyadic conversations while msp-podcast contains podcast recordings. • iemocap contains 10 speakers and msppodcast contains 1285 speakers. • iemocap contains about 12 hours of speech and msp-podcast contains more than 110 hours of speech. • iemocap was annotated by six professional evaluators with each sentence being annotated by three evaluators. msp-podcast was annotated by crowd-sourcing where a total of 11,799 workers were involved and each work annotated 41.5 sentences on average. the proposed approach has been shown effective over both datasets. we believe the proposed technique should be generic. furthermore, although validated only for aer, the proposed method could also be applied to other tasks with disagreements in subjective annotations such as hate speech detection and language assessment."
1242,"limitations we discuss the limitations of plugd in this section: (1) we explore decoupling document encoding from concrete tasks in this paper, and propose to represent documents as pluggable modules before task adaptation. therefore, plugd has a higher storage requirement than conventional encodingcoupling methods. we encourage (2) in the experiments, we adopt t5 as our ptm backbone. actually, the proposed framework can also be applied to more pre-trained models with various model architectures. besides, recent trends show that larger models tend to build more expressive text representations. it is worth exploring plugd with larger ptms with billions of parameters to learn informative document plugins. (3) in this paper, we adopt an external retriever to retrieve relevant documents for each input query. recent progress in retrievalaugmented language models shows that training the ptms with an end-to-end textual knowledge retriever can promote downstream performance. we believe document plugins can also serve as the ex- ternal knowledge base and enhancing plugd with end-to-end retrieval is a promising direction."
1243,"limitations by comparing to post-hoc debiasing methods. we hope that our study can make it more accessible for others to debias pre-trained language models with reduced computational requirements, and contribute to fair and inclusive nlp."
1244,"limitations as we stated under future work, one of the limitations is the variance decomposition-based proof. our work is based on simplified settings, i.e., linear squared regression assumption. post-ensemble variance is not evaluated due to the nature of the ens ensemble algorithm. extended experiments using vanilla bagging ensemble would enable analysis of post-ensemble variance. further investigation into refining the two stages would help understand the performance of lpms, e.g. those that are in phase-i but before the peak in figure 1. our results for multirc are based on the instance sampling, however a better sampling technique should be based on stratified sampling based on the ratio of the question types in the multirc set. however, to achieve this, the multirc set needs to be annotated for question types, which is currently missing. sampling techniques by themselves can become a research topic so that a further decrease of variance due to sampling can be achieved. although we list these items as limitations, they are also topics for future research within the greater theme of understanding the new bias-prevalence paradigm for lpms."
1245,"limitations in capturing all dimensions of fairness. further research is needed to develop comprehensive extrinsic metrics across diverse tasks. although our work has been centered around fairness in allocation-based (classification) applications, addressing fairness concerns in other types of language models, such as natural language generation models, is necessary. in generative tasks, the measurement of unfair outcomes would be distinct from the methods we have used. another area of potential future work could involve benchmarking debiasing methods for compressed models and developing new compression-aware methods."
1246,"limitation of the previous sentence representation learning approaches, which are limited to using only the input sentence. rankencoder leverages the distance between the input sentence and the sentences in a corpus to predict its semantic vector. rankencoder is universally applicable to any unsupervised sentence encoder, resulting in performance improvement, and we demonstrated this with three unsupervised sentence encoders. we achieved state-of-the-art semantic textual similarity performance by applying our approach to the previous best sentence encoder. we also showed that our approach is specifically effective for capturing the semantic similarities of similar sentences."
1247,"limitations (discussed below), our results indicate that, in general, revision-based data can be employed effectively for the given tasks, contributing towards solutions for each of the considered challenges. specifically, our suggested sampling strategy revealed that training on claim versions with a higher revision distance between them improves the performance when detecting claims in need of improvement. moreover, we found that the impact of the available types of contextual information is not only task-dependent but also depends on the quality issue that a claim suffers from. we argue that the developed approaches can help assist automated argument analysis and guide writers in improving their argumentative texts. with our work, we seek to encourage further research on improving writing support not only in debate communities but in educational settings as well."
1248,"limitations although our proposed method achieves promising results and outperforms most baseline systems on the st benchmark, it still has some limitations: (1) the performance of our method still lags behind a recent work speechut, although our approach has the advantage of consuming less time and resources; (2) we observe that the modality gap is still not eliminated and the effect of exposure bias on the modality gap still exists; (3) the performance of our approach on larger datasets and larger models remains to be explored; (4) how to apply our approach to other tasks also needs to be further investigated."
1249,"limitations in this section, we discuss the limitations of our proposed framework. our ugdre can reduce the false positive pseudo label by estimating the uncertainty of the model prediction. however, it is difficult to reduce the false negative pseudo labels by uncertainty estimation. our framework also relies on human-annotated data to train the pre-denoising model, which causes the sensitivity of our framework to the quality of human-annotated data. thus, the improvements of models that continue training on the docred dataset are not as well as on the re-docred dataset. moreover, iterative training introduces additional computing overhead, which makes the training process time-consuming."
1250,"limitations amrsim has high prediction efficiency but the training process is time-consuming. in our experiments, one epoch training on one geforce rtx2080ti took about two and a half hours. selfsupervised learning requires a large amount of training data. parsing wiki sentences into graphs requires time, but the advantage is that it can be processed offline. another issue is the length limit. transformers can only handle limited sequence lengths due to the computational and memory complexity of attention calculation. therefore, encoding large amr graphs is challenging. possible solutions include applying sliding window algorithm to split a large amr graph into several subgraphs and merge the scores."
1251,"limitations (see §8), we think the benefits of our work overshadow its limitations. we provide a simple approach and a new set of tools to interpret transformer models and compare them. the realm of input-independent interpretation methods is still nascent and it might provide a fresh perspective on the internals of the transformer, one that allows to glance intrinsic properties of specific parameters, disentangling their dependence on the input. moreover, many models are prohibitively large for practitioners to run. our method requires only a fraction of the compute and memory requirements, and allows interpreting a single parameter in isolation. importantly, our framework allows us to view parameters from different models as residents of a canonical embedding space, where they can be compared in model-agnostic fashion. this has interesting implications. we demonstrate two consequences of this observation (model alignment and stitching) and argue future work can yield many more use cases."
1252,"limitations it is important to note that the unified representation proposed in our study is just one option among many. other linearization methods may potentially yield better results. for example, research by yin et al. (2022) and aghajanyan et al. (2022) has explored using code generation with jupyter notebooks and a hyper-text language model with structured prompting, respectively. further research in these areas, such as converting all structured forms to markdown language or hyper-texts, may yield alternative unified representations."
1253,"limitations since webnlg is derived from 2015-10 version of dbpedia, factkg does not reflect the latest knowledge. also, another limitation of our work is that the claims of factkg are constructed based on single sentences, like other crowdsourced fact verification datasets. if the claim is generated by more than one sentences, the dataset will be more challenging. we remain this challenging point as a future work."
1254,"limitations despite the fact that some measures have been implemented to minimize bias in labeling, we are still explicitly aware that our dataset may contain mislabeled data due to differences in the subjective understanding of toxic language by the annotators. in addition, due to the limitation of data coverage, the samples in our dataset are predominantly in simplified chinese, with very few samples in traditional chinese, as discussed in section d.5. meanwhile, as shown in the error analysis in section 6.3, our benchmark of toxic knowledge enhanced is not practical for all types of toxic comments, lacks sufficient background knowledge, and can easily lead to spurious associations. for reasons of intellectual property, we only capture the comments rather than the full text, which affects the actual semantics of the sentence to some extent. besides that, non-textual features are not taken into account in this work, such as images and meta information about publishers. in future work, we will further research span-level and multi-modal toxic language detection."
1255,"limitations while we establish the “miraculous” ability of character-blind models to induce robust spelling 8the models were trained on the same number of tokens, but only 6% of byt5 training was on english, and we estimate 4 utf-8 bytes per t5 token. information through large-scale web pretraining, our work does not attempt to identify the mechanisms or sources through which this information is learned. possible sources within web corpora include: dictionaries containing phonetic pronunciation guides, alphabetically ordered lists, typos and other misspellings, and examples of spelling words with dashes or spaces between every character. linguistic phenomena that may aide in inducing spelling knowledge include words with predictable morphemic makeup, and cases where meaningform relation is non-arbitrary, contra saussure’s “semiotic arbitrariness”. we refer the reader to itzhak and levy (2022) and kaushal and mahowald (2022) for work in this direction. most of our image generation experiments are limited to english. we present preliminary results in appendix a showing that our byt5-based models have stronger multilingual understanding than t5. however it would be valuable to test this further, and to explore training image generation models on multilingual image-caption datasets, as opposed to merely using a pretrained multilingual text encoder. ideally, it would be possible to conduct controlled comparisons between pretrained text encoders that differ only in one regard, to isolate all factors contributing to performance. however as pretraining large language models is resource intensive, we were only able to use off-the-shelf text encoders, which often differ along multiple axes. in our text-only experiments, we isolated the contributions of character-awareness (byt5 vs. mt5/t5) and multilinguality (byt5/mt5 vs. t5). however, in our image generation experiments, these factors were conflated, as we had limited resources for training new models. still, the fact that byt5based image generation models outperform t5 despite being multilingual (which often degrades performance on english-only tasks) strongly suggests that character-awareness is the key factor for spelling ability. another limitation is that we focused on image generation models that leverage frozen pretrained text encoders. this enabled straightforward experimentation by swapping encoders and retraining the image generation module. however, it remains to be seen whether our results extend to settings where the text encoder is trained along with the rest of the model, as in yu et al. (2022)."
1256,"limitations there are a few shortcomings that we discuss below: underrepresented fine-grained lms: although we had chosen a careful sampling method focused of event-centric informative dataset aiming to increase the likelihood of fine-grained lms’ occurrence (kitamoto and sagara, 2012), we think the low frequency of fine-grained lms in idrisi-ra is a major limitation as it contains solely 25.5% fine-gained lms. human errors: there are some human errors made during annotation due to the difficulty of the task. • annotators sometimes fail in distinguishing between location and organization entities (e.g., “red cross""). • different location types could be used interchangeably for the same locations which forms a difficulty for annotators (refer to section 5.1). • annotators highlight the locations that are mentioned as descriptions within the context of the tweet. we plan to overcome these errors as part of location mention disambiguation (lmd) annotation that aim to remove ambiguity of geo/geo entities (as a sequel to the geo"
1257,"limitations although pgpl is proven to be effective according to our intensive experiments, the current design of the pgp module may not be optimal and could be improved in the future. specifically, on the one hand, to choose the proper number of nearest text instances kc for each class c, we need to obtain the statistic values µyt , µ y <t and computed values µ c t , µc<t, γt for each dataset, which makes it somewhat not easy to scale to many different datasets. on the other hand, we heuristically filter pseudo-labeled texts by the top-k selection, which may also be improved with more systematical approach."
1258,"limitations in this paper, we show that lens shows better human correlation than other metrics on wikipedia and news domains. future research can further experiment and extend lens to other domains, such as medical and children’s books, as the preference for different simplification operations can vary depending on the domain and user. additionally, our work focuses on sentence-level simplification, and future work can extend lens to evaluating paragraph- and document-level simplification. simpeval dataset and lens are also limited to the english language."
1259,"limitations the proposed method ndcr has some limitations as follows: 1) the produced representation of simple proposition sentences in the proposition generator lies in a different space distribution with the image encoding, which affects the performance of their fused representation. although we introduce the reasoning information of compound proposition text to alleviate this issue, we hope to solve it by improving the text understanding capability of pretrained vlms. in addition, adopting the pretrained textual encoder of vlms to perform proposition decomposition is inadequate due to that they present an inferior understanding for the discourse structure of long texts. 2) the performance of samples with highly similar images from video frames is quite different from that of humans. we may improve it from the perspective of image difference modelling. 3) the experimental results indicate that our method is effective at logical inference on examples with medium-length descriptions, but there is still room for improvement for longer descriptions."
1260,limitations section. we hope our analysis of rarr would help with developing new approaches for integrating attribution to lms.
