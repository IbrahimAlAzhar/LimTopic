,Text
0,"This resource paper describes the dataset in detail, providing strong baselines and first initial crossdomain experiments. It does not aim to provide an extensive set of experiments on cross-domain argumentative zoning yet.
The entire dataset is only singly-annotated. The agreement study was performed on complete documents and hence has only limited data for several labels. Due to the limited funding of the project, we could double-annotate the entire dataset.
Finally, we only test one model class (BERTbased transformers). A potential next step is to test a bigger variety of models and embeddings. Because AZ labels are interdependent within a document, especially document-level models or CRFbased models are promising methods to try. We have also tested only one method (multi-label random oversampling) to deal with the strong class imbalance in the dataset. We have not yet tested further such methods (Henning et al., 2023) or data augmentation methods."
1,"Multilingual language models such as XLM-R (Conneau et al., 2020) and GigaBERT (Lan et al., 2020) are typically pre-trained on large amounts of unlabeled text crawled from the Web. Since these models are optimized to capture the statistical properties of the training data, they tend to pick up on and amplify social stereotypes present in the data (Kurita et al., 2019). Since our coreference resolution models use such pre-trained language models, they may also exhibit social biases present on the Web. Identifying and mitigating social biases in neural models is an active area of research (Zhao et al., 2018; Sheng et al., 2021; Gupta et al., 2022). In the future, we plan to work on removing social biases from coreference resolution models.
Furthermore, while our proposed methods are highly effective, the performance of our best ensembles is still far from perfect. On OntoNotes Arabic, our best system only achieves an F1 score of 66.72%. Such performance may not be acceptable for some downstream tasks (e.g., information extraction from critical clinical notes).
Finally, even though Wikipedia is available in more than 300 languages, there are still very few Wikipedia pages for some very rare languages. Our proposed methods are likely to be less effective for such rare languages."
2,"The encoder of the LCM which we have utilized for our experiments is a basic deep neural network. Replacing it with more robust and effective architectures could help achieve better performance. Furthermore, instead of using pre-trained GloVe embeddings for the encoder, using IDDR-specific
embeddings could have been a more efficient approach. Lastly, our models have been trained and evaluated on PDTB 2.0, instead of the latest PDTB 3.0, which includes also intra-sentential implicit relations and has a more systematic sense hierarchy."
3,"limitations in spite of the strong performance of changes, its design still has the following limitations. first, changes only extracts the sentence-sectiondocument hierarchical structure of academic papers. we believe the model performance could be further improved by incorporating document hierarchy of different granularity like dependency parsing trees and rhetorical structure theory trees. we leave this for future work. in addition, we only focus on single academic paper summarization in this work. academic papers generally contain a large amount of domain knowledge, thus introducing domain knowledge from peer papers or citation networks should further boost model performance."
4,"limitations although our model improves upon state-of-the-art methods of bam by incorporating entity coreference and co-occurrence information, there are still some limitations to our model. first, it is not easy to apply our model to other domains where no coreference resolution tool is available. second, the number of nodes and edges of the generated heterogeneous graph will become enormous if the documents are long and many entities are extracted, which requires more gpu resources."
5,limitations the corpus presented in this paper is relatively small and so the
6,"limitations although the new architecture works well on pdtb-like structured data, we are often challenged with texts without clear paragraph structure. this would make it either necessary to pre-process texts and split sentences into semantically closed paragraphs such that our proposed model takes advantage of the surrounding context, or develop a new sentence-based model which was not successful in previous work. limiting the model to predict only continuous alternative lexicalizations does not highly affect results on the pdtb, but might have a more considerable impact on other text genres, e.g. speeches and debates. this would require the use of a more complex signal encoding as mentioned in section 3.1."
7,"limitations. more long term, we plan to extend our discourseaware approach towards controllable generation with given entities. specifically, instead of using the learned gr scores, the model could generate summaries with desired entities provided by human users. limitation in our method, we employ an existing ner tool (spacy) to label the entities in both the source documents and the summaries, and the performance of the ner tool may have an influence on the results of the model. thus a good in-domain ner tool may be required when the work is extended to some specific domains, e.g. medical text. in addition, we use pegasus(zhang et al., 2020) as our base model in all the experiments on different datasets, as it has delivered top performance on multiple summarization datasets. we follow the original paper on the length limits of all the datasets, however, the length of the source documents in both scientific paper datasets are much longer than the length limit (3k/6k v.s. 1024), which leaves the room for further improvement with sparse attention techniques applied (xiao et al., 2022; guo et al., 2022)."
8,"limitations we have carried out all analyses according to our best abilities. nevertheless, it should be noted that rst structures and qud structures were annotated by distinct researchers. while all annotations have been double-checked by at least one other expert for plausibility, in many cases there are alternative analyses of the texts which may also be applicable (as is usually the case for discourse structure). since we do not have direct access to the discourse creators and their goals, this limitation is unavoidable in corpus studies."
9,"limitations in our current project, we have not taken into account the temporal information that treats the historical behavior of users as a sequence of actions. thus, the model may not capture how user behaviors change over time. to ensure full support to users in need, we recommend that future work should address this limitation by considering users’ historical behaviors as a sequence of actions. moreover, although our pre-trained models achieved significant results without fine-tuning discourse embeddings, we suggest that fine-tuning these models can enhance performance by capturing the nuances of the datasets’ distribution and contexts. furthermore, conducting a detailed comparison of additional open-source large language models (llms) would provide more comprehensive insights into their performance. additionally, in addition to analyzing the efficiency of different models, it is crucial to evaluate the cost associated with implementing these models. therefore, future work should consider both fine-tuning and evaluating additional llms, while also taking into account the costs of utilizing these models."
10,"limitations the proposed model with sentence-aware encoder aims to efficiently incorporate external knowledge and bag-of-words for topic modeling, which means that in this work we are mainly interested in how documents should be encoded for topic inference. however, the decoder of topic models can also be coupled with word embeddings through factorization, such as embedded topic models (dieng et al., 2020). it is worth exploring how hierarchical semantic embeddings can be employed for topic modeling with our model. in this paper, we do not conduct any fine-tuning for the pre-trained language model. our approach reveals how the frozen pre-trained language model can be effectively used to improve the performance of the topic model with limited computational overhead, given that the parameter size of the pretrained language model is much larger than that of the topic model. moreover, fine-tuning pre-trained language models for topic modeling as an unsupervised learning task (mueller and dredze, 2021) is challenging."
11,"limitations this work is about document-level nmt, we focus specifically on methods that improve the model performance for long input sequences. due to constrained resources, this work has several limitations. to be able to train all methods including the inefficient baseline approach, we have to limit the context size to 1000 tokens. while we do a comparison to existing approaches, other approaches have been proposed to improve the performance of systems with long context information, which we do not compare against. we run experiments on three different tasks, but two of them are low resource and two of them translate into german, which was necessary because we only had access to german language experts for preparing the evaluation."
12,"limitations the current work is limited by the size of the dataset and the nature of spontaneous conversation. while the discourse relations proposed as part of this work were selected to be general and build on categories from the literature, the list is not exhaustive and it is likely that these relations may be culturally, linguistically, and situationally specific. future work in this area should validate the generality of the discourse relation system used in this work. the selection of edus and cdus for annotation is also non-exhaustive; additional segments could be included in future work. annotation quality is also a practical limitation. annotation for discourse relations typically results in low-agreement data, even among expert annotators (e.g., discogem; scholman et al., 2022). even though our research questions focus on this disagreement as a positive, other researchers may require greater numbers of annotations in order to obtain a gold label."
13,"limitations our proposed summarization model is pretrained exclusively on news datasets, however, our experiments and analysis were conducted on biographical narratives. we only studied english summarization and our processes and in particular relevance findings are likely not entirely applicable to long multi-lingual documents. moreover, single-domain trained models may propagate inductive biases rooted in the data they were pretrained on. this was evidenced in finetuning on our target dataset as the model demonstrated a moderate degree of transferability in adapting the newswire domain to our biographical discourse genre. our work studies generated summaries for long narrative text. while our taxonomy appears generalizable to other domains, investigating summarization quality of large-scale datasets, such as scientific articles, patent documents, government reports or meeting discourses was confined to the scope of baseline performance comparison."
