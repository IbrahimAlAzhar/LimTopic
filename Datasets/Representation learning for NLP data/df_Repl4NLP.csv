,Text
0,"We foresee two limitations to our work. One, the most effective defense strategies we proposed and studied are computationally very expensive. The DPA based methods train k classification models for training, which might not be practical for every researcher and NLP practitioner. The next most effective method, based on paraphrasing, also requires two large translation models for backtranslation. This is again computationally expensive and might not be suitable when GPUs with large device RAMs are not available. As we mentioned in the main text, such a paraphraser might also not be freely available for low-resource languages or specialized domains. Second, we only evaluated the defenses on textual backdoor attacks. Several attack methods are applied on weights of pre-trained models like BERT and the results might be different on those attacks.
In our opinion, the focus of future research should be to reduce computational needs of the methods we proposed so that every NLP user can use these defenses to defend their models."
1,"Although all pre-training approaches require a sufficient amount of data, given how we defined keywords, longer sequences suit our approach better than short ones for studying the effects of keyword selection. Further, as shown in this study, our findings strongly imply that the strategy we suggested for adapting PLMs can effectively enhance their performance on text classification as the downstream task. To determine whether these findings can translate to other NLP applications, however, further experiments are required."
2,"The experiments reported were performed on a dataset of French sentences, with a particular organization: sequences of sentences as input, each with a slightly different structure but sharing the subject-verb agreement rule. All sentences in the input sequence are processed together. In future work we plan to separate the distillation of rules from a sentence representation from the processing of the sequence.
We have investigated only part of the parameters in the proposed architectures. In particular, the β coefficient in the encoder-decoder and the dual VAE architectures was set to 1. Higher values may lead to more disentangled representations on the latent layer."
3,"The adversarial test sets based on masked language models can introduce new noise into the sentence context, as there is no way to automatically ensure grammatical correctness. However, there were many cases where such introduction of noise did not affect the predictions, in all three languages. Further, adversarial datasets are expected to introduce such noise, as is seen in other research on
the topic for other tasks such as sentiment analysis, and the goal of such research is also to understand model robustness in the presence some noise. It is relevant to mention in this context that the NER datasets we considered already consist of other noise and ungrammatical examples such as score cards of sporting matches (conll03-en), social media content (wnut17) and fully lowercased sentences with weakly supervised annotations (mconer21). Further, masking does not alter the entities themselves, and only changes the non-entity tokens. So, the NER models still see the same entities. While there are no established means of quantifying the quality of adversarial datasets to our knowledge, exploring human-in-the-loop approaches to select appropriate examples to include in the final adversarial test set can be one way to address the issue."
4,"One significant limitation of our work is that we only explored the capabilities of diffusion-based language models under a challenging circumstance
where it is not allowed to use pre-trained weights or grammar parsers, which means we did not utilize this kind of model to its full potential, so a future research direction could be exploring possible ways to further improve the model’s performance by leveraging pretrained weights or word embeddings, and train with enough data to find the full potential of these models.
Another limitation of our work is that we only explored one typical diffusion-based language model, so our conclusions may not generalize to special types of diffusion-based language models (such as ones that uses discrete state space). We also conducted all experiments using the exact same model architecture design. In the future, we plan to experiment with different architectures for the diffusion model, such as more sophisticated conditioning methods (currently we just concatenate the source to the target, but we would like to try other ways of conditioning on the source, such as cross attention, as these conditioning methods for diffusion models have promising performance in the image generation domain).
Lastly, we found that diffusion-based language models work well with limited data and no external knowledge or pre-trained weights, thus these mod-
els may have great potential under low-resource settings, but we didn’t apply them to any real lowresource settings (such as low-resource languages, rare domains, etc) in this paper, and we would like to do that in the future to explore the full potential of diffusion-based language models."
5,"We obtain notable QA performance through experiments. However, we conduct many experiments to find the optimal candidate for ContrastiveQA. Many of these experiments inevitably consume a lot of time and energy, and we have to heuristically determine the number of candidate sets through experiments in a limited environment. We intend to alleviate the current problems by adding a module that can solve these problems in our future research. For example, the Longformer model takes almost a day to process long text for each epoch to train. Therefore, we use smaller batch sizes with a limited number of GPUs to train the LongFormer model. However, due to the lack of GPU resources, the optimal weight of the proposed framework cannot be learned. Thus, it is necessary for further research on model weight reduction to mitigate computational resource problems."
6,"Although our method has been shown effective, it has two limitations that may be improved in the future. First, the FA model has advantages in computation but relies on an effective frequency selection strategy, which is difficult to design. We
just simply select some manual frequencies for different datasets by experience. The more effective frequency selection strategy needs further exploration. Second, there is no theoretical guarantee that the orthogonal regularization can generalize to a 3-order tensor. Our OR terms are only formally consistent with matrix orthogonal regularization, which has been empirically shown effective."
7,"Our initial work on one-shot EM shows promising results but comes with several limitations. By inspecting generated examples, we find evidence of information leakage in the LSR that causes generated sentences to be similar in topic and length to the input sentence. We also require one example at inference time in order to generate a new example for a specific word sense. We demonstrated empirically that the baseline does not have this restriction, because the baseline method generalizes well to words that were unseen during training (see Table 1). Additionally, our best approach, S2Ssemi, still requires WSD training data for pretraining of the BEM, which is used for LSR extraction. Finally, the S2S models in this paper have the limitation that the target word looks identical in both the input and output sentence, whereas the baseline is capable of generating examples with various target word forms for a given target lemma. This could be fixed in the future by lemmatizing the target word before passing it to the S2S BART encoder during training and generation, but we did not run this experiment due to our initial focus on using as little supervision as possible to perform EM."
8,One limitation is that the importance and visual salience of character instances are not measured directly. We plan to settle these in future work.
9,limitations of each of the methods and provided guidelines for nlp researchers and practitioners for using these methods. 7we tried two methods for nearest neighbor search: hypothesis only and concatenation of premise and hypothesis.
10,"limitations of current approaches are discussed in the previous section. apart from this, since the paper focuses on more foundational question of evaluating ner systems in general, we do not foresee any other potential risks involved with this research. broader impact considering the number of practical usecases of ner across industries, and the growth of multilingual nlp, ner evaluation beyond english is more important than ever before. in this paper, we explored a previously unexplored space for named entity recognition, i.e., evaluating ner systems beyond english for their sensitivity to adversarial input, which will hopefully lead into better evaluation strategies when developing ner systems across languages in future. 11https://dx.doi.org/10.6084/m9. figshare.22674079"
11,"limitation we summarize two limitations which also serve as promising directions to be explored in our future work. rada framework only considers textual domain corpus as the datastore, although this has greatly improve the coverage of domain knowledge as texts are always relatively easy to collect. however, it is widely investigated that structured knowledge such as knowledge base can also serve similar purpose. and such resources are generally in higher quality and are easier to match. therefore, it would be benefiting to further integrate such resources at certain scenario where kb is available. the other limitation regards to the scale of the rada implementation. as large language models have becoming increasingly powerful, they have demonstrated quite impressive capability in memorizing and recalling a wide range of background knowledge existed in the massive corpora they have been pretrained on. this trends of development naturally raises question for the proposed framework: will it still be beneficial when scale up to llms? and on what kinds of scenario does it brings best improvements? these are very important questions to answer, and we can certainly expect them to be explored in future works. ethical statement we evaluate the proposed method on established and publicly available datasets. there is also no human evaluation involved. this paper is not concerned with the above ethical risks. when the proposed framework is deployed into domain specific production, the domain adapted language models might express ethical-related outputs, but just as any other language models do (weidinger et al., 2021), and should be treated with according techniques to eliminate ethical risks such as bias, stereotypes."
12,"limitation of our work is that we only explored the capabilities of diffusion-based language models under a challenging circumstance where it is not allowed to use pre-trained weights or grammar parsers, which means we did not utilize this kind of model to its full potential, so a future research direction could be exploring possible ways to further improve the model’s performance by leveraging pretrained weights or word embeddings, and train with enough data to find the full potential of these models. another limitation of our work is that we only explored one typical diffusion-based language model, so our"
13,limitations of repurposed vision encoders and highlights the need for encoders designed specifically for v+l tasks.
14,"limitations the main limitation of our concatenation-based multi-ve models is efficiency: the models are significantly slower than single-ve models because of the additional visual tokens in the input; the 3- ve model requires almost twice the time to train (in real time, not training steps) compared to the single-ve models. also, in cases where images are not pre-encoded, multi-ve setups are significantly slower at inference time. however, as mentioned before, we concatenate the tokens for analysis purposes only (§5) and leave more efficient alternatives like resampling (alayrac et al., 2022) to the future. several limitations could be investigated in the future, assuming access to a larger computational budget: 1. we focused on single-stream transformers and did not take into account dual-stream or other multimodal transformer architectures. 2. we only experimented with three popular ves (one version per ve class). there are many other ves we could investigate in the future. 3. we do not pre-train our multimodal models on intermediate, auxiliary multimodal tasks (tan and bansal, 2019; lu et al., 2019; chen et al., 2020, inter alia) as achieving state-of-the-art is not our goal."
15,"limitations the proposed spc framework is model and task agnostic and can scale to different models and tasks. it is adaptable to domain/task shifts and generalizes well with low data. however, the prompt length has to be tuned for each task, and there is no principled way of determining the optimal prompt length. this work does not explore the interpretation of soft prompts or prompt ensemble. future work can investigate other loss forms derived from our spc framework, explore the combination of fine-tuning’s strong in-domain performance and spc’s cross-domain performance, and apply spc and other parameter-efficient tuning approaches to large lms. ethical statement we hereby state that our study adheres to the acl code of"
16,"limitations kgt5-context relies on the textual mentions of entities and relations (and, optionally, entity descriptions). therefore, it is only applicable to kgs that provide such information. kgt5-context may be able to handle some entities without textual features when well-described by their neighborhood; we did not investigate this though. to use kgt5-context for prediction, the kg has to be queried to obtain context information, i.e., the one-hop neighborhood of the query entity. kgt5context thus cannot be used without the underlying kg. the verbalized neighborhood of the query entity leads to long input sequences, which in turn may induce higher memory consumption and higher computational cost during training. overall, training kgt5-context is typically more expensive than training traditional kge models, which can be tuned (kochsiek et al., 2022) and trained efficiently (lerer et al., 2019; kochsiek and gemulla, 2021; zheng et al., 2020). for inference, kgt5-context first samples relation-neighbor pairs for contextualization, and then samples possible answers from the decoder. these sampling steps can lead to variance in predictive performance. we found this effect to be negligible on wikidata5m, but it may be larger on other datasets."
17,"limitations the use of auxiliary classifiers at every node of the decision tree is not feasible when the hierarchical tree is huge, such as the large hierarchical terminologies for medical literature indexing (gasco et al., 2021). besides, in table 1, even though the integration of the hierarchical information shows a consistent improvement in both the baseline and mimlroberta models, these improvements are still within one standard deviation of micro-f1. lastly, it is worth noting that we do not focus on large language models since our approach is to explore improvements on a published state-of-theart model. while they might improve accuracy, a careful exploration of those on a new task is beyond the scope."
18,"limitations syng2g-tr encodes the syntactic dependency graph because the nodes of input and output graphs should be similar. future work could include investigating the use of constituency graphs in the self-attention mechanism of transformer (vaswani et al., 2017b), where the nodes of the input graph (constituency graph) are different from those of the srl output graph. in this paper, we initialise our model with the pre-trained bert (devlin et al., 2019) model. as future study, larger and better pretrained language models will be used for the initialisation of syng2g-tr models, to achieve better performance. additionally, future studies can easily extend our work to multilingual srl benchmarks. 7this leads to a bert-based syntax-agnostic model, similar to shi and lin (2019)."
19,"limitations of bort (e.g. our explicit demarcation of phonemic and orthographic regions), and is perhaps generally well-suited for the task at hand. an additional area of future work will consist of exploring alternative training strategies. in the present work, we were quite strict with regard to preparing the test split, withholding the most frequent english words because they appeared in our aphasiabank evaluations. as our focus turns more toward downstream tasks, we will update our holdout methods to prioritize a stronger pre-trained model, holding out only as much data as needed for validation. further, seeing how our models did not train for an entire pass through the wikipedia data, we will adjust the training schedule (e.g. a ramp-up in the learning rate) and task configuration (e.g. word transform rates) to ensure we get the most out of the pre-training stage. for our aphasia-specific application, we see room to improve the noise transform with more strategic approaches. phoneme errors could be made more realistic with statistically or linguistically informed approaches (e.g. replacing phonemes with similar phonemes). to better prepare a model for semantic errors, whole-word replacements could be made with semantically similar words. limitations the models presented here were trained with the basic inventory of english phonemes found in cmudict. however, a more fine-grained phonetic analysis would require a pronunciation dictionary with more narrowly defined entries. additionally, while this paper focused on models trained with english-only resources (pre-trained bart-base, english wikipedia text, cmudict, and the english aphasiabank), the techniques should be applicable to non-english language models as well. finally, from a clinical standpoint, the model we describe in this paper assumes the existence of transcribed input (from either a manual or automated source, discussed in detail in § 2.1); in its current form, this represents a limitation to its clinical implementation, though not to its use in research settings with archival or newly-transcribed datasets."
20,"limitations exist when using certain methods. we also demonstrated how prefix tuning may have more difficulty learning sentence representations of augmented data. we further showed that contrastive learning can be a solution to these issues, and suggest that more work be done to find similar methods that can be more generalizable. limitations although we present our results across multiple datasets and models, we have largely adhered to natural language understanding tasks. how effective the augmentation methods are on generationrelated tasks warrants further study. in addition, another limitation is that we do not study the level of perturbations needed to have a negative effect on prefix tuning. knowing this can shed light on what particular types of transformations are a problem for prefix tuning so that they can be avoided. our hypothesis here is that such perturbation levels are sensitive to many factors during training, such as the difficulty of the task and the characteristics of the data augmentation approaches applied, which could be difficult to be quantified."
21,"limitations the first limitation of rse comes from data sparsity. unlike word-level relational data, the variety of sentences is much larger than words. therefore, even though we can collect more and more relational data on sentence pairs, it is generally hard to become densely connected between sentences. to alleviate this issue and generate sustainable sentence relational data, we should design automatic tools for sentence relation labelling with human-inthe-loop supervision. second, unsupervised sentence embedding also shows decent performance on semantic textual similarity tasks, and the focus is mainly on the design of unsupervised contrastive samples. therefore, exploring different unsupervised view-augmentation techniques in relational sentence embedding remains an open question. in this work, we only study incorporate dropout augmentation without much"
22,"limitations this paper hypothesizes that seq2seq models have token-level overfitting and underfitting issues, and provides direct evidence to support the hypothesis in various settings, raising a valuable problem for nlp modeling. however, this paper does not provide a solution to the problem due to the theoretical and practical challenges of measuring the convergence speed of each token. we leave the exploration of this topic to future work."
23,"limitations the ses in this paper are mainly transformer-based ses, and we are not sure whether the observations hold for other ses. however, considering that transformer-based ses dominate the current nlp community, we think it is fine to only evaluate 59 transformer-based ses. another limitation is that the sentences in heros are converted from reddit, which is an online forum and the texts on reddit may be more casual and informal. this makes the sentence pairs in heros tend to be more informal. users should note such a characteristic of the sentence pairs of heros and beware that the results obtained using heros may be different from the results obtained using more formal texts. an additional limitation is that there can be more diverse rules to create different sentence pairs other than the six subsets included in heros, and our paper cannot include them all. as a last limitation, during the construction of heros, we remove sentences that are ungrammatical based on language-tool, so our results may not generalize to ungrammatical sentences."
24,"limitation this major limitation of sen2pro lies in the computational cost due to the generation of several samples. thus, improving the efficiency of sen2pro is a future direction. besides, since we choose to concat the representation of different samples, there may be more natural ways to merge information from samples."
