{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# load the csv file\n",
        "df_short = pd.read_csv(\"df_volume_2_short_papers.csv\")\n",
        "df_long = pd.read_csv(\"df_volume_1_long_papers.csv\")\n",
        "df_trust_NLP = pd.read_csv(\"df_TrustNLP.csv\")\n",
        "df_sigmorphon = pd.read_csv(\"df_sigmorphon_w_comp_research.csv\")\n",
        "df_woah = pd.read_csv(\"df_WOAH.csv\")\n",
        "df_industry_track = pd.read_csv(\"df_volume_5_industry_track.csv\")\n",
        "df_student_research_workshop = pd.read_csv(\"df_volume_4_student_research_workshop.csv\")\n",
        "df_third_dialdoc = pd.read_csv(\"df_third_dialdoc.csv\")\n",
        "df_sustain_NLP = pd.read_csv(\"df_SustaiNLP.csv\")\n",
        "df_Repl4NLP = pd.read_csv(\"df_RepL4NLP_2023.csv\")\n",
        "df_NLRSE = pd.read_csv(\"df_NLRSE_2023.csv\")\n",
        "df_NLPConvAI = pd.read_csv(\"df_NLPConvAI_2023.csv\")\n",
        "df_narrative_understanding = pd.read_csv(\"df_Narrative_understanding_2023.csv\")\n",
        "df_matching_2023 = pd.read_csv(\"df_Matching_2023.csv\")\n",
        "df_IWSLT = pd.read_csv(\"df_IWSLT_2023.csv\")\n",
        "df_findings_acl = pd.read_csv(\"df_Findings_ACL_2023.csv\")\n",
        "df_DISRPT = pd.read_csv(\"df_DISRPT_2023.csv\")\n",
        "df_computational_approaches = pd.read_csv(\"df_computational_approaches.csv\")\n",
        "df_CODI23 = pd.read_csv(\"df_CODI23.csv\")\n",
        "df_CAWL_2023 = pd.read_csv(\"df_CAWL_2023.csv\")\n",
        "df_bioNLP = pd.read_csv(\"df_bioNLP.csv\")\n",
        "df_BEA_2023 = pd.read_csv(\"df_BEA_2023.csv\")\n",
        "df_americasNLP = pd.read_csv(\"df_americasNLP.csv\")\n",
        "df_clinical_nlp = pd.read_csv(\"df_5th_clinical_natural_language_processing_workshop.csv\")\n",
        "\n",
        "df = [df_short,df_long,df_trust_NLP,df_sigmorphon,df_woah,df_industry_track,df_student_research_workshop,df_third_dialdoc,df_sustain_NLP,df_Repl4NLP,df_NLRSE,df_NLPConvAI,df_narrative_understanding,df_matching_2023,df_IWSLT,df_findings_acl,df_DISRPT,df_computational_approaches,df_CODI23,df_CAWL_2023,df_bioNLP,df_BEA_2023,df_americasNLP,df_clinical_nlp]\n",
        "df = pd.concat(df, ignore_index=True) # adjusting the indices\n"
      ],
      "metadata": {
        "id": "FzSSzZnNX659"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace newline character '\\n' in the 'Text' column\n",
        "\n",
        "rows_to_remove = df['Text'].replace('\\n', ' ', regex=True)\n",
        "print(rows_to_remove)\n",
        "df['Text'] = df['Text'].replace('\\n', ' ', regex=True)\n",
        "\n",
        "\n",
        "# Replace characters '(' and ')' with content inside them\n",
        "rows_to_remove1 = df['Text'].replace(r'\\([^)]*\\)', '', regex=True)\n",
        "print(rows_to_remove1)\n",
        "df['Text'] = df['Text'].replace(r'\\([^)]*\\)', '', regex=True)\n",
        "\n",
        "\n",
        "# Filter out rows with length less than 272 (remove outliers)\n",
        "rows_to_remove2 = df[df['Text'].str.len() < 272]\n",
        "print(rows_to_remove2)\n",
        "df = df[df['Text'].str.len() >= 272]\n",
        "\n",
        "# Assuming 'Text' is the column containing your text data\n",
        "df['text_length'] = df['Text'].apply(len)\n",
        "df\n",
        "\n",
        "# Replace sentences containing links with an empty string\n",
        "rows_to_remove3 = df['Text'].str.replace(r'http\\S+|www\\S+', '', case=False)\n",
        "df['Text'] = df['Text'].str.replace(r'http\\S+|www\\S+', '', case=False)\n",
        "\n",
        "# Reset the index if needed (aligning the indices of dataframe)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Iterate through each row and replace if length is less than 20\n",
        "for i, sentence in enumerate(df['Text']):\n",
        "    if len(sentence.split()) < 20:\n",
        "        print(sentence)\n",
        "        df.at[i, 'Text'] = ''\n",
        "\n",
        "# Reset the index if needed\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Iterate through each sentence and split into smaller sentences based on '.'\n",
        "for i, sentence in enumerate(df['Text']):\n",
        "    sentences = sentence.split('.')\n",
        "\n",
        "    # Iterate through smaller sentences and replace if length is less than 7\n",
        "    for j, smaller_sentence in enumerate(sentences):\n",
        "        if len(smaller_sentence.split()) < 7:\n",
        "            print(sentences[j])\n",
        "            sentences[j] = ''\n",
        "\n",
        "    # Join smaller sentences back into a single sentence\n",
        "    updated_sentence = '.'.join(sentences)\n",
        "    df.at[i, 'Text'] = updated_sentence\n",
        "\n",
        "# Print rows to be removed\n",
        "rows_to_remove = df[df['Text'].str.startswith('. Did you discuss any potential risks of your work?')]\n",
        "print(\"Rows to be removed:\")\n",
        "print(rows_to_remove)\n",
        "\n",
        "# Remove rows where the text starts with 'Did you discuss...'\n",
        "df = df[~df['Text'].str.startswith('. Did you discuss any potential risks of your work?')]\n",
        "# Reset the index if needed\n",
        "df = df.reset_index(drop=True)\n",
        "# Remove 'limitations' from the beginning of each row in 'Text' column\n",
        "rows_to_remove4 = df['Text'].str.replace(r'^limitations\\s*', '', regex=True)\n",
        "print(rows_to_remove4)\n",
        "df['Text'] = df['Text'].str.replace(r'^limitations\\s*', '', regex=True)\n",
        "\n",
        "df['Text'] = df['Text'].str.replace(r'\\[CLS\\]', '', regex=True)\n",
        "\n",
        "# Define a function to remove words containing '&', '@', and '#'\n",
        "def remove_special_words(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if all(char not in word for char in ['&', '@', '#'])]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Apply the function to the 'Text' column\n",
        "df['Text'] = df['Text'].apply(remove_special_words)\n",
        "def process_sentences(text):\n",
        "    sentences = text.split('.')\n",
        "    sentences_with_future_work = [sentence for sentence in sentences if 'future work' in sentence.lower()]\n",
        "\n",
        "    if sentences_with_future_work:\n",
        "        print(\"Sentences containing 'future work':\")\n",
        "        for sentence in sentences_with_future_work:\n",
        "            print(f\"- {sentence.strip()}\")\n",
        "\n",
        "    updated_sentences = ['' if 'future work' in sentence.lower() else sentence for sentence in sentences]\n",
        "    return '.'.join(updated_sentences)\n",
        "\n",
        "# Apply the function to the 'Text' column\n",
        "df['Text'] = df['Text'].apply(process_sentences)\n",
        "def process_sentences(text):\n",
        "    keywords_to_remove = ['in the future', 'ethics', 'grants', 'appendix', 'section']\n",
        "\n",
        "    sentences = text.split('.')\n",
        "    sentences_to_remove = [sentence for sentence in sentences if any(keyword in sentence.lower() for keyword in keywords_to_remove)]\n",
        "\n",
        "    if sentences_to_remove:\n",
        "        print(\"Sentences to be removed:\")\n",
        "        for sentence in sentences_to_remove:\n",
        "            print(f\"- {sentence.strip()}\")\n",
        "\n",
        "    updated_sentences = ['' if any(keyword in sentence.lower() for keyword in keywords_to_remove) else sentence for sentence in sentences]\n",
        "    return '.'.join(updated_sentences)\n",
        "\n",
        "# Apply the function to the 'Text' column\n",
        "df['Text'] = df['Text'].apply(process_sentences)\n",
        "\n",
        "# Reset the index if needed\n",
        "df = df.reset_index(drop=True)\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download nltk sentence tokenizer data if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to capitalize the first letter of each sentence\n",
        "def capitalize_sentences(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    capitalized_sentences = [sentence.capitalize() for sentence in sentences]\n",
        "    return ' '.join(capitalized_sentences)\n",
        "\n",
        "# Apply the function to the 'Text' column\n",
        "df['Text'] = df['Text'].apply(capitalize_sentences)\n",
        "\n",
        "import re\n",
        "\n",
        "# Function to remove sentences from 'ethical consideration' to end\n",
        "def remove_ethical_sentences_to_end(text):\n",
        "    # Use regular expression to replace everything from 'ethical consideration' to end with an empty string\n",
        "    return re.sub(r'(?i)ethical consideration.*$', '', text)\n",
        "\n",
        "# Apply the function to the 'Text' column\n",
        "df['Text'] = df['Text'].apply(remove_ethical_sentences_to_end)\n",
        "\n",
        "# Reset the index if needed\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# remove the sentences which contains 'et al'\n",
        "def process_sentences(text):\n",
        "    sentences = text.split('.')\n",
        "    updated_sentences = ['' if 'et al' in sentence.lower() else sentence for sentence in sentences]\n",
        "    return '.'.join(updated_sentences)\n",
        "\n",
        "# Apply the function to the 'Text' column\n",
        "df['Text'] = df['Text'].apply(process_sentences)\n",
        "\n",
        "# remove the sentece which contains Greek letters\n",
        "import re\n",
        "\n",
        "# Function to check if a sentence contains Greek letters\n",
        "def contains_greek_letters(sentence):\n",
        "    # Using a regular expression to find Greek letters\n",
        "    return bool(re.search(r'[α-ωΑ-Ω]', sentence))\n",
        "\n",
        "# Function to process sentences and print/remove those containing Greek letters\n",
        "def process_and_print_sentences(text):\n",
        "    sentences = text.split('.')\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        if contains_greek_letters(sentence):\n",
        "            print(f\"Sentence with Greek letters (Index {i + 1}): {sentence}\")\n",
        "            sentences[i] = ''  # Replace the sentence with an empty string\n",
        "\n",
        "    return '.'.join(sentences)\n",
        "\n",
        "# Apply the function to the 'Text' column\n",
        "df['Text'] = df['Text'].apply(process_and_print_sentences)\n",
        "\n",
        "\n",
        "# Function to remove consecutive periods\n",
        "def remove_consecutive_periods(text):\n",
        "    return re.sub(r'\\.{2,}', '.', text)\n",
        "\n",
        "# Apply the function to the 'Text' column\n",
        "df['Text'] = df['Text'].apply(remove_consecutive_periods)\n",
        "\n",
        "# Print rows to be removed\n",
        "rows_to_remove = df[df['Text'].str.startswith('. did you discuss any potential risks of your work?')]\n",
        "print(\"Rows to be removed:\")\n",
        "print(rows_to_remove)\n",
        "\n",
        "# Remove rows where the text starts with 'Did you discuss...'\n",
        "df = df[~df['Text'].str.startswith('. did you discuss any potential risks of your work?')]\n",
        "\n",
        "# remove the sentence which contains '\\\\' character\n",
        "def process_sentences(text):\n",
        "    sentences = text.split('.')\n",
        "    sentences_with_future_work = [sentence for sentence in sentences if '\\\\' in sentence.lower()]\n",
        "\n",
        "    if sentences_with_future_work:\n",
        "        print(\"Sentences containing '\\\\':\")\n",
        "        for sentence in sentences_with_future_work:\n",
        "            print(f\"- {sentence.strip()}\")\n",
        "\n",
        "    updated_sentences = ['' if '\\\\' in sentence.lower() else sentence for sentence in sentences]\n",
        "    return '.'.join(updated_sentences)\n",
        "\n",
        "# Apply the function to the 'Text' column\n",
        "df['Text'] = df['Text'].apply(process_sentences)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KHUSb0oJX63f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "# Assuming 'Text' is the column containing sentences\n",
        "\n",
        "# Function to check if a string contains Chinese characters\n",
        "def contains_chinese(text):\n",
        "    chinese_pattern = re.compile(\"[\\u4e00-\\u9fa5]\")\n",
        "    return bool(re.search(chinese_pattern, text))\n",
        "\n",
        "# Function to filter out sentences with Chinese characters\n",
        "def filter_chinese_sentences(text):\n",
        "    sentences = text.split('.')\n",
        "    chinese_sentences = [sentence.strip() for sentence in sentences if contains_chinese(sentence)]\n",
        "\n",
        "    if chinese_sentences:\n",
        "        print(\"Chinese sentences found:\")\n",
        "        for chinese_sentence in chinese_sentences:\n",
        "            print(f\"- {chinese_sentence}\")\n",
        "\n",
        "    filtered_sentences = [sentence.strip() for sentence in sentences if not contains_chinese(sentence)]\n",
        "    return '.'.join(filtered_sentences)\n",
        "\n",
        "# Apply the filtering function to the 'Text' column\n",
        "df['Text'] = df['Text'].apply(filter_chinese_sentences)\n",
        "\n",
        "# Resetting the index after modification\n",
        "df = df.reset_index(drop=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "-HfAovMjYJMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "XaAFlQEmYMnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving the file"
      ],
      "metadata": {
        "id": "EF-pKjSKbwI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('df.csv',index=False)"
      ],
      "metadata": {
        "id": "bdolC089btTf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}