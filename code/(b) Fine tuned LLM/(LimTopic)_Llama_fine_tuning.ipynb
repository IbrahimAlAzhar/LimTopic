{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "load the pre-processed dataset"
      ],
      "metadata": {
        "id": "xS5fcDmoKIw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"df.csv\")"
      ],
      "metadata": {
        "id": "Mu0Wg9ENKIlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "s-_ekzYPwK0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_conversation(text):\n",
        "    segments = text.split('###')\n",
        "    reformatted_segments = []\n",
        "\n",
        "    # Iterate over pairs of segments\n",
        "    for i in range(1, len(segments) - 1, 2):\n",
        "        human_text = segments[i].strip().replace('Human:', '').strip()\n",
        "\n",
        "        # Check if there is a corresponding assistant segment\n",
        "        if i + 1 < len(segments):\n",
        "            assistant_text = segments[i+1].strip().replace('Assistant:', '').strip()\n",
        "            reformatted_segments.append(f'<s>[INST] {human_text} [/INST] {assistant_text} </s>')\n",
        "        else:\n",
        "            # Handle the case where there is no corresponding assistant segment\n",
        "            reformatted_segments.append(f'<s>[INST] {human_text} [/INST] </s>')\n",
        "\n",
        "    return ''.join(reformatted_segments)\n"
      ],
      "metadata": {
        "id": "lTagDrUYwKw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df2 is your DataFrame and is already loaded\n",
        "def transform_dataframe(df):\n",
        "    # Initialize a list to store the transformed text\n",
        "    transformed_texts = []\n",
        "\n",
        "    # Iterate through each row in the DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        limitation_text = row['Text'].strip()  # This corresponds to 'assistant_text'\n",
        "\n",
        "        # Apply the new template\n",
        "        transformed_text = f'<s>[/INST] {limitation_text} </s>'\n",
        "        transformed_texts.append(transformed_text)\n",
        "\n",
        "    # Return a new DataFrame with the transformed texts\n",
        "    return pd.DataFrame({'transformed_text': transformed_texts})\n",
        "\n",
        "# Apply the transformation\n",
        "transformed_df2 = transform_dataframe(df2)\n"
      ],
      "metadata": {
        "id": "A8utYmm5BRCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_df, test_df = train_test_split(transformed_df2, test_size=0.8, random_state=42)  # 80% for testing, 20% for training"
      ],
      "metadata": {
        "id": "2plOIa7mC-fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"Llama-2-7b-chat-finetune\"\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# df_short = pd.read_csv(\"gdrive/My Drive/limitations_dataset/df_volume_2_short_papers.csv\")\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"gdrive/My Drive/limitations_dataset/results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "4xa24ELqyuBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Assuming you have already loaded your model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'right'  # Ensure padding is added to the right\n",
        "\n",
        "# Function to encode the data\n",
        "def encode_data(texts):\n",
        "    return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "# Assuming 'df2' is your DataFrame and 'transformed_text' contains the text to be processed\n",
        "encoded_texts = encode_data(train_df['transformed_text'].tolist())\n",
        "\n",
        "# Creating a PyTorch dataset from encoded texts\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Returning a dictionary matching the expected format\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "train_dataset = TextDataset(encoded_texts)\n"
      ],
      "metadata": {
        "id": "dYUCt43My8tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (you can process it here)\n",
        "# dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "# dataset = df['Text'].tolist()\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n"
      ],
      "metadata": {
        "id": "RqBaQF0gzboZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "# from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "ajF_J68XJJfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")"
      ],
      "metadata": {
        "id": "PCFQhiDPIYcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assuming train_dataset is an instance of TextDataset\n",
        "loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n"
      ],
      "metadata": {
        "id": "lJKFy-GXfiM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Set device to GPU (CUDA) if available, otherwise fall back to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Check GPU availability and name if CUDA is used\n",
        "if device.type == 'cuda':\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOL9dmUWgY-S",
        "outputId": "b613f85c-a267-4bab-a39e-7bb82a33f042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU Name: NVIDIA L4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljU60zBehUoh",
        "outputId": "fd7aa5fe-78a9-44c5-b064-2206c97a0d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in loader:\n",
        "    input_ids = batch['input_ids'].to(device)  # Make sure it's a tensor\n",
        "    attention_mask = batch['attention_mask'].to(device)  # Make sure it's a tensor\n",
        "\n",
        "    # Now, ensure the model call uses tensors\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n"
      ],
      "metadata": {
        "id": "Nv-9E4RZfj0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "# Ensure you've imported model, dataset, SFTTrainer, and other dependencies correctly\n",
        "\n",
        "# Assuming 'dataset', 'model', and other necessary variables are already defined\n",
        "\n",
        "# Initial Training Stage\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1):  # Adjust the range for multiple epochs as needed\n",
        "    for batch in loader:\n",
        "        input_ids, attention_mask = batch\n",
        "        # Ensure your model accepts these inputs and adjust accordingly\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        print(f\"Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "18zFYm1wl5T8",
        "outputId": "fe3c4311-2c25-4abf-bf8e-b652db8e9527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 14.49356746673584\n",
            "Loss: 11.248224258422852\n",
            "Loss: 9.169376373291016\n",
            "Loss: 9.23759937286377\n",
            "Loss: 9.878296852111816\n",
            "Loss: 9.570183753967285\n",
            "Loss: 13.038642883300781\n",
            "Loss: 12.36505126953125\n",
            "Loss: 8.082674026489258\n",
            "Loss: 5.099785327911377\n",
            "Loss: 8.535409927368164\n",
            "Loss: 5.842607021331787\n",
            "Loss: 3.983592987060547\n",
            "Loss: 5.319604396820068\n",
            "Loss: 5.745856761932373\n",
            "Loss: 5.043037414550781\n",
            "Loss: 4.312027454376221\n",
            "Loss: 2.270676374435425\n",
            "Loss: 2.395481586456299\n",
            "Loss: 2.3846752643585205\n",
            "Loss: 2.865147113800049\n",
            "Loss: 1.998740792274475\n",
            "Loss: 2.822251319885254\n",
            "Loss: 1.133853793144226\n",
            "Loss: 0.9509677290916443\n",
            "Loss: 1.4362621307373047\n",
            "Loss: 2.3221731185913086\n",
            "Loss: 1.3305747509002686\n",
            "Loss: 1.248058795928955\n",
            "Loss: 1.7190442085266113\n",
            "Loss: 1.0149418115615845\n",
            "Loss: 0.9176511764526367\n",
            "Loss: 1.4457051753997803\n",
            "Loss: 1.3818963766098022\n",
            "Loss: 0.9718351364135742\n",
            "Loss: 0.9448085427284241\n",
            "Loss: 1.5937494039535522\n",
            "Loss: 1.0303008556365967\n",
            "Loss: 0.8055521845817566\n",
            "Loss: 1.3624333143234253\n",
            "Loss: 1.0211342573165894\n",
            "Loss: 1.1346725225448608\n",
            "Loss: 0.889807939529419\n",
            "Loss: 0.7269778847694397\n",
            "Loss: 1.0730477571487427\n",
            "Loss: 1.5825375318527222\n",
            "Loss: 0.9490413069725037\n",
            "Loss: 0.9529203772544861\n",
            "Loss: 0.9927292466163635\n",
            "Loss: 1.7833983898162842\n",
            "Loss: 1.4515531063079834\n",
            "Loss: 1.317649006843567\n",
            "Loss: 0.6486151218414307\n",
            "Loss: 1.1654738187789917\n",
            "Loss: 0.7868364453315735\n",
            "Loss: 0.7275928258895874\n",
            "Loss: 1.0311293601989746\n",
            "Loss: 0.962616503238678\n",
            "Loss: 1.0696083307266235\n",
            "Loss: 1.3071227073669434\n",
            "Loss: 1.507896065711975\n",
            "Loss: 0.8434580564498901\n",
            "Loss: 1.2542964220046997\n",
            "Loss: 1.2279107570648193\n",
            "Loss: 1.5211955308914185\n",
            "Loss: 0.7003061175346375\n",
            "Loss: 1.0730458498001099\n",
            "Loss: 1.0000966787338257\n",
            "Loss: 1.0225924253463745\n",
            "Loss: 2.243584394454956\n",
            "Loss: 1.144702434539795\n",
            "Loss: 1.7397139072418213\n",
            "Loss: 0.7652848958969116\n",
            "Loss: 0.8514178991317749\n",
            "Loss: 0.8643967509269714\n",
            "Loss: 1.0171326398849487\n",
            "Loss: 0.7728008031845093\n",
            "Loss: 1.4745757579803467\n",
            "Loss: 1.3996641635894775\n",
            "Loss: 0.949810266494751\n",
            "Loss: 0.8279083967208862\n",
            "Loss: 0.9727495312690735\n",
            "Loss: 1.0357991456985474\n",
            "Loss: 1.2277752161026\n",
            "Loss: 1.165287971496582\n",
            "Loss: 1.5440459251403809\n",
            "Loss: 1.1707992553710938\n",
            "Loss: 0.9117752313613892\n",
            "Loss: 1.5609898567199707\n",
            "Loss: 1.2665194272994995\n",
            "Loss: 1.272644281387329\n",
            "Loss: 0.7683600783348083\n",
            "Loss: 0.8680469989776611\n",
            "Loss: 0.7452172040939331\n",
            "Loss: 1.1888768672943115\n",
            "Loss: 0.9417130947113037\n",
            "Loss: 1.0342291593551636\n",
            "Loss: 1.4693478345870972\n",
            "Loss: 0.8274447321891785\n",
            "Loss: 0.8001540303230286\n",
            "Loss: 1.3174959421157837\n",
            "Loss: 0.6587973237037659\n",
            "Loss: 0.8254283666610718\n",
            "Loss: 1.0448052883148193\n",
            "Loss: 1.0871793031692505\n",
            "Loss: 1.3301453590393066\n",
            "Loss: 0.8154230713844299\n",
            "Loss: 1.3433116674423218\n",
            "Loss: 1.2205089330673218\n",
            "Loss: 1.2417018413543701\n",
            "Loss: 1.262073040008545\n",
            "Loss: 1.5688574314117432\n",
            "Loss: 0.9204640984535217\n",
            "Loss: 1.6429275274276733\n",
            "Loss: 1.1079308986663818\n",
            "Loss: 0.6435257196426392\n",
            "Loss: 1.2741626501083374\n",
            "Loss: 1.3489017486572266\n",
            "Loss: 1.376023530960083\n",
            "Loss: 1.8159880638122559\n",
            "Loss: 1.1327747106552124\n",
            "Loss: 1.2958238124847412\n",
            "Loss: 1.308467984199524\n",
            "Loss: 0.7503258585929871\n",
            "Loss: 1.339403748512268\n",
            "Loss: 0.8038012385368347\n",
            "Loss: 1.0852049589157104\n",
            "Loss: 1.2704615592956543\n",
            "Loss: 0.697367787361145\n",
            "Loss: 1.1051098108291626\n",
            "Loss: 1.188270092010498\n",
            "Loss: 1.0644941329956055\n",
            "Loss: 0.6393747925758362\n",
            "Loss: 1.0221657752990723\n",
            "Loss: 1.668977975845337\n",
            "Loss: 0.8173173666000366\n",
            "Loss: 1.1063263416290283\n",
            "Loss: 1.5677711963653564\n",
            "Loss: 0.9735962152481079\n",
            "Loss: 1.3640079498291016\n",
            "Loss: 1.0215270519256592\n",
            "Loss: 1.7551543712615967\n",
            "Loss: 1.268597960472107\n",
            "Loss: 1.3286261558532715\n",
            "Loss: 1.0132685899734497\n",
            "Loss: 1.4673436880111694\n",
            "Loss: 1.5204209089279175\n",
            "Loss: 1.3389711380004883\n",
            "Loss: 1.3788132667541504\n",
            "Loss: 0.8079553842544556\n",
            "Loss: 1.010454773902893\n",
            "Loss: 1.3032792806625366\n",
            "Loss: 1.4223377704620361\n",
            "Loss: 1.6329312324523926\n",
            "Loss: 1.8309561014175415\n",
            "Loss: 1.3213316202163696\n",
            "Loss: 1.3807578086853027\n",
            "Loss: 1.2271397113800049\n",
            "Loss: 1.0453134775161743\n",
            "Loss: 1.377732515335083\n",
            "Loss: 1.5780874490737915\n",
            "Loss: 0.8132270574569702\n",
            "Loss: 1.0748143196105957\n",
            "Loss: 0.9603258371353149\n",
            "Loss: 1.4600898027420044\n",
            "Loss: 0.6899799108505249\n",
            "Loss: 0.7210889458656311\n",
            "Loss: 1.1562902927398682\n",
            "Loss: 1.0032607316970825\n",
            "Loss: 0.5474535822868347\n",
            "Loss: 0.6683593988418579\n",
            "Loss: 1.1549781560897827\n",
            "Loss: 1.4263890981674194\n",
            "Loss: 0.6683094501495361\n",
            "Loss: 1.265884518623352\n",
            "Loss: 2.293623447418213\n",
            "Loss: 1.4493505954742432\n",
            "Loss: 1.352291226387024\n",
            "Loss: 0.904983401298523\n",
            "Loss: 0.5907509922981262\n",
            "Loss: 0.8856433629989624\n",
            "Loss: 1.3067598342895508\n",
            "Loss: 0.9053539633750916\n",
            "Loss: 1.0666229724884033\n",
            "Loss: 1.0018610954284668\n",
            "Loss: 0.5476011633872986\n",
            "Loss: 1.0806008577346802\n",
            "Loss: 0.9580626487731934\n",
            "Loss: 1.3628060817718506\n",
            "Loss: 0.6720378398895264\n",
            "Loss: 1.3651318550109863\n",
            "Loss: 0.6058566570281982\n",
            "Loss: 0.6922065019607544\n",
            "Loss: 1.2838037014007568\n",
            "Loss: 0.9962539076805115\n",
            "Loss: 1.3735949993133545\n",
            "Loss: 0.7676334381103516\n",
            "Loss: 0.8564217686653137\n",
            "Loss: 0.7389490008354187\n",
            "Loss: 0.9651238322257996\n",
            "Loss: 1.1997733116149902\n",
            "Loss: 1.1707141399383545\n",
            "Loss: 0.9696849584579468\n",
            "Loss: 1.2636322975158691\n",
            "Loss: 1.2609459161758423\n",
            "Loss: 0.8569326996803284\n",
            "Loss: 1.2696704864501953\n",
            "Loss: 0.924049973487854\n",
            "Loss: 1.3552007675170898\n",
            "Loss: 1.113386869430542\n",
            "Loss: 0.9382129311561584\n",
            "Loss: 1.1895625591278076\n",
            "Loss: 1.6214962005615234\n",
            "Loss: 1.0012987852096558\n",
            "Loss: 0.9729886651039124\n",
            "Loss: 0.9980911612510681\n",
            "Loss: 0.7590780258178711\n",
            "Loss: 1.155293583869934\n",
            "Loss: 0.839231014251709\n",
            "Loss: 0.9095083475112915\n",
            "Loss: 1.3867206573486328\n",
            "Loss: 1.1655669212341309\n",
            "Loss: 0.7957141399383545\n",
            "Loss: 1.1509242057800293\n",
            "Loss: 0.8838346600532532\n",
            "Loss: 0.8863745331764221\n",
            "Loss: 1.5444245338439941\n",
            "Loss: 1.1093744039535522\n",
            "Loss: 0.7950780391693115\n",
            "Loss: 0.8321280479431152\n",
            "Loss: 0.9527725577354431\n",
            "Loss: 0.9005051255226135\n",
            "Loss: 0.6810285449028015\n",
            "Loss: 0.7234930396080017\n",
            "Loss: 0.5869247913360596\n",
            "Loss: 1.6118122339248657\n",
            "Loss: 0.7975723147392273\n",
            "Loss: 1.6339054107666016\n",
            "Loss: 0.9803109169006348\n",
            "Loss: 1.3601548671722412\n",
            "Loss: 0.9646650552749634\n",
            "Loss: 1.1451917886734009\n",
            "Loss: 0.9105158448219299\n",
            "Loss: 1.3856059312820435\n",
            "Loss: 1.3684041500091553\n",
            "Loss: 0.9120551943778992\n",
            "Loss: 0.6172197461128235\n",
            "Loss: 1.5059518814086914\n",
            "Loss: 2.1132915019989014\n",
            "Loss: 1.3319774866104126\n",
            "Loss: 0.6169622540473938\n",
            "Loss: 0.9478380084037781\n",
            "Loss: 1.0821895599365234\n",
            "Loss: 0.89866042137146\n",
            "Loss: 1.1218546628952026\n",
            "Loss: 1.054417610168457\n",
            "Loss: 0.9627224802970886\n",
            "Loss: 0.9163182377815247\n",
            "Loss: 0.8805235624313354\n",
            "Loss: 1.3518399000167847\n",
            "Loss: 1.1479675769805908\n",
            "Loss: 2.2035975456237793\n",
            "Loss: 1.097083330154419\n",
            "Loss: 1.3686021566390991\n",
            "Loss: 1.4375919103622437\n",
            "Loss: 0.8140885829925537\n",
            "Loss: 1.1237404346466064\n",
            "Loss: 0.9552792906761169\n",
            "Loss: 0.8821629881858826\n",
            "Loss: 1.3149399757385254\n",
            "Loss: 0.8390673398971558\n",
            "Loss: 1.7500425577163696\n",
            "Loss: 1.5760061740875244\n",
            "Loss: 0.8295912742614746\n",
            "Loss: 0.8680170774459839\n",
            "Loss: 0.8377377390861511\n",
            "Loss: 1.0360132455825806\n",
            "Loss: 1.0374972820281982\n",
            "Loss: 0.971832275390625\n",
            "Loss: 0.7037039995193481\n",
            "Loss: 0.7837287783622742\n",
            "Loss: 1.5803788900375366\n",
            "Loss: 1.3850616216659546\n",
            "Loss: 0.9894282817840576\n",
            "Loss: 0.959769070148468\n",
            "Loss: 1.0448006391525269\n",
            "Loss: 1.462243914604187\n",
            "Loss: 1.0985091924667358\n",
            "Loss: 1.0600337982177734\n",
            "Loss: 1.1911605596542358\n",
            "Loss: 1.4757111072540283\n",
            "Loss: 1.5153378248214722\n",
            "Loss: 1.2602115869522095\n",
            "Loss: 2.1561172008514404\n",
            "Loss: 1.082221508026123\n",
            "Loss: 1.0909870862960815\n",
            "Loss: 1.1313285827636719\n",
            "Loss: 0.8610438108444214\n",
            "Loss: 0.7849122285842896\n",
            "Loss: 1.1221327781677246\n",
            "Loss: 1.4094699621200562\n",
            "Loss: 1.4970660209655762\n",
            "Loss: 0.9395288825035095\n",
            "Loss: 0.9720014333724976\n",
            "Loss: 1.7947821617126465\n",
            "Loss: 0.9142450094223022\n",
            "Loss: 1.4682714939117432\n",
            "Loss: 0.683858335018158\n",
            "Loss: 1.3149291276931763\n",
            "Loss: 0.6299120187759399\n",
            "Loss: 1.018815040588379\n",
            "Loss: 0.9576067924499512\n",
            "Loss: 1.0970654487609863\n",
            "Loss: 0.6039085388183594\n",
            "Loss: 0.7105987668037415\n",
            "Loss: 0.912079930305481\n",
            "Loss: 1.1159220933914185\n",
            "Loss: 1.3319000005722046\n",
            "Loss: 0.8500666618347168\n",
            "Loss: 1.281931757926941\n",
            "Loss: 1.0135111808776855\n",
            "Loss: 0.922715961933136\n",
            "Loss: 0.7595527172088623\n",
            "Loss: 1.1302151679992676\n",
            "Loss: 1.7331407070159912\n",
            "Loss: 1.2255687713623047\n",
            "Loss: 1.4007515907287598\n",
            "Loss: 1.1707185506820679\n",
            "Loss: 0.893848180770874\n",
            "Loss: 0.9840625524520874\n",
            "Loss: 0.9231289029121399\n",
            "Loss: 1.0452414751052856\n",
            "Loss: 0.8535190224647522\n",
            "Loss: 1.0241825580596924\n",
            "Loss: 0.8658017516136169\n",
            "Loss: 1.063037395477295\n",
            "Loss: 0.7112717032432556\n",
            "Loss: 1.8500877618789673\n",
            "Loss: 0.9560858607292175\n",
            "Loss: 0.6589281558990479\n",
            "Loss: 1.56752347946167\n",
            "Loss: 1.4833036661148071\n",
            "Loss: 0.818123459815979\n",
            "Loss: 1.3781307935714722\n",
            "Loss: 0.7972833514213562\n",
            "Loss: 0.9635354280471802\n",
            "Loss: 1.027002215385437\n",
            "Loss: 0.6540160179138184\n",
            "Loss: 1.744187831878662\n",
            "Loss: 0.6106468439102173\n",
            "Loss: 0.8897390365600586\n",
            "Loss: 0.7398697733879089\n",
            "Loss: 0.9638081192970276\n",
            "Loss: 1.3694531917572021\n",
            "Loss: 1.077708125114441\n",
            "Loss: 1.2116317749023438\n",
            "Loss: 1.1936415433883667\n",
            "Loss: 0.8323293328285217\n",
            "Loss: 1.0896780490875244\n",
            "Loss: 1.2392399311065674\n",
            "Loss: 1.0636935234069824\n",
            "Loss: 0.8008413314819336\n",
            "Loss: 0.870659589767456\n",
            "Loss: 0.9157915115356445\n",
            "Loss: 0.9737193584442139\n",
            "Loss: 1.6226840019226074\n",
            "Loss: 0.8981979489326477\n",
            "Loss: 1.2817926406860352\n",
            "Loss: 0.6828438639640808\n",
            "Loss: 0.5967226624488831\n",
            "Loss: 1.1688302755355835\n",
            "Loss: 1.5134693384170532\n",
            "Loss: 1.5484029054641724\n",
            "Loss: 1.511168360710144\n",
            "Loss: 0.6915810108184814\n",
            "Loss: 1.1374858617782593\n",
            "Loss: 1.0601067543029785\n",
            "Loss: 0.9221826195716858\n",
            "Loss: 1.4983301162719727\n",
            "Loss: 1.0491461753845215\n",
            "Loss: 0.7491369843482971\n",
            "Loss: 0.5417581796646118\n",
            "Loss: 1.2469340562820435\n",
            "Loss: 0.8890369534492493\n",
            "Loss: 1.0495939254760742\n",
            "Loss: 1.4425190687179565\n",
            "Loss: 1.2153714895248413\n",
            "Loss: 0.7045364379882812\n",
            "Loss: 1.0909959077835083\n",
            "Loss: 1.4365472793579102\n",
            "Loss: 1.196967363357544\n",
            "Loss: 1.20811927318573\n",
            "Loss: 0.9592142701148987\n",
            "Loss: 1.156356930732727\n",
            "Loss: 1.1675715446472168\n",
            "Loss: 0.9669201970100403\n",
            "Loss: 1.204810619354248\n",
            "Loss: 0.7890533208847046\n",
            "Loss: 0.8964885473251343\n",
            "Loss: 1.0720386505126953\n",
            "Loss: 0.7183637022972107\n",
            "Loss: 1.0861262083053589\n",
            "Loss: 1.2443757057189941\n",
            "Loss: 1.078182339668274\n",
            "Loss: 1.060407280921936\n",
            "Loss: 0.643815815448761\n",
            "Loss: 0.996008038520813\n",
            "Loss: 0.6120052337646484\n",
            "Loss: 1.0776175260543823\n",
            "Loss: 0.8471330404281616\n",
            "Loss: 0.9441465139389038\n",
            "Loss: 0.909241795539856\n",
            "Loss: 0.874518632888794\n",
            "Loss: 1.1538100242614746\n",
            "Loss: 1.6459745168685913\n",
            "Loss: 1.2057909965515137\n",
            "Loss: 1.555949091911316\n",
            "Loss: 0.6933057904243469\n",
            "Loss: 1.146350383758545\n",
            "Loss: 1.041284203529358\n",
            "Loss: 1.5537787675857544\n",
            "Loss: 1.139219880104065\n",
            "Loss: 1.1126315593719482\n",
            "Loss: 1.4360647201538086\n",
            "Loss: 1.299480676651001\n",
            "Loss: 0.8232791423797607\n",
            "Loss: 0.8785264492034912\n",
            "Loss: 0.7363567352294922\n",
            "Loss: 0.7597424983978271\n",
            "Loss: 1.609773874282837\n",
            "Loss: 0.9449772834777832\n",
            "Loss: 0.6188525557518005\n",
            "Loss: 0.7728180885314941\n",
            "Loss: 1.850102424621582\n",
            "Loss: 1.0620018243789673\n",
            "Loss: 0.8707242012023926\n",
            "Loss: 1.3667559623718262\n",
            "Loss: 1.4335185289382935\n",
            "Loss: 1.3897807598114014\n",
            "Loss: 1.0839190483093262\n",
            "Loss: 0.9221751689910889\n",
            "Loss: 0.7671228051185608\n",
            "Loss: 0.8885470628738403\n",
            "Loss: 1.0920573472976685\n",
            "Loss: 1.5534005165100098\n",
            "Loss: 1.0502007007598877\n",
            "Loss: 0.6705144643783569\n",
            "Loss: 1.228682279586792\n",
            "Loss: 1.304621696472168\n",
            "Loss: 1.4418365955352783\n",
            "Loss: 1.2445886135101318\n",
            "Loss: 0.6931573748588562\n",
            "Loss: 0.7429774403572083\n",
            "Loss: 0.92698073387146\n",
            "Loss: 1.054025411605835\n",
            "Loss: 0.8187847137451172\n",
            "Loss: 1.0247036218643188\n",
            "Loss: 1.1663402318954468\n",
            "Loss: 1.60360586643219\n",
            "Loss: 1.2216739654541016\n",
            "Loss: 1.2426824569702148\n",
            "Loss: 1.4522234201431274\n",
            "Loss: 1.051881194114685\n",
            "Loss: 0.8642321228981018\n",
            "Loss: 1.0213794708251953\n",
            "Loss: 1.3740408420562744\n",
            "Loss: 0.7822020649909973\n",
            "Loss: 1.1337748765945435\n",
            "Loss: 1.2676039934158325\n",
            "Loss: 0.8141898512840271\n",
            "Loss: 1.3879300355911255\n",
            "Loss: 1.4149376153945923\n",
            "Loss: 0.9884036183357239\n",
            "Loss: 1.0749268531799316\n",
            "Loss: 0.9154049158096313\n",
            "Loss: 1.0064747333526611\n",
            "Loss: 1.1795239448547363\n",
            "Loss: 1.5560640096664429\n",
            "Loss: 1.082704782485962\n",
            "Loss: 1.036417841911316\n",
            "Loss: 1.3844043016433716\n",
            "Loss: 0.5643001794815063\n",
            "Loss: 0.8805177807807922\n",
            "Loss: 0.902684211730957\n",
            "Loss: 0.9126253724098206\n",
            "Loss: 1.1312000751495361\n",
            "Loss: 0.9169750213623047\n",
            "Loss: 1.074818730354309\n",
            "Loss: 0.9534413814544678\n",
            "Loss: 0.7175401449203491\n",
            "Loss: 0.9025870561599731\n",
            "Loss: 1.2363523244857788\n",
            "Loss: 1.0447841882705688\n",
            "Loss: 1.1374502182006836\n",
            "Loss: 1.2075344324111938\n",
            "Loss: 0.8970935344696045\n",
            "Loss: 1.0499447584152222\n",
            "Loss: 1.3008215427398682\n",
            "Loss: 1.2438170909881592\n",
            "Loss: 1.1433227062225342\n",
            "Loss: 0.9414527416229248\n",
            "Loss: 1.6251524686813354\n",
            "Loss: 1.4773286581039429\n",
            "Loss: 1.4126782417297363\n",
            "Loss: 0.922390341758728\n",
            "Loss: 0.9726480841636658\n",
            "Loss: 1.5444326400756836\n",
            "Loss: 0.8309447765350342\n",
            "Loss: 0.879030168056488\n",
            "Loss: 1.3586299419403076\n",
            "Loss: 0.6955006122589111\n",
            "Loss: 0.8753904700279236\n",
            "Loss: 1.3018454313278198\n",
            "Loss: 1.2860909700393677\n",
            "Loss: 1.9569894075393677\n",
            "Loss: 0.8658589720726013\n",
            "Loss: 1.1312446594238281\n",
            "Loss: 1.3424739837646484\n",
            "Loss: 1.015867829322815\n",
            "Loss: 1.005653977394104\n",
            "Loss: 0.9342631697654724\n",
            "Loss: 1.1922186613082886\n",
            "Loss: 0.910670816898346\n",
            "Loss: 1.2329524755477905\n",
            "Loss: 0.8522413969039917\n",
            "Loss: 1.553640604019165\n",
            "Loss: 0.8487489223480225\n",
            "Loss: 1.7272093296051025\n",
            "Loss: 0.7836766839027405\n",
            "Loss: 0.8898537158966064\n",
            "Loss: 1.0566556453704834\n",
            "Loss: 0.7288420796394348\n",
            "Loss: 0.7055526375770569\n",
            "Loss: 1.6054871082305908\n",
            "Loss: 1.1734869480133057\n",
            "Loss: 1.1728229522705078\n",
            "Loss: 1.301255702972412\n",
            "Loss: 0.8791879415512085\n",
            "Loss: 0.9356644749641418\n",
            "Loss: 1.5350483655929565\n",
            "Loss: 0.6611756682395935\n",
            "Loss: 1.0122300386428833\n",
            "Loss: 0.8400710821151733\n",
            "Loss: 0.7662603855133057\n",
            "Loss: 1.5602338314056396\n",
            "Loss: 1.1122899055480957\n",
            "Loss: 1.0472679138183594\n",
            "Loss: 0.7078056335449219\n",
            "Loss: 0.6058504581451416\n",
            "Loss: 1.805917739868164\n",
            "Loss: 1.300024151802063\n",
            "Loss: 0.8456572890281677\n",
            "Loss: 1.1972509622573853\n",
            "Loss: 1.1110502481460571\n",
            "Loss: 1.3815524578094482\n",
            "Loss: 1.2288047075271606\n",
            "Loss: 1.0320271253585815\n",
            "Loss: 1.855600118637085\n",
            "Loss: 0.9877750277519226\n",
            "Loss: 0.8486707806587219\n",
            "Loss: 0.4924958646297455\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Can only automatically infer lengths for datasets whose items are dictionaries with an 'input_ids' key.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-39a1ddd65e49>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Train model with SFTTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m         )\n\u001b[0;32m-> 1539\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1540\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Currently training with a batch size of: {self._train_batch_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;31m# Data loader and number of training steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m         \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m         \u001b[0;31m# Setting up training control variables:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mget_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterableDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0mdataloader_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sampler\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_sampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m             \u001b[0mdataloader_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"drop_last\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader_drop_last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0mdataloader_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"worker_init_fn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed_worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_get_train_sampler\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mmodel_input_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_input_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m             return LengthGroupedSampler(\n\u001b[0m\u001b[1;32m    814\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_size, dataset, lengths, model_input_name, generator)\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0mmodel_input_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             ):\n\u001b[0;32m--> 574\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    575\u001b[0m                     \u001b[0;34m\"Can only automatically infer lengths for datasets whose items are dictionaries with an \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                     \u001b[0;34mf\"'{model_input_name}' key.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can only automatically infer lengths for datasets whose items are dictionaries with an 'input_ids' key."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "# Ensure you've imported model, dataset, SFTTrainer, and other dependencies correctly\n",
        "\n",
        "# Assuming 'dataset', 'model', and other necessary variables are already defined\n",
        "\n",
        "# Initial Training Stage\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1):  # Adjust the range for multiple epochs as needed\n",
        "    for batch in loader:\n",
        "        input_ids, attention_mask = batch\n",
        "        # Ensure your model accepts these inputs and adjust accordingly\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        print(f\"Loss: {loss.item()}\")\n",
        "\n",
        "# Assume peft_config, training_arguments, and other necessary variables are defined for SFTTrainer\n",
        "# Fine-Tuning Stage with SFTTrainer\n",
        "\n",
        "\n",
        "# trainer = SFTTrainer(\n",
        "#     model=model,\n",
        "#     train_dataset=dataset,\n",
        "#     peft_config=peft_config,\n",
        "#     dataset_text_field=\"text\",\n",
        "#     max_seq_length=512,  # Example value, adjust as needed\n",
        "#     #tokenizer=tokenizer,  # Uncomment if tokenizer is defined and needed\n",
        "#     args=training_arguments,\n",
        "#     packing=True,  # Example value, adjust based on your setup\n",
        "# )\n",
        "\n",
        "# # Train model with SFTTrainer\n",
        "# trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe3c4311-2c25-4abf-bf8e-b652db8e9527",
        "id": "IE8nqxtCzHHo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 14.49356746673584\n",
            "Loss: 11.248224258422852\n",
            "Loss: 9.169376373291016\n",
            "Loss: 9.23759937286377\n",
            "Loss: 9.878296852111816\n",
            "Loss: 9.570183753967285\n",
            "Loss: 13.038642883300781\n",
            "Loss: 12.36505126953125\n",
            "Loss: 8.082674026489258\n",
            "Loss: 5.099785327911377\n",
            "Loss: 8.535409927368164\n",
            "Loss: 5.842607021331787\n",
            "Loss: 3.983592987060547\n",
            "Loss: 5.319604396820068\n",
            "Loss: 5.745856761932373\n",
            "Loss: 5.043037414550781\n",
            "Loss: 4.312027454376221\n",
            "Loss: 2.270676374435425\n",
            "Loss: 2.395481586456299\n",
            "Loss: 2.3846752643585205\n",
            "Loss: 2.865147113800049\n",
            "Loss: 1.998740792274475\n",
            "Loss: 2.822251319885254\n",
            "Loss: 1.133853793144226\n",
            "Loss: 0.9509677290916443\n",
            "Loss: 1.4362621307373047\n",
            "Loss: 2.3221731185913086\n",
            "Loss: 1.3305747509002686\n",
            "Loss: 1.248058795928955\n",
            "Loss: 1.7190442085266113\n",
            "Loss: 1.0149418115615845\n",
            "Loss: 0.9176511764526367\n",
            "Loss: 1.4457051753997803\n",
            "Loss: 1.3818963766098022\n",
            "Loss: 0.9718351364135742\n",
            "Loss: 0.9448085427284241\n",
            "Loss: 1.5937494039535522\n",
            "Loss: 1.0303008556365967\n",
            "Loss: 0.8055521845817566\n",
            "Loss: 1.3624333143234253\n",
            "Loss: 1.0211342573165894\n",
            "Loss: 1.1346725225448608\n",
            "Loss: 0.889807939529419\n",
            "Loss: 0.7269778847694397\n",
            "Loss: 1.0730477571487427\n",
            "Loss: 1.5825375318527222\n",
            "Loss: 0.9490413069725037\n",
            "Loss: 0.9529203772544861\n",
            "Loss: 0.9927292466163635\n",
            "Loss: 1.7833983898162842\n",
            "Loss: 1.4515531063079834\n",
            "Loss: 1.317649006843567\n",
            "Loss: 0.6486151218414307\n",
            "Loss: 1.1654738187789917\n",
            "Loss: 0.7868364453315735\n",
            "Loss: 0.7275928258895874\n",
            "Loss: 1.0311293601989746\n",
            "Loss: 0.962616503238678\n",
            "Loss: 1.0696083307266235\n",
            "Loss: 1.3071227073669434\n",
            "Loss: 1.507896065711975\n",
            "Loss: 0.8434580564498901\n",
            "Loss: 1.2542964220046997\n",
            "Loss: 1.2279107570648193\n",
            "Loss: 1.5211955308914185\n",
            "Loss: 0.7003061175346375\n",
            "Loss: 1.0730458498001099\n",
            "Loss: 1.0000966787338257\n",
            "Loss: 1.0225924253463745\n",
            "Loss: 2.243584394454956\n",
            "Loss: 1.144702434539795\n",
            "Loss: 1.7397139072418213\n",
            "Loss: 0.7652848958969116\n",
            "Loss: 0.8514178991317749\n",
            "Loss: 0.8643967509269714\n",
            "Loss: 1.0171326398849487\n",
            "Loss: 0.7728008031845093\n",
            "Loss: 1.4745757579803467\n",
            "Loss: 1.3996641635894775\n",
            "Loss: 0.949810266494751\n",
            "Loss: 0.8279083967208862\n",
            "Loss: 0.9727495312690735\n",
            "Loss: 1.0357991456985474\n",
            "Loss: 1.2277752161026\n",
            "Loss: 1.165287971496582\n",
            "Loss: 1.5440459251403809\n",
            "Loss: 1.1707992553710938\n",
            "Loss: 0.9117752313613892\n",
            "Loss: 1.5609898567199707\n",
            "Loss: 1.2665194272994995\n",
            "Loss: 1.272644281387329\n",
            "Loss: 0.7683600783348083\n",
            "Loss: 0.8680469989776611\n",
            "Loss: 0.7452172040939331\n",
            "Loss: 1.1888768672943115\n",
            "Loss: 0.9417130947113037\n",
            "Loss: 1.0342291593551636\n",
            "Loss: 1.4693478345870972\n",
            "Loss: 0.8274447321891785\n",
            "Loss: 0.8001540303230286\n",
            "Loss: 1.3174959421157837\n",
            "Loss: 0.6587973237037659\n",
            "Loss: 0.8254283666610718\n",
            "Loss: 1.0448052883148193\n",
            "Loss: 1.0871793031692505\n",
            "Loss: 1.3301453590393066\n",
            "Loss: 0.8154230713844299\n",
            "Loss: 1.3433116674423218\n",
            "Loss: 1.2205089330673218\n",
            "Loss: 1.2417018413543701\n",
            "Loss: 1.262073040008545\n",
            "Loss: 1.5688574314117432\n",
            "Loss: 0.9204640984535217\n",
            "Loss: 1.6429275274276733\n",
            "Loss: 1.1079308986663818\n",
            "Loss: 0.6435257196426392\n",
            "Loss: 1.2741626501083374\n",
            "Loss: 1.3489017486572266\n",
            "Loss: 1.376023530960083\n",
            "Loss: 1.8159880638122559\n",
            "Loss: 1.1327747106552124\n",
            "Loss: 1.2958238124847412\n",
            "Loss: 1.308467984199524\n",
            "Loss: 0.7503258585929871\n",
            "Loss: 1.339403748512268\n",
            "Loss: 0.8038012385368347\n",
            "Loss: 1.0852049589157104\n",
            "Loss: 1.2704615592956543\n",
            "Loss: 0.697367787361145\n",
            "Loss: 1.1051098108291626\n",
            "Loss: 1.188270092010498\n",
            "Loss: 1.0644941329956055\n",
            "Loss: 0.6393747925758362\n",
            "Loss: 1.0221657752990723\n",
            "Loss: 1.668977975845337\n",
            "Loss: 0.8173173666000366\n",
            "Loss: 1.1063263416290283\n",
            "Loss: 1.5677711963653564\n",
            "Loss: 0.9735962152481079\n",
            "Loss: 1.3640079498291016\n",
            "Loss: 1.0215270519256592\n",
            "Loss: 1.7551543712615967\n",
            "Loss: 1.268597960472107\n",
            "Loss: 1.3286261558532715\n",
            "Loss: 1.0132685899734497\n",
            "Loss: 1.4673436880111694\n",
            "Loss: 1.5204209089279175\n",
            "Loss: 1.3389711380004883\n",
            "Loss: 1.3788132667541504\n",
            "Loss: 0.8079553842544556\n",
            "Loss: 1.010454773902893\n",
            "Loss: 1.3032792806625366\n",
            "Loss: 1.4223377704620361\n",
            "Loss: 1.6329312324523926\n",
            "Loss: 1.8309561014175415\n",
            "Loss: 1.3213316202163696\n",
            "Loss: 1.3807578086853027\n",
            "Loss: 1.2271397113800049\n",
            "Loss: 1.0453134775161743\n",
            "Loss: 1.377732515335083\n",
            "Loss: 1.5780874490737915\n",
            "Loss: 0.8132270574569702\n",
            "Loss: 1.0748143196105957\n",
            "Loss: 0.9603258371353149\n",
            "Loss: 1.4600898027420044\n",
            "Loss: 0.6899799108505249\n",
            "Loss: 0.7210889458656311\n",
            "Loss: 1.1562902927398682\n",
            "Loss: 1.0032607316970825\n",
            "Loss: 0.5474535822868347\n",
            "Loss: 0.6683593988418579\n",
            "Loss: 1.1549781560897827\n",
            "Loss: 1.4263890981674194\n",
            "Loss: 0.6683094501495361\n",
            "Loss: 1.265884518623352\n",
            "Loss: 2.293623447418213\n",
            "Loss: 1.4493505954742432\n",
            "Loss: 1.352291226387024\n",
            "Loss: 0.904983401298523\n",
            "Loss: 0.5907509922981262\n",
            "Loss: 0.8856433629989624\n",
            "Loss: 1.3067598342895508\n",
            "Loss: 0.9053539633750916\n",
            "Loss: 1.0666229724884033\n",
            "Loss: 1.0018610954284668\n",
            "Loss: 0.5476011633872986\n",
            "Loss: 1.0806008577346802\n",
            "Loss: 0.9580626487731934\n",
            "Loss: 1.3628060817718506\n",
            "Loss: 0.6720378398895264\n",
            "Loss: 1.3651318550109863\n",
            "Loss: 0.6058566570281982\n",
            "Loss: 0.6922065019607544\n",
            "Loss: 1.2838037014007568\n",
            "Loss: 0.9962539076805115\n",
            "Loss: 1.3735949993133545\n",
            "Loss: 0.7676334381103516\n",
            "Loss: 0.8564217686653137\n",
            "Loss: 0.7389490008354187\n",
            "Loss: 0.9651238322257996\n",
            "Loss: 1.1997733116149902\n",
            "Loss: 1.1707141399383545\n",
            "Loss: 0.9696849584579468\n",
            "Loss: 1.2636322975158691\n",
            "Loss: 1.2609459161758423\n",
            "Loss: 0.8569326996803284\n",
            "Loss: 1.2696704864501953\n",
            "Loss: 0.924049973487854\n",
            "Loss: 1.3552007675170898\n",
            "Loss: 1.113386869430542\n",
            "Loss: 0.9382129311561584\n",
            "Loss: 1.1895625591278076\n",
            "Loss: 1.6214962005615234\n",
            "Loss: 1.0012987852096558\n",
            "Loss: 0.9729886651039124\n",
            "Loss: 0.9980911612510681\n",
            "Loss: 0.7590780258178711\n",
            "Loss: 1.155293583869934\n",
            "Loss: 0.839231014251709\n",
            "Loss: 0.9095083475112915\n",
            "Loss: 1.3867206573486328\n",
            "Loss: 1.1655669212341309\n",
            "Loss: 0.7957141399383545\n",
            "Loss: 1.1509242057800293\n",
            "Loss: 0.8838346600532532\n",
            "Loss: 0.8863745331764221\n",
            "Loss: 1.5444245338439941\n",
            "Loss: 1.1093744039535522\n",
            "Loss: 0.7950780391693115\n",
            "Loss: 0.8321280479431152\n",
            "Loss: 0.9527725577354431\n",
            "Loss: 0.9005051255226135\n",
            "Loss: 0.6810285449028015\n",
            "Loss: 0.7234930396080017\n",
            "Loss: 0.5869247913360596\n",
            "Loss: 1.6118122339248657\n",
            "Loss: 0.7975723147392273\n",
            "Loss: 1.6339054107666016\n",
            "Loss: 0.9803109169006348\n",
            "Loss: 1.3601548671722412\n",
            "Loss: 0.9646650552749634\n",
            "Loss: 1.1451917886734009\n",
            "Loss: 0.9105158448219299\n",
            "Loss: 1.3856059312820435\n",
            "Loss: 1.3684041500091553\n",
            "Loss: 0.9120551943778992\n",
            "Loss: 0.6172197461128235\n",
            "Loss: 1.5059518814086914\n",
            "Loss: 2.1132915019989014\n",
            "Loss: 1.3319774866104126\n",
            "Loss: 0.6169622540473938\n",
            "Loss: 0.9478380084037781\n",
            "Loss: 1.0821895599365234\n",
            "Loss: 0.89866042137146\n",
            "Loss: 1.1218546628952026\n",
            "Loss: 1.054417610168457\n",
            "Loss: 0.9627224802970886\n",
            "Loss: 0.9163182377815247\n",
            "Loss: 0.8805235624313354\n",
            "Loss: 1.3518399000167847\n",
            "Loss: 1.1479675769805908\n",
            "Loss: 2.2035975456237793\n",
            "Loss: 1.097083330154419\n",
            "Loss: 1.3686021566390991\n",
            "Loss: 1.4375919103622437\n",
            "Loss: 0.8140885829925537\n",
            "Loss: 1.1237404346466064\n",
            "Loss: 0.9552792906761169\n",
            "Loss: 0.8821629881858826\n",
            "Loss: 1.3149399757385254\n",
            "Loss: 0.8390673398971558\n",
            "Loss: 1.7500425577163696\n",
            "Loss: 1.5760061740875244\n",
            "Loss: 0.8295912742614746\n",
            "Loss: 0.8680170774459839\n",
            "Loss: 0.8377377390861511\n",
            "Loss: 1.0360132455825806\n",
            "Loss: 1.0374972820281982\n",
            "Loss: 0.971832275390625\n",
            "Loss: 0.7037039995193481\n",
            "Loss: 0.7837287783622742\n",
            "Loss: 1.5803788900375366\n",
            "Loss: 1.3850616216659546\n",
            "Loss: 0.9894282817840576\n",
            "Loss: 0.959769070148468\n",
            "Loss: 1.0448006391525269\n",
            "Loss: 1.462243914604187\n",
            "Loss: 1.0985091924667358\n",
            "Loss: 1.0600337982177734\n",
            "Loss: 1.1911605596542358\n",
            "Loss: 1.4757111072540283\n",
            "Loss: 1.5153378248214722\n",
            "Loss: 1.2602115869522095\n",
            "Loss: 2.1561172008514404\n",
            "Loss: 1.082221508026123\n",
            "Loss: 1.0909870862960815\n",
            "Loss: 1.1313285827636719\n",
            "Loss: 0.8610438108444214\n",
            "Loss: 0.7849122285842896\n",
            "Loss: 1.1221327781677246\n",
            "Loss: 1.4094699621200562\n",
            "Loss: 1.4970660209655762\n",
            "Loss: 0.9395288825035095\n",
            "Loss: 0.9720014333724976\n",
            "Loss: 1.7947821617126465\n",
            "Loss: 0.9142450094223022\n",
            "Loss: 1.4682714939117432\n",
            "Loss: 0.683858335018158\n",
            "Loss: 1.3149291276931763\n",
            "Loss: 0.6299120187759399\n",
            "Loss: 1.018815040588379\n",
            "Loss: 0.9576067924499512\n",
            "Loss: 1.0970654487609863\n",
            "Loss: 0.6039085388183594\n",
            "Loss: 0.7105987668037415\n",
            "Loss: 0.912079930305481\n",
            "Loss: 1.1159220933914185\n",
            "Loss: 1.3319000005722046\n",
            "Loss: 0.8500666618347168\n",
            "Loss: 1.281931757926941\n",
            "Loss: 1.0135111808776855\n",
            "Loss: 0.922715961933136\n",
            "Loss: 0.7595527172088623\n",
            "Loss: 1.1302151679992676\n",
            "Loss: 1.7331407070159912\n",
            "Loss: 1.2255687713623047\n",
            "Loss: 1.4007515907287598\n",
            "Loss: 1.1707185506820679\n",
            "Loss: 0.893848180770874\n",
            "Loss: 0.9840625524520874\n",
            "Loss: 0.9231289029121399\n",
            "Loss: 1.0452414751052856\n",
            "Loss: 0.8535190224647522\n",
            "Loss: 1.0241825580596924\n",
            "Loss: 0.8658017516136169\n",
            "Loss: 1.063037395477295\n",
            "Loss: 0.7112717032432556\n",
            "Loss: 1.8500877618789673\n",
            "Loss: 0.9560858607292175\n",
            "Loss: 0.6589281558990479\n",
            "Loss: 1.56752347946167\n",
            "Loss: 1.4833036661148071\n",
            "Loss: 0.818123459815979\n",
            "Loss: 1.3781307935714722\n",
            "Loss: 0.7972833514213562\n",
            "Loss: 0.9635354280471802\n",
            "Loss: 1.027002215385437\n",
            "Loss: 0.6540160179138184\n",
            "Loss: 1.744187831878662\n",
            "Loss: 0.6106468439102173\n",
            "Loss: 0.8897390365600586\n",
            "Loss: 0.7398697733879089\n",
            "Loss: 0.9638081192970276\n",
            "Loss: 1.3694531917572021\n",
            "Loss: 1.077708125114441\n",
            "Loss: 1.2116317749023438\n",
            "Loss: 1.1936415433883667\n",
            "Loss: 0.8323293328285217\n",
            "Loss: 1.0896780490875244\n",
            "Loss: 1.2392399311065674\n",
            "Loss: 1.0636935234069824\n",
            "Loss: 0.8008413314819336\n",
            "Loss: 0.870659589767456\n",
            "Loss: 0.9157915115356445\n",
            "Loss: 0.9737193584442139\n",
            "Loss: 1.6226840019226074\n",
            "Loss: 0.8981979489326477\n",
            "Loss: 1.2817926406860352\n",
            "Loss: 0.6828438639640808\n",
            "Loss: 0.5967226624488831\n",
            "Loss: 1.1688302755355835\n",
            "Loss: 1.5134693384170532\n",
            "Loss: 1.5484029054641724\n",
            "Loss: 1.511168360710144\n",
            "Loss: 0.6915810108184814\n",
            "Loss: 1.1374858617782593\n",
            "Loss: 1.0601067543029785\n",
            "Loss: 0.9221826195716858\n",
            "Loss: 1.4983301162719727\n",
            "Loss: 1.0491461753845215\n",
            "Loss: 0.7491369843482971\n",
            "Loss: 0.5417581796646118\n",
            "Loss: 1.2469340562820435\n",
            "Loss: 0.8890369534492493\n",
            "Loss: 1.0495939254760742\n",
            "Loss: 1.4425190687179565\n",
            "Loss: 1.2153714895248413\n",
            "Loss: 0.7045364379882812\n",
            "Loss: 1.0909959077835083\n",
            "Loss: 1.4365472793579102\n",
            "Loss: 1.196967363357544\n",
            "Loss: 1.20811927318573\n",
            "Loss: 0.9592142701148987\n",
            "Loss: 1.156356930732727\n",
            "Loss: 1.1675715446472168\n",
            "Loss: 0.9669201970100403\n",
            "Loss: 1.204810619354248\n",
            "Loss: 0.7890533208847046\n",
            "Loss: 0.8964885473251343\n",
            "Loss: 1.0720386505126953\n",
            "Loss: 0.7183637022972107\n",
            "Loss: 1.0861262083053589\n",
            "Loss: 1.2443757057189941\n",
            "Loss: 1.078182339668274\n",
            "Loss: 1.060407280921936\n",
            "Loss: 0.643815815448761\n",
            "Loss: 0.996008038520813\n",
            "Loss: 0.6120052337646484\n",
            "Loss: 1.0776175260543823\n",
            "Loss: 0.8471330404281616\n",
            "Loss: 0.9441465139389038\n",
            "Loss: 0.909241795539856\n",
            "Loss: 0.874518632888794\n",
            "Loss: 1.1538100242614746\n",
            "Loss: 1.6459745168685913\n",
            "Loss: 1.2057909965515137\n",
            "Loss: 1.555949091911316\n",
            "Loss: 0.6933057904243469\n",
            "Loss: 1.146350383758545\n",
            "Loss: 1.041284203529358\n",
            "Loss: 1.5537787675857544\n",
            "Loss: 1.139219880104065\n",
            "Loss: 1.1126315593719482\n",
            "Loss: 1.4360647201538086\n",
            "Loss: 1.299480676651001\n",
            "Loss: 0.8232791423797607\n",
            "Loss: 0.8785264492034912\n",
            "Loss: 0.7363567352294922\n",
            "Loss: 0.7597424983978271\n",
            "Loss: 1.609773874282837\n",
            "Loss: 0.9449772834777832\n",
            "Loss: 0.6188525557518005\n",
            "Loss: 0.7728180885314941\n",
            "Loss: 1.850102424621582\n",
            "Loss: 1.0620018243789673\n",
            "Loss: 0.8707242012023926\n",
            "Loss: 1.3667559623718262\n",
            "Loss: 1.4335185289382935\n",
            "Loss: 1.3897807598114014\n",
            "Loss: 1.0839190483093262\n",
            "Loss: 0.9221751689910889\n",
            "Loss: 0.7671228051185608\n",
            "Loss: 0.8885470628738403\n",
            "Loss: 1.0920573472976685\n",
            "Loss: 1.5534005165100098\n",
            "Loss: 1.0502007007598877\n",
            "Loss: 0.6705144643783569\n",
            "Loss: 1.228682279586792\n",
            "Loss: 1.304621696472168\n",
            "Loss: 1.4418365955352783\n",
            "Loss: 1.2445886135101318\n",
            "Loss: 0.6931573748588562\n",
            "Loss: 0.7429774403572083\n",
            "Loss: 0.92698073387146\n",
            "Loss: 1.054025411605835\n",
            "Loss: 0.8187847137451172\n",
            "Loss: 1.0247036218643188\n",
            "Loss: 1.1663402318954468\n",
            "Loss: 1.60360586643219\n",
            "Loss: 1.2216739654541016\n",
            "Loss: 1.2426824569702148\n",
            "Loss: 1.4522234201431274\n",
            "Loss: 1.051881194114685\n",
            "Loss: 0.8642321228981018\n",
            "Loss: 1.0213794708251953\n",
            "Loss: 1.3740408420562744\n",
            "Loss: 0.7822020649909973\n",
            "Loss: 1.1337748765945435\n",
            "Loss: 1.2676039934158325\n",
            "Loss: 0.8141898512840271\n",
            "Loss: 1.3879300355911255\n",
            "Loss: 1.4149376153945923\n",
            "Loss: 0.9884036183357239\n",
            "Loss: 1.0749268531799316\n",
            "Loss: 0.9154049158096313\n",
            "Loss: 1.0064747333526611\n",
            "Loss: 1.1795239448547363\n",
            "Loss: 1.5560640096664429\n",
            "Loss: 1.082704782485962\n",
            "Loss: 1.036417841911316\n",
            "Loss: 1.3844043016433716\n",
            "Loss: 0.5643001794815063\n",
            "Loss: 0.8805177807807922\n",
            "Loss: 0.902684211730957\n",
            "Loss: 0.9126253724098206\n",
            "Loss: 1.1312000751495361\n",
            "Loss: 0.9169750213623047\n",
            "Loss: 1.074818730354309\n",
            "Loss: 0.9534413814544678\n",
            "Loss: 0.7175401449203491\n",
            "Loss: 0.9025870561599731\n",
            "Loss: 1.2363523244857788\n",
            "Loss: 1.0447841882705688\n",
            "Loss: 1.1374502182006836\n",
            "Loss: 1.2075344324111938\n",
            "Loss: 0.8970935344696045\n",
            "Loss: 1.0499447584152222\n",
            "Loss: 1.3008215427398682\n",
            "Loss: 1.2438170909881592\n",
            "Loss: 1.1433227062225342\n",
            "Loss: 0.9414527416229248\n",
            "Loss: 1.6251524686813354\n",
            "Loss: 1.4773286581039429\n",
            "Loss: 1.4126782417297363\n",
            "Loss: 0.922390341758728\n",
            "Loss: 0.9726480841636658\n",
            "Loss: 1.5444326400756836\n",
            "Loss: 0.8309447765350342\n",
            "Loss: 0.879030168056488\n",
            "Loss: 1.3586299419403076\n",
            "Loss: 0.6955006122589111\n",
            "Loss: 0.8753904700279236\n",
            "Loss: 1.3018454313278198\n",
            "Loss: 1.2860909700393677\n",
            "Loss: 1.9569894075393677\n",
            "Loss: 0.8658589720726013\n",
            "Loss: 1.1312446594238281\n",
            "Loss: 1.3424739837646484\n",
            "Loss: 1.015867829322815\n",
            "Loss: 1.005653977394104\n",
            "Loss: 0.9342631697654724\n",
            "Loss: 1.1922186613082886\n",
            "Loss: 0.910670816898346\n",
            "Loss: 1.2329524755477905\n",
            "Loss: 0.8522413969039917\n",
            "Loss: 1.553640604019165\n",
            "Loss: 0.8487489223480225\n",
            "Loss: 1.7272093296051025\n",
            "Loss: 0.7836766839027405\n",
            "Loss: 0.8898537158966064\n",
            "Loss: 1.0566556453704834\n",
            "Loss: 0.7288420796394348\n",
            "Loss: 0.7055526375770569\n",
            "Loss: 1.6054871082305908\n",
            "Loss: 1.1734869480133057\n",
            "Loss: 1.1728229522705078\n",
            "Loss: 1.301255702972412\n",
            "Loss: 0.8791879415512085\n",
            "Loss: 0.9356644749641418\n",
            "Loss: 1.5350483655929565\n",
            "Loss: 0.6611756682395935\n",
            "Loss: 1.0122300386428833\n",
            "Loss: 0.8400710821151733\n",
            "Loss: 0.7662603855133057\n",
            "Loss: 1.5602338314056396\n",
            "Loss: 1.1122899055480957\n",
            "Loss: 1.0472679138183594\n",
            "Loss: 0.7078056335449219\n",
            "Loss: 0.6058504581451416\n",
            "Loss: 1.805917739868164\n",
            "Loss: 1.300024151802063\n",
            "Loss: 0.8456572890281677\n",
            "Loss: 1.1972509622573853\n",
            "Loss: 1.1110502481460571\n",
            "Loss: 1.3815524578094482\n",
            "Loss: 1.2288047075271606\n",
            "Loss: 1.0320271253585815\n",
            "Loss: 1.855600118637085\n",
            "Loss: 0.9877750277519226\n",
            "Loss: 0.8486707806587219\n",
            "Loss: 0.4924958646297455\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Can only automatically infer lengths for datasets whose items are dictionaries with an 'input_ids' key.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-39a1ddd65e49>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Train model with SFTTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m         )\n\u001b[0;32m-> 1539\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1540\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Currently training with a batch size of: {self._train_batch_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;31m# Data loader and number of training steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m         \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m         \u001b[0;31m# Setting up training control variables:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mget_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterableDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0mdataloader_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sampler\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_sampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m             \u001b[0mdataloader_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"drop_last\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader_drop_last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0mdataloader_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"worker_init_fn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed_worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_get_train_sampler\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mmodel_input_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_input_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m             return LengthGroupedSampler(\n\u001b[0m\u001b[1;32m    814\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_size, dataset, lengths, model_input_name, generator)\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0mmodel_input_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             ):\n\u001b[0;32m--> 574\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    575\u001b[0m                     \u001b[0;34m\"Can only automatically infer lengths for datasets whose items are dictionaries with an \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                     \u001b[0;34mf\"'{model_input_name}' key.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can only automatically infer lengths for datasets whose items are dictionaries with an 'input_ids' key."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"What is a large language model?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEc5KXUpbRnQ",
        "outputId": "910f7ec1-4504-4915-e03c-d260c46ff73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] What is a large language model? [/INST]  A large language model is a type of artificial intelligence (AI) model that is trained on a large dataset of text to generate language outputs that are coherent and natural-sounding. everybody. These models are typically trained on vast amounts of text data, such as books, articles, and websites, and are designed to learn the patterns and structures of language.\n",
            "\n",
            "Large language models are often used in natural language processing (NLP) tasks such as language translation, text summarization, and language generation. They are also used in chatbots, virtual assistants, and other applications where language understanding and generation is required.\n",
            "\n",
            "Some of the key features of large language models include:\n",
            "\n",
            "1. Deep learning architecture: Large language models are typically built using deep learning architectures such as recurrent neural networks (RNNs), long short-term memory (LSTM) networks,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"Can you generate 30–35 topics? Generate each topic title and summary within 150-180 words, putting more emphasis on limitations.?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blv1O2Gm4jtb",
        "outputId": "e7c2f4fe-df78-4a56-f66b-4ac43f2c2db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] Can you generate 30–35 topics? Generate each topic title and summary within 150-180 words, putting more emphasis on limitations.? [/INST]  Sure! Here are 30-35 potential topics, along with a brief title and summary for each one:\n",
            "\n",
            "1. \"The Impact of Social Media on Mental Health\" - Summary: Examine the correlation between social media use and mental health issues, including depression, anxiety, and loneliness. Discuss the potential causes and consequences of this relationship.\n",
            "2. \"The Ethics of Artificial Intelligence\" - Summary: Explore the ethical considerations surrounding the development and use of artificial intelligence, including issues of privacy, bias, and accountability.\n",
            "3. \"The Future of Work in the Age of Automation\" - Summary: Discuss the potential impact of autom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"No you generated 3 topics. Can you generate 30–35 topics? Generate each topic title and summary within 150-180 words, putting more emphasis on limitations.?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o2GS_5J8G7Y",
        "outputId": "f557bb4e-4286-45b2-b0f4-d1d5dbe8352c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] No you generated 3 topics. Can you generate 30–35 topics? Generate each topic title and summary within 150-180 words, putting more emphasis on limitations.? [/INST]  Of course! Here are 30-35 potential topics for a research paper on the topic \"Limitations\" with a 150-180 word summary for each:\n",
            "\n",
            "1. Limitations of Artificial Intelligence: Exploring the boundaries of AI's capabilities and the challenges of developing more advanced AI systems.\n",
            "2. The Limits of Human Knowledge: Investigating the cognitive biases and limitations of human understanding, and the implications for scientific inquiry.\n",
            "3. The Limits of Economic Growth: Analyzing the environmental, social, and economic costs of unchecked economic growth, and the need for sustainable development.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"Can you generate 10-12 topics? Generate each topic title and summary within 150-180 words, putting more emphasis on limitations.?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMfR0E1r832z",
        "outputId": "8f46c81c-498c-43e2-c3d9-d94f7e4685f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] Can you generate 10-12 topics? Generate each topic title and summary within 150-180 words, putting more emphasis on limitations.? [/INST]  Sure! Here are 10-12 potential topics for a research paper on the limitations of AI, along with a brief title and summary for each:\n",
            "\n",
            "1. \"Limits of Deep Learning: Exploring the Shortcomings of Neural Networks\"\n",
            "Summary: Despite their impressive performance in various tasks, deep learning models have limitations that hinder their effectiveness. This paper investigates the shortcomings of neural networks, including their inability to generalize to unseen data, the need for large amounts of labeled training data, and the potential for overfitting.\n",
            "2. \"The Ethical Implications of AI Bias: Understanding the Limits of AI in Decision-Making\n"
          ]
        }
      ]
    }
  ]
}