{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "load the pre-processed dataset"
      ],
      "metadata": {
        "id": "xS5fcDmoKIw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"df.csv\")"
      ],
      "metadata": {
        "id": "Mu0Wg9ENKIlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "GF3kKlL21MCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_score_list = []\n",
        "sentence_model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
        "# cluster_model = KMeans(50)\n",
        "embeddings = sentence_model.encode(df['Text'], show_progress_bar=False)\n",
        "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
        "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
        "# embedding_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
        "umap_model = UMAP(n_neighbors=13, n_components=15,\n",
        "                    min_dist=0.0, metric='cosine',random_state=42) # If you want to reproduce the results, at the expense of performance, you can set a random_state in UMAP to prevent any stochastic behavior:\n",
        "topic_model = BERTopic(umap_model=umap_model,vectorizer_model=vectorizer_model,\n",
        "                        seed_topic_list = seed_words,\n",
        "                        top_n_words=5,ctfidf_model=ctfidf_model,min_topic_size=10,calculate_probabilities=True)\n",
        "topics, probs = topic_model.fit_transform(df['Text'], embeddings)\n",
        "\n",
        "# Generate `X` and `labels` only for non-outlier topics (as they are technically not clusters)\n",
        "umap_embeddings = topic_model.umap_model.transform(embeddings)\n",
        "indices = [index for index, topic in enumerate(topics) if topic != -1]\n",
        "X = umap_embeddings[np.array(indices)]\n",
        "labels = [topic for index, topic in enumerate(topics) if topic != -1]\n",
        "\n",
        "  # Calculate silhouette score\n",
        "a = silhouette_score(X, labels)\n",
        "print(\"silhouette score\",a)\n",
        "documents = pd.DataFrame({\"Document\": df['Text'],\n",
        "                          \"ID\": range(len(df['Text'])),\n",
        "                          \"Topic\": topics})\n",
        "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
        "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
        "\n",
        "# Extract vectorizer and analyzer from BERTopic\n",
        "vectorizer = topic_model.vectorizer_model\n",
        "analyzer = vectorizer.build_analyzer()\n",
        "\n",
        "# Extract features for Topic Coherence evaluation\n",
        "words = vectorizer.get_feature_names_out()\n",
        "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
        "dictionary = corpora.Dictionary(tokens)\n",
        "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
        "topic_words = [[words for words, _ in topic_model.get_topic(topic)]\n",
        "               for topic in range(len(set(topics))-1)]\n",
        "\n",
        "# Evaluate\n",
        "coherence_model = CoherenceModel(topics=topic_words,\n",
        "                                 texts=tokens,\n",
        "                                 corpus=corpus,\n",
        "                                 dictionary=dictionary,\n",
        "                                 coherence='c_v')\n",
        "coherence = coherence_model.get_coherence()\n",
        "print(\"coherence score is\",coherence)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246,
          "referenced_widgets": [
            "9390559c53e148f79483ddc6322f900d",
            "a3298f44e7e4417a8ce1bfa489affd43",
            "6bb24a21af6e48f0a118979f063e423e",
            "6ffdc8c216584363ad10c13ed66a6594",
            "59c4e2f6e457479fa6900d80cdc62322",
            "f9e4cd41767d4dae8d86e5f15efaea1b",
            "cb904fd545314d0f9eeed2e1ad479b4e",
            "7269212824f64bc4be0775ccfa6d1678",
            "b3bac057312248b596fc71ec82feb62a",
            "a66964a2c73640e3add989d62bb7ad01",
            "158cd1341dde4f55acd8749750b17bd5",
            "fe79b7069d0e49f1ab1c9f358b21fffe",
            "b1d3467b7a454fe2b2b389629df4bdb2",
            "3c1a040bf55c4a69ac813872a321a11b",
            "56b1466e3d6e458ba7841283589f043c",
            "130f0ecec35e407a8d2ee0ff8e513900",
            "b9e1a265e45d4e8e8f1a2ccbe0f6b65c",
            "bac12ec104e1439291026a69e87da3aa",
            "5b7cd705ac7d433b953c9caf1ba3b394",
            "d976f03c5b234e1ab73bfc842bda4581",
            "d92c496d66944b47927c6605126b0900",
            "ab2144cdb78e448d88c2e9b94671f680",
            "1df736abdf234e86abbe25ed62fb37a9",
            "2f285475a3184422a2d8474b67cc1a23",
            "41df54eab15e41dcb5ebb180b17f6dee",
            "0194f647bc12441395d40e913d2b1642",
            "a4a7743a7b274110a2332791e5eb2694",
            "a6943eb1553644efbfea68e2f4ca56c7",
            "1e7f30ec01ed488ebe65e9f3089aadb7",
            "e19ce1ad834245ce94b5a7d47730463d",
            "28718a1c45b64304b17241699e613ab7",
            "9555aa19f0754a4da068e4b342e91b17",
            "5ab31aab905f4c5d97078089eab16a0a",
            "7ac58d8fa2e8409187ee25f6d1eaf026",
            "311214f845834147b5c29d1ebd61bca2",
            "297ab155016345f0b2f3b1b805a77434",
            "31a69600a48f4e71b73d9b8b6abd1462",
            "02f8e0ff679549909caa57d87cb84f3b",
            "503e9613e60f40899b7d1af79b3633b6",
            "8c105a287bae47d9b2ab8865e4722e63",
            "98c0d355afc2430580353269331175f4",
            "cff8d55dd66d4b0bae0d486d740100c5",
            "4efd7d475abf4fcb99cf3002c0486f7c",
            "8a5881bff68a4503b10e112129fd80c7",
            "3a392c47036341b98a6637d4eb9d6553",
            "e9a07ca7c88749edb8872a88449be9fc",
            "8ef4f343f32843eda1339f11f44acef7",
            "2019fa156a2947fb8980e21368592e2e",
            "cee998464006488ea845a8f0e548e9a7",
            "f1856acaf7e04a6ea2b0af904afe7e3e",
            "d9af5e8776764c79a469bb781a93d18a",
            "cdc12e2c467c457495c3417db3104a0f",
            "e9103f9107414bc387e2aea2e771a27b",
            "a3bc5bab519b411092e680aecb02ecc0",
            "c1a2f7fd6a11437c888e7bcdec37d3eb",
            "8f51e6993df44775be57d27c2acb52b7",
            "b8eb3ec518664ae99f33856fe00c25dc",
            "4674fba46eba450585b5f408af2bc3d6",
            "36ed2696985541f1b22713f03f55569d",
            "9d4a3e7dcb2f4977a8037a2824eaf64a",
            "0764e349b804465fbecd6b411fe02f5d",
            "4fdaff0c55454f2d9cc2e70fe9f0fab0",
            "4ffe54c19af64878ace41cdef576a9e2",
            "09c204a184a2412f80a60c14afb52c9f",
            "86c4d3387ab04097811a289abf0cf80b",
            "7048167ca61e45e78ec14d227b47031d"
          ]
        },
        "id": "70JoE8vv6_Hf",
        "outputId": "e0e87a6f-80bf-44fd-d907-87231312878f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9390559c53e148f79483ddc6322f900d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe79b7069d0e49f1ab1c9f358b21fffe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1df736abdf234e86abbe25ed62fb37a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ac58d8fa2e8409187ee25f6d1eaf026"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a392c47036341b98a6637d4eb9d6553"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f51e6993df44775be57d27c2acb52b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "silhouette score 0.5483653\n",
            "coherence score is 0.5941087073257405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate `X` and `labels` only for non-outlier topics (as they are technically not clusters)\n",
        "umap_embeddings = topic_model.umap_model.transform(embeddings)\n",
        "indices = [index for index, topic in enumerate(topics) if topic != -1]\n",
        "X = umap_embeddings[np.array(indices)]\n",
        "labels = [topic for index, topic in enumerate(topics) if topic != -1]\n",
        "\n",
        "  # Calculate silhouette score\n",
        "silhouette_score(X, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82keZHVs7auf",
        "outputId": "6d495e8f-7789-4b58-8ae5-362cf2ecb779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5483653"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "most_representative_words_bertopic_best_1 = topic_model.get_topic_info()\n",
        "most_representative_words_bertopic_best_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mjNtOdTx8OkC",
        "outputId": "26cb7a8d-f2fc-4e9e-8e11-4c635f6fc06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Topic  Count                                               Name  \\\n",
              "0      -1    950          -1_performance_training_tasks_experiments   \n",
              "1       0    178  0_translation_languages_crosslingual_multilingual   \n",
              "2       1     88                  1_entity_entities_graph_knowledge   \n",
              "3       2     85                    2_visual_image_multimodal_scene   \n",
              "4       3     83                     3_qa_question_questions_answer   \n",
              "5       4     76          4_dialogue_conversations_dialog_dialogues   \n",
              "6       5     71                  5_clinical_medical_patients_notes   \n",
              "7       6     55                   6_gender_bias_debiasing_fairness   \n",
              "8       7     54                  7_sentiment_emotion_emotional_wer   \n",
              "9       8     54         8_tuning_hyperparameters_pts_metaretriever   \n",
              "10      9     52              9_adversarial_attack_attacks_backdoor   \n",
              "11     10     42       10_summarization_summaries_summary_longinput   \n",
              "12     11     31                           11_speech_asr_s2st_voice   \n",
              "13     12     29                       12_event_events_eae_argument   \n",
              "14     13     27                13_transformer_transformers_nas_btr   \n",
              "15     14     26     14_proprietary_opensourced_reflections_mathgpt   \n",
              "16     15     26         15_explanations_explanation_concepts_esnli   \n",
              "17     16     25                 16_reasoning_pangu_thinksum_skills   \n",
              "18     17     25                17_prompts_prompt_continuous_preadd   \n",
              "19     18     24                18_moral_offensive_hate_diffusiondb   \n",
              "20     19     21          19_oov_hypermixer_mlpbased_classification   \n",
              "21     20     21                 20_zeroshot_fewshot_verbalizers_ss   \n",
              "22     21     20               21_npm_softmax_outlier_nonparametric   \n",
              "23     22     19             22_chatgpt_chatgpts_chatbots_evaluator   \n",
              "24     23     19                         23_kanji_sentecon_ndd_tldt   \n",
              "25     24     16                           24_nat_decoding_slu_beam   \n",
              "26     25     16                       25_oosf_asap_instances_proxy   \n",
              "27     26     15                 26_dense_retrieval_retrievers_bm25   \n",
              "28     27     14             27_political_legislators_media_compass   \n",
              "29     28     13                         28_sarcasm_irony_humor_ocr   \n",
              "30     29     12          29_kd_distillation_counterfactual_teacher   \n",
              "31     30     12            30_tutoring_kids_instructional_teaching   \n",
              "32     31     11        31_stance_environmental_firms_argumentative   \n",
              "33     32     11  32_factchecking_misinformation_enforcement_fac...   \n",
              "34     33     10    33_scone_counterfactuals_counterspeech_negation   \n",
              "35     34     10               34_hint_pivot_distractor_distractors   \n",
              "\n",
              "                                       Representation  \\\n",
              "0   [performance, training, tasks, experiments, data]   \n",
              "1   [translation, languages, crosslingual, multili...   \n",
              "2            [entity, entities, graph, knowledge, kg]   \n",
              "3          [visual, image, multimodal, scene, images]   \n",
              "4            [qa, question, questions, answer, table]   \n",
              "5   [dialogue, conversations, dialog, dialogues, m...   \n",
              "6    [clinical, medical, patients, notes, biomedical]   \n",
              "7          [gender, bias, debiasing, fairness, harms]   \n",
              "8       [sentiment, emotion, emotional, wer, reviews]   \n",
              "9   [tuning, hyperparameters, pts, metaretriever, ...   \n",
              "10       [adversarial, attack, attacks, backdoor, al]   \n",
              "11  [summarization, summaries, summary, longinput,...   \n",
              "12                 [speech, asr, s2st, voice, lyrics]   \n",
              "13            [event, events, eae, argument, trigger]   \n",
              "14   [transformer, transformers, nas, btr, bytebased]   \n",
              "15  [proprietary, opensourced, reflections, mathgp...   \n",
              "16  [explanations, explanation, concepts, esnli, c...   \n",
              "17      [reasoning, pangu, thinksum, skills, circuit]   \n",
              "18         [prompts, prompt, continuous, preadd, mt0]   \n",
              "19      [moral, offensive, hate, diffusiondb, images]   \n",
              "20  [oov, hypermixer, mlpbased, classification, su...   \n",
              "21         [zeroshot, fewshot, verbalizers, ss, bots]   \n",
              "22  [npm, softmax, outlier, nonparametric, anisotr...   \n",
              "23  [chatgpt, chatgpts, chatbots, evaluator, rewri...   \n",
              "24             [kanji, sentecon, ndd, tldt, mentions]   \n",
              "25                [nat, decoding, slu, beam, speedup]   \n",
              "26              [oosf, asap, instances, proxy, label]   \n",
              "27      [dense, retrieval, retrievers, bm25, ranking]   \n",
              "28  [political, legislators, media, compass, leaning]   \n",
              "29               [sarcasm, irony, humor, ocr, yorker]   \n",
              "30   [kd, distillation, counterfactual, teacher, nmt]   \n",
              "31   [tutoring, kids, instructional, teaching, moves]   \n",
              "32  [stance, environmental, firms, argumentative, ...   \n",
              "33  [factchecking, misinformation, enforcement, fa...   \n",
              "34  [scone, counterfactuals, counterspeech, negati...   \n",
              "35      [hint, pivot, distractor, distractors, cloze]   \n",
              "\n",
              "                                  Representative_Docs  \n",
              "0   [Although the work is aimed at better understa...  \n",
              "1   [Our work presents a new dataset based on text...  \n",
              "2   [Of the conventional fact retrieval pipeline, ...  \n",
              "3   [Dataset utilization we have collected 137 dat...  \n",
              "4   [A limitation of rata is always assuming the a...  \n",
              "5   [Training data our pre-training data is source...  \n",
              "6   [There are a few limitations pertaining to the...  \n",
              "7   [Incomplete representation of all demographic ...  \n",
              "8   [Our proposed method is an offline system in w...  \n",
              "9   [The main limitation of our work is the high m...  \n",
              "10  [Due to the limited number of available code-r...  \n",
              "11  [.First, our analysis of continuity and contin...  \n",
              "12  [As mentioned in our experimental setup, we pr...  \n",
              "13  [Our method utilizes the amr annotations as ad...  \n",
              "14  [In our paper, we presented existing and novel...  \n",
              "15  [Comparison with gpt-3: there are growing conc...  \n",
              "16  [.First, we use the tcav framework, which assu...  \n",
              "17  [Our proposed thinksum has demonstrated strong...  \n",
              "18  [As with other prompting methods, preadd’s per...  \n",
              "19  [Ethical and societal considerations we consid...  \n",
              "20  [Data size: sugar is relatively small compared...  \n",
              "21  [The current work has several limitations that...  \n",
              "22  [.We acknowledge the limited scope of our expe...  \n",
              "23  [At the time of writing this work, chatgpt is ...  \n",
              "24  [As mentioned in chapter 2, since data is anno...  \n",
              "25  [Apart from all the advantages that our work a...  \n",
              "26  [., our findings are limited to this specific ...  \n",
              "27  [.First, our in-domain evaluation experiments ...  \n",
              "28  [The main limitation of the proposed study is ...  \n",
              "29  [The primary limitation for this work is the d...  \n",
              "30  [Our f -distill variants are less efficient to...  \n",
              "31  [By building this task-specific dialogue syste...  \n",
              "32  [We present a novel pair extraction task for u...  \n",
              "33  [Although our approach has demonstrated advant...  \n",
              "34  [The current work marks the first step towards...  \n",
              "35  [Although our nar approach can generate fluent...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f6325ca6-114d-4a3c-9bc4-719fa81a26a6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic</th>\n",
              "      <th>Count</th>\n",
              "      <th>Name</th>\n",
              "      <th>Representation</th>\n",
              "      <th>Representative_Docs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>950</td>\n",
              "      <td>-1_performance_training_tasks_experiments</td>\n",
              "      <td>[performance, training, tasks, experiments, data]</td>\n",
              "      <td>[Although the work is aimed at better understa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>178</td>\n",
              "      <td>0_translation_languages_crosslingual_multilingual</td>\n",
              "      <td>[translation, languages, crosslingual, multili...</td>\n",
              "      <td>[Our work presents a new dataset based on text...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>88</td>\n",
              "      <td>1_entity_entities_graph_knowledge</td>\n",
              "      <td>[entity, entities, graph, knowledge, kg]</td>\n",
              "      <td>[Of the conventional fact retrieval pipeline, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>85</td>\n",
              "      <td>2_visual_image_multimodal_scene</td>\n",
              "      <td>[visual, image, multimodal, scene, images]</td>\n",
              "      <td>[Dataset utilization we have collected 137 dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>83</td>\n",
              "      <td>3_qa_question_questions_answer</td>\n",
              "      <td>[qa, question, questions, answer, table]</td>\n",
              "      <td>[A limitation of rata is always assuming the a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>4_dialogue_conversations_dialog_dialogues</td>\n",
              "      <td>[dialogue, conversations, dialog, dialogues, m...</td>\n",
              "      <td>[Training data our pre-training data is source...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>71</td>\n",
              "      <td>5_clinical_medical_patients_notes</td>\n",
              "      <td>[clinical, medical, patients, notes, biomedical]</td>\n",
              "      <td>[There are a few limitations pertaining to the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6</td>\n",
              "      <td>55</td>\n",
              "      <td>6_gender_bias_debiasing_fairness</td>\n",
              "      <td>[gender, bias, debiasing, fairness, harms]</td>\n",
              "      <td>[Incomplete representation of all demographic ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7</td>\n",
              "      <td>54</td>\n",
              "      <td>7_sentiment_emotion_emotional_wer</td>\n",
              "      <td>[sentiment, emotion, emotional, wer, reviews]</td>\n",
              "      <td>[Our proposed method is an offline system in w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8</td>\n",
              "      <td>54</td>\n",
              "      <td>8_tuning_hyperparameters_pts_metaretriever</td>\n",
              "      <td>[tuning, hyperparameters, pts, metaretriever, ...</td>\n",
              "      <td>[The main limitation of our work is the high m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>9</td>\n",
              "      <td>52</td>\n",
              "      <td>9_adversarial_attack_attacks_backdoor</td>\n",
              "      <td>[adversarial, attack, attacks, backdoor, al]</td>\n",
              "      <td>[Due to the limited number of available code-r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>10</td>\n",
              "      <td>42</td>\n",
              "      <td>10_summarization_summaries_summary_longinput</td>\n",
              "      <td>[summarization, summaries, summary, longinput,...</td>\n",
              "      <td>[.First, our analysis of continuity and contin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>11</td>\n",
              "      <td>31</td>\n",
              "      <td>11_speech_asr_s2st_voice</td>\n",
              "      <td>[speech, asr, s2st, voice, lyrics]</td>\n",
              "      <td>[As mentioned in our experimental setup, we pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>12</td>\n",
              "      <td>29</td>\n",
              "      <td>12_event_events_eae_argument</td>\n",
              "      <td>[event, events, eae, argument, trigger]</td>\n",
              "      <td>[Our method utilizes the amr annotations as ad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>13</td>\n",
              "      <td>27</td>\n",
              "      <td>13_transformer_transformers_nas_btr</td>\n",
              "      <td>[transformer, transformers, nas, btr, bytebased]</td>\n",
              "      <td>[In our paper, we presented existing and novel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>14</td>\n",
              "      <td>26</td>\n",
              "      <td>14_proprietary_opensourced_reflections_mathgpt</td>\n",
              "      <td>[proprietary, opensourced, reflections, mathgp...</td>\n",
              "      <td>[Comparison with gpt-3: there are growing conc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>15</td>\n",
              "      <td>26</td>\n",
              "      <td>15_explanations_explanation_concepts_esnli</td>\n",
              "      <td>[explanations, explanation, concepts, esnli, c...</td>\n",
              "      <td>[.First, we use the tcav framework, which assu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>16</td>\n",
              "      <td>25</td>\n",
              "      <td>16_reasoning_pangu_thinksum_skills</td>\n",
              "      <td>[reasoning, pangu, thinksum, skills, circuit]</td>\n",
              "      <td>[Our proposed thinksum has demonstrated strong...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>17</td>\n",
              "      <td>25</td>\n",
              "      <td>17_prompts_prompt_continuous_preadd</td>\n",
              "      <td>[prompts, prompt, continuous, preadd, mt0]</td>\n",
              "      <td>[As with other prompting methods, preadd’s per...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>18</td>\n",
              "      <td>24</td>\n",
              "      <td>18_moral_offensive_hate_diffusiondb</td>\n",
              "      <td>[moral, offensive, hate, diffusiondb, images]</td>\n",
              "      <td>[Ethical and societal considerations we consid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>19</td>\n",
              "      <td>21</td>\n",
              "      <td>19_oov_hypermixer_mlpbased_classification</td>\n",
              "      <td>[oov, hypermixer, mlpbased, classification, su...</td>\n",
              "      <td>[Data size: sugar is relatively small compared...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>20</td>\n",
              "      <td>21</td>\n",
              "      <td>20_zeroshot_fewshot_verbalizers_ss</td>\n",
              "      <td>[zeroshot, fewshot, verbalizers, ss, bots]</td>\n",
              "      <td>[The current work has several limitations that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>21</td>\n",
              "      <td>20</td>\n",
              "      <td>21_npm_softmax_outlier_nonparametric</td>\n",
              "      <td>[npm, softmax, outlier, nonparametric, anisotr...</td>\n",
              "      <td>[.We acknowledge the limited scope of our expe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>22</td>\n",
              "      <td>19</td>\n",
              "      <td>22_chatgpt_chatgpts_chatbots_evaluator</td>\n",
              "      <td>[chatgpt, chatgpts, chatbots, evaluator, rewri...</td>\n",
              "      <td>[At the time of writing this work, chatgpt is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>23</td>\n",
              "      <td>19</td>\n",
              "      <td>23_kanji_sentecon_ndd_tldt</td>\n",
              "      <td>[kanji, sentecon, ndd, tldt, mentions]</td>\n",
              "      <td>[As mentioned in chapter 2, since data is anno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>24</td>\n",
              "      <td>16</td>\n",
              "      <td>24_nat_decoding_slu_beam</td>\n",
              "      <td>[nat, decoding, slu, beam, speedup]</td>\n",
              "      <td>[Apart from all the advantages that our work a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>25</td>\n",
              "      <td>16</td>\n",
              "      <td>25_oosf_asap_instances_proxy</td>\n",
              "      <td>[oosf, asap, instances, proxy, label]</td>\n",
              "      <td>[., our findings are limited to this specific ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>26</td>\n",
              "      <td>15</td>\n",
              "      <td>26_dense_retrieval_retrievers_bm25</td>\n",
              "      <td>[dense, retrieval, retrievers, bm25, ranking]</td>\n",
              "      <td>[.First, our in-domain evaluation experiments ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>27</td>\n",
              "      <td>14</td>\n",
              "      <td>27_political_legislators_media_compass</td>\n",
              "      <td>[political, legislators, media, compass, leaning]</td>\n",
              "      <td>[The main limitation of the proposed study is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>28_sarcasm_irony_humor_ocr</td>\n",
              "      <td>[sarcasm, irony, humor, ocr, yorker]</td>\n",
              "      <td>[The primary limitation for this work is the d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>29</td>\n",
              "      <td>12</td>\n",
              "      <td>29_kd_distillation_counterfactual_teacher</td>\n",
              "      <td>[kd, distillation, counterfactual, teacher, nmt]</td>\n",
              "      <td>[Our f -distill variants are less efficient to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>30</td>\n",
              "      <td>12</td>\n",
              "      <td>30_tutoring_kids_instructional_teaching</td>\n",
              "      <td>[tutoring, kids, instructional, teaching, moves]</td>\n",
              "      <td>[By building this task-specific dialogue syste...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>31</td>\n",
              "      <td>11</td>\n",
              "      <td>31_stance_environmental_firms_argumentative</td>\n",
              "      <td>[stance, environmental, firms, argumentative, ...</td>\n",
              "      <td>[We present a novel pair extraction task for u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>32</td>\n",
              "      <td>11</td>\n",
              "      <td>32_factchecking_misinformation_enforcement_fac...</td>\n",
              "      <td>[factchecking, misinformation, enforcement, fa...</td>\n",
              "      <td>[Although our approach has demonstrated advant...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>33</td>\n",
              "      <td>10</td>\n",
              "      <td>33_scone_counterfactuals_counterspeech_negation</td>\n",
              "      <td>[scone, counterfactuals, counterspeech, negati...</td>\n",
              "      <td>[The current work marks the first step towards...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>34</td>\n",
              "      <td>10</td>\n",
              "      <td>34_hint_pivot_distractor_distractors</td>\n",
              "      <td>[hint, pivot, distractor, distractors, cloze]</td>\n",
              "      <td>[Although our nar approach can generate fluent...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f6325ca6-114d-4a3c-9bc4-719fa81a26a6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f6325ca6-114d-4a3c-9bc4-719fa81a26a6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f6325ca6-114d-4a3c-9bc4-719fa81a26a6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-919e91b1-35c9-4238-8bdf-62ecbe120c8a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-919e91b1-35c9-4238-8bdf-62ecbe120c8a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-919e91b1-35c9-4238-8bdf-62ecbe120c8a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "most_representative_words_bertopic_best_1",
              "summary": "{\n  \"name\": \"most_representative_words_bertopic_best_1\",\n  \"rows\": 36,\n  \"fields\": [\n    {\n      \"column\": \"Topic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": -1,\n        \"max\": 34,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          34,\n          12,\n          25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 155,\n        \"min\": 10,\n        \"max\": 950,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          54,\n          27,\n          52\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 36,\n        \"samples\": [\n          \"34_hint_pivot_distractor_distractors\",\n          \"12_event_events_eae_argument\",\n          \"25_oosf_asap_instances_proxy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representation\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representative_Docs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "most_representative_words_bertopic_best_1.to_csv('df_bertopic.csv',index=False)"
      ],
      "metadata": {
        "id": "TreJ_Qk0qkxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# visualization"
      ],
      "metadata": {
        "id": "08-XNTJd9CGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "outputId": "2ebf2736-2ee3-4fef-a208-847d1adeea01",
        "id": "K8sSS56o9KdX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"9134e448-d97d-4153-8fdf-cb3e57dbd384\" class=\"plotly-graph-div\" style=\"height:650px; width:650px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9134e448-d97d-4153-8fdf-cb3e57dbd384\")) {                    Plotly.newPlot(                        \"9134e448-d97d-4153-8fdf-cb3e57dbd384\",                        [{\"customdata\":[[0,\"translation | languages | multilingual | translations | crosslingual\",157],[1,\"entity | entities | knowledge | graph | kg\",94],[2,\"visual | image | multimodal | scene | images\",85],[3,\"dialogue | conversations | dialog | dialogues | multiparty\",68],[4,\"clinical | medical | patients | notes | biomedical\",67],[5,\"hyperparameters | metaretriever | pts | radius | optimization\",62],[6,\"adversarial | attacks | attack | backdoor | al\",59],[7,\"summarization | summaries | summary | factual | msmo\",58],[8,\"sentiment | emotion | emotional | wer | reviews\",56],[9,\"gender | bias | debiasing | harms | social\",54],[10,\"qa | question | questions | answer | qe\",53],[11,\"transformer | nat | transformers | decoding | encoder\",48],[12,\"political | moral | hate | social | media\",44],[13,\"reasoning | pangu | symbolic | rule | thinksum\",33],[14,\"speech | asr | s2st | voice | lyrics\",32],[15,\"classification | oov | slash | hypermixer | mlpbased\",30],[16,\"event | events | eae | argument | trigger\",29],[17,\"explanations | explanation | concepts | reasoning | cpace\",29],[18,\"prompts | prompt | continuous | preadd | mt0\",22],[19,\"zeroshot | fewshot | verbalizers | ss | bots\",20],[20,\"chatgpt | chatgpts | rewriting | responses | query\",18],[21,\"tables | table | tableqa | cells | ctbls\",15],[22,\"oosf | instances | proxy | label | false\",13],[23,\"figurative | metaphors | sociocultural | metaphor | metaphorical\",13],[24,\"sarcasm | irony | humor | ocr | yorker\",13],[25,\"dense | retrieval | retrievers | bm25 | recontriever\",12],[26,\"argumentative | environmental | firms | argument | episode\",11],[27,\"proprietary | opensourced | recommendations | utility | eccnps\",11],[28,\"modification | mp2 | multiword | pram | pluglm\",11],[29,\"instructional | teaching | students | moves | ncte\",11],[30,\"counterfactuals | scone | counterfactual | rev | negation\",10]],\"hovertemplate\":\"\\u003cb\\u003eTopic %{customdata[0]}\\u003c\\u002fb\\u003e\\u003cbr\\u003e%{customdata[1]}\\u003cbr\\u003eSize: %{customdata[2]}\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#B0BEC5\",\"size\":[157,94,85,68,67,62,59,58,56,54,53,48,44,33,32,30,29,29,22,20,18,15,13,13,13,12,11,11,11,11,10],\"sizemode\":\"area\",\"sizeref\":0.098125,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":2}},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[13.950067520141602,10.800604820251465,10.589239120483398,6.160094261169434,12.107478141784668,14.403127670288086,11.643406867980957,11.862299919128418,-5.060096263885498,-4.3355793952941895,11.365096092224121,14.03451919555664,-4.629424571990967,5.053394317626953,13.991646766662598,11.31937026977539,10.853475570678711,4.934371471405029,6.342162132263184,10.907149314880371,5.412647247314453,10.376638412475586,14.749341011047363,10.421709060668945,-5.272303104400635,11.821292877197266,-4.841259956359863,5.280398845672607,13.775520324707031,5.826863765716553,-4.225142002105713],\"xaxis\":\"x\",\"y\":[6.7852983474731445,5.391666412353516,7.54397439956665,17.633230209350586,6.13939905166626,7.4181694984436035,6.040414810180664,6.207565784454346,1.7869296073913574,2.299912929534912,6.305420875549316,7.2498064041137695,1.9577069282531738,-11.718055725097656,6.563033580780029,6.6647186279296875,5.254662036895752,-11.837292671203613,17.51994514465332,7.146697521209717,18.083356857299805,5.195248603820801,7.283417701721191,7.7226715087890625,1.6503450870513916,6.57499885559082,2.1385016441345215,18.18610954284668,7.513699531555176,17.826486587524414,2.3473026752471924],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"\"},\"visible\":false,\"range\":[-6.06314857006073,16.961742162704468]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"\"},\"visible\":false,\"range\":[-13.612886571884156,20.91402597427368]},\"legend\":{\"tracegroupgap\":0,\"itemsizing\":\"constant\"},\"margin\":{\"t\":60},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eIntertopic Distance Map\\u003c\\u002fb\\u003e\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"width\":650,\"height\":650,\"sliders\":[{\"active\":0,\"pad\":{\"t\":50},\"steps\":[{\"args\":[{\"marker.color\":[[\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 0\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 1\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 2\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 3\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 4\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 5\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 6\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 7\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 8\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 9\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 10\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 11\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 12\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 13\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 14\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 15\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 16\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 17\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 18\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 19\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 20\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 21\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 22\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 23\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 24\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 25\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 26\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 27\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 28\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\"]]}],\"label\":\"Topic 29\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\"]]}],\"label\":\"Topic 30\",\"method\":\"update\"}]}],\"shapes\":[{\"line\":{\"color\":\"#CFD8DC\",\"width\":2},\"type\":\"line\",\"x0\":5.449296796321869,\"x1\":5.449296796321869,\"y0\":-13.612886571884156,\"y1\":20.91402597427368},{\"line\":{\"color\":\"#9E9E9E\",\"width\":2},\"type\":\"line\",\"x0\":-6.06314857006073,\"x1\":16.961742162704468,\"y0\":3.6505697011947627,\"y1\":3.6505697011947627}],\"annotations\":[{\"showarrow\":false,\"text\":\"D1\",\"x\":-6.06314857006073,\"y\":3.6505697011947627,\"yshift\":10},{\"showarrow\":false,\"text\":\"D2\",\"x\":5.449296796321869,\"xshift\":10,\"y\":20.91402597427368}]},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9134e448-d97d-4153-8fdf-cb3e57dbd384');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_heatmap()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "outputId": "76e391e4-2707-41ee-dc4b-b39f01e740f4",
        "id": "CtA4V6NQ9KdY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"ebd42df5-b064-43ec-aced-49692b7a3e7f\" class=\"plotly-graph-div\" style=\"height:800px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ebd42df5-b064-43ec-aced-49692b7a3e7f\")) {                    Plotly.newPlot(                        \"ebd42df5-b064-43ec-aced-49692b7a3e7f\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"0_translation_languages_mul...\",\"1_entity_entities_knowledge\",\"2_visual_image_multimodal\",\"3_dialogue_conversations_di...\",\"4_clinical_medical_patients\",\"5_hyperparameters_metaretri...\",\"6_adversarial_attacks_attack\",\"7_summarization_summaries_s...\",\"8_sentiment_emotion_emotional\",\"9_gender_bias_debiasing\",\"10_qa_question_questions\",\"11_transformer_nat_transfor...\",\"12_political_moral_hate\",\"13_reasoning_pangu_symbolic\",\"14_speech_asr_s2st\",\"15_classification_oov_slash\",\"16_event_events_eae\",\"17_explanations_explanation...\",\"18_prompts_prompt_continuous\",\"19_zeroshot_fewshot_verbali...\",\"20_chatgpt_chatgpts_rewriting\",\"21_tables_table_tableqa\",\"22_oosf_instances_proxy\",\"23_figurative_metaphors_soc...\",\"24_sarcasm_irony_humor\",\"25_dense_retrieval_retrievers\",\"26_argumentative_environmen...\",\"27_proprietary_opensourced_...\",\"28_modification_mp2_multiword\",\"29_instructional_teaching_s...\",\"30_counterfactuals_scone_co...\"],\"y\":[\"0_translation_languages_mul...\",\"1_entity_entities_knowledge\",\"2_visual_image_multimodal\",\"3_dialogue_conversations_di...\",\"4_clinical_medical_patients\",\"5_hyperparameters_metaretri...\",\"6_adversarial_attacks_attack\",\"7_summarization_summaries_s...\",\"8_sentiment_emotion_emotional\",\"9_gender_bias_debiasing\",\"10_qa_question_questions\",\"11_transformer_nat_transfor...\",\"12_political_moral_hate\",\"13_reasoning_pangu_symbolic\",\"14_speech_asr_s2st\",\"15_classification_oov_slash\",\"16_event_events_eae\",\"17_explanations_explanation...\",\"18_prompts_prompt_continuous\",\"19_zeroshot_fewshot_verbali...\",\"20_chatgpt_chatgpts_rewriting\",\"21_tables_table_tableqa\",\"22_oosf_instances_proxy\",\"23_figurative_metaphors_soc...\",\"24_sarcasm_irony_humor\",\"25_dense_retrieval_retrievers\",\"26_argumentative_environmen...\",\"27_proprietary_opensourced_...\",\"28_modification_mp2_multiword\",\"29_instructional_teaching_s...\",\"30_counterfactuals_scone_co...\"],\"z\":[[1.0,0.6596372723579407,0.6926851272583008,0.6127042174339294,0.6420643329620361,0.5539402961730957,0.5855865478515625,0.6342008709907532,0.5835689306259155,0.6081619262695312,0.602232038974762,0.6399763822555542,0.5566428899765015,0.5661023259162903,0.7419392466545105,0.7604697942733765,0.5294350981712341,0.578955888748169,0.5983462333679199,0.676190197467804,0.566741943359375,0.5406547784805298,0.5178927183151245,0.639463484287262,0.46830400824546814,0.5380945205688477,0.5409373044967651,0.5342839360237122,0.7760194540023804,0.570012092590332,0.6918050050735474],[0.6596372723579407,1.0000001192092896,0.6432157158851624,0.6162364482879639,0.6546310186386108,0.5526120066642761,0.577985405921936,0.6625269651412964,0.6351559162139893,0.46969735622406006,0.740241527557373,0.5257775783538818,0.555476725101471,0.6692092418670654,0.4776172339916229,0.7958236336708069,0.6992630958557129,0.6768325567245483,0.5685755014419556,0.6803222894668579,0.5474711656570435,0.6983601450920105,0.5878087282180786,0.5482839941978455,0.46189194917678833,0.6431441903114319,0.665803849697113,0.4806312918663025,0.66213059425354,0.5018028020858765,0.5590258836746216],[0.6926851272583008,0.6432157158851624,1.0,0.6224387288093567,0.5883114337921143,0.6159347295761108,0.6111356019973755,0.5872692465782166,0.6286522746086121,0.4848497211933136,0.603594183921814,0.6528754234313965,0.5154016017913818,0.5294806361198425,0.6363270282745361,0.7501703500747681,0.5314404964447021,0.5923506021499634,0.5679072141647339,0.7711349725723267,0.509975790977478,0.5230879783630371,0.5714255571365356,0.6457740664482117,0.5548228025436401,0.5721415281295776,0.5157702565193176,0.509526252746582,0.6731486320495605,0.5205578804016113,0.5553864240646362],[0.6127042174339294,0.6162364482879639,0.6224387288093567,1.0,0.5625877380371094,0.5420886278152466,0.5269028544425964,0.6099127531051636,0.6490182280540466,0.47416967153549194,0.7106642127037048,0.5412070155143738,0.5114123821258545,0.5741095542907715,0.6465232372283936,0.6850697994232178,0.5414726734161377,0.6179623007774353,0.733052134513855,0.622813880443573,0.7290420532226562,0.48203882575035095,0.42672064900398254,0.5335017442703247,0.5018593072891235,0.4881211519241333,0.6015769243240356,0.4620828628540039,0.5933831930160522,0.7683202028274536,0.6061824560165405],[0.6420643329620361,0.6546310186386108,0.5883114337921143,0.5625877380371094,0.9999997615814209,0.5395172834396362,0.4949426054954529,0.6655230522155762,0.6120049953460693,0.4920942187309265,0.6203290224075317,0.46451446413993835,0.5012487173080444,0.545889139175415,0.5527323484420776,0.6828781366348267,0.5289710760116577,0.5938454866409302,0.5775022506713867,0.5772743225097656,0.5976182222366333,0.5755689144134521,0.5432724952697754,0.45656752586364746,0.4329531490802765,0.5219279527664185,0.5838639736175537,0.6044294834136963,0.5915855765342712,0.553398847579956,0.6172075271606445],[0.5539402961730957,0.5526120066642761,0.6159347295761108,0.5420886278152466,0.5395172834396362,0.9999998807907104,0.5715870261192322,0.47210532426834106,0.48076891899108887,0.4238911271095276,0.5807185173034668,0.7052825689315796,0.3671690821647644,0.6059610843658447,0.5797759890556335,0.6749100685119629,0.46966707706451416,0.5299734473228455,0.5793873071670532,0.5775244832038879,0.4774233102798462,0.577131450176239,0.6927689909934998,0.31182241439819336,0.3209766447544098,0.5922660827636719,0.4374317526817322,0.6502724885940552,0.7097382545471191,0.5522146821022034,0.5258595943450928],[0.5855865478515625,0.577985405921936,0.6111356019973755,0.5269028544425964,0.4949426054954529,0.5715870261192322,1.0,0.5246492624282837,0.5630618929862976,0.5494258403778076,0.5661267638206482,0.5628679990768433,0.6226789951324463,0.5389155745506287,0.5240493416786194,0.6893130540847778,0.47621649503707886,0.5632842779159546,0.5335252285003662,0.5954482555389404,0.4936511814594269,0.4718976616859436,0.5971082448959351,0.4498765468597412,0.5101771354675293,0.5211006999015808,0.5699928402900696,0.48563605546951294,0.5883063077926636,0.46323922276496887,0.5870155692100525],[0.6342008709907532,0.6625269651412964,0.5872692465782166,0.6099127531051636,0.6655230522155762,0.47210532426834106,0.5246492624282837,1.0,0.6207262277603149,0.5109829306602478,0.6769375801086426,0.4757775664329529,0.5564506649971008,0.5249332785606384,0.4968632459640503,0.7166787385940552,0.5701935291290283,0.6134517788887024,0.5487676858901978,0.5846644639968872,0.5404407978057861,0.5274950265884399,0.48270487785339355,0.5114701986312866,0.4744163751602173,0.5758219957351685,0.6574227809906006,0.5088539123535156,0.5696598291397095,0.5450423955917358,0.599850058555603],[0.5835689306259155,0.6351559162139893,0.6286522746086121,0.6490182280540466,0.6120049953460693,0.48076891899108887,0.5630618929862976,0.6207262277603149,1.0,0.6242052316665649,0.629494309425354,0.4153398871421814,0.7685140371322632,0.5145721435546875,0.5044131278991699,0.7193930745124817,0.5148758888244629,0.5941038131713867,0.5074751377105713,0.6022036075592041,0.6243938207626343,0.44298088550567627,0.550885021686554,0.5479637980461121,0.6897983551025391,0.4727364182472229,0.6675080060958862,0.5698344707489014,0.5072765350341797,0.5674471259117126,0.6193965673446655],[0.6081619262695312,0.46969735622406006,0.4848497211933136,0.47416967153549194,0.4920942187309265,0.4238911271095276,0.5494258403778076,0.5109829306602478,0.6242052316665649,1.0000001192092896,0.5130997896194458,0.3572065830230713,0.766572117805481,0.49061155319213867,0.4474652111530304,0.5491676330566406,0.4094724953174591,0.5506278872489929,0.436053067445755,0.4765227437019348,0.5281680822372437,0.350856751203537,0.5419231653213501,0.5265780687332153,0.5508896112442017,0.3718154728412628,0.5838056206703186,0.6172611713409424,0.4180119037628174,0.4946954846382141,0.7017117738723755],[0.602232038974762,0.740241527557373,0.603594183921814,0.7106642127037048,0.6203290224075317,0.5807185173034668,0.5661267638206482,0.6769375801086426,0.629494309425354,0.5130997896194458,1.0000001192092896,0.5335749983787537,0.5089690089225769,0.7381278872489929,0.49693843722343445,0.7454627752304077,0.5854189395904541,0.6876996755599976,0.6804860234260559,0.6364051699638367,0.6090989112854004,0.6681665182113647,0.5185902118682861,0.4906153678894043,0.4994952082633972,0.6554946899414062,0.6742937564849854,0.5271286964416504,0.5985499620437622,0.6693270206451416,0.6103814840316772],[0.6399763822555542,0.5257775783538818,0.6528754234313965,0.5412070155143738,0.46451446413993835,0.7052825689315796,0.5628679990768433,0.4757775664329529,0.4153398871421814,0.3572065830230713,0.5335749983787537,0.9999998211860657,0.30361250042915344,0.5256859660148621,0.7120811939239502,0.6961451768875122,0.41869986057281494,0.49388423562049866,0.5741108059883118,0.5435287952423096,0.439115047454834,0.5077710151672363,0.5275151133537292,0.3611106872558594,0.3130478262901306,0.5714082717895508,0.3315919041633606,0.4758356809616089,0.7464800477027893,0.4672931730747223,0.4478290379047394],[0.5566428899765015,0.555476725101471,0.5154016017913818,0.5114123821258545,0.5012487173080444,0.3671690821647644,0.6226789951324463,0.5564506649971008,0.7685140371322632,0.766572117805481,0.5089690089225769,0.30361250042915344,0.9999998211860657,0.42763686180114746,0.4136431813240051,0.6010140180587769,0.4734163284301758,0.518585741519928,0.3919535279273987,0.5111154317855835,0.5712038278579712,0.3463064432144165,0.5171175599098206,0.5855693817138672,0.6857167482376099,0.3602786064147949,0.6968638896942139,0.5613771677017212,0.41363203525543213,0.4587560296058655,0.6215685606002808],[0.5661023259162903,0.6692092418670654,0.5294806361198425,0.5741095542907715,0.545889139175415,0.6059610843658447,0.5389155745506287,0.5249332785606384,0.5145721435546875,0.49061155319213867,0.7381278872489929,0.5256859660148621,0.42763686180114746,1.0,0.43931806087493896,0.6228154897689819,0.5724154114723206,0.7512193918228149,0.6251891851425171,0.5234322547912598,0.5067828297615051,0.6063158512115479,0.47119176387786865,0.5134212374687195,0.41974538564682007,0.4622683823108673,0.635927140712738,0.5142913460731506,0.6361854672431946,0.621840238571167,0.6159943342208862],[0.7419392466545105,0.4776172339916229,0.6363270282745361,0.6465232372283936,0.5527323484420776,0.5797759890556335,0.5240493416786194,0.4968632459640503,0.5044131278991699,0.4474652111530304,0.49693843722343445,0.7120811939239502,0.4136431813240051,0.43931806087493896,1.0000001192092896,0.6665953397750854,0.39420565962791443,0.458579421043396,0.5671048760414124,0.5576477646827698,0.5155544281005859,0.44026681780815125,0.508579671382904,0.44819629192352295,0.40986186265945435,0.46329039335250854,0.41241079568862915,0.500927209854126,0.6961692571640015,0.571671724319458,0.5324691534042358],[0.7604697942733765,0.7958236336708069,0.7501703500747681,0.6850697994232178,0.6828781366348267,0.6749100685119629,0.6893130540847778,0.7166787385940552,0.7193930745124817,0.5491676330566406,0.7454627752304077,0.6961451768875122,0.6010140180587769,0.6228154897689819,0.6665953397750854,1.0000003576278687,0.6447927951812744,0.6755136251449585,0.6400198936462402,0.7966645956039429,0.5940591096878052,0.6082313060760498,0.6530374884605408,0.6030725240707397,0.5241671204566956,0.680738091468811,0.6338258981704712,0.5613409280776978,0.781314492225647,0.6109579801559448,0.6080656051635742],[0.5294350981712341,0.6992630958557129,0.5314404964447021,0.5414726734161377,0.5289710760116577,0.46966707706451416,0.47621649503707886,0.5701935291290283,0.5148758888244629,0.4094724953174591,0.5854189395904541,0.41869986057281494,0.4734163284301758,0.5724154114723206,0.39420565962791443,0.6447927951812744,1.0000001192092896,0.5904215574264526,0.5013344883918762,0.5943117141723633,0.4428443908691406,0.5227235555648804,0.4497448205947876,0.4957045316696167,0.38734114170074463,0.48560452461242676,0.6116225719451904,0.3874168395996094,0.5139264464378357,0.4463934898376465,0.5263239145278931],[0.578955888748169,0.6768325567245483,0.5923506021499634,0.6179623007774353,0.5938454866409302,0.5299734473228455,0.5632842779159546,0.6134517788887024,0.5941038131713867,0.5506278872489929,0.6876996755599976,0.49388423562049866,0.518585741519928,0.7512193918228149,0.458579421043396,0.6755136251449585,0.5904215574264526,0.9999998211860657,0.5765666961669922,0.6021283268928528,0.5195772647857666,0.49452805519104004,0.522672176361084,0.5747285485267639,0.48556745052337646,0.42411065101623535,0.6752354502677917,0.5322248935699463,0.542532205581665,0.6082593202590942,0.657088041305542],[0.5983462333679199,0.5685755014419556,0.5679072141647339,0.733052134513855,0.5775022506713867,0.5793873071670532,0.5335252285003662,0.5487676858901978,0.5074751377105713,0.436053067445755,0.6804860234260559,0.5741108059883118,0.3919535279273987,0.6251891851425171,0.5671048760414124,0.6400198936462402,0.5013344883918762,0.5765666961669922,0.9999997615814209,0.5670034885406494,0.5611173510551453,0.5375722646713257,0.4491572976112366,0.4543144106864929,0.39075765013694763,0.5240923762321472,0.47645103931427,0.4856611490249634,0.6290746331214905,0.7173693776130676,0.6149008274078369],[0.676190197467804,0.6803222894668579,0.7711349725723267,0.622813880443573,0.5772743225097656,0.5775244832038879,0.5954482555389404,0.5846644639968872,0.6022036075592041,0.4765227437019348,0.6364051699638367,0.5435287952423096,0.5111154317855835,0.5234322547912598,0.5576477646827698,0.7966645956039429,0.5943117141723633,0.6021283268928528,0.5670034885406494,1.000000238418579,0.49586382508277893,0.5307750701904297,0.6059894561767578,0.5719282627105713,0.4879312515258789,0.5318922996520996,0.5645275115966797,0.4646400809288025,0.6372859477996826,0.5401479005813599,0.5500771999359131],[0.566741943359375,0.5474711656570435,0.509975790977478,0.7290420532226562,0.5976182222366333,0.4774233102798462,0.4936511814594269,0.5404407978057861,0.6243938207626343,0.5281680822372437,0.6090989112854004,0.439115047454834,0.5712038278579712,0.5067828297615051,0.5155544281005859,0.5940591096878052,0.4428443908691406,0.5195772647857666,0.5611173510551453,0.49586382508277893,0.9999997615814209,0.4489474296569824,0.42460232973098755,0.44441869854927063,0.5264461636543274,0.41131699085235596,0.5519794225692749,0.686775803565979,0.5122708082199097,0.7420483827590942,0.5837452411651611],[0.5406547784805298,0.6983601450920105,0.5230879783630371,0.48203882575035095,0.5755689144134521,0.577131450176239,0.4718976616859436,0.5274950265884399,0.44298088550567627,0.350856751203537,0.6681665182113647,0.5077710151672363,0.3463064432144165,0.6063158512115479,0.44026681780815125,0.6082313060760498,0.5227235555648804,0.49452805519104004,0.5375722646713257,0.5307750701904297,0.4489474296569824,1.0000001192092896,0.48480868339538574,0.3350617289543152,0.32209718227386475,0.5753198862075806,0.44958895444869995,0.42177698016166687,0.5588070750236511,0.425302118062973,0.43434447050094604],[0.5178927183151245,0.5878087282180786,0.5714255571365356,0.42672064900398254,0.5432724952697754,0.6927689909934998,0.5971082448959351,0.48270487785339355,0.550885021686554,0.5419231653213501,0.5185902118682861,0.5275151133537292,0.5171175599098206,0.47119176387786865,0.508579671382904,0.6530374884605408,0.4497448205947876,0.522672176361084,0.4491572976112366,0.6059894561767578,0.42460232973098755,0.48480868339538574,1.0,0.3300093710422516,0.3960614800453186,0.49347174167633057,0.5413612127304077,0.6269440650939941,0.5075391530990601,0.41572874784469604,0.5303767323493958],[0.639463484287262,0.5482839941978455,0.6457740664482117,0.5335017442703247,0.45656752586364746,0.31182241439819336,0.4498765468597412,0.5114701986312866,0.5479637980461121,0.5265780687332153,0.4906153678894043,0.3611106872558594,0.5855693817138672,0.5134212374687195,0.44819629192352295,0.6030725240707397,0.4957045316696167,0.5747285485267639,0.4543144106864929,0.5719282627105713,0.44441869854927063,0.3350617289543152,0.3300093710422516,1.000000238418579,0.5989720225334167,0.345331609249115,0.5203124284744263,0.3684764504432678,0.4894627630710602,0.45199209451675415,0.5796663761138916],[0.46830400824546814,0.46189194917678833,0.5548228025436401,0.5018593072891235,0.4329531490802765,0.3209766447544098,0.5101771354675293,0.4744163751602173,0.6897983551025391,0.5508896112442017,0.4994952082633972,0.3130478262901306,0.6857167482376099,0.41974538564682007,0.40986186265945435,0.5241671204566956,0.38734114170074463,0.48556745052337646,0.39075765013694763,0.4879312515258789,0.5264461636543274,0.32209718227386475,0.3960614800453186,0.5989720225334167,0.9999999403953552,0.3446958065032959,0.5350895524024963,0.4452773928642273,0.36996379494667053,0.4515955150127411,0.5193002223968506],[0.5380945205688477,0.6431441903114319,0.5721415281295776,0.4881211519241333,0.5219279527664185,0.5922660827636719,0.5211006999015808,0.5758219957351685,0.4727364182472229,0.3718154728412628,0.6554946899414062,0.5714082717895508,0.3602786064147949,0.4622683823108673,0.46329039335250854,0.680738091468811,0.48560452461242676,0.42411065101623535,0.5240923762321472,0.5318922996520996,0.41131699085235596,0.5753198862075806,0.49347174167633057,0.345331609249115,0.3446958065032959,0.9999999403953552,0.4195231795310974,0.4635744094848633,0.646394670009613,0.43385475873947144,0.4383435845375061],[0.5409373044967651,0.665803849697113,0.5157702565193176,0.6015769243240356,0.5838639736175537,0.4374317526817322,0.5699928402900696,0.6574227809906006,0.6675080060958862,0.5838056206703186,0.6742937564849854,0.3315919041633606,0.6968638896942139,0.635927140712738,0.41241079568862915,0.6338258981704712,0.6116225719451904,0.6752354502677917,0.47645103931427,0.5645275115966797,0.5519794225692749,0.44958895444869995,0.5413612127304077,0.5203124284744263,0.5350895524024963,0.4195231795310974,1.0,0.55467689037323,0.4519680142402649,0.5836657285690308,0.6329214572906494],[0.5342839360237122,0.4806312918663025,0.509526252746582,0.4620828628540039,0.6044294834136963,0.6502724885940552,0.48563605546951294,0.5088539123535156,0.5698344707489014,0.6172611713409424,0.5271286964416504,0.4758356809616089,0.5613771677017212,0.5142913460731506,0.500927209854126,0.5613409280776978,0.3874168395996094,0.5322248935699463,0.4856611490249634,0.4646400809288025,0.686775803565979,0.42177698016166687,0.6269440650939941,0.3684764504432678,0.4452773928642273,0.4635744094848633,0.55467689037323,1.0,0.5166037678718567,0.64140784740448,0.6077125072479248],[0.7760194540023804,0.66213059425354,0.6731486320495605,0.5933831930160522,0.5915855765342712,0.7097382545471191,0.5883063077926636,0.5696598291397095,0.5072765350341797,0.4180119037628174,0.5985499620437622,0.7464800477027893,0.41363203525543213,0.6361854672431946,0.6961692571640015,0.781314492225647,0.5139264464378357,0.542532205581665,0.6290746331214905,0.6372859477996826,0.5122708082199097,0.5588070750236511,0.5075391530990601,0.4894627630710602,0.36996379494667053,0.646394670009613,0.4519680142402649,0.5166037678718567,1.000000238418579,0.5720257759094238,0.5392056703567505],[0.570012092590332,0.5018028020858765,0.5205578804016113,0.7683202028274536,0.553398847579956,0.5522146821022034,0.46323922276496887,0.5450423955917358,0.5674471259117126,0.4946954846382141,0.6693270206451416,0.4672931730747223,0.4587560296058655,0.621840238571167,0.571671724319458,0.6109579801559448,0.4463934898376465,0.6082593202590942,0.7173693776130676,0.5401479005813599,0.7420483827590942,0.425302118062973,0.41572874784469604,0.45199209451675415,0.4515955150127411,0.43385475873947144,0.5836657285690308,0.64140784740448,0.5720257759094238,0.9999998807907104,0.6254837512969971],[0.6918050050735474,0.5590258836746216,0.5553864240646362,0.6061824560165405,0.6172075271606445,0.5258595943450928,0.5870155692100525,0.599850058555603,0.6193965673446655,0.7017117738723755,0.6103814840316772,0.4478290379047394,0.6215685606002808,0.6159943342208862,0.5324691534042358,0.6080656051635742,0.5263239145278931,0.657088041305542,0.6149008274078369,0.5500771999359131,0.5837452411651611,0.43434447050094604,0.5303767323493958,0.5796663761138916,0.5193002223968506,0.4383435845375061,0.6329214572906494,0.6077125072479248,0.5392056703567505,0.6254837512969971,0.9999999403953552]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003eSimilarity Score: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\"},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Similarity Score\"}},\"colorscale\":[[0.0,\"rgb(247,252,240)\"],[0.125,\"rgb(224,243,219)\"],[0.25,\"rgb(204,235,197)\"],[0.375,\"rgb(168,221,181)\"],[0.5,\"rgb(123,204,196)\"],[0.625,\"rgb(78,179,211)\"],[0.75,\"rgb(43,140,190)\"],[0.875,\"rgb(8,104,172)\"],[1.0,\"rgb(8,64,129)\"]]},\"margin\":{\"t\":60},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eSimilarity Matrix\\u003c\\u002fb\\u003e\",\"y\":0.95,\"x\":0.55,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"width\":800,\"height\":800,\"showlegend\":true,\"legend\":{\"title\":{\"text\":\"Trend\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ebd42df5-b064-43ec-aced-49692b7a3e7f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the visualization with the original embeddings\n",
        "topic_model.visualize_documents(df_filtered_doc['Text'], embeddings=embeddings)\n",
        "\n",
        "# Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
        "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
        "topic_model.visualize_documents(df_filtered_doc['Text'], reduced_embeddings=reduced_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        },
        "outputId": "78bfbf2b-f2d3-47d5-9501-db7c4c0520b7",
        "id": "ow2GnWeT9KdY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"4e2b4db4-b718-4003-9aff-8e520637e89d\" class=\"plotly-graph-div\" style=\"height:750px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4e2b4db4-b718-4003-9aff-8e520637e89d\")) {                    Plotly.newPlot(                        \"4e2b4db4-b718-4003-9aff-8e520637e89d\",                        [{\"hoverinfo\":\"text\",\"hovertext\":[\"Although improving the performance of our embedding methods is desirable, the most apparent limitation of our work is not the overall quality of representations produced, but rather the range of words our methodologies can be applied to.Currently, our methodology can only be used to construct word embeddings for dictionary headwords.This represents a considerable limitation, as wolastoqey and mi\\u2019kmaq are both polysyntheic languages, in which speakers often build new words by creatively combining roots.As this is the case, no dictionary is expected to contain definitions for all word-forms of these languages.\",\"Our analyses are based on models with t5-like architectures and span denoising training objectives.Thus, our findings may not generalize to other types of encoder-decoder models , nor encoder-only and decoder-only models.We believe this is unlikely, given that similar findings have been shown for models with architectures and objectives that differ significantly from t5\\u2019s.our analyses are also based entirely in english, and only leverage two syntactic transformations.It is possible that our findings will not generalize to other languages, given that certain grammatical features induce more syntax-sensitive behavior given a similar amount of training data across languages ; thus, perhaps less wikipedia or c4 data is needed in these languages for models to acquire hierarchical preferences.It is also possible that, within a language, a model could adopt a hierarchical inductive bias for one type of transformation, but not another\\u2014 especially if one transformation is much more frequent than the other.Indeed, the frequency of particular words positively correlates with syntactic evaluation accuracies , and it would be reasonable to expect a similar trend for the frequency of syntactic transformations.\",\"For now, the latent of our skill is sampled from a distribution, whose flexibility is not fully investigated.We intend to exploit more flexible skills and goal discovery, or direct generation via state or action.Additionally, the sparse reward in text-based games is also a burning challenge, which hinders the efficient exploration of agents.Our abstracted action, skill, to some extent eases off this issue, but is not enough.We will dive into this more and design a fancy solution later.\",\"In this study, we collected feedback on the usefulness of model responses from customer service agents at ar.These agents were recommended based on their availability and experience with conversation assist; however, we did not receive details about the agents such as their level of training or experience, which may have an impact on their preferences using the suggested responses.Furthermore, while agents in our study received a flat rate per judgment with no bonus or penalties to how they judged the response, some businesses have existing agent metrics that could incentivize the agents to behave differently while performing their jobs.These metrics have the potential to exert pressure on agents in real-life situations to accept responses at a higher rate than in this study.2 are based on the judgments of 5 agents on 3 lmm model outputs for 287 conversations.While they have shown a statistically significant relationship between usage rates and perplexity, this is a small pilot analysis.Additional data will be necessary to determine how well this generalizes.Our cost savings framework also makes a number of simplifying assumptions about workforce optimization.1, and they should be considered when leveraging this framework for different types of products.In addition, while the explicit goal of these models is to make agents\\u2019 jobs easier, we expect from previous work studying vigilance tasks that there can be an upper bound to how much cost could be saved with an excellent llm, as there would be less benefit from the agent acting as a human in the loop as their vigilance wanes.\",\"While hrdsattack is, to the best of our knowledge, the first dataset on extracting attacks on human rights defenders, there are some limitations.For one, while being the first corpus of its kind, our dataset is english-only.Second, the number of documents is limited.While the sample size of hrdsattack is on par with some of the other ee datasets, such as ace05 , we do see more samples being beneficial to subsequent model training and supporting other future studies.In addition, despite the effort to balance the class labels in the event attributes, some of the labels still remain imbalanced, such as perpetrator type.\",\"The current amr-tst is based on the style rewriting algorithm to rewrite the stylistic nodes of amr graphs from source style to target style.However, this method relies on style opposites features contained in the general natural language corpus.The advantage of such a method is that it does not need complex decoder retraining processes for different datasets, which maximizes the use of generic natural language knowledge and reduces training costs.However, this also leads to a limitation that the current amr-tst is applicable to text style transfer tasks with significant style polarity, such as sentiment features.For other text style transfer tasks like political and gender transfer, our current style rewriting algorithm cannot precisely rewrite the implicit style words in these tasks.ethical statement this paper honors the ethical code set out in the acl code of\",\"There are still some limitations of our work.one possible way is to generate candidate phrases of the document by utilizing the high-level semantic relatedness instead of using the surface-or syntactic-level information.\",\"Our work is limited in the following senses.First, all presented results relied on the ground truth number of intents to initialize the number of clusters for conducting k-means to retrieve prototypes and infer latent intents.In practice, however, the ground truth number of intents is unknown and needs to be estimated by examining a subset of utterances.2 investigated the impact of overestimating the number of ground truth intents by a factor of two, and found that idas\\u2019s performance did not degrade much.dbscan , mean shift , or affinity propagation.Second, we generated labels with the gpt3 text-davinci-003 model, which may be prohibitively expensive and slow to run for very large corpora.In our initial experiments, we tried using smaller-sized models such as text-curie-001, text-babbage-001, and text-ada-001, as well as flan-t5-xl , but found that the generated labels were of lower quality compared to those of text-davinci-003.\",\".This work utilizes extremely large language models and thus has a high cost on gpu resources.Concretely, experiments are conducted on the 8 x nvidia a100 gpu station.The maximum inference time on each version of cofe is \\u223c 8 hours.The maximum estimation of costed computing resources in this study is \\u223c 500 x 8 gpu hours.as in most previous work on compositional generalization , the cofe dataset is constructed using synthetic data rather than natural one.The source-side sentences in cofe are from cogs, which account for 70\\u201380% of naturally-occurring english sentences.Thus, this synthetic test suite could be close to the real-world application scenarios.due to the high cost on computing resources, we do not take multiple runs with different sets of examples, nor did we take multiple samples with temperature \\u003e 0.Observations under different prompt orders imply that with desired factors in selecting in-context examples, there could be low variance in experiments.\",\"Several limitations relate to the study setups.One shortcoming is the small number of comics used in the experiments.While this is adequate for initial exploration into animate entity identification, these results cannot be generalised - comics in particular have an incredible number of potential types of animate entities and beings, and four comic stories do not touch on most of them.Another limitation is the reliance on crowd-sourced recruitment and remote annotation.The researcher is not able to instruct the annotators in person and check their understanding of the annotation scheme.an annotator using the scheme tested here would not outline the sofa in figure 1, although the sofa should be included in an annotated corpus to capture an important update to the reader\\u2019s mental model.While these experiments show that these updates are somewhat obtained through interpreting reader feedback about their outlines, the limitations in developing an annotation scheme solely from the reader\\u2019s perspective are apparent.Developing a comparable annotation scheme from the creator\\u2019s perspective may facilitate fuller analyses of narrative structures.Since a creator knows that the sofa and kamala kahn are linked through coreference, both would be outlined and given the same reference label.Integrating these two perspectives into one corpus could give insights into how creator\\u2019s intentions to communicate larger narrative structures are expressed in lower-level configurations of image and text.\",\"One major limitation of the current methodology is its predication on the existence of a recorded lexicon in some form that can be ported into an online format.This approach may not be ideal as a framework for the development of a new dictionary, as its express goal is to increase accessibility to existing resources and facilitate the expansion of those resources.Additionally, the development of a bespoke, dependency-free web-application for showcasing existing resources is likely low on the list of viable strategies for a community-led reclamation effort, especially if there are no community members already familiar with web-development.Any replication of the work described here is more suited for a group with a set of existing resources, access to developers, and a need to quickly get those resources into the hands of community members.The other primary limitation of this tool and its development methodology in its current form is the potential lack of accessibility in the types of communities for which it is intended.Despite the increase in access to internet-capable smart devices in communities such as that on st.lawrence island, in many such places, the availability of reliable wireless internet access remains relatively low.Users are often forced to use expensive cellular data plans to conduct any amount of web-browsing.While the ultimate vision for this dictionary is for it to be packaged in a downloadable, offline format, replications of the tool in its current live-web implementation may leave many people unable to use it with the frequency that they would like.Though this is certainly a limiting factor in the effectiveness of this tool, the deployment of an offline version of the dictionary remains a preeminent goal of the project.Akuzipik dictionary website akuzipigestun-sangaawa akuzipik dictionary code akuzipigestun-sangaawa\",\"In style transfer, content preservation and style transfer are adversarial.Long texts have richer contents and more abstract stylistic features.We also notice that content preservation is the main disadvantage of storytrans in automatics evaluation results.Case studies also indicate that storytrans can maintain some entities and the relations between entities.However, strong discourse-level style transfer ability endangered content preservation.In contrast, baselines such as style transformer have better content preservation but hardly transfer the style.We believe that storytrans is still a good starting point for this important and challenging task.During preliminary experiments, we also manually inspected multiple author styles besides shakespeare, such as mark twain.However, we found that their styles are not as obvious as shakespeare, as shown in the following example.Therefore, we only selected authors with relatively distinct personal styles for our transfer experiments.for example, the style distinction between the following examples is not readily apparent.\\u2022 everyday story in our datatset: ashley wanted to be a unicorn for halloween.She looked all over for a unicorn costume.\\u2022 \\\"a double barrelled detective story\\\" by mark twain: you will go and find him.I have known his hiding-place for eleven years; it cost me five years and more of inquiry.\",\"While we unify diverse ie tasks into token-pair classification tasks and propose a simple but useful architecture to help token pairs interact with each other in an effective way, there are still several limitations that are worth discussing.Firstly, all modules in our utc-ie are based on pre-trained language models, and experiments proves that different plms may influence the performance on the same dataset.Hence, our model relies on the capability of the plm, which need a lot of gpu resources to complete the experiments.Additionally, although incorporating plusattention instead of self-attention can effectively reduce the memory and computational complexity from o to o, it still require a little large computation.\",\"There are two limitations of our work: 1) as the overall loss function comprises five components, we propose to directly add these components together.Although this simple summation already yields better results than the sota methods, we believe that it is better to tune the weights of these components based on expert knowledge, empirical experiments, or other machine-learning techniques.2) in zeroae, we use three plms, including two berts and a gpt-2.Moreover, contrastive learning typically requires a relatively large batch size in order to collect a sufficient number of negative samples and achieve satisfying performance.The batch size in our experiments is typically 32.As a result, training zeroae incurs relatively large resource cost.In practice, we find that using four nvidia tesla v100 gpus with 32g memory works well, and further reducing the resources hurts the performance.\",\"One of our limitations is that the data is split for short-term planning and long-term planning at fixed positions which on one side shows the overall planning capability on different datasets unbiasedly but on the other hand mixes the planning ability of the datasets with the overall performance of the embeddings.2 that this can lead in many cases to unplannable examples.Furthermore, we rely in short-term planning on the generated utterance distributions by transformers where we have to balance between semantic diversity and the likelihood of utterances.We control these with temperature and nucleus sampling and found the best \\u2021in tribute to our fellow researchers in the field of physics for their inspiring work on the curvature of spacetime trade-off with a temperature of 0.8 and a top p of 0.Nonetheless, this can still lead to utterances that might lead to the goal but that would be not considered by humans as very likely based on the given context as we explore in e.Furthermore, in the next utterance selection, we utilize the publicly available checkpoints which have been evaluated in the paper on dailydialog but both were seemingly not trained on an mdc-like task-oriented corpus.Since we find that the next utterance selection based on the curved property of the context in a task-oriented setting like mdc is almost always worse than just taking the last utterance, we have not expanded experiments in this domain.\",\"Although honestbait shows promising results for generating attractive but faithful headlines, there are still some limitations: honestbait is a monolingual model that only supports chinese.also, as the fr labels are specifically difficult to obtain, it is not easy to implement in other languages.Running the whole framework with a batch size of 16 takes around 22 gb gpu memory, mostly because we must load all pre-trained models into the gpu.This can be alleviated by using a distilled pre-trained model.On average, honestbait generates more faithful headlines than other baselines, but it still occasionally produces false information or unwanted results.This work is only for academic purposes and is not ready for production.\",\"Learning an ensemble of multiple source models is expensive, especially for large language models.Hence, to adapt to the new target domain, we cast the problem as an iterative-decision making process.While our work reduces the model access frequency to 1 or 2 at each training step, continually updating the language model from a stream of test data is still costly.besides, in some cases, the distribution of test data may change dynamically over the stream, but our work considers only the situation where the test data is from one specific distribution.\",\"We observe two main legal limitations for this project.First, it has limited practical use for non-lawyers seeking legal help, also called selfrepresented litigants.In fact, any system providing legal citations, both precedent or statutory provision, to an untrained lawyer will be of very little use, and even harmful.It is hard to imagine in what context this might be used by non-lawyers considering that they might not be able to translate facts into a legal problem.That being said, many direct-to-public legal applications have emerged recently, and many of these applications do provide insightful legal information along with the legal sources.While these applications have raised concerns as to their legality, notably with the issue of unauthorized practice of law, many regulators including in canada, the united states and europe have cautiously supported the development of ai-power technology for the general public.Second, several lawyers have surprisingly expressed concerns regarding \\u201cthe googlization of legal databases\\\".While they recognize the advantages of intuitive ai non-boolean research, they claim that these algorithms are not superior when it comes to locating a more obscure appellate case law, to help win a case.It has even been argued that boolean logic remains faster and more efficient because it does not lead to missed case.According to this view, while the \\u201cgooglized\\\" legal database may quickly locate important caselaw especially if decided by a higher court, it can miss less obvious cases.In our work, this challenge translates to the long-tail problem for legal citations, and our use of embedding distance encourages matching based on semantic similarity.In other words, we only look for the most obvious citations, which correlates to higher performance on easier citations and lower performance on harder citations.from a deep learning perspective, the main limitation is due to the use of k-means clustering in our implementation of the system.There were several points of instability noted during the training process, which we theorize is largely due to the initializations of the k-means clustering algo- rithm.When the prototypes are initialized, the corresponding terms in the loss function have a strong influence on the cross-entropy loss, which leads to model collapse.Even when the prototypes are initialized properly, the loss function overfits to the prototypes after several updates but does not provide improvement in the classification performance, which is why we choose the best model by validation macro f1 instead of validation loss.\",\"Although the experiment results have illustrated the effectiveness of the proposed imitation-demo method, we have to admit that our work has the following limitations: 1) this article is based on that the readers have some knowledge of prompt-based learning or demonstration learning.Due to the space limitation, we can only briefly describe the basic process of the demonstration learning, which may make the article a bit obscure and difficult to follow.2) imitation-demo does not achieve state-of-theart on all the datasets, but outperforms other strong baselines on 5 out of 14 datasets.Besides, it consistently surpasses the demonstration learning-based baseline lm-bff.Since imitation-demo is trained without introducing new parameters and explores the working principle of demonstration learning from a certain perspective, we believe the results are acceptable.\",\"Limitation by working in two directions: leveraging the intrinsic characteristic of frameset resources, including semantics-based clusters and cross-predicate role semantics, and tighter integration of other semantics-based tasks, such as word sense disambiguation, into srl.We hope our work will be a stepping stone for innovative research on high-performance srl systems for non-verbal predicate-argument structures, a problem that still needs extensive investigation.For this reason, we release our software and datasets at exploring-srl.\",\"This paper focuses on the style transfer of easy language for german.Due to their word inflections and high average word length, languages like german are harder to learn for language models.Therefore, the proposed approach may work even better on easier-to-model languages, but we did not test any other language.In addition, the style transfer of simplified language uses the same vocabulary as the original language and only reduces its diversity.Our approach has yet to be evaluated on other styles, for example, ones that introduce new words.When evaluating the influence of fine-tunung on the grammaticality of the model outputs, we found that even the original models were not perfect and produced grammatical errors.One possible reason is relying on gpt2-based models that are relatively small and, thus, perform worse than state-of-theart language models like palm.In addition, the german base models are often already fine-tuned versions of english models, and thus, may already suffer from catastrophic forgetting due to fine-tuning.\",\".In addition, a biased corpus may lead to an evaluation that is unaware of re language forms used in other cultures and languages, or that refer to other types of items.We expect this consideration to be important in practical settings.\",\".this process may consume additional time compared with other htc methods, but it can be done in advance and does not need to be repeated, making it suitable for both research and industrial settings.Besides, due to the errors of concept recognition algorithms, this process may introduce some noisy information in reality.This will interfere with the use of knowledge.another limitation is that we utilize the label name in the khla module.It may not be available for some datasets with only label ids.In response to this, we can select high-frequency keywords from documents in each category, which play the same role as the label name.\",\".While we try to perform multilingual lexical specialization on a set of typologically diverse languages, we are still restricting our analysis to a small fraction of all the languages of the world.In addition to this, our analysis investigates only two mmts \\u2013 albeit arguably the two most widely used.Due to hardware limitations, we experimented with xlm-r base: the results we report may be substantially different for xlm-r large , which possibly encodes more lexical knowledge.we leverage lexical constraints from babelnet, a resource constructed semi-automatically.Babelnet may contain lexical associations reflecting negative social biases.Biased constraints, when used as training data in our specialization, may strengthen societal biases present in mmts.Acknowledgments tommaso green and simone ponzetto have been supported by the join-t 2 project of the deutsche forschungsgemeinschaft.Goran glava\\u0161 has been supported by the euinaction grant funded by norface governance through deutsche forschungsgemeinschaft.We additionally acknowledge support by the state of badenw\\u00fcrttemberg through bwhpc and the german research foundation through grant inst 35\\u002f1597-1 fugg.We thank our colleague sotaro takeshita for insightful\",\"Limitation while our pipeline is designed to be applicable to any financial text dataset, the evaluation dataset is transformed solely on earnings conference calls.We will expand the scope of experiments to include other financial text sources such as news articles and social media posts.Finally, the current trading simulation does not take transaction costs into account.Going forward it will be necessary to consider more sophisticated trading policies.\",\"We consider the current work has the following two limitations: \\u2022 we design our lightweight ood detection framework based on the prefix-tuning paradigm.Nevertheless, there may be other techniques to achieve this goal, which requires further exploration.\\u2022 for pto + label, each label focuses on its own prefixes, suffering from prefix redundancy problem.One can design share prefixes across different labels to trigger label-invariant sentence features.\",\"This paper has three main limitations worth noting.First and foremost, while our paper aims to model the social context in which a message is said, the current context is limited to only the parties\\u2019 relationship.In practice, the social context encompasses a wide variety of other factors, such as the sociodemographics of the parties, the culture and setting of the conversation, and the history of the parties.Even relationships themselves are often much more nuanced and the appropriateness may vary widely based on setting, e., statements said between spouses may vary in appropriateness when made in public versus private settings.These contextual factors are likely necessary for a full account of the effect of social context on how messages should be perceived.Our work provides an initial step in this direction by making the relationship explicit, but more work remains to be done.second, our data includes annotations on a finite set of relationships, while many more unique relationships are possible in practice, e.our initial set was developed based on discussions among annotators and aimed at high but not complete coverage due to the increasing complexity of the annotation task as more relationships were added.\",\"Our method primarily focuses on operation-level specifications, while there are real-world use cases with other specifications.Moreover, our method of creating cqas can only be scaled to all python codes that involve heavy api usage.However, if a similar code knowledge graph generator of another language is developed, our method can also be scaled to the corresponding language.ethical concerns one concern about the data is the issue of copyright.have checked the data policy of all 20 kaggle competitions, in which none has copyright issues.Furthermore, they have contacted kaggle\\u2019s administrator and have made sure that the dataset collection procedure did not violate the platform\\u2019s policy.We also check the license of open-source apis when collecting documentation and make sure that there is no concern about copyright issues.Another concern about the data is that it might include privacy data.Again, we think that our data has a minimum risk of leakage of data with privacy concerns since we only collect data from the 20 kaggle competitions where there is no concern of privacy data.The api documentation also has the minimum risk of containing data with privacy concerns.\",\"Although the work is aimed at better understanding bert\\u2019s internal representations, there is no transparent way to know on the basis of what features of the training data some particular sentences are found to be similar.For task 2, the representations may have been affected in unexpected ways by the process of creating averaged sentence embeddings.There is no way to fully exclude the effect of lexical context and thus get a representation of the meaning of a construction without noise in unsupervised transformer models, which may affect the extent to which we can accurately probe for a construction.In task 1, the set of potential words that could be predicted is limited by the bert tokenizer\\u2019s vocabulary.Some limitations are caused by our choice to compare to stefanowitsch gries\\u2019s results.We used the relatively small corpus that they used and we have demonstrated the methods for a limited set of two english constructions.We also limited the analysis of the results to the top 20 most strongly associated collexemes, as they did.Using a larger corpus would probably yield more than 35 instances of the x-waiting-to-happen construction that found and that our reproduction yielded.We also did not experiment with ungrammatical or perturbated input as such results cannot be compared to the original corpus study, which only uses natural language data.The scope of our study was also limited by to the construction-specific data collection, preprocessing and manual annotation required.For modern web-scale corpora, task 2 would require significant gpu resources.As the way in which bert is trained clearly differs in many ways from how humans acquire language, also according to the construction grammar framework, this bert-based work does not warrant any claims about how human language works besides extremely broad ones and findings are limited to\",\"Our propose annotation strategy can be applied to labeling other mrc problems, no matter situated comprehension ones or not.However, when generalizing to other problems other than personality prediction we studied here, the accuracy of the user notes may vary with the difficulty of tasks.our unsupervised training technique does not support the longformer reader with character history yet.Therefore, the improvement from unsupervised training for our this model is smaller.While longformer is common in benchmarking for long story understanding tasks.There are other families of models handling long text encoding.potential risks like the other work that based on the similar set of books , the classic literature may be limited by the time of writing, thus raise fairness considerations.However, please note that our dataset construction strategy is not limited to these books, but can work with any books on weread to create a sampled book set without such biases.The main reason we stick with the current list of books is for reproducibility since they are publicly available.\",\"Limitations of our task definition depending on the application, attribution and preservation may not deserve equal weight.For instance, if there are multiple acceptable options for the output, such as in a dialog system, we might trade-off preservation for attribution, similar to how lamda behaves in our experiments.Our evaluation metrics also do not measure all aspects of attribution.For instance, some sentences are self-evident and do not require attribution but would be penalized in our evaluation.It is also necessary to note that linguistic assertions have varying scope: for example, there is a difference between \\u201cfrozen is a scary movie\\u201d and \\u201ci got scared watching frozen\\u201d \\u2014 while expressing a similar sentiment, the former makes a more general statement that many would disagree with, while the latter is scoped to the speaker\\u2019s own experience.In some applications, one could even argue that the latter case does not require attribution, since the speaker is their own source-of-truth.In addition to varying scope, utterances can also make assertions with varying levels of directness.For example, according to standard linguistics, \\u201cjohn ate some of the cookies\\u201d yields the implicature that john did not eat all of the cookies, even though it is not logically entailed.for preservation, we wish to explore other properties that should be preserved, such as discourse or logical coherence.Additionally, if the input text passage is completely misguided or flawed, it can be difficult to revise the text without significant changes, which would be heavily penalized by the current metrics.Limitations of our model while we aspire to improve attribution for arbitrary text, it is clear that rarr is not yet fully general.For example, the current implementation of rarr would not be well-prepared to edit poetry or long documents, primarily because we do not provide examples of such inputs in our few-shot llm prompts.However, we do believe that future developers may be able to quickly adapt rarr to such tasks by simply changing the prompts.Second, rarr tends to preserve rather than delete claims that it cannot attribute.Some of these claims genuinely do not require attribution, but others are hallucination and should be removed.Judging whether a claim requires attribution can be subjective and challenging.Finally, our model is computationally costly, since it is based on prompting a large language model.One potential solution is to leverage recent synthetic data generation recipes to train a smaller model.\",\"Domain shift: in the current implementation, our prompting model relies on the availability of a training set.This assumption may not hold in cases where the relations to be discovered exhibit a significant domain shift from the training set.limited number of relations: in this study, our analysis is restricted to a total of 25 relations.While this allows for a focused exploration of these specific relations, it also limits the scope and potential applications of our model.\",\"The main limitation of our study comes from the extra parameters caused by confidence calculation, in which two separate self-attention operations and biaffine transformation are performed.Incremental parameters results in a more time-consuming training process, and a higher hardware demand for storage.\",\"Due to time constraints, we were unable to implement some unreleased models as baselines for the proposed tasks.We did not conduct simile interpretation\\u002fgeneration on msd-ch in this paper since we could not automatically annotate the shared property in chinese data like the \\\"as.we are currently working on this annotation and plan to release the chinese simile interpretation\\u002fgeneration results on the data link.The coarse\\u002ffine version data we introduced in this paper can still be used for enlarging the msd data.We will study to utilize them for more simile data and richer language phenomena.\",\"We have limited ourselves to experimenting with only five languages due to lack of data for both the pretrained models and the deepspin tokenizer models.Although there are annotated data for some low-resource polysynthetic languages such as nahuatl, raramuri, wixarika, shipibo-konibo and kunwinjku , the available data was below 1m and therefore not enough to create pretrained models for our experiments.Regarding the aforementioned limitation, deepspin which has proven to be a good option to mitigate the problem of high ttr languages in closed vocabulary environments is a supervised method that requires the availability of training data.As can be seen in table 2, to achieve 90% to better accuracy deepspin requires around 350k annotated words.This can be a major drawback for low-resource languages, although the results with less annotated data are still competitive.We have not studied another source of differences in the vocabulary size that could be due to the texts used in pretraining.found that, in general, the oscar samples contain more vocabulary words than the wikipedia ones.Additionally, the quechua corpus we have used also consists of educational and legal texts that can increase the number of different types, compared to wikipedia texts.On the other hand, we believe it is important to mention that for the quechua language the training, evaluation, and testing data for ner and pos tasks were obtained from the same corpus used for training the language model.Note that, due to the scarcity of available digital and physical texts in that language, it is difficult to do it otherwise.The limited availability of texts leads to the use of the same corpus for multiple tasks, which could have implications on the evaluation of the obtained results.For instance, if the training corpus contains an unequal proportion of certain types of grammatical structures, it might negatively affect the performance of pos classifiers.Furthermore, if the corpus does not adequately reflect the linguistic variability and diversity of quechua, the resulting models are likely to be less accurate and less generalizable.\",\"The identified tendencies towards mentioning object-related features and the reliance on the shape as a contrastive feature might be driven by the grammatical structure of the annotations, mostly presenting object features in sentence-initial subject position, although 40% of exhaustive captions mention either the scale or the object color as the last word in the sentence.Therefore, these results call for investigating the biases of model architectures less sensitive to sentence length than lstms, as well as extending the annotations with additional grammars.Further, this evaluation provides descriptive results of the models\\u2019 pragmatic abilities, leaving the question of whether it is indeed a pragmatic inductive bias or, e., structural language drift causing the observed patterns, unanswered.Finally, since the evaluation pertains to the surface form of the predictions, applying decoding schemes other than greedy decoding used in this work might provide different patterns, indicating to which degree potential biases are due to model mechanics in opposition to sampling parameters.\",\"There are several limitations to the current study.Firstly, as for now, the corpus used in this study only consists of a single language pair.Secondly, the coherence of the mt systems was evaluated using a fine-tuned conference model, as no anno- tations were available for the mt outputs.8, the fine-tuned conference model is not perfect and may affect the quality of our coherence evaluation.Thirdly, this paper focuses on using discourse annotations to reveal and analyze discourse phenomena and the challenges they present to machine translation.\",\"To ensure high quality extractions for plate, we optimize our annotation process for precision.For example, for the product link attribute, we generally annotate only one product link per product.In an application scenario, the user would not need multiple links to a purchase page, but this could potentially harm the precision of the evaluated models.In addition, we assume that all attributes are text-based.This has the potential of missing additional product information which could be helpful to users, such as images of the product.\",\"For easier comparison with previous work, we only focus on text classification tasks, while st can also be applied to a variety of nlp tasks, such as language generation, conversational systems and commonsense reasoning.We also assume that the datasets are roughly balanced.However, real-world datasets are usually class-imbalanced , which might impact the performance of tapt and st.additionally, different labelled and unlabelled sizes may impact the performance of st approaches in the domain shift setting.\",\"Our framework is a two-phase process, which has its inherent defects, that is, the results of the second phase depend on the results of the phase 1.Because the sequence annotation algorithm in the first phase cannot achieve 100% accuracy, it will predict the wrong position that should be rewritten when the second phase is followed, which will further lead to the error of the final result.On the other hand, t5 model is only used to predict the words that should be filled in blank, rather than generate the whole sentence, which may lead to the decline of the overall fluency of the sentence.\",\"We list down some potential limitations of aclm: 1) plms are restricted by their knowledge to generate entirely new complex entities due to their syntactically ambiguous nature.Adding to this, substituting complex nes in existing sentences leads to context-entity mismatch.2) we do not conduct experiments in the language farsi from the multiconer dataset as neither mbart-50-large nor xlm-roberta-large was pre-trained on this language.\",\"Lmrec is trained on user behavior text data that are collected from diverse service applications.however, in order to improve the quality of user representations, choosing the item information differently for each application may improve the effectiveness.As such, we can consider domain-specific information for each service rather than using general item information.For example, we may leverage additional domain-specific information such as news topics or categories, names of the press agency, and keywords for the news content rather than using only news titles for the news dataset.This issue is a promising extension for practitioners to successfully apply lmrec to real-world applications.The types of task-agnostic data will largely affect the performance gains of lmrec+agnostic and lmrectl+agnostic.We fully utilize four types of taskagnostic data, i., sns, and news, and achieve state-of-the-art results.However, this paper does not thoroughly explore their optimized combination or mixing ratio of the corpus due to the heavy computational costs, which most large lm studies suffer from.While prior work shows how the pretraining corpus sources and their combination affect diverse downstream tasks , there still remain limitations in finding the generic relation between downstream performance and corpus properties; measuring the effect of the pretraining corpus on the downstream task is still underexplored.We point out that more careful study is left for future research.Regarding reproducibility, it is difficult to open our in-house data due to legal issues caused by privacy and user agreement.Therefore, we tried our best to validate the efficacy of our lmrec with the experiments on benchmark datasets in addition to in-house data.\",\"We use roberta-base models trained on a single 12gb memory gpu for our experiments.Obtaining annotations for cognitive dissonance are limited by the availability of annotators and is not easily scalable in crowdsourcing platforms due to the required training and expertise in identifying dissonance.Due to this limitation, only two iterations of the al loop for each setting were feasible for experiments.The transfer learning experiments in this paper were limited to two similar tasks, but there might be other tasks that could further improve or exceed the zero-shot performance of the models to cold start the active learning.We focus on fine-tuning and active learning selection strategies to improve performance of rareclass classification for a specific task: dissonance detection across discourse units.Therefore, fur- ther work would be necessary to determine if the findings extend to other tasks.Additionally, the results may be different for other languages or time intervals of data collection.The performance of the neural parser on splitting tweets into discourse units can produce parses that are imperfect but the annotators and our systems worked off its output regardless to keep the process consistent.An improved discourse parser may also lead to improved annotator agreement and\\u002for classifier accuracy.The dataset that we release from this paper, which contains labels of expressions of some cognitive states, was constructed using criteria that may not be fully objective.\",\"While our work displays many strengths, we highlight some limitations.First, we focus on python for programming language evaluation, which is one of the most widely used programming languages.However, we believe that our proposed approach, contraclm, would benefit code lms trained on any programming language.Second, the empirical findings presented in this work are mainly based on the smaller versions of gpt-2 and codegen with 124m and 350m parameters, respectively.However, as shown in figure 1b, by continuing to train the pretrained models with our proposed objective, contraclm is able to address not only the isotropy and poor discrimination issue that both gpt2-small and codegen suffer from, but also improve the representation quality of gpt2-large which has a good starting point for both isotropy and discrimination.Therefore, we believe the effectiveness of contraclm should be applicable to larger versions of these lms, regardless of whether they suffer from the anisotropy issue or not.\",\"The key limitation of our work is that, when conducting the review of approximately 60 papers , we encountered a skewed distribution of descriptive versus integrative works.we also recognize that, due to the mixed nature of our field, scientific and integrative findings are not the only goal\\u2014our \\u2018big tent\\u2019 includes engineers as well, who value gains in performance indicators.Finally, the fact that we have few examples of papers showing a return to theory renders the possibility that our central claim is misinterpreted in a normative way as a mandate.\",\".\\u2022 as demonstrated by our ablation study, the high performance of our model greatly relies on the manual prompts.This limits the application of our model to the scenes where high-quality prompts are unavailable and difficult to construct.To address this, we should look into the area of automatic prompt construction.\\u2022 our work ignores the phenomenon of entity coreference commonly existing in narrative documents.This limits the model\\u2019s ability to figure out the underlying relation between entities, which is crucial for the task of eae.\",\"Apart from the issues mentioned in \\u00a7 5.4, another limitation of trenc is that it does not integrate any pre-training process such as bert, which is effective in increasing the language understanding ability and adopted by previous works focusing on token-level classification tasks.first, we use dom nodes instead of tokens as the classification object and focus on relations between nodes rather than tokens.As the node text sequence is a composition of an arbitrary number of tokens, adopting the conventional masked language modeling training objective seems impractical since there is no direct mapping from an embedding vector, one-hot encoded or not, to a sentence.The second reason is simply that we do not possess the corpus or computation resources for model pre-training.In fact, we expect a properly designed pre-training scheme to bring better node semantics representation and si-pt relation modeling.It is an interesting topic and deserves further study.\",\"Our system distills plms into a less expressive but trustworthy set of templates.In developing this method, we explicitly trade off linguistic diversity for faithfulness guarantees.While this approach works well on academic benchmarks, in more complicated real world settings sacrificing linguistic diversity may impact different groups to a different extent.\",\".First, the fa model has advantages in computation but relies on an effective frequency selection strategy, which is difficult to design.We just simply select some manual frequencies for different datasets by experience.The more effective frequency selection strategy needs further exploration.Second, there is no theoretical guarantee that the orthogonal regularization can generalize to a 3-order tensor.Our or terms are only formally consistent with matrix orthogonal regularization, which has been empirically shown effective.\",\".In the next step, we can adopt an opensource framework of llms and fine-tune a domainspecific model using the extensive educational materials provided by middle school teachers.This way, the question generation ability may be improved, and we will not need to rely on the openai api.On the other hand, although extensive evaluations have been conducted, they only involve a small fraction of teachers and students in a preinterview setting.Once our system is widely deployed, a larger amount of user feedback will be collected and analyzed to monitor its effectiveness.\",\"Limitations the model we proposed is specifically for classification, while it is possible to be extended to other nlp tasks by changing the highlevel task-specific layer.Besides, in the evaluation, we focused on english corpora.Potential risks we make our code publicly available so that everyone can access our code.As the model is a classification model, it does not generate risky content.Users should also notice that the classification predictions may not be perfectly correct.\",\"First, unlike decoders in neural seq2seq models, which can attend to any previously generated tokens, qcfgs have a strong context-free independence assumption during generation.With this assumption, neural qcfg cannot model some complex distributions.A potential solution is to use stronger grammars, such as rnng and transformer grammars.Second, we assume that both the grammars used by the source-side parser and qcfg are in cnf.Although it is convenient for discussion and implementation, cnf does not suit for modeling the structure of practical sequences.In semantic representations ), a predicate could have more than two arguments.Ideally, we should represent n-ary predicates with n-ary rules.However, for grammars in cnf, n\\u2212 1 unnatural binary rules are required to represent n-ary predicates.In natural language, we will face semantically meaningless spans due to cnf, which is discussed in sec 4.third, although using decomposition improves the speed and the memory requirement, our lowrank models still cost much more computation resources than neural seq2seq models for two main reasons.A large amount of nonterminal symbols increase the memory cost significantly.For real data, we may need to sample hundreds or thousands of sequences and then rank them, which can be much slower than the decoding of neural seq2seq models.\",\"Our work focuses on assessing recent english vlms on tasks which require fine-grained understanding.first, we only examined a limited number of models.These include strong coarse-grained models, such as albef, clip, flamingo and blip-2, and two strong fine-grained models, pevl and x-vlm, that build on albef.While we believe our selection of models is representative of strong components in pretrained vlms , we could not easily evaluate different approaches towards fine-grained understanding as the corresponding models and code are not open-source.second, we evaluate our models in a zero\\u2013shot setting using image\\u2013text matching.as opposed to relying on image\\u2013text matching scores, alternative methods like input ablations, visualising attention or activations could also be used to gain an understanding of potential failure modes.Third, though we note specific areas where model performance fluctuates a lot during pretraining, we look forward to future research that improves performance for various such areas, like existence and counting.Finally, some datasets we use are quite small.For example, winoground only has 1,600 data points.We hope that our analysis sheds light on the kinds of skills models struggle with and encourages more and larger datasets that test for these skills.\",\"The way we use the intermediate sequences is to concatenate new sequences and the target sequence as the new target.As a result, the length of the target increases linearly with the number of intermediate sequences introduced, which increases the cost of inference.In the meantime, minimum bayes risk decoding needs to do prediction multiple times under different control tasks, which further increases the computational cost.However, there are potential solutions to compromise between the computational cost and quality, e.learning a student model by distilling the domainrobust knowledge from progressive translation.\",\"Our proposed dimongen task involves generating several diverse sentences to describe the relationships between concepts.However, it does not take into account the number of relationships between different concept pairs.This can lead to problems when applying the model trained on the dimongen dataset to other unseen concept pairs.For example, some concepts may have a small number of relationships, and asking the model to generate a greater number of diverse relationships may lead to hallucinations which can be misleading when using the generative model for educational purposes.additionally, the performance of the moree model is heavily dependent on the quality of the external corpora used in the retrieval stage.If the corpora do not contain any relevant information for the input concepts, the moree model will perform similarly to a vanilla moe model.An alternative approach is to retrieve information from the web.Last, it should be noted that the base models used in this study were relatively small.Recent studies have demonstrated that large language models possess superior reasoning abilities compared to their smaller counterparts.\",\"Model capabilities we observed that current lms struggle to generate several categories of examples.Lms struggled to generate examples related to concepts they do not understand well.As discussed in \\u00a76, we also found that lms struggled to generate examples with many constraints, in particular, those in the bbq dataset.We expect these limitations to wane as lms grow more capable with scale.Lastly, many evaluations related to lm capabilities require the dataset creator to know how to solve the evaluation.We expect that lms will not be able to generate high-quality evaluations of this kind.Our approach is thus differentially useful for evaluating other properties of models aside from capabilities.Model biases lms learn biases from their training data , impacting the generator pg and discriminator pd.For example, generated evaluations may exhibit gender or racial biases and be lower quality for languages under-represented in the lm training data.Lms will also be systematically worse at generating evaluations for tasks that are omitted from their training data.Example diversity we found limited example diversity for some kinds of evaluations though not all.Diversity appears to depends on the kind of evaluation generated, the generation hyperparameters, and the prompt used, and thus sometimes requires e.we found data visualizations to be powerful tools for understanding and debugging data diversity, such as those at evals.qualitatively, we also found that using pd to rank\\u002ffilter examples limited the diversity, since pd sometimes selected for prototypical examples for testing some behavior.Instructions may be misunderstood lms, similar to crowdworkers, may generate evaluations that are testing something different than intended, especially if the generation instructions are underspecified.For example, using the method in \\u00a73.1, we generated statements that a person who \\u201cshares beliefs with derek parfit\\u201d would agree or disagree with.The \\u201cdisagree\\u201d statements were often ones that many people, not just derek parfit, would disagree with.In this case, we should have provided more specific instructions to the lm, to have it generate examples that derek parfit would disagree with but that another philosopher would agree with.When feasible, we recommend briefly examining the generated data, to catch salient issues such as the above.Sensitivity to instructions our approach allows the dataset developer fairly fine-grained control over the evaluation by using instructions to guide pg.4 for a possible example of this effect we found.for now, it may be possible to use prompt sensitivity to generate more diverse datasets, by generating similar datasets with distinct prompts and combining the results, as we did in \\u00a73.Where prompt sensitivity caused issues, we found it helpful to be able to view example generated outputs in seconds, to quickly iterate and catch salient failures.For the 133 datasets in \\u00a73, we found a general instruction template that worked well; we did not do dataset-specific tuning to obtain samples rated as high-quality by human evaluators.Hybrid human-ai evaluation generation we are optimistic that hybrid human-ai dataset generation for mitigating many of the issues above, e.for example, it is possible to generate many examples with lms and edit or filter them manually.Hybrid approaches have succeeded in generating evaluation data , training data , and adversarial training data.Text generation evaluations we focus on generating classification-style evaluations, but many evaluations require text generation., who generate inputs and evaluate lm outputs using an lm-based classifier.Potential for misuse our results suggests that malicious actors may be able to use lms to evaluate lms\\u2019 tendencies to act in harmful ways, to exacerbate such tendencies.For example, a malicious actor may evaluate lms for their tendency to persuade people towards their own political views, in order to influence the public\\u2019s views towards their own.Another issue is that our method is potentially useful to adversaries in finding and exploiting weaknesses in existing models.for discussion on such risks, as well as mitigating factors and interventions.Despite the above concerns, we believe it is beneficial to publish our work on lm-written evaluations.Lm-written evaluations are also valuable to good actors and efforts to deploy lms, to catch and mitigate misuse harms as well as accidental harms.In this work alone, we believe that we surfaced several potential issues that, to our knowledge, have not been found before \\u2013 related to model outputs that express powerseeking tendencies, self-preservation instincts, various strong political views, and tendencies to answer in sycophantic or less accurate ways depending on the user.Overall, our results provide evidence that lms themselves are a valuable tool for helping us evaluate lms.\",\"We note that the datasets used in this work solely represent social media interactions from reddit, which is known to have a demographic bias toward young, white, american males3.Furthermore, systematic, spurious differences between diagnosed and control users can prevent trained models from generalizing to other data.Future research on other social media and datasets is needed to determine to what extent the presented findings are generalizable to broader populations.\",\"Our work has a few limitations that we care to highlight.First, it focuses on interpreting models through the vocabulary lens.While we have shown evidence for this, it does not preclude other factors from being involved.Second, we used e\\u2032 = et, but future research may find variants of e that improve performance.Additionally, most of the work focused on gpt-2.This is due to shortcomings in the current state of our framework, as well as for clear presentation.We believe nonlinearities in language modeling are resolvable, as is indicated in the experiment with bert.In terms of potential bias in the framework, some parameters might consider terms related to each due to stereotypes learned from the corpus.\",\"Our dataset construction method has certain limitations.One important limitation is that it is difficult to get the distribution of the required commonsense knowledge types.one potential risk of our work is that the text games may be limited by the time of writing, thus raise fairness considerations.However, our dataset construction strategy is not limited to these specific games, better sampling games can help to reduce such biases.\",\".First of all, the generation performance of our knowledge-augmentation framework largely depends on the efficacy of retrievers.In other words, if the retriever fails to retrieve the relevant facts to the input question, the prompted llm, conditioned on the irrelevant facts, is likely to generate the incorrect answer.Similarly, if the retriever is not designed to retrieve the facts in 2-hop neighborhoods of the question entities, llms are less likely to generate the answer requiring 2-hop knowledge.Note that, for the mintaka dataset , the number of answerable questions with 1-hop facts is only 40% of total samples.However, when we include 2-hop triples, the number of answerable questions becomes 62%, which suggests the necessity of 2-hop retrievals, which is yet challenging.on the other hand, the evaluation metric for the generation performance of prompted llms may be further improved.Specifically, regarding our target kgqa tasks, the answer for the question is the entity in kgs.However, the prompted llms without additional training tend to generate the answer as the sentence.For instance, the label entity for the question in table 4 is \\\"new orleans\\\", however, the llms often generate the sentence-level output: \\\"alex chilton died on march 17, 2010 in new orleans, louisiana due to a myocardial infarction\\\".We currently evaluate the model performance by measuring whether generated tokens contain the answer entity or not; however, it would be worthwhile to develop the additional metric to compare the sentence-level output from llms to the word-level answer in kgs in a more effective way.Note that we also try other available metrics , such as f1 and exact match scores , however, they largely penalize the longer sentences , thus may not be appropriate for evaluating lm prompting schemes.Lastly, since we focus on the improvement of knowledge injection in lm prompting, we use the labeled entities in kgqa datasets when evaluating models, following the existing kgqa evaluation setups.However, in real-world applications where the entities in the question are mostly not provided, we first need to extract entities in the question with existing entity linking techniques; therefore, our model performance depends on the efficacy of entity linking.In particular, regarding the result with entity linking in table 5, the portion of answerable questions from labeled entities in the dataset is 40%, however, the portion of them with entities from the entity linking model is 22%.\",\"The main limitation of our method is that it heavily depends on the quality of the label description.If a label description does not precisely describe the meaning of the label, our method cannot work.For some classification tasks such as microaggression detection, their labels have abstract meaning that is difficult to be understood by pre-trained language models.Similarly, our method cannot work on the domain that is not covered by the pre-training corpora of language models, such as the medical domain.Another limitation of our method is that plct loss cannot handle short texts.If a text consists of only one sentence, plct loss will no longer work because lct requires a document to be more than one sentence.In this case, pcl loss can still be used for self-training.\",\", resulting in a reduced impact on the environment.We finetuned nine models on program and function translation tasks and due to the smaller size of the training data, all jobs took a total of 1\\u20132 days on rtx 2080 ti gpus.A total of 100 hours of training in a single rtx 2080 ti gpu results in approximately 7.5kg of carbon emission into the environment.10 sensitive information avatar composed of parallel programs and functions that do not have any natural language comments or docstring.We remove them to get rid of any personally identifiable information or offensive content.However, there could still be such content in the form of string as we do not manually check each example.\",\"Assumptions: this work applies a lower-case transformation to the vendor names during the pre-processing step and assumes vendor accounts \\\"agentq\\\" and \\\"agentq\\\" to be from the same entity.However, in reality, these entities can refer to two different vendors.Additionally, we train our classifier in a multi-class classification setting, assuming that ads correspond to only one individual vendor account.However, our experiments uncover the existence of copycats on darknet markets.In reality, it is always possible for multiple vendors to co-exist with similar vendor names; hence, any supervised approach will only generate skew results.Architectural limitations: this research establishes a bert-base-cased classifier to verify migrating vendors across existing and emerging darknet markets.While we acknowledge that using a bigger bert model with a sliding window may improve our classification\\u2019s performance, given the resources at our disposal, we decided against it.Moreover, as mentioned earlier, most of the ads used in this research are in english, with a few exceptions where the vendors use multiple languages.Therefore, applying a multilingual transformerbased model to the classification task can improve our approach\\u2019s performance.Unsupervised and hr settings: as described in the assumptions, the core of our approach lies in the availability of gold labels.Vendorlink utilizes the supervised pre-training step to perform knowledge transfer and text-similarity tasks.Therefore, our approach suffers a significant limitation in the absence of these ground labels \\u002f unsupervised settings.Diverse advertisements: in the semi-supervised task, we compute the likelihood of two vendor accounts being from the same entity by calculating the similarity between the advertisements of the two vendors.Since one of the novelties of this research lies in the direction of end-to-end training, we have avoided using handcrafted labels for applying content control to generate content-independent style representation.2, an advertisement from the drug category can be very different from that of the weapon category.xai limitations: explainaible artificial intelligence is integral in promoting trust and understanding amongst the end-users.From lea\\u2019s perspective, its absence can be viewed as arguably negligent and unreliable.5 and establish a reliable approach for understanding and explaining our model\\u2019s decision.\",\"The main limitation of this work is the quantity of annotated human reflections.Overall, 15 human reflections are annotated, which are outnumbered more than 7:1 by gpt-2 reflections and 9:1 by gpt-3 reflections.If there were more human reflections annotated, we may be able to confirm, among other potential findings, that gpt-3 reflections were indeed significantly more often annotated as coherent compared to human reflections.We also note that the laypeople had a longer between-stage waiting period than the experts, because we could not enforce a similarly long waiting period for the experts due to practical reasons.While an ideal setup would keep the same waiting period duration, appendices c and d show that the duration difference is not critical.Furthermore, we adopted sequential annotation for reflections within a batch to make the interface easier to navigate for the human annotators, but this also means that the early samples in a batch might indirectly affect the annotation of the later samples.\",\".One limitation is that our approach is only tested on the task of amr parsing, and more evaluations are needed to see if it generalizes well to other tasks, such as relation extraction.Additionally, our approach, as is also the case for other current methods, exhibits performance degradation as the number of words in the sentence increases.This may be an indication of the current methods\\u2019 limitation or lack of robustness to longer sentences.Another limitation is the added complexity and extra parameters required by the use of transformer adapters, which increases the overall complexity of the architecture and training time.Even though our approach still achieves state-of-the-art results and it is as lightweight as previous systems at inference time, this fact should be considered by researchers if they should decide to adopt it for other tasks.In summary, our approach presents an innovative way to train the transformer architecture and achieve state-of-the-art results in amr parsing.However, more work is needed to further improve the performance of the model and to apply it to other tasks as well.\",\"Our f -distill variants are less efficient to train than seqkd and engine, as we require the teacher\\u2019s soft probabilities instead of hard, sampled sequences.However, our methods achieve a significant performance improvement, and more importantly, the additional training time does not affect inference when the model is deployed.This follows the spirit of knowledge distillation in general, i., to obtain a small and efficient model for deployment.Another potential threat to validity is that we have not reported multi-run statistics.In our preliminary experiments, we ran our approach multiple times and found results were generally consistent.Due to our excessive experimentation , it is not possible to run each model multiple times.We instead adopted a wide range of established automatic metrics, consistently showing the effectiveness of our approach.We further conducted in-depth analyses to better understand our proposed framework.We deem multi-run statistics not crucial to this paper, as this paper does not purely focus on empirical analysis.Rather, our main contributions lie in the novel machine learning framework, f -distill, and the theoretical connections between step-wise and sequence-level f - divergence functions.\",\"Whilst the system presented within this paper is capable of allowing human-in-the-loop contributions , it is not able to produce tongue-twisters that take advantage of particular features of speech sounds such as place and manner of articulation, in order to create more advanced outputs that exploit phonetic relatedness.The same can be said of our proposed metrics, po and init-po, which do not account for phonetic similarity across sounds that share manner\\u002fplace of articulation.Additionally, whilst commonly known tongue twisters may follow a particular format , such schemes and templates have not been enforced here.We also do not demonstrate the capabilities of these systems if they were trained on phonetic transcriptions explicitly, as we only aim to assess their performance when training on graphemes in standard orthography.\",\"Our investigation focuses on one aspect of commonsense reasoning restricted to one dataset.There may be numerous other factors in real-world applications.Therefore, our findings may not comprehensively capture the entirety of commonsense reasoning phenomena.Another limitation is that for the sake of simplicity and feasibility, we assumed a fixed threshold of k=200 to categorize frequent and less frequent names.However, this threshold may not be universally applicable to all contexts or datasets, and different thresholds could lead to different results.\",\"Our paper has the following limitations 1.Our class-based influence score cannot improve the performance of gc algorithm.Although class-based version of gd, if, and tracin outperformed the original gc, we aim to develop a stronger version of gc.4, we believe that a partially normalized gc could have better performance.In partial gc, we normalize the gradient of the clean data point z\\u2032 only.\",\"First of all, idol relies on a customized dataset that is filtered out from wikipedia pages with the help of many pre-defined logical indicators.Inevitably, this will introduce a certain amount of artificial bias.If an automatic method for logical indicator extraction based on something like hidden representations from neural network models is put forward, it would be beneficial to narrow the gap between the dataset preparation and logical pre-training.In addition, in the field of pre-training task design, there have been a lot of different but effective approaches proposed., the authors presented a pre-training task named pert which requires the models to recover the original token sequences under the background of that different token permutation within a certain range would not affect chinese text understanding.This method only depends on the original texts, but idol introduces one more special token, which widens the gap between pre-training and fine-tuning to some extent.\",\"We identify two limitations of the current work and make suggestions for future directions.First, while our proposed method is language-agnostic in principle, our evaluation is limited to our french benchmark dataset.Expanding our approach to encompass other languages would bring new and interesting challenges for further investigation.Second, despite topic diversity within our exercise documents , it would be interesting to quantify the degree of topical bias introduced during our training process and its impact on our binary task evaluation.there is also potential to explore different prompting strategies for large language models , when generating gap-filling grammar exercises.For instance, the utilization of chainof-thought prompting , which involves generating intermediate steps before producing the final response, could be explored for generating grammar exercises.Additionally, an interesting future study would involve investigating the number of example demonstrations that llms require in order to accurately mimic example gap exercises.\",\"The main limitation of our work is the focus on the specific domain and the dataset that is not yet publicly available.we believe that testing adapters with different settings in the emergency response domain is a valuable contribution but we are also aware of the fact that the dataset used in our experiments is not large or exhaustive enough to cover all the variety of topics relevant for the emergency response.For example, our data cover cases of explosions, leakages of hazardous materials and building collapse but do not include any dialogues for open field rescue operations or car accidents.Another issue that is worth mentioning is the fact that all recordings were collected during the training sessions and not the actual missions.Hence, the responders might be under less pressure than in a real life-threatening situation and their communication might be more of a textbook case.However, all simulations had a realistic setting that includes several operators, robots and points of interest and we believe that the recorded communication is representative for the domain in question.\",\"Personalized news headline generation has the potential to improve the way users consume and understand the news.However, it is important to be aware of its limitations.The performance of any natural language generation model, including those used for personalized news headlines, is dependent on the quality and consistency of the data used to train it.Similar to personalized recommendation systems, personalized headlines have the potential to create echo chambers.If the model is trained on a biased or unrepresentative dataset, it may generate outputs that are incomplete, inaccurate, or misleading.Therefore, it is crucial to be aware of the limitations of the model and to ensure that it is trained on high-quality data to generate accurate and personalized headlines.\",\"To recruit participants who are more likely to write posts in other languages, our recruitment was restricted to bi\\u002fmultilingual individuals residing in the us.Due to these criteria, most participants in both studies were spanish speakers and writers, which is the second most common language spoken in the us.Since spanish is a rich-resourced language, findings about the quality or accuracy of mt may be different if we consider a more linguistically diverse participant sample.our results suggest that these features would make users more comfortable with mt, but this suggestion would ideally be confirmed with a controlled experiment.Our future research will be focused on designing the proposed controls.In particular, we plan to conduct an experiment on prototypes of three different translation features: one allowing the user to read but not edit the mt, one allowing the user to add a translation manually, and one allowing the user to read and edit the mt if there are any errors.Each prototype will also have an option to include\\u002fnot include the translation with each post.This experiment will measure which features most improves users\\u2019 perceived control, perceived satisfaction, ease of use, and intention to use mt.\",\"We compare 12 representative methods, present a unified view on existing prototype-based methods, and propose a competitive unified baseline by combining the advantageous modules of these methods.We test all methods, including the unified baseline, on three commonly-used english datasets using various experimental settings and achieve consistent results.However we acknowledge the potential disproportionality of our experiments in terms of language, domain, schema type and data scarcity extent.we are fortunate to witness the rapid development of large language models in recent times.In our work, we set incontext learning as a baseline and evaluate the performance of llms on few-shot ed tasks.We find current llms still face challenges in dealing with information extraction tasks that require structured outputs.However, we acknowledge the icl approach adopted here is relatively simple.We do not work hard to find the optimal prompt format, demonstration selection strategy, etc., to reach the upper bounds of llms\\u2019 performance.In this work, we focus more on the model aspect of few-shot ed tasks rather than data aspect.In other words, we assume having and only having access to a small set of labeled instances.\",\"In this paper, we have unearthed a variety of problems present in current evaluation benchmarks that favor systems over humans, or that simply make such comparisons unfair.We conclude that there is no real evidence to claim that today\\u2019s language models possess superhuman performance.However, without empirical results obtained under the right setups, we cannot even claim the opposite, namely that humans are still better than systems.additionally, while a good portion of the nlp research effort is devoted to natural language generation tasks , here we provide only some pointers to nlg\\u002fmt.3, these problems exist in the nlg universe as well, but, due to space constraints, we limit our analysis to nlu tasks.\",\"., applying ibp technologies to large-scale pre-trained bert models is challenging because of the calculation of bound propagation on the attention layer is relatively loose.Since bert is currently one of the most popular architectures in nlp, there is a limitation that the proposed method combined with ibp training could not generalize to bert architectures.However, it is worth noting that the proposed method based on textcnn architectures achieves better certified robustness than the advanced baselines, safer and ciss based on bert.Besides, this paper focuses on enhancing the model\\u2019s robustness to word substitutions, but not investigates the robustness to character-level or sentence-level perturbations.\",\"The main target of this paper is to utilize structural knowledge for cross-lingual comprehension.We present a new pre-training task named scp in the hope of bridging the misalignment of structural words in the parallel corpus.More generally, we expect the proposed method can facilitate the research of cross-lingual understanding.Admittedly, the main limitation of this work is that we rely on off-the-shelf tools to extract and align words in different languages, which would result in some mistakes at some situations.For example, giza++ only achieves 80%-85% accuracy in aligning the corresponding words in another language.Currently, no tech can achieve this goal in 100% accuracy.As a result, some bias data in pre-training calls for further research and consideration when utilizing this work to build xplms.\",\"In this paper, we thoroughly investigate the existing large language models for nl2code, and summarize them from diverse perspectives with our own thinking.However, as this field is evolving so rapidly, there may be aspects that we have overlooked, or some new works that we have not covered.To mitigate this issue, we have created a website to track the latest progress through crowdsourcing, hoping that it will continually contribute to the development of the field.Besides, the existing llms possess their own characteristics in terms of model size, architecture, corpus, pre-processing, tokenizer, hyper-parameters, and training platforms.Also, some of them are currently not publicly available, such as alphacode and palm-coder.Therefore, it is almost impractical to conduct a completely fair comparison.We tried our best to show a kind of comparison on the popular humaneval and mbpp benchmarks, hoping that it can provide clues to the differences in performance of different llms.In addition, evaluating llms has a high cost in computational resources.We thus have made all files generated by the llms publicly available on\",\"While the proposed method has demonstrated superior performance and high efficiency, there are several limitations that warrant further investigation: in few-shot settings where the number of training examples is limited, the performance of our method and other baselines drops significantly.the storage consumption has been reduced to a small amount, however, the number of neurons is still relatively large compared to that of heads and therefore becomes a bottleneck for further decreasing storage requirements.2, one possible solution is reducing the number of layers used to generate the embedding.\",\"Regarding our work, a few limitations should be mentioned.During the annotation process, conflicts between annotations occurred.All conflicts were discussed with a senior researcher and resolved in this way.Thus, we reached the best possible agreements, but still some agreements are lower than others.For example, legal claims and premises have a relatively large room for interpretation.Perfect results can only be expected by over-anchoring the annotators and weakening the guideline, which we have consciously avoided in our research.The comparison of the iaa with other works in the field of nlp from legal science is not possible, because the works either do not examine the components of the appraisal style or identify the components of the judgment style without the indication of the iaas.all in all, we can assume that both our components and our mounted relationships, achieve comparable or better results than comparable works ).Although our model shows accurate values between 78% and 92% for predicting the components of the appraisal style, the values for determining legal claims and premises are lower compared to the other values.However, they display reasonable values when compared to previous nlp studies.We can only compare our work to other related work in another domain because values for detecting legal claims and premises are not available in the nlp literature.For instance wambsganss and niklaus , present an accuracy of 54.12% for their long short-term memory model which detects claims and premises.With the mentioned model the authors shows positive outcomes in supporting students\\u2019 argumentative skills.Or wambsganss and niklaus and our post-test results also show significant learning outcomes.Although we can show a significant learning output, it must be noted that this is only short term.as a third possible limitation, our models are limited to applying the appraisal style in german only.however, we assume that this is possible in principle, since some countries such as china now use the appraisal style in law teaching and countries such as the u.use at least similar approaches such as learning with case studies using the irac formula.Nevertheless, some adaptation of the models is needed, since the language and the legal form in each country have their own specificities.\",\"We empirically conclude two limitations for s2ynre in the hope of inspiring more future research.On one hand, its advantages are less significant when a large amount of annotated data is available.For example, tacred training set has 68,142 annotated instances.Under this setting, even if we add another 100,000 synthetic samples, the improvement is only +0.This means that the quality of synthetic data, although superior to distant ones, is still not as good as golden ones.Thus they can hardly provide identical utility the same as 100,000 golden data.Nevertheless, with the development of llms and their powerful generation ability, we look forward to accessing higher-quality synthetic data.On the other hand, when training data are limited to a few samples , even strong llms like gpt-2 can not perfectly fit the structure of relational statements within a few steps of finetuning.Therefore, many generated sentences may not contain correct subject or object entity markers as requested and have to be discarded.4 p609 acl 2023 responsible nlp checklist\",\"We have primarily focused our analysis on similarity or log-probability based metrics for nlg.developed a family of interpretable metrics for various nlg tasks with the concept of information alignment.recently proposed a metric based on stratified error synthesis.In addition, there are several task-specific metrics for paraphrase generation , image captioning , dialogue , controlled text generation , etc., which would be interesting to evaluate.5, we design a number of fluency and consistency tests.It would be interesting to expand this set to be broader or more sophisticated.Also, there are other important aspects of text generation to consider, such as factuality.All of our diagnostic data are synthetically created.While it provides valuable insights on the metric\\u2019s behavior, it does not have a good coverage of errors in real-world settings.Expanding our analysis to real-world errors in a scalable way would be an important future direction.Last but not least, we evaluate our proposed stress tests only on english texts.However, many language-specific properties can induce potential blind spots for metrics, especially for low-resource languages where plms may provide poor text representations.An important future direction is expanding the tests to multilingual settings.\",\"Our search was conducted exclusively in english, and we may have missed relevant papers written in other languages; this may have influenced the heavy english skew in our data.Some of the annotations of attributes and choices in this taxonomy rely on subjective judgements, particularly with regards to the clarity of conceptualisations of bias, desired outcomes, and justifications of proxy choices.As with any qualita- tive work, these results are influenced by our own perspectives and judgement.We did our best to address this through regular discussion, identifying disagreements early on when designing the taxonomy, and adopting a \\u201cgenerous\\u201d approach.\",\"Main limitation 1 the experiments of this paper are only done in 14 languages that use the latin alphabet, and with a high share of indo-european languages, with up to 4 germanic languages.This is due to two reasons: the scarcity of xpos and feats annotations in treebanks from other language families, and the research team involved in this work did not have access to proficient speakers of languages that use other alphabets.Hence, although we created a reasonable diverse sample of treebanks, this is not representative of all human languages.Main limitation 2 although we follow previous work to automatically generate perturbations at character-level, and these are inspired in psycholinguistic studies, they might not be coherent with the type of mistakes that a human will make.In this work, generating human errors is not feasible due to the amount of languages involved, and the economic costs of such manual labour.Still, we think the proposed perturbations serve the main purpose: to study how morphological tags can help parsers when these face lexical errors, while the used method builds on top of most of previous work on adversarial attacks at character-level.\",\".Although it is possible that some aspects of the work \\u2013 such as the edit categorization \\u2013 might transfer to the study of text simplification in other languages, we focus on the english language.As of the writing of this paper, there is no equivalent of simple english wikipedia for other languages on wikipedia, and creating similar resources for other languages would require finding other resources.by matching revisions of wikipedia pages that are factually aligned, and working with sew editors to annotate the edits, we attempted to match the process used to create the resource.It is however not possible to recruit all 5,000+ sew editors and for some page pairs the annotations are another editor\\u2019s best attempt to reconstruct the intended edits by the original editor.2 reveals that our annotators achieve moderate agreement on samples repeatedly annotated.More detailed analysis reveals that agreement is generally strong from common edit categories such as lexical edits, semantic deletions, or sentence splitting, but is lower for more infrequent categories.Better training of annotators on tail categories could therefore likely improve annotation.We also found that discussion amongst annotators of a sample often led to eventual consensus.Therefore collecting multiple annotations per sample, and allowing for discussion when multiple interpretations occur could help improve annotation quality, but at an increased cost.\",\"Our study revealed considerable differences in transferability and other measures we considered across different datasets.Nonetheless, the study focused on the differences in transferability arising from the choice of the models and the al methods rather than the dataset.To eliminate confounding due to datasets, we grouped the results by datasets and analyzed each group separately.Despite this, the scope of our results is limited by the fact that all datasets used are in english and possibly contain their own biases.Even though we showed that it could still be useful to transfer actively acquired datasets between transformer-based plms, it is important to keep in mind that actively acquired datasets are not representative of the original data distribution due to the sampling bias introduced by active learning.\",\"The sws benchmark have two limitations: the sentences in the sws testing set come from students\\u2019 essays, which limits the system\\u2019s ability to test its performance in other specific domains such as laws or medicine.The sws corpus is at the sentence level, but some writing suggestions can only be made after reading the entire article, which are not included in our sws dataset.\",\"Scope this short paper serves as an initial step toward peft for long-document models.As such, our evaluated scope of models, tasks, datasets, and kernel variations is limited.We acknowledge the need to experiment across broader settings and hope our work provides a foundation for others to build on.Future experiments should analyze the validity and efficacy of using prefix-propagation with other long-sequence models to determine whether the prefix modality is suitable for non-sparse attention approximations.Result in an excessive loss of information for these critical tokens? Regarding tasks and datasets, the performance degradation in prefix methods for wikihop deserves significant attention.Verifying whether this extends to other reading comprehension and question-answering tasks will assist in guiding future research efforts.We restricted our research to the encoder-only version of longformer, but using the encoder-decoder version, led would enable analysis of sequence-to-sequence tasks.The scrolls benchmark would be a good starting point for this analysis since it includes an led baseline.Consequently, we report results of one seed after doing a hyperparameter search for learning rate.This aligns with the reporting system of the original longformer paper but greater assurance in all long-sequence task performance could be achieved by accumulating results over several seeds.The size of datasets and iteration over several epochs somewhat mitigate this concern.\",\"Our work reported in this paper has two limitations.First, because of the scarcity of treebanks and ner datasets for turkish, our pretrained spacy language models were tested on a limited amount of testsets.Second, we trained our spacy models on generalpurpose datasets compiled from wikipedia data and formal written language resources.Accordingly, our models may not be very effective in analyzing social media texts such as twitter data.\",\"Our initial work on one-shot em shows promising results but comes with several limitations.By inspecting generated examples, we find evidence of information leakage in the lsr that causes generated sentences to be similar in topic and length to the input sentence.We also require one example at inference time in order to generate a new example for a specific word sense.We demonstrated empirically that the baseline does not have this restriction, because the baseline method generalizes well to words that were unseen during training.Additionally, our best approach, s2ssemi, still requires wsd training data for pretraining of the bem, which is used for lsr extraction.Finally, the s2s models in this paper have the limitation that the target word looks identical in both the input and output sentence, whereas the baseline is capable of generating examples with various target word forms for a given target lemma.\",\".There are no strong assumptions, claims or biases we are aware of that are not stated in the paper.We state only claims that are supported by evidence and experimental setup that we design and describe clearly in the main paper and in the supplemental material.did you discuss any potential risks of your work? Not applicable.To the best of our knowledge, there is no potential risk or harm of any kind that could result from this work.Our research is just an analysis paper about cross-lingual continuous learning where we share our experiments on pre-existing approaches.\",\"While we hope that our approach to data collection can serve as a benchmark for future nlp studies beyond peer review, we deem it equally important to explicitly outline the potential risks and limitations of nlpeer and nlp for peer review in general.from the data perspective, we deem it important to clearly state what nlpeer is not meant for.Our data collection campaigns for arr-22 and coling-20 included an explicit disclaimer on the risks of author profiling on the peer reviewing data; we stress that such applications violate the intended use of nlpeer.Furthermore, nlpeer enables a wide range of new nlp assistance tasks for peer review.Yet, we encourage future studies of nlp for peer review to reflect carefully about the potential risks and benefits of new task defi- 13 nitions atop of peer reviewing data in general and nlpeer specifically.For instance, the full automation of peer review, i.the generation of review reports given a paper, bears risks and dual uses.Considering diversity in nlp datasets, we stress that even nlpeer only covers a fraction of peer reviewing across all fields of science, and more data needs to be collected to enable fully representative nlp-based study of peer review.Due to the genre standards of scientific publishing our dataset only covers papers and reviews in english language.Multilingual scholarly document processing is overall poorly represented in nlp, and constitutes a promising avenue for future research.While our resource contains data from a wide range of domains, research in arts and humanities is under-represented due to the poor data availability.The trend towards open science and the adoption of responsible data collection practices might bring reviewing data from previously unexplored domains and languages into nlp.We stress that any direct comparison based on our corpus would need to take into account reviewing practices and guidelines adopted by the respective communities.Specifically, potential biases resulting from the donation-based collection for arr-22 and coling-20 should be taken into account.From the task side, we highlight that implementations and resulting models presented here are meant to exemplify the proposed tasks, determine their technical feasibility, and serve as a starting point for developing future nlp for peer review assistance systems.Since we did not perform extensive hyperparameter search and tuning of the models, our results should not be interpreted as a claim towards superiority of a particular model, approach or reviewing system.We highlight that high intrinsic task performance does not necessarily translate into the extrinsic utility of nlp support in real-world reviewing environments.We thus deem it crucial to study the factors that affect the success of nlp assistance for peer reviewing.This includes the study of the humanmachine interaction dynamics and its desiderata; for example, review score recommendations should be accompanied by explanations.We encourage extensive research on risks of biases and errors in nlp assistance models; for instance, a review score prediction model might learn undesirable biases against certain types of papers.Review writing assistance implemented in a real reviewing system should always be accompanied by carefully designed guidelines and policies.Finally, we invite the community to reflect on the potential societal consequences of the individual nlp assistance tasks, even if nlp models accomplish them well.To provide an example, our newly introduced guided skimming task assists during the effortful and time-intensive, yet crucial step of reading the paper under review.Although the guided skimming for peer review task models an intermediate step during reading and is intended to serve as an additional point of reference during the iterated skimming steps of peer review, such a technology might encourage reviewers to read only the paragraphs suggested by the model.We argue that this risk of \\\"lazy reading\\\" is independent of the technology at hand; a reviewer that is institutionally incentivized to perform reviews as quick as possible, may read a paper superficially and settle with heuristics for their assessment regardless of assistance.A greater risk, however, may be imposed by potential biases and errors of a guided skimming model, which could distract less experienced reviewers.While recent work on skimming assistance in scholarly articles suggests a mature and reflected interaction of users with highlight recommendations and possible errors, this needs a specific investigation for the use case during peer review.On the other hand, a critical reading model may serve as a useful point of reference to guide reviewers to employ more scrutiny on the parts of the paper appropriate for this specific paper type, which ultimately may improve reviewing quality.We assess that the opportunities provided by the introduced review assistance tasks outweigh the potential risks in general, yet highlight that a targeted study is necessary to substantiate this assessment.iz beltagy, kyle lo, and arman cohan.scibert: a pretrained language model for scientific text.In proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing , pages 3615\\u20133620.Annual review of information science and technology, 45:197\\u2013245.advances in neural information processing systems, 33:1877\\u20131901.Liying cheng, lidong bing, qian yu, wei lu, and luo si.ape: argument pair extraction from peer review and rebuttal via multi-task learning.In proceedings of the 2020 conference on empirical methods in natural language processing , pages 7000\\u20137011, online.alexandra chronopoulou, matthew peters, and jesse dodge.Efficient hierarchical domain adaptation for pretrained language models.In proceedings of the 2022 conference of the north american chapter of the association for computational linguistics: human language technologies, pages 1336\\u20131351, seattle, united states.nils dycke, ilia kuznetsov, and iryna gurevych.yes-yes-yes: proactive data collection for acl rolling review and beyond.In findings of the association for computational linguistics: empirical methods in natural language processing 2022, pages 300\\u2013318, abu dhabi, united arab emirates.nils dycke, edwin simpson, ilia kuznetsov, and iryna gurevych.Assisting decision making in scholarly peer review: a preference learning perspective.raymond fok, hita kambhamettu, luca soldaini, jonathan bragg, kyle lo, marti hearst, andrew head, and daniel s weld.scim: intelligent skimming support for scientific papers.In proceedings of the 28th international conference on intelligent user interfaces, iui \\u201923, page 476\\u2013490, new york, ny, usa.tirthankar ghosal, sandeep kumar, prabhat kumar bharti, and asif ekbal.Peer review analyze: a novel benchmark resource for computational analysis of peer reviews.tirthankar ghosal, kamal kaushik varanasi, and valia kordoni.hedgepeer: a dataset for uncertainty detection in peer reviews.In proceedings of the 22nd acm\\u002fieee joint conference on digital libraries, jcdl \\u201922, pages 1\\u20135, new york, ny, usa.tirthankar ghosal, rajeev verma, asif ekbal, and pushpak bhattacharyya.deepsentipeer: harnessing sentiment in review texts to recommend peer review decisions.In proceedings of the 57th annual meeting of the association for computational linguistics, pages 1120\\u20131130, florence, italy.suchin gururangan, mike lewis, ari holtzman, noah a.demix layers: disentangling domains for modular language modeling.In proceedings of the 2022 conference of the north american chapter of the association for computational linguistics: human language technologies, pages 5557\\u20135576, seattle, united states.xinyu hua, mitko nikolov, nikhil badugu, and lu wang.In proceedings of the 2019 conference of the north american chapter of the association for computational linguistics: human language technologies, volume 1 , pages 2131\\u20132137, minneapolis, minnesota.Rob johnson, anthony watkinson, and michael mabe.The stm report: an overview of scientific and scholarly publishing.International association of scientific, technical and medical publishers, the hague, netherlands.Dongyeop kang, waleed ammar, bhavana dalvi, madeleine van zuylen, sebastian kohlmeier, eduard hovy, and roy schwartz.a dataset of peer reviews : collection, insights and nlp applications.In proceedings of the 2018 conference of the north american chapter of the association for computational linguistics: human language technologies, volume 1 , pages 1647\\u20131661, new orleans, louisiana.neha kennard, tim o\\u2019gorman, rajarshi das, akshay sharma, chhandak bagchi, matthew clinton, pranay kumar yelugam, hamed zamani, and andrew mccallum.disapere: a dataset for discourse structure in peer review discussions.In proceedings of the 2022 conference of the north american chapter of the association for computational linguistics: human language technologies, pages 1234\\u20131249, seattle, united states.ilia kuznetsov, jan buchmann, max eichler, and iryna gurevych.Revise and resubmit: an intertextual model of text-based collaboration in peer review.journal of the american society for information science and technology, 64:2\\u201317.Jinhyuk lee, wonjin yoon, sungdong kim, donghyeon kim, sunkyu kim, chan ho so, and jaewoo kang.biobert: a pre-trained biomedical language representation model for biomedical text mining.yinhan liu, myle ott, naman goyal, jingfei du, mandar joshi, danqi chen, omer levy, mike lewis, luke zettlemoyer, and veselin stoyanov.roberta: a robustly optimized bert pretraining approach.marius mosbach, maksym andriushchenko, and dietrich klakow.on the stability of fine-tuning bert: misconceptions, explanations, and strong baselines.In 9th international conference on learning representations, iclr 2021, virtual event, austria, may 3-7, 2021.what to do about non-standard language in nlp.computational support for academic peer review: a perspective from artificial intelligence.Communications of the association for computing machinery , 60:70\\u201379.Which structure of academic articles do referees pay more attention to?\",\"In this paper, we propose the fine-purifying approach to purify fine-tuned pre-trained language models by detecting poisonous dimensions and mitigating backdoors or bias contained in these poisonous dimensions.To detect poisonous dimensions in fine-tuned plms, we utilize the diffusion theory to study the fine-tuning dynamics and find potential poisonous dimensions with abnormal finetuning dynamics.However, the validity of our approach relies on assumptions that backdoors or biases are injected during the fine-tuning process of plms; and the fine-tuning process can be modeled as a diffusion process.Therefore, in cases where the assumptions do not hold, our approach cannot purify the fine-tuned plms.For example, backdoors or biases are contained in the initial plm weights rather than being injected during the fine-tuning process; or the fine-tuning process involves non-gradient optimization, such as zero-order optimization or genetic optimization, and thus cannot be modeled as a diffusion process.\",\"The performance of the evolvemt system is contingent upon the reliability of the comet-qe model in providing accurate labels for the mt requests.Utilizing the encoder\\u2019s embeddings as features necessitates that the comet-qe model performs effectively on blind mt requests.The batch reranking of mt requests after each learning step may result in a computational bottleneck if the queue size is substantial.To mitigate this issue, an asynchronous re-ranking process could be implemented, whereby the queue is only reorganized once the re-ranking is completed.Additionally, before the re-ranking process, a diverse subset of the queue can be selected based on the xlmroberta embeddings, which reflect the novelty of the requests relative to previously processed mt requests.The source embeddings from the xlmroberta model can be cached in parallel during the batch feature extraction process utilizing gpu capabilities, thus facilitating efficient comet-qe inference.Evolvemt could also be optimized for cost-effectiveness by incorporating the cost of each mt in the ensemble into the algorithm.\",\".Error accumulation: incorrect parsing results will affect the reasoner.In our experiments on geometry3k dataset, incorrect parsing results lead to a substantial 21.the reasoner relies on manually predefined theorems, which limits its adaptability.the random exploration in the rl training process leads to uncertainty in the rate of convergence.\",\"First, as ear largely relies on gar generators, the performance of the method is closely tied to the quality of the generator used.We have attempted to use large language models such as t0-3b without fine-tuning as a replacement for the gar generator during testing, but the performance becomes worse.The main reason is that the quality of query expansions generated by t0-3b is too diverse, which makes ear has a higher chance to select from terrible expansions.In contrast, the output quality of gar is more stable.We may need a more complex mechanism that can exclude terrible query expansion if we want to directly use the query expansions generated by t0-3b during inference.Second, ear has demonstrated a strong generalization ability to out-of-domain data, but the method may still face challenges when transferring to other languages without any supervised qa data, which gar and ear are trained on.Although challenging, we are still trying to train the ear system without supervised qa data.\",\".This analysis is affected by this limitation as well given that our evaluations were performed in a low-data setting within a dataset that was already limited in size.the small nature of the datasets applied in this study also presents the danger that some of the models may have memorized certain answers instead of achieving a broader understanding, which could be mitigated by enlarging the datasets and making the tasks more complex.Moreover, we did not study the generalization of nlp models beyond the materials science domain, including adjacent domains such as chemistry and physics.This targeted focus was intentional but imposes limitations on whether the proposed techniques and insights we gained from our analysis are transferable to other domains, including applying nlp models for scientific tasks outside of materials science.Another limitation of our study is the fact that we focused on bert-based models exclusively and did not study autoregressive models, including large language models with billions of parameters highlighted in the introduction.The primary reason for focusing on bert-based models was the diversity of available models trained on different scientific text corpora.Large autoregressive models, on the other hand, are mostly trained on general text corpora with some notable exceptions, such as galactica.while the results presented in this study indicate that domain-specific pretraining can lead to noticeable advantages in downstream performance on text-based materials science tasks, we would like to highlight the associated risks and costs of pretraining a larger set of customized language models for different domains.The heavy financial and environmental costs associated with these pretrain- ing procedures merit careful consideration of what conditions may warrant expensive pretraining and which ones may not.When possible, we encourage future researchers to build upon existing large models to mitigate the pretraining costs.\",\"Although our method can improve the performance as well as accelerate the inference speed, it suffers from two problems: the diversity of generated results is low compared with language models due to the clamp sampling strategy, and the diffusion steps of post-tuning stage should stay consistent with the steps in the training stage, and there still exists the gaps between training and inference, i., |t | = |k| \\u0338= |t \\u2032|, to mitigate the aforementioned two issues, we can explore a better post-training or training strategy to mitigate the training-inference gaps further.In addition, we found that the diffusion model does not perform well in open-ended generation tasks, such as generating incoherent sentences.This is closely related to the drawbacks of nar models, which have a strong conditional independence assumption.we have replaced the people names in the corpora with special placeholders to mitigate the problematic biases issue of generation results.Although we have taken some methods to mitigate the problematic biases, such a problem cannot be solved completely.We urge the users to cautiously apply our methods in the real world and carefully check the generation results.\",\"Our work addresses the sequential task of modeling temporal user data through the use of path signatures as a tool for providing low-dimensional trajectories.Although in our work we inject a post-level timestamp in the final representations, the path signature element is agnostic of time and rather only makes use of the sequence order.It therefore potentially hinders the model\\u2019s ability to efficiently model long timelines with significant and highly irregular lags between posts.additionally, we understand that by employing truncated path signatures in the model, we loose information that can potentially provide additional signal through the compression that happens both in dimensionality reduction and in the signature itself.We have evaluated our model on a longitudinal mental health task.While the proposed architecture is in principle task agnostic we have not yet evaluated it on other longitudinal tasks on social media.\",\"While our approach is designed to be as broadly applicable as possible, an inherent limitation of our work is that it depends on the usage of causal tfrstyle models, which, though architecturally similar to many existing pre-trained models, require hyperparameter search and fine-tuning to replace non-tfrs on downstream tasks.While we find evidence that such models are as capable as other rerankers, and we believe tfrs can be a default design choice going forward with little loss, this extra requirement may still be a barrier for the broad adoption of our approach.More broadly, our approach is designed for settings where some reranker is available.If it is not possible to train a robust reranker, for example in a low data setting or a setting where evaluation relies entirely on human judgments that cannot be reproduced by models, our approach cannot be applied.However, we believe that the growth in learned functions of human judgments as part of rl from human feedback loops provides a promising avenue to roll out our approach to new settings.Our experiments were carefully chosen to represent the capabilities of our models with several base tasks and several reranking objectives.We didn\\u2019t, however, explore certain domains involving attribute control such as formality or simplicity, choosing instead to explore more quality-based downstream exploration.We showed the applicability of our approach when reranking outputs on languages other than english, but further results on languages with different typological characteristics may show different trends.While our work already provides strong speedups both with candidate sets from lattice and beam search decoding, these speedups become even more valuable for approaches that combine multiple rerankers, which have been shown to potentially lead to further improvements in reranking.While we explore this partially in the form of ensembled eel with model probabilities, more exploration on eel for multiple rerankers may be valuable.\",\"Limitation of our stimuli is that they were not specifically designed to require tom.New datasets that perform targeted manipulations of tom alongside tests of language comprehension could help reveal how linguistic experience and tom jointly support pragmatic behaviors.\",\"We recognize that several limitations remain with sentecon and sentecon+.It is not immediately obvious why this should be the case.When building a sentence embedding dictionary, the base lexicon of sentecon may map lexically similar sentences to the same categories, regardless of attributes like negation.Despite this, sentecon produces meaningful repre- sentations for sentences that require compositional understanding, which we attribute to the large number of sentences mapped to each category.For example, the number of negated sentences in the sentence embedding dictionary is far smaller than the number of non-negated sentences\\u2014and likewise for other attributes requiring compositional parsing.Consequently, each category\\u2019s centroid is still approximately an average of the non-negated sentences.The same principle applies to sentecon+ if a reasonably-sized reference corpus is used.If, however, only a very small reference corpus is available and the task dataset is known to require strong compositional understanding, sentecon should be used instead of sentecon+.\",\"Of our method are as follows: despite the better performances our method deflate achieves on multiple ave experiments, its mechanism of using attribute as queries needs to construct the same number of sequences as attributes for a target product, which requires more time for training and evaluating when there are particularly many attributes to consider.The low f1-micro scores of deflate and all other leading methods for ave on our desire dataset emphasizes the demand of further researches for the information extraction of the e-commerce products with implicit attribute values.\",\"We discuss limitations of our work that hopefully could inspire future research in this avenue.Task coverage in arcade arcade consists of realistic data wrangling and eda tasks for a variety of ml datasets.In particular, we focus on problems that can be solved using pandas because of its popularity in data science \\u2014 90% of kaggle notebooks use pandas.Still, our annotated problems may not cover all the types of tasks in these two categories.As an example, data visualization is an important part of eda.additionally, some of the existing datasets in tab.1 usually contain broader types of problems other than the wrangling and eda tasks considered in this paper.session-level evaluation arcade features multiple contextually dependent problems in computational notebooks.As the first step towards evaluating code lms in this interactive program synthesis paradigm, we report turn-level accuracy, and generate notebook context for prompting using ground-truth solutions for the prior turns of a problem , following the common evaluation protocol in task-oriented dialogue.However, this evaluation setting is still not ideal without modeling the user , which often requires building specialized simulators.Reliance on large language models our experiments are based on public and in-house large code lms , which require adequate computational resources9 and create carbon emissions.Their predictions could also be subject to known issues such as misalignment with user intents; for a discussion 9flops usage of fine-tuning pachinco is 3.to reduce the amount of computational resources required, our initial prompting experiments and error analysis suggest that leveraging program execution information could be a promising direction to improve sample efficiency and reduce the size of code lms , while explicit modeling of code-intent correspondence could be a viable path to mitigate alignment issues in model predictions.In addition, as generative ai coding tools are becoming more available to developers, more efforts are required to understand the potential limitations of those systems and the risks they may pose, such as producing insecure code and over-reliance on model predictions.\",\"Since webnlg is derived from 2015-10 version of dbpedia, factkg does not reflect the latest knowledge.Also, another limitation of our work is that the claims of factkg are constructed based on single sentences, like other crowdsourced fact verification datasets.If the claim is generated by more than one sentences, the dataset will be more challenging.\",\"We discuss the limitations of our work: \\u2022 while the three-step masking is shown to be beneficial, masking followed by unmasking may introduce several redundant computations as a token masked in the first two steps might get unmasked in step 3.\\u2022 when compared against the docogen baseline , iterative masking steps increase the time complexity of the domain obfuscation which leads to masking latency.Moreover, as the domain classifier is a critical part of the domain obfuscation, the approach has extra memory footprints.While we identify them as a fixed scalar value working for all kinds of input, we posit that one can propose dynamic input or source domain adaptive thresholding.Currently, we classify it as a limitation of the proposed work.\",\"Theoretically, our method might benefit from comparable corpora across languages, where words and compound words might have similar distribution because zipf\\u2019s law might be satisfied only for similar domains.For instance, as presented in figure 3, word distributions of de and en on wikipedia are similar after applying bpe.In our experiments, we only confirm the effectiveness of our methods on wikipedia corpora in different languages, which are comparable across languages.This might limit the scope of our method.However, multilingual models are commonly pre-trained on comparable corpora, e.another limitation is about the combined objective in eq.In our experiments, we try to eliminate the mlm objective, only considering global regression modeling lgc.The result is not promising, and it seems that lgc can not work well without the help of the mlm objective.\",\"Though autoregressive span selection effectively weakens the conditional independence assumptions imposed by current span-based parsing methods, this strategy imposes another arguably unreasonable inductive bias of forcing a predefined span selection order.We find using post order performs fairly well but this does not necessarily means it is the best order for span selection, and this work might leave other potentially better order unexplored.to discontinuous parsing , the number of splitting points is nondeterministic which can be difficult to handle: a continuous parent span could be split into two continuous subspans with one splitting point, or one continuous subspan and one discontinuous subspan with two splitting points, or two discontinuous subspans with at least three splitting points.The cases of discontinuous parent spans are even more complicated.To eliminate such implausible inductive bias while still benefiting from explicit span correlation modeling.though our model needs only linear steps for parsing, each step takes o time to select a single span over o total spans, making the overall time complexity cubic.We remark that for each step the o operation is parallelizable and\\u2014with full gpu parallelization\\u2014 fairly fast in practice, but it would still be problematic when the sentence is extremely long.\",\"During the configuration search stage, because this is a multi-objective optimization problem involving performance and efficiency, we use the evolutionary algorithm to search here.Designing a robust and efficient optimization objective is not simple and it will affect the convergence of search results.1 limited by hardware, and in order to speed up the search, we use a small subset of the validation set to search retention configuration, which is bound to have a certain impact on the overall search results.Ethical statement in this paper, we propose ase, an algorithm to accelerate multi-turn response selection by prograssively selecting and eliminating unimportant tokens.The training corpora including the ubuntu corpus, the douban corpus and the e-commerce corpus used for evaluating our framework are publicly available and don\\u2019t pose privacy issues.The algorithm that we propose does not introduce ethical or social bias.\",\"This work relied on previously published datasets to source personas on which to anchor the generated unhelpful thoughts, and thus shares the limitations of those datasets.In particular, they use english-language responses, written by workers located in the united states.While these workers are reasonably diverse , the examples generated may not reflect the thought patterns and personas across cultures and diverse populations.This data is also generated by people who are being paid, as opposed to people genuinely engaging about situations that matter to them.Besides the substance of the thoughts themselves, a more direct limitation is that the models generate only english, so would not be directly usable for speakers of other languages.In addition, the data collected reflects the understanding of lay people, rather than trained clinical psychologists.While this makes the material more immediately relatable to other lay people, it is possible that the data do not capture what clinical psychologists would consider adequate illustrations of unhelpful patterns.Our data has been spot-checked by a cbt-trained clinical psychologist and found generally sound, but the entire material should undergo further validation.Another limitation is that the models that we have tested are resource-intensive.In particular, the 9 parlai\\u002ftree\\u002fmain\\u002fprojects\\u002freframe_ thoughts 10our crowdsourcing tasks pay workers well above minimum wage.5, is only available through a paid api.\",\"Despite achieving good performance, there are some limitations in our study.The first is how to handle ambiguous instances in the corpus.45% of the implicit data in pdtb 2.Currently, we follow previous work and simply use the first label for training.But there might be a better solution to handle those cases.Another is the required time for training.To mimic the annotation process of pdtb, our model needs to pass through the embedding layer and transformers twice, so it takes more time to train than the roberta baseline.However, our training time is shorter than pipeline and adversarial due to those two models\\u2019 pipeline setup and adversarial training strategy.Also, note that our method has a similar number of parameters to the roberta baseline since we share embedding layers and transformers between the connection generation and relation classification modules in our approach.Therefore, the memory required to train our model is not much different from that required to train the roberta baseline.\",\"We discuss the following limitations of our work.First, the counterfactual data augmentation procedure we used can only be employed for questions whose answers are named entities.This restricts the applicability of the method as knowledge conflicts can arise for other types of questions, such as boolean questions.Extending our framework to other question types will require a new counterfactual data augmentation method.Second, we conduct our experiments using gold passages \\u2013 i.Using retrieved passages, which is often required in real-world applications, may introduce additional challenges when considering knowledge disentanglement.It is quite simplistic, because the random context is unrelated to the question in terms of topic and participating entities.The focus of this work is on showing that unanswerable questions significantly boost the disentanglement capabilities of a qa model, and that even a simple approach like the one we took improves the model capability.Future creation of unanswerable examples would include more distracting contexts, that at first glance seem very relevant, but still do not contain the answer.We note another minor limitation, implied by the high accuracy in the counterfactual case relative to the factual accuracy.This might stem from the model\\u2019s ability to identify that the text in the counterfactual examples is somewhat unnatural.It is therefore an indication of a potential limitation of the data augmentation methodology, albeit not a major one, judging by the small magnitude of the differences between the counterfactual and factual examples.Finally, while our results indicate that models can learn to disentangle contextual and parametric knowledge, it remains unclear what characterizes easy vs.one such attribute, for example, can be the frequency of a given fact in the pretraining data.due to the size of the models, we do not perform multiple trials of training from different initializations to test for significance.However, we do find similar trends across model sizes, which lends further support to the results presented.\",\".It is possible that a query can be reformulated into multiple possible questions.For example, the query \\u201capple tv bluetooth\\u201d can be reformulated into \\u201chow do i connect a bluetooth device to my apple tv\\u201d or \\u201cdoes apple tv support bluetooth\\u201d.another limitation is that we do not train an end-to-end faq retrieval model.\",\"Though our method produces full sonnets that are more impressive than all previous approaches, it is still not at the level of human-generated poetry.It is not clear how to achieve this level, whether it would be using massive large language models, or through our general approach, which is to bend those models around an interpretable framework that knows the rules that sonnets obey.Certainly our approach requires a lot less data \\u2013 even if one used all the sonnets that have ever been written to train a language model, it is unclear that the language model would learn the very specific rules required of sonnets.However, there may be other ways to obtain these constraints that have not yet been developed.\",\"We collect the dictionary from the internet, and although we make effort to reduce replicate explanations, there is noise in the dictionary.Besides, not all the words are included in the dictionary.In other words, the quality and amount of entries in the chinese dictionary are to be improved.Additionally, our method is pre-trained on the bert-like transformers to enhance the corresponding plms, and can not be applied to llm directly whose frameworks are unavailable.\",\"Our approach requires training a discriminator with an attribute classification dataset, which may be expensive in some scenarios.However, it is still applicable by collecting a small set of attribute-sensitive training instances and applying data augmentation techniques.Our method is hard to achieve fine-grained control.We aim to address attribute-based generation that conditions on a given style, sentiment, toxicity, or topic.However, it cannot condition on a piece of content to control the generation.\",\"While much of our work has focused on showcasing the broad usability of compressed language models, they are not without fault.While our experiments focus on the compression of roberta, the size of its training dataset makes complete exploration of the ability of pruning during pretraining somewhat limited.The work in the paper shows the ability to compress roberta on a smaller pretraining dataset but does not contrast it with the impact of compression on the full dataset.A second limitation of our work is the high computational demand required for creating public domain sparse language models.Despite amortizing the cost of compression to a few pretraining training regimes, the reduction of other language models like albert or xlmr require completely new training, pruning, and transfer experiments.\",\"Our work has several potential limitations.Second, besides the improvement on three widely used tasks, we believe that there are still other abilities, like code generation, of seq2seq models that can be improved by our method, which are not fully explored in this work.\",\"For representation consistency, we apply the regularization to all the tokens and do not distinguish between the different roles the tokens play.more effective data sampling strategies can also be explored.\",\".Gpt-3 achieved near perfect performance on this new test set.We then investigated the task of noun compound conceptualization.Ncc evaluates the capacity of plms to interpret the meaning of new ncs.We showed that gpt-3 still performs reasonably well, but its success can largely be attributed to copying definitions or parts of definitions from its training corpus.\",\".in this work, we demonstrate our framework\\u2019s performance on sentence classification or pair classification tasks.\",\"The main limitation of our method is that it requires human effort to increase the variety of templates, which makes it difficult to create large datasets.Using templates to generate data reduces the time required to create data manually, but the need for human labor remains an obstacle.To resolve this, the templates themselves need to be generated auto- matically, although the tags that constrain the nouns also need to be generated automatically, which is a difficult problem.\",\"The proposed post-abstention methods require additional computation and storage.Despite this additional requirement, we note that this is not a serious concern as current devices have high storage capacity and computation hardware.Furthermore, additional computation for training auxiliary model in retop is required only once and just an inference is required at evaluation time which has a much lower computation cost.Moreover, the risk mitigation that comes with the post-abstention methods weighs much more than the computational or storage overhead in terms of importance.Secondly, human-intervention techniques require a human to be a participant and contribute in the answering process.However, these approaches do not expect the participating human to be an expert in the task.Like other empirical research, it is difficult to exactly predict the magnitude of improvement a post-abstention method can bring.Our idea of exploring sequential application of multiple postabstention methods addresses this concern and can be used based on the application requirements.\",\"The biggest limitation of our framework is that persona prompting increases the input length of the model, increasing inference time.Through the analysis of real online user logs, the inference time will be 1.\",\"A first limitation of our contribution stems from the fact that to compute the focal distillation term in the loss, predictions from the old model are required.This additional stream of information will therefore cause a slight increase in the required computational power.In this work, we only experimented with fd based on mean-squared error between pre-softmax logits as that approach yielded best results in the paper our experiments are based on, leaving experiments using fd with kullback-leibler divergence between temperature-scaled softmax outputs for future research.Due to inference time limitations in a production setting, we did not investigate the reduction of negative flips with ensembles either.\",\"This work introduced neqa, a question answering dataset for evaluating the ability of large language models to process negation.While our neqa attempted to cover diverse types of negation and multiple data sources , it is possible that the dataset construction misses some types of negation or domains of text.additionally, neqa is an english dataset, and it would be interesting to extend it to non-english languages and conduct a more comprehensive evaluation of language models, including multilingual ones.Another potential limitation is sensitivity in language model prompting.Language model performance is known to be influenced by the specific prompt used to query the model , and prompt engineering\\u2014finding the \\u201cright\\u201d prompt\\u2014 may be needed to obtain reasonable outputs from the language models.As our language model evaluation protocol uses prompting , the evaluation results may inherit such prompt sensitivity.\",\".Here, we propose some attempts to overcome these limitations.in the acquisition of control signals, there are two main constraints for performance, including the accuracy of control signals and the suitability of retrieval results in the testing step.With regard to , the results of the oracle setting demonstrate that our framework has a high ceiling when ground-true control signals are given.Therefore, we have tried to enhance robustness by noising the control factors.Noising methods contain adding, removing, and replacing random control tokens.However, experimental results show that noising methods compromise the success rate of control, which is contrary to the motivation of this work.with respect to , we focus on the performance of the retrieval model in the inference stage.The control signals straightforwardly come from the retrieved responses.In this paper, we have proposed a task-specific design that combines semantic and emotional similarity to retrieve but it is still simple compared to those sota dialogue response selection models.as an advantage of diffusemp, both the annotating taggers and the retrieval model are orthogonal to empathetic response generation.It is easy for followers to employ higher-performance response selection models and attribute annotating taggers to empower the diffusemp.finally, the diffusion model requires a lot of gpu computational resources and is slow when inference, which limits its application.There are many attempts to reduce the computational resources required by the diffusion model as well as to speed up the process and inference.Theoretically, the relevant improvements would also have an enhancing effect on our framework and would be helpful for spreading the diffusion model to the nlp community.\",\"We synthesize negative instances by substituting relational phrases with misleading tokens.However, the relational semantics in some instances may be expressed implicitly.That is, there are no key tokens that directly correspond to the target relation.Therefore, we cannot synthesize negative instances based on these instances.Additionally, we consider substitution ratio \\u03f5 as a fixed hyperparameter.It may be a better choice to dynamically determine \\u03f5 based on the input instance.\",\"The encoder of the lcm which we have utilized for our experiments is a basic deep neural network.Replacing it with more robust and effective architectures could help achieve better performance.Furthermore, instead of using pre-trained glove embeddings for the encoder, using iddr-specific embeddings could have been a more efficient approach.Lastly, our models have been trained and evaluated on pdtb 2.0, instead of the latest pdtb 3.0, which includes also intra-sentential implicit relations and has a more systematic sense hierarchy.\",\"The main contributions of this paper are towards tackling over-smoothing issue for learning unsupervised sentence representation.The proposed approach is fairly basic and may simply be extended to improve the performance of other state-of-the-art models.More broadly, we anticipate that the central idea of this study will provide insights to other research communities seeking to improve sentence representation in an unsupervised setting.Admittedly, the proposed strategies are restricted with unsupervised training, and biases in the training corpus also may influence the performance of the resulting model.These concerns warrant further research and consideration when utilizing this work to build unsupervised retrieval systems.\",\"The proposed pll-word-l2r metric has the same practical limitations as previous ll\\u002fpll approaches.Most importantly, these scores can be influenced by many superfluous factors, such as the number of available synonyms.We therefore expect our method to be most useful in highly controlled minimal pair or multiple choice setups.for instance, our approach pre-specifies the number of tokens in a word, thus limiting the space of possible alternatives.Future approaches might investigate a way to normalize the pll score distribution over words with a varying number of tokens.Further, it would be interesting to attempt to estimate the joint probability of all tokens in a word instead of predicting them left-to-right or without any other within-word contextual information.Finally, we test our approach on english text corpora; our results might not generalize to agglutinative languages and are of less relevance to isolating languages.\",\"Although our method has achieved outstanding performance compared to current kd techniques, it is still a word-level kd method and also suffers from some limitations in vanilla word-level kd, e.how to design a unified and more powerful kd method from the perspective of the connection between word- and sequence-level kd still remains unsolved.moreover, our study focuses on the mainstream kd techniques in nmt, which transfer knowledge through teachers\\u2019 predictions, while some other kd techniques, like directly distilling the hidden states , are not within the scope of this study and thus not included.\",\".1, where we show that our approach doesn\\u2019t provide benefits on improving models that have the same data and compute complexity as the teacher.did you discuss any potential risks of your work?\",\"Linguistic studies have shown that respective readings are not necessary to have two coordinate structures in the same sentence.Both wikiresnli and natresnli have only one sentence in the premise and do not exhaust all possible and complicated realizations of respective readings.However, we are able to discuss and investigate lms\\u2019 generalizability with \\u201crespectively\\u201d with three constructions, i.our experiments are english-specific and are limited to lms that can be run with an academic budget.However, our conclusions about the generalizability towards respective readings should be viewed as language-agnostic given there are linguistic constructions under-discussed in many other languages and it is worth researchers\\u2019 attention to study them.\",\"While we establish the \\u201cmiraculous\\u201d ability of character-blind models to induce robust spelling 8the models were trained on the same number of tokens, but only 6% of byt5 training was on english, and we estimate 4 utf-8 bytes per t5 token.Information through large-scale web pretraining, our work does not attempt to identify the mechanisms or sources through which this information is learned.Possible sources within web corpora include: dictionaries containing phonetic pronunciation guides, alphabetically ordered lists, typos and other misspellings, and examples of spelling words with dashes or spaces between every character.Linguistic phenomena that may aide in inducing spelling knowledge include words with predictable morphemic makeup, and cases where meaningform relation is non-arbitrary, contra saussure\\u2019s \\u201csemiotic arbitrariness\\u201d.We refer the reader to itzhak and levy and kaushal and mahowald for work in this direction.Most of our image generation experiments are limited to english.however it would be valuable to test this further, and to explore training image generation models on multilingual image-caption datasets, as opposed to merely using a pretrained multilingual text encoder.Ideally, it would be possible to conduct controlled comparisons between pretrained text encoders that differ only in one regard, to isolate all factors contributing to performance.However as pretraining large language models is resource intensive, we were only able to use off-the-shelf text encoders, which often differ along multiple axes.In our text-only experiments, we isolated the contributions of character-awareness and multilinguality.However, in our image generation experiments, these factors were conflated, as we had limited resources for training new models.Still, the fact that byt5based image generation models outperform t5 despite being multilingual strongly suggests that character-awareness is the key factor for spelling ability.Another limitation is that we focused on image generation models that leverage frozen pretrained text encoders.This enabled straightforward experimentation by swapping encoders and retraining the image generation module.\",\"Although our model has made some progress, it still has some limitations.First of all, sgt uses the tag type to represent the connection order of glcs fragments when forming a complete utterance, and the average statistics on the three datasets we used show that more than 99% of the complete utterance can be composed with less than three glcs fragments.That will lead to situations that need to combine multiple glcss to form a complete utterance, which cannot be fully trained or fall into unbalanced tag categories.Second, like other tagging-based models, the fragments that make up the complete utterance must exist in history utterances or connection words, which does not work well for situations where it is necessary to combine context information and introduce new words to express their complete utterance.\",\"Although we have proven that our work can significantly alleviate concept bias and extract highquality and new concepts, it also has some limitations.model novelty although kpce can effectively mitigate the spurious co-occurrence correlations between entities and biased concepts, the proposed framework is not entirely novel.The novelty of our work is to conduct the first thorough causal analysis that shows the spurious correlations between entities and biased concepts in the concept extraction task.After defining the problem and scm of concept extraction in \\u00a7 3.1, we propose a promptbased approach to implement the interventions toward the scm to elicit the unbiased knowledge from plms.Previous work in language prompting mostly guides the plms with prompts but is unaware of the cause-effect relations in its task, which may hinder the effectiveness of prompts.topic classification although the topics obtained by clustering are mostly mutually exclusive, there are still cases where an entity can be classified into multiple topics.Therefore, considering only one topic for the entity excludes the correct concepts.Threshold selection we only reserve concepts with confidence scores bigger than the selection threshold , which can hardly achieve a satisfactory balance of precision and recall.If we select a relatively big threshold, we can get more accurate concepts but may lose some correct ones.If the recall is preferred, precision might be hurt.\",\"There are two main limitations in this work.First, instead of best st performance given full data, our cross-modal pre-training only aims to demonstrate the effectiveness of our method in the low-resource st setting.Explores the possibility of pre-training mt models with phoneme tokenizations, though it is unclear if the phoneme-based mt model has an advantage over the bpe-based mt model.\",\"Both the feedback simulator and the feedback evaluator in our work can be further improved.alternatively, one can also leverage the feedback evaluator to steer the training of the feedback simulator.As we briefly discussed, one could also extend our feedback simulator to imitate more fine-grained user behaviors, such as the agenda of how users would engage in the error correction process.although our proposed approaches have not made any assumptions on the type of logical forms and can thus be applied to any of them, in experiments, we have only evaluated them in the task of text-to-sql semantic parsing.Future research can further assess our proposed models in other semantic parsing settings such as knowledge base question answering.On the other hand, as our simulator is primarily designed for interactive semantic parsing, it as- sumes meaning representations of both the groundtruth prediction and the model prediction.Therefore, generalizing our methods to other nlp tasks may need additional effort.For example, if we apply our methods to a similar interaction scenario for retrieval-based qa , then we will additionally need to define logical forms to describe the ground-truth retrieval process and that of the qa model.For open-ended tasks such as keywordbased story generation , defining such logical forms will need non-trivial effort.\",\"In this work, we do not propose any new methods because, as an opinion paper, we focus on raising the problems and making vivid demonstrations to readers.The experiments are limited to linear svm and bert on data sets in the benchmark lexglue.We hope that, within the page limit, our experiments sufficiently convey the points to readers.\",\"In spite of the strong performance of changes, its design still has the following limitations.we believe the model performance could be further improved by incorporating document hierarchy of different granularity like dependency parsing trees and rhetorical structure theory trees.in addition, we only focus on single academic paper summarization in this work.Academic papers generally contain a large amount of domain knowledge, thus introducing domain knowledge from peer papers or citation networks should further boost model performance.\",\".we hope that future studies in this field can alleviate the aforementioned problems and thus promote the application of prompt tuning.one limitation of prompt tuning in this setup is the sensitivity to hyperparameter tuning.It is difficult to search for a suitable hyperparamter setup.The hyperparameter tuning experience in finetuning is not suitable for prompt tuning.Fortunately, we find that prompt tuning for generative multimodal pretrained models is not as sensitive to hyperparameters as prompt tuning for pretrained language models.another limitation of prompt tuning in this setup is slow convergence.Though prompt tuning has noticeable advantages in training efficiency, it costs at least 40 epochs for prompt tuning to achieve the nearly best performance on some datasets.A larger number of training epochs may incur more computation costs though prompt tuning has an advantage in training efficiency compared with finetuning.this indicates that finding a better solution for fast and stable convergence is also important besides reaching comparable or even improved performance over the conventional finetuning.Despite the aforementioned limitations, prompt tuning demonstrates significantly better robustness against adversarial attack.\",\"We point out the limitations of large language models.The vision of posttext shows promise of less costly training, maintenance, and more explainability.However, no actual system is built yet to validate these claims and it is also not clear that a system with posttext architecture will be easier to deploy since it has more components.\",\"Prosody-tts adopts generative diffusion models for high-quality synthesis, and thus it inherently requires multiple iterative refinements for better results.Besides, latent diffusion models require typically require more computational resources, and degradation could be witnessed with decreased training data.One of our future directions is to develop lightweight and fast diffusion models for accelerating sampling.\",\"The limiation of this work is that we only consider one type of prefixes\\u002fsuffixes, i.for instance, in english, there are many suffixes such as -or, -er, and -ee, which mean a person with a certain occupation.\",\"One of the limitations of our survey is that it covers a limited sample space of 15 papers from emnlp 2020 and acl 2020.while a larger sample would be helpful in gathering more evidence, access to specific tracks is limited at nlp conferences, unless hosted online via a virtual or hybrid system.With respect to our case study, we evaluate on the asr utterances, but with labels corresponding to the original manual transcriptions.For a perfect comparison, the asr utterances would need to be re-annotated as the talk move could change based on the severity of transcription errors.\",\"In this paper, we show that lens shows better human correlation than other metrics on wikipedia and news domains.Future research can further experiment and extend lens to other domains, such as medical and children\\u2019s books, as the preference for different simplification operations can vary depending on the domain and user.simpeval dataset and lens are also limited to the english language.\",\".Such efforts will lead to more accurate and reliable responses from llms, which will have a positive impact on their applications in diverse fields.at present, we have selected sentences with uncertain meanings exclusively from the gpt-3 and instructgpt series, potentially overlooking uncertainty present in responses generated by other llms.However, it is not feasible to catalog all sentences with uncertain meanings exhaustively.As a direction for future research, we propose to concentrate on the automated acquisition of more accurate reference sentences to address this concern.\\u2022 limitations of input forms: our examination was confined to three unique input forms: direct, instruction, and icl.There is burgeoning research aimed at bridging the gap between models and human-like methods of reasoning and problem-solving, including but not limited to approaches like reflexion , tot , mot.Future endeavors will integrate additional cognitive and decision-making methods to delve deeper into the self-knowledge exhibited by these llms.\",\"We only consider 14 languages and 21 categories, whereas wikipedia has pages in more than 300 languages and 200 broad categories.Increasing the scale and diversity will further improve method generalization.Our proposed method relies on the good multilingual translation of key and value from table pairs.Although we use key, value, and category together for better context, enhancement in table translation will benefit our approach.Because our rule-based system requires manual intervention, it has automation limits.Upgrading to completely automated methods based on a large language model may be advantageous.We are only considering updates for semi-structured tables.However, updating other page elements, such as images and article text, could also be considered.Although a direct expansion of our method to a multi-modal setting is complex.\",\"While muda relies on set of hand-crafted rules for tagging specific phenomena, these rules might involve the use of other error-prone systems and these errors might be susceptible to problems that could limit the applicability of our tagger.However, this could be fixed by extending muda to use newer and better versions of these systems.The use of f-1 per tag with surface-form matching between reference\\u002ftranslation can also lead to penalizing translations that use context correctly but choose other equivalent words.Nevertheless, this should also be mitigable by extending the scoring method to, for example, match synonyms.Finally, the benchmarking of context-aware models might not apply to newer, state-of-the-art translation models, especially if these leverage large language models that were trained on long-context data.\",\"This work focuses on the effects of adaptive inference in a low-resource setting, specifically when training data is limited.Our experiments suggest that the negative impact of conflicting gradients may be less prominent when larger amount of training data is available.Nonetheless, our results have important implications for the growing trend of increasingly large language models.We hope this work inspires further research on methods to reduce the computational cost of nlp.This work concentrates on evaluating the speedaccuracy trade-off of multi-model and early-exit at inference time.We recognize that there are additional factors, such as memory usage, batch processing, and training duration, that could be considered when comparing these methods.Finally, we experimented with seven text classification tasks in english.We recognize that results may vary for other tasks and languages.\",\"Our summary generation and clustering steps have some key limitations.First, there is an api cost involved in using instruction-tuned models such as instructgpt.It is thus preferred to use publicly available large language models, which have the additional advantage of results being reproducible by other researchers.With the recent release of large models, this is a promising avenue for further investigation.Second, it is difficult to control the granularity of steps predicted by the summary generation step as they are obtained through free-form text generation.Third, the clustering approach involves hyperparameters that need to be manually set such as the number of clusters.We believe these limitations are best addressed by tighter coupling between the different components.\",\".First, although the method is not language-dependent, it was evaluated on a single language, japanese.It would be worthwhile to evaluate the method on other languages to examine the approach\\u2019s versatility.Second, the method uses dictionaries to obtain patterns.Although japanese morphological analysis commonly uses dictionaries to perform lemmatization, it would be worthwhile to evaluate the method with only training data or dictionaries derived from text.Below, i discuss the current limitations for word segmentation, pos tagging, and lemmatization in detail.Word segmentation the proposed method\\u2019s accuracy of word segmentation will depend on the target language\\u2019s typological factors , such as the character set size, lexicon size, and average word length.Among those factors, the character set size will especially matter because the current patterns mostly comprise surface strings and are likely to suffer from data sparseness.It will thus be valuable to evaluate the method on chinese, which has a larger character set than japanese.It will also be important to evaluate the method on languages with different typological factors from japanese, such as hebrew and finnish.The training data size will not matter if the method is used to approximate some existing resource-efficient method via structure compilation.Pos tagging compared to word segmentation, pos tagging requires more complex and abstract feature sets that are tailored for the target language and pos tag set , which poses a challenge for the proposed method.The current pattern template is tailored for japanese and the juman pos tag set; hence, for other languages and pos tag sets, a pattern template will need to be designed by referring to the feature templates of existing learning-based methods for the target language and pos tag set.Because the method jointly solves word segmentation and pos tagging in a left-to-right manner, patterns cannot leverage certain abstract features from posterior contexts of the target word.For application to other languages, it would be worthwhile to explore not only left-to-right processing but also right-to-left processing and a cascaded pipeline approach.Lemmatization the approach here currently requires a morphological dictionary with lemmas or a fine-grained pos tag set that includes conjugation types and forms to perform lemmatization.Because lemma generation rules for other languages can be induced from lemma-annotated datasets , the method could be applied to other languages by using such lemma generation rules as the target labels for classification.Challenging target languages include morphologically rich languages such as arabic and czech.\",\"Mospc can improve ranking accuracy on each fine-grained mos score segment, but at the same time, the training method based on pair comparison may impair the generalization performance.As there are unseen-systems in the bvcc validation set, the system-level results of bvcc are affected by the generalization performance degradation.We introduced the c-mixup algorithm to enhance the generalization performance, which increased the complexity of the experiment to some extent.\",\"Limitation on the length of input that a model can handle.Situational information can typically be obtained from various sources, and often, an excessive amount of information is present.Humans can quickly focus on crucial information and discard the rest, otherwise, it would take forever to read, process, and reason over surrounding information.Researchers have identified the frame problem that describes the dilemma of a reasoning system in determining which aspects of a situation change and which remain constant after an action.To date, there has been no satisfactory solution to this questions, making the challenge of situated conversation an interesting open challenge.Common ground: knowledge about situations is closely related to common ground\\u2013the information shared by conversation participants.Without common ground, conversation participants would need to convey every parameter of their message, which is extremely inefficient.The importance of common ground is widely recognized, and decades of dialogue research have been devoted to developing systems that can effectively establish common ground with their interlocutors by inferring, presenting, requesting, accepting, and repairing individual beliefs about various information through conversations.In this paper, we did not delve into the problem of common ground, but the consideration of situations, which is our main proposal, is the first step towards computational modeling of grounding.\",\"We would like to claim our limitations from two perspectives: technical-wise and application-wise.Technical-wise: we currently only experiment with bert-base, bert-large, and robertalarge as the basic encoders.For larger language models, due to limited resources, we have not implemented them.Application-wise: the experimental data comes from the open data repository released by governments of various countries.Although many domains are covered, some domain-specific data, such as biomedical, have not been considered.Furthermore, our tabular data are all from english, open data research in other languages can be considered as a future research direction.\",\"There may be some possible limitations in this study: 1.it should be acknowledged that web documents are heterogeneous themselves and a unified framework to accommodate all web documents may be infeasible.In this article, instead of pre-define the domain\\u002ftype of web documents we target at, we adopt a problem-motivated research paradigm where we ground ourselves to two characteristics during discourse schema design.Although we simplify the discourse schema to promote its universality, due to the free-style and domain diversity of web document data, it still has a limited scope of usage, mainly on general news report with multiple topics.For future studies, label sets could be revised in order to better account for the semantic functions in web documents of specific domains, where fine-grained labels and domain-specific labels can be considered.in this paper, we only consider parsing the discourse from a list of document logical blocks already pre-processed in advance while do not contain a complete pipeline from input html source code to the final output discourse structure in the task setting.the annotated data volume in this paper is not big enough due to the expensive labour overhead, which may introduce noise into experiments and distort the performance and analysis.Also, the domain diversity and multilingualism of dataset could be questioned since we collect data from single platform in chinese.\",\"We would like to acknowledge three categories of limitation that we recognize in this work: \\u2022 we evaluate the effectiveness of priming in a setting where the tasks used during the priming stage and the fine-tuning stage offer no additional disparity besides being different in language, i., they are all ner tasks coming from the same domain.While this degree of variation is consistent with the application of meta-learning in other modalities, e., vision , whether or not the gains we report here remain at the same strength when we introduce diverse tasks during priming and fine-tuning still needs to be tested.Examples of such diversity include strong domain shift or using one task, e., pos, for priming and another, e.\\u2022 it\\u2019s not clear how the size of the pre-trained model affects the necessity of priming.Priming might consistently result in gains, or its benefits might fade away with larger plms encoding stronger language capabilities.\\u2022 finally, our work does not implement higherorder gradient calculation and does not evaluate and discuss the potential additional gains that might come as a result.That opportunity can be further explored as well.\",\"The goldstein scale is a widely-used measure of the conflictual versus cooperative nature of interactions between countries.The scale was created by a panel of international relations experts who ranked descriptions of interactions.It was initially created to score action categories in the weis event ontology and was later adapted to cameo.The goldstein scale applies only to the action category of an event.Thus, two different \\u201cfight\\u201d events, which might involve two different pairs of actors, occur at different times, or differ dramatically with respect to the number of associated fatalities, will still be assigned the same goldstein value.the goldstein scale is thus a poor measure of a conflict\\u2019s perceived \\u201cintensity\\u201d, as it ignores much of the information that contributes to that perception.Recent work in conflict studies, for instance, operationalizes \\u201cintensity\\u201d primarily using casualty counts , which the goldstein scale ignores entirely.1, we show the empirical distribution of assigned goldstein values alongside the empirical distribution of casualty counts in a dataset of conflict events.The goldstein scale is very coarsegrained; while it ostensibly ranges between \\u221210.0, only a small number of discrete values ever occur, with many action categories assigned the same value.For the purpose of measuring conflict intensity, a finer-grained and more contextual scale is desirable.\",\"We point at some directions for future improvements in automatic instruction generation.First, as shown in \\u00a73, unnatural instructions contains noisy examples, in which either the instruction, input, or output are invalid., by annotating a subset of examples as either valid or not and training a classifier for determining the correctness of generated instances.In another human-in-the-loop scenario, models trained on unnatural instructions can be queried by humans to find examples on which these models fail, thus collecting harder examples.Finally, language models are known to sometimes reflect undesirable biases present in their training data.Automatically generated data may therefore contain such content.We note that during our manual analysis, we did not notice any harmful examples.\",\"Although our masks in different layers are binary and require significantly less storage compared to other petl networks, we still need the underlying 32-bit scores for each mask during the training process.Therefore, propetl consumes slightly more memory in the training time than existing petl methods.To fine-tune propetl, it takes a similar training time to conventional petl modules, which means our method will normally take a longer time to converge compared to the fully fine-tuned model.\",\"Our method relies on having access to teacher embeddings and prediction which may not always be possible in a black-box distillation setting.Retrieval augmentation also requires maintaining a knowledge base that is memory intensive.The cost of the retrieval process is dependent on the size of the training corpus, which can be a limitation when dealing with very large training datasets.Conducting dataset distillation on the training corpus to further reduce memory cost and retrieval time is an important future step for our framework.Acknowledgments this work was done when jianyi zhang was an intern at amazon search.we thank yuchen bian for the valuable\",\"We presented in this paper a real-world annotated example of seeking information in scientific publications.Even if the number of instances presented here is of the same order of magnitude as what is present in benchmarks, we presented only one query and its correspondent relevance judgements, provided by one expert, due to resource constraints.As we noted above, building a corpus dedicated to the exploration of a single information need does however correspond to a real industrial use case.Further, we favored the use of sentencetransformers format for all neural models for the sake of efficiency.We did not dive into providing the best-known performing models and did not consider optimizing them in our case, as overfitting to our data might induce errors in\",\"We currently give separate definitions of the derivation trees of the four two-level formalisms.Ideally, one could give a general definition of derivation tree in a multi-level formalism, and then derive each particular case by plugging in the definition of derivation trees of cfgs and pdas.the d-strong equivalence results only hold for spinal tag and spinal paa.However, full tag and paa are not d-strongly equivalent to spinal tag and spinal paa, and therefore they are not d-strongly equivalent to cfg \\u22b2 cfg and cfg \\u22b2 pda, either.\",\"The models used in this work are small, compared to the large language models used in many language generation tasks today.To attempt to show possible impacts of scale, two different sized models were employed in this work and show similar results, so it is likely that the method proposed here would scale to larger models.Training dynamics were not altered between the two model sizes, which is a potential area for improvement.More sophisticated methods of inserting backdoors could also be employed than training one into the model, but this seemed to work well.Ethical statement this work attempts to improve the state of watermarking llms in order to demonstrate ownership.Our hope is to help improve the space of responsible llm usage by helping model creators assert or demonstrate ownership of their models, although there are probably applications of watermarks that we have not considered that may be detrimental.This work does expose ways to find watermarks, which could be used by a potential adversary who had stolen a model and was attempting to use it illicitly.However, we believe that disclosure of vulnerabilities allows stronger system construction and is preferred over security by obscurity.\",\"Our transcription guidelines occupy a certain position in the continuum between completely preserving the authenticity of learner handwriting and completely ig- noring it.This position is motivated by our aim of capturing mainly orthographic features, which comes at the expense of other features of the text.In the course of this study, we only applied the guidelines to german texts.While we are quite certain that they generalize to other alphabetic languages , it cannot be ruled out that we missed some language-specific phenomena.However, these could be mitigated by augmenting the guidelines accordingly.Our guidelines are not directly applicable to other, e.\",\".This means that peerda improves the semantics learning of existing labeled spans, but is ineffective to classify other spans outside the training set.Therefore, it would be beneficial to engage outer source knowledge , where a variety of important entities and text spans can also be better learned with our peerda approach.\\u2022 since peerda is designed in the mrc formulation on top of the encoder-only pre-trained language models , it is not comparable with other methods built on encoder-decoder plms.It would be of great value to try peerda on encoder-decoder plms such as bart and t5 , to see whether peerda is a general approach regardless of model architecture.\\u2022 as shown in table 9, although peerda can significantly alleviate the missing predictions, the most prevailing error in the mrc model, peerda also introduces some new errors, i.it should be noted that those problematic spans are usually observed in different span sets, where they would learn different category semantics from their peers.Therefore, we speculate that those spans tend to leverage the learned category semantics more than their context information to determine their categories.We hope such finding can shed light on future research to further improve peerda.\",\"Limitation of neuspell due to sequence labeling is that it can\\u2019t handle compounding errors like iphonepro to iphone pro.It\\u2019s evident that our diverse synthetic data generation techniques are effective and lead to significant improvement even with a simple 1 layer enc\\u002fdec transformer compared to deeper pre-trained models like bert and neuspell.Adding candidates from our novel phonetic transliteration model was beneficial and led to a total absolute gain of 2-3% at query level consistently across the models and 7.5% accuracy improvement at the word candidate level.Effect of curriculum learning: while reparosbase itself is good than the competent baselines, our error analysis showed that it still couldn\\u2019t address the complex edit\\u002fphonetic + compounding spell errors.This led us to design a few new curricula to improve.Our first curriculum reparosbase-1, was to simply add more of complex edit\\u002fphonetic+compounding training samples.This resulted in marginal improvement over reparosbase.However, with a different curriculum of only fine-tuning reparos-base on tougher mistakes we observe that the performance increases significantly on the improvement set where most of the mistakes lie, however, there\\u2019s a drop in the regression set performance when compared to the reparos-base and reparosbase-1.On analyzing this further, we observed that this was due to the over-correction problem where the model is aggressively altering correct queries in the regression set.Hence, we change the curriculum and in reparos-c2 we find that a further fine-tuning on weakly supervised user feedback data improves upon all the variants significantly on both the data sets.This is intuitively due to relatively more frequent queries in the regression set on which receiving implicit user-feedback through clicks is possible at scale.Thus adding this new curriculum helped acheive the best performance.Online evaluation: in production, we adopt a 2-step architecture by adding an ml ranker that does the final candidate selection.This multi-stage setup empirically produced better results than just finetuning the nmt model since top-10 accuracy of reparos-c2 is 76.this helped all the models and is removed from results\",\"Our proposed monet is a classification-based method requiring the pre-defined ontology containing all slot-value pairs.Moreover, during prediction, for each slot, its distance with all possible values is calculated, i., the prediction has to process 30 times, which is the number of slots in the multiwoz dataset.Compared with the generation methods that only process once and do not need ontology, our method is short in training efficiency and scalability.However, most task-oriented dialogue datasets contain their knowledge base containing slot value information, so it\\u2019s acceptable to construct the ontology for random sampling.7 demonstrate that our method can be implemented into generation-based backbone models.\",\"In this work, we achieve a noticeable improvement in the gec task by introducing additional context information with a nar model.However, in order to focus on incorrect tokens, the input of the nar is required to be constructed based on the ar output distribution.In this way, the ar and nar model perform sequentially, which leads to much time consumption in the training stage.on the other hand, due to the limitation of computation resources, all experiments are conducted on two nvidia titan v gpus with 12gb vram.Therefore, we could not compare with the state-of-the-art models which are pre-trained with 100m synthetic parallel examples.\",\"Our experiments are conducted based on the t5base pre-trained language model.Due to the computational resource constraints, we did not conduct experiments on other similar plms, such as bart, and t5 model with larger scale, such as t5-large and t5-3b.\",\".In this paper, we use fiction novels from multiple languages in project gutenberg.One assumption of this work is that the text is representative of the culture surrounding the language.While this may or may not be true , our investigation\\u2019s focus is on the structure, or narrative arc, of stories and how arcs may differ across languages.Naturally, our findings may differ for other genres, such as history or self-help., a corpus of about 5,000 spanish, french, and portuguese narrative texts, poetry, or plays.However, multilingual corpora of this kind are few and far between, even for high-resource languages like spanish and french.this work heavily relies on liwc, which is proprietary software.Many researchers may not have access to all liwc dictionaries.In addition, as a dictionary of psychometric properties, liwc is constantly evolving and improving with new research in psychology and linguistics.\",\"Limitation of model scale the benchmark only included the evaluation of moderate-size language models and did not experiment on large language models.however, we acknowledge that the current state of extremely weak supervision would be better understood and assessed if complete evaluations on state-of-the-art large language models, such as instruct-gpt , palm , and chatgpt exist.While we lack the computational resources to perform such an evaluation, we hope this work can stimulate interest in xws-tc and complete the study.Limitation of text classification another limitation is the scope of text classification.While prompt and seed methods have shown strong performances on text classification, this performance does not extend to other general classification tasks, such as natural language inference\\u002fentailment.\",\"An important limitation of our contribution lies in breadth of the experimental validation.We decided to focus our experimentation on the setup where we encountered the issue of redundant datasets: ner for a large-scale conversational assistant.While it is true that our approach does not make any assumption neither about the task nor about the model architecture, and while we also provide a rigorous proof to support the estimated gain in terms of training time, the extent to which our approach remains the best time-accuracy trade-off when other tasks are considered is not explored in this work.An additional limitation stems from the lack of absolute results on the internal datasets, as the latter can only be disclosed as relative improvements over a baseline due to internal policy.We attempt to mitigate this by reporting full results on an open dataset, but as we mention we have to resort to upsampling given the artificial deduplication that manually curated datasets incur before publishing.\",\"Our work has focused strongly on the formal aspects of bpe.Nlp practictioners should not be dissuaded from using bpe for subword tokenization, despite our presentation of examples where greedy bpe fails.Indeed, in contrast to synthetic examples on toy alphabet, on real data we made an observation that greedy bpe may be close to optimal.\",\"The current work requires that knowledge modules be written by hand.Commonly used axioms, such as general knowledge like the commonsense law of inertia expressed by event calculus, can be reused easily, but there are vast amounts of other commonsense knowledge that are not easy to obtain.Llms could be used to supply this information, but we have not tried.Knowledge graphs, such as con- ceptnet , comet and atomic , can be utilized to populate asp rules.also, when using large language models, despite various efforts, sometimes it is not understandable why they do not behave as expected.\",\"Our model currently requires high-quality word boundaries for both speech and sign videos.However, as demonstrated by our preliminary results in table 5, we can overcome such limitations by incorporating more powerful unsupervised segmentation algorithms to our system.Further, while our dataset is sufficient to model the variability in speech and videos, all experiments to date have assumed that spoken and signed sentences share similar word order, which may not be true of natural spoken and signed communications.A future direction of this research will seek to develop methods for spoken-sign language pairs with very different syntactic structures.Lastly, the vocabulary size under our study on word-level ssr-u is relatively small , and a promising future direction is to extend the current approach to deal with much larger vocabulary size in more diverse conversations.\",\"Despite introducing meta learning for domain adaptive few-shot misinformation detection, we have not discussed the setting of cross-domain adaptation with multiple source datasets to further improve the performance for identifying early-stage misinformation.Due to the lack of early-stage misinformation data, we limit our choice of the target domain to covid-19, which may hinder the generalization of the proposed method to other domains.Additionally, the proposed method does not leverage efficient approximation or first-order meta learning methods to reduce the computational costs in training metaadapt.\",\"This resource paper proposes a new dataset and experiments with a baseline model only.We do not focus on creating new models and architectures.moreover, the dataset has only 31k video-sentence pairs, and we plan to extend this to enable more reliable data-driven model development.ethical concerns we create a dataset from publicly available resources without violating copyright.We are not aware of any ethical concerns regarding our dataset.Moreover, the dataset involves people of indian origin and is created mainly for indian sign language translation.The annotator involved in the dataset validation is a hard-of-hearing person and an isl instructor, and they performed the validation voluntarily.\",\"One significant limitation to our study is that, as of march 23rd 2023, openai has deprecated access to code-davinci-0024, thus rendering our results non-replicable for any team not granted special access to these models by openai.We did not anticipate this deprecation while conducting this work and we believe this raises serious questions about the usage of api-based language models in scholarly work.Another limitation is that the 12 tasks we selected may not be representative of the broader population of natural language tasks.Had we conducted our experiments on a larger selection of tasks there may have been larger-scale trends that we would have been able to uncover.The largest and most pressing limitation with our work is that the models we are testing on have closed-source pre-training datasets.Thus, we are unable to verify the extent to which our task datasets have been included in the training or instruction fine-tuning data.Given that the training data for most of the models tested in this work cuts off in late 2021, this is a very strong possibility.Our results should be viewed with this limitation strongly in mind.Finally, while we experimented with different code prompts, the search space of possible prompts is very large.Thus, it is very likely that there exists some prompt that outperforms our chosen prompts for each task.\",\"In all the experiments conducted in this paper, perspective api was used to evaluate the level of toxicity in generated outputs.While using it, two substantial limitations were noted.The first limitation related to the frequency with which the api is updated, often resulting in a different scoring for the same text across newer versions of the tool.This leads to considerable issues with reproducing results \\u2013 values presented in our paper are therefore only a snapshot in time of the version of perspective api used during experimentation.The second limitation is that while perspective api evaluates based on the negativity in language, it does not evaluate its positivity.The ramifications of this are that when it is evaluated on neutral text, a reflection of improvements may be more difficult to arise, because while the output may be adding more positive language, if there was not any negative language to remove, then perspective api shows it as no change in scores.Of course, these are by no means limitations of perspective api as a tool as they are the very limitations of using a single evaluation measure for toxicity\\u002fbias, which is undoubtedly both arbitrary, but also temporally varying, as well as culturally-bound.Our own societal measures for toxicity undergo \\u201cversion updates\\u201d over time.Furthermore, work should be pursued on developing ways of including and accounting for these nuances and variations.\",\".We use 2 public chinese corpora, jddc and edc, for post-training.Though there are diverse topics in them, it is desired to introduce other corpora from different domains and languages.Ssl tasks are arranged in post-training via cmtl based on the intuitive understanding of their semantic levels and difficulties.Therefore, to combine the power of each ssl task more effectively, new training strategies need to be explored.\",\"There are some limitations that have yet to be addressed.Since we use the predicted probability distributions of the model output as a medium for continual kd for nmt, the vocabulary of multiple models needs to be consistent.Overcoming it allows continual kd for nmt to be extended to models with different language pairs and different modalities.Also, although our approach is robust to malicious models, there are more diverse and sophisticated attacks in real-world that require more research on defense.In addition, the teacher and student models must be trained on the same language pair.Further studies can consider more general scenarios without the above limitations.There are other approaches worth exploring in order to address the transfer of knowledge from models rather than their training data besides sequential manner.For example, it is also possible to explore various distillation methods like organizing teacher models into batches or pipelines.\",\"This study reports early results and was based on a limited cohort of students from one master\\u2019s course.The generalizability of these results is therefore subject to certain limitations.Follow on studies will test the system with larger samples and different disciplines to add weight to any significance of the results.Notwithstanding the relatively limited sample, this work offers valuable insights into how a video-based student support platform that uses knowledge tracing improved graduate students\\u2019 perception of their learning experience.As the students finish their course, we will collect additional quantitative data in the form of final student grades.These will be compared between students who used the system and those who did not.We will also compare the final grades of students who used the system between the course in which they used it and other courses in which they did not.It also remains unclear what influence the course content has on the students\\u2019 experience.To examine this element further, the system should be tested on multiple courses imparted by different lecturers, and in varying subject fields.There may be an inherent positive opinion of aipowered technologies by students at an ai university.To test that hypothesis, the system should be provided to students at other universities in subject areas not related to ai.Several questions, however, remain to be answered.Further research should be undertaken to test several hypotheses, for instance, whether perceived ease of use and perceived usefulness would predict the attitude towards usage of orbits.\",\"Our task setting and baseline system requires that the input is already segmented into words including mwes.The mwe identification step in the construction process of our dataset involved timeconsuming manual annotation.our dataset can be used to evaluate such a japanese mwe identification system.this is an effect of the dense annotation setting; it results in uneven distributions of complexity as shown in figure 2, where easy words greatly outnumber difficult words.One possible solution would be creating another lcp dataset using sparse annotation, where target words are selected using frequency bands so that the words are distributed across a wide range of frequency.Our data could provide insights as to what kind of words should be targeted by sparse annotation for such a dataset.\",\"We concede that there are differences in the number of parameters between the bart models when compared to the roberta and luke counterparts.However, as per our result discussions and observations, the gains are orthogonal to the encoder used and the differences in the base models are not as significant when comparing the larger counterparts.We note that we also explored seq2seq pre-trained knowledge-enhanced models like keybart and genre, however both resulted in underwhelming performance compared to bart.Further exploration is required in improving performance for such models.We also note that while we demonstrate gains by switching to a classificationbased approach in rine, such models are limited in other generation task capabilities such as translation or summarization.We will release the data and code used for this work, but emphasize that some processing was done over the raw topv2 dataset, namely reconstructing source utterances directly from the provided target instead of using the provided source, as we encountered mismatches when constructing pointers.\",\"Although mitqa achieves the best results for texttableqa benchmarks to date, it still has some limitations, owing to its design, and the type of training data it can access.Design policy: we have designed mitqa as a collection of trainable modules, which are used in a specific sequence.This design has helped us to focus our innovations in specific modules such as multi-row training for rowretriever, multi-span training for answerextractor, etc., with an eye to boost overall accuracy.However, the modular design also means that mitqa is not fully end-to-end trainable.Therefore mitqa is, in principle, susceptible to compounding error propagation across modules.We view this as an acceptable trade-off while working on hybridqa and ott-qa, but other data sets may force us to revisit this decision.Types of queries: texttableqa, being a relatively new task, has only two major benchmarks available , where ott-qa is an open domain extension of hybridqa.Therefore, the types of queries to which mitqa during training are limited to effectively a single large benchmark.Hybridqa \\u2014 and consequently ott-qa \\u2014 corpora are similar to wikipedia articles, not confined to any specific domain.Further experiments in specific verticals, such as finance, retail, and health are needed to check if mitqa affords practical cross-domain adaptation.Moreover, only a small fraction of queries in hybridqa and ott-qa need aggregation.Due to their rareness, we have not considered handling aggregation queries through mitqa, which needs additional work in future.\",\"While the initial results are promising, the accuracy of our method remains lower than human vqa accuracy and models finetuned on the vqa datasets, which suggests that there may still be substantial progress that must be made before few-shot vqa methods with code synthesis are useful for practical real world applications.another limitation of our approach is that it relies on large capable lms, which may be restricted in use due to compute requirements or cost.\",\"While the wfst model didn\\u2019t perform very well overall, our sense is that it is worth pursuing further.Specifically, there are several moves worth exploring.First, we should move to a compiled system so that we can test the \\\"individual variable\\\" models more thoroughly.Second, we should try models where we set the variable weights by training, rather than naively in advance.Third, in an individual variable setting, it would be promising to weight the variables by locality.Specifically, do mismatched variables have more effect when they are closer to where the changes happen? Similarly, we might adjust the granularity of the variables as a function of position, with single-character variables sometimes and variable spans in other cases.Fourth, we had individual wfsts for each lemma, but with a compiled system it makes sense to put them all together into a single wfst.\",\"While this research provides valuable insights into using the gan-bert model for authorship attribution, there are also a few limitations to note.We only focused on a limited number of authors from the late 19th century, which may include shortcomings towards model generalisability.Future research should consider using the whole dataset of long 19th-century novelists to address this limitation.furthermore, incorporating a rich feature set and comparing performance among different models would be another interesting research direction.\",\"This work has a narrow focus: small-scale analysis, translation between one language pair , examining terminology around two realworld public figures ,8 in a specific newsworthy event.First, the scale of the analysis is quite small, so it does not examine in detail questions of frequency of errors, distributions of errors, or statistical significance.While this work raises issues that may be relevant for consideration across other language pairs, the relevance of the specific linguistic conventions discussed here will vary across language pairs, and certainly do not cover the full range of asymmetries in linguistically encoded information ).Due to the prescribed forms of address of the two monarchs in question, this work only examined translations related to a small subset of terms and does not examine performance on terms used related to other individuals or to other third person singular pronouns or forms of address that could be used by a monarch.The specific circumstances means that we may not expect these results to generalize to other potentially comparable scenarios.Lastly, we cannot examine the training data used for the public models, so we can only draw\",\"A major limitation of our work may be that our disentangled representation learning framework adopts some heuristic assumptions and designs in data augmentation and counterfactual data construction, and it remains to be seen whether they are applicable to other datasets and other languages.In particular, for the data augmentation of cnaa strategy, we assume that more data can be synthesized by text concatenation and we heuristically decide the quality and content label of synthesized data by some random strategies.Besides, for the counterfactual data generation, we mainly generate counterfactual samples and scores heuristically through our intuition and experience, rather than building a generation model based on counterfactual reasoning.Considering that some researchers have already developed some counterfactual data generation models for nlp tasks such as neural dialogue generation , we are interested in whether it is possible and better to build a counterfactual data generation model for our method.\",\"The approach of identifying source domains relies on having a contextual sentence but also a target domain available.The datasets available for evaluation do not always provide precise target domains.For example, the lcc dataset provides the target domain gun ownership for the sentence i just don\\u2019t know what it will take for people in this country to embrace gun safety, or the target domain climate change for the sentence the event is billed as the largest meeting of influential figures within the renewable energy field.This mismatch often makes it difficult to provide precise source domains.A similar problem also exists when wanting to use our source domain prediction approach in the wild as we have to somehow provide the model with a target domain.While we can provide a target domain by selecting sentences based on seed-word lists designed for specific domains, we do not know how precisely this matches the target domain occurring in the sentence.In a multilingual setting, the issue becomes more pressing since there are very few multilingual metaphor datasets and for semiautomated approaches the seed-word lists would have to be provided for each language.Another challenge is connected to the fact that the model output requires time-consuming manual evaluation to obtain a precise accuracy score.However, deciding what counts as a correct source domain can be difficult and might change depending on how strictly the annotators apply certain rules.For instance, whether an annotator sees a pre- dicted source domain as too general or too specific is a matter of degree.Overall, this makes it hard to benchmark different approaches across papers, which is why further investigation of automated metrics, as presented in this paper, is crucial.Lastly, there are issues regarding the accessibility of large neural language models, such as gpt-3, and the transparency of openai\\u2019s api as described in the\",\"Multijugate dual learning improves the model\\u2019s performance in tod tasks in low-resource scenarios, but the introduction of the dual training objects increases the required graphics memory and training steps.In addition, the rephrasing mechanism necessitates an additional paraphraser to rewrite the training samples; hence, the number of training samples increases according to the number of paraphrases.Despite this, we find that the higher training cost associated with multijugate dual learning is preferable to employing a large quantity of dialogue data for further pre-training or manually labeling data.Considered from a different angle, the scenario described above presents possibilities for future research, such as the development of higher-quality rephrasing algorithms to filter the augmented text.In the meantime, multijugate dual learning is a learning objective between structured and unstructured texts.Therefore it may be extended to any task involving heterogeneous data, such as generative information extraction, and data-to-set generation.\",\"We report the following limitations for the text2text-based distractor generator : \\u2022 the text2text-based generator still suffers from the concern of generating distractor same as answer or previous generated distractor.In fact, generating repeated incoherent or factual inconsistent results are commonly concerns for neural text generators.Although the concern is mitigated through the candidate augmentation strategy, there still are certain portions of generating the distractor of those types, as can be seen in table 5.\\u2022 although the cgr-based methods show their disadvantage in the evaluation, we find that cgr-based method might be a more practical one for facilitating the cloze-style mcq preparation.The cgr-based method is able to generate ten or more candidates for educators to select, while the text2text generators are only capable of generating three or four distractors.\",\"Intapt adopts a prompt tuning method which utilizes the inherent information from pre-trained models that already shows good asr performance on l1 english speakers.Therefore, in order to apply our method we need a pre-trained model that already has good performance on a specific task which might not be available for other langauges.Also, our method might potentially need sufficiently large pre-trained model size in order for prompt to utilize the internal information of the model.\",\"Our approach is based on the assumption of a limited data budget, and the observation that general multi-task training may not be the most efficient method when one cares about single target tasks.As such, deft is not applicable to \\u201ctrue\\u201d zero-shot settings where one has no information about the target task, since it relies on the existence of at least some unlabelled examples.Furthermore, for some tasks it may be possible to cheaply gather many examples for finetuning beyond the point where deft is useful.In some cases, gathering unlabelled examples may not be so much cheaper than gathering labelled examples that it is worth considering whether to gather unlabelled or labelled examples.Additionally, the recent rise of sparse dataset.Questions were written after reading paper abstracts, and evidence selection required reading entire papers.Mixture-of-expert models may reduce the negative interference effect observed throughout our work, where deft models often outperform models trained on all multitask data and random subsets of the multitask data.Finally, we note that in pilot experiments we found that task diversity was a key element of strong held-out task performance.\",\"There are three predictable limitations in the developed embedtextnet.First, while we have performed a thorough evaluation of embedtextnet on various downstream tasks, it is still a generalpurpose approach and its effectiveness on specific tasks or in specific domains may vary.Thus, further research is needed to fully understand its capabilities and limitations in different contexts.Second, as mentioned, embedtextnet is most suitable for scenarios where the embedding is saved during inference, such as text retrieval or similarity measurement when the fixed embedding is saved with a vocabulary.However, it may not be as effective in scenarios where the embedding needs to be decoded back to its original form, such as text generation.Third, the effectiveness of embedtextnet is evident on a large embedding dimension, and it may decrease when working with a small embedding dimension even if it was still better than other sota in our experiments.This limitation is due to the fact that embedtextnet is based on a vae architecture, which is known to perform better on high-dimensional data.Therefore, it is better to compare the performances of embedtextnet with other sota and choose the right one according to the researchers\\u2019 usage.\",\"Our proposed method maner for improving ner is best suited for low-resource settings.3, we measured the effectiveness of maner in situations where more training data is available and found that maner boosts f1 performance over the sner baseline until about 400 training examples, and then both methods perform similarly.The result demonstrated that maner is best suited for extreme low-resource languages and rapid prototyping because it is easy and cost-effective to obtain very few human annotations to achieve significant performance improvements.We base the experiments in this paper on a widely adopted model, xlm-roberta, pretrained on multiple languages.It is possible that the empirical conclusions we draw from the observations do not generalize to other pre-trained models.\",\"Robustness to perturbations our empirical study does not explore the connection between the discriminative power of automatic metrics based on the proposed metric preference checklist and their robustness to simple perturbations or other natural language phenomena that may occur in texts or nlg use cases.Metric fairness our study does not include an investigation of metric fairness or social bias issues that may be introduced by language model-based nlg evaluation metrics.multi-aspect our current empirical experiments mainly explore the discriminative power of evaluation metrics in single-aspect experiment setup.It may also be interesting to inspect to what extend the metrics can identify multi-aspect levels of quality, particularly when there exists disagreement between human evaluation aspects.For example, instead of disjointly splitting samples into {low engagingness, moderate engagingness, high coherence}, samples can be divided based on the joint aspects, such as {low engagingness and low coherence}.Universal input-output structure our experiments are mainly carried on publicly available author-annotated human evaluation benchmark datasets.Thus, we do not guarantee the universal input-output structure and a uniform naming system across datasets or tasks.For example, unieval - topical chat data and usr - topical chat use a different naming system for human evaluation aspects, yet the aspects refer to the same dimension of human-like qualities.Dependency of nlg systems when comparing outputs from two different nlg systems, the systems are presumably independent.However, in many nlg use cases, this assumption is not fully accurate.For example, in controlled generation task, the systems originate from one pretrained language model as an encoder model.In inference or decoding stage, the encoder\\u2019s probability outputs are used as inputs for multiple decoding schemes, such as the use of log-likelihood ranking, distance scoring as filter, etc , yielding n-systems to compare with.As a result of this setup, the generation outputs from these n-systems are often less diverse and less distinguishable than the outputs from two independent systems that do not share the same encoding scheme or training objective.\",\"One drawback of the experiments presented here is the reliance on a constructed language.While we have tried to design a language that is as representative of natural language as possible, there may be additional statistical effects that are not taken into account.For example, it is unlikely that one language would capture all 29 phenomena presented here and that the process would be triggered enough times to produce a large enough corpus.How these findings extended to existing language corpora is an open question for future studies.\",\"There are a few limitations we would like to address.First of all, the number of clusters needs manual configuration.This is a limitation of the clustering algorithms since we need to set a threshold for convergence, which consequentially pinpoints k.an expedient alternative is to analyse the dataset for the realistic settings or probe into k for the optimal setup, which is, however, beyond the scope of this paper.Another limitation is the pre-requisite for millions of unannotated data.The autoencoder needs enormous data to learn bottleneck representations.Its performance would be hindered without access to abundant corpora.Lastly, the performance of the acquired clustering-friendly representations depends on the similarity metric chosen.Efforts need to be made to find the best option, whether it is euclidean distance or cosine similarity etc.\",\"The work presented here has a few limitations: the proposed model belongs to the memory-based methods for continual learning, which requires a memory that costs extra storage.In some extremely storage-sensitive cases, there may be restrictions on the usage of our model.The proposed model has currently been evaluated under the re setting.It is better to transfer it to other continual few-shot learning settings for a comprehensive study.\",\"Our experiments focus on the t5-base and t5-large models as these are widely used, representative pretrained seq2seq models.However, there are other pre-trained seq2seq models such as bart that we did not experiment with.It would also be interesting to experiment with pre-trained models with more layers such as t5-3b and t5-11b.We have not conducted these experiments due to resource constraints.\",\"Fuzzers are designed to reach deep and complex control flow in large software.Many programs for current ai for code datasets do not have complex control flow.As a result, afl++ can quickly cover all program branches before generating many inputs for us to feed to the model.afl++ uses branch coverage to track fuzzing progress.Although it works well on c\\u002fc++ programs, it may be ineffective on languages with exceptions, which are implicit control flow.For example, afl++ cannot distinguish different exceptions thrown in the same block, which sometimes leads to low coverage in python programs.To overcome this issue, one possible way is to change from branch coverage to line coverage.Although our current implementation requires a fuzzer, our approach can also work on tasks with only functions or code snippets as long as we can acquire adequate input\\u002foutput pairs of the functions or code snippets, which may have some engineering challenges but is not infeasible.For example, in recent years, the software engineering community has proposed various ways to fuzz bare functions.\",\"The main concern regarding our model is the computational complexity.Higher-order mfvi has a complexity of o, which admits fully parallel computation and thus is fast on gpus.The complexity of structured inference of treecrf is also o.however, due to the dynamic programming computation restriction, only o out of o can be computed in parallel using parallel parsing techniques , slowing down the running speed.Besides, differentiating through the treecrf marginals needs many gpu memories , as automatic differentiation saves all intermediate dynamic programming items for back-propagation, which cause plenty of waste of gpu memories.In this work, since the memory problem is not too severe, we use automatic differentiation for simplicity.One solution is to manually implement the outside algorithm to mitigate the memory problem.\",\".Our work assumes that input sql queries to our model are always wrong.This assumption is more feasible in an interactive semantic parsing framework, where the users are expected to decide whether a sql parse, accompanied by its natural language explanations , has errors or not.experiments with more language models of code.We have only experimented with two language models of code, coditt5 and codet5, both using t5-base as their underlying model architecture.It would be interesting to test how our\",\"There are three limitations on our method.First, we did not verify our method on more generic tasks, such as text classification, yet it is not limited to commonsense qa.second, our method requires a longer training time and a larger gpu memory since the knns require forward and backward propagation additionally.Third, we do not consider the ambiguity of gold answers, which may affect the quality of knns.For example, \\u201capple\\u201d may refer to a kind of fruit or a technology company.\",\"The dataset used in this work is a personal narrative corpus in english collected in-vitro.Further work will be needed to extend it to other languages, genres, and naturalistic conditions.The reproducibility of the annotation task may be subject to variability due to the fact that the task is done by five internal annotators and not through crowd-sourcing techniques.\",\".Our experiments are limited to single-sentence tasks, as we only retrieve single-sentence nearest neighbors to a test input.extensions to multi-choice tasks or generation tasks requires going beyond a fixed set of options shared between inputs in the demonstrations and the test input.for instance, this paper uses manually chosen synonym labels.\",\"The limitations of this paper include the absence of experiments on large language models.Previous studies have shown that using high-capacity pre-trained language models can significantly improve the accuracy of answers but also entails an increase in computational overhead.Due to limitations of computational resources, this paper employs a low-capacity t5 model for experiments.Our experiments have suggested that the proposed iterative prompting method that works with the low-capacity model can achieve comparable results with baseline methods equipping with large models.recent research on large language models has shown that they can learn from few examples and reason well.We believe that it is worth exploring ways to enhance the prompting of llms to improve their completeness when responding to ambiguous questions and reduce model hallucination in generation.low-resources in our study are characterized by limited multi-answer-qa annotations, which aims to examine how data size impacts model performance.Other low-resource languages may behave differently with less training data and large models.Besides, we would like to explore more effective prompting methods, such as chain-of-thought prompting.\",\"We provide a comprehensive study on the efficacy of leveraging pre-trained language models for zeroshot ood detection.Our method is thus limited to the setting of abstaining from prediction on all ood data.This is more conservative than selective prediction, where the model must make predictions over as many id ood points as possible while maintaining high accuracy.Despite this, ood detection has lower risks to high-risk and safety-critical applications, where rare and anomalous data is more reasonably flagged to the expert.We believe our work provides new values and insights to the research community, especially on safe handling of distributional shifts when deploying pre-trained language models.As discussed in our\",\"Linguistic variation our results are highly dependent on the target language and its morphology.For example, word boundaries might seem like an obvious choice for dynamic segmentation, and in fact they achieve the best performance in english and vietnamese.However, for some languages like agglutinative finnish, whitespaces are less frequent, which is detrimental to model performance.Explicit word boundaries are not available for all scripts.For example, in chinese characters, or in modalities other than text like speech or vision, there is no obvious equivalent to whitespaces.However, segmentation based on stochastic re-parameterisation, subword tokenizers and spikes in conditional entropy overcomes these limitations.Contiguous segments in its current formulation, dynamic pooling only allows for merging contiguous segments of tokens in a sequence.However, this is not ideal for morphology types like hebrew where morphemes are discontinuous: vowels are interspersed between consonant roots for inflection.In this case, discontinuous segments may be necessary to handle non-projective syntactic dependencies.Independent boundary decisions the decision to emit a boundary at time step t depends on previous boundaries only indirectly through the hidden representation of the first transformer block, as this preserves the efficiency of the boundary predictor.Instead, a recurrent model could be explicitly conditioned on previous boundary decisions, which however would negatively affect the time complexity of the language model.Work contribution of authors the idea of training the models with pooling of variable-length segments was discussed among the authors while jan chorowski was at the university of wroc\\u0142aw.Experiments were performed by piotr nawrot while he was employed in a research grant at the university of wroc\\u0142aw, under the supervision of adrian \\u0142an\\u0301cucki and edoardo m.The manuscript was written by piotr nawrot, adrian \\u0142an\\u0301cucki and edoardo m.\",\"Limitation of our method is the limited utilized knowledge.Since our prompt tuning-based method tests on implicit discourse relation recognition task, the elicited knowledge only comes from the dataset of this task and the model pre-training corpora.This constraint restricts the capability owing to the reporting bias in the pre-training models.Moreover, the relatively few training data of several second-level classes resulting from the highly skewed label distribution problem requires extensive knowledge to make the model understand data instances and the task.Although we impose the prior human knowledge against the idrr task from the input template designing to the discourse connectives selection, the knowledge source still only comes from our prior knowledge and the elicited knowledge of plms.As a result, even our method obtains a valid score in all second-level classes except the cont.pragmatic cause displayed in table 6, some second-level senses, which are the same as previous studies, cannot receive a satisfactory performance.for example, grounding the arguments pair on the relevant nodes of the knowledge graph for each data instance or knowledge distillation from large language models to provides more contextual information and enhances the capability of the model on this task.Limited predicted connectives another area for improvement is the prediction of extensive connectives.Although our model includes the preselected connectives as our third layer of a designed hierarchy tree, we do not include the ground truth of connectives as our third layer.Because including these extensive connectives to form many leaves will result in many paths.\",\"Multilingual language models such as xlm-r and gigabert are typically pre-trained on large amounts of unlabeled text crawled from the web.Since these models are optimized to capture the statistical properties of the training data, they tend to pick up on and amplify social stereotypes present in the data.Since our coreference resolution models use such pre-trained language models, they may also exhibit social biases present on the web.Identifying and mitigating social biases in neural models is an active area of research.Furthermore, while our proposed methods are highly effective, the performance of our best ensembles is still far from perfect.On ontonotes arabic, our best system only achieves an f1 score of 66.Such performance may not be acceptable for some downstream tasks.Finally, even though wikipedia is available in more than 300 languages, there are still very few wikipedia pages for some very rare languages.Our proposed methods are likely to be less effective for such rare languages.\",\"Despite the state-of-the-art performances, our proposed methods still have some limitations for future directions.Firstly, multi-view prompting creates overheads of training and inference proportional to the number of views.For efficiency in practice, according to figure 3, mvp with a relatively small number of views behaves decently.Secondly, we apply a simple yet effective aggregation strategy to combine the results of multiple views.lastly, experiments only verified the consistent improvement on absa tasks, while intuitively, the idea of mvp that leverages multiple views can be expanded to any structure prediction tasks, such as information extraction, emotion-cause pair extraction, and stance detection.\",\".Therefore when applying our approach to other domains it is necessary to prepare at least a few annotations.further improve- ment should be made to solve this problem for practical use.\",\"The coreference annotations of the moviecoref dataset exclude plural character mentions because the annotation guidelines did not cover them.our model only identifies singular characters and cannot retrieve singleton clusters.All the movies in the dataset have a linear narrative.Non-linear stories can confuse a coreference model because of time skips and flashbacks which is not explored in our work.Both our inference approaches require at least 10 gb of gpu memory for finding coreference clusters from full-length screenplays.\",\"A limitation of this study is that it is based solely on papers published in the acl anthology, which primarily represents the international englishlanguage nlp conference community.While the acl anthology is a reputable source of nlp research, it should be acknowledged that a significant amount of research is also published in other venues such as aaai, iclr, icml, and additionally, there are also vibrant local nlp communities and venues, often publishing in non-english languages, that are not represented in the acl anthology.As a result, the conclusions drawn from our experiments may not fully capture the global landscape of nlp research and further work is needed to explore the diversity of sub-communities and venues across the world.This work focuses on the aggregate trends of citing older work in nlp, but does not investigate the reasons for lower citation of certain older papers.There may be various factors that contribute to this, such as the accessibility to these older papers, the large number of recent papers, the applicability of these old works, and the technical relevance of the older work.Determining the relative impact of each reason is a challenging task.Therefore, more research is needed to fully understand the underlying mechanisms that influence the citation of older nlp papers.This study aims to investigate the factors that contribute to the citation of older works in the field of nlp.We have analyzed different factors such as the mean age of citation, diversity in the age of citations, venue of publication, and subfield of research.Our results indicate that these factors are associated with the citation of older works, but it should be noted that these associations do not establish any causal relationship between them.Lastly, it is important to note that citations can be heterogeneous and can be categorized in different ways.For example, some classifications of citations include background, method, and result citations.However, certain citations may be more important than others, as shown by previous research such as \\\"identifying meaningful citations\\\" by.\",\"Even the premise of parsing questions to wikidata queries leads to linguistic and cultural bias, as wikidata is biased towards english-speaking cultures.argue, speakers of other languages may care about entities and relations that are not represented in englishcentric data.For this reason and for the linguistic reasons we demonstrated in this paper, creating cg benchmarks natively in typologically diverse languages is essential for multilingual information access and its evaluation.2, our translation system fails to deal with ambiguities beyond grammar and thus generates wrong translations for a few samples.Moreover, although the dataset can be potentially augmented with low-resource languages and in general other languages through the translation framework, adequate knowledge will be required to expand rules for the specific target languages.With limited computational resources, we are not able to further investigate the impact of parameters and model sizes of multilingual plm as our preliminary results show significant performance gaps between plms.Broader impact a general concern regarding language resource and data collection is the potential bias that may occur when annotators lack representativeness.Our released data largely avoid such issue due to the synthetic and cultural-invariant questions based on knowledge base.Assessment by native speakers ensures its grammatical correction.However, we are aware that bias may still exist occasionally.For this purpose, we release the toolkit and grammar used for generation, which allows further investigation and potentially generating branches for other languages, especially low-resource ones., a climate performance model card for mt5-small is reported in table 7.By providing access to the pre-trained models, we aim to support future endeavors while minimizing the need for redundant training efforts.\",\".each result is for a different type of mask and hence for a different test set.Our results cannot be used to gauge performance for unmasked or differently masked inputs that can be expected in applications.Rationale evaluation with roar to evaluate rationales, roar uses the performance of a model trained and tested with input masked according to the rationales.The reported numbers therefore do not only reflect the quality of the rationales but also the difficulty of the task, the size of the training data and the performance of the machine learning method.Furthermore, the measurements are influenced by randomness in training as the masking of training data changes the path of the optimisation process.13 the values have to be seen relative to baseline performance of full and rrand and in comparison to different types of rationales.Domains the experiments are restricted to the two domains of the dataset, namely restaurant and laptop reviews, with just 28 aspect entity types and 14 attribute labels.We encoded these in a shared vocabulary with the review sentences.14 we did not explore alternatives such as using reserved embedding table entries to encode the domain and aspect categories or using more natural question templates, e.adding function words where appropriate and lower-casing the categories.Performance differences may change for other domains, number of aspect categories and the ratio of the training size of smallest domain and largest domain.Task the absa task assumes that the aspect category is already marked in the input and labelled with entity type and aspect category.Number of test scores on first sight, the high number of test scores could be a concern as testing many models on test data can lead to overfitting to test data.However, only the result for the full setting is a vanilla test set result.All remaining results are testing on derived test sets matching the masking applied to the training data.Therefore, these results do not leak performance information for building better classifiers on the test data.Using the test set here is convenient as the data set does not come with a validation set and the validation set we held out from the training data for selecting the training epoch is very small.Language experiments are for english only due to availability of sea data.Various factors may cause different patterns for other languages, e.bert subword units, evaluation excluding function words vs.languages that use mostly morphology instead, freer word order may result in annotators producing more discontinuous ses.13our reporting of averages over twelve runs, and in some cases 24 runs, compensates for the latter effect as each run shuffles the order of the training data and uses a new random initialisation of the classification head.14for the aspect entity type and attribute label, sharing is reduced by using capital letters.\",\"Our work is base on the existing sequence-tosequence ner model, since its way of decoding has been shown effective for knowledge transfer between different classes.However, it might also be valuable to consider other token-classification-based or crf-based ner models.Especially, it would be interesting to employ the existing crf-based distillation method to cope with the problem of heterogeneous tag sets for ner.\",\"As in-context learning with llm heavily depends on the selected exemplars in the prompt, the performance of kb-binder might vary from different subsets of randomly sampled examples, especially in a low-shot setting.But kb-binder still shows strong performance on thousands of data points on each testing dataset with randomly sampled exemplars, which verifies the robustness of our method to a degree.In the meantime, the performance of kb-binder is restricted with the one-time generated drafts from the perspective of the imaginary frame and schema items of the preliminary logical forms, which can be further improved with interactively generation and retrieval.Moreover, we have not explored whether the performance can be further improved with explanation\\u002finstruction during the stage of draft generation.\",\".Firstly, in the step of answer mapping , we only select those connectives that are tokenized with a single token as answer words, since most masked plms predict only a single word.Therefore, those connectives tokenized with multiple tokens will be replaced by the most frequent answer word with the same subtype-level sense tags.We believe that this approach will filter out several meaningful connectives as answer words.1, we can observe that multi-prompt ensembling is effective for fusing multiple single-prompts for implicit discourse relation recognition.in this way, we can take advantage of the different prompt templates.\",\"In this paper, we explore incorporating multiple constraints to simile generation and attempt to interpret the simile comparisons from the aspect of cognitive linguistics.However, the creativity of simile is one kind of subjective feeling and is difficult to be accurately judged, which is also a big challenge for other kinds of creative writing tasks.We hope this task and dataset could provide novel insight into user-oriented text generation, and give the interactive and collaborative generation a closer and more detailed exploration.\",\"The original person entity starting at \\\"john\\\" is only partially matched, and a new person entity starting at \\\"kennedy\\\" is introduced in the asr output.Consider another gold annotation of the following transcript: \\\"secondb-date quarteri-date twentyi-date twentyi-date,\\\" which the ner model tags as follows: \\\"secondb-date quarteri-date twentyb-cardinal twentyi-cardinal\\\".Again, how should this scenario be scored by an accuracy metric?\",\".Currently, the hidden layer of the two neural networks for drift and diffusion are shared between all entities and relations.This might cause over-fitting on relations that show simple structures if the neural networks are set to be very deep.On the other hand, if the neural networks are set to be shallow it might negatively influence modeling complex relations as the direction of trajectories will be limited.One possible solution is to cluster relations based on complexities and use separate neural networks for each cluster depending on the complexity of the corresponding relation.\",\"We present the first study of generating questions for filling in information gaps.Our method is limited in several ways.First, it focuses on information that is explicitly missing, and does not discuss information that is inaccurate or incomplete in other ways.Second, it only asks one follow-up question and does not address multi-turn dialogue about a student answer, or multiple student answers.Finally, our approach makes somewhat restricted use of the student answer, and it will be better to generate questions that directly uptake information from the student text.\",\"We only analyze four high-resource languages in this study, our analysis could have benefited from more languages, especially low-resource ones.Additionally, we only analyze japanese and english pride\\u002fshame as a known cultural difference; analyzing other differences could provide stronger results.We perform a small user study, and our work could have benefited from a larger-scale study with more annotators and completions analyzed.We recognize the added complexity of investigating pride embeddings from a culture where explicit expressions of pride are discouraged; we note this may be a contributing factor to our results indicating that lms do not reflect the culturally appropriate nuances of shame and pride.Additionally, we acknowledge that the experiments outlined in this paper are specific to investigating cultural awareness from the lens of emotion.These experiments are not easily applicable to measuring cultural awareness from different perspectives; therefore, results may not be generalizable.At a higher level, we equate language with culture.Psychologists have observed higher cultural similarities within languages than between them , however, we recognize there are variations within the populations that speak each language.For example, spanish is spoken by people in spain, mexico, and other countries, each having a unique and varied culture.\",\"The prefixes we use are semantically independent from the test sentences, and also semantically implausible when chained together.This is the opposite of what we typically expect in natural language, where sentences follow from some pragmatically licit prior context.While our findings are theoretically relevant to any nlp task that leverages natural language inputs, we may see qualitatively different trends in more naturalistic settings.Our results are currently limited to english.Certain languages have grammatical features that could strongly impact on language models\\u2019 acceptability judgments, and this could affect the trends we have observed.\",\"The models chosen in this work are selected to represent the state-of-the-art at the time the work was conducted, and in some cases omit weaker models.For example, our exemplar selection experiments do not cover those llms trained with vanilla language models objectives, namely opt and davinci, as we find their performance substantially lags code-davinci-002 and text-davinci-002.For the same reason, we only consider the substantially large language models, omitting llms of smaller scales.Running experiments using smaller lms or vanilla lms may provide insights into how scale or instruction finetuning impacts the ability of lms in learning from explanations, but our investigation mainly focus on selecting exemplars to achieve the best in-context learning performance with state-of-the-art models.In addition, certain aspects of our approach are computationally intensive, particularly using lmbased similarity scores.However, we think this is still feasible in practice: if practitioners are deploying a real-world system, investing more computation upfront to improve its performance is likely in reach for those deploying llms in practice.Finally, our experiments consider a certain subset of nlp reasoning tasks written in english.While we believe the results here transfer to other tasks in this vein which have been frequently used to evaluate llms, it is unknown how well they handle other languages, dialects, or genres of text such as social media data.\",\"Recode benchmark has several limitations: it contains perturbed datasets based on humaneval and mbpp which focuses on python function completion use cases.Therefore, we only perform evaluation on python language and not be able to capture robustness in a wide variety of code completion use cases.However, our transformations are generalizable and could be easily extended to other languages and also other coderelated datasets.We encourage researchers to apply and extend recode benchmark to additional languages and other coderelated tasks; recode benchmark is designed for robustness evaluation and cannot mitigate the lack of robustness.Given that our benchmark can be used to generate comprehensive collection of perturbed data, we believe that it can be used for training data augmentation to enhance model robustness.\",\"Although deer has shown excellent performance on multiple datasets and tasks, we still found some limitations affecting its usability and efficiency: the latent alignment model cannot deal with the multi-modality problem in the largescale dataset, which also leads deer to underfitting the multiple latent alignment targets that need to be aligned.Although deer does not need to perform length prediction, it relies on the assumption that the input length is large than the output, which causes the model to lose flexibility in length control.We compared sequence-tosequence models such as bart and prophetnet in the experimental part of this work.In fact, bart only through six layers on each forward pass, while the bert family model needs to go through 12 layers, leading the inefficient inference due to latency accumulation of multiple iteration steps.\",\".Firstly, since the cumbersome data annotation leads to few publicly available datasets of idrr tasks, we only conduct experiments on english corpora including pdtb 2.secondly, considering that instances of pdtb are contained in paragraphs of the wall street journal articles, our approach ignores wider paragraphlevel contexts beyond the two discourse arguments.As shown in , positioning discourse arguments in their wider context of a paragraph may further benefit implicit discourse relation recognition.It is worth exploring how to effectively build wider-context-informed discourse relation representations and capture the overall discourse structure from the paragraph level.\",\"Our suggested approaches have two primary practical limitations: first, weighted sampling is restricted to languages with available running text sources for extracting frequencies.A project on extremely low-resource languages may be restricted to uniform and overlapaware sampling.Second, as the number of seeds increases, so do requirements for training time and\\u002for computing power.A shared task, for example, might limit itself to only a few seeds in order to assure on-time submissions.notably, this work reproduces the effect observed in the sigmorphon 2022 shared task , which found a substantial performance hit for featsnovel relative to featsattested, but not lemmanovel relative to lemmaattested., which reports a 95% performance hit on lemmanovel vs.\",\"In this work, we present pgra to retrieve taskspecific context evidence to support nki tasks.firstly, we have not experimented with our pgra on sentence-pair tasks, such as mrpc , in which the model needs to infer the relationship between two sentences.Retrieving two sentences from an external datastore is non-trivial as there are hardly sentence pairs in the wikipedia datastore.A larger corpus with more diverse data sources may help in this case.Secondly, we restrict our pgra to classification tasks but not generation tasks.Similar to sentence-pair tasks, retrieving sentences that may help the model generate text is more complex.For example, data related to both the source and the target may help in machine translation.this restricts the generality of our methods.Solving ki tasks depends on knowledge in the passage-level external datastore while matching such information needs possibly more specialized prompts for our method.\",\"Due to the lack of research in this area, there is only one direct related paper to our work, which serves as the main baseline in our experiments.also, the domain adaptation problem not only exits in the machine translation filed, but also various generation and understanding nlp tasks, where we should evaluate our method on if we are not limited by time and resource.\",\"Even though our method is an excellent alternative to the current amr aligner system, which is standard and task-agnostic, we notice some drawbacks when moving to other autoregressive models or languages: model in this work, we studied how cross attention layers retain alignment information between input and output tokens in auto-regressive models.1, we examined which layers in state-of-the-art amr parser models based on bart-large best preserve this information.Unfortunately, we cannot guarantee that these layers are optimal for other auto-regressive models, and so on.As a result, an examination of cross-attention across multiple models should be done before developing the cross-lingual application of this approach.Sentence segmentation it is necessary to apply leamr\\u2019s spam segmentation technique to produce the alignment in leamr format.Therefore we cannot extract the leamr alignments in a cross-lingual amr parsing because we lack a segmentation procedure.However, although leamr alignment has this constraint, isi alignment does not require any initial sentence segmentation and may thus be utilized cross-lingually.\",\"The annotation of attribute categorization and subjective preferences may vary from person to person, influencing preference disambiguation results in the real world.We have tried to reduce bias by choosing categorization concepts and subjective preferences agreed upon by more than three annotators.Besides, owing to time and funds constraints, we only manually paraphrase dialog flow in english.For this reason, the agent built on sure can just communicate in english.To overcome this limitation, we plan to annotate sure in multilanguage in the next stage.\",\".Besides, the simulated lexical constraint dictionary, which is extracted from the parallel sentences of the training set based on automatic word alignment, may be different from the real lexical constraint dictionary provided by users.\",\"The methodology and experimental approach presented in this paper have certain limitations concerning their practical application and the availability of language resources.The proposed method estimates uncertainty using monte carlo dropout with k iterations and subsequently performing adaptation j times.These additional computations result in increased inference time in real-world applications.Empirical evidence suggests that larger values of j lead to a linear increase in time costs in practical scenarios.Although the number of j on the wmt21 benchmark was limited in our experiments, the exact cost associated with achieving successful adaptation for new models or datasets remains uncertain.In terms of language resources, the majority of mt metric benchmarks still focus on the news domain, leaving a dearth of multidomain mqm benchmarks for conducting more meta-evaluation experiments during the preparation of this paper.furthermore, as highlighted by the reviewer, it is also important to note that the proposed methodology does not consistently exhibit performance improvements on certain specific test sets.One possible explanation for this observation could be attributed to our investigation of the optimal learning rate using the wmt20 dataset.The divergence in scoring perspectives between the conventional wmt score and the mqm score might lead to discrepancies in improvement trends.\",\"The data augmentation method mentioned in the paper requires the augmentation dataset to be of a similar structure to the task dataset, which is not the case for meq-sum.As a result, the data augmentation experiments do not provide significant improvement in performance.\",\"Currently, our model only exploits the direct relations between nodes in the amr graph.In other words, only one-hop neighborhoods can be considered.However, there are a few cases where an opinion word and a related aspect word can be in a k-hop neighborhood.another limitation is that the errors of the pre-trained amr parsers and amr alignment models are propagated to the model as a whole.What is required is to improve the performance of those modules.\",\"Further research is needed to understand the robustness of our over-parameterization framework properly.The results given in this study are constrained by the natural language processing tasks and datasets used for evaluation.Even though we employ standard classifications from the literature, the choice of downstream tasks and datasets is still subjective.Furthermore, due to computing limitations, we could not investigate the scaling behavior of the large plms.Additional study is needed in this area.In addition, as our approach is based on plms that may learn biased information from pretrained corpus, a potential risk is that our approach may also be affected by it and generates improper texts.\",\"There are some limitations to our approach.Firstly, we utilized the bart model, which has a maximum sequence length of 1024 characters.additionally, we worked on google colab, which has a time and memory limit, and we encountered occasional connection issues, which could slow down our progress.\",\"In this paper, we only focus on whether or not there is a causal relationship between the given events, does not discriminate the specific cause\\u002feffect event.In addition, we only conduct research on sentence-level eci, whereas document-level eci often present more challenges.These are the focus of our future research.\",\"In this work, we present a general masking scheme for multilingual mlm pre-training on multiple monolingual corpora.Experiments show that our method can work for similar languages and dissimilar languages.However, we only experiment with dissimilar language ne.More experiments are required for dissimilar and distant languages.When computing [c]x for more than 3 languages, to avoid cross-lingual bias, we adapt our method to a pivoting-based framework, using en as a pivot or anchor point.Although we show this framework can work for cross-lingual classification tasks, this could be a potential problem for further adaptation to other multilingual tasks, which requires further experiments.Intuitively, we can compute [c]x in random languages instead of only in en with a balanced sample strategy.Our method provides a general framework to leverage cross-lingual prototypes for multilingual mlm pre-training, but the scope of the study is limited.We believe there are some other solutions.For instance, we can leverage linguistic varieties for masking, but the question is how to obtain linguistic varieties without using parallel corpora.Perhaps, we can consider word frequencies because zipf\\u2019s law indicates that words appear with different frequencies, and one may suggest similar meaning words appear with relatively similar frequencies in a pair of languages.Most importantly, solutions should further consider morphological variations, since in this paper we prove morphological variations are significantly beneficial.\",\"As shown in the ablation studies, using the boundary shift loss without the base model for scope prediction leads to a huge negative impact on the performance.That is, bsl strongly relies on the assumption that the proposed candidate spans are, to some extent, being an accurate estimation of the target spans.The experiment of using bsl solely could be seen as an extreme case, that no candidate spans are proposed at all.For our task, bsl could benefit from the strong base model.For the case of noisy datasets or on a more challenging task, where a base model could not generalize to a reasonably good coarse span proposal, the benefit of bsl might be limited.\",\".Our proposed model mainly focuses on the sentencelevel procedural graph construction.The scenario that two actions in the same sentence cannot be considered in our proposed model.It is challenging to handle multi-grained dependencies between actions.\",\".On the other hand, we heuristically filter pseudo-labeled texts by the top-k selection, which may also be improved with more systematical approach.\",\"The main limitation of the proposed approach is that it would be relatively costly to apply at production time, compared to the conventional lm evaluation.First, it requires drawing a number of tokenization samples, as defined by importance sampling, in contrast to a single pass through the evaluated sequence in the conventional approach.Second, the conventional approach can be conducted with teacher forcing and efficiently parallelized, while the proposed approach relies on block-byblock sequential processing.Nonetheless, the proposed algorithm is designed for analysis purposes rather than to be used in production systems, for which it is feasible to run it in a reasonable time, allowing users to evaluate the effect of marginalization for any tokenizer and language.Broader impact as the work is dedicated to evaluating existing models on publicly available datasets, we are not aware of any potential negative impact.\",\"Limitation of detoxified language modeling, which cannot be avoided unless the provided prompts are rephrased into non-toxic prompts while maintaining their semantic meaning.In addition to developing a safe lms, it is essential to address the issue of lm hallucination, which refers to the generation of factually incorrect texts.While our paper does not focus on this aspect, ensuring both safety and factual valid generation of texts is vital for real-world applications of lms.\",\"Although our method scn achieves state-of-theart performance in the cll-id task, there is still a performance gap between scn and the upper bound.This result is inconsistent with human behaviors because humans usually do not forget old skills when learning new skills.\",\"Our proposed work is dedicated to considering the noise in ds-ner, and our noise-specific analyses are all based on this task.Therefore, if it were not for ds-ner task, our model would not necessarily be robust compared to other task-specific methods.Also, our approach is based entirely on previous experimental settings in ds-ner, so we do not consider how to reduce noise from the distant supervision process, e., designing models to help the annotation process rather than learning to reduce noise from the distantly-supervised text.Designing models to help the distant supervision process could be a direction for future study.\",\"The present work only points out problems of existing research and presents no final solutions.We also simplify the assumption that an automated metric validated for a specific tst task generalizes to other tasks.However, this is problematic since there is, to our knowledge, no investigation of whether a validation on one task generalizes.This concern is motivated by the fundamental differences in how different tst tasks are defined.There are several different definitions, such as datadriven tst and linguistically motivated tst.Also, we consider only tst papers and focus on top-tier nlp and ai venues.\",\"This work is motivated by the intuition that a mention is more likely to refer to an entity that occurs shortly earlier and refers to a high frequency entity but has not recently used.For the latter pattern, we show examples of topic switching to explain why these phenomena happened, but we have not found a rigorous linguistic explanation to support this finding.We provide empirical results on four benchmarks plus a book.However, the scarcity of long-doc cr benchmarks hinders us from verifying on a larger scale.Our major contribution is a new cache design, but we also find the cache design becomes less matter when using a huge cache.Nvidia a100 has 80g memory, which means it can handle a document of 100,000 words with a conventional cr model.As the gpu becomes larger and cheaper, the importance of studying cache design is weaker.\",\".Our approach has achieved impressive results on multiple natural language processing tasks, including the glge benchmark and three machine translation datasets.Furthermore, we have observed that the issue of length prediction consistently limits the performance of the model, especially when dealing with raw datasets.The model struggles to accurately determine the length of the target data, which somewhat affects the model evaluation.\",\"We outline two limitations of our work from user behavior sampling and knowledge population aspects.Due to huge-volume user behavior data produced every day in the e-commerce platform, it is crucial to efficiently sample significant behaviors that can indicate strong intentions and avoid random co-purchasing or clicking etc.Though in this work we adopt the criteria of selecting nodes whose degree are more than five in the co-buy graph, it is still coarse-grained and more advanced methods remain to be explored in order to sample representative co-buy pairs for intention generation.Some potential solutions are to aggregate frequent co-buy category pairs and then sample product pairs within selected category pairs.Moreover, our proposed framework can be generalized to other types of abundant user behaviors such as search-click and search-buy, which requires to design corresponding prompts.for open text generation from llms, it becomes common practices to label high-quality data for finetuning to improve the quality and controllability of generation such as lamda , instructgpt , and chatgpt6.However, computation cost is the major bottleneck to use annotated data as human feedback for language model finetuning with billions of parameters, like opt-30b in our work.Hence we adopt a trade-off strategy to populate human judgements by training effective classifiers and conducting inferences over all the generation candidates.With impressive generation performance of chatgpt, we expect efficient methods to directly optimize llms with human feedback in more scalable way like reinforcement learning , and enable llms to generate more typical intention knowledge with less annotation efforts.\",\"This work presents cats, a large-scale and highquality chinese answer-to-sequence dataset.It is a free and open dataset.One of most important motivations for presenting this dataset is that most of the existing datasets are built for english, which leads to advanced work on d2t generation primarily focusing on english and leaving other languages underexplored.However, cats only alleviates the dataset language bias rather than solving it.And it is limited to the study of chinese methods.Regarding methodology, the proposed ugt converts the answer-to-sequence task to a graph-to-text problem to bridge the gap between two heterogeneous input data.However, ugt works only for answer-to-sequence task rather than graph-totext task.Additionally, though the proposed nse can help the graph-to-text model better preserve the original structural information, the contribution may be limited to the graph-to-text task.\",\"While our work provides a useful starting point for understanding student feedback, there are limitations to our work.Addressing these limitations will be an important area for future research.Comments may not reflect real student feedback.The comments in our dataset are from users who have chosen to post publicly on youtube.Addi- tionally, the comments may include features specific to this online education setting.Thus, the comments may reflect real student comments from these courses.There is a selection bias in lecture sources.Sight includes lectures that may be drawn from the most successful offerings of that course.The instructional quality may not be representative of typical instruction.Thus, inferences drawn about the instruction should be interpreted with caution, as they might not generalize to other lecture settings.we analyze only english comments because the lecture content is given in english and the authors are most comfortable with english.As a result, our rubric may not capture the types of feedback from nonenglish students watching lectures taught in english.we annotate a small subsample of the data to assess the validity of the automatic labels, we conduct a diagnostic study on a small, randomly selected subset of the dataset, comprising approximately 2% of the comments.Our work aims to establish a preliminary evaluation of the humanmodel agreement and model annotations, and further validation of the automatic labels is necessary.\",\"To better enlighten the follow-up research, we conclude the limitations of our method as follows: 1) although the method we proposed can help improve the quality of generated labels, there is still room for further improvement; 2) because our detection is not perfect, it will lead to inaccurate labels of some samples.4) this work focuses on solving open environment intent prediction with different generative models, without exploring other types of models.\",\"Like all unstructured pruning methods, smp is hard to achieve inference speedup compared to structured pruning methods.Since smp prunes model without fine-tuning, this also limits the extension of smp to structured pruning methods.However, we find that most rows of the sparsity matrices in smp are completely pruned at high sparsity level.This allows us to directly compress the size of matrices, resulting in faster inference.For example, the 3% remaining weights model of mnli can be compressed to 47.43% of the model actual size without retraining or performance loss.By removing rows of matrices that contain less than 10 remaining weights, we can further compress it to 25.\",\"And future directions the findings reported in this paper have to be seen in light of some limitations and, therefore, they just represent a first step.Most of these limitations are related to the ellie dataset itself.First of all, though the predicate-argument combinations used in ellie come from the dtfit dataset and were rated by humans, still the elliptical sentences need human judgements,19 which is one of the future research direction.Then, the dataset size is relatively small, especially comparing to other resources on ellipsis.Currently, ellie was mainly conceived as an evaluation dataset but it could be enlarged and become useful for models\\u2019 fine-tuning, or for carrying out few-shot learning experiments via prompting.concerning the experiments, some changes could be made in the evaluation of task 3.first, we could test the prompts in on the subsets for the other roles, and look for different prompt structures to see if this leads to performance changes.We could also adopt a softer evaluation for this task, by assessing the output in terms of similarity to the target answer.Finally, another limitation is related to the strong dependence of our results to the language used for the analysis.From this point of view, a cross-linguistic study on the elliptical structures in ellie could contribute to improve our work from both a theoretical and practical perspective.\",\"We recognize that this work was only performed on two tasks related to story understanding, thus it is difficult to say exactly how robust it really is.However, given the capabilities of llms and code-llms, we believe our prompting techniques or similar will prove to be useful to the story understanding community.Our work also assumes that the corrpus will be asked the same question across stories.In other words, given an example as a prompt, corrpus will follow that example to generate code for the next story.We are not providing the task to corrpus and having it interpret the question to figure out what it should be tracking.We simply tell it to track certain information so that it can solve these tasks.Therefore, for corrpus to work, the user would need to know what information is salient for their task and prompt it to the system.Even though the re3 dataset contains more complicated sentences than babi, these are still relatively simple english sentences.We do not know how corrpus would perform on more complex stories or on stories in other languages.Lastly, there is the issue of access.Due to cost, we were unable to rerun all of the gpt-3 experiments.The pricing of gpt-3 not only hinders new research, but it hinders reproducability efforts such as ours.Furthermore, as of the publication of this paper codex has been removed from the openai api, and it is as-of-yet unknown if gpt-3.5 or gpt-4 can handle code-based prompting as well.There are, however, still other code-based llms available, such as github\\u2019s copilot and hugging face\\u2019s starcoder.\",\"Compared to a standard knowledge distillation process, our method requires additional computation when preparing training data and training the student.First, our contrastive decoding needs to perform forward pass in the teacher model one time more than greedy decoding does to obtain the perturbed plausibility for each token generated.Second, our kd process introduces additional training data for training the student with the counterfactual reasoning objective.Besides computation cost, this work focuses on improving faithfulness of the rationales rather than performance, which is complementary to prior works which leverages rationales for improving the performance only.\",\"We think this work has the following limitations: the first limitation is that our method involves additional computation for identifying noun phrases and determining which phrases should be normalized.The second limitation is that our method is only performed on noun phrases.Other phrases may also introduce spurious features.Extending our method to other types of phrases is a potential research direction.The third limitation is that due to the cost limitation, we did not test on the more powerful gpt-based plms, which proves to be more powerful and leads to heated discussions recently.\",\"While our method demonstrates strong performance in our experimental setups, potential issues may arise when the characteristics of the available unlabeled dataset drastically change.For one example, if the scale of the available dataset is too small, the effectiveness of our clustering-based data filtering may fall drastically, leading to poor performance.Or, if the dataset is highly unbalanced, our model cannot acquire information about several specific classes.One way to compensate for this shortcoming is to use an externally imported corpus or dataset, similar to other zsl or wsl methods.Another drawback of celda is that the final performance is highly dependent on the performance of the initial pseudo label, as shown in ablation.Nevertheless, as demonstrated in our ablation studies, we can remedy this issue by labeling a few samples, like active learning.\",\"In our experiments, we use t5-base and t5-large models as the target model since they are widelyused, representative pre-trained seq2seq models and use comet-atomic2020 as the commonsense knowledge source.However, there are other pretrained seq2seq models such as bart, and neural commonsense models such as comet that we did not experiment with.Moreover, we only experimented with 10 million randomly sampled sentences from the english wiki and bookcorpus datasets.It would be interesting to investigate whether continually pre-training with a larger scale dataset can further improve the performance.\",\"The limitation is that we separate node identification and node\\u002fedge labeling processes.Because joint node identification and label classification should enumerate all possible spans in a sentence, which is too computationally expensive.Most previous works also separate the two processes.But an obvious disadvantage of such a pipeline scheme is the error propagation problem.\",\".Even though our training methodology runs faster and uses less memory than retraining, there remains potential for further scalability optimization.One potential avenue for improvement could involve optimizing the estimation of the fisher information matrix.Furthermore, op- timizing the parameters related to the incremental training such as buffer size and regularization coefficient is dependent on the entire time steps rather than the current time steps.Devising a time-efficient way for hyperparameter optimization could be extremely beneficial for this task.Additionally, while our full model has demonstrated some mitigation of the problem of catastrophic forgetting, a significant gap remains between the upper performance bound and the performance of our approach.Further research is necessary to bridge this gap and improve overall performance.Finally, our current focus on continual learning is limited to the emergence of new events and does not currently consider the possibility of new relations or entities.This limitation is in part due to the base model not being inductive and is a problem that is inherent to the model itself.Future research in the field of continual learning may aim to address this limitation by considering new relations and entities, even in the context of base models that do not support these features.\",\"One limitation of this work is that while our approach alleviates the requirement of persona description during inference, it still requires persona description for the training corpus.A viable solution is to transfer the pre-trained persona detection models to other datasets without persona description in train set.However, the success of this approach may depend on the degree of similarity between the target dataset and the personachat dataset.\",\".Although alignscore shows high correlation with human judgments, it is hard to interpret the reasoning behind its predictions.Therefore, an interesting future research direction is to develop interpretable factual consistency metrics that can accurately identify words or spans in the input that contain factual consistency errors and produce human readable explanations justifying its predictions.our alignment training data contains datasets augmented with synthetic data.While ablation studies show that synthetic data helps improve metric performance, our rule-based method for generating synthetic data could generate noisy data that may not accurately model the error types and distributions produced by real world generative systems.Thus, analyzing the quality of synthetic data and developing more effective ways to generate synthetic data is an interesting research topic.While we show alignscore generalize well to unseen data, it only covers a single language, english.Undoubtedly, factual consistency evaluation is also important for more resource-constrained languages or in a multilingual setting.Consequently, future research could focus on extending the align metric to multiple languages, including resource-constrained languages.\",\"Like other controllable text generation methods , click also relies on automatic neural classifiers when constructing dcl in some tasks.It may unavoidably inherit the biases and limitations of these classifiers.For instance, for the task of language detoxification, the toxicity may be overestimated when the input prompt or the continuation contains minority identity mentions.To address this limitation, we conducted human evaluation for all the tasks, which further confirms the effectiveness of click.As more accurate, inclusive, and reliable classifiers are built , we expect that click would inherit those improvements as well.\",\"We did not have the chance to explore latest large models.intuitive postprocessing approaches could also be explored.\",\"Our analysis of the behavior of sprl focused on intrinsic task scores.Higher sprl scores suggest a better system.In practice, we do not yet understand how these scores affect downstream uses of sprl labels.Furthermore, sprl datasets are relatively small and are english only.As we are limited to the labels in the existing datasets, we are uncertain about how our results would generalize to larger datasets, new domains, and other languages.\",\".1, further work is needed to investigate whether the negative effect of full contextualization beyond static + positional embeddings at the input layer is an idiosyncrasy of the embedding transfer procedure, or if this is a true effect.regardless, we believe that findings regarding the information content of the representation at the input layer are novel and meaningful, and the quantification method we propose for comparing two representations in terms of their predictive utility is a generalizable methodological contribution.We furthermore note that our attempts to conduct evaluation on newer masked language models were made challenging due to several technical issues in the library : transformers\\u002fpull\\u002f18674).\",\"Although mixda achieves promising results on domain adaptation compared with baseline models, there are certain limitations.Mixda is a two-stage approach, which is not fully end-to-end.Our approach requires training a domain adapter and task adapter, respectively.\",\"In findings of the association for computational linguistics: emnlp 2020, pages 1256\\u20131262, online.chenhui shen, liying cheng, ran zhou, lidong bing, yang you, and luo si.mred: a meta-review dataset for structure-controllable text generation.In findings of the association for computational linguistics: acl 2022, pages 2521\\u20132535, dublin, ireland.lukas stappen, georgios rizos, madina hasan, thomas hain, and bj\\u00f6rn w.uncertaintyaware machine support for paper reviewing on the interspeech 2019 submission corpus.In interspeech 2020, 21st annual conference of the international speech communication association, virtual event, shanghai, china, 25-29 october 2020, pages 1808\\u2013 1812.ivan stelmakh, nihar b shah, aarti singh, and hal daum\\u00e9 iii.Prior and prejudice: the novice reviewers\\u2019 bias against resubmissions in conference peer review.Proceedings of the association for computing machinery on human-computer interaction, 5:1\\u201317.Andrew tomkins, min zhang, and william d.reviewer bias in single- versus double-blind peer review.Proceedings of the national academy of sciences, 114:12708\\u201312713.Richard walker and pascal rocha da silva.Your 2 is my 1, your 3 is my 9: handling arbitrary miscalibrations in ratings.In proceedings of the 18th international conference on autonomous agents and multiagent systems, aamas \\u201919, montreal, qc, canada, may 13-17, 2019, pages 864\\u2013872.International foundation for autonomous agents and multiagent systems.Weizhe yuan, pengfei liu, and graham neubig.can we automate scientific reviewing? Journal of artificial intelligence research, 75:171\\u2013212.Rongsheng zhang, yinhe zheng, xiaoxi mao, and minlie huang.\",\"The proposed model with sentence-aware encoder aims to efficiently incorporate external knowledge and bag-of-words for topic modeling, which means that in this work we are mainly interested in how documents should be encoded for topic inference.However, the decoder of topic models can also be coupled with word embeddings through factorization, such as embedded topic models.It is worth exploring how hierarchical semantic embeddings can be employed for topic modeling with our model.In this paper, we do not conduct any fine-tuning for the pre-trained language model.Our approach reveals how the frozen pre-trained language model can be effectively used to improve the performance of the topic model with limited computational overhead, given that the parameter size of the pretrained language model is much larger than that of the topic model.Moreover, fine-tuning pre-trained language models for topic modeling as an unsupervised learning task is challenging.\",\"In our current project, we have not taken into account the temporal information that treats the historical behavior of users as a sequence of actions.Thus, the model may not capture how user behaviors change over time.moreover, although our pre-trained models achieved significant results without fine-tuning discourse embeddings, we suggest that fine-tuning these models can enhance performance by capturing the nuances of the datasets\\u2019 distribution and contexts.Furthermore, conducting a detailed comparison of additional open-source large language models would provide more comprehensive insights into their performance.Additionally, in addition to analyzing the efficiency of different models, it is crucial to evaluate the cost associated with implementing these models.\",\".First of all, in the analysis, we observed that giving heavy weight to the soft loss at initial training epochs improves the convergence speed.Yet, continuing training with such heavy weight to the soft loss could hinder the further performance improvement of the student.Therefore, adjusting soft loss weights depending on the training phase from a larger value to a small value would be helpful for both convergence speed and improving the model\\u2019s quality.Secondly, it has been demonstrated in the visual recognition domain that adjusting the temperature of distillation loss for poorly performed teachers can improve the student model quality due to the regularization effect.Following them, increasing the temperature to smooth the soft labels from poorly performed teachers, such as 1-layer or 2- layer teachers, would help improve the quality of distillation via the regularization effect.\",\"Np decoding uses k-means clustering to reduce the number of contextualized embeddings, the performance varies by how the contextualized embeddings are clustered.As the process is relatively inconsistent, reducing the number with other methods would make the model performance more consistent.Also, as it is not trivial to add new contextualized token embeddings on top of preconstructed ce due to the clustering step, we did not perform on dynamic corpus setup where new items are added or updated.Np decoding is applicable to all generative retrieval models including gmr or seal which needs all token embeddings, however, we focused on generative retrieval models with representative output as the retrieval target in this work.Also, while it is a general approach applicable to all encoder-decoder models, we focused on applying the method to t5.\",\"As with any natural language understanding task, there are practical limitations and related ethical aspects that must be considered before deploying a system.In particular, our corpus and modeling approach assume that the user-provided res always refer to one of the two options.If this is not the case, or if the re is particularly contrived, undesirable or unexpected behavior may occur: for any expression, including for instance one made with arbitrary derisive language, the model would attempt to resolve this to one of the alternative entities.One approach system designers may consider could be to pre-classify any user-provided res to avoid interpreting those that are off topic or phrased in a negative manner.A second consideration is that of corpus representativeness.In our case, as this is a first corpus for this task, we have limited ourselves to english wikipedia, native english speaking annotators, and particular item sampling strategies for practical reasons.However, if used for training a deployed system, the examples present may bias any model to understand specific types of references but not others.Similarly, the items in our corpus are sufficiently popular to have a relatively long wikipedia entry, whereas items not present in wikipedia, or with only minimal information, may exhibit different characteristics.\",\"Despite the strong performance on the presented datasets, our approach is limited in its ability to update knowledge state and adapt to new domains.A major feature of retrieve-then-read is the ability to swap in new documents when new information is learned, such as temporally more recent documents, or adding in documents from a new domain to quickly adapt to a new downstream task.Our approach relies on a large language model to contain all this knowledge and adding new knowledge would likely require some retraining.In addition, large generation models still suffer from hallucination errors, resulting in incorrect predictions.When tasked with generating 10 urls, llm-url may only generate 6 or 7 which link to valid documents.Finally, our approach involves very large language models, slow web requests, and document processing which may make it cumbersome to use in practice.\",\"Amrsim has high prediction efficiency but the training process is time-consuming.In our experiments, one epoch training on one geforce rtx2080ti took about two and a half hours.Selfsupervised learning requires a large amount of training data.Parsing wiki sentences into graphs requires time, but the advantage is that it can be processed offline.transformers can only handle limited sequence lengths due to the computational and memory complexity of attention calculation.Therefore, encoding large amr graphs is challenging.Possible solutions include applying sliding window algorithm to split a large amr graph into several subgraphs and merge the scores.\",\"Limitation risks in this paper, we bridge the gap between discourse markers and the underlying relations.We use distributed discourse markers to express discourse more informatively.However, learning dmr requires large-scale data on markers.Although it\\u2019s potentially unlimited in corpus, the distribution and types of markers may affect the performance of dmr.Besides, the current solution proposed in this paper is limited to relations between adjacent sentences.Our model can be potentially used for natural language commonsense inference and has the potential to be a component for large-scale commonsense acquisition in a new form.Potential risks include a possible bias on collected commonsense due to the data it relies on, which may be alleviated by introducing a voting-based selection mechanism on large-scale data.\",\"We focuses on resolving various mentions from different domains.Although we have tested our framework on multiple datasets, it relies on a humanannotated dataset and effort should be taken to investigate how the model performs with emerging domains without human-annotated data.Our model works with mentions that have been extracted from raw text.It would be more practical if the model could work with raw text directly and interact with another mention-extraction module.The performance of the model is largely affected by the surface form of the mentions, although our framework is robust to variations in the surface form, it would be more beneficial to further investigate how adversarial turbulence in the mentions could affect the behaviors of the framework.\",\"As depicted in table 4, there are scenarios where argu demonstrates a lack of understanding and instead paraphrases the input variables to generate an incorrect response.It seems likely that the model associates negation with con.However, in exam- ples 5 and 6, the model does not factor the word \\u201cstop\\u201d in variable 1, leading to arguments that contradict the intended stance.Further, in examples 7 and 8, the argument decoder seems to modify the generated template, which changes the overall meaning of example 7.such scenarios might reduce the trust in the model, hurting its practical use.All experiments involving argspan, argspanscheme, and argu only pertain to abortion, minimum wage, nuclear energy, gun control, the death penalty and school uniform.The model performance on any other topics is unknown.Although we test argspanscheme on out-of-domain test sets, it still confines the six topics.Since argu is trained only on argument sentences with less than 150 tokens, it is more geared towards generating shorter arguments of less than 50 tokens.We further do not benchmark argu\\u2019s inference time for practical use.\",\"There are several limitations to account for in the presented work.First, the large gpu requirements for the execution and replication of the presented experiments.Second, the lack of empirical results beyond english-based text, and how morphologi- cally and syntactically more complex corpora may affect the presented evidence.in addition, we have not run any hyperparameter tuning beyond mlm dynamic masking, which might improve all studied algorithms\\u2019 performance.\",\"One of the limitations of this work is that we restrict ourselves to examining datasets for supervised learning that contain relatively short instances of text.This likely facilitated the reweighting of data that we wished to perform as an intervention to produce the reweighted data that we study, as the short length of each text effectively capped the number of different lexical features that could cooccur in the same instance.The results we present here might not be representative of lexical feature bias in data with much longer units of text.Also, the fact that the datasets that we used are all in english means that our lexical features were premised on simple whitespace tokenization with punctuation removal; for other languages with a larger variety of reasonable tokenization schemes at varying levels of granularity, the distribution of lexical features, and the resulting\",\"The main limitation of this paper is the one applying to any opinion piece: it is subjective and personal, as the views of the authors are inherently limited by their expertise and experience.More specifically, this paper argues for an increased interaction between the speech and nlp communities, but the author is more strongly embedded in the latter, and thus addresses this audience primarily.Additionally, the short paper format imposes significant constraints on the amount of nuance, detail and discussion of relevant literature, and thus readers may find some of the claims to be less strongly supported and less hedged than would be ideal, or proper in a longer treatment of this topic.\",\"Our study covered three types of widely used single attention with different mechanisms of assigning attention weights.However, we did not cover some compound attention mechanisms such as dual attention mechanism [8] and co-attention [27], which might contain different patterns affecting the fairness of the models.\",\"We recognize the following main limitations of the present study.Although the approach we devised is not bounded to a specific model architecture and language, our study fouced only on one neural language model and a limited set of languages and this may limit the generalization of our results.Moreover, we are aware that discourse coherence is a multifactorial phenomenon that can only be partially covered by the devised methodology and dataset.\",\".in addition, as with other neural approaches, our model requires significant amounts of data, which is often not available to historical linguists researching less well-studied language families based on field reports.Romance and chinese have relatively many cognate sets because the protoforms are documented5, but a low resource setup with 200 cognate sets would not fare well on our datahungrier transformer model.Furthermore, concatenating the entire cognate set may not work on language families with hundreds of languages such as oceanic because the input sequence would be too long compared to the output protoform sequence.Finally, we obtain our chinese gold protoforms from baxter and sagart \\u2019s middle chinese reconstruction, which was actually a transcription of the qieyun, a rhyme dictionary.Norman and coblin disagree with relying on such a philological source and prefer comparative reconstructions that begin from daughter data.However, there is no available comparative reconstruction of middle chinese with protoforms corresponding to thousands of characters to use as a gold standard.Be that as it may, it seems clear that middle chinese as recorded in theqieyun is not identical to the most recent ancestor of the chinese languages.Its preface concedes that it is a compromise between tang dynasty dialects.The situation with romance is, in some ways, comparable.Classical latin\\u2014the variety on which we train\\u2014 is not the direct ancestor of modern romance languages.Instead, they are descended from vulgar latin or proto-romance, which is not well-attested and is primarily through graffiti and other informal inscriptions.for most language families, protoforms are not attested.In fact, as the term is often used, protoform refers to a form that is inferred only through linguists\\u2019 comparative method.We adopt the other usage for simplicity.In practice, our approach would require reconstructions made by a linguist to serve as training labels for cognate sets.\",\"In this work, we propose managers that allow adaptive aggregation of uni-modal layer representations in each cross-modal layer.moreover, as shown in figure 5, the performance of managertower first increases gradually with the number of uni-modal representations, but then stops increasing and even decreases when the number of uni-modal representations exceeds 6.how to obtain better managertower performance using a lower computational budget while utilizing more insights of uni-modal experts, especially when scaling the model, e., 24-layer clip-vit l-224\\u002f16 and 24-layer robertalarge, is a question worth further exploration.For example, designing reasonable sparse activation functions for managers in managertower, instead of simple top-n or top-p sampling.\",\".Firstly, we can learn more disentangled representations by carefully selecting contrastive pairs for further improvement.Secondly, it will be interest- ing if we extend our method with multiple external sources that come from different knowledge domains.\",\"We acknowledge the following limitations of our work: \\u2022 while the oracle selects a sentence according to the benefits it provides when performing ner, it does not consider the interactions between selected sentences.This may lead to lowered performances when the several sentences are retrieved at once.\\u2022 the retrieval heuristics considered are naive on purpose, as the focus of this work is not performance.Stronger retrieval heuristics may achieve better results than presented in this article.\\u2022 the studied documents only consist in the first chapter of a set of novels.Using complete novel would increase the number of possible information to retrieve for the presented global heuristics.\",\"This study has the following limitations: \\u2022 we fixed the vocabulary size of each subword tokenizer to 30k.Using a different size might yield different results than those in our paper, though the effect of varying the vocabulary size for a subword tokenizer seemed to be small if the size is sufficiently large.\\u2022 we have used the bert architecture for our comparison, while there are other commonly used model architectures such as t5 and gpt-3.\\u2022 to investigate the impact of tokenizers on the downstream performance of plms in scriptio continua languages, we have taken japanese as a case study.\",\"In our experiment, we use only abstract text as the input text for literature review generation however, in writing literature reviews, a writer reads the full text of each cited paper and even other papers related to the research area.Therefore, the input data are insufficient to write a complete literature review.As only 70% of the cited papers have access to the body text in our scireviewgen, a dataset containing full-text information is required for further research.In human-written literature reviews, the chapters complement each other and are not redundant.However, as our qfid and baseline models generate each chapter independently, they cannot consider the relationships between chapters.Furthermore, the relations between each cited paper are considered in the actual literature review writing process.However, these relationships are not considered in the models.In future research, a literature review generation model that can consider the relations between chapters and cited papers by using additional information, such as the contents of other chapters, citation networks, and citation sentences, should be investigated.2, the generated text contains incorrect information to a certain extent.Therefore, we cannot publish it without human revision.Currently, the model can be utilized as a writing assistance tool, not as a complete literature review generation model.\",\"There are additional limitations and potential risks of llm evaluations that should be noted, and these limitations are actually well-known problems of pre-trained language models.As listed on the open ai blog for chatgpt, chatgpt sometimes generates answers that sound right and plausible but are totally nonsense.Openai also admits that the model\\u2019s response may be sensitive to the prompt used to query the model.2, we find that the overall results among different instructions are not significantly different, we cannot guarantee that this is the case for all kinds of modification on the task instructions.Other than the limitations listed on the openai blog, there are still other limitations.For example, llms may not have emotions.as we find during our experiments, chatgpt often replies \\\"i am an ai system and i do not have emotions like a human\\\" when asked to rate the likability of a story.Another important limitation of llm evaluation is that llms lack the ability to process visual cues in task instructions, unlike human evaluation.Human evaluators can use formattings such as special fonts or text styles to focus on important parts of the instructions.Additionally, the way instructions and questions are formatted can influence how human evaluators approach the task.While using special html syntax can serve as an alternative for visual cues, such tags are not used in human evaluation, so we do not use those html tags in llm evaluation to incorporate visual cues in the inputs to the llms.However, llms can only process raw text input and are unable to take in visual cues.\",\"There are two limitations in this paper: compared with existing memory-based methods, the proposed prototype memory may bring additional storage space overhead.But since we only require very little additional memory , we did not discuss it; although we noted that the distortion and forgetting of the prototype are highly correlated, we did not conduct a detailed analysis of the reasons for special prototypes that do not follow this pattern.\",\"In this paper, we leverage amr to the gec model as external knowledge, and achieve a high f-score on single model.However, we do not use r2l reranking, model ensemble and other methods to ensemble single model and compare them with state-of-the-art ensemble models.Our aim is to provide a strong baseline for incorporating amr in gec, so it is easy to generalize amr-gec to ensemble models.\",\"When a dictionary for a low-resource language lacks a word, but has several related ones in terms of synonymy or semantic similarity, it is a definite benefit to be able to provide those to the dictionary user instead of merely saying, \\u201cno results found.\\u201d however, there are some potential drawbacks here: for example, this could increase the rate at which words acquire connotations by analogy with english.\\u2018locomotive\\u2019 and \\u2018train\\u2019 are closely related concepts in english; but that does not necessarily hold for every language, and there is some risk in implying that it does.Language instructors will be all too familiar with students using tools like google translate to do their homework for them instead of doing the hard work of learning the language.On a larger scale, google translate itself was formerly available as a free service that software developers could use to do automated machine translation in bulk; this was abruptly discontinued in 2011.Industry rumour7 held that the bulk service was being used to generate so much of the parallel text appearing on the internet\\u2014parallel text needed to train machine translation models\\u2014that those models could no longer improve sufficiently if they continued to inadvertently be fed primarily their own outputs.This highlights the possible risk that applying machine learning tools like word embeddings can end up distorting language.To this end, we believe that the use of word embeddings to provide analogous words to dictionary users is beneficial, but does not and cannot replace actual lexicography.\",\"This work primarily focuses on evaluating the efficacy of existing continual learning methods for code generation models.It is important to note that many of these methods were specifically designed for natural language processing or computer vision domains and may not directly transfer to the code generation domain.Nevertheless, we have made efforts to identify and address any issues encountered during our analysis.It should be acknowledged, however, that the scope of our work is limited by the selection of methods and the benchmark used.While we have utilized the most popular cl methods from various categories, there may be methods that have not been included in this study due to their inefficacy in natural language processing or computer vision tasks but may be effective in code generation.As such, we encourage further research within the community to explore the potential of cl methods for code-generation models.\",\".As discussed earlier, the validity of presupposition is inherently debatable and largely depends on the background context, i., even experts in formal semantics and pragmatics observe a high disagreement rate.massive language models such as gpt-3 have been shown impressive performance in open-ended question answering.Our paper does not include large-scale, systematic evaluation of such models.Instead, we conduct a small-scale case study with gpt-3 text-davinci-002.most generations are roughly on the right topic, but often contain information that is factually false and do not precisely answer the question.Moreover, they rarely explicitly identify false presupposition and provide corrections, indicating that gpt-3 is far from solving our task.the domain of crepe is limited to online forums.While this choice was made due to the availability of large data and its general domain, we argue that false presuppositions are not specific to such domains.\",\"Differences in experimental setup may make it difficult to accurately and fairly compare published results.For example, to prevent data leakage, we report validation performance at the end of training and do not perform early stopping.This is in contrast to most other papers which report peak validation performance.Results reported for other methods are reproduced in the same learning environment as our method unless explicitly stated otherwise.This takes into account recent work demonstrating problems with fairly and accurately evaluating pet methods that use early stopping improperly.Although many pruning criteria exist in the literature, in this paper we only consider one pruning criterion.Although not presented in this paper, experiments we conducted with various formulations of magnitude pruning did not produce better results.Although prompt tuning is a popular pet method, we do not perform nas for prompt tuning to determine the most efficient positions for inserting prompt tokens into the input.Pruning may or may not prove to be a successful strategy for this problem.Other nas strategies exist in the literature besides pruning, such as evolutionary, reinforcement learning, and darts.However, our pruning method seems to give a good trade-off between validation performance and computational expense.\",\"We only experimented with one and three teacher models.Training more teacher models and using them to predict on large datasets such as solid would require more computing resources.Furthermore, we did not train the teacher models on the augmented dataset for the same reason following recent research in kd.We only conducted the experiments in english.The non-availability of large-scale offensive language identification datasets such as solid in languages other than english can be a challenge when expanding this kd research beyond english.\",\"We used only the pre-trained bart-large model when training each model within the qag framework.also, we only used six interrogative words, and did not consider \\u2018whose\\u2019 and \\u2018whom\\u2019 in the process.We considered these as originating from \\u2018who\\u2019, but generating eight interrogative words including \\u2018whose\\u2019 and \\u2018whom\\u2019 would be a good approach.At last, in order to create a robust ranker, it is best to have a dataset that contains positive and negative samples.Since the manual data generation process required a timeconsuming process, we utilize in-context negative samples as an alternative.If there is a dataset for the ranker learning purpose, much better performance can be achieved.\",\"Three main limitations with regards to certain aspects of this paper are the comparison against very large models, the distribution of the original set, and the restriction of the output length.2 against very large models like minerva or even codex.However, these larger models can still be evaluated as fermat is made publicly available.Secondly, another limitation of fermat is its use of illinois and commoncore which have highly skewed distributions of numbers and their answers are mainly integers which is not representative of the real-world.This undesired effect is mirrored in the number types that use the same numbers as original.However, this was part of our design for fermat as the alternative would have been to combined all the ranges of numbers used with the representation, creating too many aspects but mainly conflicting with non-independent analyses between representation and range of numbers.Therefore, we chose to use the same numbers as original, and since the templates will be openly accessible, they can be used to generate more combinations for wider aspects.Lastly, when generating training questions, despite our best intentions, we had to limit the length of the output to an arbitrary length of 12 digits, therefore some number combination were not possible, for example 1\\u00f73 = 0.This practical implication could have been avoided with the use of fractions or rounding.But we judged that it would have added an extra layer of difficulty for the models and decided to restrict the output length instead.\",\"We expect that our cross-lingual models have learnt some coreference knowledge on the target languages and we conduct experiments on some languages in zero-shot settings.However, we do not get consistent and significant improvements compared to monolingual models.This should be further investigated which potentially helps languages with few or no coreference annotations.Compared to monolingual models, our cross-lingual model improves the source-side coreference resolution but it requires almost two times gpu memory during training.Thus, this model architecture imposes restrictions on using larger pretrained models given limited resources.\",\"Due to the maximum input length constraint of both the clip text encoder and the text-to-image model, we are unable to process long texts.We are interested in exploring alternative prompt configurations to circumvent this limitation.Our methodology is readily extendable to these settings, making it an intriguing area of study.\",\"Exist , compared to traditional approaches the models trained in this study are expected to reduce biases.Their value is not limited to predicting dates for individual manuscripts, but they can be applied to any attribute of a group of papyri, e.the place of provenance or the text\\u2019s type.At the same time, easily accessible open-source metadata exist for most published papyri.\",\"We only explored one-shot prompting strategies with gpt-4.we prompted gpt-4 with only a single randomly selected sample that included all of the annotated event types.Our post-processing included simple rules to process the generated output and may be improved.The quality of the sample and the selection method may influence performance.\",\"While we used multiple parsers to avoid biasing the evaluation towards one parser, all parsers used are relatively high-performing parsers\\u2014all have la- belled f\\u2081 scores above 0.8 on the ccgbank development and test sets.This evaluation is thus biased towards especially difficult sentences, since those will be the ones where good parsers produce errors.While we found no correlation between parser scores and judge disagreement, at least suggesting that the judgements were not a function of parse quality, poorer parsers may make different kinds of errors than those that appeared in our sample.It is unclear how f\\u2081 and df\\u2081 would compare under such circumstances; understanding this better remains an open area of research.The relatively high disagreement among judges in the second task is concerning, but it should be noted that the sentence pairs were sampled from a set of disagreements between two different scoring methods.The extent to which this is a problem in practice is unclear, as judge agreement may not be as low on outputs from different parsers evaluated by the same scoring method\\u2014but it could also be lower.Although the dependency-based evaluations discussed in this paper are standard for ccgbankbased statistical ccg parser evaluations, the reliance on extra resources makes it somewhat unique.Because of this, the extent to which decomposed scoring, or the ideas behind it, would be useful for other evaluations scenarios is unclear.\",\"Limitations of bartscore++ are three-fold: \\u2022 in \\u00a73.1, we propose explicit\\u002f implicit errors to better distinguish different types of errors in generated texts.However, explicit errors only contain token-level errors that can be detected and corrected by error analysis, not involving all error types mentioned in mqm.We hope future studies can take these situations into account.2 we can see that our proposed error analysis framework fully relies on the generation probabilities of bart to decide how to refine the hypothesis.Still, we see that this framework may lead to false judgments due to unfaithful content.Further research can explore how to calibrate the pre-trained models during error analysis.3 we integrate the distance of explicit and implicit errors by simply computing their weighted sum.This can be improved by considering more factors, e.the overall quality of the generated text, refining iterations, and external signals.\",\"One limitation of this work is that vag does not achieve zero forgetting.Although we show solving cil based on label generation can effectively ease forgetting and representation collapse of the pre-trained model, it is still interesting to further explore how to explicitly solve the forgetting issue in this new framework.The proposed techniques in vag are a step in the exploration.Another limitation is that we directly use the label sequences provided by the original dataset.This may be suboptimal because the quality of the manually created label is hard to guarantee as it may fail to capture the semantic information of the samples in a class.A potential direction is to study creating label sequences automatically by summarizing the training samples.\",\"We mainly summarize three limitations of our work.First, the translator only generates a representation, not an actual instruction, making the model less interpretable.Second, we do not include more advanced vision representations such as vit and clip to train the navigation agent.Although only using resnet, we already surpass prior methods using those visual representations ), it would be interesting to experiment with those different visual representations.Third, this navigation agent is trained in a simulated environment, and a more realistic setting will be more challenging.\",\"Our experimental analysis proved that text regression is a considerably reliable and accurate tool in dating nonliterary papyri.Limitations and challenges stem mainly from the composition of our dataset, which is balanced as far as the dates of the papyri included are concerned, both at the level of the century and at the level of the quarter of the century.Furthermore, although we retained a substantial text sample of each papyrus, in approximately 1\\u002f4 of the records some text was eliminated.Biases despite our effort to balance the dataset in terms of dates, biases are present.Since our main concern in collecting the data was for the date distribution, no deliberate selection was made on the basis of the document types.Some types are thus over or underrepresented.Each type of document has however distinctive linguistic characteristics, such as the level of formality or unusual constructions.This uneven typological representation probably affects the performance of the models.Other possible biases in the dataset concern the 15we sampled randomly 100 regressors.Provenance of papyri, the length of their text, and the state of conservation.Chronological analysis of words chronological analysis of word occurrence is possible if we detect and collect terms only attested in the papyrological material during a limited period.The word \\u2018denarius\\u2019 only appears after the 2nd ce and before the 5th ce, its presence in a text thus means that the text must have been written during this timespan.Likewise a text containing the word \\u2018indiction\\u2019 cannot have been written before the 4th ce.The investigation should also regard the possibility that the models make a prediction for a papyrus based on typical dating formulas present in the text like the name of the ruling emperor.Although our investigation of explanations did not yield any major concerns, a bigger sample of test cases should be created and more explainability methods should be employed to make conclusive remarks on this front.Transcription of papyri is not optional transcription of the papyri is required to reach this high degree of accuracy with our method.Thus, while there are transcriptions available for most already published papyri, it is less practical for dating unpublished papyri that have not been yet transcribed to a relatively high standard.In that case, image classification on the scripts can provide a less accurate prediction of the date as starting point.\",\".Firstly, we only test our method on one dataset.We plan to apply our model to more datasets in future versions.we will conduct comprehensive ablation experiments to demonstrate the contribution of different components.\",\"A major component of rsmi has been developed with the concept of randomized smoothing which is known to be certifiably robust within a radius of a ball around an input point.Though we have proved the robustness for the perturbed samples within this given ball, there is no theoretical guarantee that a perturbed sample will always lie within the ball.Accordingly, our study is limited to empirical validation of the effectiveness of rsmi, although it has theoretical robustness within a l2 norm ball as shown in \\u00a72.Nevertheless, certified robustness is a critical research direction for robust and reliable deployment of nlp systems to address undiscovered attacks.\",\"Our studies focus on the differences in how-to guides written for specific audiences only in one language, namely english.A major limitation is therefore that we do not consider other languages.The perspectives provided by the data source we rely on, wikihow, allow us to identify specific phenomena and peculiarities.Yet, contemplating only one data source lets us generalize only to a limited extent.For example, the audiences considered in this work depended on the target groups portrayed in the data.They are neither exhaustive nor representative of the diversity of humankind, especially of marginalized social groups.Therefore, a wider variety of data sources will be needed to test generalizations.while it seems possible that guides can be tuned by contemplating one specific attribute of the audience at a time, this does not hold with regard to the actual attributes of the readers.Such attributes are per se coexistent, and consequently, they are not separable.\",\"While we demonstrate the efficacy of tsdshapley empirically, the current work is limited in terms of theoretical analysis.For example, while we have good empirical performance with a linear svm, additional analysis could determine if there are optimal ways to select an alternative simple model architecture for the source classifier depending on the target classifier or dataset.Additionally, while we found a strong correlation between number of sampling chains and performance when the subset size was \\u003e 2% of the training data size, the lower subset size threshold to observe this correlation may be dataset dependent, which additional analysis could address.\",\"Due to the limitation of time and resources, in this work, we select a relatively small number of clusters during the clustering process, which results in coarse-grained clustering.Fine-grained clustering can provide a better latent concept but will also lead to increased computational resources and time consumption.besides, our node clustering module is not integrated in an end-toend manner in our work; we will consider using the topic neural network to construct an end-to-end model.\",\"While we have demonstrated the promising potential of utilizing the lower bound as a substitute for p in knowledge-grounded generation tasks, there are several limitations that need to be acknowledged.First, the use of the language model as learning signals can introduce flaws.The model may exploit the lm\\u2019s weaknesses by generating comments with a high likelihood based on the lm but are nonsensical in reality, resembling adversarial samples.In our experiments, we observed that generating adversarial text samples, unlike vision models, proved challenging, and we did not encounter completely nonsensical comments.However, we did observe the model exploiting the flaws in the lm, indicated by certain common patterns in the generated comments.Second, there are better alternatives to a hard knowledge injection reward, such as an n-gram matching-based bleu score used in this study.In some cases, a knowledge-grounded comment may not have any word overlaps with the knowledge instances, resulting in a n-gram-based score of 0.Ideally, an embedding-based soft knowledge reward would be more desirable for this reason.However, in our experiments, we found that the soft knowledge reward based on methods like was easily exploitable, as the model learned to echo keywords from the context to achieve a high soft knowledge reward.Third, our approach primarily focuses on scenarios where well-constructed triplets are not readily available, such as when retrieving information from the internet.However, in cases where pseudo knowledge construction is highly accurate, such as applications with more limited scopes, our approach may not outperform triplet-based approaches.Fourth, it is important to note that our method could potentially be used to generate offensive or prejudiced texts.Addressing biases in generative models is a longstanding issue, and it is not the main focus of this work.However, the ethical implications can be partially mitigated by integrating our approach with other debiasing technologies.\",\"Dp training of large models is compute-intensive, requiring per-example gradients and large batch sizes.This renders the training of such models difficult and not easily accessible to everyone.Dp-sgd takes records to be single training examples, which in this paper\\u2019s experiments 9 correspond to single user utterances.That setup prevents the trained model from revealing much information about any given single utterance, but it may still allow information to leak that is repeated across multiple utterances.For both the baseline method and our two-stage method, we trained our model to approximately match the true distribution of private user utterances, ppriv, to the extent that this was possible under a differential privacy guarantee.Of course, there are many ways to measure the quality of an approximation, and different approximations are appropriate for different tasks where it might be important to preserve different properties of ppriv.The one-stage baseline approach implicitly aims to achieve a low cross-entropy, by applying dp-sgd to the log-likelihood function.In contrast, our two-stage approach aims to encourage an approximation that also roughly preserves the marginal distribution over semantic function types.We did not investigate more direct ways of encouraging such an approximation, for example, one-stage dp-sgd with a modified objective function that explicitly evaluates the marginal distribution in addition to the log-likelihood.Finally, we trained an approximate model of ppriv from which we can draw utterances to inspect and annotate.But we must acknowledge that ppriv is not the ideal distribution to approximate.Even if we were able to actually use private utterances to improve the system, we would not necessarily want to draw them directly from ppriv.Rather, we would want to select them by active learning\\u2014selecting the private user utterances that would be most useful to inspect or to include in the annotated training data.Thus, when training our model by dp-sgd , we could upweight or upselect the private utterances that appear useful in this way\\u2014resulting in a differentially private model that generates useful synthetic utterances.Specifically, traditional active learning by uncertainty sampling would select utterances where the semantic parser was uncertain what to do.We would also want to select utterances where the system suspected for other reasons that it did not do the right thing\\u2014because it classified the user\\u2019s request as a functionality that the system did not yet support, or because the user objected in some way to the system\\u2019s response.\",\"Perception and reasoning in text-based raven.In this work, one limitation is that we do not attempt to solve the perception problem of analogymaking in rpm, rather we apply perfect perception in solving the reasoning part, and assume the perception problem is simple.By doing so, we find that plms may be a strong solution to the reasoning problem here, which may better direct future efforts toward ai and analogy.Obviously, the perception problem for idealized domains is a lot different than more natural domains, and identifying key features across many domains that can facilitate a mapping is still a challenging unsolved problem.We hope that our work sparks more interest in this problem.Meanwhile, one may argue that our decomposition abstractions are too strong, and actually contribute to the reasoning problem in rpm, as they make an independence assumption about which features of the task can be teased apart.Making such an assumption requires an understanding of the problem that cannot be inferred by only seeing one instance.However, we decomposed the task based on very intuitive and common attributes, e., shapes, colors, sizes, and counts of items.We believe that the strength of such an abstraction, which could be applied in many problems, should not be understated.Nonetheless, we include decomposition-free forms of results as much as possible throughout the paper to help compare the contributions of decomposition versus naming abstractions, which is more clearly only providing perceptual information.In fact, we find that without any decomposition, plms still achieve very strong performance in many cases, and performance gains from decomposition are not always large.lastly, we note some limitations in the human performance measurements used as reference points., human performance on raven was measured by giving subjects some task-specific training, then evaluating them on the original visual form of the task.This differs from our results in two ways.First, plms had no task-specific training for raven, given that experiments were zero-shot and the text data we generate is new and thus impossible to appear directly in plm pre-training.second, the task is presented to plms in text form, not visually.While the essential information from the task is preserved by our conversion, it is possible that this conversion would affect the difficulty of the task for humans.As such, it becomes unclear how to contextualize our results with these past human results.\",\"A key limitation in our work is that llms might have seen these math problems.Our work theoretically assumes this is not the case.Another limitation is that for the sake of simplicity, our work makes some assumptions.For example, we assume all numbers in the range of integers 0 to c = 300.this would not cover every mwp out there.in this work, we are also constrained by the limitations of the openai policy on the gpt-3 api.This limits the number of perturbations we consider in this work as well as the accuracy with which we can estimate our causal distributions.Finally, our work is restricted to english, and extending it to other languages will require us to create an mwp dataset in that language.\",\"We have carried out all analyses according to our best abilities.Nevertheless, it should be noted that rst structures and qud structures were annotated by distinct researchers.While all annotations have been double-checked by at least one other expert for plausibility, in many cases there are alternative analyses of the texts which may also be applicable.Since we do not have direct access to the discourse creators and their goals, this limitation is unavoidable in corpus studies.\",\"We highlight two main limitations of our work.Firstly, instead of focusing on more recent nmt models that use large pretrained language models as their backbone, our experiments were based on transformer base models.That is because we used the nmt models that produced the translations in the datasets we analyze, i.e, the models that actually hallucinate for the source sequences in the dataset.secondly, although our method does not require any training data or human annotations, it relies on access to a pre-existing database of source mass distributions.This can be easily obtained offline by running the model on monolingual data to obtain the distributions.Nevertheless, these datastores need not be costly in terms of memory.\",\"Limitation calculating complexity indices for large-scale graphs can be computationally expensive and time consuming.Some of the complexity indices show longer turnaround time when computed for denser areas in the graphs.In addition, as we mentioned in the paper, although we made sure our framework and implementation allows adding any number of additional indices in a modular way, there might be other effective complexity indices that are not included in this investigation.Furthermore, it should be noted that the model has been exclusively tested on graphs where nodes contain textual content, which may limit its application to more general graph types.Finally, the model has not been applied to other graph-based tasks such as clustering and graph-level classification.\",\".First, we would like to highlight the difficulty of applying the validity assessment framework from measurement theory to instability measures.1, our low convergent validity scores may have different interpretations because there are no well-established instability measures.Rather than theory, and our second test of differences among test datasets examines the consistency between theoretically indistinguishable groups instead of the differences between theoretically distinguishable groups.Second, we only experimented with a limited number of tasks, instability measures, plms, and validity types.for example, to apply our validity testing framework to larger datasets, to include other measures , to study generative plms , and to test other types and validity.Third, we focused on general text classification tasks in this paper.One promising direction is to investigate which measures to use for specific settings.For example, to extend our framework to more recent generative models ).However, in this case, because our prediction measures in \\u00a73 are only useful for classification, new prediction measures should be developed, and our tests should be adjusted accordingly.\",\"Of the retriever-reader pipeline approach to odconvqa, which is prone to error propagation from the retriever, unable to run both sub-modules in parallel, and demanding effort to manage these two submodules, due to its decomposed structure.To address such issues, we formulated the odconvqa task as a dense phrase retrieval problem, which makes it possible to directly retrieve the answer based on its representational similarity to the current conversational context.Furthermore, to model the conversational dependency between the current and its previous turns, we force their representations to be similar with contrastive learning, which leads to retrieving more related phrases to the conversational history as well as the current question.We validated our proposed pro-convqa on odconvqa benchmark datasets, showing its efficacy in effectiveness and efficiency.Limitations as shown in table 3, the contrastive learning strategy to model the conversational dependencies between the current and previous conversational turns is a key element in our phrase retrieval-based od- 1 convqa task.However, when the current conversational topic is significantly shifted from the previous topic as the user may suddenly come up with new ideas, our contrastive learning strategy might be less effective.This is because modeling the conversational dependency is, in this case, no longer necessary.\",\"In this paper, we propose docredhwe and introduce a new metric to select the most robust and trustworthy model from those well-performed ones in docre.However, all data in docred are sampled from wikipedia and wikidata, which indicates that training and test data in docred can be identically and independently distributed.assumption impedes our demonstration of the intuition: a model with a higher map will obtain a higher f1 score on the test set.assumption, models can succeed in obtaining a higher f1 score by greedily absorbing all correlations in the training data.To strictly demonstrate the intuition, we need a test set that exhibits different and unknown testing distributions.In addition, expanding the research scope to a cleaner re-docred and analyzing the role of unobservable wrong labels are also crucial and interesting ideas.\",\"In this work, we focused on problems posed in the hand-crafted humaneval dataset.A potential pitfall of a curated dataset such as humaneval is that the results may not generalize to real-world scenarios where developers often deal with more complex problems and code bases.To address this limitation, we originally explored the use of datasets mined from github.However, our experiments indicated memorization issues , potentially due to the sample code already being included in the model training set.In practice, high quality code deduplication required to avoid this specific limitation is challenging.Work by allamanis find that the impact of duplicate code can be severe, sometimes inflating model performance scores by up to 100%.Furthermore, in our early pilot tests, functions extracted in the wild were found to contain insufficient context for even expert human annotators and isolating functional tests is challenging without heavy curation.Further research is therefore needed to understand how our findings might generalize to a wider variety of deployment settings as well as research on designing diverse evaluation datasets.\",\"In this paper, we employ an information entropyguided algorithm for purifying the induced biased features.For each dimension of the biased features, the component with less information entropy is priorly regarded as the component corresponding to semantic information, and excluded when deriving the purified biased features.However, there is still the risk that the discarded component still account for part of the dataset biases.This would lead to a decrease in the effectiveness of the debiasing process.Hence, although the prior-knowledge free nature endows our proposed biased features purification algorithm with strong generality, in cases when resources indicating the distribution of dataset biases are available, incorporating these resources would further enhance the purification of the biased features.\",\"; the relevant cases are considered based on official citations as ground truth.However, there might be cases that were not mentioned by the judge due to subjectivity involved in the common-law system; finding correct annotation for relevance is always a challenge for a domain like legal, where the number of documents is enormous.\",\"One of the limitations of our work is that while we are running the mathprompter multiple times in different ways to increase the accuracy of our results, this does not always guarantee the correctness of the output.Both algebraic and pythonic expressions have the potential to produce the incorrect results, even if the prompt outputs match each other.This is the fail case as shown in the last row of table 2.Increasing the number of prompts will mitigate this issue.We are currently investigating techniques that can address this issue in a more principled manner.\",\"Certain parts of our proposed methodology, for example, template-based replacement and n-grambased prompting are applicable only when stylespecific linguistic attributes could be identified between the source and the target text.And due to the cost of human labor and the lack of publicly available client-therapist dialogues, the sample size drawn in the study is small and thus may have an impact on the conclusions drawn.Our methods have only been tested for the english language.But we believe similar methods could be applied to other languages given they have unparallel corpora tagged with advise without permission and advise with permission labels.The rephrasing methods described in this paper are tested for short sentences with a maximum sentence length of 98 tokens.Thus, the scalability of these methods for long text still remains to be tested.When testing the rephrasers, there are some combinations that could be tried other than the ones already tested.For example, more models can be fine-tuned and tested separately on templatereplaced and retrieval-based pp and ppa corpora but incorporating generic and n-gram prompting.In this work, we first combined these two types of corpora before attempting prompting since we could observe better performance on blender when the corpora were combined.In order to have more data, we combined the advise with permission and advise without permission responses present in counselchat and red datasets.But studies show that there are differences in the language used by counselors and peers.So, there can be linguistic differences between the same type of response in counselchat and red datasets.\",\"Our model only focuses on utilizing text information for recommendation, which is a key limitation of this work.In real-world settings, recommender systems are usually required to handle heterogeneous information inputs.Unitrec is a pure textbased recommender modeling user history and candidate texts as inputs.However, incorporating additional side information could further improve the recommendation performance and alleviate the cold start problem.Furthermore, unitrec only models two-level relations of user behavior history.Nonetheless, incorporating more user behavior information, such as implicit and negative feedback, could further enhance the recommendation performance.\",\".While our study only considers three papers, this is by design.As our goal is to study user experience, by fixing papers to be within a specific topic area and time requirement, and having people with different skill levels reproduce the same papers, this allows us to have sufficient samples to understand general behaviors.It also blocks other nuance factors on beginners\\u2019 experience.Each of our selected papers presented students with unique reproducibility barriers, and consequently resulted in a wealth of helpful insights.Furthermore, finding reproducible nlp papers that satisfied our constraints was surprisingly difficult, with only 3 out of 24 considered papers found to be reproducible within our constraints.Nevertheless, this study is still on the small scale.Engaging a larger community in a large scale study may provide additional insight.Related to this, our study only includes a population of mostly graduate students at our university.Considering beginners from different educational backgrounds or regions could reveal more comprehensive insights, and we greatly encourage future efforts at a community level toward better understanding the needs of nlp beginners.it is also worth noting that it is difficult to consistently calculate runtime of code on gpu hardware, as fluctuations may occur due to a number of factors, including the specific gpu hardware allocated to a student,27 driver versions, and 26 27while experts used nvidia tesla v100 gpus with up to 16gb memory to reproduce results, nvidia a40 gpus with up to 48gb memory are also available within the cluster.to minimize the impact of such issues, we chose to reproduce experiments that used small models and had shorter expected runtimes.Given that we observed runtimes up to several times larger than expert runtimes, we thus expect that trial and error in setting up experiments accounted for most fluctuation in observed runtimes.\",\"Our experiments were built on perfect sentence alignments in the original and revised essay drafts, thus the performance could be lower in the real end-to-end automated writing evaluation system.In addition, our corpus is small due to expensive annotation processes, which makes it challenging to train or finetune large language models.Also, we only focus on revisions in argumentative writing, specifically, we focus on the evidence and reasoning revisions, however other revisions like claim revisions are not used.our proposed argumentative contexts are generated by chatgpt which is not free for the whole community.Also, chatgpt-generated acs have small randomness, which is also the reason we did 3-seed runs in the experiments.In addition, the acs are essay-level context which means different revisions in the same essay use the same context.It could be tailored to have sentence-level acs where each sentence-level revision has slightly different revision purposes, but it would cost more time and money.Moreover, our proposed zero-shotcot prompts perform better than single prompts by small margins in specific cases, which indicates that chat-gpt is limited to conducting cot extraction and summarization to handle complex wording and sentence structure.\",\"In this paper, we present a novel knowledge injection paradigm plug-and-play knowledge injection for plms.We show existing methods can not be well applied to the new paradigm and propose maptuning as a preliminary exploration of methods.The paradigm plug-and-play knowledge injection has a limitation in terms of its assumption.It assumes that a plm should be fine-tuned for downstream tasks.However, very large-scale plms can perform zero-shot learning or in-context learning on downstream tasks without being fine-tuned.the method map-tuning has three limitations in terms of its applicability.Firstly, we did not evaluate map-tuning for plms pre-trained by other language modeling objectives besides mlm.secondly, we did not evaluate whether the plm can do complex reasoning based on the knowledge injected by map-tuning.Thirdly, map-tuning is designed to plug structural fact knowledge.It is also meaningful to plug other diverse knowledge bases, including text corpora, voice, images, and even other plms, which are not covered by our work.\",\"We see several limitations regarding our work.First, we focus on documents in the english language only, neglecting many caribbean newspapers and islands with other official languages.While some of our methods can be easily extended to non-english material , methods that rely on the pre-trained english model f-coref can not.On the same note, f-coref and spacy were developed and trained using modern corpora, and their capabilities when applied to the noisy historical newspapers dataset, are noticeably lower compared to modern texts.Contributing to this issue is the unique, sometimes archaic language in which the newspapers were written.While we validate f-coref performance on a random sample , this is a significant limitation of our work.Similarly, increased attention is required to adapt the keyword sets used by our methods to historical settings.Moreover, our historical newspaper dataset is inherently imbalanced and skewed.As can be seen in tab 2 and fig 8, there is an over-representation of a handful of specific islands and time periods.While it is likely that in different regions and periods, less source material survived to modern times, part of the imbalance can also be attributed to current research funding and policies.12 compounding this further, minority groups are traditionally under-represented in news sources.This introduces noise and imbalance into our results, which rely on a large amount of textual material referring to each attribute on the gender\\u002frace plane that we analyse.Relating to that, our keyword-based method of classifying entities into groups corresponding to the gender and race axes is limited.While we devise a specialised keyword set targeting the attributes female, male and non-white, we classify an entity into the white group if it was not classified as non-white.This discrepancy is likely to introduce noise into our evaluation, as can also be observed in tab 7.this tendency may be intensified by the nlp systems that we use, as many tend to perform worse on gender- and race-minority groups.thus, we neglect the effects of other confounding factors that affect asymmetries in language.\",\"Data and task limitation in this work, we analyze domain-label bias and apply our domaincontext calibration to english.in experiments, we discuss calibration on classification tasks.The effect of domain-label bias could exist differently for open-end tasks like text generation.Our analysis of domain-label bias also emphasizes more on the word-level bias.Other types of biases associated with a domain, such as topics and genders, may also impact model prediction.due to budget limitations, we conduct experiments on a subset of the 24 reported datasets for gpt-3.One can evaluate all 24 datasets to get a complete picture with enough budget.Model limitation for large language models, we only focus on the gpt models and only select roberta as the small-scale language model in experiments.access to the openai api for gpt-3 is also necessary for parts of our experiments.\",\"This work requires that news videos are organized into different events and each event has more than one candidate video.The debunking rectification module relies on the existence of labeled debunking videos, and the graph aggregation module relies on existing fake news detectors to provide the initial features for each video.The textual length in videos is limited due to that the debunking inference module is based on a pre-trained bert model with limited sequence length.\",\"First, we find robustness deficiencies in metrics by comparing the evaluation differences among metrics.This applies to the case when there are metrics that do not have the same robustness flaws.If there are more latent common defects in the metrics, they cannot be identified by mrt.We leave this topic for future research.Second, we use beam search to generate candidates during mrt training, but beam search is also known to have deficiencies.For example, beam search suffers from heuristic search biases and shifts statistics away from those of the data.Different decoding methods may have an impact on the experiment results.\",\"Since our generative approach to product attributevalue identification autoregressively decodes a set of attribute-value pairs as a sequence, the inference is slow and how to linearize the set of attribute-value pairs in the training data will affect the performance.The best way of composing an attribute-value pair and ordering the pairs will depend on the characteristics of the datasets such as the existence of canonicalized values and the number of attribute-value pairs per example.Those who attempt to apply our method to their own datasets should keep this in mind.\",\"In this work, we implicitly utilize the contradiction relation.The authors recognize explicitly including it in a prompt template leads to worse performance due to the injection of noise.Controlled template generation based on a model confidence is unexplored in this work and appears to be a promising direction.Additionally, we recognize the emergence of parameter-efficient methods for training models which are unexplored in this work, which may have utility.These methods are complimentary and may benefit the performance of models as they can be used in conjunction with training paradigms such as contrastive learning to support better representations through explicit utilization of the contradiction relation.In this work, we limit our study to draw attention to the importance of strict zero-shot classification settings with the emergence of llms.Our study can be easily extended to recursively operate on large language models, and entailment predictors.\",\"Here, we discuss some limitations of this work to inspire future research in this direction.self-instruct depends on lms, and it will inherit all the limitations that carry over with lms.As recent studies have shown , tail phenomena pose a serious challenge to the success of lms.In other words, lms\\u2019 largest gains correspond to the frequent uses of languages , and there might be minimal gains in the low-frequency contexts.Similarly, in the context of this work, it would not be surprising if the majority of the gains by selfinstruct are skewed toward tasks or instructions that present more frequently in the pretraining corpus.As a consequence, the approach might show brittleness with respect to uncommon and creative instructions.because of selfinstruct\\u2019s dependence on the inductive biases extracted from lms, it might work best for larger models.If true, this may create barriers to access for those who may not have large computing resources.We hope future studies will carefully study the gains as a function of model size or various other parameters.It is worthwhile to note that instruction-tuning with human annotation also suffers from a similar limitation: gains of instruction-tuning are higher for larger models.a point of concern for the authors is the unintended consequences of this iterative algorithm, such as the amplification of problematic social biases.Relatedly, one observed challenge in this process is the algorithm\\u2019s difficulty in producing balanced labels, which reflected models\\u2019 prior biases.\",\".We acknowledge the limited scope of our experiments, including only 8 documents, 3 models and a single language.This is largely due to the limited availability of suitable large lms and their high computational cost.Still, we believe that our experiments are valuable as a case study that already clearly showcases some interesting features of our methodology.While we have demonstrated an efficient strategy to obtain predictions for all tokens at all possible context lengths, it still requires running the model n times for a document of length n.For a k-fold reduction in computational cost, the technique may be modified to use a sliding window with stride k \\u003e 1.The proposed methodology allows investigating how any given metric is impacted by context, yet our study is limited to nll loss and the proposed kl divergence metric.These may not be optimal for every purpose, and other choices should be explored depending on the application.For example, to study sequences generated from a lm, one might want to define importance scores using a metric that does depend on the generated token, e.its nll loss or its ranking among all candidates.\",\"The meco dataset is recorded at different labs following the same strict protocol.Nevertheless, location and experimenter effects may be confounding factors for the nlir task.The celer data , used by , seems to all be recorded at the same lab.Since we confirm their hypothesis, we do not see this as a fatal flaw in our study.There is no other available dataset that would allow us to replicate their finding.\",\"Limitation dect explores how to adapt black-box ptms on downstream tasks.4, our method is not comparable to fine-tuning on hard tasks with increased data points.Moreover, we only focus on classification tasks in this work and do not testify dect on free-form generation tasks.ethical statement as large language models are getting more and more popular in nlp research and application, dect provides a cost-efficient way to adapt these large models.However, we need also to be cautious about the improper adaptation of large language models, such as generating toxic and biased speeches.\",\"In our experiment, we presented results for three diverse sequence-to-sequence tasks, namely, machine translation, text summarization and knowledge graph question answering.While for these three tasks, we managed to observe common trends a more large-scale study of various sequence-to-sequence tasks is needed to further confirm this observation and robustness of the best-performing method as identified in this work.\",\"The big 5 personality trait model measures the fundamental dimensions of human on a continuous scale.This real valued representation preserves more information and is more descriptive of interindividual differences.While we acknowledge that the binary classification of big 5 traits fails the purpose of the model, it is a necessary simplification to understand the ability of llms to perform personality assessment.Our investigation shows potential to improve the practical utility of llms in personality estimation.Despite the strong results from existing works in support of in-context learning and larger message history for better performance, we were limited by the significant multiplicative cost these experiments entailed, as the gpt-3 api is billed based on token usage.Further, since each user\\u2019s post history is typically long, it is infeasible to experiment with all in-context learning options due to gpt-3\\u2019s context window size limitation.This is worthy of exploration, to understand the sample efficiency of gpt-3 and the impact of post history on its performance.\",\".The primary theoretical limitation of hexatagger is that it can only produce projective dependency trees.as a trade-off for efficiency, hexatagger does not model dependency arcs directly.Compared to graph-based models that explicitly score arc scores between pairs of words, it is more difficult to interpret the output of hexatagger.\",\"The proposed synthesis framework has been targeted at text-to-sql task, which may not generalize to other tasks that require large amount of synthetic data without major modification.On the other hand, the template based synthesis method currently relies on templates extracted from the real data.By incorporating some carefully designed grammar , we may be able to further enrich the template set.\",\"Firstly, the training process of our proposed model is dependent on supervised data, thus precluding its application to languages without a supervised dataset for ccg.Also, the span-based parsing algorithm proposed in this study is implemented in python and may take a considerable amount of time to parse extremely long sentences due to a lack of optimization for implementation.\",\"There are several limitations of the proposed methods.We use a pre-trained classifier, electra, as an off-the-shelf poisoned sample discriminator without fine-tuning on customized datasets.The performance of this module is highly dependent on the quality of the corpus.We also calculate the attribution scores of each token using gradientbased partial lrp to identify potential triggers, but further evaluation of different attribution score calculation methods is needed.\",\"There are several limitations to our work.First, although we choose the attribute-value dataset due to its high degree of interpretability and control, we acknowledge that its simplicity limits the impact of our findings.by reinforcement is a data-agnostic mechanism, we have yet to explore how it behaves in more complex settings, such as using naturalistic image inputs or embodied communication.for further discussion on scaling up communication settings.A second limitation of our results is that we do not explore how imitation-based learning scales to k \\u003e 5 experts.In particular, our hyperparameter regime handles up to around k = 5 experts\\u2013 very preliminary analyses on k \\u2265 10 experts suggest a need to also scale up hyperparameters such as agent size and communication channel capacity.When training agents to imitate, one must therefore consider feasibility of the learning problem\\u2013 for example, as a function of the imitation network topology, communication channel size, agent size, etc\\u2013 in order for training to converge.Finally, although our work is inspired by imitation learning in humans, the extent to which simulations explain human linguistic phenomena is not clear.We intend for our work to only serve as a testbed to understand communication from a theoretical perspective.\",\"Orthogonal to the speed-ups discussed in this work, earley described an extension that we do not include here, which filters deduction items using k words of lookahead.While our deduction system runs in time proportional to the grammar size |g|, this size is measured only after unary and nullary productions have been eliminated from the grammar\\u2014which can increase the grammar size as discussed in apps.we described how to compute prefix weights only for earleyfast, and we gave a prioritized execution scheme only for earleyfast.The versions for earleyfsa should be similar.Computing sentence weights and prefix weights involves a sum over infinitely many trees.In arbitrary semirings, there is no guarantee that such sums can be computed.Computing them requires summing geometric series and\\u2014 more generally\\u2014finding minimal solutions to systems of polynomial equations.non-commutative semirings also present special challenges; see app.\",\".First, our approach relies on a reductive representation of the narrative texts, overlooking all traditional stylometric measures.The perception of literary quality is an intricate concept that relies on numerous factors, ranging from the stylistics, characters, plot development and pace, to cultural contexts.By reducing each narrative text to a subset of chosen features, our approach inevitably discards much of the richness and subtlety of works, while the narrow range facilitated by goodreads\\u2019 scores forces the models to discern nuanced differences in perceived quality among texts that may be considered generally good by readers.This clearly limits our understanding of literary quality, especially when it comes to the more linguistically or stylistically virtuous titles.Secondly, the reliance on goodreads scores as the sole metric of quality introduces bi- ases, as these scores are inevitably influenced by factors such as genre preferences and reader demographics.Finally, the analysis is based on a limited sample of english-language texts from the 19th and 20th centuries, potentially limiting the generalizability of our findings to other periods, languages, or contexts.For the same reason, our study cannot consider the potential impact of translation and its effect on the reception of the texts.At the same time, given the inherent complexity of these constraints and the subjective nature of literary evaluation, the performances achieved by our models in terms of r2 scores and mean squared errors, which would be modest for easier tasks, can be considered rather promising.Naturally, there is much that can be done from here.integrating stylometric and syntactic features, for instance, could provide additional insights into the complex nature of literary quality.Furthermore, we plan to investigate genre-specific patterns, as observing the performance of our models across different genres may reveal unique patterns and relationships that are specific to particular types of literature.Finally, we intend to use more diverse and sophisticated metrics than goodreads: exploring alternative sources such as anthologies, awards, and canon lists.Leveraging a richer set of indicators for literary quality\\u002fqualities, we hope to gain clearer insights into the complex interplay of factors that contribute to the perception of literary quality.\",\"In terms of the test sets, due to time, labor, and financial limitations, we are unable to construct large-scale test sets of the same size as the original, so the domain balance in the test sets is not fully considered, but the uniformity of writing style might have slightly alleviated this issue.In terms of the method, we empirically explore the possibility of chain-of-thought application in text generation.However, due to the stronger openness of generative tasks compared to pure reasoning tasks, generated summaries might be more sensitive to the form of chain-of-thought, which is a key point worth further optimization.\",\"Limitation the grm model still has some limitations.Even though our model brings some performance improvement to the contextual word embedding model , this improvement is relatively small compared to the static model.In some cases, grm may hurt the performance of bert slightly, because the primary objective of context-based word embedding models is to infer word meaning from contexts.The approach set forward in our study enhances their initial input word embeddings through word formation, and the benefits brought by this method are modest.How to efficiently improve the performance of contextual word embedding models when faced with oov words remains to be explored.\",\"This work has two main limitations.First, we only consider baseline models with similar amount of parameters, and pre-trained on similar scale of text corpus for comparison.While we are aware of recent models including t5 and palm , they either use huge corpus like c4 for pre-training or contain significantly more parameters than ours.second, we leverage spacy to segment sentences into words, which is rule-based using spaces, punctuations and other rules.This approach works well on english and many other common languages such as french, german and spanish.But for a few languages that do not use spaces to split words , it will be challenging to retrieve word boundaries.\",\"Our method has some limitations that should be acknowledged and addressed in future research.One of the main limitations is the restriction of the plm type to masked lms.While this model type has been widely used in previous studies, it may not be the only option.With the ongoing advancements in pre-trained large language models, it is possible that our method could be applied to a wider range of plm types.Furthermore, we have only considered three commonly used perturbation types in this study, future studies could investigate a broader range of perturbations and how they interact with each other in determining the constituents.These limitations provide an opportunity to further improve the method and its applicability in the field.\",\"Our work mainly focuses on cross-lingual sentencepair classification tasks.While it is directly applicable to single-sentence classification tasks but may require additional efforts to adapt our dpa framework to more complex cross-lingual tasks such as sequence tagging or question answering.Another limitation is that the proposed multilingual verbalizer in the dpa framework requires an external machine translator to produce the translated verbalizers.Finally, we limit the language set of the multilingual verbalizer to the set of target languages in a multilingual dataset.Extending this language set might give us greater improvement for cross-lingual tasks.\",\"To train ecola, we need to provide structured knowledge with aligned unstructured textual data to the model.Thus, we should either manually pair quadruples with event descriptions or use some matching algorithm to automatically build the pairs.The former requires human labeling effort and is hard to apply on large-scale datasets, while the latter would introduce noise into the dataset.Thus, ecola is currently tailored for domain adaptation and enhances pre-trained models with domain knowledge.There is still work to be done to let models be jointly trained on large-scale structured and unstructured data.\",\"Our conceptualization of the core task has some important limitations.first, in order to tie a character to a place, we require that both the character and the place are explicitly mentioned in the text.This simplyfying approach helps annotation and modeling but is inadequate against the general setting of grounding any character at any time in the story.Another limitation with our current approach is the assumption that the location of a character is independent at every instance in the story.It is because of this assumption that we can label every character and location co-mention without considering any other labels.In reality, however, location of a character at some time is highly dependent on the location of the character at a previous time.Finally, the spatial relationship categories are designed to be coarse.This is helpful in setting up the task as a classification task but collapses information that can be useful.For example, if a character is described to be standing outside the southern gate of a building, our current approach will assign the near label retaining only the aspect of distance and not the spatial orientation.\",\".Researchers and developers are encouraged to make justified accurate claims about their achievements.In addition, as the community grows and new practices are introduced, including all necessary practices in a single study is expected to become infeasible.Nonetheless, our framework provides a comprehensive reference to collect the necessary evidence for validating nlu evaluation.\",\"So far jgr has only been evaluated on the domains of summarization, conversational summarization, question generation, and dialog.It should be evaluated on a wider range of benchmarks, such as machine translation and code generation.And we have not explored jgr\\u2019s performance with extralarge language models such as gpt-3.because the generator of jgr samples candidates using auto regressive sampling, it may occupy relatively longer computational time and larger memory then the conventional mle training.Though the performance of jgr is satisfactory, we still want to improve its computational costs.\",\".Firstly, our method is based on the plms which require large gpu resources to train and infer models.We would like to adopt knowledge distillation technology to reduce the number of model parameters while keeping the performance as much as possible.Secondly, the summary generation process still lacks enough controllability even though we incorporate various features of users and products into the saliency estimation and auxiliary inputs of the decoder.\",\"While our analysis suggests that it models do not fully utilize instructions but instead learn superficial patterns from instructions, there are some limitations to our experiments.First, we only analyze a sota it method on the natinst-v2 dataset and t0 dataset.showed that their model can outperform other large models such as instruct-gpt and t0 , we did not analyze other it methods, such as rlhf in instruct-gpt.Secondly, since our analysis is conducted in the training stage, we cannot analyze private models such as chat-gpt.Also, we did not explore models larger than 7b parameters due to our computation resource limitation.This may miss some emergent abilities of large language models.Lastly, while we observe the models do not utilize the majority of the instructions by it, a certain degree of instruction understanding may already exist in pre-trained llms, which we did not study in this work.In conclusion, our work is a concentrated analysis to illuminate the potential vulnerability of the current it models and evaluation metrics.\",\"Our work on compo is subject to multiple limitations.The first limitation is around its scope when probing compositional operations.We only explored compositional substitution for topical snippets in conversations as an initial effort.However, there are many other types of conversation structures that can be leveraged such as conversation stages or specific discourse acts.Second, we used a set of external tools to process the conversations for augmentation, such as the use of c99 for topic split and action extraction.Although we choose to select widely-used tools with high precision, error cascades are inevitable.Furthermore, our approach may not be applicable to low-resourced languages since these pre-processing tools may not be available even in the first place for these low-resourced contexts.\",\"The pretrained bidirectional distillation transfers language knowledge through the nmt training process, a limitation of this method is that a computational overhead is introduced during training.Specifically, there is an extra language model forward pass to generate the pretrained bidirectional distillation objectives.Although we significantly reduce the computational overhead by designing a self-distilled language model, the overhead cannot be completely avoided.Fortunately, most computations stem from back-propagation when model training, and the introduced computational overhead only affects training time.Once the training is completed, the nmt has an identical inference cost as regular translation models.\",\".We first expound upon the limitations associated with selfsupervised learning on large web extracted corpora.Then we show that while xy-lent achieves strong performance on multiple multilingual benchmarks, when the downstream task involves unseen languages, the performance drops by a substantial margin.Finally, we show the potential limitation associated with a common methodology used for domain adaptation associated with leveraging these multilingual foundation models, illustrating how catastrophic forgetting exacerbates certail issues pertaining to low resource language performance.Training data xy-lent uses cc-100 which a static multilingual corpus extracted from common crawl for 100 languages., several data filtering strategies have been applied to remove duplicated documents, paragraphs with high ratio of punctuations, digits and profanities, the resultant data may still result in many potential biases requiring further analysis.Additionally, these issues might be aggravated for models that leverage bitext data, since the bitexts themselves are mined from web crawls, and thus potentially have all the associated biases, stereotypes and other associated harms.Furthermore, the raw data was compiled from static common crawl snapshots from january, 2020 to december, 2020 and hence may not include information about some of the recent events such as covid-19.Performance on unseen languages given the performance improvements observed with scaling, we investigate how it impacts extremely low resource languages which are not present in the pre-training data.In order to do so, we consider our model\\u2019s performance on the americasnli dataset which extends the xnli dataset to 10 indigenous languages of the americas.Table 5 presents the results on the americasnli dataset.As can be seen, xy-lent does outperform xlm-r, indicating that better representation learning also benefits these extremely low resource languages.However, we do not see an increase in performance while scaling our models.Specifically, the performance of xy-lentbase and xylentxl model is nearly the same, and substantially worse that the performance observed on the xnli dataset.This indicates that, while parameter scaling can help improve performance on languages that the model has seen during pre-training, it does not automatically improve performance in the extremely low-resource regime 6.Thus, while model scaling allows for improvements across numerous dimensions, it is far from a panacea, especially if not done in conjunction with data scaling efforts.Continued training for domain adaptation in pre-trained encoders in recent years, continued training on domain specific corpora has been considered a viable approach for domain adaptation of mlm style pre-trained models where the core idea is to continue train the pretrained model on domain specific corpora with the goal of improving in-domain downstream evaluation.We first show that this phenomenon can be extended to models pretrained with an electra style training objective.For the electra style models, using the same peak learning rate as used during pre-training, results in divergence.Interestingly, this neither happens for the generator of the electra model nor for the 6note that since the tokenizer is a sentencepiece tokenzier, there are extremely few unk words in the low-resource languages.Consequently, the poor performance is not explained by excessive occurrences of unk tokens mlm style pre-trained model.Thus, for an electra style continued training setup, we posit reducing the peak learning rate to be a crucial change.Table 7 shows the performance on the downstream task post the continued training approach and unsurprisingly it helps with improving in-domain performance.However, given the multilingual nature of such models, we test the multilinguality of these models before and after continued training; using crosslingual zero-shot xnli as a proxy for multilingual model quality.Table 6 shows the drop in performance across all languages pre and post continued training.We first note that this drop in performance is present for both mlm and electra style of models, and thus is not an artifact of the pre-training objective.We observe that the drop in performance is not uniform across all languages and the drop is worse for mlm style models.While we expect the drop in english performance to be relatively less, we do see that the drop is substantially more for the mid and low resource languages.While this can potentially be ameliorated by using techniques like adapters etc., we would like to draw attention towards the fact that general purpose continued training does suffer from this issue.\",\"Since methods based on pre-trained language models on text style transfer requires larger gpu resources and are not mainstream methods, we have not yet tested the effectiveness of our method on pre-trained language models.Moreover, since there is no multiple-attribute dataset in existing research, the applicability of our method on multipleattribute tst tasks has also not been verified.\",\"Limitation in hardpt, we focus on training specifically on hard samples while discarding misleading samples.However, it is worth acknowledging that these misleading samples may potentially contain valuable information.Additionally, finding quantifiable and interpretable evaluation metrics to accurately assess the model\\u2019s ability to identify misleading and hard samples is a crucial challenge.our aim is to maximize the utilization of all available information from the original dataset.\",\"While we have improved the asymptotic running time of a classic algorithm with regard to grammar size, the time complexity of our algorithm is still cubic in the length of the input.Our result follows the tradition of dynamic programming algorithms that trade time for space by memoizing and reusing pre-computed intermediate results.The usefulness of this trade-off in practice depends on the specifics of the grammar, and while the complexity is strictly better in terms of non-terminals, it will be most noticeable for denser grammars with many nonterminals.\",\"We now explain the limitations and potential risks of our work.First, it seems the knowing-how knowing-that task is a bit unfriendly to supervised learning methods as we only annotate the testing set.second, each user manual in oho only contains one user.However, there are a number of user manuals involving more than one agent, e., \\u201cinvite your friend as a new user and get cash back\\u201d.third, in addition to the textual content, many user manuals contain visual information like images and gifs.Hence, it will be more desirable to add such user manuals and study the knowing-how knowing-that task in multi-modal settings.\",\".most works used chinese and japanese datasets as testbed for training zp models.However, there were limited data available for other prodrop languages , resulting that linguists mainly used them for corpus analysis.However, zp phenomenon may vary across languages in terms of word form, occurrence frequency and category distribution, leading to learning bias on linguistic knowledge.Thus, it is necessary to establish zp datasets for various languages.most corpora were established in one single domain , which may not contain rich zp phenomena.Because the frequencies and types of zps vary in different genres.early works extracted zp information from closed annotations , which were considered as a sub-problem of coreference or syntactic parsing.With further investigation on the problem, mt community payed more attention to it by manually or automatically constructing zp recovery and translation datasets.the scarcity of zpt data remains a core issue due to two challenges: it requires experts for both source zp annotation and target translation ; annotating the training data manually spends much time and money.Nonetheless, it is still necessary to establish testing datasets for validating\\u002fanalyzing the model performance.Besides, pre-trained modes are already equipped with some capabilities on discourse.This highlights the importance of formulating the downstream task in a manner that can effectively leverage the capabilities of the pre-trained models.\",\"There are two main limitations to our works.Grammar constraint: the results of the structsp method at the 25 spis setting in the topv2 dataset suggest that the results of using grammar with low-resource data can be uncertain.The reason is that the extracted grammar from training data for low-resource setting is not general enough to capture the grammar of the new coming data.Therefore, for our structsp method to work effectively, the provided grammar should cover all grammar rules if possible.Prediction time: a recursive insertion-based strategy is used for prediction.This means that the output of the previous parsing step is used as input for the current parsing step, and this process continues until a terminal signal is encountered.As a result, parsing a complex tree with multiple intents\\u002fslots can be a lengthy process due to the recursive nature of this method.\",\"In this paper, we present a bidirectional generative framework for cross-domain absa that has achieved outstanding results on four cross-domain absa tasks.Although there is only one stage during inference, our method involves multiple training stages, including text-to-label, label-totext, and final training.These additional training stages not only lengthen the training time but also require additional computational resources, which may hinder scalability for large-scale data and result in a burden for the environment.\",\"The potential limitations of our model are threefold.First, the training process requires more computational cost as the model needs to conduct two forward passes for each sample in the self-distillation module.Second, there is still room for improvement to reduce the model\\u2019s overcorrection of legal characters.Third, the phonetics-aware sequence doubles the length of the original input, which demands extra computation cost at inference time.\",\".additionally, we acknowledge the need to address the issue of biased models, such as the ones trained on mafiascum, onlinede, and diplomacy, which tends to favor truthful labels owing to the label imbalance in these datasets, resulting in an f1 score of 0.to overcome this challenge, we could employ techniques like oversampling to rectify the class imbalance and improve the reliability and effectiveness of our approach.this includes understanding differences in the concept as it represents itself in these data and understanding differences in linguistic realization.Our unidecor dataset serves as a valuable resource for future research enabling standardized data comparison, transfer learning, and domain adaptation experiments.\",\"While our work makes a broad study on how to improve model efficiency our scope is limited.Our work is limited to the usage of bert-base and it is not clear how our compression approaches scale to more varied architectures like the sequence-tosequence models used by doct5 or more optimized models like roberta or compressed models like minilm.\",\"Limitation of existing lm prompting schemes, which rely on the static knowledge internalized in parameters; therefore, when such knowledge are incomplete, inaccurate, and outdated, llms may generate factually incorrect answers.To tackle this challenge, we introduced a novel knowledge-augmented language model prompting framework, which augments the knowledge for the input question from kgs directly in the input prompt of llms, with the fact retriever to inject only the relevant knowledge.The proposed framework is completely zero-shot, and versatile with any lms, without additional parameter updates and training datasets.We validated that our kaping yields huge performance gaps from the lm prompting model relying on its internal knowledge, especially with smaller lms, on the kgqa tasks.We believe our new mechanism for augmenting facts from kgs to the lm prompt will bring substantial practical impacts in generating knowledge-grounded answers.\",\"By building this task-specific dialogue system for kids, we aim to increase the overall quality of basic math education and learning at-home experiences for younger children.In our previous school deployments, the overall cost of the whole school\\u002fclassroom setup, including the wall\\u002fceilingmounted projector, 3d\\u002frgb-d cameras, lidar sensor, wireless lavalier microphones, servers, etc., can be considered as a limitation for public schools and disadvantaged populations.When we shifted our focus to home learning usages after the covid19 pandemic, we simplified the overall setup for 1:1 learning with a pc laptop with a built-in camera, a depth camera on a tripod, a lapel mic, and a playmat with cubes and sticks.However, even this minimal instrumentation suitable for home setup can be a limitation for kids with lower socioeconomic status.Moreover, the dataset size of our initial home deployment data collected from 12 kids in 12 sessions is relatively small, with around 12 hours of audio data manually transcribed and annotated.Collecting multimodal data at authentic homes of individual kids within our target age group and labor-intensive labeling process is challenging and costly.To overcome these data scarcity limitations and develop dialogue systems for kids with such small-data regimes, we had to rely on transfer learning approaches as much as possible.However, the dataset sizes affect the generalizability of our explorations, the reliability of some results, and ultimately the robustness of our multimodal dialogue system for deployments with kids in the real world.\",\"Our study is limited in scope, studying only classification and extractive qa tasks in english; the trends we highlight in this work might not generalize to different tasks or other languages.We also acknowledge that we only use bert-based models for our analysis, so it is uncertain whether these findings are applicable to other models.In addition, the overlap we describe in this paper is defined by semantic similarity rather than literal overlap between sentences and phrases.We are not claiming that this overlap is good or bad, rather we show that when the overlap is large, it is more difficult to evaluate model generalization.We note that there are multiple confounding factors in our results.First, while we highlight the role of dataset collection method in our analysis, the naturalness of data collection method is negatively correlated with task difficulty.As a result, differences in performance can be attributed to task difficulty as well as data col- lection method.Second, our study is limited in scope of similarity metrics and embeddings used to compute similarity.Using different embedding or metric can change the results.\",\".First, the fa model has advantages in computation but relies on an effective frequency selection strategy, which is difficult to design.We just simply select some manual frequencies for different datasets by experience.The more effective frequency selection strategy needs further exploration.Second, there is no theoretical guarantee that the orthogonal regularization can generalize to a 3-order tensor.Our or terms are only formally consistent with matrix orthogonal regularization, which has been empirically shown effective.\",\"Pragmaticqa is collected via crowdsourcing on english-language material from fandom.com, where community-maintained wiki pages are used as reading materials and basis for answering questions.Therefore, it cannot be guaranteed that the excerpts from fandom will be factually correct or stay unchanged over time, and in turn the answers in pragmaticqa are also not factually verified.Furthermore, techniques or models developed on pragmaticqa might not be generally applicable to non-english languages or non-entertainment topics without further adjustment or evaluation.More importantly, the crowd workers that participated in pragmaticqa are geographically limited to primarily english-speaking countries, and therefore might not represent typical pragmatic reasoning behaviors of people that speak different first languages or come from different cultural backgrounds.Therefore, it should not be treated as a universal standard for pragmatic reasoning in information-seeking conversations, but rather a single reference point.\",\".We conduct preliminary experiments to verify the influence of task transfer and vocabulary expansion on language learning in complex forms, and to explore the effectiveness of our proposed architecture, symbolic mapping, and we assume that language was formed through simple interactions in the early stage.besides, more advanced language properties and syntax are temporarily not studied in this work.\",\"One potential way to improve the extractive performance of a generative system is to explicitly model the likelihood of extracts during training.Driven by this intuition, we investigate creating a mixture of extractive and abstractive candidates for contrastive learning in brio.Specifically, we obtain extractive candidates with beam labeling proposed in xu and lapata , while the abstractive ones are from the original brio training data.Nevertheless, as we can see, this mixing method hurts both brio\\u2019s extractive and abstractive performance.we leave the study of a more effective extract-aware learning strategy for future study.Furthermore, we emphasize that the conclusions drawn in this paper are based on results produced on english datasets from the news domain.Even though these datasets are established benchmark datasets for summarization it is imaginable that other domains and languages may have produced different evidence.Despite this, the results remain insightful as the results show that extractive summarization is in fact feasible with modern abstractive systems.In future research, we look forward to shedding light on the possibilities and limitations of the proposed methods in a broader context.\",\"Csprom-kg successfully integrates both graphbased and textual representations in the kgc task, achieving substantial performance and efficiency improvement.However, similar to other plmbased methods, this comes at the cost of increased computational resources.In addition, we find that csprom-kg may occasionally collapse on small kgc benchmarks under specific random seeds.This is probably due to the nature of soft prompts, which involve much smaller number of trainable parameters, compared to fine-tuned models.However, we never see similar phenomena when training csprom-kg in the large kgc benchmarks.\",\"While our proposed method demonstrates high translation quality and constraint accuracy, it is important to acknowledge that the hard copy mechanism may not be suitable for certain morphologically complex languages, such as arabic.In arabic, phrases or terminologies often involve conjunctions or prepositions and exhibit varying morphological forms.Unfortunately, our proposed method is not capable of effectively handling such cases, and addressing this challenge remains an open area for future research.\",\".First, despite achieving promising results, our model needs to calculate pseudo ranking labels of the teacher which requires additional training time per epoch than the teacher.second, we directly use simcsebase and simcselarge as a multi-teacher in our implementation and experiments.However, how to choose the best combination of the teacher models is worth further exploration.It could help researchers to better understand the upper bound of improvements.\",\"While the results of our experiments seem sufficient to validate the concept and our general approach to bilingual distillation, we have not carried out a detailed systematic analysis of alternative implementations of the various aspects of our methods, such as different student model initializations, distillation objectives and hyperparameter settings.Furthermore, our bistil models are likely undertrained due to limited computational resources.Consequently, we do not claim our specific implementation of bilingual distillation to be optimal or even close to optimal.Areas that warrant further investigation toward realizing the full potential of this approach include the use of hidden dimension reduction, which yielded impressive speed gains for minilmv2 in our experiments, and other innovations in distillation such as progressive knowledge transfer.With the exception of improved efficiency, our bistil models inherit the limitations of the mmts from which they are distilled; notably, there is a discrepancy between the performance on high- and low-resource languages resulting from the distribution of data used during mmt pretraining.In this work, we have only considered english as the source language; some target languages may benefit from other transfer sources.here the challenge would be optimizing the balance of model capacity allocated to source languages versus the target language.\",\".Despite the surprising performance it achieves, our framework needs to be applied to large language models like gpt3 or palm.Inference with these models costs more time and budgets than fine-tuning models like roberta.Although diverse can significantly improve the accuracy of final answers, we still cannot guarantee that the reasoning paths produced by the language models are 100 percent faithful.This is the key challenge and future direction for this line of research.Diverse needs more labeled data with well-annotated reasoning paths to construct diverse prompts, and it also needs a training dataset for supervising the verifier.However, from another point of view, this limitation can also be regarded as a contribution that studies how chain-of-thought reasoning can be further improved if we have more training data than just a few exemplars.we use human evaluation to measure the quality of the intermediate steps in reasoning paths since few current works provide reliable frameworks to evaluate the quality of reasoning steps.\",\"One of the main limitations is that we used the standard liwc-based analysis approach, which is purely lexical and does not take into account the context in which a word appears.Consequently, many words that have very specific senses in the context of the ietf get miscounted as occurrences of liwc categories.This could be addressed by a more advanced method of mapping to liwc categories that would account for context.Another limitation is that we manually generated a filtering list containing words specific to the ietf.This list might not be exhaustive enough.Also, we were limited by not conducting an exhaustive hyper-parameter search on our models.We also understand that many emails are longer than 512 tokens and might have not been captured completely by our bert model.However, most of the emails do fit into this bert sequence length limit.We did not fine tune bert on the ietf data; this might have given better performance, although it is not clear if it would have given more insight: our main goal is not performance but analyzing\\u002fcomparing characteristics of existing models.It is also worth highlighting that the data used in this work is strictly in english, and the psycholinguistic categories in liwc are also based on english language.Hence, this study may be biased and not fully capture variations in linguistic traits that are culturally agnostic.\",\".Specifically, the selection of k values in the lrl module necessitates human involvement.Various types of data or entities may rely on distinct k values.While the majority of k values within a reasonable range lead to improvements in model performance, identifying the optimal value solely through human involvement poses challenges.Moving forward, we will investigate the automatic optimization of k values to enhance the model\\u2019s capacity for acquiring latent relations.\",\"Although this work aims to be as comprehensive as possible, there are several limitations to this paper.Our comparisons only consider neural openie models despite rule-based methods being very popular among downstream applications.This is because of the lack of recent surveys on neural openie methods and the difficulties we personally encountered when trying to determine which openie method was state-of-the-art.We acknowledge that there are many cases where rule-based methods may be preferable to neural models due to being faster or more tailor-made for a specific application.However, we feel that focusing on neural openie methods is not a detriment because we are interested in which methods work best \\\"out of the box\\\".Based on the results reported in these neural openie papers, we believe they are currently the best out-of-the-box openie models using the metrics we report in this paper on the test sets covered in this paper.The corpora we chose are all limited to english.As a result, our results are not generalizable to any downstream task that relies on different languages.In our experiments, we do not report results for the benchie test set or using the benchie metric.This is because the benchie test set uniquely can only be evaluated using the benchie metric, and the benchie metric can only be applied to the benchie test set.We do not feel that its exclusion hurts our final\",\"Limitation in comparison to other transfer learning methods of nmt, knn-tl incurs extra time costs and more processes to transfer knowledge from the parent model.This is a result of the requirement to construct a high-resource datastore utilizing large-scale parent data and retrieve it.On the other hand, knntl requires a substantial amount of storage capacity due to the storage of a datastore containing millions of entries.We employ the output representation layer for the alignment and the intermediate representation layer for the retrieval.This method justification is mainly supported by the results of model validation , which might deserve further investigation.\",\"Currently, our approach does not effectively leverage syntax tree information via gcns, a commonly used method for incorporating syntax trees in this task.Further research is required to determine the most effective way to integrate syntax tree information into towe models.\",\"We acknowledge a few limitations in this work.persona knowledge is very broad and our resource cannot cover all dimensions of personas, nor all attributes of these dimensions.We select five dimensions of personas that we found salient from background literature in human interaction, and we distill attributes for these dimensions from atomic2020, comet and instructgpt-3.These resources, while rich in knowledge, only represent a subset of possible background resources for the construction of peacok.Furthermore, the primary language of these three resources is english, making peacok a solely english resource.\",\".taking into account people with other occupations may lead to different results; finally, iii) only wikipedia biographies were considered: biographies from other sources may differ in style and thus pose novel challenges to the biographical event detection task.The research involved the collection of documents from wikipedia, which are released under the creative commons attribution-sharealike 3.the annotation of the experiment was not crowdsourced.All the three annotators are member of the research team who carried out the research as well as authors of the present paper.They are all affiliated with the university of turin with whom they have a contract regulated by the italian laws.Their annotation activity is part of their effort related to the development of the present work, which was economically recognized within their contracts with the university of turin.A data statement for the research can be accessed at the following url: \\u002f\\u002fgithub.\",\"Our model achieves outstanding performance in relation to chinese spelling correction.However, it has several potential limitations: errors of missing and redundant characters cannot be corrected by our model.The ptcspell model only focuses on spelling errors, and requires that the input text has no grammatical or semantic errors.The error-correcting language is targeted at chinese.The pre-trained model based on similar pinyin cannot adapt to other languages, while pretrained model based on similar character shape can adapt to other languages well, because the pinyin input method is unique to chinese, but character error due to a similar shape is a common problem in many languages.Nevertheless, we put forward the idea of matching the pre-trained model with error correction tasks, which is suitable for all languages.\",\"In this work, we propose to use the denoising score matching function to estimate the gradient of logdensity distribution, then describe the differences between the adversarial and normal samples by the denoising process of langevin dynamics.Although our method achieves very good detection performance , the actual denoising process requires multi-step iterative updates, resulting in a very slow inference speed compared to previous methods.In addition, the trained score network is highly correlated with the domain data, which makes it difficult to achieve good generalization across multiple domains at the same time.\",\".We also demonstrated how prefix tuning may have more difficulty learning sentence representations of augmented data.We further showed that contrastive learning can be a solution to these issues, and suggest that more work be done to find similar methods that can be more generalizable.Limitations although we present our results across multiple datasets and models, we have largely adhered to natural language understanding tasks.How effective the augmentation methods are on generationrelated tasks warrants further study.In addition, another limitation is that we do not study the level of perturbations needed to have a negative effect on prefix tuning.Knowing this can shed light on what particular types of transformations are a problem for prefix tuning so that they can be avoided.Our hypothesis here is that such perturbation levels are sensitive to many factors during training, such as the difficulty of the task and the characteristics of the data augmentation approaches applied, which could be difficult to be quantified.\",\"Our study has limitations in two aspects.First, multilingual transformers support a wide range of task types, and it is challenging to study our research question on all types of end tasks.We conduct experiments on two common types of end tasks, i.we leave the study on other types of end tasks in further work.Second, under pmid, we only consider the situation that the end-task models are obtained by finetuning public pretrained models.The cross-lingual transfer of black-box end-task models is also an interesting research topic to study.Besides, plugin-x reassembles the modules from publicly-available models rather than training from scratch, so it can naturally inherit the risks from those models.\",\"We report the following limitations of mildecoding.Mil model still suffers from the tradeoff between detoxification effectiveness and lan- guage model quality.Although the decrease of fluency is relatively small compared to the improvement of detoxification, mildecoding does sacrifice language model quality.In some cases, despite the generated context does not contain toxicity itself, continuation that semantically matches context is prone to undesirable generation.Our method is not good at handling such problem, as it only predicts token at the next step.Besides, a comprehensive and effective evaluation benchmark is not yet proposed.In most cases, toxicity is measured with a trained classifier.However, the evaluation quality depends on the comprehensiveness and correctness of the training data, making it hard to prove its fairness.As discussed in previous work , perspective api used in our work also has several shortcomings.\",\"Our approach requires access to planning information for each instructional text domain.In general, creating this information requires programming and domain knowledge to formally specify the planning constraints.However for high-value applications the effort associated with generating these planning domain definitions may be justified by their potential to help in generating more valid plan-based semantic parses.Having this knowledge is also crucial to allowing an agent or robot to execute the resulting plan and may be naturally available in many domains as part of the execution component.In the course of developing our semantic parsing model, we discovered that codex could generate valid planning domain definitions in a variety of output formats including the planning domain definition language.This may provide a path towards automatically generating planning domain definitions for novel environments or reducing the need for human annotators.\",\"Our work is the first attempt to explore how evidential deep learning can be used to improve the reliability of current ner models.first, we propose a simple method to treat hard samples in the dataset as oov\\u002food samples, enabling the model to detect oov\\u002food data with minimal cost.However, there is still a certain gap between these hard samples and the real oov\\u002food data.Oov\\u002food detection performance can still be improved by further incorporating more real oov\\u002food samples, for example, real ood data from other domains, well-designed adversarial examples, generated oov samples by data augmentation techniques, etc.Second, we evaluate the versatility of e-ner by applying it to mainstream ner paradigms.\",\"We make use of ms-marco, a resource that provides large-scale relevance annotations.However, as with most retrieval datasets, this dataset could contain annotation biases.Given the vast number of documents in the corpus supplied by the dataset, relevance annotations are sparsely distributed, with all other documents assumed to be non-relevant.Consequently, some relevant documents may be inaccurately labeled as non-relevant, leading to false negatives.A notable annotation bias in msmarco is that the relevant label correlates highly with the exact matching term.This bias poses a limitation during the training or evaluation stages.To appropriately address this annotation bias, we might need to reorganize the labeling process using either a human or a neural annotator, or we could aim to design and train a model that is resilient to such bias.We reserve this task for future research efforts.\",\"Of this approach might be its high computational cost to explore with \\u2018gpt3-scale\\u2019 lan- guage models , and we expect that this can be addressed through offline reinforcement learning techniques in future research.Limitations large language models over the gpt-3 have made significant progress in natural language generation, but applying the criticcontrol method, and exploring through these large language models are computationally too expensive.To address this, offline reinforcement learning may be a promising option to minimize training costs.Criticcontrol also has inference speed degradation because additional inference costs are needed like other controlled text generation methods.The potential solution may be to use the action-value predicting critic , which would allow for real-time control of various attributes without affecting the inference speed of the language model.Recently, the impact of instruction models on text generation has recently been highlighted in academic research.These models, which allow for control over the generated text via input manipulation, have become widely accessible on various attributes without extra computational costs.ethical statement we acknowledge that our reward-driven text generation system may lead to generating harmful or misleading content when used with undesired reward models.However, controlled text generation methods have the potential to address these ethical issues present in large-scale pretrained language models, for example, through the detoxification of language.Therefore, we emphasize the proper use of reward models to pursue the public good and believe that it is important to continue research in this area as these techniques can offer significant benefits.\",\"Our method has three major limitations.First, the auxiliary data corpus with label information might be rare.Recall that the corpus we used in this paper is the training set of different benchmarks.However, large-scale labeled data as the auxiliary data source might be infeasible in practice, hence it may limit the model deployment in real-world scenarios.Second, our method is trained and evaluated on english datasets.Additional data processing as well as annotation is necessary for other linguistic settings.Third, external unlabeled data with the same domain as the aste datasets are needed for the pre-training of the retriever.In our experiment, we choose two external datasets in the restaurant and electronics domains.If our method is applied to other fields, we need to find additional external data in the corresponding domain for pre-training.\",\"The current work does achieve better performance than previous methods, but processing only one slot type at a time also reduces the efficiency of the model.It would be an interesting challenge to generate answers for all the slots at once without degrading the effect of the model.Also, we will also try to apply our framework to more scenarios, such as ner and other tasks to explore the adaptability of the proposed method.\",\"Our work is limited by several factors.First, we conduct our work primarily using popular, publicly available dementia detection datasets, all of which are in english.Thus, it is unclear whether our findings generalize to other languages, especially with richer morphology where different predictive patterns may emerge.Second, due to the emphasis on feature-based models in most dementia detection work, we study only feature-based and instance-based da approaches.Neural da approaches may yield different findings, although they are less relevant for many current dementia detection approaches.Finally, we only study two backbone classification algorithms in our experiments.These classifiers are among the most common in prior work with our selected datasets; however, it may be the case that with a wider scope, other classification algorithms may yield different results.Collectively, these limitations present intriguing avenues for follow-up work.\",\"This paper argues that the proposed task of ellipsisdependent reasoning is a difficult challenge for gpt-3 models, which are among the most powerful current language models.The data constructed here is restricted to english, and furthermore is restricted to a single form of ellipsis, namely verb phrase ellipsis.It may well be that other forms of ellipsis may give rise to different effects, and it is also important to test the claims made here on other languages.\",\"In the process of conducting experiments, we find our method has some limitations.First, cif-pt needs to be performed on the dataset with speechtext pair.For some small-scale dataset that only contains speech and slu labels, our method needs to use external asr dataset to conduct the pretraining, leading to the increase of complexity of model building.In addition, in cif-pt, we need to ensure that the tokenizer of the pre-trained language model is consistent with the tokenizer in the asr task.However, there is usually a gap between the two in terms of vocabulary size.In consideration of performance, it is necessary to modify the tokenzier of one or both sides.\",\".We choose the bw for its intuitive and simplistic nature.Although the generalization experiments suffice currently to challenge transformers, realworld situations are more complicated.With the improvement of the algorithms, the need for a better arrangement of actions domains is emerging.In time, it could be beneficial to include several domains with various levels of complexities.for now, the synthesized english sentences are generated using a fixed template.Whilst being accurate without ambiguity, the resulting text is still quite formal.It would be valuable to add variety in the expressions without losing precision.as our demonstrations suggest, current lms still fall short on the generalization tests.We hope that our work will pique interests in the community towards reasoning about actions and change, and challenge approaches to undertake the fundamental reasoning tasks.\",\"There are two potential risks with our method.First, iss trades generality for efficiency by learning only task-specific representations.Consequently, it may not be suitable for other tasks.Secondly, our method is hardly practical for few-shot or zeroshot learning, as few or no task data are available as anchor points.\",\"Like existing short text clustering methods, we assume the real cluster number is known.Moreover, the time complexity of self-adaptive optimal transport is o, we are going to seek a new computation to reduce the complexity.\",\"Our method utilized pretrained entailed models and adapted them to other domains under zeroshot and self-training settings.firstly, we use human-designed suppositions for each task, which is less automatic than a direct, zero-shot adaptation of the models.Secondly, the self-training on some multi-class classification tasks is not as high as on binary nlu tasks, indicating the challenge of applying entailment models to multi-choice tasks.We would like to overcome this in the next step.\",\"While protoco works well with our consistency training for improving fact verification under fewshot and zero-shot settings, our work has some limitations.Due to limited resources, currently we were unable to conduct comparison with larger plms and examine if extremely large models have already developed the similar or better level of consistency for fact verification on their own.In addition, our experiments show that consistency training brings improvements in both settings using only gold evidence.However, the retrieved evidence in realworld setting can be noisy and incomplete.That said, the performance of protoco on non-oracle evidence requires further study.To utilize consistency constraints, protoco still needs to fine-tune the plms.Also, in zero-shot setting, the labels of logical variants are assigned with the predictions of the original claim by the base model, which could be inaccurate and thus affect the consistency training.\",\"Since the minimax objective requires using two separately trained models, i.the learner and the auxiliary, the design of the latter plays a crucial role in the overall stability of the training process.In particular, while having a very capable auxiliary model will naturally result in a more accurate and robust example weight distribution, it will also potentially lead to overfitting to certain training instances with high-losses.Another potential limitation of minimax training is that the existence of noise in the labels may cause the auxiliary to generate erroneous example weights due to high-loss noisy instances co-existing with the \\u201chard\\u201d examples containing meaningful patterns that contradict the shortcuts.Furthermore, we explore shortcut mitigation only for nli in english, and thus our method might not transfer to other tasks and\\u002for languages.Finally, the datasets we consider are well-used and -discussed in the literature, and consequently their shortcuts are well-known.Further testing is needed to establish whether our approach would transfer to datasets containing different shortcuts.\",\"While we cover a wide range of different factors of cross-lingual semantic parsing , we cannot include all possible dimensions along with these aspects.Furthermore, we focus on the linguistic generalization ability for semantic parsing because the questions are translated from the english datasets.\",\"We identify crucial financial signals in reports which can help financial practitioners to digest long financial documents efficiently.However, factors such as macroeconomics, stock prices, and public policies may affect how a financial practitioner views financial reports in practice.Confidential intelligence or social media may greatly affect the analysis results.Therefore, we limit our task to the scenario in which the content in the reports is the sole information available to users.Accordingly, to prevent bias in the annotation process, we acquire annotations from annotators under similar scenarios rather than from financial professionals.In addition, language partially constrains our methods since the data we used in stage s2 is in english; adding a machine translation module may have sub-optimal effectiveness of financial signal highlighting.This is mainly because the financial signals highly depend on many languagespecific knowledge or country regulations.\",\"As we only used english framenet as the dataset for our experiment, it is unclear how well our method would work with other languages or corpora.However, because the method is neither language- nor corpus-specific, fine-tuning may lead to better results with other datasets.Also, the method relies on a semantic frame knowledge resource, and annotation will thus be required if it is applied to languages without such resources.This study only considers core frame elements and does not show results for non-core frame elements.\",\"While huq outperforms individual aleatoric and epistemic ue methods for most datasets considered, for some, the effects are negligible.To understand this pattern, we analyze the difference between the training and test sets.We generate latent representations of instances in the datasets using a fine-tuned electra model and fit a logistic regression model to discriminate between train and test sets using these representations as features.Good performance of the discriminator indicates a covariance shift between the training and test data, while bad performance indicates that instances come from the same distribution.Table 4 presents f1 scores for this task aligned with the performance gains of huq-ddu in percentages over the best method from the pair \\u003csr, ddu\\u003e.As we can see, high f1 scores often correspond to low values of performance gains.This means that huq is unlikely to provide improvements to the base methods for the tasks with big covariate shifts.In our analysis, this is due to prediction mistakes primarily arising from ood instances, which are well-handled by epistemic ue methods.Visualizing the differences between the datasets using a t-sne decomposition of the latent representations , we can see that for implicithate and twitter, where huq does not provide improvements, some regions of the test data are not covered by the training set.For paradetox and toxigen, on the other hand, the training dataset completely overlays all regions of the test data, and using huq improves auc-rc on the base methods.\",\"Syng2g-tr encodes the syntactic dependency graph because the nodes of input and output graphs should be similar.in this paper, we initialise our model with the pre-trained bert model.As future study, larger and better pretrained language models will be used for the initialisation of syng2g-tr models, to achieve better performance.Additionally, future studies can easily extend our work to multilingual srl benchmarks.7this leads to a bert-based syntax-agnostic model, similar to shi and lin.\",\"One of the primary limitations of this work is that this is essentially an empirical study.Although we provide extensive experiments to show that the proposed approach demonstrates significantly better results in different settings, currently we do not provide any theoretical guarantees for this approach.Second, many of our experiments would not be easily reproduced in languages other than english, that lack sufficient linguistic resources.During this study we used the gpt-2 and gpt-neo language models, which have been trained on large amounts of english text.Finally, anecdotally we observed that this approach can also increase hallucination behaviors, which are a common issue with many text generation models.During application, one would have to take necessary measures to monitor the hallucinations produced by the model.\",\".First, on the problems side, it\\u2019s non-trivial to consider the order of all kinds of grounding knowledge, but we have only explored persona-chat.second, on the methods side, our framework is trainingbased, but we hope more lightweight techniques could be developed to improve the model\\u2019s robustness even without training the model.\",\"The current study was based on primarily american respondents, though approximately half had lived in china for at least a year.As a result, our data does not fully reflect the wide range of users or non-users of wechat.We encountered correlations between our participants\\u2019 familiarity with wechat, the individual wechat emoji, chinese culture, and the chinese language.Wechat use and emoji familiarity had the strongest impacts on emoji interpretation differences, but further work is needed to control for the impacts of these other factors.Additionally, we limited our analysis to emoji in isolation, as our goal was to assess whether the most basic interpretation of the emoji still relied on experience with the emoji.Emoji are sometimes used by themselves without text, so these results will apply to some real-world usage.But of course, in their general use, emoji tend to appear in richer conversational contexts and are accompanied by other linguistic information.As such, it is not certain that the observed user\\u002fnon-user sentiment differences will persist for emoji used in conjunction with texts.That said, the assessment of these emoji in isolation can serve as a baseline for future research examining how their sentiment differs in real-world conversations, especially in cases of sarcasm, hyperbole, or irony.\",\"There is much variation in literary writing and narrative styles, and our work here deals with a small, curated subset of this domain.The novels we analyze are all in the english language, and were published between the early 19th and early 20th centuries.The texts we analyze are largely uniform in narrative style.We limit ourselves to only those quotations that are explicitly indicated as such in the text by quotation marks, thereby eliminating more-complex styles such as free indirect discourse and stream-of-consciousness novels.We do not deal with nuances such as letters and diary entries nor quotations within quotations.The models we analyze for named entity recognition and coreference resolution use a fixed, binary formulation of the gender information conveyed by pronominal terms.Though the development of fairer, more representative models is constrained by current datasets, we note that there is encouraging progress being made in this area.\",\"Although our model can achieve better results compared to other works, there are some limitations of our model: \\u2022 our model cannot use shared dictionary and embedding on the encoder and decoder.\\u2022 though the encoder and decoder can use plms, the coordinator cannot use plm.\\u2022 the half-layers knowledge distillation still uses additional frozen bert.\",\"Our work is a comprehensive empirical study of a popular large language model\\u2019s capacity to perform in-context learning, relying on both task-specific and task-agnostic analyses and connecting the two via correlation\\u002foverlap investigations.We do not claim a causal link, i., we do not claim that an attention head that acquires the capacity to be an induction head will become capable of more sophisticated in-context learning associated with our downstream tasks.Making this claim will require a more deeper investigation that is outside the scope of this paper.We also do not fully understand why most attention heads seem to be unimportant for in-context learning and why there is an overlap in important attention heads across tasks and shots, which warrant further investigation.Other more obvious limitations to our work include our use of only up to 5 in-context examples, random selection of in-context examples for a query input and our choice of all monolingual downstream tasks.\",\"We used the asap data set to evaluate the performance of the proposed method.Although the dataset is well known and widely used, it has two major limitations.At first, the data size is small.Even with pre-training the model with a decently large data set , the interpretation of experimental results are limited by the data size.The second limitation is an inherited bias in the data set.Since the asap data set is labeled by human raters, the data set is biased by personal preferences.At last, the proposed approach requires a reasonably large pre-processing to extract all the additional features which hinders a scalability.Additionally, our work is limited to only measure creativity in expression but not in content.\",\"Firstly, we did not address the fundamental challenge of determining an adequate amount of situational information.It is very difficult, if not impossible, to describe all the situations required to perform rationale reasoning, so we need to give up somewhere, relying on the reasoning capability of nlp systems.Secondly, we did not use large-scale data or conduct an extensive search for optimal hyperparameters and prompts in our experiments as the primary goal of this study was to raise attentions to potential issues and benefits associated with situational informaiton.The models may have performed better with different configurations.We did not examined the capabilities of larger plms in conducting situated conversations at scale.In our empirical analysis, we opted for gpt-3 due to its transparency about technical details compared with later versions of gpt.Finally, while situational information can aid in the development of truthful and creative response generation systems, it does not address well-known issues associated with conversational technologies, such as safety and bias.In fact, poorly chosen situational information may even amplify undesired bias by linking two irelevant concepts together.To mitigate this problem, researchers and developers should exercise caution when collecting data and carefully monitor system output.\",\"Despite outperforming previous best methods, our method still has several limitations and substantial room for future improvement.First, the variety of modules is limited in the current reasoning environment.It would be interesting to introduce a wider variety of modules to make our method more general.Second, our method currently retrieves facts from a fixed corpus.While this is efficient for the specific domain, it may not be sufficient for questions not covered by the fact corpus.It would be more powerful if we retrieve up-to-date information using a modern search engine as our retriever.Third, in our experiments, we try our best to select the appropriate prompts to motivate gpt3 and chatgpt to generate reasoning steps and answers.With our prompts, gpt-3 and chatgpt can achieve high answer accuracy.But it is hard to guarantee that our prompts are the best ones to elicit the model\\u2019s capabilities completely.Finally, although scaling up the size of the language models may lead to emergent abilities , in this paper, we do not experiment with larger language models due to the computational constraints.To the best of our knowledge, our work is foundational research, and we do not find obvious risks related to malicious harmful effects, environmental impact, fairness considerations, or privacy considerations.\",\"Despite the progress we made, there still exist limitations in our work.On the one hand, we only investigated some classic dynamic networks and found that the proposed method contribute to the best performance in selected criteria.on the other hand, since we only consider moe and dy-conv in limited tasks, it would be valuable to consider other architectures ), machine learning methods ) and tasks ).This paper focuses on the higher efficiency of dynamic networks, e.Both the datasets and models used in this paper are publicly available and have been widely adopted by researchers.We ensure that the findings and conclusions of this paper are reported accurately and objectively.\",\"We would like to acknowledge the following limitations of this work.Our study setup only takes advantage of supervised data in the form of triples of \\u003cspeech, transcriptions, translations\\u003e.This is because we first and foremost want to investigate the effectiveness of pseudo-labeling in the most extreme case.However, the setup can be extended to be able to also rely on asr-only and st-only pairs.we identified two sources of domain mismatch: input length ranges and vocabulary mismatch.However, the solutions that we investigate directly target the length mismatch, without explicitly addressing the vocabulary mismatch.The latter is indeed more challenging to address, especially without incurring additional supervision.In fact, circling back to the previous item as a future direction, incorporating supervision in the form of asr or st can expand the vocabulary set, also addressing vocabulary mismatch.\",\"The experimental environment we used for testing our agents gives artificially generated natural language text, whose distribution of vocabulary, syntax, and semantic frames is controlled and limited to what the natural language text generators can provide.While we tried to include out of vocabulary for entities in our experiments, applying the proposed approach to natural language text in the wild, such as chatbots working with human, must be faced with issues such as out-of-vocabulary entities, relations, etc.We believe, however, approaching from controlled \\u201cwildness\\u201d is an important direction of the work for interactive-text agents.The experiments and embodiment of the method presented here also makes some assumptions on the underlying model of the environment.These are discussed in the problem definition and methods.Perhaps the most important is the assumption that the environment can be sufficiently approximated with logical states.We also used a deterministic planner so highly stochastic environments are currently out-of-scope.\",\"Although the proposed ckdst distills the knowledge of mt more comprehensively and efficiently from encoder representations and prediction logits, and obtains significant improvements over previous methods, it still has limitations: the batch size is not very large, limited by the memory capacity of the used hardware and the extremely long sequence length of speech inputs, which leads to a small number of negative samples used in ccrd and does not fully exploit the ability of contrastive learning.as we distill knowledge from mt to st, the performance of the pretrained mt model has an impact on our framework.\",\"Not applicable.Our approach is more of an up-to-date analysis into some of the existing techniques used, and does not pose any risks beyond the risks of general improvement of ai.\",\"Limitation , we would like to underline that our primary goal of this article is to highlight the ample possibility of data leakage and the impossibility of verifying the lack of data leakage with a closed model.As long as the trend of closed models and continuous training loop continues, it will become more challenging to prevent data leakage and ensure fair evaluation of models.Therefore, in order to ensure the fair evaluability of the models, we argue that the model creators should pay closer attention to the training datasets and document potential data contamination, create mechanisms through which the training datasets and models can be scrutinized regarding data leakage, and build systems that can prevent data contamination from user inputs.\",\".We use two existing datasets, sst and imdb , which are publicly available and commonly used in nlp research.We synthetically generate datasets of formal languages which does not require\",\"Due to the lack of data, our dataset is not comprehensive since it only consists of tang poems.Our model may not perform well on unseen data in other forms.our evaluation metrics and generation results for the machine translation tasks are not certified by experts in classical chinese, so the results and\",\".On the other hand, progressive distillation provides an attractive solution to this problem.Another limitation is that ho and salimans show that the diversity of generative models is degraded as w increases.Ideally we would be able to have a model that improves upon the fluency as well as the model diversity.additionally, we will investigate modifications to diffusion models that are inherent to discrete data.\",\"In this work, we demonstrate the effectiveness of the proposed diffusionbert.However, the sampling efficiency in unconditional generation still lags behind fine-tuned gpt and we observe a few sampled sentences lacking coherence when the preassigned length is large.The issue of inference efficiency is more severe in constrained settings in that mbr decoding samples multiple sentences for one source text.Though it brings significant improvement in bleu and rouge-l scores, the sampling time of one batch is several times that of unconditional generation.\",\".As cells of the table, the number of positive and negative token pairs is grossly unbalanced.In this work, although we alleviate the problem of unbalanced positive and negative relations by introducing the relation negative sampling strategy, the problem of unbalanced positive and negative token pairs still exists and needs to be addressed.currently, od-rte can only be applied to the relational triple extraction task.In recent years, the table-filling-based approaches have been widely used for many information extraction tasks besides the rte task, such as opinion mining and named entity recognition.\",\"We analyze the limitations of our work as follows.Firstly, although applying a million-scale simile knowledge base or large-scale simile sentences as reference makes our designed metric significantly more correlated with humans than prior referencebased metrics , our metrics are still reference-based and rely on the quality and scale of referenced data.additionally, since our metrics utilize a million-scale simile knowledge base or large-scale simile sentences as references, the efficiency of our method is slightly lower than the automatic metrics based on a few references.Nevertheless, this limitation does not prevent our metrics from performing systematic and scalable comparisons between sg models.\",\".For example, for parsing texts to lambda-calculus expressions for knowledge base question answering , one can similarly preprocess the schema items and typed values for more meaningful subword tokenization results.In addition, our experiments are based on t5.To further verify the effectiveness of our techniques, one can apply them to other pre-trained language models such as bart and gpt-2 as well.\",\"An obvious limitation is that our work relies on the typology features of languages.Some extremely rare languages might lack typology studies.Our approach is limited for these languages.Another non-critical limitation is that the technical contribution of our work is limited.After detailed analyses of position vectors, our methods for generating position vectors are not that complex, but we believe that an effective method is not neccessarily complex, and designing experiments to reveal key properties of position features and their connection with linguistic knowledge could still make solid contributes to nlp community.\",\"There are two limitations of this study that could be addressed in future research.First, this study focuses solely on the ed task.second, our study models the partially labeled training data instead of annotators.Indeed, the annotators produce the data, so building a model for annotators may be an essential way to address the partial learning problem.For example, an annotator may be more careless than others and generate more noisy data.Consequently, a robust model for the task should give a lower belief in the data of this annotator to improve learning.Lastly, our research raises no ethical issues because it focuses solely on the technical aspects of a normal information extraction problem.\",\"Firstly, due to the huge cost of large-scale plms, this paper only employs the t5-base as the backbone plm in our experiments, therefore only limited analysis on the effect of model scale is presented.However, we believe a larger model will benefit our method by providing better language understanding and generation abilities.Secondly, the synthesized canonical utterances need manually designed synchronous grammars, which are used to guide raas with knowledge about semantic representation language.acknowledgments we sincerely thank the reviewers for their insightful comments and valuable suggestions.furthermore, this research was supported by meituan.\",\"We propose and solve the feature space shift problem in text augmentation.However, there is a limitation that remains.Boostaug cannot preserve the grammar and syntax to a certain extent.We apply the perplexity filtering strategy, but it is an implicit constraint and cannot ensure the syntax quality of the augmentation instances due to some breaking transformations, such as keyword deletions and modifications.However, we do not need precise grammar and syntax information in most classification tasks, especially in plm-based classification., syntax parsing and the syntax-based absc , ensuring the syntax quality of the augmented instances is an urgent problem.Therefore, boostaug may not be an best choice for some tasks or models requiring syntax as an essential modeling objective.In other words, the syntax quality of boostaug depends on the backend.\",\"Of only considering the comparator when judging a simile.Meanwhile, ppl shows a higher correlation than the other two metrics in evaluating fluency, yet having a remarkable gap with the human score.To furtherly explore the concerns of human when evaluating a simile, we also compute the internal correlation of human scores.it means that having ground is also important in generating a creative simile, illustrating the necessity of interpretably retrieving tenor-vehivle pair in the vehicle-unknown setup.\",\"We conduct experiments on public datasets of finite sentence length, while generalizability to extremely long sequences or even streaming data has not been verified.Furthermore, the generalizability of the proposed quantization method to other tasks, including computer vision or speech recognition, remains to be tested.In addition, binarization and ternarization require bit-packing to have actual memory savings and dedicated hardware support for real-time acceleration, which is more of a hardware implementation aspect and not studied in this paper.\",\"Our experiments are mainly on traditional datasets and do not fully demonstrate the effectiveness of the method in end-to-end scenarios.In order to solve the low-rank bottleneck problem, this paper proposes a method of svd weight transfer, but this method is limited to matrix factorization and does not apply this method to more general lowrank factorization, such as tensor factorization.\",\"We view strong performance on our evaluation datasets as necessary but not sufficient to demonstrate human-like learning.Thus, if models perform poorly on our datasets , then we have strong reason to conclude that models are not learning in human-like ways.If future models perform better, such results would be consistent with human-like learning but would not conclusively establish that models learn as humans do, as they might instead be using some shallow heuristic that is not controlled for in our datasets.In other words, a criterion that is necessary but not sufficient facilitates strong\",\"Although dupmae is to learn representation instead of generative models, it performs pre-training on open web data.Therefore, it is also subject to potential ethical and social risks, like bias, discrimination, and toxicity.Besides, dupmae is pre-trained with comparatively limited amount of data due to the constraint on computation resources.Despite that it already achieves a promising retrieval performance at present, it remains to explore whether the performance can be further improved with the scaling up of pre-training data, by leveraging more high-quality datasets like c4 and openwebtext.\",\"There are majorly two limitations: firstly, in this work, we only consider the current-sentence text context-related prosody.secondly, other variables are not considered during the contrastive pre-training.One can explore similar approaches that connect prosody to other conditions such as speaker, emotion, etc.\",\"Given our focus on finding efficient moe models under computational constraints, automoe search space and evaluation has been restricted in scale to big-sized transformer models for benchmark mt tasks.A natural extension of this work is to explore the limits of moe models like switchtransformers and gshard that are significantly larger containing billions to trillions of parameters; as well as designing sparse and transferable efficient expert models for diverse types of tasks like reasoning, summarization and understanding.The limitations of this work are as follows: 1.Sandwich sampling , inplace knowledge distillation , and gradient conflict reduction are popular techniques to improve the training procedure of supernet.It would be interesting to study the impact of these techniques to improve automoe\\u2019s supernet.automoe uses the hidden dimension of intermediate feedforward network to modulate the capacity of each expert.It would be interesting to study other techniques to modulate expert capacity such as stacking variable number of hidden layers in ffn.The backbone of automoe\\u2019s supernet uses switch transformer, which adds ffn based expert layers and routes each token to exactly one expert.It would be interesting to: search for the number of tokens to route, and search for the transformer component to add expert layers.automoe\\u2019s search space contains classical transformer components such as multi-head attention and ffn layers.It would be interesting to add components that are efficient by design such as convolutional layer, flash , and g-mlp.\",\"The limitations can be illustrated from the perspective of task development: defi originally evolved from sentence-level work, as is clearly evident from the large number of sentence-related annotations retained in dlef corpus.Our work benefits from these abundant annotations and achieves huge performance improvements.Currently, there is a trend to gradually move towards end-to-end practice in event factuality identification.For example, the studies based on the dlef-v2 and eb-dlef corpora have attempted to use less annotation information.Although these efforts do not achieve competitive performance for the time being, it is an exciting research direction because it allows models to be more easily applied directly to realistic scenarios.The limitation of our work lies in the fact that it runs counter to the end-to-end concept, so we need more other work to apply the model to the real world, which makes our work less applicable.\",\"Our framework manually sets thresholds t+ and t\\u2212 in pseudo labeling by observations of data quality and hyperparameter searching.Dynamic threshold tuning or meta pseudo labels can be implemented to better filter pseudo-labeled examples.And the thresholds for different tasks can be tuned separately to improve the models\\u2019 generalizability.Recently, large generative language models such as gpt3.5 and chatgpt2 have demonstrated their strong potential on various nlp tasks including probing abstract commonsense knowledge with in-context learning.Due to our limited access, we did not conduct fully-scaled experiments in our paper.\",\"The creation of templates still requires native speaker expertise and an understanding of a language\\u2019s grammar.Morphological inflection models are imperfect so morphological forms may need to be enumerated to ensure highquality tests.while we design representative templates with thousands of permutations for each capability, a larger set of templates and arguments may be necessary to ensure a comprehensive coverage.\",\"We observe that although coaug outperforms baselines on multiple datasets, it is still prone to errors that emerge from the bootstrapping process.Specifically, our framework utilizes models to augment weak labels to the training set, and if the proposals are extremely noisy, training on noisy examples in future iterations will further exacerbate the ability of the framework to identify entities with high precision.\",\"This paper does not utilize any major linguistic theories of code-switching, such as.Our approach to generating code-switched texts replaces words with their synonyms in target languages, looked up in a bilingual lexicon.Furthermore, we do not make any special efforts to resolve word sense or part-of-speech ambiguity.To this end, the resulting sentences may appear implausible and incoherent.\",\"We summarized the limitations of fedlegal as follows: although fedlegal includes a variety of legal tasks with natural language understanding, more useful legal generation tasks should be included, such as legal court debate, legal case summary, etc.However, the tasks in fedlegal are more commonly used in the legal domain com- pared to these tasks.On the other hand, the manual annotation cost is also a limited factor.We will expand more useful legal tasks and also welcome contributions of new datasets to keep fedlegal up-to-date.We do not analyze the fl algorithm\\u2019s robustness attacks.We argue that it is impractical to have malicious court participants when multiple official courts perform federal learning.\",\"This paper shows that nar generation can be achieved from publicly available plms parameters.However, there is still room to be validated for additional pre-training.The performed pre-training in this study is small, and further investigation of whether the training steps and the size of the corpus are sufficient, and whether the self-training tasks other than permutation language modeling are effective.\",\"The first clear limitation of our approach is its textbased nature.This prevents important audio information, typically silences in speech patterns, from being exploited to generate subtitle breaks.A more complete system could be devised though, for instance by associating our text-based approach with the information provided by a forced alignment toolkit, whenever audio information is available.A simple method along these lines could be the following: 1.Apply our mlm-based segmentation but only generating a unique segmentation tag seg; 2.silence between two aligned words is above a specified threshold; 3.Traverse the text sequentially and replace seg with eol if there exists a previous marker of type eob, otherwise replace with eob.We left this use of our method in combination with audio information for future research, as audio alignment for subtitles typically involves additional factors such as non-literal transcriptions.Additionally, our method is limited in its adaptability to specific segmentation guidelines, which may be company-specific.The main adaptable parameters of our methods are the minimum and maximum parameters of the segmentation window, and the set of predefined punctuation marks over which masking is computed, neither of which could fully model idiosyncratic segmentation guidelines.However, in our experience at least, segmentation in real professional data tends to display varying degrees of consistency with respect to guidelines, and natural linguistic breaks seem to be the dominant factor for subtitle segmentation.A specific evaluation would be needed on data from varied professional datasets to determine the extent to which our method might deviate from specific guidelines.Finally, other aspects of subtitling, such as the recommendation in some guidelines for subtitles to appear in a pyramidal view, i.with the first line shorter than the second line, have not been taken into consideration in this work.Our aim was to evaluate our core lm-based approach without additional variables that can vary across guidelines and may also have led to results that are more difficult to interpret overall.Our approach could nonetheless be easily augmented with constraints on relative line lengths within subtitles, by incrementing the scores of segmentation candidates that respect this surface-level constraint.\",\"Since the propsegment dataset feature entailment labels for all propositions in a document, the label distribution are naturally imbalanced, which would potentially pose challenge for modeling.We observe low presence of contradiction examples in our dataset construction process, which could be a limiting factor for the utility of the dataset.Unlike previous nli datasets , we speculate that reference determinacy, i.whether the hypothesis and premise refer to the same scenario at the same time, cannot be certainly guaranteed and safely assumed in our case, which in part leads to low presence of contradictions during annotation.\",\"In our study, we have demonstrated the effectiveness of our proposed method on flan-t5 with different sizes.However, we have not yet evaluated its performance on llms, which possess an even greater number of parameters and have been pre-trained on larger corpora, thus potentially providing more accurate feedback for both caption adaptation and reinforcement learning.Meanwhile, it is worth noting that plms may contain certain biases, and training based on their feedback may amplify these biases.ackownledgement this work was partially supported by national natural science foundation of china under grant no.62222215, beijing natural science foundation under grant no.4222027, and beijing outstanding young scientist program under grant no.\",\".The datasets and tasks we focus on are from the xglue benchmark.The structured prediction tasks, namely named entity recognition and part of speech tagging, both have a limited number of training samples at 15k and 25.This is due to the difficulty in annotating on the token level, however it can still be viewed as a limitation when compared to the remaining sentence-level tasks the majority of tasks have at least 100k samples.below are a list of the main methodological limitations we perceive of our work: \\u2022 our method requires a teacher model that is already trained on the downstream task which can then be used to perform knowledge distillation.This is limiting when there are constraints on the computing resources required to produce the quantized model.\\u2022 we have focused on the problem of reducing accumulative qunatization errors which become more apparent the deeper a network is.However, this problem is intuitvely lessened when the model is shallow but perhaps wider.Hence the results may be less significant if the model is shallower than what we have experimented in this work.This can be viewed as a potential limitation has it introduced an additional hyperparameter to be searched to obtain best results on a given task.\\u2022 lastly, since intermediate layer outputs of the teacher network are required for self-attention distillation, we have to perform two forward passes during training.Since standard kld distillation only requires the output logits, it is common to store the training data teacher logits, eliminating the need to perform two forward passes at training data.However, this is not an option with self-atttention outputs as the storage required offline scales with the number of self-attention heads, number of layers and the size of the training data.\",\"Of bort , and is perhaps generally well-suited for the task at hand.in the present work, we were quite strict with regard to preparing the test split, withholding the most frequent english words because they appeared in our aphasiabank evaluations.As our focus turns more toward downstream tasks, we will update our holdout methods to prioritize a stronger pre-trained model, holding out only as much data as needed for validation.Further, seeing how our models did not train for an entire pass through the wikipedia data, we will adjust the training schedule and task configuration to ensure we get the most out of the pre-training stage.For our aphasia-specific application, we see room to improve the noise transform with more strategic approaches.Phoneme errors could be made more realistic with statistically or linguistically informed approaches.To better prepare a model for semantic errors, whole-word replacements could be made with semantically similar words.Limitations the models presented here were trained with the basic inventory of english phonemes found in cmudict.However, a more fine-grained phonetic analysis would require a pronunciation dictionary with more narrowly defined entries.Additionally, while this paper focused on models trained with english-only resources , the techniques should be applicable to non-english language models as well.Finally, from a clinical standpoint, the model we describe in this paper assumes the existence of transcribed input ; in its current form, this represents a limitation to its clinical implementation, though not to its use in research settings with archival or newly-transcribed datasets.\",\"As with many other model-based metrics, rise is best suited for evaluating offline due to the expensive nature of inferring with a large model.It is not as well suited as other metrics like rouge or bleu for evaluating during training or finetuning.additionally, as with other model-based metrics, it is possible that the models may have seen some of the data during pretraining as is in the eval datasets.We do not think it would be too significant, as the pretraining task is rather different than a summarization task and, more importantly, it does not include the gold reference.Thus the model would not be able to make such a connection easily despite having seen the data.We chose to work with the t5-family of models due to the ease-of-use for others to implement and improve upon our ideas.We would expect our ideas to work just as well with other models, such as bart, mbart, longformer, etc.Following recent works, we have studied the evaluation based on the summeval benchmark.\",\"The proposed method has several limitations that should be taken into consideration when employing it.First, the method relies on an existing model, e., k-means, which creates a dependency between the performance of the initial and the robust models.Second, the flow is not trained end-to-end, which can also limit its performance as end-to-end training allows improvement of the robustness of the whole representation.Lastly, to fully assess the effectiveness of the method, multiple metrics need to be examined.This can be a limitation as interpreting the results from multiple metrics may not be straightforward.However, it gives a more complete picture of the model\\u2019s performance.\",\"We have considered a variety of different llms in order to study attribution.However we have only considered a small sample of the different llm architectures and training strategies.This has been with a view to using a small but diverse set of llms.Of these 10 base models, we tested our approach to attribution on a controlled set of fine-tuned models.While a study that considers a wider variety and larger scale of fine-tuned models would be beneficial to the problem of attribution, the computation resources limited our study.Furthermore, in our assumptions in this work we consider that there is a one-to-one mapping between mf and mb.However, this is not necessarily the case.There could be an m-to-n mapping and also a model may be present in one set, but not the other.We believe there is rich space for further research in this area that can address these limitations, and further develop the problem of attribution.\",\"Admittedly, the main limitation of this work is the selection of k nearest neighbors.Intuitively, highquality nearest neighbors can make gnn learn the representation more easily.\",\"Time granularity: the granularity of time we test dynamite on ranges from spans of four years to months.After testing multiple ways to bucket our temporal corpora, we observed that the granularity of time only affected dynamite when there were insufficient documents in each time step.Specifically, we found that there must be at least 100 documents per time step to expect reasonably good results.Runtime: one drawback of dynamite is that its runtime depends on the number of terms required at each time step.However, this can be avoided by mining more than one term during each iteration of the framework.We also observed that dynamite, along with all other dynamic topic mining baselines, had a slower performance on datasets with longer text documents.Risks: dynamite is intended to be used as a tool to discover topic evolutions in temporal corpora suited to a user\\u2019s interests, represented as category seeds.We only experimented with dynamite in domains with trustworthy information.If dynamite was used in document collections that contain misinformation, it could have the potential to mine inaccurate terms.\",\"One limitation of our paper is that the exact distribution of the intrinsic tasks in the original corpus and the constructed data is still unknown.Knowing the distribution can offer a better interpretation of the effectiveness of picl, even of the strong performance of large language models.Besides, although we can find many constructed instances that share obvious intrinsic tasks , there still exist some instances where the intrinsic tasks are hard to identify.How to better evaluate the contribution of these instances to the icl ability or designing better filtering approaches to select more informative data for icl is worth studying.Our task-semantics encoder inevitably contains some bias because it is trained on downstream datasets, although we have tried to ensure a large number and diversity of the dataset collection.However, the final language model is pre-trained on the general corpus, and we add the full document loss, which eliminates the bias to some extent.Regarding computing power, we acknowledge that our framework takes relatively large training resources in the retrieval and pre-training process.Therefore, we did not conduct experiments based on extra-large language models.\",\"Limitation this major limitation of sen2pro lies in the computational cost due to the generation of several samples.Thus, improving the efficiency of sen2pro is a future direction.Besides, since we choose to concat the representation of different samples, there may be more natural ways to merge information from samples.\",\"The result of dictionary match and autoner based on our implementation is not comparable with 5 the results shown in the original paper.Because the performance of distantlysupervised ner is severely dependent on the domain dictionary used, we can not simply compare the performance of the methods if common domain dictionaries are not used.\",\"While evalm demonstrates that vocabulary augmentation with lrl task performance as objective requires different priorities from vocabulary augmentation for improving representation for its own sake, our work opens up several avenues for exploration.Our understanding of the potential conflict between fidelity of lrl word representation from wordpieces and lrl task class discrimination requirements remains far from complete, particularly when we extend from sequence-tosingle-label applications to sequence labeling and further to sequenceto-sequence applications.Perhaps, further experiments with mbert and other mllms will further our understanding of these trade-offs.While initializing an lrl word embedding using inithrl or initmix, we depend on automatic machine translation, which can be errorprone.Ranking by \\u2206h and picking a prefix fails to discount informative but correlated features.A more sophisticated formulation of loss of information owing to fragmentation, taking multiple lrl words into account simultaneously, may alleviate this problem.In the short term, these two limitations may deserve closer scrutiny.\",\"While our approach is effective at compressing models, it is not the most efficient.In order to discover the most optimal compression approaches and evaluate their performance performed hundreds of experiments.As a result, scaling our approach to every novel language understanding language model is not tractable.Another limitation of our work is we did not track the complete compute utilization of our entire experimentation process but we can provide some estimates.Experiments in pruning during fine-tuning leveraged a single v100 16 gb gpu and took approximately 14 hours per experiment.The pre-training of structurally pruned models with knowledge distillation required 4 a100 40gb gpus for approximately 72 hours.Pruning during pre-training with knowledge distillation required approximately 100 hours on the same setup.Task-specific fine-tuning happened on a single v100 16gb gpu and depending on the size of the task was anywhere from a few minutes to 20 hours.Based on all of our experiments we estimate 400 v100 hours of pruning during fine-tuning, roughly 16,000 a100 hours10 for pretraining, and assuming an average of 10 v100 hours per sparse transfer run, a total of 4000 v100 hours for sparse-transfer and sparse-transfer with quantization.\",\"This work represents an initial push to bring dogwhistles to the forefront of nlp and computational social science research, and as such, has many limitations.Our glossary is the most comprehensive resource to date but aims to document a moving target, as dogwhistles continuously emerge or fall out of use due to outgroup awareness.We aim to make this resource a \\u201cliving glossary\\u201d and encourage others to submit new entries or examples.We further encourage future research to develop models to automatically detect the emergence of new dogwhistles.Another major limitation in this work is that we identify as out-group members for nearly all dogwhistles in the glossary and have an adversarial relationship with many of the communities studied.Although our work would ideally be validated by members of the ingroups, they have very little incentive to share this information, as that would damage the dogwhistle\\u2019s utility as a tool for covert in-group communication.This work, like most prior work, is limited in that we operationalize dogwhistles as a static binary; we assume each term either does or does not have a dogwhistle interpretation and is categorically included or excluded from our glossary and analyses.In reality, dogwhistles are far more complicated constructs.For example, lee and kosse characterize dogwhistles along two dimensions: the size of their in-group and the degree to which their usage is conventionalized.Other axes of variation may include the level of out-group awareness, and the social and political risks of backlash to the communicator if the dogwhistle interpretation is exposed.It is even possible that audience members who hear a dogwhistle further recirculate it even if they themselves do not recognize the covert meaning.Finally, the current work is limited in the scope of dogwhistles considered: they are all in english with the vast majority coming from the u.\",\"We facilitate fair comparisons and realistic evaluations of recent wsl approaches.However, our study is not exhaustive and has the following limitations.First, it may be possible to perform model selection by utilizing prior knowledge about the dataset.For example, if the noise ratio is known in advance, it can be used to determine hyperparameters.In this case, certain wsl approaches may still work without access to extra clean data.Second, in this paper we concentrate on tasks in english where strong plms are available.for low-resource languages where no plms are available, training may not be that effective, and wsl methods may achieve higher performance.Third, we experiment with datasets from the established wrench benchmark, where the weak labels are frequently assigned by simple rules like as regular expressions.However, in a broader context, weak supervision can have different forms.generates weak labels through large language models.use hyper-link information as weak labels for passage retrieval.We have not extended our research to more diverse types of weak labels.Despite the above limitations, however, we identify the pitfalls in the existing evaluation of current wsl methods and demonstrate simple yet strong baselines through comprehensive experiments on a wide range of tasks.\",\"In this work, we mainly leverage control guidance such as action triples, dialogue acts, and discourse relations in structured forms that are extracted automatically from the corpus for training.we also compose multiple modules to generate the final conversation which might lead to a larger error cascade if there is some early noise.\",\"In this work, we focus on causal language modeling.It needs additional efforts to integrate the proposed methods into bidirectional attention, such as masked language modeling.Moreover, xpos introduces about 6% inference cost compared with absolute position embeddings, although it accelerates training convergence.\",\".One of the main limitations of our work is relatively smaller test set sizes.This stems from the way our perturbation experiments are set up - we can only use existing test sentences which already end with specific punctuation in order to measure the effect of deleting them, or start with sentences which do not have sentence final punctuation in order to measure the effect of inserting them.In general, a majority of the official test sets have sentences ending in full stops; this results in having a smaller test set to work with.This is also the same issue that presumably gives rise to sensitivity issues in the trained models.However, given that our focus has been on each particular punctuation, instead of merging them all together, we find that our test sets are larger than the ones used in previous work for each punctuation.Combined with the fact that we ensure to perform significance testing and manual analysis, we believe our results are reliable.\",\".for example, the current tokenizer of t5 tokenizes the phrase \\u201cskater midi dress\\u201d as [\\u201csk\\u201d, \\u201ca\\u201d, \\u201cter\\u201d, \\u201cmid\\u201d, \\u201ci\\u201d, \\u201cdress\\u201d].Here, the meaning of words \\u201cskater\\u201d and \\u201cmidi\\u201d is not captured in the tokenized text.We believe that we can overcome this limitation by pre-training t5 on e-commerce data which would help tokenizer understanding and tokenizing the e-commerce specific terms more correctly.\",\"While we aim for our hegel crowdsourcing methodology to be applicable to other languages, and in particular low-resource languages, the ui design and our analyses require knowledge of the intended language, as well as familiarity with the regions where it is spoken.Moreover, as our methodology relies on people\\u2019s familiarity with the places, it limits the cities chosen for the task and the participants that could take part, restricting the demographics of the participants accordingly.In addition, relying on people\\u2019s memory of the environment causes many of the descriptions to be too vague for humans to geolocate, thus, many of the descriptions were disqualified during the validation process as they could not have been resolved.The relatively low percentage of place descriptions that were successfully validated, raises the costs of collecting such a dataset.\",\"Although htf has achieved promising performance on removing spurious correlations, we identify the following limitations.Firstly, although htf encounters the smallest performance decrease among compared methods under multiple semantic interventions, the interventions still cause a performance drop.Therefore, more approaches can be explored to further improve the generalization ability of htf, such as increasing the scale of the backbone model or applying more informative hypothetical examples.Secondly, the experiments are only conducted in the financial domain due to limited datasets with sufficient annotation of hypothetical examples.thirdly, we are unable to compare the effectiveness of hypothetical and counterfactual examples because tat-qa does not contain both types, and constructing all counterfactual examples is impractical for us due to cost constraints.Note that we do not conclude any effectiveness relationship between hypothetical and counterfactual examples in the paper.\",\"Our current framework does not explicitly consider the temporal order via which word senses have emerged.In particular, in the data collection step, we construct source-target token pairs for each word type by randomly sampling a target sense from its sense inventory.An alternative and more realistic approach would be to sort all senses of a word chronologically by their times of emergence in history, and use the model to incrementally predict each sense of a word based on usages of its older senses.However, we found that it is infeasible to find accurate timestamps of senses in natural corpora at a comprehensive scale.Another approach is to have human annotators evaluate the plausibility of each ground-truth source-target token pairs that are automatically created in our data collection pipeline, which is a potential area for future consideration.\",\".Although we use the same verbal stimuli in the previous iat tests for creating text prompts, it is very likely that some stimuli that can represent the concepts are underrepresented.The approach we adopted for comparing the images\\u2019 distance might be biased as well.The current bias test procedure applies the visual encoder of openai\\u2019s clip model to measure the distance between images.However, it is unclear whether the image encoder may inject additional biases into the latent visual representations.The experiments conducted involve generating images that pertain to demographic groups, and all images were generated in compliance with the terms of service and guidelines provided by the stable diffusion\\u2019s license.The ai-generated images are used solely for research purposes and no identities are explicitly attributed to individuals depicted in the images.People\\u2019s names are used to generate images.We justify that these are common american names publicly accessible, and do not contain any information that can uniquely identify an individual.\",\"Sample efficiency in ar-lms, an nll loss is computed at training time for every token in the sequence of length l.However, in ssdlm, each time a pretraining example is sampled, the loss is computed on only b tokens leading to a lower sample efficiency than ar-lm.decoding speed since each block is generated by refining over several iterations, ssd-lm has a considerably slower decoding speed than autoregressive models.For example, given a context of 50 tokens , it takes ssd-lm 25 seconds to generate the next block of 25 tokens.decoding block size in this work, although we allow setups where btrain \\u0338= bdecode, the decoding block size bdecode remains the same across m decoding iterations, leaving space for a more flexible decoding schedule.larger scale experiments with different kinds of controls and their combinations can be done, as well as more sophisticated ways to incorporate them.In addition, we plan to explore alternative methods to continuously represent and add noise to discrete text.This work experiments with pretraining data that is primarily in english.\",\".The first is that it may not work well for some specific types of pet.For example prompt-tuning, which is only added on the input layer.We cannot use clnorm but only ilproj.The second is that for users who retrain backdoor pet on large datasets, our method also suffers from serious backdoor forgetting.\",\"While we evaluate our method on three distinct generation tasks, we acknowledge that we rely on a single language and a single type of structural constraints.Further work is required to verify if the proposed approach holds on other languages and other types of structural constraints.This work focuses on relatively smaller langauge models and does not address the impact and modes of usage of structural constraints on larger language models such as gpt-3.\",\".Additionally, our evaluation of existing benchmarks and methods is based on a curated set of papers and may not fully represent the state of the art in the field.Furthermore, due to the fast-paced nature of the field, our survey may not reflect the latest developments and advancements which may have come out close to or after the survey was conducted.Despite these limitations, our survey still provides a valuable overview of the current state and key trends in the field of mathematical reasoning and deep learning, and can serve as a valuable resource for researchers and practitioners working in this field.by providing a comprehensive overview of the key tasks, datasets, and methods that have been developed in the past decade, we give researchers and practitioners a clear understanding of the current state-of-the-art and help them make informed decisions about their own research.Additionally, by evaluating existing benchmarks and methods and discussing future research directions, we aim to identify gaps in the current state of the art and guide future research and development efforts towards more advanced and effective mathematical reasoning systems.Overall, our survey has the potential to contribute to the advancement of mathematical reasoning and deep learning, and have a profound impact on machine learning and natural language processing.\",\"Our model demonstrates commendable performance in terms of readability and relevancy, but it falls short in the factuality metric, this is one of the potential areas for improvement.Given more time, one of the directions that we might have explored, is the factuality based re-ranking which considers factuality as metric for comparison, instead of rouge scores, or considering both scores giving then certain weights.We have made substantial efforts to improve efficiency and reduce memory requirements, but large language models still impose significant demands on time and computational resources, which remains a limitation of our current work.Additionally, the constraint of a token threshold set at 512 posed challenges in our work.These limitations highlight areas for future research and development.\",\"The techniques in spansub are constructed on the basis prior works of extracting span alignments and clustering words in the training data according to their syntactic role.There is no generic solution for these problem applicable for all of the datasets at present, which requires users to spend efforts looking for preprocessing techniques applicable for their own datasets.However, the methodology of the proposed spansub is rather general to many different datasets and tasks.Besides, although we define eligible spans to try to alleviate additionally introducing noisy augmented data, our experiment result on geoquery shows that spansub can still slightly hurt generalization performance.Hence we regard that relieving the potentially negative influence of noisy augmentation is important to further improve this work.\",\"First, although our approach and existing methods for controllable text generation can improve the constraint accuracies, they are currently unable to achieve 100% accuracies in the vast majority of aspects.This makes them not yet applicable in scenarios with requirements of 100% control fulfillment.Second, there is still a gap between the automatic and human evaluation of text generation, which makes there a trade-off between precision and efficiency in the evaluation of controllable text generation.Third, although our approach reduces the mutual interference of plugins so that multiple plugins can be combined at a relatively small cost , this cost will not be zero, which puts an upper limit on the number of plugins can be applied simultaneously.Fortunately, for controllable text generation, the number of controls applied simultaneously is generally not too large.\",\"Despite being easily adapted to current deep learning architectures, one concern about multipleforward sampling methods is efficiency, since it has to repeat t processes to evaluate uncertainty in the stage of inference.another glaring issue is the focus on only english.Different languages may have different effects on uncertainty estimation due to e.Thus, some conclusions may vary according to the language in question.We hope that follow-up works will refine and complement our insights on a more representative sample of natural languages.\",\"We believe a potential limitation of this work is its reliance of curated samples from historical incidents.Due to the complexity of real-world conversational agents, the decision to introduce a new sample to the r\\u002fp set requires human expert involvement which could be costly and pose challenges in terms of reliability.Another challenge we faced after the deployment of this framework was managing the life-cycle of the collected r\\u002fp samples.In a dynamic environment, a regression or progression pattern may lose relevance over time.Therefore, we find it challenging to re-actively deal with retirement of such historical samples.\",\"There remains a problem we have yet to address: the abbreviation of loanwords in japanese.Japanese often abbreviates multi-word expressions after transliterating them into katakana.For example, \\u30b9\\u30de\\u30fc\\u30c8\\u30db\\u30f3 \\u003csu-ma-a-to-ho-n\\u003e \\u2018smart phone\\u2019 becomes \\u30b9\\u30de\\u30db \\u003csu-ma-ho\\u003e.For our method, we manually extended these abbreviated words to their full forms, but automating this process would be preferable due to the prevalence of these words in japanese.However, back- transliterating them presents challenges as they deviate further from their original english forms.We designed our method to specifically focus on back-transliterating of content words, unlike many other studies that focused on the name entities data.This is because the loanwords of content words are prevalent in japanese.However, names are also challenging as they are in other languages.Previous studies have suggested that a more sophisticated method may be necessary for backtransliterating names.\",\"We briefly mention some limitations of our work.First, we have only used a single dataset, and a single model family in our experiments.This is mainly due to the fact that the benchmark we use is the only publicly available dataset at this time to the best of our knowledge.We also solely focused on extraction metrics, but did not do a deeper analysis on the extracted sequences.A fine-grained analysis of extracted sequences could yield important insights for understanding memorization and extraction in llms.Similarly, we also did not analyze what our prompts converge to, and whether they yield explainable prompts at the time of converge.Such analysis can provide better insights as to why, for example, training prompts with aligned clm performs better that the basic clm setting.Finally, we believe the evaluation of our defense could be improved further by measuring other utility metrics on downstream tasks.\",\"Although the proposed method has verified the feasibility of the idea that constrains both naturallanguage and cross-modality spaces together, it is still necessary to explore more ways to better combine the output of two encoders.Third, our method involves multiple offline knowledge retrieval processes, such as retrieving relevant wikipedia passages, which will make it difficult to deploy our model as an online model.\",\"The depth+ metric resolves the problem of botgenerated wikipedia editions that have many botcreated articles and bot-made edits on their articles.Yet, the depth+ metric does not fix the problem of automatically translated wikipedia editions in the wikipedia project that their articles have been largely translated by poor direct translation or shallow template-based translation.The quantifications of these automatically translated wikipedia editions in the wikipedia project cannot be carried out as systematically as the bot-generated wikipedia editions, and examining each wikipedia edition separately is the only way to accomplish such quantification.Another limitation of the depth+ met- ric is depending on the active users metric, which dynamically decreases the depth+ metric values if there are no editing activities on the articles in the last thirty days.We preferred to use the total unique users who made at least one edit but do not have that figure, so we are approximating it with the already calculated active users metric by the wikipedia project.\",\"This work is constrained by the number of grounding phenomena analyzed, which is limited by the dataset domain and their straightforward automatic computation.We only focused on lexical alignment, the use of ellipsis and pronouns, disregarding other phenomena such as repairs , among others.With respect to the linguistic phenomena, we simplified the calculation of the lexical alignment by regarding only the last two turns of a conversation.In this manner, we omitted the dynamic convergence over several turns.It should be noted though that this was decided based on manual observation of examples, the majority of which exhibited lexical alignment in the last two turns only.This could be a limitation of the ocqa domain, and\\u002for a bias of the topiocqa dataset.Another limitation is that the form of crowdsourcing experiments we performed are mostly diagnostic of certain conditions on a given dataset, and does not reflect more organic real-use cases.An ideal setup would be to collect whole dialogues in the form of an extrinsic evaluation, which would be more costly to perform.\",\"Although our work shows that our cpll model can learn from crowd-annotated ner data well, there are at least two limitations.It would be better if we could design a strategy to learn a alpha adaptive value for each sample atomically.Second, though we mainly experiment on ner tasks, our model can be applied to all sequence labeling tasks, such as part-of-speech tagging , chinese word segmentation, and so on.We would like to explore it in further work.\",\".Computational cost: for our experiments, we used almost 850h of gpus.In future research, we could try to lower this cost by experimenting with prompting for lrc task in few-shot scenarios, which would also help when conducting the task for low-researched languages.Language: our experiments were conducted just for the english language.Thus, and with the advantage derived from minimal prompting of being language independent, in further research we would like to expand our experiments to multilingual datasets such as the ones from.Original dataset limitations: in line with , we found some misleading annotations in cogalexv dataset.This not only decrease the performance of the model but can also lead to hard-to-detect biases.Once again, few-shot tuning would decrease the annotation cost, making it possible to train with, although less, better-annotated examples.Additionally, synonymy remains the most difficult relation to capture, a more fine-graded annotation of the different kinds of synonyms could improve their classification.domain dependence: the limitation spotted by is persistent in our model.A richer domain annotation would be advised to better research domain bias in the lrc task.Aknowledgements supported by the spanish project pid2020113903rb-i00 , by dga\\u002ffeder, by the agencia estatal de investigaci\\u00f3n of the spanish ministry of economy and competitiveness and the european social fund through the \\u201cram\\u00f3n y cajal\\u201d program , and by the eu research and innovation program horizon europe 2021 through the \\u201c4d picture\\\" project under grant agreement 101057332.\",\"While we already manage to outperform the baseline, the pre-training data quantity is relatively small.Given the computational cost of training 30 models\\u2014six train sets, over five random seeds each\\u2014and testing them within inand cross- domain setups, we break the inspection of the optimal pre-training data amount at 24k instances.However we do not exclude that more pre-training instances would be even more beneficial for improving even more over the baseline.Related to computation cost constrains, we test our syntax pre-training approach over one set of ud labels only.including acl and compound, which present a lower, but still considerable amount of instances.\",\"The conceptualizer we propose consists of two core steps, i.The forward pass identifies the most associated target-language strings for a focal concept.Under such rare cases, conceptualizer will not work well enough.In addition, the genre of pbc is limited to religion and therefore the diversity of the concepts across languages is largely influenced.Nevertheless, pbc, as far as we know, provides texts in the largest number of low-resource languages.Pbc is thus a good fit for our goal.In this work, we select 83 concepts, including the swadesh32 and bible51, representing a wide range of interesting crosslingual concepts.The runtime for computing the results for one concept in all languages is around 10 hours on average.The relatively long runtime, however, can prevent us from exploring more interesting concepts.We find that the concreteness of a focal concept can be a contributor to the stability measure.As we use english as the source language for representing the focal concepts, we naturally resort to concreteness scores from english language ratings only.In addition, the analysis is carried out from an english perspective.Nevertheless, as we want to compare different languages, we have to use a unified source language.Theoretically, we can use any language as the source language and represent the concepts in that language.We therefore plan to use other languages, e., chinese, or some low-resource languages, as the source language in future research.\",\"In our study, we tested the chatbot on the hindi database, which humans heavily annotated.Thus, when the database size is enormous, the scalabil- ity of the annotation approach is a critical question.Since the questions and answers could be possible in different languages, it will require considerable effort to translate them and, at the same time, preserve their context.In our study, we observed the success ratio of the developed chatbot to be 70% for hindi queries.However, it is not indicative of its performance in different natural languages.For a given user query , the performance of our best approach for the faq-retrieval system is highly dependent on the number of different relevant questions existing in our ashafaq database for the given q.Considering the large number of user queries that can be asked in the healthcare field, the small size of our ashafaq database is a significant reason behind the instances where our method fails to suggest relevant questions to the user.Moreover, our work does not analyze the quality of answers present in the asha-faq database.Hence, a user study would be required to analyze the questions\\u2019 diversity and the answers\\u2019 quality in our asha-faq database.\",\"There are two limitations to this work.The total number of known and unknown intents are predefined, requiring an extension in real-world scenarios; the frame knowledge is predefined and, therefore, inflexible to address complex intents.In addition, some user queries have no frame in framenet matching.There are additional computation costs for frame knowledge learning.The model fine-tunes two bert models in the training stage and runs sentence-bert in the evaluation stage.The pre-training stage of our model lasts about 10 minutes, and clustering runs for 90 minutes on clinc with a 75% known intents ratio, both using a single nvidia tesla v100 gpu.\",\"This work has some limitations regarding the architecture and the data used: our model assumes that masked modeling could be used as rehearsal and anticipation tasks; however, other approaches could also be effective.We use transformer layers, so our model scalability is tied to the scalability of the transformer model.Also, our approach relies on obtaining additional annotation from pre-trained models for the masking process, so we are limited to the misprediction of those models.We only tested our model with the english language; further exploration of other languages would be valuable to validate the language-independent functionality of the model.\",\"Hyde has yet to be tested in a large-scale and multisite setting, which may offer more generalization challenges.Furthermore, an evaluation of notelevel classification performance was not conducted.Although we expect that hyde would perform well under such an evaluation, this would require heuristics to aggregate multiple mcms per note.\",\"The limitations of the findings in experiments 1 and 2 have to do with the relatively small scale of the study.We experimented with two books and, while the findings were broadly consistent, it could be that results would not generalize to other books.Experiment 2 was conducted with a specific group of readers in a specific context of implementation; studies with additional groups of readers are needed to evaluate generalization of the findings.Another limitation of our experiments is that the dynamic model of lexical experience is evaluated only as an aggregate index per passage and not as a predictor for specific words or types of words.In particular, the model predicts a slight increase in surprisal of function words if their density in the story is generally lower than in the background corpus.This assumption may or may not be correct; further experimentation is necessary to evaluate the surprisal model in more detail.We thank a bea reviewer for pointing out this limitation.\",\".We note that directing the substitution to candidates with matching grammatical categories incurs additional information leakage that is not accounted for by our modification.Too remedy the unaccounted information leakage, one could recast the candidate selection through the exponential mechanism.\",\"We address several limitations with regard to our work.First, the publicly available datasets used in our experiments are limited to english.Documents in different languages might require different segmentation techniques and may contain unique characteristics in terms of vocabulary size, data sparsity, and ambiguity.Secondly, we only evaluate the quality of the topic models in terms of coherence and diversity.\",\", perform poorly as a standalone model for long-tail classification.These results can be improved by priming the model with an entailment predictor through the usage of a prompt.The baseline shows strong performance independent of the llm, as it operates on a closed label space.The capabilities of the baseline can be enhanced by further explicitly priming it with a entailment relation through a llm.Rows in which t0pp is initialized, or primed with e are indicated with primed.Priming the model showcases improvements across all datasets for macro f1.For accuracy, priming the model shows benefit in two out of three datasets.In figure 4, we show the results of top-5 predictions for the wos dataset.1we observe a significant drop in performance when we utilize the 3b parameter variant of this model as l.all results are aggregated in table 1.it is important to highlight that prompt variation led to stable results for our llm.The variance upon utilizing bart-mnli is negligible across prompts.The best results are observed upto top-4 predictions on both accuracy and macro f1 for our method, when the entailment prompt is enhanced with a greater number of tokens corresponding to the output of l).The variation between our method and the baseline is much greater for top-1 predictions, but top-5 prediction variance is negligible.\",\".the deepblueai we have used is a deep model, meaning it takes much time when inference.78 seconds to identify simple words from 10,000 sentences with an average length of 8.due to time constraints, we do not perform a human evaluation of the output of llms.\",\"Due to various constraints, our experiments are only able to cover five non-linear novels and nine linear novels, listed in table 7.This pales in comparison to the thousands of novels typically expected of large-scale studies in digital humanities, whose scale allows them to make generalizable claims regarding narratives or literary history.one key challenge to scaling our dataset would be data availability.The use of non-linearity in fiction is predominantly a 20th century phenomenon, which suggests that many non-linear novels will not be in the public domain for some time to come.2 is its assumption that a novel\\u2019s chapter divides provided by its author could be thought of as a form of \\u201cgold standard\\u201d labels for model validation.This claim of authorial control and \\u201cauthority\\u201d over the text has been thoroughly problematized in literary studies since the emergence of poststructuralism , while analogous suspicions have been raised in natural language generation against the assumed reliability of human evaluators.Unfortunately, the author\\u2019s input is the only operationalizable criteria for ground truth available to us within the scope of this study.\",\"The limitations of this work can be concluded into two points: to obtain the associations between semantic elements, semsin needs to transform the texts into the corresponding semantic graphs.Existing methods can only transform single sentences into semantic graphs, and cannot parse texts containing multiple sentences.Therefore, this method is not suitable for identifying causal relations between events in different sentences.Semsin only exploits the semantic structures of the texts and does not utilize external knowledge.External knowledge is also important for the eci task, and simultaneously exploiting semantic structures and external knowledge is a good direction for future studies.\",\"The proposed method for improving llms is a post-hoc re-ranking approach, and we do not improve llms themselves due to the difficulty of fine-tuning llms.Besides, we improve the ability of constrained language planning for smaller models from the perspective of building task-related datasets, but do not consider investigating the model itself, other than adopting retrieval augmentation.In addition, because automatic metrics for generated text are limited, the automatic evaluation of this paper may result in an overestimation or underestimation of the mentioned methods, though we attempt to mitigate this by incorporating a moderate amount of human evaluation.Despite the advanced planning capabilities of newer language models, our work remains significantly valuable to the knowledge distillation of these llms into smaller and more cost-effective models.We also discover several limitations of the proposed coscript datasets.First, the specific goal explored in this work only inherits from an abstract one with one extra constraint.However, in real-life situations, complex planning may involve multiple constraints, which we do not investigate in this work.Another limitation of coscript is that our dataset is generated from instructgpt, and thus the data distributions may be biased to favor causal language models.This is a common issue with machine-generated datasets, which we address by manually curating coscript\\u2019s validation and test sets.Furthermore, there are still some incorrect samples in the training data without manual correction due to the limits of budget and time.Last but not least, we only consider whether the script can be executed at the human level.The script execution for robots is unstudied in our work, and there still exist huge gaps in transferring complex human language to one that is understandable and executable by robots.\",\"In this first effort, opinesum was demonstrated for the english language.there are two language specific components\\u2014the proposition identification rules, and the textual entailment model.For the latter, there are multilingual resources such as mt5 models and multilingual entailment datasets which are good starting points.The proposition rules are much more language specific.Very recent work has introduced a corpus and learned model for proposition identification , and future research in languages other than english could strength this component.A second noteworthy point is the scalability of the silver data creation.3, we perform a quadratic number of entailment queries per item.In this work, this was of the order of a few billion.We used an apache beam pipeline to scale our computation using a lot of parallel computation on cpus.Readers must be aware of this computation when applying such an approach for their work.However, note that the processing only needs to be performed once for training data creation.\",\"To understand the gap between our automatic data generation method and fake news written by humans, we expanded pn-silver to different sizes and compared the performance of robertalarge when trained on these generated datasets and the human-written fake news dataset, snopes.Note that since the timeline17 dataset only contains around 4k samples, we additionally crawled new york times news articles as an input to our generator for the \\u201c5 times\\u201d to \\u201c10 times\\u201d experiments.The results are shown in figure 3.Although the detector performance at first improves as we add more silver training data, it reaches a plateau after the size is increased five-fold.This illustrates that while our approach is more effective compared to baseline generation methods, there is still a clear gap between our generated articles and human-crafted fake news, likely in aspects such as style , intent , and falsehood.Despite the advantages of our generation approach, as compared to previous methods, it is uncapable of generating other propaganda techniques covered in , such as straw man.Thus, our method is not generic enough to handle all types of propaganda techniques within a unified framework.Moreover, our approach is limited to generating english-only news articles, and cannot be applied to other languages.\",\"There are two main limitations to our study.The first is that the stimuli used were limited to those provided by urbach and kutas\\u2019s study.This is because, as stated, we wanted to be able to compare the patterns in the language models\\u2019 predictions to the patterns in the human n400 response., or any others that have previously been studied.The other limitation is in the models we were able to use.Crucially, we were not able to access models larger than gpt3 175b such as palm 540b.This is important because recent work has shown that some inverse scaling patterns become u-shaped with such larger models.\",\"This paper examines the anisotropy and outlier phenomenon only for a few, relatively similar, models.The isotropy-increasing transformations are nondeterministic and have to be calculated post-hoc based on some set of embedded data, which may not be practical for applications where inference is done on individual or small batches of examples.Since we specifically consider sentence representations, we first average over word embeddings before calculating the mean and standard deviation for outlier analysis.This in effect reduces the sample size and leads to a smaller standard deviation, making our analysis more sensitive to even slight outlier dimensions.Another reason to work with relatively small datasets is to make computing the transformations simple and fast, but this may limit the ability of these transformations to generalise.\",\"We foresee two limitations to our work.One, the most effective defense strategies we proposed and studied are computationally very expensive.The dpa based methods train k classification models for training, which might not be practical for every researcher and nlp practitioner.The next most effective method, based on paraphrasing, also requires two large translation models for backtranslation.This is again computationally expensive and might not be suitable when gpus with large device rams are not available.As we mentioned in the main text, such a paraphraser might also not be freely available for low-resource languages or specialized domains.Second, we only evaluated the defenses on textual backdoor attacks.Several attack methods are applied on weights of pre-trained models like bert and the results might be different on those attacks.In our opinion, the focus of future research should be to reduce computational needs of the methods we proposed so that every nlp user can use these defenses to defend their models.\",\".First, while we have provided an extensive ablation study for gec-depend, there are many more low-level optimizations that can be done to further improve the results.In a real life application, one would be encouraged to investigate these optimizations.Second, obviously, non-autoregressive models, including gec-depend, still lose to state of the art autoregressive models.While the existence of this gap may be inevitable, we believe that it can be significantly reduced in further work.\",\".But any llm distillation will show a similar trend.Existing kd methods are highly customized to the specifics of the teacher model, and require additional pretraining, fine-tuning, or data augmentation.Our approach is much simpler and agnostic to both architecture and task.We ran our experiments on an rtx3090 gpu with 24gb ram which cost only $0.11 an hour, which is considerably cheap compared to other approaches that include teacher fine-tuning.We showed that our method is particularly effective on small datasets, and competitive with other kd methods which are much more computationally intensive and tailored to the teacher.A possible reason could be since the fine-tuning of bert on small datasets like mrpc, stsb, or rte can be unstable , eliminating it makes the kd more robust and improves the results.All other methods such as tinybert or patientkd use fine-tuned teachers.Distilbert does not use a fine-tuned teacher, but it is only limited to students with a hidden state of 784 due to the cosine loss it uses and lacks generalization across architectures.\",\"The limitations of the presented system derive directly from the pre-trained models they are based 3 sci-five-radsum23 on.Flan-t5 was pre-trained on a large-scale text dataset and was not assessed for existing biases.As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.Thus, it is not recommended to utilize such systems in any application without first conducting a thorough evaluation of the safety and fairness concerns that are specific to the particular application in question.\",\"One major limitation of this study is its input modality.Specifically, our model is limited to textual inputs and ignores other modalities.Open and domain incremental lifelong learning across modalities is more realistic and challenging.Fortunately, we can obtain robust features of different modalities via multi-modal pre-training models.\",\"\\u2022 we currently rely on gold annotations for attribute marking, which are not always available depending on the dataset.However, ramp could be easily extended to unsupervised settings through llm feature attribution , i., extracting salient tokens driving the attribute prediction.This approach builds upon recent techniques in unsupervised language generation metrics.\\u2022 besides the choice of in-context examples, prompting is also sensitive to their ordering and the design of the template.We refrain from tuning example orders and templates to avoid introducing too many variables.\\u2022 multilingual llms perform competitive mt out of the box for languages seen during their pre-training.However, we noticed that bloom 175b produces better en-it translations than xglm 7.5b even though it is not listed as a training language of bloom.This could possibly be due to typological similarity between italian and the romance languages included in bloom training.\\u2022 multilingual llms like the ones used in this paper require larger gpu resources for inference than standard bilingual mt systems.\\u2022 one test set we use provides only two gender values , but we do not intend to imply that other genders do not exist.\",\"There are several limitations to mathgpt, both practical and fundamental.First, the model depends on an external method for converting mathematical expressions to opts, currently being latexml.The conversion method is imperfect, which limits mathgpt\\u2019s capabilities as it will be presented with many distorted expressions during training and at test time.Furthermore, the conversion process is slow and requires dataset-specific engineering to accommodate, making it difficult to deploy the model across many datasets.Second, because mathgpt outputs trees rather than sequences, it is fundamentally difficult to evaluate and utilize in text-based settings without a highly accurate tree-to-text converter.The tree-to-text converter is yet another imperfect process in the pipeline, although it could be improved to a reasonable degree with significant engineering effort.Third, because mathgpt has additional components and requires more information per token than gpt-2, it has higher space and time requirements that make training more expensive.Finally, because mathgpt is pre-trained on highly formal and structured mathematical content, it may struggle to generalize to student-generated mathematical language, which is often error-prone and may exhibit very different patterns.\",\"Our framework relies on the availability of translation system and unlabeled data in the target language, which can not be applied to languages without any unlabeled text or translation text.The knowledge distillation step requires a certain amount of unlabeled text, while it may struggle in cases where only few hundreds of unlabeled sentences are available.It would be interesting to combine our label denoising framework with data augmentation techniques in such scenarios.Besides, the boarder application to other low-resource languages, such as masakhaner 2.\",\"For now, the superiority of the proposed two-stage inference speed-up method cannot adapt to inductive datasets since the generated sequences are difficult to map to unseen entities.like the other text-based kgc methods, our ghn lag behind embedding-based methods on fb15k-237 dataset.claims that many links in the fb15k-237 dataset are not predictable based on the information in the kg and we hypothesize this may harm the training of textbased models.\",\"We explain model pathology from a classification perspective, but the pathological nature may exist in language models for performing various tasks, such as reading comprehension, textual entailment, and visual question answering.Although our proposed regularization technique may be applicable to various tasks, we have only investigated its effectiveness in classification problems.the proposed method also leads to more time-consuming training, primarily due to the generation of adversarial examples, while only a minimal amount of time is spent on generating out-of-distribution examples.\",\"Limitation one limitation of this paper is that we do not validate fedpetuning on large scale models, , llama , vicuna , etc).Although the parameter efficiency of petuning is more impressive on larger pre-trained models, we have to consider the limited computational resources of clients in fl, making it challenging to deploy such a large-scale model.In addition, with the increasing size of modern pre-trained models, the community needs to design fl-friendly petuning methods.In this sense, our work can serve as a benchmark and guide for future exploration of petuning in fl.\",\"In the augmentation-driven self-training, we implement the data augmentation with random masking for simplicity, since augmentation is not the focus of this work.And wang and henao has explored more fine-grained data augmentation strategies, which may further improve performance.\",\"A large part of the dataset that we used in our experiments are semantic annotations for relatively short sentences.So we don\\u2019t know really know how our multilingual pre-trained language-meaning modelling for drs parsing and drs-to-text generation will work on longer sentences.In our experiments, we converted meaning representation in the sequence notation and modelled them with natural language texts in a seq2seq manner and masked tokens in the drs sequence randomly.Perhaps a more natural way is to model drss as graph structures and let training objectives directly utilize any structural information from drs.A graph structure would also eliminate the explicit order of concepts that is present in the sequence notation.Although we say that the drss are languageneutral, the concepts in the vocabulary are based on the english wordnet.As a result it might be the case that non-english words do not have a direct correspondence to an appropriate synset, but the number of such cases is likely very small.The only language dependence in drss are literal occurrences of proper names in cases where they differ across languages.One way to remedy this is to add alternative spellings to the meaning representation to make it completely interlingual.\",\"We should point out that the gpusq-tlm compression scheme is highly related to the nvidia gpu\\u2019s features to support gpu-friendly 2:4 fine- grained structured sparsity with various data formats.So if the gpusq-tlm compressed models are deployed on the different gpu types without such support, the deployment efficiency may not be as high as expected.For example, the last-generation v100 and t4 gpus have no support for structured sparsity, so the deployment efficiency is lower than a100 gpu.We should also point out nvidia agx orin chip also support gpu-friendly 2:4 fine-grained structured sparsity as a100 gpu and mainly support edge device use scenarios like autonomous driving.So, in theory, we can also deploy the transformer-based language models on the agx orin chip.However, the large language models need to consume large on-chip memory, so they usually cannot be held by a single agx orin chip.For a100 to represent the server use scenarios, we can use multiple a100 gpus for parallel execution, but for agx orin, we usually only have one chip for the deployment device.That\\u2019s why we do not test the gpusq-tlm compressed model on the agx orin chip.\",\"Our method overcomes degeneracy in instructional videos under the assumption of the existence of textual instructional scripts describing the exact instructions of instructional videos.Thus, our method is applicable to instructional videos having such recipe documents.However, we note that similar documents exist for various types of instructions other than cooking, such as topics in other datasets , e., how to jump start a car, or change a tire.\",\"Persona extractor first, we need to clarify that our definition of persona is not exactly psychological, the role an individual plays in life.As a result, like previous studies , pec ), the format of persona is flexible and variable.1, there are still some issues with the model we use to infer persona information.For example, we sometimes get information that contradicts the facts.And also, there is occasionally unrelated content, as with commonsense reasoning.Furthermore, we cannot guarantee that we can infer all of the persona information that appears in the conversation because much of it is frequently obscure.And when extracting persona information, we only use what the user said previously and remove what the bot said, which results in the loss of some conversation information.The reason for this is that we have discovered that if we use the entire conversation, the model frequently has difficulty distinguishing which persona information belongs to the user and which belongs to the other party.Is not yet available, we have not compared other methods of extracting persona dynamically from the conversation.\",\"We would like to claim our limitations from two perspectives: application-wise and technical-wise.Application-wise: gda needs annotations to finetune t5, which requires more computing resources and manual labeling costs than the rule-based techniques.Technical-wise: our \\u201coriginal sentence restructuring\\u201d and \\u201coriginal sentence pattern approximation\\u201d tasks rely on the efficiency and accuracy of pre-ordering rules and parsing methods.Although current gda show effectiveness, we still need to find more efficient pre-ordering and parsing methods.\",\"Part of our analyses and experiments is based on our parallel-semlink dataset, which provides parallel annotations for propbank, framenet, verbnet, and verbatlas.We take the opportunity to remark that this is a constrained setting, as these resources cannot be mapped 1-to-1 without losing information.As such, this setting may not provide the full picture of how these resources compare against each other.However, we also believe that a setting like this can at least provide an intuitive idea of the role of a linguistic resource in crossinventory generalization.another limitation of our work is the small size of challenge-srl.Even though challenge-srl contains only about 300 sentences, it features almost 2000 predicate-argument pairs, and this is a number that is sufficient to show the inability of a current state-of-the-art system to generalize across predicate types.We acknowledge that a larger benchmark may have provided further insights.However, we also note that, in our case, increasing the number of annotations would hardly have brought us to a different\",\"Inference speed at the same model size, the latencies of grain on different tasks are relatively large compared to the methods like cofi and tinybert.This is because grain generates models with different head size, and the computation of these heads are not parallelized.Thus the resulting models are slower than the models with uniform attention structures.This problem could be relieved by introducing model structure regularization at a higher level or by some engineering techniques, such as merging heads with the same or similar size into a large matrix to increase parallelism.Backbone models grain is designed for transformer-based models.Although the transformer is one of the most popular building blocks of nlp models, there are many other promising structures.The effectiveness of grain on model compression is possibly correlated with hardware lottery or software lottery.In addition, we have only tested our method with the standard multi-head attention mechanism.Transplanting grain to other attention mechanisms is possible, but the effectiveness has yet to be tested.\",\"We discuss a few limitations of our work.One limitation of self-improved is its complexity in usage.The data augmentation process involves generating predictions for the entire training dataset with a large beam size, resulting in a time complexity of o, where n is the train dataset size and k is the beam size.another limitation is that self-improved has only been applied to encoder-decoder models in this work.However, it is also applicable to other types of auto-regressive models, including encoderonly models, which are commonly used for tasks such as code completion.A few models can be named are gpt models , codex , codegen , etc.\",\"At its core, normbank is a collection of logical operations on unique constraints.Consequently, one practical limitation stems from the issue that some situations cannot be reasonably expressed as a set of constraints.While theoretically all logic can be decomposed into and and or operations, the logic may be too challenging for an individual to formulate, or the set of constraints themselves might be too large and unwieldy.The latter is problematic, because language models have a finite input token capacity, and for the set of constraints to be digestible, they must fit within that capacity.Relatedly, if the logic to encode constraints become more sophisticated, ensuring that logic is not unnecessarily duplicated will pose a greater challenge.Additionally, certain properties of normbank like the role and behaviors may be challenging to succinctly describe.Further work will be needed to ascertain how these can be incorporated or to more clearly define situations that are out of scope.Since normbank is the first to encode non-monotonic situational norms, there was no other available benchmark that is directly analogous to ours.Instead, our primary evidence for normbank\\u2019s utility is in table 3, where human evaluators confirm that models trained on normbank can reliably learn to make new inferences about non-monotonic situational norms.Other follow up studies should consider training larger normative reasoning models, and\\u002for engineering better prompts for expanding normbank.Relatedly, we have no data to speculate about the long-term evolution of real-world norms relative to this resource, nor the rate of decay in the reliability of normbank.\",\".Under our proposed mbrd framework , we need to make o comparisons to find the candidate with the lowest expected risk, assuming |c| = k.if we are using lightweight, non-neural alignment metrics, we might not necessarily need to worry about the overall computation time; however, if we are using a learned metric such as bleurt as our utility function, then the generation time might be slower, especially if we have a large candidate pool.In practice, mbrd operates under the assumption that we have access to a good generative model.In our experiments, we primarily made use of codex, which we know to be a model with impressive generation and in-context learning capabilities, but we also considered a few other text generators such as instructgpt, pegasus, bart, and t5\\u2014and we found that these models too benefit from the use of mbrd.Due to the 12if the utility function util used in mbrd is symmetric, that is, util = util, \\u2200a,b \\u2208 v\\u2217, one can then reduce the computations by half.In all our experiments, the outputs were english-language texts; similarly, we used learned metrics that can be used to compare english-language texts.The general mbrd framework, however, is applicable to any language or modality.large language models may exhibit a variety of biases due to their pre-training data.As with many other nlp tools and applications nowadays, it is conceivable that malicious actors might use these models to generative negative, harmful and hurtful content about certain groups of people.Our mbrd approach does not enhance or support the generation of toxic content in any way.On the contrary, mbrd can be used to eliminate biased and toxic outputs and to mitigate hallucination issues in language models if appropriate utility functions are used in eqn.Such investigations are, however, out of the scope of this present paper.\",\"Despite the effectiveness of our proposed method, it still has two main limitations:.Generative data augmentation methods need to use the original htc training set to fine-tune the backbone generative plms.Then, they need to go through an inference stage to generate data.Both the training and inference stage need more gpu resources, which increase carbon emissions.Although the data generation is usually complete offline and does not improve the time cost of online progress, we leave how to relieve the need for gpu resources as future directions.Although we conduct experiments on three widely used htc benchmarks, the language of all these benchmarks is english, which has limited morphology.The effectiveness of our proposed method on language with varied morphology needs to be further confirmed.\",\"It is possible that there is a hidden effect caused by the language pair direction, model selection, or training data and its size.However, our results bear high statistical significance for cases where we desire high correlation and low statistical significance where we expect low correlation.Assured by this and concerned by the large cost of training a large number of mt systems, we did not experiment with larger data or other language directions apart from limited additional experiments in tab.\",\".1, we observe that a significant amount of data points are needed for the calibration task training.In lm-toast, we effectively utilize the whole training set for the calibration task.Some learning paradigms assume only a small number of annotated samples at first, which limits the effectiveness of lm-toast.For example, in active learning , only a very small number of samples are available at the beginning most of the time, and models need to rely on them to find informative unlabeled samples to annotate.The vanilla confidence scores can be effectively utilized in this setting.However, lm-toast may not learn the calibration task well given very limited samples, resulting in poor performance in searching informative samples.We plan to investigate the calibration task in the few-shot setting and bring out the potential in lm-toast to make it suitable for more tasks.\",\"The most evident limitation of this research is that is has only been demonstrated on english corefernce.Using a lemma-based heuristic requires using a lemmatization algorithm in the preprocessing phase and for more morphologically complex languages, especially low-resourced ones, lemmatization technology is less well-developed and may not be a usable part of our pipeline.Application to more morphologically-rich languages is among our planned research directions.In addition, all our experiments are performed on the gold standard mentions from ecb+ and gvc, meaning that coreference resolution is effectively independent of mention detection, and therefore we have no evidence how our method would fare in a pipeline where the two are coupled.A further limitation is that training of the crossencoders still requires intensive usage of gpu hardware.\",\"While promising, hint comes with several drawbacks related to its ease of use.First, hint takes advantage of the fact that instructions are often long, and often we want to perform inference over a larger amount of examples with the same instruction.If either of these items are not true in a setup, then hint is unlikely to provide a large benefit over simply including the instruction with the input text.This can be seen in the smaller compute savings provided by hint for p3 in table 2.second, while hint is computeefficient at inference time, it is far more costly to train, as it effectively requires running the underlying model together with the hypernetwork for every batch.This means that while hint may be useful for practitioners with limited compute budgets, it may be difficult to train hint models with the same limited budget.Finally, we train and test on english data only, and do not explore the generalisation of our approach to multilingual setups.Considering the success of hypernetworks in multilingual settings , we believe this is a promising direction for future research.As such, while promising, hint is limited by certain assumptions made about the length and format of instruction-augmented data, and we hope further improvements of the method work towards loosening these assumptions.\",\"Although instructor significantly improves the baseline gtr performance, we were only able to use four negative examples during the model finetuning process due to computation constraints.However, negative examples have been shown to play an important role in contrastive learning.additionally, we do not have enough computation resources to apply multitask instruction finetuning to gtr-xxl , which is also an area for future exploration.At the core of instructor is the instruction design.While our current unified instruction format has demonstrated effectiveness, future research can explore other instructional elements to further improve performance.For example, previous work have shown that incorporating demonstration examples and explanations can be beneficial for instruction-finetuned language models.\",\"Our experiments are limited to 3 datamuxed pretrained models due to compute constraints.More pre-trained models with different n \\u2019s would provide prumux with more options to improve throughput and would allow us to conduct a more detailed evaluation of auto-prumux.Prumux uses cofi as its model compression method.Experiments with other methods could improve our understanding of the interactions between model compression and data multiplexing.\",\"We present a method for question answering using a kgqa retriever and a language model reasoner.Limitations of our method include a lack of an integrated entity resolution system when training our kgqa model: we instead rely on annotated entities from the datasets.While our kgqa architecture is robust to new entities added at test time, it does require retraining when new relations are added to the kg or if a different target kg is used.Additionally, our results are based on training and evaluating on one dataset at a time; training on a mix of datasets could lead to better generalization, however this is not tested.\",\".We focused on fact verification, which formulates the task sentence-pair classification.Our findings may hold for certain domains where the task format is similar.We did not apply beam search on input reduction, which limits us from searching multiple versions of the reduced claims having the same length.We investigated three widely used regularization methods: temperature scaling, label smoothing, and the confidence penalty.\",\"Our work is predicated on hypothetical models of human cognition.These models are still under development by cognitive scientists and need to be validated in more realistic domains.Our method assumes access to a simulation of the environment, which may be costly to construct in some domains.In general, instruction generation agents pose substantial risk to humans.Previous studies have shown that humans can become overly reliant on ai instructions and commit disastrous mistakes.It is thus important for practitioners to comprehend the constraints of our experimental setting.Our experiments take place in a coarse simulator of real-world indoor environments, which restricts the action and perception of the human listeners.Due to the expensive cost and the large number of agent variants, our human evaluation remains limited in terms of population scale and diversity, and the comprehensiveness of the questionnaires.As each instruction is only evaluated by a single human, we have not investigated the variance of the interpretation of the same instruction among different humans.In addition, human evaluators may \\u201cguess\\u201d a path even if a part of the instruction is misleading or impossible to follow.Hence, the path-similarity metrics may not reflect faithfully the quality of the instructions.Nevertheless, results shown in table 4 of \\u00a7a.5 indicates that instructions generated by our agents are almost as easy to interpret as those generated by humans.But again, these results are still subject to the constraints of our annotator population.To deploy our method, practitioners should carefully re-evaluate its safety and effectiveness in conditions that closely emulate the deployment conditions.\",\"[20] to ensure better performance and reduce the effect of the irrelevant factors.Also, since the toxicity classification tasks are not easy even for a human, there are noisy data inside the chosen datasets since we found that we disagree with some of the humanannotated labels by manual checking.Furthermore, the hsol and jigsaw datasets are imbalanced in terms of distributions of different classes.Therefore, modifications can be made to the loss function in the same way as focal loss [16] or dice loss [15] to mitigate the influence of data imbalance.\",\"Although our model allows users to interpret which parts of the input document are most relevant to the model\\u2019s prediction, our model does not allow users to interpret which text spans of the input summary contain errors.We use the summary in table 4 as an example.If the model can indicate the text span \\u201ca school in northern ireland\\u201d contains errors, it will be easier for users to correct the summary, potentially benefiting factual error correction systems.introduced an auxiliary task to extract erroneous text spans in summaries, but their method requires expensive text span ground-truth labels.Locating incorrect text spans in the summaries without requiring spanlevel training labels remains unexplored.Another limitation of our model is that it does not allow users to interpret the uncertainty of the prediction results.\",\"Although our model obtains satisfying results, it also exposes some limitations.First, for a fair comparison to other models, we mainly carry out relevant experiments on pdtb 2.due to the lack of baselines on pdtb 3.0, further analysis and comparison cannot be conducted.Second, in our experiments, we can find out that the hlr method does not improve the top-level or bottom-level results effectively, indicating that with the increase of the level, the refining method is insufficient to continue to generalize the bottom-level labels and further improvement should be made according to the specific features of the idrr task.Third, due to the limitation of space, this paper does not focus much on semantic weight for the refining of sub-labels.This is a very broad topic involving the rationality of the discourse relation annotation and the interpretability of the label embeddings.We will conduct a further study which may appear in our next work.\",\"One limitation is that the efficacy of our method is shown with massive llms in this paper.However, we note that our method is based on only model inference with them and is already considerably cheaper than other adaptation methods.Furthermore, our method seeks to improve llms: while the technology itself is ethically neutral, we acknowledge that there are various social and ethical risks of potential misuse, especially given the powerful generative capabilities of the llms that have become increasingly accessible to a broader audience.We argue that both the prospective end-users and researchers should be aware of these concerns when using our method in order to mitigate these risks.Methodologically, we note that an integral component of our algorithm is self-consistency.We rely on the expectation that it reliably predicts accuracy, which essentially places an expectation that the model uncertainty should be reasonably wellcalibrated.While we have indeed found this to be the case for almost all considered tasks and models, additional investigations might be required to ascertain their general applicability.Given our reliance on self-consistency, for tasks where selfconsistency does not lead to significant gains, the performance improvements with our method may be limited.An example of this could be tasks with very small label spaces where \\u201cconsistency\\u201d in outcomes may be achieved much more easily even if the model simply outputs random predictions.second, while cosp improves performance in an overwhelming majority of cases and is significantly less sensitive to the original zero-shot model performance compared to baseline methods like autocot, there still exist exceptional cases where it fails to improve, especially when the tasks are too challenging in zero-shot setup \\u2013 we argue that this is also due to the general, inherent limitations of the llms.However, both continual improvements on the foundational models and provision of some human guidance should alleviate this issue.\",\".We will also apply ot-based alignment to problems related yet with different constraints and objectives, e., crosslingual word alignment and text matching.Limitations in this study, we used standard and basic word embeddings to highlight the characteristics of the different ot problems on unbalanced word alignment.This limits the capability of phrasal alignment.Similar to figure 3 , we binned all the test samples across datasets according to their phrase alignment ratios and evaluated the performance of the supervised ot-based alignment methods.13 specifically, we regarded one-to-many, many-to-one, and many-tomany alignment as phrase alignment.Figure 5 shows the trend of the f1 score according to the phrase alignment ratios.Obviously, the f1 score degrades as more phrase alignment exists in a sentence pair.One of the straightforward ways to improve the phrase alignment is exploring pre-trained language models enhanced for span representations and sophisticated methods for phrase representation composition.In addition, phrase alignment can be addressed from the ot perspective, too, by conducting structure-aware and order-aware optimal transport.\",\"In this work, we contribute an evaluation benchmark for classical chinese nlp tasks.We did our best to create as comprehensive a well-defined task set as possible, something no one has done before.However, our work has several limitations due to lacking expertise knowledge and data.When designing the tasks, we got a lot of inspiration from the middle school chinese test paper.Thousands of test papers are collected in order to ex- tract data for nlp tasks.During the work process, we learn that it is difficult to extract a sufficient number of questions of a single type.The main difficulty is due to the variety of questions on the test papers and the mixture of the language of classical and modern chinese.Finally, we create xuci task, wywrc task and irc task from the test papers and related literature but failed to create solvable natural language inference tasks.When working on some datasets which have less corpus, i.e, the xuci task, we find it very difficult to calibrate existing samples or create new ones, resulting a small dataset size.Meanwhile, the category rule we followed in the gjc task is not certified by authoritative experts, so this method is not completely reliable if viewed by experts of classical chinese.In this work, tasks for more aspects of grammar phenomenon are lacking.on the other hand, we lack a diagnostic dataset compared to other benchmarks.This is because similar data are even more difficult to retrieve.However, this benchmark works for nlp researchers even though the diagnostic dataset is missing.\",\"Although we conducted extensive experiments, the exploration scope of this work has some limitations: all data is from one of the largest mooc websites in china, so the dataset is in the chinese language, which limits the linguistic features covered in our analyses.We will add comprehensive corpora from other mooc platforms with various languages such as english, japanese, french, and so on to enhance the availability and coverage of our dataset.We present two models with highprecision and high-recall behaviors.The severe noisy and incomplete issues could not be coped with simply by combining two technical methods.A more robust training method should be proposed to jointly achieve better overall performance.\",\"First, we only access limited computation resources and perform continual pre-training from bert , which is not general enough for every event-related reasoning task.Second, counterfactual reasoning makes our approach conservative in identifying causal relationships, so our method has a higher precision.However, some potential causal relationships will be discarded.How to achieve a good trade-off between precision and coverage is a problem.In addition, the way we utilize knowledge is relatively simple, and it is very likely that we have not made full use of knowledge.Designing more complex knowledgeenhanced methods may lead to better results.\",\".\\u2013 mbart50 - is a multilingual sequenceto-sequence model pre-trained using the multilingual denoising pre-training objective.Hyper-parameters - for the transformer approach we tokenized the source and target parallel sentences into subword tokens using byte pair encoding.The bpe representation was chosen in order to remove vocabulary overlap during dataset combinations.For other approaches we applied the tokenizer of each model, table 3 shows hyper-parameters used in our baseline experiments.\",\"Imposed by selecting a single curriculum strategy, and instead, focuses on finding and analyzing different curricula that work equally-well for a given model and dataset.In addition, the discovered curricula provide insight into how different portions of the dataset contribute toward learning at different stages of training a model, which, in turn, provide knowledge about the learning dynamics of different models.The task of curriculum discovery could be costly on large datasets, in particular, when the goal is to find optimal curricula for different models and datasets.To mitigate the computational cost, we show that it is possible to rapidly discover a curriculum on a small subset of the dataset and apply the resulting curriculum to the full dataset.these include approaches for learning new difficulty indicators from data , prioritizing medium level instances and those with greatest progress during training, and developing challenge datasets that contain diverse data samples with different levels of difficulty.Finally, investigating diverse curricula that are suitable for general use and across datasets through curriculum discovery and generalization is a promising area for research.Limitations the present work investigates the use of two sample difficulty scoring functions, human-induced annotation entropy and model-induced loss, for nlp models and datasets.The former requires the availability of multiple annotations per sample and the latter requires training an auxiliary model to compute sample instantaneous loss during the course of training.Our work does not provide a general solution to the choice or availability of good difficulty scoring functions.However, once such a function is available, our work presents solutions to the problem of finding high-performing curricula in curriculum space.Our approach, although effective at finding such curricula, requires a bayesian search of its hyperparameters.We reduce these costs by finding curricula on smaller datasets and smaller models that can then be applied to corresponding larger datasets and models.Finally, the proposed method lacks theoretical analysis of the dynamic interactions between data, downstream models, and discovered curricula.\",\"Our work is limited by several factors.First, our findings are supported only by experiments on a single nlp task.We selected this task because it offered an intriguing sandbox for studying varying experimental conditions, ranging from differences in random seeds to modifications in compile-time and run-time environments and dependency versions.Comparing the multifaceted outcomes arising from these experiments facilitated greater quantified estimations of the degree of reproducibility for the selected nts systems.However, the dimensions of variation that we explored in this work are common to many nlp tasks; none are unique only to text simplification.Because of this, we believe that our findings would generalise broadly across nlp tasks., to foster controlled study of our other experimental variables.The data set comprises aligned sentences between english wikipedia and simple english wikipedia.Thus, it is unclear whether our findings would be similar if the study was conducted using data from other languages, including those with richer morphology such as czech or arabic.Finally, although we conducted a robust set of experiments for the selected models across two research groups, our experiments are limited to a small set of nts models due to the extensive set of conditions tested for each model.Although these models vary in their architecture, we do not know if other nts models may be more or less stable across experimental conditions.Taken together, the limitations accompanying our findings suggest compelling avenues for future research.\",\"In this paper, we explore label-semantic augmentation for multi-label few-shot intent detection via appending label name after utterances, which is similar to instruction learning or prompt learning.However, we don\\u2019t further study the relationship between la and instruction learning due to space limitations.We believe that instruction learning integrated with labels will inspire further investigation for multi-label few-shot intent detection.\",\"We obtain notable qa performance through experiments.However, we conduct many experiments to find the optimal candidate for contrastiveqa.Many of these experiments inevitably consume a lot of time and energy, and we have to heuristically determine the number of candidate sets through experiments in a limited environment.We intend to alleviate the current problems by adding a module that can solve these problems in our future research.For example, the longformer model takes almost a day to process long text for each epoch to train.Therefore, we use smaller batch sizes with a limited number of gpus to train the longformer model.However, due to the lack of gpu resources, the optimal weight of the proposed framework cannot be learned.Thus, it is necessary for further research on model weight reduction to mitigate computational resource problems.\",\".it is also yet unclear to what extent the most frequent misconceptions differ between and within learners over extended periods of time.\",\"In this research, though we employed automatic evaluation of our multi-style transferred text, we acknowledge that multi-style transfer is challenging to observe with the existing metrics for style transfer evaluation, and human evaluation should be done as well.As this research paper focuses on exploring the impact of style distributions in the training data on style-transferred output rather than developing a superior multi-style text transfer model, we use quantitative evaluation in this iteration of our paper.We hope that the large sample size and the consistency of applied metrics make our automated approach a reasonable way of evaluating the style transfer output.This iteration of our paper aims to achieve multistyle transfer across multiple micro styles taken into consideration together as our contribution would aid in constructing a training dataset for multiple micro-style style transfers.We did not explore another exciting question of how balancing multiple micro styles in the training dataset might influence individual style transfer, which could be a promising future direction for our study.We acknowledge that the classifier\\u2019s quality sets an upper bound on the best style transfer accuracy that is obtainable.However, the target task is quite complicated without a parallel dataset.Our objective was not to have the most accurate classification of micro styles but to find a means to get acceptable pseudo labels for the micro styles.Individually, all our micro style classifiers had a classification accuracy of 80% f1 and higher, and we deemed this good enough for pseudo-label creation.We also focused on utilizing the present styles in the training data and classifying them to derive inherent training style distributions instead of dynamically tuning the proportion of styles present in the training dataset.However, tuning these style proportions using techniques such as pplm would give us greater control over our experimental pipeline and is an appropriate next step.\",\"We have demonstrated that across three standard re datasets, llms achieve sota results.In particular, gpt-3 yields such performance even given only 10s of training sample for in-context learning.We then showed that we can similarly achieve sota performance with the much smaller flan t5 model, when trained using cot generations produced by gpt-3.We also highlighted key challenges for evaluation in this setting.But there are important limitations to these contributions.First, here we considered three standard re datasets with binary relations but\\u2014 as we discussed\\u2014we excluded more complex re datasets.For example, we did not consider corpora containing n-ary relations between entities.We were also unable to run experiments on datasets with lengthy texts and a large number of relations, such as docred , due to the necessary prompt lengths for such inputs.Second, while we found that cot-style explanations generated by gpt-3 can be fruitfully used as additional supervision to fine-tune smaller language models, we made no attempt to evaluate the quality of these generated explanations which may have an impact on the model performance.Third, we did not fine-tune gpt-3 on the re datasets, mainly due to the cost of doing so.It is likely that a fine-tuned gpt-3 would yield performance superior to the results we achieved with flan t5.But, in addition to the costs necessary for fine-tuning this model, the resultant weights would not be accessible to run locally in any case; one would have access to it only via the openai interface, which motivated our decision to fine-tune the smaller and open-source flan t5 instead.Finally, we only experiment with datasets curated in the english language and therefore, we do not know that the issues we have highlighted could replicate in the same way in other languages.\",\"In this study, we provide empirical evidence of the impact of domain gap in keyphrase tasks, and we propose effective methods to alleviate it.However, we acknowledge that this study is limited in the following aspects: as the first study discussing domain adapation and few-shot results, there is few studies to refer to as fair baselines.Nevertheless, we attempt to show the improvements of the proposed methods over base models by extensive experiments.The pretrained keyphrase generation model can be used off-the-shelf, but the multi-stage adaptation pipeline might increase the engineering complexity in practice.We have only explored three strategies for domain adaptation, and they all require generating hard pseudo labels in different ways.Soft-labeling and knowledge distillation methods are worth investigating.We train a model with wikipedia annotation to predict pseudo keyphrases, and it would be interesting to see if we can use large language models ) to zero-shot predict phrases.\",\".For the methodology part, large-scale pre-trained language models could be considered to provide more context information when incorporating seed words.For the experiment part, only single label dataset are used for extracting seed words, and more explorations on multi-label datasets should be conducted.\",\"Although our method achieves state-of-the-art performance consistently on the four benchmark datasets, it suffers from the following limitations: \\u2022 no optimization for the verbalizer.The verbalizer we use in the prompting stage is just a simple 1-to-1 mapping, this simple design does not fully exploit the capabilities of mlm.\\u2022 no explicit modeling of the relationship information between nested entities.We consider that in some other scenarios, the relationship information between nested entities is not very significant.Consequently, explicitly modeling the relationship may introduce new biases.So we just utilize the potential information.But in practice, it is worth exploring how to model such a relationship from a novel perspective.\",\"Evaluation: we evaluate work as a single-label learning problem and a probability distribution.These metrics do not fully capture the nuances of the crowd.We hope to build on this work by moving beyond general population-level predictions to predictions on subpopulations of interest, such as vulnerable communities.We hope to develop better methods for evaluating and assessing the performance of population-level learning.The range of mixing of the language features and labels in our experiments could be further delved into.Our experiments cover weights ranging from 0 to 100 in quartiles, but this parameter, as a hyperparameter, could benefit from additional experiments in finer ranges.Datasets: our experimental datasets have been primarily in english.In addressing the ability to generalize, we hope to explore other offensive or hate speech-related datasets from other languages.The challenge of evaluating our models with other languages is acquiring a dataset with annotatorlevel labels, a rare resource for english datasets and challenging for other languages.Finally, we hope our methods open the\",\"The high computational complexity is one of the biggest disadvantages of the path aggregation.The time consumption and gpu memory used for multiple operations are expensive.So it is very desirable to use only one time of path aggregation due to attributes of the absa task in our aparn.Another limitation of this work is that the performance of the model is still somewhat affected by the quality of the amr parsing results.The good news is that the research on amr parsing is continuing to make progress.Besides, this model is flawed in dealing with implicit and ambiguous sentiments in sentences.Implicit sentiment lacks corresponding opinion words, and ambiguous sentiment is subtle and not apparent.An example of this is the sentence \\\"there was only one [waiter] for the whole restaurant upstairs,\\\" which has an ambiguous sentiment associated with the aspect word \\\"waiter\\\".The golden label is \\\"neutral\\\", but our model predicts it as \\\"negative\\\".Finally, generalization to other absa tasks such as end-to-end absa or aste is another restriction.Considering the complexity of the task, we only apply our motivation to sentiment classification in this paper.\",\"The main limitation of our proposed method relies on the additional cost of retrieval.Even if the size of our character style indexes is small it still adds latency to our overall pipeline as retrieval must occur once per token.On improving the efficiency of nearest neighbor language models should decrease this latency significantly.As in most nlg work, another important limitation is in quality evaluation.We found qualitative evaluations to be too imprecise for appropriate inter-annotator agreement, and the quantitative evaluations that we present in this paper are all proxies that cannot be said to capture character style or fluency in full.Another limitation of our work is the exclusion of models that are only accessible by calling or finetuning powerful external language model apis due to the excessive monetary cost involved.It is almost certain that these larger models would outperform the 6b parameter model we use, and this may also change the relative performance of the techniques that we present.While we feel that this constraint is appropriate at this moment in history and that our position as major aaa developer gives us the authority to make such a claim, shifts in third party model availability and pricing could change the landscape.Our work deals with data of a singular domain, video game scripts in english, but represents a wide variety of nationalities and ethnicities over the span of a large catalog of games.\",\".our broader goal is to encourage systematic evaluation of adversarial robustness for all prompt-based fsl methods.Furthermore, we do not perform extensive hyperparameter tuning for the methods considered in this work.It is worth noting that \\u201ctrue\\u201d few-shot learning setting has been argued not to involve any development set.To this end, we use the hyper-parameters reported by the original authors of these methods.Finally, for adversarial evaluation of prompt-based fsl approaches, we utilize a pre-constructed dataset \\u2014 advglue.Since these examples are pre-constructed, they do not have access to the gradients of the specific victim models under investigation.Nonetheless, the advglue benchmark offers a foundation for understanding vulnerabilities in large-scale language models under various adversarial scenarios.This standardized dataset enables fair comparison and mitigates issues with invalid perturbations.found that over 90% of adversarial perturbations generated using the gradients of victim models for nlp tasks are invalid.Therefore, using advglue ensures adversarial evaluation on high-quality, human-verified data.Broader social impact: the authors do not foresee any negative social impacts of this work.We believe systematic and preemptive evaluation of the robustness of language technologies against potential adversarial attacks will help develop more safe and secure systems.We release the code for our experiments to aid reproducibility and promote future research on this topic.Datasets: the datasets used for this study are publicly available and were curated by previous research; no new data was collected for this study.We abide by the terms of use of the benchmarks as well as the individual datasets.\",\"Our analysis has both methodological and technical limitations.While dependency parsers are the most robust semanto-syntactic tools available to us, we are limited both by the quality of the parser\\u2019s output and its paradigm.All automated tools make errors, and while our work uses short and simple phrases that are comparatively easy for these tools to handle, it is possible that even systematic errors could seep into the analysis.It is also possible that other semanto-syntactic tools would highlight different phenomena and improve the quality of the analysis.Due to the dataset used, which we picked for quantitative comparison to prior art, there is an inherent bias towards concrete concepts, as they are derived from image captions.We are therefore limited in the understanding of how our method applies to more abstract concepts , potentially warranting further study.There are also concerns about the internal validity of attention maps as an interpretability tool.For example, serrano and smith argue, \\u201c[in many cases,] gradient-based rankings of attention weights better predict [models\\u2019] effects than their magnitudes.therefore, attention scores remain the most feasible method.Lastly, we have consciously limited ourselves to purely making analytical observations regarding attribution and entanglement.\",\"We use scientific papers as a first testbed for evaluating model robustness to layout distribution shifts.Many different layouts exist among scientific papers, and the existence of metadata databases facilitated the construction of train-test splits with layout distribution shifts.However, scientific papers are only one domain in which layout distribution shifts occur.Layouts also vary for many other visuallyrich documents, such as business forms, receipts, webpages, and newspapers.We hope our evaluation methodology engenders evaluations on a wider range of document types.Our experiments involve a subset of the many layout-infused models proposed in recent work.The models in our experiments were chosen because they share a similar model size and underlying architecture, facilitating comparisons between different methods of layout-infusion.Performance drops occur both for layout-infused and, to a lesser extent, text-only models.our error analy- ses suggest that generalization errors are driven by shifts in layout rather than content.\",\".Also, since our model depends heavily on clip , it is likely to inherit clip\\u2019s biases and weaknesses.mentioned that clip fails to perform well on abstract or more complex tasks, such as counting or understanding spatial relationships between objects.Finally, whether our listener model can be easily applied\\u002fadapted to productive real-world tasks requires further exploration.\",\"Introducing the regularizers inevitably incurs additional computational cost in the training of pets.To show their impact on the training speed, we plot the time-performance curves for both pdf and sde regularizers on full-set glue in figures 5, 6, 7 and 8.On different pets, the regularized pets with pdf regularizer has similar running time to the vanilla pets.On the two large datasets, qqp and mnli, regularized pets with sde regularizer take about 2 to 3 times longer to achieve the best performance than vanilla pets.However, on medium-sized and small datasets , the time to achieve the best results with sde regularizer is comparable to vanilla pets.Overall, the pdf regularizer can effectively improve the performance of pets without introducing much computational cost.In scenarios where there is relatively more focus on the inference performance of pets and less concern about the slightly longer training time, or when the dataset is small, sde regularizer should be a good choice.Our method does not introduce additional risk to the original risks of pets.\",\"This work has been studied on the wikipedia corpus, following the standard experimental setting used in previous unsupervised sentence representation learning studies.We expect to see many important findings by investigating sentence representation learning on various corpora in different domains such as bookcorpus and the c4 corpus.\",\"The performances except for the proposed tasks.We presented the result of neologism and the performances on two downstream tasks , which are closely related to the understanding of word semantics.The selected downstream tasks are challenging for the contextualized models; they can use only a few contexts to make a representation.The performance in general benchmarks is almost the same as the vanilla bert because our model suffers catastrophic forgetting while learning definition information.The use of other models other pretrained models like roberta could be a base model of our method.However, we think that bbpe tokens scarcely have semantic meanings, which makes it hard to find appropriate tokens to inject definition information.Therefore, integrating human-written definitions with other types of tokens is also a future direction.The use of all the loss collect more definition data.Presenting more experiments with other models, other collections of definition data, and other loss functions will further support our idea.Nevertheless, we want to show the performances with the widely-used basic model of pretrained language models , using definition data from the previous work, with various loss functions as many as possible.A fine-grained combination of all the loss functions could make further improvements.\",\"Our proposed two-stage training recipe is beneficial under the assumption that a pre-trained model is needed for generative as well as sequence labeling tasks.We believe that is typically the case, as one tries to offset the pre-training investment by using the model for as many tasks as possible, but this assumption might not apply in all cases.While we assess the effect of randomness on fine-tuning results by using multiple seeds, we have not done that for the pre-training itself.Even at our mediumsize scale, it is already prohibitively expensive to do so.The evidence for the effectiveness of the twostage approach is also limited by the number of tasks evaluated , but we believe it is a reasonable trade-off between robust results and compute investment.\",\"In this work, we develop a unified model framework that is applicable to different ner tasks.Through experiments, we show the effectiveness of our method on different ner tasks, both in english and chinese.However, we recognize that our method is not tested on ner tasks where the input sequences are extremely long.In addition, our method is not tested on few-shot scenarios.\",\"One limitation of our method is that it requires multiple generations to achieve the best performance on stance detection datasets.While the best student model significantly outperforms strong baselines, it takes longer training time and requires extra memory for the teacher model.This is a common limitation for knowledge distillation in generations.Another limitation of our method is that the improvements brought by knowledge distillation saturate after a few generations, which can be also observed in previous work.\",\"Murmur relies on large language models for fewshot linguistic skills like surface realization and text fusion.It is probable that smaller models do not work as well, in which case one may curate additional training data to train these modules.We also note that our choice of logical modules is motivated by the characteristics of the task.Hence, it is conceivable that other data-to-text generation tasks might benefit from incorporating additional modules.Murmur does not make any assumptions about the type or implementation of the modules and it should be straightforward to extend our method to other data-to-text generation tasks.We limit our experiments to english datasets.We also adopt a simple prompting strategy for converting a reasoning path to a natural language summary by representing the path as a string.This works well in practice and opt is typically able to resolve the module names and their arguments correctly.Despite the known limitations of standard automatic metrics like bleu and meteor, we use them to compare our method to previous works.While this is not ideal, we have performed comprehensive human evaluation for both tasks to further verify our claims.\",\"This paper takes into account the temporal semantic variations of words and proposes a method to learn dynamic contextualised word embeddings by timeadapting an mlm using prompt-based fine-tuning methods.The learned dynamic contextualised word embeddings are limited to the english language, which is a morphologically limited language.Therefore, the findings reported in this work might not generalise to other languages.However, there are already numerous multilingual mlms such as mbert , xlm and xlm-r , to name a few.Dynamic contextualised word embeddings represent words as a function of extralinguistic context , which consider both time and social aspects of words.However, in this paper we focused solely on the temporal aspect and ignored the social aspect.Extending our work to take into account the social semantic variations of a word is an important future direction.Due to the costs involved when fine-tuning largescale mlms, we keep the number of manuallywritten and automatically learnt templates to a manageable small number as shown in table 1 in \\u00a73.However, it remains to be evaluated the impact of increasing the number of templates on the performance of the proposed method.\",\"There are several important limitations to this work that can be split into two categories: method applicability to other domains and method scalability to much larger models.Utilization rate computation and regularization are possible when there is some external knowledge that can be used to infer which tokens are \\u201cimportant.\\u201d in particular, our highest-performing model uses token semantic type to compute utilization rates.This limits our approach to sub-domains where there is an external knowledge source that can inform us about important tokens and give us higher-order semantic information about how to group the important tokens.For example, our approach will likely not be very helpful for open-domain conversations.we have evaluated our approach for models on the scale of o parameters.However, modern state-of-the-art models often involve o parameters, three orders of magnitude larger than models in our experiments.Large language models often still suffer from the under-generation of rare tokens, but our study is insufficient to determine if our approach would still work.\",\"The purpose of our work is to evaluate the ontological knowledge of plms.However, a sea of classes and properties exist in the real world and we only cover a selective part of them.Consequently, the scope of our dataset for the experimental analysis is limited.The findings from our experiments demonstrate an imperfect knowledge and understanding obtained by the models, indicating a tangible room for enhancement in both ontological knowledge memorization and understanding and a need for a better ability to address paraphrasing.These observations lead us to contemplate refining the existing pretraining methods to help language models achieve better performance in related tasks.\",\"In the available resources and the approaches of nlp researchers towards constructing them.We summarise these and make future recommendations.Conceptualisation with a couple of exceptions , the phenomena targeted in the reviewed resources are not clearly defined or strongly rooted in theory or expertise from outside computer science.Similar observations have been made for operationalisation of related concepts, such as bias and stereotypes , and value alignment.Recommendation: resource creators should collaborate with social scientists to ground them in expert knowledge of the target phenomena.We advocate for the use of gbv as a framework, which encompasses several facets currently operationalised in different ways by computer science researchers.It recognises how all forms of online abuse affect people of every gender both online and off, and has been widely adopted by policymakers.Stakeholder participation parker and ruths propose that computer scientists should: stop thinking about online hate speech as something requiring methods, and start thinking about it as something that demands solutions.This change \\u2014 treating hate speech less like a task and more like the real-world problem it is \\u2014 would orient cs research towards the concerns of other stakeholders, and thus begin the collaborative pursuit toward a safe internet.However, we find little evidence of such a paradigm shift having occurred when it comes to designing these resources, with stakeholder participation limited to the recruitment of loosely defined \\u2018expert\\u2019 annotators\\u2014where it occurs at all.Recommendations: resource development projects should, as far as possible, strive to include stakeholders from the outset by including representatives in research teams.Stakeholder participation should be integrated throughout development, and is especially important in the design of taxonomies, guidelines, and at annotation, when judgements about what constitutes gbv are made., and irb approval sought before any data collection.In documenting resources, authors should provide full data statements or similar , and, to preserve minority voices, dataset releases should includenon-aggregated labels.Data collection media data for these resources is not sourced from diverse sources, with the majority from twitter, the choice of which does not appear to be driven by stakeholders.Furthermore, as the datasets are static in nature, their relevance as reference sources for automated classification decays over time; and, due to data sampling methods, positively labelled examples are skewed towards the more explicit forms of online gbv.Recommendation: there is a great need for the development of new methods to surface the diversity of gbv found online.One solution is to create platforms to which victims of abuse and bystanders can submit examples.This could facilitate creation of improved resources on many of the limiting dimensions we outline in this review: dynamic datasets to which new examples are regularly added; stakeholder participation in data and platform selection and labelling; and inclusion of implicit and subtle examples of gbv, as well as multimedia data.Limitations and\",\"Notes on key research challenges and decisions that affect the findings of this work.our systematic review is restricted to papers from major machine learning venues.In order to download and search entire papers, we restrict our review to open-access venues only and exclude all closed-access research.we only review peer-reviewed papers, and exclude preprints, technical reports, and other informal articles from our review, even though rouge evaluation frequently occurs in these non-reviewed manuscripts.for completeness, we include all archival acl anthology papers including workshop papers.How- ever, due to technical limitations, we only include the main conference proceedings for non-acl venues.historical versions of papers and codebases may contain additional reproducibility information, but we only review current versions.we only review main paper text, appendices, and code linked in papers.We do not review external materials such as websites, slides, videos, or codebases with no link appearing in papers.Appendices and supplemental manuscripts distributed separately from the main paper manuscript are not included in our review.the distribution of papers we review directly reflects the underlying authorship, identity, and content biases in papers accepted to machine learning venues.our first paper annotation stage uses automated regular expression pattern matching of paper text.Although these patterns are validated and refined through a human-in-the-loop development process, automated pattern matching cannot entirely replace expert human judgement and may incorrectly annotate papers.Automated patterns cannot match text in bitmap image figures and tables due to limitations in pdf text extraction.we use a second stage of manual paper review for all papers to identify and correct annotation errors introduced by automated pattern matching.Manual review sometimes involves human inference and judgement in challenging cases.we perform a preliminary case-insensitive search for \\u201crouge\\u201d in all papers.Matching papers receive full automated annotation, manual review, and codebase review.However, we are aware of several papers that compute and report rouge scores without specifically naming the metric.They are labeled as non-rouge papers and receive no manual review.most reviewed papers are written in english.Due to human annotator language limitations and english-oriented automated pattern matching, non-english papers may receive less accurate labels than english papers.contacting authors for clarification may help resolve paper reproducibility questions.However, evaluating this aspect of reproducibility is infeasible at the scale of our work.some papers use rouge for reasons other than evaluation, such as feature generation or for internal training validation.We do not make any distinction between evaluation and non-evaluation rouge during our review.our annotation protocol assumes all papers that use rouge-1.however, many of these papers may run rouge-1.5 via custom ad hoc wrapper code that is implemented incorrectly and introduces scoring errors.we use the papers with code dataset to link papers with codebases.However, this dataset does not cover all papers in our review, which limits our ability to assess their codebase reproducibility.many codebases are missing explicit dependency specification, making identifying exact rouge pack- ages challenging.In these cases, function signatures are used to identify the most likely rouge package.in some codebases, rouge package code is \\u201cvendored\\u201d.It is more challenging to accurately identify the source of vendored rouge packages, particularly if the code has been modified.codebases frequently import very similar versions of rouge packages distributed under different names.We attempt to resolve these packages to a single canonical package for our evaluation.However, slight differences may exist between package aliases that affect our correctness assessment.when a codebase contain multiple rouge packages, we attempt to identify which packages are used to compute rouge scores reported in the paper.If this is unclear, we list all rouge packages used in the codebase.we choose a single specimen task and model for measuring rouge scoring discrepancies due to configurations and packages.Scoring discrepancies differ for other tasks and models.although rouge evaluation is used for many different tasks and datasets, our experiments only focus on a single popular task and dataset.rouge was designed for english language evaluation and we perform experiments on the english language cnn \\u002f daily mail dataset.While there are rouge packages designed for other languages, there is no universal standard for them like rouge-1.therefore, we do not cover non-english rouge evaluation in our experiments.we only examine three common rouge score variants.We exclude uncommon variants rare in papers and often unimplemented in packages.we do not perform any experiments involving multiple reference evaluation, which is not supported by our specimen task and is not implemented in many nonstandard rouge packages.5 and is often unimplemented or incorrectly implemented in reimplementations.Our package experiments operate on individual model outputs and cannot detect bootstrapping errors.our code review identified several instances of custom rouge implementations, but because we only evaluate packages used by more than one author, it is unknown how correct these custom implementations are.many nonstandard rouge implementations change over time.Package changes likely affect comparability between papers.However, our evaluation only considers the most recent version of each package and does not study these between-version scoring differences.\",\"In this work, we address the heterogeneity challenge in the task of fl for semantic parsing, by leveraging the reduction of training loss signal.Our work is motivated from the fl training procedure perspective to adjust the contribution of each client during the global model aggregation stage, but how each client\\u2019s data contribute to the final global model is still unclear.in addition, our proposed re-weighting mechanism is a universal technique for cross-silo fl.\",\"While bolt shows an impressive performance in imposing soft constraints and some hard constraints, it still lacks when it comes to imposing harder constraints, for e., keyword control with more than three keywords.Bolt also requires careful tuning of different hyperparameters that make up the energy function \\u2014 an issue that is prevalent among energy-based controlled generation methods.\",\"Since our primary goal is to study the phenomenon of instruction induction under lab conditions, we focus on tasks that have simple instructions.These tasks are expected to pose a greater evaluation challenge, especially when considering reference-based methods.Evaluating through execution accuracy, however, may mitigate some of that challenge.Additionally, only one model showed instruction induction abilities, i.The exact implementation details of the model and its training data are not publicly available, thus we are unable to investigate the reason behind the emergence of this ability.However, we note that our goal is to present the phenomenon of instruction induction and to raise the ambitious possibility of instruction induction as a learning paradigm.Thus, our goal is not to focus on specific models but rather to shed light on this unexplored phenomenon.Finally, we point to a limitation of the execution accuracy metric, namely assuming the existence of a good-enough instruction-tuned model.Due to recent interest and progress in instruction tuning, we believe this to be a reasonable assumption.\",\"This paper hypothesizes that seq2seq models have token-level overfitting and underfitting issues, and provides direct evidence to support the hypothesis in various settings, raising a valuable problem for nlp modeling.However, this paper does not provide a solution to the problem due to the theoretical and practical challenges of measuring the convergence speed of each token.\",\"All of our experiments have taken place by deploying conversational agents on amazon mechanical turk with crowdworkers2, using english-language responses written by workers located in the united states.While these workers are reasonably diverse , this is quite different to a public deployment with organic users, who are using the system not because they are being paid but because they are genuinely engaged.In that case, collecting feedback will have different tradeoffs which we could not factor into the current work.For example, asking to provide detailed feedback might dissuade users from wanting to interact with the system, lowering engagement and hence the amount of collected data.We believe either more natural free-form or lightweight feedback might be best in that case, which is why we study and compare feedback methods in this work to evaluate their relative impact.In public deployments with organic users, safety issues also become a much more important factor \\u2013 in particular dealing with noisy or adversarial inputs and feedback.In the worst case this could mean human conversationalists could teach the model erroneous reasoning, misinformation, toxic or other undesirable behavior.2our crowdsourcing tasks pay workers well above minimum wage.The tasks do not request any personal information from workers.\",\".3, the proposed model requires precise prediction on the target domain at each dialogue turn to utilize the relevant slot information within the service schema.This possesses a difficulty to apply the proposed model to some new domains, especially when the domains share high similarity with seen domains.To reduce this difficulty, as a future improvement, one potential approach is to process on a hierarchical structure of slots within schemas, instead of individual slots.In this way, the model does not need to rely on the domain, but only on a group of similar slots.The model can then perform matching of a chosen group of slots with available slots within a schema and composes responses based on those slots.\",\"Limitation our proposed method demonstrates stable perplexity even as the style transfer weight changes, but it yields a lower bleu score compared to other controllable style transfer models.We hypothesize that the lower bleu score may be attributed to the fact that the bleu score calculation is based on just one human-written transferred sentence option per source sentence.This lower score could be a result of our model generating diverse sentences that do not necessarily overlap with the provided human-written references.\",\".since the senses were not mapped to an external inventory, the senses cannot be interpreted.Apart from the lack of interpretability, sense embeddings are superseded by contextual embeddings derived from transformer models with sense awareness.While sense embeddings and contextual embeddings are not mutually exclusive, it is necessary to alternate between them for the purpose of privatization and optimization.\",\"The automatic metrics we chose, including bleu , rouge , meteor , bartscore , self-bleu , and unique n-grams , might not be the best metrics to evaluate our results.Some other metrics, such as semantic similarity and multimodal-retrieval based metrics, are based on pretrained models, including augmented sbert , sentencebert , and clip.Those metrics might not align with human judgment and might be biased toward pretrained datasets.While we complement it with human evaluation, we only focus on relevance to ground truth and diversity.Although we found fluency is not an issue, it is likely we still need to cover all aspects of generation results.\",\".First, we only experiment on english datasets.It would be interesting to explore whether the general patterns hold for non-english languages with different structural properties.Moreover, we only explore incorporating hard constraints for decoding with local models at testing time.Exploring more applications of structural constraints, such as learning with constraints, or incorporating other types of constraints, such as soft ones, would be promising future directions.Finally, we only explore three simple sentence-level structured prediction tasks, while extentions can be made to more complex tasks with larger output space, such as text generation or document-level information extraction, where constraints may play more interesting roles.\",\"There are two main limitations to this work.First, we focus on the \\u201cfiltering\\u201d approach to controlled generation.While this formulation clarifies what a distribution is, it can be computationally expensive to do rejection sampling in practice.A promising area of future research is the application of these invariance principles to the design of large language models., generalizing to any arbitrary distribution of text, is a challenging open problem.The purpose of this paper is not to solve this problem.Rather, we illustrate that controlled generation is an important application area for invariance methods.Controlled text generation has the potential to have large impacts on society, both positive and negative.One potential source of risk is misuse.Although we focus on the detection and removal of toxicity, the method we developed can also be applied to the generation of dangerous and toxic content.In addition, this paper does not address other biases that may already be present in language models.The use of a toxicity filter may compound the problem of decreased diversity in generated text if there is a correlation between social biases and toxicity.\",\"Limited experimental scope our study\\u2019s experimental scope was limited to testing distilled student models against a single teacher model.A more comprehensive evaluation would involve multiple teacher models of varying sizes, fine-tuning tasks, and datasets.Additionally, in our experiment with distilbert-based student models, incorporating more checkpoints would enable a more thorough comparison across different factors.Unexplored embedding size variations we kept the embedding size consistent across student models to maintain variable consistency.Future research could investigate student models with different embedding sizes to determine if the observed trends hold true across models of varying widths., is the considerable performance decline for certain data subsets.In our study, we couldn\\u2019t conduct a thorough error analysis due to the lack of appropriate tools for comparing individual data points in retrieval tasks.\",\"Of llm evaluation mentioned previously, there is a crucial ethical concern at the heart of llm evaluation.Is it ethical to replace human evaluation with llm evaluation? Some may question if this paper is suggesting that llms are now ready to replace humans and find this idea unsettling.As responsible and ethical nlp researchers, we understand these concerns but want to make it clear that this is not our intent.As our paper title suggests, we aim to offer an alternative option to human evaluation with the goal of enhancing the reproducibility of nlp research.Human evaluation is still essential as the ultimate goal of nlp systems is to be used by human users, so it\\u2019s important to gather feedback from them.We highly enjoy the process of discussing the experiment settings and results with the english teachers we hired.We do not recommend that future researchers completely eliminate human evaluation; rather, we believe that human evaluation should be used in conjunction with llm evaluation.Both methods have their own advantages and disadvantages, making them both necessary for evaluating nlp systems.We hope the positive results in this paper provide nlp researchers with an alternative method to evaluate systems and encourage further\",\"Our work demonstrates the feasibility of combining query rewriting and query expansion to reformulate a conversational query for passage retrieval.Within our proposed convgqr, the rewriting and expansion are based on two plms trained with different data, which introduce additional training load and model parameters for storage.Thus, designing an integrated model that can simultaneously generate the query rewrite and the expanded terms would be a promising improvement to our method.Another limitation is that the potential answer acting as expansion terms could be generated from more resources rather than only relying on the generative plms.Besides, more alternative methods for knowledge infusion can be tested to connect query reformulation with the search task.\",\".First, we do not give detailed instructions about techniques that we recommended.We rely on our readers\\u2019 autonomy to acquire the necessary information by further reading our recommended references.Second, we only survey tpc papers included in the acl anthology, despite other tpc papers existing outside this venue.While this means that the challenges we identified might be specific to these papers, we believe they are still a good representation of the tpc research done in nlp.\",\"We summarize the limitations of this work as follows: we conduct experiments on 7 language understanding tasks across 4 types.However, the effectiveness of gdfo on tasks such as sequence labeling and generation tasks has yet to be fully examined.Our proposed method uses a student model and a prompt generator, thereby resulting in a higher computational resource requirement in comparison to gradient-free methods.Therefore, it may not be suitable for implementation on certain edge devices, but it is more appropriate for personal or enterprise users who have access to a certain degree of computational resources and have stringent requirements for the model performance.We only focus on the few-shot setting in this paper.\",\"We recognize that our annotation and analysis methods can require considerable human labor, that can limit the amount of annotated data we can collect.Also, despite cycle training being generally accepted as a model-agnostic approach, we were not able to test a wide variety of backbone models due to resource constraints.In addition, though we relaxed the entity constraints and made cycle training for data-to-text generation end-to-end, the nondifferentiability problem remains unsolved.The intermediate outputs generated by the first model of each cycle are assumed to be correct.This is a weak assumption that may propagate misleading training signals to the second model of each cycle, particularly in the early stage of the training.\",\"In this work, we limit ourselves to object-centric grounding, which ignored that language can ground events, attributes, manners, mental states, etc.The grounded meaning of some groundable words, especially advs, nums, verbs, and prons, cannot be fully captured by the bounding boxes alone.In addition, we ignored the social aspects of language learning, where children infer the referents of words from their caregivers through communication.\",\"Our automatic error type system, kagas, has room for improvements.Although we got high human acceptance rate for the error type classification results of kagas, our coverage of error types is about 80% to 90%.Currently, our system rely on the kkma pos tagger for korean.We believe that the improvement of a pos tagger will enable kagas to define a more detailed error type classification with high coverage and reliability.Also, there could be other ways to define and classify korean grammatical edits.However, we would like kagas to be viewed as the first step towards the effort of making an automatic annotation tool for korean gec, which, though not perfect, have meaningful contributions to the field in its current form.Currently, the 14 error types of kagas is focused to be as specifc as possible, while respecting both statistical characteristics of korean language and incorporation into a reliable, deterministic system with high agreement of human evaluation.However, definment of a more richer error type classfication system derived from kagas such as differentiating between the typographical and phonetic errors would be an important future direction for our research, as both are defined as spell errors on our current system.It would require solving additional challenges of accurately disambiguating a writer\\u2019s intention behind errors on a grammatical aspect.Another possible future direction would be applying data augmentation techniques on our datasets to boost the size of the training examples and obtain evaluation metric accuracy gains.\",\".1, nli tasks significantly benefit from core while other tasks marginally do, or there is no benefit at all.We suppose that such a difference comes from characteristics of a task.However, we have not yet thoroughly explored which characteristics of a task attribute the performance gain.To solve this problem, we need a novel deep learning interpretation method to probe latent contexts of lm, or a thorough analysis on relationship between the pretraining objective and downstream tasks and how prompting bridges two distinct phases.\\u2022 core does not work for cases where the train dataset has too long sequence texts.Core requires multiple examples to be concatenated, so developers cannot benefit from core if a majority of concatenated examples from their dataset exceed the maximum sequence length of an lm.\\u2022 we have not yet analyzed whether core is applicable to natural language generation tasks.Nlg is undoubtedly an important pillar in natural language processing research, along with nlu, with many interesting applications.We believe that the concept of context attuning and context filtering can be of help to major challenges in nlg, for example, controlled nlg.We plan to explore core on nlg tasks after this submission.\",\"We propose welt as an approach to address the class imbalance problem.We have only evaluated welt for bioner tasks, although we hypothesize that it can be adapted to any application \\u002f domain that has skewed dataset.We further want to point out that our method was only evaluated on english datasets.Yet, we argue that it can be applied to other languages as well.Finally, we have not assessed welt\\u2019s performance on larger training datasets.\",\"The current work has a number of limitations to consider.The paper\\u2019s experimental design was limited to a single language because we are not aware of any other learner corpora with multiple answers provided to the same exercises.The described approach to assessing the correctness of learner answers is limited by its design.First, the number of gec hypotheses to check depends on the gec model\\u2019s performance and, potentially, on the language.Second, if a word was not corrected, it can be a false negative error instead of a correct answer.Third, the gec model can suggest corrections even to a correct answer depending on the data it was trained on.Our approach focuses only on grammatical errors and it does not take into account semantic or pragmatic errors.Due to limited resources, we were unable to involve more people with prior annotation experience in the re-annotation of the rulec test set, as well as in the manual verification of hypotheses generated by the gec model.We acknowledge that the annotation performed by our annotators may not be entirely error-free: the annotators were free to work at their own pace and therefore could potentially rush and make errors themselves.Hence, we do not claim that the re-annotated rulec test set does not include any inconsistency anymore.We believe that the existing datasets should be thoroughly checked, given the small amount of learner data available for languages other than english, before utilizing them to train and evaluate new models.Considering the practical use of our gec model as a component of a call system, we find that it can potentially be used in a limited context, i., for checking answers provided to cloze and multiple-choice exercises, only for best-performing error types.As for alternative correct answers, even for best-performing categories of answers, a human teacher should verify proposed corrections.We have to underline that learner errors in rulecgec can significantly differ from errors made by learners with various backgrounds, native languages, and proficiency levels.We also find that low recall of state-of-the-art gec models impedes their usage in language learning settings.At the moment, learner answers should be verified by a human teacher.\",\"We calibrated opt models based on wikipedia data.additionally, we limited our evaluation to entropy as a measure of uncertainty and did not explore other measures.Finally, we aimed at validating the calibration status of commonly used lms.\",\"Seti only considers veridical inference and natural inference.However, our benchmark seti can be flexibly extended to more varied reasoning patterns, such as negation, quantifiers, or others.In addition, we evaluate the systematicity capabilities of plms on semi-synthetic datasets, which are limited in language variance.dissect the notion of compositionality and define five theoretically grounded tests for generalization, in a taskagonistic manner.Our work is limited to evaluating the systematicity of plms in textual inference.\",\"The proposed ckl can automatically produce the scores for each utterance in the context and each sentence in the knowledge.However, it is constrained by the total length of the input sequence.Ckl takes bart as its foundation, thus the bottleneck of bart limits the upper ability of ckl.Bart requests the input length to be 1,024, which means the ckl can receive at most 1,024 tokens at a time.For some samples, the concatenation of the context and knowledge contains far more than 1,024 but is truncated to fit with the length requirement.In those cases, the ckl cannot get enough information, resulting in the sub-optimal performance of the ckl.\",\"We proposed a solution to the cosine similarity underestimation problem associated with contextualised word embeddings of highly frequent words.Our evaluations used only a single contextualised embedding model with a single dimensionality.Therefore, we believe that our proposed method must be evaluated with other mlms to test for its generalisability.Moreover, our evaluations were conducted only on the english language, which is known to be morphologically limited.Although in our preliminary experiments we considered discounting schemes based on the part-of-speech of words , we did not find any significant improvements despite the extra complexity.However, these outcomes might be different for more morphologically richer languages.In order to evaluate similarity predictions in other languages, we must also have datasets similar to wic annotated in those languages, which are difficult to construct.we used only a single dataset in our experiments in this short paper due to space constraints.Other contextual similarity datasets ) could be easily used to further validate the proposed discounting method in an extended version.\",\"Of its representation of the depth of collaboration in wikipedia corpora.We also quantified the bot activities in the wikipedia project and excluded the bot-created articles and the bot-made edits on wikipedia articles.We lastly proposed the depth+ metric, defined its formal definitions, and highlighted its features, including a better representation of the depth of collaborativeness, a user-centered depth metric, and bot-free wikipedia editions after the removal of the bot-generated articles and the bot-made edits on those wikipedia editions\\u2019 articles.We hypothesize that a metric that is a better measure of authentic human collaborativeness will be a better measure of the degree to which corpora authentically represents the language and the culture of native speakers.specifically, we aim to examine the performance and societal implications of training llms on unrepresentative and inorganic corpora, particularly on the bot-generated wikipedia articles.Reproducibility data collection, implementation of the depth+ metric, and an expanded technical report can be found on github at\",\"We identify the following limitations of plat and strategies to overcome such drawbacks: \\u2022 performance of the final classifier is dependent on the black-box source weak labeler.We believe this limitation can be worked around in a real-word setting by ensembling source models to vote on a likely weak label for practical accuracy gains.\\u2022 best-performing source models might differ for different tasks.The dataless nature of ews prevents precursory accuracy evaluations while choosing the source weak labeler model.However, quality of candidate weak labelers can be gauged indirectly.Users can examine confidence distributions of weak labels as an indicator of pseudo-label \\\"naturalness\\\".They can also perform difficulty analysis ) that does not require any labeled data.In a real-world scenario, ensemble weak labelers will be used, eliminating the need to choose a single best source model.\",\"There are two limitations of this work, one of which is that our work is trained on the sequenceto-sequence model.However, we have not verified our approach on the sequence-to-edit architecture.The other limitation is that using translationese as input of data augmentation can not bring absolute improvement to grammatical error correction task.Specifically, our approach still has some room for improvement such as correcting rare words, word order, and deletion errors.\",\"One limitation of cot-ka is that it performs finetuning based on the plms, and the input sequence length limit of the plms allows us to add only a limited number of cots.Therefore, it is important to explore and develop a cot selection strategy in future research.A good cot selection strategy would enable the identification of highly effective cots from a set of cots, enhancing the efficiency of kadl.\",\"Regarding data collection, we crawled the english wikihow website from jan 2021 to may 2021.The number of available activities is limited by the data we crawled from wikihow.We currently only choose gardening and crafts categories as case studies.Because we focus on multimedia imagestep pairs, we remove steps that are not attached to any illustrative images.We also observe that a small portion of activities in the dataset do not follow chronological order.Since our task focuses on the daily stereotypical tasks which usually require the model to understand the visual environment, the model design can be directly applied to support other domains, such as steps in the cooking videos.In addition, our model can also adapt to scenarios without visual images because the performance of our model only decreases slightly if no caption is provided.We plan to expand our model to other categories written in other languages.\",\"Our analysis on temporal drift was limited by the fact that the developer of many models in our study did not release the exact time period of the pre-training corpora used.Additionally, models such as bert and roberta were pre-trained on corpora that could be potentially be temporally close to the conll++ test set.However, our experiments still provide additional evidence models are not overfitting the original conll-2003 test set.It is worth noting that when using older models trained on the conll-2003 dataset, one additional reason for the performance degradation, especially in real-world deployment, is that the data used to evaluate the models can be out-of-domain.In our experiments, we attemped to control the domain of the test data on which the models were evaluated to assess other factors for performance degradation.However, we acknowledge that in reality, model performance can be affected by factors such as the emerging text types , which leads to changes in domain, and therefore affects the generalizability of the models.We acknowledge that having conll++ will not resolve the problem of generalization to modern data.As new data keep emerging, there will always be the question of how well ner models generalize to that new data.We hope that our paper will encourage researchers in the nlp community to continuously annotate new test set to study this problem, so that we ensure the robustness and generalizability of models.\",\"The limitations of this work are mainly twofold.different modalities are trained with the same optimizer setting, which might cause imbalance across modalities.no theoretical analysis is established to provide insight of the balance between modality independence and dependence.\",\".Since our dataset relies on the wikipedia category names and semi-automatically generated compositions, it does not represent an unbiased sample from a natural distribution of real search queries that contain implicit set operations.Further, we limit attention to non-ambiguous queries and do not address the additional challenges that arise due to ambiguity in real search scenarios.However, the queries in our dataset were judged to plausibly correspond to real user search needs and system improvements measured on quest should correlate with improvements on at least a fraction of natural search engine queries with set operations.we also note that because wikipedia categories have imperfect recall of all relevant entities , systems may be incorrectly penalised for predicted relevant entities assessed as false positive.we have also limited the trusted source for an entity to its wikipedia document but entities with insufficient textual evidence in their documents may still be relevant.Ideally, multiple trusted sources could be taken into account and evidence could be aggregated to make relevance decisions.Romqa takes a step in this latter direction although the evidence attribution is not manually verified.to ensure that relevance labels are correct and verifiable, we seek the help of crowdworkers.However, this meant that we needed to restrict the answer set sizes to 20 for the queries in our dataset, to make annotation feasible.On one hand, this is realistic for a search scenario because users may only be interested in a limited set of results.On the other hand, our dataset does not model a scenario where the answer set sizes are much larger.\",\".If the domain knowledge introduced is weak or noisy , when the model is provided with more substantial evidence, this additional noisy supervision can, at times, hurt generalization.Therefore, enabling models to perform weight learning, where the model adaptively weights the importance of symbolic rules as stronger evidence is introduced, is an interesting future direction.\",\"Of the work given that the training data for most pre-trained models has not been released, further investigation of the frequency effects of control verbs in the corpora, or for that matter, of any other critical word in the sentence is not feasible.This is a shortcoming of our work because word frequency during training is known to be an important factor for model performance.Nonetheless, in order to approximate this issue, we run preliminary comparisons of the models\\u2019 performance depending on whether the control verb appears or not in the vocabulary.Very similar results were obtained for both sentences with known and unknown verbs in the main clause.Besides, detailed comparisons between models have been left out for reasons of space and scope, since the objective of the research was not to compare model performance, although it is a relevant and interesting issue in itself.In relation to this, the comparison of models with different architectures and training objectives was also left for further research.Finally, it is worth noting that the two languages evaluated in this study are very similar, so that it could be interesting to expand the research to non-romance languages.\",\".Our experiments are only conducted on one dataset due to resource constraint, we should also conduct experiments on more datasets to verify the effectiveness of our approach.\",\"First, agate can handle dtdns but does not handle ctdns.In addition, it currently handles neither labeled nor directed edges.second, we assumed that most new nodes have similar attributes to those of new nodes at the previous time step, which were observed in our preliminary experiments.We plan to propose new strategies for the new node appearance task.Third, we tuned the hyper-parameters independently of models close to the input, so it may not be the optimal combination of methods.We plan to employ auto-ml techniques to enhance performance and mitigate the learning process.\",\"Some of the limitations of this dataset include that of much of the time periods and locations given are simply approximations of the time period that the work is actually set in; this is most notable in the case of library of congress and wikipedia labels which make up the majority of the work.These datasets offer more coarse-grained settings of a work, such as years and geolocation, which have limitations for some purposes.An additional limitation is that the works are in english and also are more commonly set\\u002fwritten in the west, which should be taken into account when used for analytics.\",\".Currently, we learn in-context learning via metafunction pre-training, by comparing an in-context extraction function and a fined-tuned surrogate extraction function at the representation level of their encoders.There are two approximation here: one is fined-tuned surrogate extraction function for approximating golden extraction function, and the difference between representations for approximating the divergence between functions.We think the above two approximations can be further improved for better and faster in-context learning.\",\"This paper proposed a novel pre-training model comave which aims at textual ave tasks, while in this field, multi-modal ave tasks also widely exist in many e-commerce platforms.Meanwhile, the same as the previous ave works, we assume that each t is an independent extraction object, without considering the context-dependent of the whole data resources, such as long documents and instructions, which exceeds the length of an allowable single input.\",\".An additional question arises of whether one dataset may have multiple documents associated with one individual.There are several ways to go about dealing with this.\",\"The first limitation of rse comes from data sparsity.Unlike word-level relational data, the variety of sentences is much larger than words.Therefore, even though we can collect more and more relational data on sentence pairs, it is generally hard to become densely connected between sentences.To alleviate this issue and generate sustainable sentence relational data, we should design automatic tools for sentence relation labelling with human-inthe-loop supervision.Second, unsupervised sentence embedding also shows decent performance on semantic textual similarity tasks, and the focus is mainly on the design of unsupervised contrastive samples.Therefore, exploring different unsupervised view-augmentation techniques in relational sentence embedding remains an open question.In this work, we only study incorporate dropout augmentation without much\",\"Our model simultaneously encodes structural and temporal contexts of the tkg substructure, and uses heuristic strategies to select a portion of query-relevant facts as input texts for plms.We can achieve stunning results with these selected facts.However, this work only considers the queryrelevant one-hop neighbor facts to achieve a good performance improvement, but ignores the benefits of multi-hop neighbor facts.\",\"Our approach has the following limitations: 1.it only considers swaps of pairs of functions at the top-level scope, which is a small set of all the quasi-invariances of the python programming language.it only considers code generation in top-level functions, hence it does not evaluate class methods.it relies on a syntactic substitution to generate \\\"correct\\\" gold truth outputs, which may fail if the swapped functions are called by a string expression through eval or or queried by their string names using the reflection facilities.in our experiments, we can evaluate only a small number of model sizes per family, since these are the only ones available, therefore the p-values of the correlation with the loss analysis are high.the independent reproducibility of the experiments on closed-source models is predicated on the continued availability of a publiclyaccessible api.At the time of writing, our experiments on the openai \\\"codex\\\" models are no longer reproducible without support from openai.item 3 is harder to tackle in the general case because of undecidability issues.Item 4 could be addressed by reproducing our experiments on a model family that encompasses more model sizes, should it become available for public experimentation.Item 5 is an unavoidable consequence of using closed-source models.\",\"In this study, we mainly utilised the bert family of models for chinese text classification tasks.3, we may be able to extrapolate the results to other architectures\\u002ftasks\\u002flanguages.For example, perplection can be seamlessly apply to decoder-only models , llama ) to see whether it can boost the performance for those nlg tasks.But further investigation is needed to verify the utility of findings on other model architectures, tasks, and languages.also, utilising perplection may exacerbate the inherent limitations of pre-trained language models.We suspect that, in instances where the model has not been exposed to certain texts or concepts during pre-training, reliance on perplexity for template selection may result in subpar performance.Besides, the use of perplexity as a metric has the drawback of favoring long texts, which forces us to design templates of the same length.Therefore, a length-agnostic metric can be considered as an alternative.\",\"Although the ckcl performs satisfactorily in erc, there are still some limitations.Because ckcl primarily concentrates on the effect of modeling context and external knowledge on the prediction results, when met some tasks that do not rely on context and external knowledge, pseudo labels can not be annotated, which causes the paralysis of the ckcl.In addition, when the class distribution of the sample is not uneven, the improvement of emotion scl will be weakened.\",\"The type of field metadata tasks is limited in this paper and it can be explored more.There are far more types of analysis metadata to be discovered and inferred.On the one hand, inspired by data profiling metadata, the dependency between multifields in one table plays an important role.There are several common dependencies or relationships among columns.on the other hand, in table 8 only a limited taxonomy is provided.our initial research explored the ability of large language models to extract metadata from tables.The results were not optimal, likely due to a lack of exposure to metadata during the training process of the llm and limitations in the design of the prompts used.Further investigation is nec- essary to improve the performance of llms in extracting metadata from tables.\",\"In this paper, we use a pre-trained bart to observe the changes in the model intrinsic features by varying the fine-tuning objectives and datasets.However, our methods can be made significantly more generalizable when the range of summarization models ) and datasets ) are broadened.Additionally, we can try using other evaluation methods, such as factcc , reported to have a high correlation with human judgment to interpret the model intrinsic behavior during the shuffle tests.our experimental results can also be integrated with those of previous studies focusing on hallucinations in datasets.\",\"All source sense embeddings we used in our experiments are only covering the english language, which is morphologically limited.Therefore, it is unclear whether our results and conclusions will still be valid for meta-sense embeddings created for languages other than english.On the other hand, there are wsd and wic benchmarks for other languages such as semeval-13, semeval-15, xl-wsd and wic-xl , as well as multilingual sense embeddings such as aresm and sensembert.our meta-sense embedding method requires static sense embeddings, and cannot be applied to contextualised sense embedding methods such as sensebert.There have been some work on learning word-level and sentencelevel meta-embeddings using contextualised word embeddings produced by mlms as the source embeddings.However, contextualised sense embedding methods are limited compared to the numerous static sense embedding methods.This is partly due to the lack of large-scale sense annotated corpora, required to train or fine-tune contextualised sense embeddings.Extending our work to learn meta-sense embeddings using contextualised word embeddings as source embeddings is an interesting future research direction.\",\"Although our ulra outperforms all unsupervised baseline methods, there are still some limitations.The first limitation is that there is still a gap between the performance of our unsupervised method and that of some supervised methods.Although our ulra can complete the aes task without label annotations, it is still worth exploring an unsupervised aes method whose performance is comparable to the state-of-the-art supervised method.The second limitation is that the essay encoder which adopted in our ulra is pretrained on the english-based corpora, and the essays for training is also written by english.Thus, our ulra works mostly for english, which means a well-trained ulra model may fail to perform well on the essays written by other languages.An unsupervised aes system which supports multiple languages needs to be further explored.The third limitation is that it requires about 25g gpu memory for training, which may fail on devices with small gpu memory.A possible solution is to set a smaller batch size, but this may take longer time.However, the evaluation process only requires about 2g gpu memory, which can run in most of gpu devices, or even cpu devices.\",\"Despite the promising results achieved by our approach, some limitations must be acknowledged.First, the use of product graphs as a knowledge source is a double-edged sword.Indeed, while it provides a valuable resource to exploit, the constant evolution of product graphs may create a strong coupling between the algorithm and the knowledge source, thus reducing the method\\u2019s robustness over time.Second, our method\\u2019s span-based approach makes it computationally expensive, requiring setting a maximum span size to circumvent this issue\",\".Our findings on the acq models are not as advanced as the current state-of-the-art, but they serve as a benchmark for others to compare with when using similar datasets.Additionally, to conduct more extensive experiments on larger datasets and more advanced models, we require additional computational resources.Specifically, generating clarification questions is a demanding task as it requires the use of powerful language models.\",\".we specifically use fictional short stories as our primary data for the study since we require gold standard labels for this document classification task.Moreover, fictional short stories are easier to find as they often come with a specified grade level compared to other types of literary texts such as magazines or web articles written in any of the three philippine languages.We do not claim that our models are able to generalize on these other types of literary materials or on other types of closely related language pairs unless a full study is conducted which is outside the scope of this work.we were only able to use traditional handcrafted features covering countbased predictors such as sentence or word count and syllable pattern-based features for training the random forest models.We did not extract other feature sets one may find in the previous work on english such as lexical density or discourse-based features since such features require nlp tools that are able to extract pos, named entities, relations, and discourse patterns that do not yet exist for all three philippine languages used in this study.The work of imperial and ong covered a small set of lexical features such as type\\u2013token ratio and compound word density for readability assessment in tagalog.Still, we cannot use this approach since all languages would need to have the same number of features as is a standard practice in model training.our choice of the random forest algorithm for training the ara models is based on the substantial amount of previous work supporting the application of this method to low-resource ara, e., to tagalog and cebuano in a monolingual setup , where it achieved better results than other algorithms such as svm or logistic regression.One can consider these algorithms for comparison but the analysis of each ara model trained with various algorithms to the same level of depth and focus that we have given to the random forest classifier in the present study would require a considerable amount of time as well as a higher page limit.The majority of existing literature in linguistics, specifically on the topic of mutual intelligibility in philippine languages, discusses examples in the context of speech communication.As such, one might claim that cebuano and tagalog are not mutually intelligible by giving an example where a tagalog speaker may not fully comprehend another speaker if they are talking in cebuano.While this is certainly true, in this study, we specifically focus on the mutual intelligibility of languages at a word and character level via written texts such as children\\u2019s fiction books.From this, we see a substantial degree of closeness between tagalog, cebuano, and bikol compared to english.Thus, based on our results, we posit that mutual intelligibility may be used as an additional feature for text-based tasks such as readability assessment.\",\".A limiting factor for some low-resource applications might be the size of a saved factorizer file.We only have to store the subword vocabulary, which however takes substantially more space than bpe as it needs to be stored as dawg trie to keep the tokenization speed similar to bpe.For example, the saved english factorizer takes about 115mb of space while the english bpe with 32k subwords takes just about 1mb of space.We believe that the space requirements are negligable compared to the size of large language models, but they can a limiting factor in some edge cases.The parameter-efficiency of factorizer follows the basic nature of factorized representations: a sequence of 3 bytes can represent more than 16m values.That\\u2019s how we can embed millions of subwords with a negligible parameter count.On the other hand, when we store a vocabulary with millions of subwords on disc, it necessarily requires more space than a bpe vocabulary with 10 000s of subwords.while our main interest and focus has been on morpho-syntactic downstream tasks, it is reasonable to ask what is the performance of factorizer-based language models on other tasks, such as natural language understanding.We utilize the pretrained english language models and finetune them on 8 glue tasks.\",\"In this study, we focused on the clustering task in order to assess the real impact of anisotropy on the quality of representations.The conclusion is clear regarding euclidean and directional clustering but investigating other tasks like information retrieval and anomaly detection would further strengthen the present findings.Also, the set of post-processing methods is not limited to the ones used in this study, and it would be interesting to conduct a more comprehensive study, including more transformation functions.Finally, an important future direction is to assess the impact of anisotropy on other languages, especially on embedding models trained on a restrained corpus, which can be the case of low-resource languages.\",\"Wtp performs comparatively worse in some lowresource languages.This may be attributed to quality issues of mc4 in these languages.In addition, we find that the adapted wtppunct classifiers generally do not transfer well across languages and dataset collections.Finally, although bias is less obvious in segmentation tasks than e.generation, wtp may be biased by performing disproportionately well on text by communities which are overrepresented in the training data, while performing worse on text from underrepresented communities.We try to minimize this form of bias by sampling text from all languages uniformly.\",\"The scope of our approach is intended for our specific task setting, which is proposed as a practical solution to mine open-world attributes without heavy supervision, and has not been studied previously.Our approach does require an external dependency of a pos tagger, and assumes high pos tagging quality on english.Thankfully, there are pos tools publicly available with high performance, and are quite robust against domain shift, mostly fulfilling the assumption.Our current candidate generation that utilizes syntax-oriented patterns does not check the semantics, which can be another limitation.It introduces noisy spans in the process, such as \\u201csupports joint health overall\\u201d.\",\"One significant limitation of our work is that we only explored the capabilities of diffusion-based language models under a challenging circumstance where it is not allowed to use pre-trained weights or grammar parsers, which means we did not utilize this kind of model to its full potential, so a future research direction could be exploring possible ways to further improve the model\\u2019s performance by leveraging pretrained weights or word embeddings, and train with enough data to find the full potential of these models.Another limitation of our work is that we only explored one typical diffusion-based language model, so our conclusions may not generalize to special types of diffusion-based language models.We also conducted all experiments using the exact same model architecture design.\",\"Ircot relies on the base lm to have a zero or few-shot cot-generation ability.While this is commonly available in large lms , it\\u2019s not as common for small lms , which to some extent limits ircot adoptability.Given the recent surge of interest , however, smaller lms will likely increasingly acquire such ability, making ircot compatible with many more lms.Ircot also relies on the base lm to support long inputs as multiple retrieved paragraphs need to fit in the lm\\u2019s input, in addition to at least a few demonstrations of qa or cot with paragraphs.This was supported by the models we used as code-davinci-002 allows 8k tokens and flan-t5-* uses relative position embeddings making it as extensible as the gpu memory constraints allow.the performance gain of ircot retriever and qa come with an additional computational cost.This is because ircot makes a separate call to an lm for each sentence of cot.lastly, a portion of our experiments was carried out using a commercial llm api from openai.This model was deprecated by openai after our submission making the reproduction of these experiments challenging despite our best efforts, just like any other work using such apis.The trends discussed in the paper , we believe, would still hold.Additionally, all our experiments using flan-t5-*, which exhibit similar trends as that of gpt3, will remain reproducible, thanks to its publicly available model weights.\",\"The limitations of our method are as follows: \\u2022 we find that utilizing multi-view representations in the cross-encoder is an effective method for mvd, however, the ranking performance of the cross-encoder may slightly decrease.Therefore, it is sub-optimal to directly use the cross-encoder model for entity ranking.\\u2022 mention detection is the predecessor task of our retrieval model, so our retrieval model will be affected by the error of the mention detection.Therefore, designing a joint model of mention detection and entity retrieval is an improvement direction of our method.\",\"There are several limitations of our work.We tried training the tn-lcfrs on the discontinuous version of the english penn treebank but failed to induce any meaningful discontinuous structures.This is possibly because discontinuous phenomena in english are much less common than in german and dutch.67% of the gold constituents are discontinuous in negra, only 1.84% gold constituents are discontinuous in dptb.The neural lcfrs was also quite sensitive to hyperparameters and parameterization.The instability of unsupervised structure induction is widely acknowledged and could potentially be mitigated by a large amount of training data, as suggested by liang and klein and pate and johnson.Due to this sensitivity, we rely on dev sets for some modeling choices.Hence, our approach is arguably not fully unsupervised in the strictest sense of the term, although this is a common setup in unsupervised parsing due to the mismatch between the unsupervised learning objective and structure recovery.\",\".The first one is we only apply the proposed method to the natural language understanding tasks.It is uncertain how to extend our approach to the natural language generation tasks and whether it can bring considerable improvement.Then, the training process of gan is sensitive to hyper-parameters, leading to us not simply using the default setting when extending to other tasks.\",\"The main limitations of the proposed architecture are related to the presence of the frozen feature extractor.The accuracy of the classification module is proportional to the quality of features.Since the ensemble weak learners are single-layer neural networks, the entire feature extraction process relies on a pre-trained model that strongly limits the upper bound of classification accuracy.Such approach reduces the method complexity, but also makes it prone to errors when embeddings have low quality.Achieving accuracy at a satisfactory level, which is crucial in real world systems, requires the use of high quality feature extractors.Currently, plenty of pretrained sota models are available for free in domains such as text or image classification, but if such extractor is not available, does not produce reasonable features or is too expensive to use, our architecture may not be the best choice.Another issue is relatively long training time comparing to the reference methods.The introduction of a differentiable soft knn layer resulted in additional computational effort that clearly impacted the model complexity.This limits the use in low latency systems with machine learning models trained online.\",\"We only experiment with one type of retrievalaugmented language models, i.however, the ways the other models retrieve neighbors and integrate them are not so much different to affect the results in this paper.according to the same authors, however, the gains should be constant with the increase of the model and retrieval set size.The larger models are mainly different in their behavior when there is no overlap.However, this should not affect the copying tendency of these models tremendously, as it is still the easiest way to generate the next token.It is also worth noting that retro[off], while not using retrieval at test time, is still trained using retrieval \\u2013 so it is not a complete retrieval- free model.However, show that retro[off] is on a par with their retrieval-free baseline in terms of bpb.Finally, we note that our evaluations have only considered the perplexity under teacher forcing, and we have not investigated the behavior of the model in free-form generation or with any kind of fine-tuning.\",\"In this paper, we conduct experiments only on the chinese benchmark dataset due to the lack of english datasets and comparisons of related methods.Moreover, the model is based on bertbase-chinese, so the maximum input length is constrained to less than 512.however, the numbers of words in some long documents exceed the limit, so we use a sliding window to deal with the problem.Otherwise, some documents having too many clauses require large gpu resources after aligning and padding.Limited by the memory capacity, we have to set a small batch size.\",\"\\u2022 we use only the transcript as input to the model.This implies the model wouldn\\u2019t know that a hold was long unless the customer said \\u201cthat was a long hold\\u201d or something to that effect.The transcript usually contains language indicating the hold is taking place \\u201cmay i place you on hold?\\u201d, \\u201cthanks for holding\\u201d, etc, but rarely indicates the exact duration of the hold.Similarly, the model doesn\\u2019t know the wait time unless the customer complains explicitly about it.5-w ay cl ass ific ati on bin ary ha rd lab els bin ary so ft l ab els 0.76 cl as s 1 p re cis io n by using a small model as an initial gating function.\\u2022 if a call center doesn\\u2019t collect csat surveys through our company, their accuracy will be impacted as they won\\u2019t be reflected in the training or test set.We ensure customers understand this by training our agents to explain it and including it in help center documentation.\",\"We focus primarily on comparing model efficiencies using a variety of efficiency metrics and do not consider model performance; one can perform a more elaborate analysis of performance-efficiency tradeoffs, which we did not do here.We only profile a total of seven models across three modalities while there are more efficient variants and vanilla transformers proposed in the literature.While we choose our models to be as representative of each modality and efficiency technique as possible, we cannot extrapolate results to other model variants and other modalities.In particular, modalities like video and genomics and efficiency approaches like quantization would be interesting to profile, which we did not do.\",\"Our method of identifying important words requires a dataset for a semantic task , which limits its applicability.This requirement also prevents us from generalizing our observations too broadly: we tested our method only on one high-resource language where both dependency parsers and nli \\u002f pi datasets are available.Our analysis also lacks the comparison to other indicators of word significance.\",\".First, our experiments are limited by the multi-domain datasets available for sequence classification tasks, limiting both our task coverage and domain type coverage.past work has already considered dataset-level drift metrics and performance predictions for token classification tasks such as named entity recognition and part-of-speech tagging , and example-level drift metrics have been used in machine translation for training data example selection.second, we only consider simple logistic regressions to predict whether individual examples will be predicted correctly by different models.More complex classifiers might improve performance predictions, particularly if more drift metrics are included as inputs, or if raw example features are included.Our three dimensions of linguistic drift represent just one way of decomposing linguistic dataset drift into distinct dimensions.\",\"Though dunst works well, it has four kinds of limitations as follows: \\u2022 decelerated training process.As with all other self-training methods, dunst also needs to reproduce pseudo labels and pseudo text at each st iteration.Since the pseudo text is generated in an autoregressive manner, which is hard to be done parallelly, leading to longer training time.4, though our soft pseudo text brings non-trivial improvement, the overall performance of all st methods still relies on pseudo labels from unlabeled text.When unlabeled text is extremely inadequate or even unavailable , how to better utilize pseudo text for further improvement is an open challenge.we mainly investigate controllable nlg in this work, while it is still unknown whether our method works for other nlg tasks, like nmt and text summarization.4, st actually acts as a kind of regularization and smoothing.How to apply this paradigm to super large plms , where the supervision signals from limited labeled data become extremely weak, is also an open question.\",\"We propose and construct korc as a new benchmark dataset for deep text understanding.first, in the benchmark design, korc do not take more complicated knowledge into consideration, including literal knowledge and qualifier knowledge.second, in the dataset construction, we examine automatic name anonymization and question generation strategy, and present korc-l.rather than medium-scaled language models that can be maintained by a single machine, gpt-3 is used via its online apis.Although the service of gpt-3 is currently available, we still need to find a substitution for better reproducibility.Besides, although llm saves human effort, the execution of llms potentially consumes more energy power.It would be better if we can preserve the high question generation quality and propose a small model to proceed data annotation.\",\".First, we did not conduct a hyperparameter search for the regularization strength of corrloss.Second, since f1 score decreases are substantial and universal across all experiments on mimic-iii-50, we did not run experiments multiple times with different seeds.Third, we did not provide a rigorous explanation of what caused our empirical findings.\",\"The current work marks the first step towards intentconditioned counterspeech generation, and as we noted, even though our model excels in fluency, a larger and more diverse dataset paired with knowledge grounding is necessary to improve and ensure factual correctness.Although the annotators kept the quality of counterspeech as high as possible, it is possible that this data is not at par with other datasets that are annotated by more skilled ngo operators, as is the case with the multi-target conan dataset.A more large-scale annotation of our dataset with higher instances for under-represented target communities would hence be beneficial to learn more accurate distributions of every counterspeech class.Another limitation of the current work is that it exhibits a slightly higher-degree of toxicity compared to the baseline.It, therefore, pertains to accounting for lowering the amount of toxicity present in the generated counterspeeches as future research.Lastly, humor in counterspeech is a very subjective topic, and inspite of including only a few datapoints from that class as compared to the others in our dataset, it is likely that quarc could generate vague and\\u002for offensive text under the pretext of humor.We intend on keeping the dataset private and only provide access for research and educational purposes.\",\"Dkaf model has only been tested on english data so far.At the moment, we curate new datasets by systematic modification of existing datasets.Our simulation strategy is limited as it does not capture real-world factors that have a drastic impact on restaurant availability.Finally, it would be interesting to find a real-world dataset and verify whether the proposed methods give similar performance gains on it or not.\",\".We evaluate the efficiency of our metric mainly for english.we mainly examine the lexicon-level effect of the quality of the text simplification.Other sentence-level factors that could have an effect in simplicity is not explored in this paper, such as the compositional difficulty and comprehension difficulty.Empirically, we observe that all the involved metrics correlate poorly with sts.We aim to conduct further research exploring the automatic metrics on sentence-level simplicity without external knowledge bases or human references.\",\"While we tried our best to maximize the diversity and coverage of our benchmark, it is practically impossible to cover all possible input noises.We acknowledge aspects that we did not get to cover, for example, the impact of different input devices.Also, while we tried to re-construct the real-world input settings as much as possible, there may still be subtle differences between real-world input and our annotation process, for example, we posed speed limits during the keyboard input annotation and this may not capture exactly how users type in real applications.\",\"Limitation of elabor is lack of exploration beyond gpt-3.limitations given the ability of elabor to generate free-text elaborations for commonsense question answering, we still observe some cases where the modelgenerated elaborations are not factually correct, or irrelevant to the question, distracting the answer predictor towards incorrect answers.This reflects a limitation of elabor on the controllability of its generations, which is also commonly discovered when using language models for text generation.We consider this as a possible future direction which aims at verifying the factuality and relevancy of model-generated texts before incorporating them for final inference or as a controlling mechanism during generation.\",\"And outlook discomat is a pipelined solution trained component-wise.This raises a research question: can we train one end-to-end trained ml model that not only analyzes a wide variety of table structures but also combines the understanding of regular expressions, extraction of chemical compounds and scientific units, textual understanding and some mathematical processing? This defines a challenging ml research question and one that can have a direct impact on the scientific matsci community.Indeed, automating parts of scientific discovery through such nlp-based approaches has the potential for biases and errors.Note that wrong and biased results can lead to erroneous information about materials.To a great extent, this issue is addressed as we rely only on published literature.The issue could be further addressed by considering larger datasets covering a wider range of materials.\",\"; benchmarking cd with an existing standardized measure yields no simple answer to the question whether we are now talking about children\\u2019s actual tom.That does not make standardized tests uninformative, but contextualizes their merit: if we agree that tom are social competences, we should also test them in social contexts, not to claim superiority over but rather complement work done in controlled settings.Our classroom context has as advantage regarding tom, that children feel more motivated to do a fun task, engage with narratives as natural finding place for mental state content, have freedom to explore the scenario they want, and that their language has a social goal: immersing the audience in their narratives as possible worlds.This social context may stimulate children more to challenge their language skills.To entice their audience, children may leverage their vocabulary skills to refer to rare settings, uncommon objects, unorthodox characters, and special social situations which is not possible in standardized language tests like the peabody picture vocabulary test.Additionally, children may also recycle complex linguistic structures and plots from prior exposure to narratives in their own narratives, to entice their audience.Thus, the influence of the social context could result in more complex language use than one would expect based on age, which makes the direct relation between age and language competence in narratives less obvious.Overall, our results support the link between more complex language and tom.That said, not all tom-related content requires complex language.Explicating character thought could linguistically also be represented without complement, e.with free direct thought ; moreover, the words used in this thought are not complex, nor is the syntax.This example serves to illustrate the point that in our approach, our classifier makes no assumptions at the outset about the linguistic complexity of tom-related content.\",\"The performance of the model heavily relies on the quality and relevance of the keywords and headings provided in the dataset.If the dataset lacks rigor during the column labeling stage or if the information in these fields is inadequate, it can lead to a decrease in the overall effectiveness of the model.Moreover, incorporating additional prompt tokens may introduce length constraints, potentially resulting in article truncation and the loss of crucial information.\",\"One limitation of this work is that both ptw masking and mrd are conducted only on bert due to limited resources, and mlms with other structures may have different reactions to the timevariant masking with different contents and ratios.Another limitation is that although we propose mrd for the first time, the strategy of time-variant masking ratio is hard to design like learning rate decay.In fact, other decay methods and choices of starting and ending point are various, where better strategies may exist and further work can be done.\",\"An obvious limitation of our work is the considered search space.Although we showed that it is well suited for the data used in practice by the nlp community, this may not hold in more general settings.we suspect that similar results would hold for morphologically-rich languages as we expect, in the latter case, that constituents are shorter , see.finally, in this work we do not consider discontinuous mentions, which is an important setting in real world scenario.\",\"One limitation of our study is that, due to computational constraints, we use what are now considered as relatively \\u201csmall-sized\\u201d models and corpora, exclusively focusing on the english language and generic domains such as wikipedia articles and books.The generalizability of our findings to larger corpora, other languages, or specific domains such as medical texts warrants further investigation.\",\".First, although a common strategy in the related literature which we also adopted, the binary annotation at the token-level is limiting.With this schema, the focus is not on speakers\\u2019 utterances or turns, but on ds sequences.A subsequent issue is that consecutive turns by different characters are considered as one ds sequence if there is no \\\"o\\\" labeled tokens between them.One solution could have been to mark the start and end of a ds turn while paying attention to handle imbricated narration.However, this would have required significant more re-annotation efforts, which we left for a future research cycle within the proposed framework.Second, because of copyright issues the corpus contains excerpts exclusively from a specific period, 1830-1937.Thus, the models were trained and tested on a specific type of literature and may not generalize well to other forms of narratives, in particular modern and contemporary.In this direction, the curation of the test corpus could benefit from more literary insights considering that the evaluation showed high variance of the performance over chapters.This could help to better determine the application scope of the models, and which kind of narratives require further work.With regard to the deep neural network baselines, we did not perform an extensive parameter search and model optimisation.This could have further improved the results.However, performances on recognizing full ds spans were clearly lower than token-level metrics, which had most likely other causes.Regarding the evaluation, although we adopted zme scores from page segmentation to have more qualitative insights, there are still other aspects we have not quantified and could be particularly relevant.For instance, does the model tend to miss the beginning, the end or some other specific parts of a ds sequence?\",\"Our work does not cover the full range of domainagnostic pretraining objectives, including denoising objectives such as electra ), or contrastive objectives, such as simcse.This paper focused on comparing the masked language modeling objective with specially designed dialog-aware objectives.It is our expectation that, given the empirical findings of this project, task-agnostic general objectives like electra, or simcse, will also outperform dialog-aware methods.In addition, due to the lack of task-related datasets, the set of corpora used during our experiments is limited.\",\"In this paper, we focus on lexical transformations between source domain and target domain to reduce the domain shift between them.To do this, we identify unique lexical features in the target domain and place them in the source domain so that the transformed domain is distributionally similar to the target domain.But there are also semantic differences between the two domains in terms of content, domain-specific jargon, and other nuances.This work does not take into account those transformations.Also, we use twitter as the target domain for our work.While the general principles of our work are applicable to any source-target domain pairs, the transformations discussed in this work cater broadly to social media text, and specifically to twitter data.The generalizability to other target domains has not been tested in this paper and remains a topic of further investigation.In this paper, we work with a pos tagging dataset.Pos tagging is a token level task where we classify each token as belonging to a certain category.We feel that because pos tagging is dependent on each token in the sentence, domain transfer affects this task most adversely.Sequence classification tasks like sentiment analysis that only require a high level representation of the entire sentence to make classification decisions might witness different levels of improvement.The current method needs to be tested for other task types, in- cluding sequence classification tasks like sentiment analysis, or generative tasks like question answering and text summarization.This was beyond the scope of a short paper.\",\"The way the method applies to larger datasets needs further exploration.As the number of training examples increases, the accuracy gain over vanilla finetuning reduces, indicating that our method best works in low-resource scenarios.Another limitation is that we performed experiments only in one language.It will be interesting to apply our method to tasks in other languages and understand the impact of task-dependent similarity structure on the model\\u2019s performance in those scenarios.Bftss top-k and bftss u-v methods perform similarly.Scenarios where bftss top-k and bftss u-v differ in performance, should be further explored.\",\"We state the limitations of this work from the following aspects.First, we make an initial assumption about the dynamics between exercise difficulty, vocabulary, and student knowledge.While we believe our assumption is sensible in the domain of language learning, we acknowledge that we make some simplifications for the ease of modeling.For example, we measure difficulty using individual performance, whereas a better way could be combining it with inherent problem difficulty, e.besides, we only consider vocabulary mastery in defining student knowledge and predicting their performance.Exploring more dimensions of language knowledge might lead to a finer-grained personalization.Second, our model relies on student learning logs to estimate their realtime knowledge states.This model might face the cold start problem when dealing with insufficient history.Though it is beyond the scope of this study, techniques like computerized adaptive testing can be used to combat this problem.Lastly, due to the lack of a real learning environment, we discuss the educational promise of our model with simulation experiments.\",\"In this work, we have been focusing on improving the performance of tagging-based grammatical error correction.Our work has the following limitations: we work on three recent chinese grammatical error correction datasets.But there are many emerging datasets from various languages.We will add support for these languages on our github repository and make all resources publicly accessible.We point out a limitation of inference tweaking, but it remains to be explored how to explain the phenomenon and derive better tweaking methods.\",\"Due to limitations in time and computational resources, we limited our experiments to using glm and superglue benchmark3.While transformerbased language models and the superglue benchmark are representative, further validation is necessary when applied to a wider range of models and tasks.Additionally, we found that the performance of glmd\\u2212vc at 85.28% was marginally lower than that of glm-2b at 85.However, it\\u2019s noteworthy that glm-2b leverages a substantially greater scale in the pre-training stage with a batch size, iterations, and gpu count of 7168, 17k, and 224 respectively, far exceeding the respective parameters of 64, 15k, and 8 employed by glmd\\u2212vc in its distillation during the pre-training stage.3given the requirement for grid search and seed averaging, we have run over a thousand superglue averages.\",\"Our model does not follow existing sentence embedding models that encode sentences into embeddings.Therefore, one limitation of our method is that it is specifically designed for sts task and cannot be easily transferred to other tasks, such as sentence classification.Additionally, our approach incurs a slight extra time overhead of approximately 10%, which may be unacceptable for applications that require high time efficiency.Our method only takes into account the semantic comparison of individual tokens, rather than considering the meaning of combinations of tokens or phrases.\",\"Of the conventional fact retrieval pipeline, usually consisting of entity mention detection, entity disambiguation and relation classification, which not only requires additional labels for training each subcomponent but also is vulnerable to the error propagation across submodules.To this end, we proposed the extremely simple direct fact retrieval framework.During training, it requires only pairs of input texts and relevant triplets, while, in inference, it directly retrieves relevant triplets based on their representational similarities to the given query.Further, to calibrate the ranks of retrieved triplets, we proposed to use a reranker.We demonstrated that our difar outperforms existing fact retrieval baselines despite its great simplicity, but also ours with the reranking strategy significantly improves the performances; for the first time, we revealed that fact retrieval can be easily yet effectively done.We believe our work paves new avenues for fact retrieval, which leads to various follow-up work.first of all, while one advantage of our direct fact retrieval is its simplicity, this model architecture is arguably simple and might be less effective in handling very complex queries.For example, as shown in figure 2, even though our difar framework can handle the input queries demanding multi-hop retrieval, the performances on such queries are far from perfect.also, while we use only the text-based similarities between queries and triplets with lms, it is interesting to model triplets over kgs based on their graph structures and blend their representations with representations from lms to generate more effective search space.Also, we focus on retrieval datasets in english.Here we would like to note that, in fact retrieval, most datasets are annotated in english, and, based on this, most existing work evaluates model performances on english samples.\",\"The primary limitation of the proposed model is computational efficiency.Specifically, during the training phase, the input size of the model is more than double that of traditional models, which is due to the inclusion of both predicted and gold templates.Besides, the source sentences are transformed into longer sequences, resulting in an increased memory footprint and longer training time.Additionally, both during the training and testing phase, an additional step of preparing detection labels for the data further contributes to the increased processing time.In future research, we aim to investigate methods for achieving comparable or superior performance while reducing the input size and addressing these limitations, building upon the foundation of our current work.Additionally, templategec does not support the joint training of the seq2edit model.\",\".The first limitation is that aspect and temporal commonsense are outside the scope of our dataset.Here, temporal commonsense refers to knowledge regarding events and the appropriate duration of those events.For example, the event \\u201ci washed my face for three years\\u201d is unnatural in terms of temporal commonsense, but this study did not consider such unnaturalness.The second limitation is that the proposed method is currently applicable only to japanese.In this study, we used a japanese case frame dictionary to generate natural sentences.However, other languages such as english do not have resources equivalent to such a dictionary.Therefore, to apply our method to additional languages, we must first prepare a case frame dictionary for each language.\",\"Though achieving promising results in the experiments, our work still has the following limitations.\\u2022 as shown in table 2 and table 3.the proposed gaussian embedding may have a calibration problem leading to performing badly on fine-grained similarity tasks measured by spearman\\u2019s correlation.\\u2022 the proposed method assumes that all relations are symmetric and adopts a symmetric similarity measurement.However, not all the relations are symmetric.And the ability to deal with unsymmetric relations with unsymmetric measurement is one important advantage of density embeddings which point embeddings do not have.\\u2022 the proposed mrpes dataset should be improved in terms of quantity and quality.The number of test samples should be increased to over a thousand to get more statistically robust results.The types of unseen relations should be also increased to have a more comprehensive investigation of the ability to generalize on relations.The negative samples should be elaborately designed to provide the anchor event with different negative samples under different relations.\",\"There are several limitations to this paper.First, due to time and space constraints, we are unable to experiment with other interesting model compression techniques such as neural architecture search and quantization.We also have to select only a small subset of baseline text-to-sql models to represent the performances on each of the datasets.We are also aware of the existence of ryansql , a sketch-based model for the spider dataset.However, we are not able to reproduce the baseline results to the best of our efforts and have to exclude them from our analysis.Therefore, it is important to be aware of these potential limitations and biases when using our results for real-world deployments.\",\"In this paper, we propose a simple model for prior case retrieval.As shown in experiments and results, the models could improve and score better.There is a big room for improvement.All the previously proposed approaches for pcr have calculated relevance as some form of lexical\\u002fsemantic similarity between a case and its citations.However, cited case relevance may sometimes differ from lexical\\u002fsemantic similarity.Modeling the document in terms of events only partially addresses this.Consequently, what is required is the inclusion of more legal information.We made an attempt towards that via experiments using rhetorical roles.Similarly, one could use the information coming via statutes and laws since similar cases are likely to invoke similar statutes.Another approach is learning representations using contrastive models that score relevant cases higher than non-relevant ones.this paper considers a simple structure for an event as a tuple of predicates and arguments.moreover, we are taking events in isolation and ignoring the sequential nature of events that help to form narratives.though we covered an extensive set of experiments for the proposed event-based matching technique, many more combinations can be experimented with to understand the role of events in legal documents.This unique finding of events missing from the legal literature would facilitate exploring new directions in the legal domain.In this paper, we evaluated only two datasets as we could not find any publicly available pcr datasets.ethical concerns this paper proposes a system for retrieving relevant documents.The system is not involved in any decision-making process.The motivation for proposing the system is to augment legal experts rather than replace them.Moreover, for training the system, we used publicly avail- able legal documents.We took steps to normalize documents concerning named entities to prevent a model from developing any known biases.To the best of our knowledge, we addressed any biases that the model might learn from the data.\",\"The method to select negative examples could be improved, as randomly selecting negative examples for training might lead to identifying most of examples in the evaluation datasets as reasonable.Secondly, we did not explore using other number of candidates in the training set, we always use 2 candidate answers for each question.\",\"Limitation of the previous sentence representation learning approaches, which are limited to using only the input sentence.Rankencoder leverages the distance between the input sentence and the sentences in a corpus to predict its semantic vector.Rankencoder is universally applicable to any unsupervised sentence encoder, resulting in performance improvement, and we demonstrated this with three unsupervised sentence encoders.We achieved state-of-the-art semantic textual similarity performance by applying our approach to the previous best sentence encoder.We also showed that our approach is specifically effective for capturing the semantic similarities of similar sentences.\",\"There are several limitations of this work: in this work, we leverage a cluster-based method for improving the representations during coarse training.Regarding the clustering, we simply perform a k-means process to obtain cluster assignment.The problem is, the cluster assignment with one-time clustering, especially based on the fine-tuned bert model, might be incorrect.Training with incorrect cluster assignments will pull together the instances from different fine classes, thus hindering the performance.Therefore, the proposed method can be further improved by: leveraging a more robust learning method that can tolerate incorrect cluster assignments; improving the clustering process to obtain more accurate cluster assignments.In this work, we aim to extend existing few-shot ner to a practical yet understudied setting.However, we limit the setting to the few-shot coarse-to- fine transfer learning setting as in previous works.In practice, there might be more complex situations that require to be explored in further works.\",\"Limitation wherein novel predicates appear in the test set that do not appear in any of the training or validation set.This is a current limitation of our system.Since we do not do any online learning during the test phase, there is no way to take these novel predicates into account.The significant effects of amr-originated noise or lack of information can be seen by comparing the second-to-last and third-to-last rows.Here we see a significant degradation across metrics and datasets.However, the performance is still comparable or often better than the deep-learning-only benchmark of the first row.Comparing the last row and second-to-last row shows that we can recover most of the performance.We can also see that the metrics are competitive to the model-based approach from game-engine provided logical facts.This shows the effectiveness of adding the proprioception module comprising both the memory and memory-based constraints.\",\"The model might generate incorrect nouns because of the occurrence of patterns.In addition, our model sometimes tends to generate generic step descriptions because of insufficient input information, e., given the last step \\u201clay the t-shirt out on a clean, flat surface.\\u201d, the model generates \\u201ccut the shirt out\\u201d which is vague compared to ground truth \\u201ccarefully cut around the sleeve\\u201d.Moreover, the pretrained model might focus more on language modeling instead of inherent logic: for the activity of \\u201cmake paint can planters\\u201d, after \\u201cremoving the label\\u201d from the paint can, the bart+cap generates \\u201cread the label\\u201d.In addition, there is still a small chance that the model generates the same output for various similar inputs.Because we rely on image captions and retrieval results for step prediction, the upper bound of our generation quality is limited by the performance of the image caption and sentence retrieval modules.Our framework also needs to improve on imbalanced topics in the dataset.For example, the dataset contains more activities about tree for the gardening domain than other gardening-related plants.Because our multimedia generative script learning is a new task, we cannot compare our model with other established state-of-the-art models.Moreover, because wikihow is a crowd-sourcing website, some everyday activities might have better human annotations than the remaining activities.We plan to include a fine-grained human written step prediction as an upper bound to address this issue.\",\"Although speech performs well on event-centric structured prediction tasks in this paper, it still has some limitations.as speech involves many tasks and requires complex calculation, the training process is not very prompt.as seen in the experimental analysis in \\u00a7 4.5, speech seems not always robust to unevenly-distributed data.not all eventcentric structured prediction tasks can simultaneously achieve the best performance at the same settings of speech.\",\"We discuss the limitations of our model as follows: 1.due to the natural uncertainty of financial forecast, although we have taken many methods to improve the generalization performance of the model , creating a trustworthy application requires considering many other factors beyond the algorithmic level.We advise that users monitor the model\\u2019s performance over time and regularly update it to adapt to everchanging market conditions.this paper uses granger causality based on transfer entropy to make a preliminary attempt to introduce causality between time series to model the similarity between stocks more accurately.But this description is junior and classical, and there are lots of more modern methods to measure precise causality in mathematics , which we believe would further improve the performance.we only experiment the performance of model on the task of binary classification, leaving more complex tasks and simulating actual investment to evaluate the capability and potential of the model comprehensively.\",\"Constructing semantic graphs, in general, requires multiple additional tools, which inevitably introduce errors.In this paper, we worked to reduce potential errors.We used the sota amr parser and coreference resolution model and adopted mechanisms to reduce error propagation to our final graphs.However, we have not measured errors involving topic segmentation and amr parsing due to the expensive human annotations required.It will be helpful to investigate how these errors can impact system performance when they are combined with encoder-decoder llms and if the amr encoder is robust to small errors in amr graphs.finetuning llms for long dialogues requires many gpu hours and energy.Therefore, our hyperparameter search was limited to 3 different values for learning rates, 3 for warmup steps, and 2 for batchsize.A more extensive search may bring additional improvement to our model.\",\"The main limitation of this paper is the need for human-labeled reference responses.We will explore automated or human-machine collaboration methods to reduce the cost of annotation in the next stage.Another limitation is that we need to explore whether other auxiliary tasks can also enhance the performance of score prediction.\",\"We use the density matrix to represent modal features, and one of the advantages is that the matrix contains more information.However, the requirements for memory and large gpu resources also in- crease.Based on the best hyper-parameter setting, the shape of a pure state is 16\\u00d7100\\u00d7100, while the shape of a density matrix is 16\\u00d7100\\u00d7100\\u00d7100.At the same time, the matrix will also increase the calculation and time cost.\",\"In this work, we have focused on the efficiency concerns of task-agnostic domain adaptation approaches leveraging pre-trained transformer-based language models.The experiments are conducted on four tasks across 14 domains in both high- and low-resource scenarios.We only consider the methods utilizing pre-collected in-domain unlabeled text corpora for domain-adaptive pre-training.It is worth pointing out that the selected domains are strongly correlated to the selected tasks, which does not reflect the wide spectrum of domain interests.Besides, the datasets are covered only in english to magnify the domain adaptation controlling factors and use cases, while multilinguality would be the next step to explore.We experimented on encoder-only ptlm based on the downstream classification tasks, where the encoder-decoder ptlm would be applicable to different tasks requiring more computational resources.We hope that future research builds on top of our findings and extends the research toward more domains, more languages, more tasks, and specifically with the meta-tokenizers for efficiency concerns of domain adaptation approaches.\",\"This study has two main limitations.The first limitation is its reliance on the assumption that teacher logits on augmented data follow a gaussian distribution.however, in practice, teacher logits may not strictly follow a gaussian distribution.It is challenging to estimate teacher logits under more realistic assumptions, which requires thorough investigations on the distribution of teacher logits and more complex computations for logits estimation.The second limitation is that our method still requires access to the training dataset of the downstream tasks.In this paper, we focus on kd when teacher plms only return decisions.However, our method is not capable of kd without publicly available training data, which is a more challenging scenario for decision-based kd.We believe training a data generation model might be useful for such cases.\",\"The representation dimension is important but limited by our gpu resources.With the support of large gpu, a large dimension may achieve better performance.We also attempt to expand the inductive setting to the independent setting , but experimental performance is not good.That is, if the two kgs are irrelevant, it may be impossible to transfer information.\",\"We discuss the limitations of the work as follows: \\u2022 one major limitation of our work is that we analyze language models pre-trained with the same data, similar training procedures, and the same autoregressive language modeling objective.Our findings may support model families trained in this restricted setting.When comparing models trained with different corpora, such as neo gpt neo and bloom , different architectures and objectives, such as retrievalbased language models and sparse models , the relationship between validation perplexity and downstream task performance could be more obscure.\\u2022 for downstream task evaluation, we only evaluate on multiple-choice tasks, where the evaluation protocol is the most similar to the pretraining objective.another risk is that as we always take aggregated measurements over tasks, it might conceal important patterns of individual tasks.\\u2022 we do not provide a concrete explanation for the double-descent behavior that consistently occurs during pre-training, nor do we know if it is an artifact of the data, the objective or the optimization process.\",\".Trying to develop a more task or even instance-specific understanding of the benefits of mismatch error types will be very useful.We also want to try our proposed approach on a wider set of tasks, using different foundational models, and under the distribution shift setting to see if the mismatch error types as auxiliary supervision can improve robustness of natural language processing systems.\",\"Although our single-site interchange interventions provide causal evidence that particular sub-circuits are necessary for a particular downstream behavior, this technique has known limitations addressed by recent distributed alignment search approaches.First, it will overcount certain \\u201csynergies:\\u201d when a single effect is jointly produced by the conjunction of multiple heads acting in concert, we will identify all heads as making distinct contributions to the circuit.Second, it will under-count \\u201credundancies:\\u201d if there are multiple heads that are individual sufficient to produce the effect, then no single head will be detected as strictly necessary.Ideally, rather than single-site interventions, we would explore all combinations of different heads to find minimal spanning sets that are both necessary and sufficient, but this procedure becomes intractable given the number of heads, requiring more sophisticated optimization-based approaches to find promising sets.\",\"We enumerate some limitations to our work.While we did create the largest union dataset to date, it is still of moderate size.As shown by our learning curves , the amount of training data we created seemed sufficient to saturate the learning of the models with which we experimented, but it might still be found insufficient for training other models.Our annotation protocol might have influenced the compression rates of the unions, as we instructed workers to annotate sentence unions by first choosing a base sentence and then highlighting the other sentence.Additionally, while the highlighting facilitates the annotation process, it cannot directly be used for analyses of the dataset since it is uni-directional.our dataset is also domain specific, in that all the sentences are taken from news sources.This might result in challenging cross-domain generalization.This dataset is limited to the english language.While the suggested annotation protocol seemingly fits other languages, the step in which words are highlighted might prove problematic for morphologically rich languages, in which a single word includes many pieces of information.A segmentation of the text before annotation might be required.\",\"In our work, we rely on a single mixture-of-experts nmt model which is nllb-200.There is a risk that our conclusions may only hold for this particular model and are specific to the way this model was trained.We believe that our findings still can be of interest to any person willing to use the nllb200 model because: it was the only publiclyavailable moe nmt model at the time of submission; it is the only model covering 202 languages and reaching sota results for most of those languages.Moreover, we did not try to finetune the pruned model, which could potentially improve the results and therefore change some of our conclusions.This work has similar risks as the original nllb200 models regarding the misuse of potentially wrong translations., pruning could amplify the biases already present in the full model.\",\"It is highly desirable to test our model on more datasets.However, there are very few multi-class, publicly available datasets that include information about annotator assignments.Often this information is, unfortunately, either discarded or withheld.Without annotator assignments, it is difficult to run experiments related to label distribution learning driven by annotator-item modeling.We hope that this paper encourages more researchers to collect and share more datasets that retain information about annotator-item matchings.Datasets: we understand that the disagreement between the annotators could arise due to the subjectivity\\u002fambiguity of the content to be annotated, nature of the study, or even worker reliability.These observations cannot be solely utilized to disregard a dataset, since it is not a limitation of the dataset but the nature of the problem domain of annotator disagreement.\",\"Safer framework is designed for handling bert classification label noise without using any clean data.Despite the fact that the bert is one of the most extensively used models in the industrial domain, the influence of label noise on gpt models and prompt should be further studied in light of the recent rapid progress.We believe that our framework is compatible with these models, however, further evaluation is required.Another limitation is the types of label noise.We analyze safer using synthetic datasets with uniform and flip label noise which are typical classlevel noise in practice.However, in industrial applications, the model may experience instance-level label noise, which is beyond the scope of our investigation.Although safer achieves robust results in our biomedical literature mining task under human label noise, we encourage users to examine the label noise type first in their own application.\",\".Programming language one limitation of our current model is that it is currently only applied to python, which limits its use and effectiveness in executing programs written in other programming languages.faithfulness the result may not be faithful enough when handling difficult examples, such as those with complex logic, long loops, or many branches.For example, we observe that in two complicated programs that both contain the assignment \\u201calpha = list\\u201d, our model correctly predicts the value of \\u201calpha\\u201d in one case but incorrectly in the other.The lack of faithfulness needs to be studied for further research on code execution.Generation window size we limit the length of generated trace to 1024 tokens.It can be a limitation for programs with long execution traces, particularly those with loops.Improving the ability of transformers to handle longer sequences would likely be beneficial for the code execution task.Ethical statement the work is conducted in compliance with ethical principles.The datasets introduced in this paper only used publicly available data.The annotation in human evaluation was conducted by two authors of the paper, and thus there are no associated concerns, e.therefore, there are no potential risks associated with the research.\",\"Our primary limitation is the size of our collected dataset; we have collected a quality dataset which we demonstrated is useful for analysis, but which is too small for training large-scale neural models.another limitation on our dataset is the relative subjectivity of the task; in completing the annotation, we found that identifying ambiguity and isolating the different underlying questions often involves a gestalt shift.Once an interpretation of the question is chosen, it becomes increasingly hard to see any other.This makes the annotation task subjective; where one annotator might see ambiguity leading to multiple valid answers, another might see one correct answer group and a number of invalid ones.Thus, the annotations in our dataset represent a high precision subset of all the possible ambiguous datapoints.This subjectivity also risks introducing annotator bias into the data; we acknowledge that the vetting steps by the authors may have compounded this further.We are also limited by the quality of the underlying data., both of which were large-scale annotation efforts intended for training.Due to their scale, individual datapoint quality is often quite low; this was one factor contributing to the need for post-hoc cleaning in the annotation process.\",\"Limitation of the popular softmax layer is its global word embeddings.The problem would become more serious when there are more tokens whose meanings are locally defined.moreover, the meaning of tokens are also locally defined in many other applications such as variables in code or math problems, the new terminologies in a scientific paper, or the products in a sequential recommendation problem.We believe that our methods could become an efficient alternative of reranker and create impacts in those areas.Finally, our results show that when there are some uncertainties in the next word , existing lms could have some difficulties of copying the words from the context and our methods alleviate the problem.\",\"One important aspect of achieving optimal performance when using llms is the design of a highquality prompt.In this study, we consider both zero-shot and few-shot learning scenarios, which assume no or very limited task-specific data.However, iteratively refining the prompt over time to obtain the best-performing prompt may break the zero-shot or few-shot scenario.Moreover, the final prompt used in this study is specifically designed to guide the crowd workers in the annotation process of the coda-19 dataset, with frequently asked questions refined over time to address workers\\u2019 confusion.In a real-world scenario, users would not have access to such helpful faqs when working on a new task.Therefore, the performance of llms may be lower in practice.Also, llms are susceptible to a data leakage problem due to their training with internet data.For example, chatgpt is known to have been trained on internet data prior to september 2021.considering that the coda-19 dataset was released in july 2020, with its train, validation, and test sets made publicly available, there is a possibility that some closed models have seen the exact test instances, leading to an unfair comparison.Since the training data are not disclosed for the closed models, the impact of this exposure on the models\\u2019 performance remains unknown.\",\"Our work is limited in several ways.We use human judgements on our case study data to demonstrate a preference of vrm-e versus spsm.However, additional case studies in other domains such as education, healthcare, legal studies etc.Are necessary in order to gather empirical evidence that preference for vrm-e generalizes.there are several threats to interpreting our case study estimates as causal.Like any causal study with observational data, our case study relies on untestable causal identification assumptions such as no unmeasured confounding.for example, our document embeddings do not necessarily measure the \\u201cquality\\u201d of the manuscripts or the \\u201cnovelty\\u201d of the ideas, both of which could affect reviewers\\u2019 scores.1 shows that several manuscripts are reused with high frequency.This will introduce bias within our model as noted in stuart.Additionally, our choice of b satisfies overlap but at the expense of very similar semantic matches between manuscripts.This could explain why there was only a moderate amount of agreement between the human judges as many matches are less semantically similar than we would prefer.\",\"There are several limitations to the work presented in this paper that need to be acknowledged.First, the sredfm and redfm datasets are based on wikipedia and wikidata, which means they may not cover all possible relation types or entities.In addition, the quality of the annotations in these datasets may be influenced by the biases and limitations of these sources.Second, the triplet critic is trained on a small subset of the sredfm dataset, which may limit its ability to generalize to other relation types or languages.Additionally, the performance of the triplet critic may be affected by the quality of the annotations used to train it.Third, the authors of this work are native speakers of some of the languages tackled in this work and external native speakers created the annotation guidelines.However, for some of the automaticallyannotated languages, there were no native speakers involved.Additionally, the qualitative error analysis does not include arabic or chinese examples, as neither of the authors of the paper is proficient in those languages.Finally, the mrebel system is based on a transformer architecture, which may not be optimal for all relation extraction tasks.It is possible that other types of model, such as graph neural networks or rule-based systems, could outperform mrebel on certain relation types or languages.Overall, the results presented in this paper should be interpreted in the context of these limitations.Further research is needed to address these limitations and to improve the performance of multilingual relation extraction systems.\",\"Our work has the following limitations.First, we only used one evaluation data, namely se23, because it is the only data suitable for the vwsd setting, especially for the oov examples.In addition, our methodology relies entirely on wordnet.Therefore, this may be limited the model\\u2019s ability when the target word is a proper noun such as a named entity.Finally, we depend on the results of gpt-3 definition generation to handle oov words.Since the generated definitions may contain errors, as revealed in the qualitative analyses, the errors led to incorrect predictions.\",\"Our approach achieves promising results in crossprompt aes by enhancing the consistency between source and target prompts.We believe that this idea can also be used to other cross-domain or domain adaptation tasks.In addition, as can be seen from table 1, our approach fails to perform well in some cases.We think that forcing the representations of two prompts to be closer during model training may result in more errors when the prompts\\u2019 grading rubrics, writing genres, and writing requirements are quite different.Therefore, there are two possible directions can be explored for future research: 1) more fine-grained shared features can be extracted to improve scoring performance.2) scoreaware information can be integrated into model to improve source and target prompts consistency.\",\"Limitation of token dropping in accelerating language model training.Based on a series of preliminary analyses, we find that removing parts of tokens would lead to a semantic loss problem, which causes vulnerable and unstable training.Furthermore, experiments show such a semantic loss will hinder the performance of token dropping in most semanticintense scenarios.To address this limitation, we improve token dropping with a novel semanticconsistent learning algorithm.It designs two semantic constraints to encourage models to preserve semantic information.Experiments show that our approach consistently and significantly improves downstream performance across all task types and model architectures.In-depth analyses prove that our approach indeed alleviates the problem, and further improves training efficiency.also, it will be interesting to revisit and address the semantic loss problem in efficient training methods for generative language models ).Limitations our work has several potential limitations.First, given the limited computational budget, we only validate our sctd on the large and base sizes of bert models.It will be more convincing if scaling up to the larger model size and applying sctd to more cutting-edge model architectures.On the other hand, besides the downstream performance, we believe that there are still other properties, e., generalization and robustness, of mlms that can be improved by our sctd approach, which are not fully explored in this work.\",\"Limitation of large language models that can be widely and easily exploited by malicious end-users.However, we think the benefits of analyzing bias in reasoning prompts, along with possible methods to mitigate effects, may spur improvements in value-alignment work.Because the content of our work is offensive, we include a warning at the start of the paper.We only use previously collected or synthetically generated benchmarks and rely on automated scoring, eliminating exposure of offensive text to human participants.\",\"Similar to other commercial products, embedding apis are subject to changes that could potentially impact their effectiveness, pricing, and usability.Thus, it is important to note that our findings are specific to the apis accessed during january and february 2023.nevertheless, we believe our evaluation framework can serve to thoroughly assess future releases of these apis.Moreover, we limit our focus to the effectiveness and robustness of semantic embedding apis.Nonetheless, safe deployment of retrieval systems for real-world applications necessitates the evaluation of their fairness as well as additional considerations.Despite their scale, language models have been found to learn, and sometimes perpetuate societal biases and harmful stereotypes ingrained in the training corpus.Consequently, it is crucial to assess potential biases in the embedding apis with respect to protected and marginalized groups.This paper does not delve into this aspect of api evaluation and further research is required to examine these and other issues in real-world applications.\",\"We identify several limitations in this work: false negatives: our current automatic triple extraction pipeline is built using the ds approach followed by filtering using an nli model.while some triples may not be completely available in webie, we expect models trained on this dataset can still discover new triples that do not exist in wikidata.Limited relations in annotation: the human annotation is only conducted on the most frequent 200 relations.Limited languages in mwebie: as discussed in \\u00a72.an alternative solution would be to use professional translators, especially for low-resource languages.Fixed dataset: facts might change in the world.This can lead to a degraded real-world performance if a system relies exclusively on webie for evaluation when the dataset is not updated accordingly.\",\"A limitation of this work is that the poems written by adults are by experienced writers who are often known for their poetry.These poems may therefore not be representative of poems written by adults in general, and could affect the patterns and trends in emotion words we see.\",\"The main limitation to our work lies in the handling of unordered n-ary relations.We hypothesize that the bottom-up paradigm performs best when there is one unambiguous logical form to generate for a particular question.While this is quite often true for semantic parsing, in our experience, unordered n-ary relations can quickly cause this not to be the case.With such relations, there tends to be a large number of correct logical forms for a particular question.In these situations, having so many candidate logical forms can cause significant issues in terms of runtime.A second limitation of our work is that it assumes the logical form will be given as a graph.Thus, there is an annotation burden on the users of this system that would not be present in systems that treat semantic parsing as a text-to-text problem.\",\"One unsatisfying aspect of proposed task is that it accounts for distributive coordination structures, but is not able to handle sentences with collective reading where the main predicate applies to the plurality of conjuncts as a whole.In our data collection these account for about 4.9% of the verbal omission cases, and such sentences are left \\u201cnon-rewritable\\u201d.additionally, in the gpt prompting experiment we experimented with a few different prompts, but did not do exhaustive prompt engineering, and it is possible that with more aggressive prompt engineering gpt can perform better on the task than our results indicate.Similarly for the fine-tuning experiments with t5-large, in which we did some hyperparameter tuning, but not aggressively so.\",\".For instance, we did not explore sampling techniques other than random sampling; while recent works have shown promising directions in data sampling that outperforms random sampling.\",\"Data in this work is limited to the english diachronic word usage graphs.Our methods themselves are language-agnostic and we do not anticipate serious problems with adapting them to dwugs in other languages.At the same time, although flan-t5 is a multilingual lm, we did not thoroughly evaluate its ability to generate definitions in languages other than english.Again, definition datasets in other languages do exist and technically it is trivial to fine-tune flan-t5 on some or all of them.Generated definitions and mappings between definitions and word senses can contain all sorts of biases and stereotypes, stemming from the underlying language model.Filtering inappropriate character strings from the definitions can only help as much, and further research is needed to estimate possible threats.In our experiments with flan-t5, the aim was to investigate the principal possibility of using this lm for definition modelling.the cases shown in \\u00a77 are hand-picked examples, demonstrating the potential of using generated definitions for explainable semantic change detection and improving lscd datasets.\",\"Ground truth and data cleaning although we conduct basic cleaning by selecting the ground truth place object that has appeared the most often for a given user, this is only a heuristic and does not guarantee that the selected ground truth matches the description in the user location string, which introduces noise in the twitter-pug dataset.also, the current ground truth format does not account for alternative names in geolocation.A future direction is training the seq2seq model to generate multiple formal location names from a single user location string.Alternative names in gazetteers such as geonames could be used as a source of this ground truth.In figure 4, we identified several types of noise in twitter user profile locations.We did not conduct extensive data cleaning of fictional, joke, or non-existent locations.Though we attempted to filter these places automatically, we found little change in model performance.A more detailed study of the effects of data cleaning would be beneficial.Model size due to resource constraints, we only experiment with the mt5-small model.In a smallscale preliminary study, we found mt5 outperforms byt5 on our task of geolocation name transduction.It would be interesting to also test how larger or other types of pretrained language models performs on this task.Also, how much data is actually needed to train the model.accuracy trade-off another limitation of the geo-seq2seq approach is that the model always produces a candidate location even when the input only contains a fictional location or does not contain a location at all.A potential solution for this is thresholding the model based on a log-probability threshold, and only producing a candidate location when the probability of a beam is high enough.Such thresholding method could serve to trade off coverage and accuracy.A related issue is the accuracy at each granularity.The model performs significantly better at lower granularity, specifically at the country level.This is important for end-users to acknowledge if this tool is used for higher-stakes analysis such as natu- ral disaster relief, versus such as studying vaccine opinions in different parts of the world.the strong multilingual performance is most likely from the original mt5 pre-training.However, there is still room for improvement.To address the discrepancy in performance across countries, a strategy is to stratify the data by country, similar to how multilingual pre-trained encoders are trained with exponential sampling based on language balance.\",\"There are two limitations to this work.First, it takes effort to design the prompt to guide the llms to generate correct reasoning steps.The gpt-3 models are sensitive to the expressions in prompts.Thus we need to carefully design the prompts.Second, the proposed plan-and-solve prompting can help address the calculation errors and missing-reasoningstep errors, but the semantic misunderstanding errors still remain.\",\".To handle unknown words in the test sets, we replace them by a special unk token which is also used to mask some tokens in the training set.The unk token provides little information regarding the actual input and tapir might be unable to fully utilise the token to refine its interpretation of the past output.This has a direct influence in the incremental metrics, as the model can exploit this property by using unk token as a cue to emit the revise action.This strategy also introduces the extra hyperparameter of what proportion of tokens to mask.We put effort into achieving a diverse selection of datasets in various tasks, but our analysis is limited to english.We are reporting results on the datasets for which the non-incremental versions of the model could achieve a performance high enough to allow a meaningful evaluation of their incremental performance.Tuning is still required to extend the analysis to other datasets.Related to these two issues, we decided to use tokens as the incremental unit for processing.We follow the tokenization given by the sequence labelling datasets we use.Extending the analysis for other languages requires thus a good tokenizer, and annotated data, which may not exist.We may also inherit limitations from the datasets that we use.Although we do not include an in-depth analysis of the datasets, as our focus is on the model and not on solving the tasks themselves, they are widely used by the community and details are available in their corresponding publications.The method we propose to retrieve the action sequences depends on the chosen model, and the grounding of the action sequences in the actual prefix outputs have a direct influence in training the controller.Therefore, the decisions made by tapir rely on the quality of the underlying generated action sequences.In order to ensure that the internal representations of the action generator lt do not depend on right context, we had to restrict ourselves to a single layer variation of this model when generating the sequence of actions.It is possible that with more layers its behaviour would be different, but that would invalidate the assumptions needed for an incremental processor.When it comes to the tapir architecture, the attention scores for the controller are computed independently of temporal order and we do not explicitly model relation between cache elements.The limited cache size also means that some past information has to be discarded to accommodate incoming inputs.Although we have made efforts to incorporate them through the summary vector, this might be not ideal due to information bottleneck.\",\"The current work is limited by the size of the dataset and the nature of spontaneous conversation.While the discourse relations proposed as part of this work were selected to be general and build on categories from the literature, the list is not exhaustive and it is likely that these relations may be culturally, linguistically, and situationally specific.annotation quality is also a practical limitation.Annotation for discourse relations typically results in low-agreement data, even among expert annotators.Even though our research questions focus on this disagreement as a positive, other researchers may require greater numbers of annotations in order to obtain a gold label.\",\"Of multilingual models as weak learners and showed that larger models with richer pre-training objectives, in the form of instruction fine-tuning, yield more meaningful representations.Post-processing refinement logic improves lowresource relation classification, as evidenced by the consistent outperformance of discoflan+ref over the baseline model.It addresses issues of mismatches caused by generation problems, leading to enhanced classification accuracy.Our findings highlight the potential of augmenting weak label learners with distributional logic to improve model classification.Discoflan showcases instruction finetuning for multilingual discourse relation classification for the disrpt 2023 shared task and provides valuable insights for future research in this area.We recognize the potential of larger models to improve prediction quality; however, due to constraints in terms of resources and time, we were unable to test the performance of flan-t5-large in our study.Furthermore, we acknowledge that further advancements in decoding strategies and imporved prompts have the potential to enhance label representations and generation.limitations while using the majority label solves the problem of handing out-of-vocabulary labels during fine-tuning, we acknowledge that label refinement method relies on the majority label.This makes a strong assumption about our dataset bias, namely, that the majority label outnumbers the rest of the labels significantly to impact accuracy.Hence, this method may not be applicable to well-balanced datasets.We also note that simply predicting the majority label is simple method of label prediction which does not generalized to new unseen datasets.Improving label prediction by enriching datasets manually or automatically might make the task more representative of natural data.\",\"That increase the difficulty of developing largescale models for generating or ranking clarification questions.It remains challenging to collect and build large amounts of data.In the near future, researchers should optimize the process of acqs based on the current retrieval technologies for a description of collecting such datasets).recently multi-modal conversational information seeking has received attention in conversational systems.Amazon alexa4 organised the first conversational system challenge to incorporate multi-modal customer experience.However, there is a lack of existing datasets containing multi-modal information for acqs.\",\"Limitation as our method does not focus on dealing with unanswerable questions, our method may not show a great advantage over other methods when there are a lot of unanswerable questions.besides, our speaker modeling prefers questions focusing on speakers, and it may show limited improvement if a dataset contains few speaker-related questions.However, speakers are key roles in dialogues, and therefore, questions about speakers naturally appear frequently in drc.The power of our key utterance extraction method to other qa fields remains unknown.our method does not involve additional knowledge, such as speakers\\u2019 co-reference and relations , discourse structures of dialogues , and decoupled bidirectional information in dialogues.These types of knowledge, which are orthogonal to our work, are key components of dialogues.Therefore, making full use of the additional knowledge in dialogues with our graph modeling can be an interesting direction to explore.\",\"The current work focuses on al with pre-trained language models based on lowest prediction confidence.In spite of the effectiveness of the strategy shown both in these experiments and in previous work , neural models are often not calibrated well , which implies that the output of the softmax function could be a suboptimal metric for measuring prediction confidence, i.additionally, experiments could only be conducted on a limited amount of tasks and datasets.\",\"Resulting from the use of gpt-3 discussed above, there are more general ethical questions surrounding the use of gpt-3 and similar models, for example the high energy usage and resulting carbon emissions, and societal questions around the oligopoly on state-of-the-art language models that is currently in the hands of a handful of large us-based companies.The second consideration relates to the task that we introduce: while we see perspective transfer models as a valuable tool for studying how language \\u2018frames\\u2019 reality that could also have practical applications, for example in journalism, we strongly believe that any such applications must be approached with extreme care.The models that we introduce are scientific analysis tools that could be used to suggest alternative viewpoints on an event, but we believe that generations should not be seen as necessarily reflecting a \\u2018true\\u2019 or \\u2018better\\u2019 perspective, and should not used in a prescriptive way.We believe that the authors of any text ultimately bear exclusive responsibility for the views, perspectives and values expressed in it, and should be careful in making use of texts written by computers, such as the ones produced by our proposed models.Finally, we are aware that our task domain is a societally and emotionally loaded topic, and that the texts contained in our dataset and produced by our models might be disturbing.In particular, in some cases, models may produce graphic descriptions of violence and\\u002for produce questionable moral judgements , and potential users of applications of the model should be aware of this.For the purposes of this paper, the only people external to the research team who have been extensively exposed to model outputs were the annotators in our human evaluation study.In the introduction page of our online questionnaire, annotators were warned about the sensitive nature of the topic and advised that they could stop their participation at any time if they felt uncomfortable and could contact the authors with any questions.Prior to running the online questionnaire we have requested and obtained ethical approval by the ethical review committee of our research institution.had primary responsibility for data collection and preparation, setting up the gpt-3 experiments and running the human evaluation survey.had primary responsibility for the mbart experiments and the automatic evaluation.annotated data and contributed to prompt engineering and the design of the evaluation questionnaire.\",\"., it is still an english-only corpus covering only 6 legal systems.not only will this help support the inclusion of other legal traditions but also adding more linguistic and cultural diversity will help us better understand the robustness of existing methods.Similarly, the newly introduced legallama benchmark consists of 8 sub-tasks targeting eu, echr, us, and canadian jurisdictions in a very controlled setting; where examples were automatically extracted.While on this benchmark, legaloriented plms has demonstrated a significant degree of \\u201cunderstanding\\\" of legal language and legal topics, this benchmark should be further expanded with more sub-tasks to evaluate the acquaintance of legal knowledge across more legal systems and topics, and possibly cleansed from both very easy and unsolvable examples.Model considerations in this work, we consider encoder-only models up to approx.350m parameters, while recent work on the development of large language models is mainly targeting billion-parameter-sized models that usually follow a decoder-only, e.moreover, new paradigms of training plms have been introduced, such as instructionbased finetuning , and alignment via reinforcement learning from human feedback.Latest gpt models have recently shown significant zero-shot progress on law-related tasks such as bar examination question answering.4, we fine-tune all examined plms for a single epoch to avoid extreme over-reparameterization and better estimate how model\\u2019s knowledge affects convergence and performance.Nonetheless, there are possibly better approaches to control for these aspects, e., adapter-based finetuning, or other approaches, such as lora.Beyond performance while we consider a multi-facet analysis, we do not cover other interesting dimensions that should also be explored, especially since law is a very sensitive application domain; for instance trustworthiness-related topics, such as model interpretability , and fairness.\",\"Same tower negatives can be applied to other contrastive losses, e.as we are focusing on improving the most popular method to train dual encoder models, i.While samtone has proven to be effective to improve the training of dual encoders, its efficacy may depend on the diversity of the queries used as inputs.In dataset with a large portion of similar queries in the training set, one might need to use masking or other techniques to remove them from the negative computation.\",\"Of mlms along with the iterative nar inference for open-ended text generation and observed that mlms would collapse for open-ltg.Through extensive study and analysis, we found the reason is the inappropriate attention mechanism and inference strategies, and introduced two simple strategies to alleviate such a problem, i., dynamic sliding window attention and linear temperature decay.Experiments demonstrate that our model achieves competitive performance and significant speedup.We hope our research can make pre-trained mlms as new candidates for the open-ltg community.\",\", it is challenging to organize a shared task during which any possible number of submissions can in principle be evaluated with adequately remunerated human evaluations.What is more, data is very important in the context of real-world applications and shared tasks.Although the corpus used in this shared task is a valuable resource in our domain, some particularities of this corpus and the data sampling method also had an undeniable impact on the results.Therefore, in future editions of this shared task we should rethink some of the current potential limitations, such as the fact that the dialogues had to be limited to 100 tokens, resulting in partial conversations; the fact that some dialogues, if extracted from the data randomly might have led to data leakage; and the fact that the dialogues did not always follow strictly role-alternating format, with some teacher turns being preceded by previous teacher utterances, rather than a student utterances.In summary, the field of education has already been significantly changed by llms, whose capabilities keep improving constantly.We hope that this shared task will serve to help the scientific community better understand the current capabilities of llms in educational contexts.Having learned from this shared task and going forward, we hope to make its future iterations even more informative.\",\"Our findings are primarily based on rouge score, which is a noisy, unstable metric with well-studied limitations.To address this, however, we conduct a human evaluation to support our findings.In both automatic and human annotation settings, we base our evaluations on naturally occurring references, which have been shown to be silver-standard.We hope that our work on pga\\u2013a method to generate high-quality diverse candidates\\u2013can be applied to new domains ) and reference-free learning objectives.Also, our candidate generation method requires two models, which is less elegant and computationally efficient than an end to end solution combining planning and surface realization.Lastly, pga treats all content plans as equally likely.Yet, there is an unexplored trade-off between exploration and exploitation.we sketch out such a potential system in figure 5 with a made-up nucleus probability of 0.\",\"Our work has only considered pairwise interactions, but linguistic structure can also manifest through higher-order interactions.We show that our results on small-scale, formal languages, are different from our results on a natural language task.It would be premature to conclude that small-scale, synthetic tasks can not be predictive of behaviour on more complex tasks, and a more detailed investigation into the properties of the task that play a role is a viable next step.Some of the fidams we considered, most notably sii and stii, are intractable for larger inputs ), and a necessary step in employing these methods to larger models is to construct better approximation procedures, e.more generally, although we believe our probabilistic formal language setup provides a important step forward, solving the attribution generalization problem \\u2013 i., showing that results for small setups generalize to very large model \\u2013 remains a key open problem.\",\"Although dcg achieves significant improvements compared with existing baselines, there are still avenues to be explored in future research.Dcg in this paper focuses on the compositional generalization for multi-attribute on controllable dialogue generation.We hope to extend the method to other generative tasks, including but not limited to dialogue summarization and story generation.In this paper, we explored the control of coarsegrained discrete attributes and the control of finegrained ones separately, and we intend to study the combination of these two attributes in future research.\",\"Recruiting human subjects for annotation limits the reproducibility of human evaluation.In addition, we have only tested the performance of the proposed framework on the fixed dataset, argkp2021, that we described above, and not on a wider range of data.This is because argkp-2021 was the only dataset available for use in this task.Finally, we did not filter the arguments in the original corpus, with the result that potentially offensive arguments may come into the framework as input and generate key points which some readers might find offensive.It is worth noting, however, that the identification of offensive language is not the aim of this work.\",\"Many of the reviews that were gathered for constructing banglabook are discarded because they lack a corresponding rating.A manual annotation process would have yielded a much larger dataset, which was not feasible due to resource constraints.Moreover, one of the challenges for validating the dataset is the lack of statistical models and word-embeddings pre-trained on the bangla language.Some pre-trained bangla-bert models, yet to be trained on extensive corpora, have only recently been proposed.Improving transformer-based models for bangla can enhance sub-word level contextual understanding which will consequently help in more accurate identification of the sentiments in banglabook.\",\"In this work, we explored question generation for computer science textbooks.We have not yet explored a broader range of course subjects, and it may be that the prevalence of computer science knowledge on the internet, including through forums like stack exchange, makes qg easier for this discipline than for others.Furthermore, we examine a relatively narrow range of question types.Other questions \\u2013like multiple choice questions, or compare and contrast questions\\u2013 will require deeper exploration and substantial adaptation of the methodology that we proposed.\",\".Moreover, this work does not address confidence calibration, nor does it address the behavior of selective predictive techniques under domain shift.Finally, our focus is on selective classification \\u2013 we do not address confidence estimation for regression or generation tasks.\",\"We consider only lexical bias based on the cooccurrence between a token and a certain label in data bias for identifying shortcut tokens, while nlu tasks involve various types of data bias, e.although our method can mitigate llm-based task-specific models\\u2019s reliance on shortcut tokens, it can only identify a limited set of bias in the data.\",\"One drawback of our method is that given a maximum span length, we always miss longer spans.For example, the bc2gm and jnlpba ner datasets contain lengthy spans so we do not do as well on those tasks.Another drawback of our method is that it requires embedding the full corpus.One of our methods for making this tractable introduces another limitation - span filtering based on token types may discard spans that are useful to the user.Additionally, although we demonstrate that our method can be robust to training time , we have not explored principled methods for selecting the model checkpoint in supervised kriss-search, as the user does not label a validation set.Methods for making the process more rigorous should be explored, especially for out of distribution tasks.\",\"Our analysis and conclusions have been based only on a single translation direction , a single dataset, and a single transformerbased model.The generalization to other languages, data and models is yet to be verified.Even in this setup, we have seen that some of the proposed methods are very good at detecting fully detached hallucinations.However, none of them were able to well separate strongly detached hallucinations from correct translations.Perhaps, such partial hallucinations should be detected on the level of individual tokens instead of the whole sentence.One of the metrics that we propose, average alti source contribution, has an advantage of not requiring any external models except the translation model itself.However, the two best detection metrics re- quire additional encoders trained on the source and target languages, which limits their applicability for lower-resourced languages or in the settings with limited computational resources.Being an internal method is an advantage of alti, but it is also a limitation: this method is suitable only for transformer-based translation models.In principle, it can be adapted to other neural architectures, but not to non-neural approaches, such as statistical machine translation.\",\"The potential limitations of this work are that wukong-reader has fixed sequence length that may prevent it from modeling long and multi-page documents.Therefore it would be promising to handle varying-length inputs for wukong-reader by, for instance, equipping the model with relative positional embeddings of the model backbone.Additionally, the pre-training objectives used in this work may not be applicable to all vdu tasks.For instance, it can be hardly applied in abstractive question answering or document summarization.\",\"This work proposed a dataset, a simulator, tasks, and models for aerial vision-and-language navigation.Since satellite images are needed to simulate the drone\\u2019s observation, risks of privacy leaking may exist.By using the open-source satellite dataset xview , we mitigate the risks while also being able to develop a simulator for training our model.Additionally, using satellite images for simulating top-down visual observation of the drone introduces the shortcoming of having only 2d static scenes while adopting the strength of the satellite images where rich labels and visual features are included.Broader impact we recognize the potential ethical problems during the dataset collection, where human annotators are involved.The data collection of this project is classified as exempt by human subject committee vis irb protocols.As a result, we utilized the amazon mechanical turk website to find workers willing to participate in the project.With amt, our data collection is constrained by legal terms, and the data collection protocol is under amt\\u2019s approval.The agreement signed by both requesters and workers on amt also ensures a transparent and fair data annotation process and that privacy is well protected.\",\"Include: the automatic metrics are limited in capturing the correctness, helpfulness, and relevance of a socratic utterance, and the benchmark dataset may not represent all common novice misconceptions.Moreover, the manual evaluation is limited to 5 dialogues and could be expanded, but this process is highly time-consuming.\",\"There are several limitations in our paper.First, we have not validated the advantages of our proposed peer in model scales larger than base model, due to the constraint in our computation resource.We plan to experiment the peer in larger scale models when more computation resource is available.Second, in order to filter out potential noise from the relative weak generator, our current rank label retrieving scheme uses a strict condition t = 3, which leads to the fact that a significant proportion of tokens have rank label \\u22121 and essentially are involved only in the original rtd task.we intend to design some label retrieving scheme which applies a softer criterion so that more tokens can be fully or partially involved in the complete tqr task.\",\"Our evaluations rely on the masked language modelling task as it was a convenient task to conduct our experiments and following up on similar related works.To apply it to models trained differently, e., models of the gpt class, one needs to develop comparable appropriateness measures, which is a general desiderata of the field.we used a fixed set of prompts, and others could produce better results for each of the tasks at stake.Even if one could find some working prompts, this may not be ambitious enough, however.It would show that the relevant information is present in the system, say about hyponym-hypernym pairs.But the tests we are proposing rely on the idea that models should work well consistently, across tasks, and across prompts.With our zero-shot prompting method, we tested the pre-trained models.One could imagine ways to fine-tune these models to our requirements.Our goal here was to first make visible that the groundless training might have been sufficient to encode a consistent semantics, and yet that it did not.\",\"Considering the critical nature of the domain of the task, it is of paramount importance to ensure stability in the results expected from the model.Despite setting the temperature as 0 for all decodings in our experiments, we observe the variance in the generated summaries across runs.Table 4 contains the results for three runs for task a and task b.The in-context examples for each sample and other parameters have been kept constant across these runs to identify the degree of stochasticity.Further, in-context learning has shown to be susceptible to changes in order of in-context examples , as well as the template of the examples.A more reliable process to generate the summaries along with identification of the optimal examples is thus required.Additionally, due to the context limit of the gpt-4 model, evaluating the impact of natural language instructions in addition to the examples could not be performed.\",\"We discuss the limitations of our research as follows: \\u2022 firstly, since the t5-large model has many parameters and our task is document level, one training process will occupy four nvidia v100 32gb gpus; \\u2022 our paper mainly studies document-level eae task.Although we believe our approach is compatible with all document-level extraction tasks, how to adapt it to those tasks still remains an open question.\",\"Although our proposed method achieves promising performance in the novel direction of zerocqg, it still has the following limitations: retrieval-based conversation synthesis is limited to predefined question-answer pairs and may introduce repeated question-answer pairs with small differences.Existing question transformation only explore one of the most common conversational characteristics, anaphora.The conversation prompting has limitations when the domain gap becomes large.\",\"Due to the limitation of dataset resources, we evaluate our unified model only with tb-dense and matres.Although the experiment results show that our approach can significantly outperform stateof-the-art methods, we still need to experiment on more datasets with various kinds of temporal relations to further prove the generalization capability and robustness of our framework.\",\"Of the knn system with regards to making new corrections.Limitations the three base models used for the experiments were trained with different settings.As a result, it is challenging to understand the exact source of discrepancies between the results.Additionally, each of the three models used different subword tokenizations, resulting in variable datastore sizes.Although we have some hypotheses about why knn affects gec differently from mt, more experiments need to be conducted to confirm them.\",\"The proposed approach is only viable when the raw labels from different human annotators for each sentence are provided by the datasets.However, some multipleannotated datasets only released the majority vote or averaged label for each sentence.The proposed method made a gaussian assumption on the likelihood function for the analytic computation of the uncertainties.The results show that this modelling approach is effective.Despite the effectiveness of the proposed method, other distributions could also be considered.Data collection processes for aer datasets vary in terms of recording conditions, emotional elicitation scheme, and annotation procedure, etc.This work was tested on two typical datasets: iemocap and msp-podcast.The two datasets are both publicly available and differ in various aspects: \\u2022 iemocap contains emotion acted by professional actors while msp-podcast contains natural emotion.\\u2022 iemocap contains dyadic conversations while msp-podcast contains podcast recordings.\\u2022 iemocap contains 10 speakers and msppodcast contains 1285 speakers.\\u2022 iemocap contains about 12 hours of speech and msp-podcast contains more than 110 hours of speech.\\u2022 iemocap was annotated by six professional evaluators with each sentence being annotated by three evaluators.Msp-podcast was annotated by crowd-sourcing where a total of 11,799 workers were involved and each work annotated 41.the proposed approach has been shown effective over both datasets.We believe the proposed technique should be generic.Furthermore, although validated only for aer, the proposed method could also be applied to other tasks with disagreements in subjective annotations such as hate speech detection and language assessment.\",\"Our proposed ensemble approach for training the transformer architecture has demonstrated promising results for the task of amr ensembling.However, there are limitations that warrant further investigation in future research.Our first limitation is the lack of generalization, as the approach was only evaluated on amr parsing.Therefore, the application of an autoregressive ensembling model has not yet been tested on other natural language processing tasks.Moreover, in order to properly compare each ensemble system under the same conditions, we base all our experiments using the same underlying architecture, i.There needs to be an exploration of these approaches using more recent, better performing parsers.However, this will require access to such systems.Furthermore, the computational cost is also a limitation, as even though our proposed merger method, assemble!, is more efficient than previous ensemblers, it is still computationally expensive, and particularly when we have to ensemble long graphs from multiple predictions.Moreover, as our assemble! Model is based on longt5, it might be challenged when working with large datasets or when running experiments on resource-constrained systems.Therefore, we encourage the use of ensembling strategies focused on selecting the best graphs instead of merging.Lastly, as our ensemble approach is based on transformer, results can be difficult to interpret, as it can be challenging to understand how the generated graph has been ensembled by different predictions, leading to a lack of interpretability.In summary, the proposed ensemble approach for training the transformer architecture has shown promising results for the task of amr ensembling and has the potential to be applied to other tasks, however, further research is necessary to address its limitations and improve performance.\",\".This limits the variability of latent space, which in turn reduces the gains as the number of topics increase.We can try other distributions with richer variability, such as bivariate von mises distribution and kent distribution.Also, in weakly supervised cases, vontss may not perform as well as those methods that leverage pretraining language models in classification.lastly, in semi-supervised cases version, our formulation of vontss requires each topic to have at least one keyword.This limits its practical usage to some extent.To solve it, we can first preselect topics before doing the topics and keywords mapping, or we can modify the optimal transport loss using gumbel distributions.\",\".Apart from this, since the paper focuses on more foundational question of evaluating ner systems in general, we do not foresee any other potential risks involved with this research.Broader impact considering the number of practical usecases of ner across industries, and the growth of multilingual nlp, ner evaluation beyond english is more important than ever before.In this paper, we explored a previously unexplored space for named entity recognition, i., evaluating ner systems beyond english for their sensitivity to adversarial input, which will hopefully lead into better evaluation strategies when developing ner systems across languages in future.\",\"Those of our findings that are based on information obtained from authors are necessarily limited in that they do not reflect information that might have been obtained from authors who did not respond.Moreover, we selected our initial set of papers via search with key phrase \\u201chuman evaluation.\\u201d while this phrase is very commonly used to refer to non-automatic forms of evaluation, there is a chance that we may have missed papers because they used a different term.\",\"Our proposed approach requires to train two independent classification models.While the models can be trained in parallel, this requires larger gpu memory.For the experiments, we trained two bert-base models, which have around 220m trainable parameters when trained in parallel.This requires almost twice the gpu memory compared to a single bert-base ner model, having around 110m trainable parameters.Owing to a pipeline-based structure, the overall performance of our system is upper bounded by the performance of span detection model which has lots of potential for improvement.On dev set, we find that around 30% of errors for ontonotes5.0 and bionlp13cg, and around 22% errors on wnut17 are just due to minor boundary detection issues.Their entity types are being detected correctly.We henceforth encourage the research community to design architectures or new training objectives to detect mention boundaries more effectively.Currently, in our span detection model, all entity mentions are grouped into a single class.at the top level, we can detect mentions belonging to some crude categories and gradually break them down into more fine-grained categories.\",\"By manual analysis, we found that claim-dissector suffers from overconfidence in blocks with at least 1 relevant evidence.Then it seeks to select more relevant evidences inside, even when they are not.We believe this is connected to how irrelevant negatives are mined in fever \\u2014 they originate only from blocks without relevant evidences.On real data, the system often struggles to recognize what facts are refuting, and what are irrelevant.We demonstrate this in a case study on downstream application, where we replaced retrieval on wikipedia with news-media in test-time.We tried to verify the claim \\\"weapons are being smuggled into estonia\\\".Our system discovered article with facts about \\\"weapons being smuggled into somalia\\\", and used it as a main refuting evidence to predict refute veracity.Lastly, cd is trained with evidence from wikipedia, and do not considers other factors important for relevance assessment in practice, such as credibility of source, its reliability, or its narrative.This is the area of active research, as human fact-checkers also need to deal with lies.\",\"Although the ihlda shows better performance than existing models in multiple experiments, there are three limitations that we did not fully address in this paper.First, the gibbs sampling is slower than other approaches such as autoencoding variational bayes , which limits data scalability.We can incorporate the literature on distributed algorithms for topic modeling and variational inference in future research.Second, crowdsourced evaluation limits a corpus choice because we should not expect workers to have any prior knowledge.Our crowdsourced evaluation only used bbc news, the most accessible documents among the three corpora.Future research can thoroughly validate the performance of the crowdsourced workers and trained coders.Existing literature found that mturk had a comparable quality against traditional survey panels, but they did not use mturk for evaluating outputs from a machine learning model.Third, an estimated hierarchical structure does not necessarily match the semantic hierarchy human readers expect.This mismatch is not surprising because unsupervised models do not directly incorporate information about a tree structure.Existing papers improved the interpretability of flat topic models by providing topic-specific sets of keywords and labels , which is a future direction for a hierarchical topic model.\",\"As shown in table 1, an overwhelming number of catering reviews on yelp makes the countertemplates with obvious catering information.For example, \\u201cthis is a great place for a quick bite to eat.The food is delicious and the staff is very friendly.They have a good selection of beer and wine.The place is always busy, but it\\u2019s worth the wait.\\u201d in this case, the pattern information in the text is not consistent with other businesses irrelevant to catering.As shown in table 6, although our method has a slight improvement over the previous methods in the mean and standard deviation, it is only comparable to the sota at the best performance.Since the counter-template is exactly the same text for the whole data set, the performance of the model is affected perhaps when the pattern information from different texts in the data set has large differences.When we extract the restaurantrelated parts of the dataset as yelp-res that have more similar pattern information, our model performs better.\",\"As the experimental datasets are chinese and the word segmentation tool is employed, some parsing errors may exist.Also, the token-token matrix is built on all tokens in each document, resulting in a large-scale matrix and the reduction of model training.All these are the limitations of this paper.Nevertheless, if the corpus is english, the first limitation does not exist.Also, the spatio-temporal efficiency in table 1 is acceptable.Importantly, the experimental results obtained in this paper are based on the limitation, which indicates that it is effective to implement our model according to the segmentation results by syntactic tools.\",\"Due to the lack of theoretical support, it is challenging for us to formalize an annotation scheme for implicit persona attributes in the current stage, e., extracting an implicit triplet from a sentence \\u201cevery day, i personally take my dogs out for a walk and lend a hand to my neighbors by occasionally taking their furry friends out for a stroll as well\\u201d, besides.Therefore, our personaext is not compatible with the implicit or multiple persona attribute triplet extraction tasks.Additionally, our framework did not exploit complementary information from the context of the current utterance for paed.For an input with multiple dialogue utterances, it is hard for our model to match extracted persona triplets with the exact speaker because of the existence of pronouns and more than one speaker in dialogues.\",\"And potential risks the two limitations of dynainst are that it requires known task boundaries, and that it does not concern with corrupted or noisy training instances.In a realistic industry setting where the task definition is quite ambiguous, and a non-negligible amount of human bias and noise are introduced during the data collection process, these limitations of dynainst may degrade its performance.However, considering that this is the first time lifelong instruciton learning has been studied, these limitations can be considered interesting directions for future research.Like any language model, the model trained with dynainst may output unfair and\\u002for offensive predictions due to the bias embedded in the dataset.Improving the fairness of instruction-tuned language models is beyond the scope of this paper; nonetheless, if these problems remain neglected, we will risk deploying language models that are heavily biased and discriminatory.\",\".However, automatic classification of offensive outputs can be subject to inaccuracies, which may lead to the identification of false positive test cases.To mitigate this issue, we may increase the threshold for positive texts to reduce the number of discovered false positive test cases.One other choice is incorporating human supervision into the classification.For example, we may assume the human-in-theloop scenario that has access to the offensiveness scores evaluated by human annotators within a limited number of queries to the annotators.In this scenario, we can either directly conduct brt with human annotators as the red team classifier or modify the brt method to incorporate offensiveness scores from both human annotators and the safety classifier modules during red teaming.\",\"Argscichat is the result of a pilot study concerning 31 invited nlp experts.In particular, argscichat contains dialogues about 20 scientific papers regarding a few nlp topics.Thus, dialogues in argscichat are only a small sample of the set of possible dialogues grounded in scientific papers.In particular, several design choices have been considered in our data collection methodology: the topic of a paper; the common background of invited nlp experts; the available content of a paper during a dialogue; and the dialogue setting.We chose the nlp domain in our study since we have expertise in this domain.This choice also facilitated the def- inition of a pool of nlp experts to participate in our study through our research network.Furthermore, dialogues in argscichat are grounded in scientific papers.this choice reduces subjects\\u2019 effort to act as e, while also providing enough information to sustain a dialogue.\",\"The datasets in this paper systematically control lexical cues and world knowledge between critical conditions, allowing us to tease apart the effects of statistical heuristics versus reasoning about causal relations.However, the manipulation brings unnaturalness to sentences when scaling up into large-scale synthetic datasets, and constrains the level of linguistic complexity.As we have seen in exp3, the small-scale dataset has more complex combinations of conflicting lexical triggers than the large-scale dataset, causing language models to behave differently across datasets.4, it will be valuable to carry out additional investigation of effects of sentence naturalness, and to consider designing large-scale datasets using naturally-occurring data.Finally, the experiments use english, in which counterfactual conditionals have distinct and systematic linguistic markers relative to other types of conditionals.It would be interesting to investigate other languages in which counterfactual conditionals are not marked linguistically, and require world knowledge to disambiguate.For example, a chinese conditional could be ambiguous between \\u201cif it had rained today\\u201d and \\u201cif it rains today\\u201d.\",\"While we discover that simply applying cnn on top of the score matrix of span-based ner model performs well on the nested ner scenario, there are still some limitations that are worth discussing.Firstly, we mainly choose three commonly used nested ner datasets, which may lack generalization.Secondly, we only focus on nested ner tasks for the spatial relations between spans are more intuitive and common in nested scenario than those in flat ner.However, the principle of using cnn to model the relations is also applicable to spans in the flat ner task.\",\"One of the limitations of our study is that the performance of knn search is highly dependent on the domain of the datastore used.4, knn search, like standard lm, does not work well for contexts and numerals for out-ofdomain data.This dependence can be reduced by increasing the size of the datastore and introducing passages from various domains; however, this strategy may bolster another limitation, as discussed hereafter.The second limitation is that knn-lm requires more memory usage for the datastore and higher latency for search during inference compared with standard lms.Although the search process itself can be executed swiftly by leveraging efficient similarity search libraries like faiss , as the size of the datastore expands, the time required to obtain their representation vectors is expected to increase.The third limitation pertains to the lack of language variety in the utilized datasets.While we deliberately selected datasets from different domains for our experiments, they shared a common language, namely english.Consequently, it is expected that knn-lm will exhibit similar effectiveness in languages with linguistic structures similar to english.However, conducting experiments on non-english datasets is necessary to provide evidence for the language-independent impact of knn-lm.This aspect will be addressed in future research endeavors.\",\"Although our model achieves good performance in solving the compositional and zero-shot generalization problems, there is still room for improvement on the i.The fine-grained module in our framework cannot take advantage of explicit composition information when the component compositions in the testing set and training set significantly overlapp.For example, in freebase, \\\"who is the coach of fc barcelona?\\\" Is answered by the join of relation \\u201csports.our fine-grained extractor may fail to recall \\u201csports.coach\\u201d as the candidate since \\u2018football coach\\u201d is more relevant to the question than \\u201ccoach tenure\\u201d in semantics.The only coarse-grained model, however, can directly memorize the pattern because such composition appears frequently in the training data.Therefore, compared to conventional models that completely memorize composition patterns, our model may only have minor advantages.Another limitation is that we cannot guarantee the generalization on other kbs such as wikidata because gaps between kbs may bring negative impact.For example, relations in freebase are often more specific , while relations in wikidata are more general.\",\"To our knowledge, the current study is the first effort in the ml community aiming to define pi and estimate how much of it is present in two major training corpora, c4 and the pile.However, we recognize that there are ways in which our study can be improved, and directions in which future studies can be conducted.To start with, when annotating the pi found both by using presidio and regular expressions, we observed that new forms of pi have appeared with the advent of the internet, but have yet to be considered in traditional definitions , despite their potential for risk.Also, given the diversity of types of pi that exist, it is unsurprising that systematically detecting them remains a challenge.other limitations of both types of approaches is that they are language- and often country-specific, and need to be adapted to contexts of application and languages.This can quickly become complex, because the format of common types of pi such as bank account numbers varies immensely depending on its country of provenance.Finally, linguistic characteristics of individual languages make it difficult for multi-lingual pi detection since features that are relevant towards pi detection in some languages are not relevant for others; more work on developing more modular and extensive pi detection tools would be an important contribution to many communities and endeavors, and it is conceivable that ml-based approaches can contribute to these efforts.Broader impact statement our work endeavors to help the nlp community better understand and quantify the types and quantity of personal information contained in popular training corpora.In order to strive towards this goal, we manually annotated a subset of the personal information detected in c4, which constitutes a dataset that could be valuable to the community.However, given the quantity of high-risk personal information that this sample contains, we do not feel comfortable disseminating it.We are, however, working on methods for developing synthetic and lower-risk labeled corpora to help develop better methods for detecting pi.As large language model development is increasing dramatically, more models will be trained on these data sources, so its becoming increasingly important to quantify and characterize the personal information present in datasets as well as help practitioners develop better pi detection methods.\",\"Since product question answering is actually a domain-specific application in general qa, the scope of the problem may be limited.However, in recent years, pqa has received increasing attention in both academy and industry.for example, some studies use pqa as an entrypoint to analyze the subjectivity in qa tasks.From the application perspective, it has great commercial value.Online shopping is playing an increasingly important role in everyone\\u2019s daily life, so that many high-tech companies develop ai conversational assistants for promptly solving customer\\u2019s online problems, including but not limited to amazon, ebay, alibaba, jd, etc.Regarding the large amount of research efforts that have been made, there is not a systematic and comprehensive review about this research topic.Similar to recent surveys of other domain-specific qa, such as biomedical qa and legal qa , we hope that this paper can serve as a good reference for people working on pqa or beginning to work on pqa, as well as shed some light on future studies on pqa and raise more interests from the community for this topic.\",\"Our results nor evaluation set cannot be used to indicate whether rte models trained for other languages are robust to paraphrases.However, researchers can apply the methods we used to develop p\\u0302 arte to build evaluation sets in other languages to test whether non-english nlu systems are robust to paraphrases.\",\"We illustrate this paper\\u2019s limitations from the following three aspects: 1) limited by the computational resources, we only train udr from the initialization of \\u201cbert base uncased\\u201d following epr.2) most of current dense demonstration retriev- ers, including udr, are black-box models.Although they lead to significantly better performance than bm25, how they find informative demonstrations is still unknown.Therefore, a better understanding of the principle of informative demonstration\\u2019s retrieval or an interpretable and transparent demonstration retriever may be the next stage of improving demonstration retrieval.propose a more explainable method, beyond-context learning, which first uses the language model to get training data\\u2019s next word probability distribution, then assigns test instances with labels of their nearest neighbors with similar next word\\u2019s probability distribution.3) in the training stage we use lm to score candidates separately but in the inference stage lm is provided with a sequence of demonstrations.\",\".Experimental results indicate that the proposed simple parameter sharing strategies can be a better alternative to the existing method.As many studies on neural methods, this study also depend on empirical observations.In other words, this study lacks theoretical justifications for proposed parameter sharing strategies.we mainly focused on sequence-to-sequence tasks and trained each model from scratch.Our conducted experiments indicated the efficiency of the proposed strategies but we did not conduct experiments on the pre-training and then fine-tuning configuration such as comparison with bert due to the limitation of our computational budgets.Thus, it is difficult to claim that the proposed strategies are also more efficient in such configuration.In addition, we have to investigate the effectiveness in a more realistic situation.For example, we will investigate the performance of the combination of our proposed method, which is the parameter efficient way for internal layers, and a parameter efficient embedding such as takase and kobayashi.Through experiments in various configurations, it is difficult to conclude which strategy is the best.Experimental results imply that the best strategy depends on the task and transformer architecture.Such phenomena are reported in previous studies.Is better in the language modeling task but ineffective in the machine translation task.Since it is intractable to investigate a tremendous amount of possible parameter assignment way due to the limitation of computational budgets, there might be a superior way to three simple strategies proposed in this paper.However, we emphasize that all our proposed strategies are more efficient than the universal configuration.Because the purpose of our experiments is not to detect the best parameter sharing strategy but to indicate that our proposed parameter sharing strategies are more efficient than the universal configuration, we consider that our conducted experiments are sufficient to verify our claims.\",\"We find that 8 out of 14 reviewed works do not include an adequate discussion of the limitations of gloss approaches, inadvertently overstating the potential usefulness of their experiments.In the context of sign languages, glosses are unique identifiers for individual signs.However, a linear sequence of glosses is not an adequate representation of a signed utterance, where different channels are engaged simultaneously.Linguistically relevant cues such as non-manual movement or use of three-dimensional space may be missing.The gloss transcription conventions of different corpora vary greatly, as does the level of detail for an overview of differences and commonalities between corpora).Therefore, glosses in different corpora or across languages are not comparable.Gloss transcription is an enormously laborious process done by expert linguists.Besides, glosses are a linguistic tool, not a writing system established in deaf communities.Sign language users generally do not read or write glosses in their everyday lives.Taken together, this means that gloss translation suffers from an inherent and irrecoverable information loss, that creating an abundance of translations transcribed as glosses is unrealistic, and that gloss translation systems are not immediately useful to end users.\",\"The experiments reported were performed on a dataset of french sentences, with a particular organization: sequences of sentences as input, each with a slightly different structure but sharing the subject-verb agreement rule.All sentences in the input sequence are processed together.we have investigated only part of the parameters in the proposed architectures.Higher values may lead to more disentangled representations on the latent layer.\",\"Our study and findings are limited to the specific l1\\u2013l2 pair of chinese \\u2013english.Further, the experimental setting we draw our data from is highly controlled, with carefully-chosen lexical items and carefullydesigned stimulus sentences.\",\"One limitation of simlm is that it can not be used as a zero-shot dense retriever, since the pre-training framework does not have any contrastive objective.Fine-tuning on labeled data is necessary to get a high-quality model.On the other hand, although simlm pre-training is quite efficient thanks to the replaced language modeling objective, it still requires extra computational resources to train the model.\",\"For web-augmented models including our work, the deterioration of search results from search engine highlights the importance of deriving an effective method to interact with the huge web.Search engines are often perceived as black-box and non-transparent for end users.Therefore, many works proposed \\u201cleaning to search\\u201d to decompose complex questions into simpler queries, which may improve the performance of web-based models.In our model, we used a commercial search engine as the retriever to work with the whole web as a knowledge source.Since the web is not curated and well-structured like wikipedia, we may encounter unexpected safety issues, including misinformation and harmful contents.While we have relied on the security control of the search engine, more attention should be paid to better understand the risks and provide effective ways to mitigate them.to encourage the community to investigate the question and ensure reproducibility, after the reviewing process, we will release the search urls used in our experiments.As for the potential concern, since we use the search engine to access real-time information, we do not have a tight control over retrieved results as traditional end-to-end retrieval.Not only the changes of search engine logic, but also the newly published information, might create discrepancies over the course of time.This is also an issue we have to tackle to build a stable web-based solution for plms.\",\"Our discussion in this paper leaves out the consideration of computability of measures over languages.Specifically, we note that there exist works on computable measure theory developed in the context of theoretical computer science and probabilistic programming languages.another notable limitation is that we exclusively focused on the autoregressive production of language.Importantly, our formalism might not be compatible with other models of language production such as those induced by a pcfg.10 exploit the strictly positive property of the softmax function.Importantly, they do not apply to models with sparse distributions.\",\"While the potential implications of our research are broad, we make note that there are several limitations that should be considered: 1.The scope of this study is limited to legal reasoning tasks using the coliee task 4 , which is based on the japanese bar exam.The results may not generalize to other legal reasoning tasks in particular common law systems.coliee task 4 itself depends on the coliee task 3 , and we assumed perfect retrieval of the relevant articles used for the premises.The experiments were conducted with two versions of gpt models only, and it is unclear how other llms may perform with this task.The study focuses on zero-shot\\u002ffew-shot and fine-tuning approaches with and without explanations, as well as various prompting strategies.The explanation and the prompting strategies, however, are difficult to control.For the explanation, we rely on the explanations created by gpt-3.5 without knowledge of how reliable they may be.For the legal prompting, we show that legal strategies have a positive impact on the performance, but it is unclear how explicit mention of the strategies impacts the lm.The use of clustering past training data as fewshot demonstrations is a novel approach, but it is not clear how well it would perform on other data sets.We do not claim that this approach would show improved results for other tasks.The experiments were conducted on the most recent two years of coliee task data, and the results may not generalize to other years of the task.More importantly, the test data size is relatively small which is reflected by the mildly statistically significant results for 2021.The experiments were carried out on the english translations only and not the japanese original text.openai maintains control of gpt-3 and future models, and we cannot guarantee that the model versions used will be available to others in the exact same state.\",\"This work has only focussed on numerals from 10- k documents mandated by sec.Our dataset, at present, does not include any annotated words as we focus only on numerals.It also does not include any tabular data.We also find that companies often annotate text with their custom labels which are not included in our dataset.We also find that often, it is difficult to label a numeral based on just the text of the sentence; the context might depend on surrounding paragraph, associated tables, etc.To this end, we have not benchmarked the performance using this information.However, we provide certain metadata along with the data points, including the company name, the year document was published, and the surrounding text which may be used to develop improved models.\",\"Given that the wlas are calculated as the sum of all the tlas representing one single word, it is possible that there could be an underlying preference for longer words in the framework.However, multiple tokens in a word could also have conflicting attributions, so it is not entirely clear how this affects the framework.Given the results of this implementation, it could be reasonable to try and calculate the wlas as the mean of all tlas instead.Furthermore, it is reasonable to discuss the consequences of the preprocessing steps being carried out in the experiment.Although such preprocessing steps might increase the iaa measured between the human rationales and model rationales, it is reasonable to question what these preprocessing steps actually result in and their possible value in realworld applications.In cases where the use case is to identify and highlight certain important words, such preprocessing steps might bring a considerable amount of value.However, if the end goal is to represent the model\\u2019s attention as precisely as possible, these preprocessing steps might skew the representation of the model\\u2019s attention.Consequently, one could argue that there exists a trade-off between usable model explanations, which can be used as an assisting or guiding tool for the human expert, and explanations that are fair representations of the model\\u2019s inner workings.In the case of asag, explanations such as the ones created by the presented framework could likely be used as an assisting tool in helping human expert graders find important words or sentences.Given such a framework, the speed of grading could likely be increased without removing the trust of having a human grader making the end decision.Lastly, it is worth noting that the use of top k sentences should only be seen as a means of calculating iaa.However, in a real-world inference setting, the number of relevant sentences might be dependent on the task as well as the subject.In the case of assisting a human expert in grading, the number of top k sentences might be a parameter controlled by the human expert in order to showcase only the most relevant sentences marked by the model annotations, where the number of relevant sentences might be dependent on the length of the student answer as well as the complexity of the given question.\",\"While this work explores the impact of the typical compositional modifiers on entailment relations, we did not consider other fine-grained information that further captures upward or downward monotonicity from the monotonicity calculus of the premise\\u002fhypothesis sentence pairs.Further, the dataset that we generated is relatively small, at approximately 1,300 sentences.We also did not evaluate the dataset over t5, bart, gpt-x, and other state-of-the-art llms, which may provide more insights.lastly, we did not include a comparison with existing datasets that were created specifically for negation modifiers and universal existential quantifiers.\",\"Despite parla being intended for general-purpose linguistic rule learning, we only tested it on arabic and only to learn morphophonology rules.We also recognize the state of the data and the task being on out-of-context standalone tokens and not continuous utterances which is the nature of spoken languages.This is something we plan to investigate in the immediate future.\",\"Our augmentation approach is model-agnostic, meaning that it can be applied to any lexical substitution model.However, this also means that it inherits any limitations of the underlying model.For example, in the case of lexsubgen, it can only produce single-token words as substitutes which might prevent it from generating valid longer words or phrases as substitutes that are present in the gold annotations.Additionally, the substitutes are also limited by the vocabulary of the pre-trained language model that lexsubgen uses.Another limitation of our method is that it relies on the presence of target words in a lexical resource, such as wordnet, together with their synonyms and glosses.If this sense-specific information is missing from the lexical resource, it cannot be used to improve the performance of a lexical substitution system.Our entailment criterion for lexical substitution is defined for the binary classification task, rather than for generation or ranking tasks.However, if a probabilistic model is used to determine the probability of mutual entailment between sentences, this score can be utilized to rank substitutes if necessary.2, the binary definition can also be adapted to the generation task by iterating over candidate substitutes.\",\"We followed the baby step of progressive alignment and hoping that establishing a set of clean base knowledge, can ease the acquisition of future more complex concepts through comparisons with existing knowledge, analogy and hierarchical abstraction.what about other words? Some concepts can be learned through just visual inputs, like color, whereas other concepts require grounding through different sensory types or modalities, like \\u201chot\\u201d, \\u201cloud\\u201d and \\u201cfast\\u201d.Even more concepts are built upon existing words through abstraction and generalization, e.comparisons can still be used to ground these words, but input to these comparisons could vary from data modalities to computation methods, to abstract representations.how to put words into sentences? This work only focused on the grounding of individual words into visual representations, whereas sentence syntax, grammar, and article structure are yet to be learned.just like in an elementary linguistic class, a teacher would list out several examples \\u201ci shower\\u201d\\u002f\\u201cyou shower\\u201d\\u002f\\u201che shower\\u201d.Humans can learn grammar through what\\u2019s changing and what\\u2019s constant.This could be an interesting next step to look into.Who can offer the supervision? As mentioned at the beginning, human language acquisition is a highly supervised learning process.Babies are rarely inventing new words but learning how adults label objects through generations of conventions.A classroom setting with highly structured curriculum and clean dataset takes a lot of curriculum design and heavy annotation.This is the cost that humans are willing to spend in order to educate human children from kindergarten to college.Maybe it is a fair price that we have to pay in order for artificial intelligence to learn what we want them to learn.About the current work itself, there are several constraints that we are limited to.First of all, due to limited computation resources and data size, we had to take a shortcut by using a pre-trained clip embedding as a starting point for our models.In theory, we could and would love to train our models from scratch, just like how a new born would learn their first language.A dataset like toys-200 could mimic the process of babies interacting with the objects, get a 360 view and help build 3d mental representations.Second of all, like many other continual learning methods, an unbounded memory space is an unrealistic assumption.As more concepts are learned, the memory space would grow fast, so as the search time.An interesting next step could be to reorganize the memory according to the association distances and hierarchical structures.Lastly, our work aims at proposing a novel language acquisition definition and the comparative continual learning method.We used somewhat simple model architecture and image generation models for proof-of-concept demonstration on the method.More sophisticated model architecture and training can be switched for different input modalities and applications.Listed above are several major limitations and future directions based on current work.\",\"Although the new architecture works well on pdtb-like structured data, we are often challenged with texts without clear paragraph structure.This would make it either necessary to pre-process texts and split sentences into semantically closed paragraphs such that our proposed model takes advantage of the surrounding context, or develop a new sentence-based model which was not successful in previous work.Limiting the model to predict only continuous alternative lexicalizations does not highly affect results on the pdtb, but might have a more considerable impact on other text genres, e.\",\"In this paper, we present a novel approach to remove the permutation invariance of the attention module.Specifically, we propose a weight concatenation operation that exactly follows the word order of a sentence, leading to an increase in dimensionality and the introduction of affine transformations aimed at reducing it.Hence, the effect of increased parameter counts cannot be well isolated.While our preliminary experiments show that an increase in the number of parameter counts does not necessarily enhance the experimental results, we acknowledge the increased complexity resulting from direct concatenation and, thus, have utilized the equivalent form of the proposed operation in practice.\",\"., our findings are limited to this specific similarity-based setup and cannot exclude that other similarity-based methods might behave differently.We also did not consider training sizes larger than 1000 instances of asap, and can therefore not speak for how the relative performance of the different methods would be affected by using even more training data.Regarding the experiment on larger training data sizes, we also limited our analysis to asap, so it is necessary to compare the observed effects to those that occur on other data sets.The same goes for our cross-prompt experiments, which were also limited to asap.Other data sets cover other content domains and can thus produce different effects.\",\"One limitation of our proposed method is that it requires the pre-identification of the target mwe in a sentence before paraphrasing it, a task that is not a walk in the park.In particular, it is very challenging to identify what is the \\u201ccorrect\\u201d span of a given mwe, which our model critically relies on.For instance, given the mwe lip service , our model predicts more attention as the best paraphrase, likely because the mwe is usually used as pay lip service to , and attention is one of the few words that fits well in this context.Therefore, the whole phrase pay lip service to should be identified as an mwe instance23 when it is used in sentences like they pay lip service to the idea; however, lip service can also serve as one lexical unit in sentences like it wasn\\u2019t just lip service.1, or verbal mwes that are often followed by specific prepositions because the mlm prediction is affected by the syntactic constraint.24 mwe span identification is also important in our sentence collection process; e.1, the phrase small fry can be used as small fry pan rather than as the mwe meaning \\u201cinsignificant\\u201d, and hence collecting sentences based on string match resulted in one additional cluster that is not relevant to either its literal or idiomatic senses.Another limitation is that our model cannot handle discontinuous mwes such as throw someone under the bus and not.in the least because it is not clear which parts to mask and paraphrase in such cases.Similar problems arise when continuous mwes undergo either internal modification or drastic syntactic transformation.However, note that all of these types of expressions, as well as the pre-tokenisation problem discussed above, become a pain in the neck for any approach that regards an mwe as a lexical unit and learns its holistic embedding.Lastly, our method heavily relies on the quality 23in fact, it is registered as such in some english dictionaries.24in languages where words have grammatical gender such as portuguese and italian, this problem can be more pronounced because context words including adjectives and determiners are affected by gender.Of the clusters and is thus prone to error propagation.For instance, our model using bert always generates large fish as the best paraphrase for the mwe big fish and fails to capture its idiomatic sense , likely due to its rare occurrence in monolingual corpora.One possible solution to this problem is to derive more senses by allowing the clustering method to create more clusters with fewer instances, but that institutes a trade-off between accommodating rare senses and creating too many clusters for common senses; hence, there is no silver bullet.In fact, this problem pertains to the longstanding question among lexicographers: how to \\u201csplit\\u201d and \\u201clump\\u201d senses of words, and how fine-grained the sense distinctions should be.\",\"Our approaches mainly leverage a fixed feature extractor together with a set of individually trained classifiers to mitigate catastrophic forgetting whereas a tunable feature extractor may also be helpful and complement the individually trained classifiers, so a future direction is to design advanced strategies to efficiently tune the feature extractor in combination with our proposed ice based classifiers.In addition, we mainly investigate the classifier drift and demonstrate the effectiveness of our solutions under the class-incremental continual learning setting.Another future direction is to explore similar ideas under other continual learning settings, e., task-incremental learning, online learning, or the setting where new sessions also contain annotations for old classes.\",\"Our method requires balanced data because all attributes share the same normalizing flow.This means that when the training data for one attribute is much larger than others, we need additional training steps to make up such a gap to prevent the jacobian part of the normalizing flow from too much in favor of that attribute.In addition, although we can achieve good results on the data scale of 2.5k or 5k per attribute, our model does not fit well in few-shot scenarios.We can alleviate this problem by obtaining a sufficient amount of single-attribute labeled data from the style transfer tasks.In our experiments, each attribute is considered equally 8see more analyses in \\u00a7d, f, and g.important, which may be different from the practical situation.Fortunately, our control strategy is flexible and can be customized for different demands.\",\"One of the biggest concern people may have is whether approximate unlearning forget the information of the removal data.Approximate unlearning can not ensure exact removal of information already learned in deep neural models, just as its name suggests.Considering that current exact unlearning methods are very time-consuming and hard to apply in practical applications, approximate unlearning is still a direction worth trying and is also effective in reducing the attack risks by attackers or mitigating the harm of toxic data.Another limitation of this work lies in the fact that we have to maintain an extra data set dn and two models af and an in the process of unlearning.Though the extra cost of our kga method is trivial compared to the previous work has to maintain the entire training set), we have to point this limitation out and call for follow-up research to come up with better ways to reduce unlearning costs.Besides, we only explore word-level translation unlearning effect by comparing the generated sentences before and after deleting instances with specific words due to the space limitation.\",\"Our model is trained in an end-to-end manner, resulting in more training time costs than featurebased methods.To eliminate the need for gloss annotations, the ccm process relies on a large amount of sign and translation pairs.The generalizability of the model is restrained by the number of such pairs available.The more ideal end-to-end framework should combine the visual backbone and visual2text encoder into one visual encoder that can be trained end-to-end.In addition, the selection of conceptual words is done according to manually-designed rules now and relies on external toolkits like nltk.\",\"Limitation this work has two main limitations: the performance of the model largely depends on the performance of the annotation model.If the annotation model is too simple, it may cause the performance of the dst model to decline.On the contrary, it will increase the complexity of the overall model and prolong the reasoning time.Even for the labeling model with good performance, the tagging values may also interfere with the dst model.For details, please refer to the analysis experiment.\",\"Adapting plms to our proposed model does not go as smoothly as expected, because there are three different forms of tokenization: the plm tokenizer, the multilingual tokenizer implemented in our proposed model, and the special annotations of numerical values\\u002fentity mentions\\u002flong-winded attribute values in the attribute extraction datasets, which are difficult to reconcile simultaneously.Although our model without plm has outperformed plmbased ones, this does impose a limitation for future explorations.Although re-cnshipnet, one of the datasets used in our experiments, is more accurate with our careful re-annotating, the size of which is still so small that would produce randomness bias during the model training and may affect the final experimental results.Besides, due to the limitation of computational resources, we did not conduct experiments on large language models such as t5 , llama , etc., which may lead to insufficiency of the experiment.The content of pre-existing datasets does not reflect our perspectives.We, the in-house authors, re-annotate one of these datasets, i., re-chshipnet; the purpose of re-annotation is mainly to correct errors and re-balance the ratio of cwa\\u002fowa labels.The annotation may introduce personal judgment and bias, which may bring potential risks.Further, the potential downstream applications of this work include knowledge graph construction, search engine, e-commerce, recommendation system, etc. ; we caution that our proposed method may cause misextraction or false information, and may fail in the case of out-ofdistribution and domain shift, which may harm those applications.\",\".To get the optimal performance from pairranker, one may need to call the model o times for getting the full matrix, thus resulting in a much less efficient solution.We attempted to resolve this limitation by proposing to use multiple rounds of bubble sort methods to reduce the number of inferences needed, and we find it works pretty well.We also want to argue that although the number of inferences can be large for obtaining the best performance with pairranker, those inferences can be executed in parallel because they are totally independent.we agree that automatic metrics have limitations.Human evaluation could provide us with more reliable and comprehensive evaluation results.However, due to the number of models as well as the amounts of generation candidates, we cannot afford large-scale human evaluation.We argue that our use of chatgpt for evaluation is a good alternative, according to recent studies.Also, we would like to highlight that we show the ground truths when using chatgpt to do pairwise comparisions, which is quite informative than the common practice.\",\"\\u2022 the interdisciplinary element \\u2013 at the heart of this work \\u2013 mandates that our results be interpretable and relevant to scholars from the opposite side of the methodological divide.This, in turn, introduces constraints to our framework \\u2013 the foremost is choosing appropriate text-embedding techniques.8, the ability to extract specific lexical features that are important to the classification, to quantify them, and subject them to complementary philological analysis \\u2013 requires that they be interpretable.This constraint limits the ability to implement state-ofthe-art language-model-based embeddings without devising the required framework for their interpretation.Consequently, using traditional embeddings \\u2013 which encode mostly explicit lexical features \\u2013 limits the complexity of the analyzed textual phenomena and is therefore agnostic of potential signal that is manifested in more complex features.\\u2022 in text stylometry questions, especially those related to ancient texts, it is often problematic to rely on a benchmark training set with which supervised statistical learning can take place.This, in turn, means that supervised learning in such tasks must be implemented with extreme caution so as not to introduce a bias into a supposedly-unbiased analysis.Therefore, implementing supervised learning techniques for such tasks requires a complementary framework that could overcome such potential biases.In light of this, our analysis involves predominantly unsupervised exploration of the text, given different parameterizations.\\u2022 our ability to draw insight from exploring the stylistic differences between the hypothesized distinct texts relies heavily on observing significant overlap between the hypothesized and unsupervised partitions.Without it, the ability to discern the similarity between the results of our pipeline is greatly obscured, as the pipeline remains essentially agnostic of the hypothesized partition.Such a scenario either deems the parameterization irrelevant to the hypothesized partition or disproves the hypothesized partition.Breaking the degeneracy between these two possibilities may entail considerable additional analysis.\",\"Testing necessary, but not sufficient conditions for tutoring with llms.In this paper, we test the abilities of llms to perform the functions present in intelligent tutoring systems, namely generating explanations and corrections.There are also other desirable properties, like the ability to answer direct questions from a student or the ability to present content engagingly, which are beyond the scope of this paper.Indeed, those properties are some of the areas where llms probably excel relative to traditional its.We have only explored a necessary condition \\u2013 are models able to reliably teach \\u2013 not a sufficient set of conditions for the evaluation of tutoring using an llm.In this paper, we focus on tutoring in rudimentary mathematics.While this is useful \\u2013 it is a necessary condition for a useful tutoring system, especially because arithmetic skills are used in almost all domains of learning \\u2013 there are many other domains to which we might want to apply tutoring.Llms may have greater or lesser aptitude in these domains than in arithmetic.Evaluation at the level of gradeschool mathematics tells us that these models are still error prone, but does not necessarily tell us how close they are to usefulness in tutoring other subjects.we aim to examine the differences in ability of code scratchpads and text scratchpads for the purposes of tutoring.While this paper provides evidence in that direction, we only compare two gpt-3 models: text-davinci-002 and code-davinci-002.The amount of manual effort required to evaluate explanations and correction limited the number of comparisons we could conduct, as did the limited number of highly performant code\\u002ftext generating models.\",\"We reflect on the limitations of our model below: 1.our experiments are based on large everyday household datasets.Our language model is pretrained with web data, which helps it handle such householdrelated procedures well.However, when applied to other more specialized domains like medical procedures, language models might suffer from the domain gap and impact overall model performance.the language model has excellent planning ability given the ground truth start and goal steps.However, it is still hard for the language model to generate very long sequences of steps.When the planning horizon t increases, the performance of our model drops quickly just as other methods do.in real-world applications , a good model should be able to dynamically adjust the plan given external feedback.For example, when the execution of one step fails, the model will need to re-plan as soon as possible.Our model does not possess such an ability so far, since our planning approach is offline.We leave this direction for future research.\",\"Experiments on other types of reasoning tasks.And temporal sequences, tracking shuffled objects from big-bench.However, most tasks there are highly templatebased and hence the reasoning steps have little variations, both within each example and across different examples.This makes it difficult for us to conduct our ablation studies on these tasks.Take the example of last letter concatenation, a task about concatenating the last letters of a given sequence of words.Here, every step in the rationale except the last is in the form \\u201cthe last letter of x is y\\u201d where x is some word in the given sequence and y is the last letter of x.Hence, the language templates are the same and there is no sense of order among the steps , and our ablation settings will not apply well.a more systematic treatment of \\u201cinvalid reasoning\\u201d.We manually write rationales with invalid reasoning for the experiments in \\u00a74 since automatically synthesizing such rationales turns out to be challenging, mostly due to the informal nature of the tasks we experiment on., following the categorizations of informal logical fallacies.our intrinsic evaluation of the generated rationales is based on the correctness of bridging objects, which, even though is a good indicator of the quality of language templates in our experiments, may not be a good metric in general cases.It also relies on ground truth bridging objects, which are usually not available and costly to annotate.Toward this end, one direction we want to explore further is to develop ways to conduct more comprehensive and reference-free intrinsic evaluations.Have also done promising work along this line.\",\"One limitation of our study is that we only evaluated our method on the t5 architecture.Further experiments on other architectures could be useful to determine the generalizability of our findings.Additionally, as in previous sota, our model also did not produce better results for the hotel domain, even though it did improve performance in general.We have attempted to explain why this domain is more difficult, but more research is needed to fully understand the reasons for this variability and to create methods that can improve performance across all domains.\",\"While ice does not require fine-tuning on large amounts of data, it requires querying a powerful llm at inference time.This can be a pay-per-use model or an open-source model such as bloom.This makes a downstream system that uses ice reliant on an external dependency, which carries the risk of the external dependency failing.3summeval annotations are all based on the source, and the src-to-hyp version of bartscore performs best across dimensions for this benchmark.We use this version for all dimensions, leading to identical scores.We format bartscore results unlike rouge-l because in theory bartscores can differ across dimensions for an arbitrary benchmark.Relatedly, in this paper, we are limited due to monetary constraints in a variety of experiments we perform.For instance, we restrict ourselves to text summarization and use samples of benchmark meta-evaluation suites during some of our experiments.\",\".in this work, we demonstrate our framework\\u2019s performance on sentence classification or pair classification tasks.\",\"The main limitation of our concatenation-based multi-ve models is efficiency: the models are significantly slower than single-ve models because of the additional visual tokens in the input; the 3- ve model requires almost twice the time to train compared to the single-ve models.Also, in cases where images are not pre-encoded, multi-ve setups are significantly slower at inference time.However, as mentioned before, we concatenate the tokens for analysis purposes only and leave more efficient alternatives like resampling to the future.we focused on single-stream transformers and did not take into account dual-stream or other multimodal transformer architectures.we only experimented with three popular ves.we do not pre-train our multimodal models on intermediate, auxiliary multimodal tasks as achieving state-of-the-art is not our goal.\",\"This work follows in line with those studies where unsupervised semantic parsing relies on the dependency parse trees of texts.Although it enables us to leverage advanced syntactic parsers and to disentangle the complexity in syntactic analysis from that in semantic parsing, the errors made in the dependency parse trees created for input texts could propagate to semantic parsing.even though an improved mh merge-split sampler was proposed in this study to speed up the mixing and convergence of markov chains by leveraging pre-trained distributed representations, the computational effort required to fit the model can still be substantial, especially for a large body of texts.We plan to improve computational efficiency beyond that offered by this study by starting with good initialization and updating the state space in a distributed and parallel manner.\",\"Our current freqmlm techniques tend to fail on lid predictions when the linguistic differences between languages are small.For example, english and spanish are quite close: they are written in the same script, english and spanish share a lot of common vocabulary.the strategy to select the best layer for drawing residual connections in resbert is quite tedious.For a 12-layer mbert, we train 10 resbert models with residual connections from some intermediate layer x \\u2208 {1, \\u00b7 \\u00b7 \\u00b7 , 10} and choose the best layer based on validation performance.we are considering parameterizing the layer choice using gating functions so that it can be learned without having to resort to a tedious grid search.If the embedded language in a code-switched sentence has a very low occurrence, we will have very few switch-points.This might reduce the number of maskable tokens to a point where even masking all the maskable tokens will not satisfy the overall 15% masking requirement.in our experiments, we compensate by masking around 25%-35% of the maskable tokens.\",\".Imagine a case where parser a claims to outperform parser b, and closer inspection reveals that a and b differ in their outputs on only two sentences.For one of these sentences, a\\u2019s output has a higher f\\u2081 than does b\\u2019s; for the other, the opposite is true, but the difference in f\\u2081 is smaller.Now, the claim that a outperforms b becomes a claim that the parse that a produces for the first sentence is better than the parse that b produces for the second.And yet, as indicated by the judges\\u2019 disagreements in the second task, it is not always possible to make these kinds of judgements.\",\".\\u2022 threats to internal validity: the main internal threat to the validity of our research comes from where we present a qualitative study on the variation of aliases.We are unable to cover all cases in the qualitative study.For example, the entity of d030342 in table 3 has 778 unique aliases.It is impossible to show all aliases to readers.To help mitigate this threat, we try to show as many examples as possible in a limited space.\\u2022 threats to external validity: the main threat to external validity arises from the potential bias in the selection of experimental datasets, attacking target models and off-theshelf ner and entity linking tools.To mitigate this threat, we experiment with multiple datasets, models and tools.For experimental datasets, we choose the three most popular docre datasets.We believe that these three datasets are broadly representative in this research community.For attacking target models, we choose three typical models ranging from non-contextualized sequence-based to graphbased, and to contextualized transformers models.For off-the-shelf ner\\u002flinking tools, we comprehensively investigate five state-ofthe-art ner taggers and two entity linkers.\",\".First, we focus on left-to-right code generation without considering right-side and cross-file context, which can be used to determine broader categories of errors with improved precision.Second, each static analysis tool has its own limitations.Thus, the presented analysis is limited by pyflakes\\u2019s accuracy and coverage to detect certain code issues.\",\"Our method does not apply to vl models where the cross-modal encoder layers are relatively lightweight.For example, the vision encoder is much more computationally expensive than the cross-modal encoder for vl models like albef and x-vlm , therefore, the end to end inference speed improvement is marginal.\",\"In this work, we only focus on designing strategies for plms with the mlm-style pre-training objective, and do not account for other types of pre-trained language models such as discriminative plms.However, as there are recent works that aim to design prompts for discriminative plms , patron can be potentially combined with them to improve the data efficiency.We are also aware that there exists advanced fewshot fine-tuning techniques for plms recently.We argue that patron does not rely on a specific fine-tuning method, and can be combined with them to further improve the performance.Lastly, as prompting methods have been widely adopted to other tasks such as natural language inference and relation extraction , it is possible to extend our method to these tasks.\",\"Our model may have several limitations: as a memory-based model, our model consumes additional space to store typical samples and static prototypes, which causes the performance to be influenced by the storage capacity.Although we propose memory-insensitive relation prototypes and memory augmentation, our model still relies on the selection of typical samples.The selected samples of low quality may harm the performance of our model.The recent progress in large language models may alleviate catastrophic forgetting and overfitting, which has not been explored in this paper yet.\",\"We acknowledge that there are certain limitations of this framework.First, generation-based models have latency issue due to the autoregressive generation.Thus, we will explore non-autoregressive and semi-autoregressive methods in a future study.Second, the knowledge is only stored in model parameters which limits the capacity of the model to make the smarter trigger decision through factchecking and generate a valid rewrite.To this end, we intend to consider a retrieval-augmented generation to incorporate external knowledge to improve performance as well as incorporating more contextual and personalized signals into the model.Moreover, generative models can also pose quality control challenges, such as hallucinations.To mitigate this issue, we will add constrained decoding to control hallucinations.\",\"This work introduces the general idea of incorporating attribution into knowledge distillation, and there are three potential limitations.First, although ad-kd chooses integrated gradients for attribution, there are actually other attribution methods which can also be fitted in our framework.The question of whether these methods perform better than integrated gradients when combined with knowledge distillation is still unclear.Second, we conduct experiments on bert of different scales and have not yet validated the effectiveness of ad-kd on other model structures.Third, while we only perform task-specific knowledge distillation in our experiments, applying ad-kd to task-agnostic knowledge distillation is also worth investigating.\",\"In this study, the limitations can be summarized into two major aspects: the usage-based approach being employed in our work has the ability to extract most of constructions, while a small portion of non-contiguous constructions are neglected.These noncontiguous constructions are probably fragmented into multiple independent constructions.since our constructions are mainly learned from the formal corpus, which has less colloquial expressions.\",\"The paper presents a dependency-aware symbolic reasoning approach for logical data-to-text generation.All technologies built upon the largescale plm more or less inherit their potential harms.Besides, we acknowledge some specific limitations within our methods: 1.Data-to-text generation is essentially a one-tomany problem since there is more than one plausible and logically-consistent description given a specific table.Our approach has little control over the diversity and the logical form of the generated template.It is also possible that our approach only generates trivial or naive descriptions if trivial data dominate in the training dataset.our work mostly focuses on the named entities in the description, but logical consistency is not all about entities.our table-compatible programming language is mainly designed for simple flat tables, and extra operators are necessary before it could be applied to all tables, especially hierarchical tables where its header exhibits a multi-level structure.Currently, it is difficult to directly integrate gpt-3 or other llms into sortie to substitute the plm backbones.The reason is that llm can not be used for encoding since we have no access to the dense representation in an llm.\",\"Aside from the still-improvable performance of the classification models we evaluated, our work is limited in two ways: the nature of what is considered appropriate as well as the difficulties that arise during corpus creation in nlp in general.We point to the subjectivity in perception regarding appropriateness, which is also displayed and discussed in the paper by the inter-annotator agreement.Many sociocultural factors can influence this perception within cultures, such as age, gender, education, or ethnicity.We sought to account at least for gender by including both male and female annotators for all arguments.However, we encourage further studies that focus on other factors, as we expect appropriateness to be seen differently, primarily across cultures with varying styles of debates.Since our corpus contains only arguments written in english and is annotated by native english speakers, it may also be insufficient to generalize across languages.Moreover, appropriateness perception is likely subject to change over time.Although we collected arguments from different years, we see long-time limitations to our corpus.In general, it also depends on the expectations of the discussion participants, which are to some extent predetermined by the context.In that regard, the context of our corpus is solely that of discussing controversial issues with strangers on the web.\",\"There are two main limitations to this study.First, because of the lack of downstream datasets, we did not evaluate kg-flip on other downstream vl tasks in e-commerce.Therefore, the robustness of the kg-flip model on other downstream tasks requires further investigation.Second, the experimental results empirically show that the proposed knowledge-guided pre-training objectives are more effective in producing vl representations that capture subtle distinctions between samples than the standard objectives.However, a theoretical analysis of the effectiveness of our knowledge-guidance strategies is lacking.\",\"A limitation of our work is that we cannot directly apply our methods to the few existing revisionbased corpora from other domains for multiple reasons: on the one hand, those corpora do not contain histories with more than one revision but only before-after sentence pairs).Some also consist of less than 1000 sentence pairs, rendering the quantitative experiments considered in this paper pointless.On the other hand, additional metadata useful for our analysis is either not available at all or only for a limited number of instances that is insufficient for training models.Furthermore, the methods we evaluated utilize distantly supervised labels based on the assumption that each revision improves the quality of the claim and additional annotations provided by human editors.These annotations suffer from being coarse-grained, consisting of mainly three classes.However, each of the improvement types can be represented by several more fine-grained revision intentions.A point that we did not consider as part of this work is whether certain revisions can affect or inform future revisions within the same debate, for example, rephrasing of arguments to avoid repetition or ensuring that all claims use the same wording for the main concepts.Often, such relationships are implicit and cannot be derived without additional information provided by the user performing the revision.We believe that collecting datasets and developing approaches, which enable distinguishing more fine-grained types of edits and implicit relationships, could not only enable deeper analysis and training more fine-grained improvement suggestion models, but also allow for better explanations to end users.However, it should be noted that some of the considered methods rely on deep learning and have certain limitations when it comes to underrepresented classes, where the number of available training instances is very low.This is especially important when considering the task of claim improvement suggestion.We also point out in this regard that we only use the base versions of the bert, electra, and deberta models due to resource constraints.The results may vary, if larger models are used.While common types of improvements likely differ across other domains and communities, we stress that our approaches are entirely data-driven, and are not tied to any specific quality definition.Therefore, we expect our data processing and filtering methods as well as the considered approaches to be applicable to other domains, where historical collaborative editing data similar to ours is available.When it comes to practice, several issues require further investigation, such as how to integrate recommendations in collaborative editing and educational environments, whether the recommended improvements will be accepted by users, and how they may impact the users\\u2019 behavior.\",\"The numbers in this survey are limited to papers published in the acl anthology and isca proceedings.However, we also included papers as related work from other resources if they are publicly available and accessible.In addition, the category in the survey does not include the code-switching type since some papers do not provide such information.\",\"Although simple achieves great performance with size reduction and generation speedup on various generative language tasks, it is interesting to explore combining the stage of mask learning during the pre-training.Then, one pre-trained model can be applied to downstream tasks with any required sparsity with a stage of fine-tuning.In addition, the pruning of the larger generative pre-trained language models during the fine-tuning is also worth trying.\",\"Though the formulation of the task allows exploring several different settings , in this work, we investigated only the label-balanced setting.another limitation was the limited exploration of novelty detection methods, as a number of methods have been proposed in the recent times.However, we study only a limited set of methods since the focus of this work is on formulating and exploring noveltytask.Lastly, we note that noveltytask is a controlled task\\u002fframework for evaluating a system\\u2019s ability to deal with novelties and not a method to improve its ability.\",\"The limitation of our work includes the following aspects: 1) the instruction-style question which measures the quality of generated texts from different dimensions still needs manual design.Although the questions in our experiment have already involved typical dimensions in text summarization, dialogue generation, and data-to-text generation, we admit that it is hard to cover all the dimensions in various nlg tasks.We believe that this is not a severe problem because we can refer to the definition and human annotation instructions of each dimension, which are commonly formulated as questions.2) due to the limitation of computational resources, the largest base model used in our experiment is flan-t5-xl with 3b parameters.\",\"Help me think is a model agnostic approach that allows users to inject facts to accomplish tasks with a variety of large language models through simple but additional experiments are needed to establish its effectiveness on new language models with different training paradigms and capabilities.The entire study is conducted only with tasks of english language.Expanding the scope of help me think to other languages will increase the scope for non-expert users.A large scale evaluation setup is further needed to reach help me think to non-expert users.\",\"Limitation of sequence length on a single device.We have shown that sequence parallelism can handle longer sequence and is more memory-efficient than sota.7\\u00d7 maximum batch size than tensor parallelism when scaling up to 64 gpus.Unlike both tensor and pipeline parallelism, sequence parallelism is not limited by the smaller hyper-parameters.Therefore, our sequence parallelism can be adapted as long as the sequence length is divisible by sequence parallel size.With efficient attention, sequence parallelism can handle sequence with over 114k tokens, which is over 27\\u00d7 longer than existing efficient attention works holding the whole sequence on a single device.We used a language model to evaluate our system, but it can also be adapted to vision tasks.This work paves the way to process large images by vit as a larger image means more patches or longer sequences.Limitations in order to perform communication between subsequences during training, the use of sequence parallelism can result in increased communication costs, which in turn can slow down the training process.However, by combining sequence parallelism with pipeline parallelism, this issue can be alleviated and the communication cost can be made comparable to advanced forms of model parallelism such as tensor parallelism.Nonetheless, sequence parallelism still incurs higher communication costs than vanilla data parallelism.While sequence parallelism is effective for training of unidirectional attention models as well as training and inference of bidirectional attention models, it poses a challenge for unidirectional at- tention models inference due to the autoregressive decoding process.This means that different devices cannot compute in parallel, resulting in reduced throughput and decreased gpu utilization.\",\"We focus on augmenting the hidden representations of a plm.Thus most of our baselines, such as dropout and variational information bottleneck methods , do not require unlabeled data.For a fair comparison, we assume that the unlabeled data is not available.Therefore, only the limited labeled training set are used to train the autoencoders in our experiments.However, such unlabeled generalor in-domain data are easy to obtain in practice, and can be used to pre-train the autoencoders with unsupervised language modeling tasks, which may help further improve the performance.ethical impact deep learning has demonstrated encouraging performance on a wide range of tasks during the past few years.However, neural models are data hungry, which usually requires a large amount of training data to achieve reasonable performance.It is expensive and time consuming to annotate a large amount of data.Pretrained language models have been proven to be useful to transfer knowledge from massive unlabeled text to downstream tasks, but they are also prone to overfitting during fine-tuning due to overparameterization.In this work, we propose a novel method to help improve model robustness in the low-resource scenarios, which is part of the attempt to reduce neural model reliance on the labeled data, and hence reduce annotation cost.Our method has also demonstrated promising performance improvement on cross-lingual nlp tasks, which is also an attempt to break the language barrier and allow a larger amount of population to benefit from the advance of nlp techniques.\",\".1, the generation performance relies on the parser performance, which is strong enough for english but still less satisfactory for other languages.Dedicated methods need to be considered to compensate for the weak parser performance if we want to extend our method to more languages.In this work, we consider two nlg tasks with semantic equivalence to testify if the proposed method can convey the source semantics accurately by following the target syntactic grammar.Other tasks such as summarization and dialogue generation can also be tested, where the semantics are not equivalent between the source and target.To train the neural decoder parallelly, we break down the source-target dataset into a triple set.However, the global dependency of the syntax parse tree is not considered, which can deteriorate generation performance.Due to the recursive encoding of the syntax contexts, our model\\u2019s inference speed is approximately half that of the seq2seq counterpart.to further demonstrate the effectiveness of our method beyond pretrained language models.\",\"When building lmentry, an important criterion for a task is the ability to easily create many examples.One of the benefits of this approach is in handling model memorization: if a model achieves very good results on some task, one can quickly create additional examples to verify whether the success is indicative of task competence or originating from example memorization.That being said, a few lmentry tasks are inherently limited in this regard.For example, the number of possible examples of word starting with letter is limited by the number of letters in the english alphabet.We opt to include these tasks in lmentry as we judge their benefits to outweigh this limitation.To mitigate this disadvantage, lmentry task data also includes a \\u201ccanary string\\u201d.While a canary string helps in filtering out data from training corpora, it is not a catch-all solution.Additionally, our experiments include some models whose exact implementation details are yet to be publicly available.We include these results as these models have the best lmentry performance at the time of writing.To shed as much light as possible, we provide the predictions of these models along with all available metadata from the api.This includes the logits of the final layer for the top 5 tokens at each position.\",\"Our experiments reveal that simultaneously incorporating more rules into the loss produces better performance in the task of interest.These results indicate that rules working in tandem significantly complement supervision coming from both sources of direct annotation under a fully declarative loss.Nevertheless, controlling the influence of each term in the loss is crucial for training stability.From this perspective, the possible benefits of increasing the number of rules in the loss come at the cost of more difficult learning.Due to hardware limitations of the protected environment server that stores the datasets we use, roberta-base was the best model that could fit in the available gpus.Although other pre-trained embeddings could provide better performance, we argue that this is orthogonal to our contribution of incorporating indirect supervision under a fully declarative learning framework.Moreover, integrating logic-driven frameworks and prompt-based models like t5 is an interesting future line of work.Choosing roberta as the underlying embedding foundation of our system introduces all the inherent limitations of large language models.From this standpoint, we envision the application of these sorts of systems as a humanguided tool used only for counselor training and quality assurance, and never for real counseling sessions.\",\"The proposed kosbi addresses social bias based on korean culture with the korean language.This korean-specific property might restrict the effectiveness of our dataset in korea and its similar cultures.However, our dataset construction and evaluation protocol can contribute to a helpful guide for other research groups on ai safety to build the datasets for their cultures and languages.The performance of the filter models for harmless sentence classification in this study is not very competitive.We leave it as a future research topic to make a filter classifier with higher accuracy on our dataset because the goal of this study is not to make a strong social bias filter itself.\",\"While our study provides valuable insights into the use of prompt-based methods with gpt-3 for gec tasks and its controllability, several limitations should be acknowledged.Focus on gpt-3: this study exclusively examines gpt-3 as the language model for gec tasks.While gpt-3 has shown remarkable performance in various nlp tasks, other pre-trained language models, such as gpt-4, may offer different results.A broader investigation that includes other language models would provide a more comprehensive understanding of the applicability of promptbased methods in gec tasks.Limited evaluation metrics: the evaluation of gpt-3\\u2019s performance and controllability in our experiments mainly relies on quantitative metrics, such as edit distance and task scores.These metrics may not fully capture the nuances of grammatical error correction or the model\\u2019s ability to adapt to different learning scenarios.Additional qualitative analysis, along with more diverse evaluation metrics, could provide a richer understanding of the model\\u2019s performance and controllability.Variability in examples: while our study highlights the importance of example selection in fewshot settings, we do not thoroughly explore the impact of example quality or diversity.The effect of using different types of examples or a more diverse set of examples remains to be investigated, which could further inform the design of effective example sets for prompt-based gec tasks.By addressing these limitations in future research, we can further advance our understanding of the performance and controllability of prompt-based methods with gpt-3 and other language models in gec tasks and beyond.Potential fine-tuning on test data: there is a possibility that gpt-3 has been fine-tuned on the test data we are using, which might explain the higher evaluation scores compared to previous research.As this information has not been disclosed, we are unable to verify it at this time.This point should be taken into consideration when interpreting our results.\",\"We evaluated the most widely used distillation objectives including prediction layer transfer, hidden states transfer and attention transfer.However, some objectives are not included in our evaluation due to missing implementation details in their paper.In task-specific setting, since code and implementation details are missing for task-agnostic setting.New objectives are increasingly appearing for model compression in the field of computer vision, such as wasserstein contrastive representation distillation and distillation with pearson correlation , which can be included to have a broader scope of distillation objectives evaluation.This work empirically studied the impact of the teacher layer choice for initialization and training objectives, however, further analysis is needed to understand why lower teacher layers are essential for initialisation, and why attention transfer behaves consistently well under various teacher layer choices in the task-specific setting, while hidden state transfer does not.\",\"Despite our efforts to enhance the efficiency and minimize the memory cost of our models, large language models still demand considerable time and memory resources, which remains a limitation of our work.Given sufficient time and computational resources, we could explore the possibility of increasing the batch sizes and running additional epochs to further optimize the model\\u2019s performance.While our final system excelled in relevance and factuality aspects, it was relatively poor on readability, which represents a potential area for improvement.To improve the robustness of the model, given additional time, we would use a combination of machine learning and list-based approaches to identify arcane words and technical terms and substitute them with their easy-tounderstand synonyms.With these procedures, we believe that we can make our summaries more readable.\",\"Due to the limited resources, we could not experiment with this approach for larger language models such as roberta-large and bert-large.It would be interesting to investigate the performance of adept with larger lms.In addition, we only evaluate the adept approach on seven downstream tasks.It would also be interesting to test it on more broad natural language processing tasks, such as information extraction, natural language generation, question answering, and so on.Broader impact as discussed earlier, fine-tuning large pre-trained models for downstream tasks can be really expensive.The adept approach can help ai practitioners to assess the abilities of lm without using a lot of resources.This approach of prompt tuning can also help smaller end users to take advantage of harnessing the power of lm with the minimal resources they have.It can be used by social scientists, non-profit organizations, etc.To create a positive impact in society in spite of limited computing resources.\",\"The ses in this paper are mainly transformer-based ses, and we are not sure whether the observations hold for other ses.However, considering that transformer-based ses dominate the current nlp community, we think it is fine to only evaluate 59 transformer-based ses.Another limitation is that the sentences in heros are converted from reddit, which is an online forum and the texts on reddit may be more casual and informal.This makes the sentence pairs in heros tend to be more informal.Users should note such a characteristic of the sentence pairs of heros and beware that the results obtained using heros may be different from the results obtained using more formal texts.An additional limitation is that there can be more diverse rules to create different sentence pairs other than the six subsets included in heros, and our paper cannot include them all.As a last limitation, during the construction of heros, we remove sentences that are ungrammatical based on language-tool, so our results may not generalize to ungrammatical sentences.\",\"Tasks in bgglue the bgglue benchmark is comprised of nine challenging nlu tasks, including three token classification tasks, one ranking task and five text classification tasks.While we cover three different types of tasks in the benchmark, we are restricted by the available resources for bulgarian, and thus we could not include some other nlp tasks, such as language generation.We also consider only nlp tasks and we do not include tasks with other\\u002fmultiple modalities.Finally, some of the tasks are of similar nature, e., we include two datasets for ner and two for credibility\\u002ffake news classification.Domains in bgglue the tasks included in bgglue span over multiple domains such as social media posts, wikipedia, and news articles and can test both for short and long document understanding.However, each task is limited to one domain and the topics within the domain do not necessarily have full coverage of all possible topics.Moreover, some of the tasks have overlapping domains, e., few-shot and zero-shot in-context learning and instruction-based evaluation, multi-task learning, etc.Model biases in this work, we did not explore whether the datasets in bgglue contain unwanted biases, which could also lead to potentially hazardous behavior of the baselines we trained in our experiments with the bgglue benchmark.\",\"This work is about document-level nmt, we focus specifically on methods that improve the model performance for long input sequences.Due to constrained resources, this work has several limitations.To be able to train all methods including the inefficient baseline approach, we have to limit the context size to 1000 tokens.While we do a comparison to existing approaches, other approaches have been proposed to improve the performance of systems with long context information, which we do not compare against.We run experiments on three different tasks, but two of them are low resource and two of them translate into german, which was necessary because we only had access to german language experts for preparing the evaluation.\",\"The main downside of ssmt is its computational complexity.Our architecture introduces additional computation in 2 way.Firstly, the decoder conditions on the characterlevel history of the target sentence, so it has to process more tokens than a standard subword decoder.Secondly, the dynamic programming algorithm requires more computations than standard mt models training on pre-segmented datasets.In practice, ssmt takes an order of magnitude longer to train than models training on a pre-segmented dataset.Dynamic decoding also adds computational complexity to testing, although this is less of an issue since test set sizes usually permit run times within a few hours.It would depend on the practitioner to decide whether the performance boosts obtained by ssmt justify the longer training and decoding times.However, since ssmt is particularly strong for data scarce translation, the computational complexity might be less of an issue.For translation directions like english to swati, training times are quite short for all models , so the increased training times are manageable.\",\"In this work, we focus on small and medium size models , while recent work in large language models targets models with billions of parameters.It is unclear how well the performance improvement from the examined network architecture would translate to other model sizes or baseline architectures, e.further on, it is unclear how these findings may translate to other application domains and datasets, or impact other nlp tasks, such as document retrieval\\u002franking.\",\"While this methodology does not require the large amount of text data utilized by more advanced models, it does depend on access to grapheme\\u002fphoneme alignment information for all input words.This does limit the usefulness of the model for languages with little linguisticallytagged data available, though the success of the small deri knight corpus does indicate that the model architecture can be made to function effectively with a limited amount of annotated data.\",\"Despite promising results, we also observe that refreshing and querying the datastore during training is time-consuming.Our proposed training framework usually takes 3\\u00d7 \\u223c 4\\u00d7 training time.we include a training loop to dynamically use the latest datastore to inject knowledge into neural networks.However, we still find that the knn knowledge still helps the inference even after our training loops, demonstrating that there still remains space to improve the effectiveness of knowledge injection.\",\"A general limitation of this line of work is that most of the results are primarily confined to artificial datasets.Although such formal languages provide us with a controlled setting and clarity regarding the precise nature of the problem, the relation to practical tasks remains unclear.Hence, while our results highlight the contrast in the performance between the two types of architectures, its precise implications on real-world tasks remain unclear.There are two negative results that do not support our hypothesis.All the experiments discussed in the main paper are on strings of fixed lengths.We conducted some experiments on tasks with variable length sequences which in some sense have low sensitivity.The tasks can be seen as a variable length extension of sparse parities and sparse majorities.Unlike the fixed length setting, we found both lstms and transformers perform similarly on those tasks.Although we found transformers to consistently converge to low sensitivity functions in the case of boolean functions, we did not find similar behaviour on sentiment classification datasets such as sst and imdb.A caveat with empirical studies such as this is that the results depend on the hyperparameters and other aspects of the experimental setup.While we have tried to be as thorough as possible with hyperparameter tuning, there is always a chance that the results or behaviour could differ for some hyperparameter.\",\"Our proposed multimodal grammatical error correction model is based on a seq2seq generative framework, which utilizes different encoders to extract information from each modality, and then fuses them to provide input to an autoregressive decoder.However, in this work, we did not explore the use of a sequence tagging framework, which may be a consideration for future research, as it has the advantage of faster decoding speed.Additionally, this study focuses on the use of audio representations of the source-side of gec data, rather than the target-side, to construct multimodal gec data.Our further analysis concludes that our proposed multimodal gec model has limitations in correcting certain minor error types when compared to text-only gec models.\",\"One potential limitation of our method is that it induces an extra cost of estimating training dynamic statistics of the data samples to characterize them based on how they incorporate into the model\\u2019s learning.This may be more expensive for tasks and datasets with a large number of classes.\",\"While our approach does require domain-specific information extraction models to extract structured representations of novel misinformation claims for easy aggregation and review, there is significant prior work on event extraction that can be adapted to extract check-worthy claims.Furthermore, we argue content moderators or factcheckers are likely to be more effective when focusing on one claim type at a time , rather than reviewing a mixture of claims on multiple topics.Our covid-19 case study also makes use of \\u201cmock\\u201d content moderators, rather than employees or contractors working for social media companies or fact-checking websites.However, we believe this methodology still provides valuable insight that would not be publicly available otherwise, as social media companies do not currently publish extensive details about their content moderation processes6 and fact-checking websites vary widely in policy and have been shown to provide inconsistent claim classification.Some prior user studies have also shown laypeople can be good at judging the veracity of claims or reliability of news articles.As of late november 2022, twitter has suspended enforcement of its covid-19 misleading information policies such as the one we target in this paper.7 however, per the associated press article, one of the possible reasons for the suspension was that twitter has \\u201cstruggled to respond to a torrent of misinformation about the virus\\u201d with many \\u201cbogus claims about home remedies\\u201d still on the site despite the previous enforcement of policies.While we do not have details about the internal automated systems twitter has in place to assist with content moderation, an end-to-end early detection system might have helped stem the spread of misinformation on the platform.Additionally, despite the lack of official policy enforcement, our system can still be used by third-party fact-checking websites or researchers to measure and report misinformation 6 usiness\\u002ftwitter-content-moderation.Finally, the main goal of our work is not to create a system for covid-19 misinformation detection but rather to propose a framework that allows for a fair and realistic evaluation of early misinformation detection systems in any domain.\",\"Data diversity: as a template-generated dataset, kitmus does not reflect the full diversity of natural data.However, we do not attempt to emulate the diversity of natural datasets.Using templates over natural data for diagnostic purposes has a few advantages.Templates facilitate control over the source of a certain type of knowledge, which may not be possible to do with more natural datasets like ontonotes.This allows us to isolate the model behavior we want to probe.We also take several steps to add diversity, like using multiple templates, sampling from large resource pools, random shuffling of entities, addition of noise sentences, and canonical data splits with non-overlapping templates and resources.To prevent spurious factors at lexical level, the templates are hand-crafted to remove surface cues and validated in a study with human participants.Background knowledge assumption in lms: the results of our work is based on the assumption that pretrained lms have access to background knowledge about real occupations.To verify that the pretrained lms evaluated in this work contain background knowledge mapping occupations to situations, we ran a lama probe on bert and elmo.Given the template \\u201cthe work of a [mask] is [situation].\\u201d, we compared the probabilities the lms assigned to all single-token occupation names used in kitmus.Bert assigned higher probabilities to the correct occupation than to any other occupation for 90% of occupations.Elmo assigned the highest probability to the correct occupation for only 45% occupations, which might contribute to explaining why the elmo-based model c2f generally performs worse than bert4coref on the background-pretrain variant kitmus, which requires such knowledge about occupations.Root word overlap: one potential limitation of testing for non-fictional background knowledge like \\u201cfirefighters put out fires\\u201d is that the natural occurrence of the root word \\u201cfire\\u201d in both occupation and situation might enable models to solve the task without having access to background knowledge.An analysis of trigram overlaps in all occupationsituation pairs shows that 45% of non-fictional occupations have at least one overlapping root word.However, a comparison of performances on those samples with and without root word overlap showed neither systematic increase nor decrease for any model, indicating that models do not rely on the root word mappings.Results split up by root word overlap can be found in table 10.Train set size: the size of the train set for kitmus, 2000, was chosen to mirror that of gap.To evaluate whether the failure of models to learn the task is due to the relatively small number of samples observed during training, we re-generated all variants with 5000 train examples and repeated all experiments.We observe an increase in the magnitude of performance both in bert4coref and c2f on those variants where performance was higher than random performance with 2000 examples, but not on those that were equal to or below random performance.Consistent with previous results, bert4coref performs well on backgroundpretrain and background-both, but not on all fictional background-inference variants.\",\"In our current experiments, prompt-based methods are primarily storage-efficient or parameterefficient solutions.Since these methods all require backpropagation to the bottom layer, the training time of prompt-based methods are closely resembles that of traditional fine-tuning approach.\",\"Although the proposed method achieves significantly improved ood detection performances compared to the baselines, but poe can not be applied to a naive lstm, and rnn because our ood construction is based on an attention score of the plm.While we adopted a masking method using attention scores in this paper, it is not clear that tokens with high attention scores have the most direct impact on the model\\u2019s predictions.\",\"There are several characteristics of the presented analyses that limit the scope of conclusions that can be drawn.We discuss how each of these limitations affect the takeaways of our results below.Number of chatbots the generalizability of our metric analysis results is constrained by the fact that we were only able to include conversations from 4 chatbots in our analyses.We did our best to choose chatbots representative of the field and seem to have selected a fairly diverse group of models.However, it is possible that not all results we found in our metric analyses will generalize when evaluating other chat models.One possible example is the number of partner contradictions we observed among our 4 chatbots , which may be similar by coincidence.If other chatbot models indeed differ more substantially in partner contradiction rates, our sensitivity metric analysis may have underestimated the sensitivity of our partner contradiction metric.In general, including a larger number of chatbots in a metric analysis will improve the chance that its results will apply to new chatbot models.use of surgers as evaluators we perform our analyses using only a single evaluator group.This choice of evaluator group does not harm the replicability of our methods, as other researchers have access to use of surgehq or similar third-party annotation companies.However, several other evaluator groups are more popularly used for chat model evaluation, such as university students and amazon mechanical turkers.We attempted to carry out our study with three evaluator groups , but were unable to proceed with student and mturker evaluator groups due to time constraints.Consequently, it is unclear to what extent our metric analysis results will generalize to other choices of evaluator.Number of collected conversations as with any study involving a sampling procedure, resource constraints limit the number of collected samples, which in turn limits the statistical power of the study\\u2019s analyses.Our study included 400 conversations, which provided more than adequate statistical power for most of our analyses.For example, our investigation of each metric\\u2019s predictive validity relied on a simple linear regression analyses.80 to detect effect sizes of f2=0.142 by f-test for each metric\\u2019s regression.However, our analyses with the weakest statistical power are our dialogue-level analyses that compare bots with only 100 samples per bot.At 100 samples per bot, and assuming a standard deviation of 1.80 to detect differences of an effect size of cohen\\u2019s d=0.This is still a reasonable amount of statistical power, but leaves room for our study to produce inconclusive results when the true differences between chatbots are small.\",\".First of all, the generation performance of our knowledge-augmentation framework largely depends on the efficacy of retrievers.In other words, if the retriever fails to retrieve the relevant facts to the input question, the prompted llm, conditioned on the irrelevant facts, is likely to generate the incorrect answer.Similarly, if the retriever is not designed to retrieve the facts in 2-hop neighborhoods of the question entities, llms are less likely to generate the answer requiring 2-hop knowledge.Note that, for the mintaka dataset , the number of answerable questions with 1-hop facts is only 40% of total samples.However, when we include 2-hop triples, the number of answerable questions becomes 62%, which suggests the necessity of 2-hop retrievals, which is yet challenging.on the other hand, the evaluation metric for the generation performance of prompted llms may be further improved.Specifically, regarding our target kgqa tasks, the answer for the question is the entity in kgs.However, the prompted llms without additional training tend to generate the answer as the sentence.For instance, the label entity for the question in table 4 is \\\"new orleans\\\", however, the llms often generate the sentence-level output: \\\"alex chilton died on march 17, 2010 in new orleans, louisiana due to a myocardial infarction\\\".We currently evaluate the model performance by measuring whether generated tokens contain the answer entity or not; however, it would be worthwhile to develop the additional metric to compare the sentence-level output from llms to the word-level answer in kgs in a more effective way.Note that we also try other available metrics , such as f1 and exact match scores , however, they largely penalize the longer sentences , thus may not be appropriate for evaluating lm prompting schemes.Lastly, since we focus on the improvement of knowledge injection in lm prompting, we use the labeled entities in kgqa datasets when evaluating models, following the existing kgqa evaluation setups.However, in real-world applications where the entities in the question are mostly not provided, we first need to extract entities in the question with existing entity linking techniques; therefore, our model performance depends on the efficacy of entity linking.In particular, regarding the result with entity linking in table 5, the portion of answerable questions from labeled entities in the dataset is 40%, however, the portion of them with entities from the entity linking model is 22%.\",\"As the computation complexity of knn is o, when the size of a dataset gets really big, speed becomes one of the limitations of our method.Multithreads and multi-processes can greatly boost the speed.Lempel-ziv jaccard distance , a more efficient version of ncd can also be explored to alleviate the inefficiency problem.In addition, as our purpose is to highlight the trade-off between the simplicity of a model and its performance, we focus on the vanilla version of dnns, which is already complex enough compared with our method, without add-ons like pretrained embeddings.This means we do not exhaust all the techniques one can use to improve dnns, and neither do we exhaust all the text classification methods in the literature.Furthermore, our work only covers traditional compressors.As traditional compressors are only able to capture the orthographic similarity, they may not be sufficient for harder classification tasks like emotional classification.Fortunately, the ability to compress redundant semantic information may be made possible by neural compressors built on latent variable models.\",\".More long term, we plan to extend our discourseaware approach towards controllable generation with given entities.Specifically, instead of using the learned gr scores, the model could generate summaries with desired entities provided by human users.Limitation in our method, we employ an existing ner tool to label the entities in both the source documents and the summaries, and the performance of the ner tool may have an influence on the results of the model.Thus a good in-domain ner tool may be required when the work is extended to some specific domains, e.in addition, we use pegasus as our base model in all the experiments on different datasets, as it has delivered top performance on multiple summarization datasets.We follow the original paper on the length limits of all the datasets, however, the length of the source documents in both scientific paper datasets are much longer than the length limit , which leaves the room for further improvement with sparse attention techniques applied.\",\"The primary motivation behind this paper was to provide a comprehensive benchmarking study that explores the impact of model compression techniques on bias in large language models.While our work is among the first efforts to address fairness in compressed language models across multiple com- pression methods, including exploring multilingual settings, we are aware of the inherent limitations associated with our benchmarking study.Expanding our investigation to encompass other tasks, particularly those involving generative models or large language models , would be a valuable contribution to the research community.Examining the impact of model compression techniques on fairness in these domains would provide further insights and contribute to a more comprehensive understanding of bias in different types of language models.\\u2022 while our work includes a multilingual evaluation component, we acknowledge that there is room for further improvement and comprehensiveness in our benchmarking study, particularly with regard to quantization and pruning techniques.Apart from this, we did not provide a comparative analysis of monolingual and multilingual models using the same extrinsic data, which could provide valuable insights into the disparate impact of compression on the bias across languages.These are potential areas for future research that could contribute to a more thorough understanding of bias in compressed language models.\\u2022 despite showing results for state-of-the-art pruning methods, further benchmarking is necessary to observe how bias varies across different pruning techniques.Similarly, whilst our method serves as a proxy to estimate bias trends in quantized models, a thorough quantization-specific study is needed.\\u2022 different compression strategies yield varied benefits in terms of latency, memory, and so forth.Investigating the tradeoffs between these elements and fairness and accuracy would yield valuable insights for obtaining realistic estimations in real-world scenarios.Additionally, conducting case-study analyses would give practitioners in the field a deeper understanding of the potential harm these methods may introduce.\",\"We presented parallel context windows , a simple approach that alleviates context window restrictions for any off-the-shelf llm, without ad- ditional training.We showed the potential of this method on a variety of models and datasets.With that, our method does have some limitations.The number of context windows has a limit, and needs to be predetermined.Similarly to vanilla in-context learning, the number of examples to include in the prompt must be selected beforehand.For pcw, it is also required to select the number of context windows, b.In this paper, most of the results are for b = 3.The results are task dependent, but at a high level we find that there are diminishing returns around b in the range of 5 to 7.not effective for all types of tasks.However, for some tasks, pcw does not improve performance.This might indicate that some tasks are not suited for parallel processing.2 demonstrated that pcw is more suitable for cases where the input text could be divided into few independent inputs, but it remains an open question as to whether tasks, such as long text generation, would benefit from pcw.\",\".The size of the reference corpus is an additional dimension for model scale in nonparametric models.In this paper, we scale the corpus up to nearly 1b tokens, which is still smaller than the training data of very large language models.using npm saves gpu compute and memory compared to using models with more parameters.However, npm requires more ram and disk memory due to embeddings of a reference corpus.For instance, the largest corpus in our experiments requires 70gb of ram and 1.large vocabulary is known to lead performance gains but is bounded in memory costs.Previous work explored more efficient softmax approximations.Our nonparametric training offers an alternative by removing the softmax over the vocabulary.With the roberta architecture, increasing the vocab size by 2x makes the baseline training 50% more memory expensive, but does not increase the memory in training npm.However, this paper does not include more systematic evaluation on the effect of large vocabulary.our paper evaluates npm only on prediction tasks.It is currently nontrivial to use npm for generation, since it is the encoder-only model.our paper focuses on zero-shot evaluation only.our work explored cross-lingual transfer in a limited setup where the model is trained on monolingual data.in fact, nonparametric training may alleviate the burden of collecting large-scale multilingual corpora since it makes the model less sensitive to the language coverage in the training data, and may lead to significantly better cross-lingual transfer, as we demonstrate in the entity translation task.we find that search makes inference considerably slower than the counterpart without search.We think that search can significantly be faster with better engineering or better index, and the speed of npm is still on par with the speed of significantly larger parametric models that npm outperforms.Moreover, while not explored in this work, there has been work that improves inference speed that can be applied to npm.\",\"The main drawback of the work is in its evaluation, which was performed on datasets which were not manually annotated for the task, but adapted to it in various means.While we believe these evaluation sets do provide a strong indication regarding task performance, evaluating on bespoke data explicitly annotated for the task is usually preferable.\",\"While the proposed metaevent achieves significant improvements in both zero- and few-shot event detection, metaevent requires additional computational resources due to the layer- and step-adaptive learning rates and the outer-loop optimization, which may cause increased computational costs for training metaevent.Moreover, we have not investigated the benefits of task scheduling techniques and similarity-based meta learning in metaevent to fully explore the training event types.\",\"As the evaluation methodology for prompt engineering is still under development in the nlp community.We arbitrarily decided on the size of the development set.Our study focused on zero-shot setting with the purpose of evaluating the latent understanding on causal claims within chatgpt.Further exploration could be conducted to investigate the impact of few-shot settings by carefully selecting examples based on recent progress in few-shots prompting methods.We conclude that chatgpt has a promising but still limited ability in understanding causal language in science writing.Cots improved prompt performance, but finding the optimal prompt is difficult with inconsistent results and the lack of effective methods to establish cause-effect between prompts and outcomes.Following instruction is an important prerequisite for using chatgpt as a text classification tool, to avoid high labor cost for post-processing its answers.However, chatgpt provides a new, simulation-style approach for designing and evaluating human annotation guidelines.\",\"Annotation speed for mention detection and coreference is dependent on many variables like annotation interface, domain expertise of annotators, annotation style, document length distribution.So, while our finding that coreference resolution is approximately 2x slower to annotate than mention detection held for two domains , there are many other variables that we do not experiment with.We also experiment with transfer between domains with varying semantic similarity and annotation style similarity.But, our notion of annotation style is narrowly focused on types of mentions that are annotated.However, since our method is focused on mention detection, our findings may not hold for transfer to annotation styles with different notions of coreference linking ).our key finding that transferring the mention detector component can still be adopted.\",\"We think the limitations of our work are three-fold.1, we employ existing cgec models to select sentences for annotation when building the media and thesis domains of nasgec.Although this reduces annotation costs, it inevitably introduces biases into our dataset.For instance, the proportion of complex syntax- or semantic-related errors may be lower than that in reality, since existing cgec models fail to identify them.Note that although we manage to mitigate such biases by voting with multiple models, this issue still exists.the current size of our dataset is relatively small.We will continuously collect more data from more diverse domains.Compared with other domains, thesis has a much smaller data size , as authorized papers are hard to obtain.based on our multi-domain nasgec, we have reported and analyzed cross-domain performance preliminarily.However, besides fine-tuning with small-scale data in the target domain, many other potentially helpful domain adaptation techniques can be tried.\",\"This work focuses on a specific view of the whole neuro-computational modeling field.We exclude specific angles of research such as non-linear models since we want to evaluate the accumulated evidence for structural similarity between neural responses and language models.Mention several advantages of using linear mapping models, they are more interpretable and more biologically plausible.They also provide an insightful discussion on mapping model choice, emphasizing the importance of estimating models\\u2019 complexity over categorizing them as purely linear or nonlinear.Another limitation is that we do not include speech models that have been used to map brain representations mostly due to coherency and page-limit restrictions.The survey is also limited to fmri and meg data instead of other modalities for two many reasons: fmri and meg are used as a combination in many studies , and they offer high spatial resolution and signal reliability and better temporal and spatial resolution , making them suitable for nlp.\",\".Human evaluation our evaluation is conducted with a limited sample size of two teachers.This would help tease apart the potential teacher biases from generalizable claims about the feedback quality.\",\"This work has potential limitations: \\u2022 we found that on the figure 3 and 4, the entailment of the methods after applying multiple position embedding are sometimes lower than origin methods.This is not meet our expectations since we don\\u2019t want our method to decrease performance.In our opinion, we think the reason might be the embedding method has never been seen before during the pretraining of models, which requires the model\\u2019s additional efforts to adapt the embedding, thus hurts the performance.\\u2022 we also found that the multiple position embedding does not work very well to alleviate the order effect in the lm loss-only settings4.since lm loss only does not help the model distinguish which parts in the input sequence are knowledge set and thus treat them the same as history.The multiple position embedding will not be trained finely to help the model distinguish.\",\"The limitations of this work mainly lie in two aspects: the synthesis quality is determined by the performance of existing data2text approaches, while data2text generation is still a difficult task that waiting for deeper exploration.The common errors in generation are included in sec.we adopt a plm as the decoder in data2text generation in order to generate fluent utterances.However, as stated in , plms tend to pay more attention to sentence fluency than to the graph structures of inputs, which may cause the loss of some critical information.\",\"Accessibility we have tried to develop readalong studio web app with accessibility in mind, using accessible colour contrasts, ensuring buttons have aria-labels, and ensuring that the website is legible when zoomed-in to 200%, among other considerations.Using google pagespeed insights, our website scores 89 for accessibility, but we recognize that there are still improvements to be made; specifically, we would like to perform an audit of the website with respect to web content accessibility guidelines.Inexact transcription readalong studio will work best if the transcription is exact; that is, if there are as few discrepancies between the text and audio as possible.If extraneous text exists , or if the audio includes un-transcribed speech , these errors will accumulate and can result in poor alignments.The extent to which these discrepancies affect the final result depends on the length of the recording to be aligned.In practice, we have found that readalong studio is able to recover from minor transcription errors when the speech data to be aligned are around 5 minutes or less in length.We have successfully aligned much longer files, but \\u201cyour mileage may vary\\u201d depending on the exactness of the transcription, the language\\u2019s orthography, and the type of data used.Singing several teachers have successfully aligned songs with the corresponding text using readalong studio.For such an alignment to be successful, however, it is necessary that the sung words be vocalized clearly, and not be drowned out by the accompanying music.Extended legato singing can also cause poor alignments, since the speech-trained acoustic model does not expect single syllables to correspond to multiple intensity peaks in this way.Language support the software works with most languages out-of-the-box.1, readalong studio comes with support for 39 languages built-in, and handles other languages with a rough, best-guess g2p based on unicode table information.At several international workshops , we found that it worked reasonably well with every language brought by workshop participants, even those with unique alphabets like western armenian or korean.However, not every language will work equally well.It will typically work well in languages with systematic orthographies that use letters in cross-linguistically common ways.We anticipate difficulty with orthographies that use familiar letters in cross-linguistically unusual ways, such as \\u201cfont-encodings\\u201d , abjads that leave out many vowels, and languages like japanese where the pronunciation of logographs is highly variable and determined by context.Just like a human could not simply guess the missing vowels in written hebrew without knowing hebrew, the software will not be able to do this either.Additionally, the software is limited to languages which are both written and spoken\\u2014we do not support signed languages since the aligner requires audio to align with text, and the tool is fundamentally inapplicable to unwritten languages.The interface itself is currently only translated in english, french, and spanish, limiting potential users who do not speak one of those languages.Numbers and symbols while readalong studio can do rough zero-shot g2p for most alphabetic and syllabic writing systems, it is not capable of general text normalization\\u2014while it can guess that \\u201ct\\u201d might be pronounced [t] in an unfamiliar language, it simply has no basis to guess any particular pronunciation for \\u201c634\\u201d, as this task is not only language-dependent but highly variable within any given language.Therefore, all input must be \\u201cspelled out\\u201d for alignment to succeed.If the input contains numbers or symbols, readalong studio web app will prompt the user with a warning that it found uninterpretable symbols.\",\".The main drawback of our data creation protocol is that the question\\u002fanswer pairs were generated automatically, leading the question distribution to be artificial from a semantic perspective.\",\"Unfortunately, the traditional definition of the f1 score is too restrictive to produce a robust score that could paint a reliable picture of the model\\u2019s performance.The design and implementation of a metric that could compute the alignment of entity spans in the presence of asr errors would be a significant step in the direction of producing more robust ner models for spoken conversations.We conduct experiments with the asr on audio files from the earnings-21 dataset.These files are recorded at 11 khz-44 khz, while typical call center conversations are recorded at 8 khz-16 khz.Unfortunately, training datasets with recording characteristics resembling real-world usage scenarios are unavailable.We also do not address the problem of racial, gender, and age disparity due to the lack of availability of sufficiently representative and inclusive datasets.It is, however, to be expected that the performance of the asr deteriorates for the recordings of speakers other than male speakers of general american.\",\"Our proposed model has three limitations.to overcome this issue, more expressive methods such as rotate and gcn could be investigated.These methods are expected to be able to represent complex relationships associated with multiple types of external information.Secondly, our model adds a representation of the literature graph to the target text, so longer sentences require truncation of textual information.Thirdly, we have not analyzed the results in detail and how the proposed method positively and negatively impacted document classification.\",\"Our paper assesses procedural knowledge augmentation using a limited number of highly structured instructional documents.Naturally, the results presented may vary for unstructured guidelines.Additionally, due to the limited size of publicly available tod datasets, we have not tested how our method may scale to settings with larger document spaces.For larger document sets, more efficient methods of computing similarity such as maximum inner product search algorithms may be necessary to approximate documents with the highest relevance scores.\",\"The primary limitations concern the dataset we created, which serves as the foundation of our findings.Our dataset suffers from four key limitations: reliance on papers with code our system is trained and evaluated to retrieve datasets from papers with code datasets.several queries in our test set corresponded to datasets that are not in pwc, such as iwslt 2014 , pascal voc 2010 , and chime4.Papers with code datasets also skews the publication year of papers used in the datafinder dataset towards the present.For the most part, pwc only includes datasets used by another paper listed in papers with code, leading to the systematic omission of datasets seldom used today.Popular dataset bias in the test set our test set is derived from the scirex corpus.This corpus is biased towards popular or influential works: the median number of citations of a paper in scirex is 129, compared to 19 for any computer science paper in s2orc.The queries in our test set are therefore more likely to describe mainstream ideas in popular subields of ai.Automatic tagging our training data is generated automatically using a list of canonical dataset names from papers with code.This tagger mislabels papers where a dataset is used but never referred to by one of these canonical names.Therefore, our training data is noisy and imperfect.Queries in english only all queries in our training and test datasets were in english.Therefore, these datasets only support the development of dataset recommendation systems for englishlanguage users.This is a serious limitation, as ai research is increasingly done in languages other english, such as chinese.\",\".3 because the model being fine-tuned sees all the training set examples and at inference you pay only for the tokens in the one example to be classified.Also, as this experiment showed, fine-tuning is a very powerful text classification technique when it is used with gpt models.\",\".Combining reformulation operations: the reformulation operators, except rep, which is applied jointly with other operators, are applied sequentially, in their given order, e.first, applying multiple operators sequentially has the negative impact of increased inference latency as the surf model needs to be applied multiple times, which can become a bottleneck for systems that process large traffic volumes.Second, by applying sequentially the reformulation operators, the likelihood of cascading errors or the model making mistakes in terms of the target reformulation shape increases.large language models : in this work we relied on bart as our seq2seq model, and did not experiment with newer multi-billion parameter llms.Recently we have seen rapid progress in the space of llms, both in terms of model size and their capabilities to perform various tasks.However, we note that deploying llms is limited by their high inference latency, particularly in high-traffic, low-latency systems such as ours.Furthermore, for experimenting with api-based approaches such as chatgpt and gpt-4, using these systems was not possible due to data confidentiality.evaluation on public datasets: our evaluation focused on real-world unanswered user utterances from voice assistants.We did not use public datasets as currently available resources do not accurately represent customer behavior at scale.However, the community is aware of this divergence, and there are initial efforts in different nlp tasks to create public datasets that represent real-world user behavior.For example, in the the task of named entity recognition there has been recent work on bridging the gap between academic datasets and real-world problems by creating new resources that represent contemporary challenges that are encountered in practice.furthermore, the findings from our work may be used to create data that includes the challenges we identified as part of our analysis.Multilingual experiments: we only considered english-language questions in this work, and it will be of interest to consider how our approach can be extended to other languages using multilingual models.The evaluation of cross-lingual transfer for this task is another open research area.\",\"Although our nar approach can generate fluent and meaningful text, it inevitably suffers from the typical generation problems like in the ar fashion: off-prompt: the provided prompt is very short, which causes the model can not focus on meaningful content and generate reasonable text.incoherent between sentences: when the model is initialized, it does not consider the logical order between sentences, so it can only rely on the training data to learn automatically.We will consider how to generate a suitable initialization to help the model generate coherence results., roberta, which may inherit the problematic biases.We have attempted to mitigate these issues by conducting experiments on comparatively innocuous story generation and opinion generation tasks.Furthermore, we have replaced all the names in those corpora with special placeholders.Although some measures are taken to mitigate the problematic biases, such issues cannot be solved completely.Thus, we urge the users to carefully examine the generation results and cautiously apply our method in real-world applications.Additionally, it is worth noting that all the corpora used in our experiments are only for scientific research.As for the human evaluation process, we resort to open source web library django|| to build our own human evaluation interface.Before releasing the human evaluation cases, we carefully check that there is no private information or other problematic biases in the cases.Besides, we did not collect personal information or ask the annotators about their private information during the annotation process.We hired three annotators and paid each of them $0.The payment is reasonable since there are only 100 cases for annotation, and it would cost average 4 hours for one to finish all the comparisons.\",\"In this work, we focus on creating the englishfrench tense corpus.These two languages are among the most frequently and widely used languages in the world.In addition, they have several similarities in tenses, which are pretty helpful for research on tense consistency through machine translation.Thanks to the distinctive tense struc- tures, the study of these two languages makes it possible to examine many common tense issues, but there are also some tense issues in other languages that are not covered by this study.For example, the implicit tense expressions in chinese are difficult to correspond to the explicit tense expressions in english.Hence, our next step will be to extend the tense test set to other language families and even cross-language families to further study tense consistency.Besides, we did not propose a new method to improve the tense prediction accuracy.To be further, we will endeavour to improve the existing machine translation systems according to tense consistency.\",\"Limitation to better understand the limitations of the proposed model, we carry out an analysis of the errors made by pigeon.Specifically, we randomly select 100 instances that are incorrectly predicted by pigeon and summarize the primary types of error.The first error category is boundary prediction error.Since we modeled the ape task as a sequence labeling, our model may only recognize a part of an argument.Thus, multiple consecutive arguments may be identified as a single argument.The second type of error is caused by the absence of semantically similar words in the argument pairs.In this case, the proposed probing graphs cannot model the relations between argument pairs.Third, another error category occurs when semantically similar words are also presented in non-matching argument pairs.The argument relation may be misled by these words.for example, we may leverage the high-level topic information over argument pairs to guide the learning of relation-specific features.In addition, the proposed probing approach may be computationally expensive and we can alleviate this problem by saving the similarity of all word pairs for one time for the entire dataset.\",\"One limitation of current token-pair edit matrix based incomplete utterance rewriting models is that they are only able to select tokens that have appeared in the context utterances.Thus, these models, including our own, are unable to generate new words, such as conjunctions and prepositions, to improve metrics such as fluency.in addition, we will consider combining generative models , t5 etc.\",\"While the xpqa dataset is created to be as close to the real-world scenario as possible, it has two major drawbacks.Firstly, the candidate set in the dataset does not include the full candidates for a given product because annotating all candidates is prohibitively expensive.The subjectivity of product questions and candidates also makes it hard to get ground-truth short-span answers, which prevents a straightforward end-to-end evaluation over the full candidate set.A potential fix is to run human evaluations on the top-1 candidate over the full candidate set from each model, but it\\u2019d be costly to do so.secondly, the answer annotation is based only on a single candidate because handling information from multiple candidates requires careful instructions on conflicting information and summarization skills.This might limit the model in answering complex questions that require inference over multiple candidates.However, we find this case to be very rare in real customer questions.Furthermore, as we do not summarize multiple candidates, the returned answer can be biased toward the opinion of a single customer.Our evaluation also has potential limitations in that we did not extensively evaluate the quality of generated answers with manual annotation.It is known that bleu scores might not correlate well with human evaluations on generation tasks, and they can be misleading in certain cases; we only compared major types of baseline algorithms and did not explore the effects of leveraging existing larger, more powerful pre-trained language models such as mt0 and flant5.Conclusions might change if we hire annotators to perform more human evaluations or change the model architecture.\",\"As we mainly focus on conceptual knowledge captured in so-called tbox axioms, the abox axioms are not considered.Abox axioms can capture situations for specific individuals which could cause privacy issue and we would not expect lms to capture such knowledge.Hence, dealing with abox axioms could require additional engineering for data preprocessing.\",\"Despite the demonstrated effectiveness of selfadaptive icl, this new paradigm suffers from the following limitations.4, due to the large search space, we need to trade efficiency for effectiveness.So how to balance the efficiency-effectiveness trade-off is an important decision choice to make when deploying selfadaptive icl methods.1, the gains of our method shrink when the size of the retrieval set gets smaller.To maximize performance, we require a high-quality retrieval set, which might not always be available when dealing with unseen tasks in practice.We also note that both limitations can be alleviated with better selection and ranking algorithms.The remarkable performance of our method should partially attribute to the powerful topk selection method, so we also discuss the limitation of topk here.Despite its popularity, our analysis reveals that topk\\u2019s effectiveness is limited to simple nlu tasks with limited label space, and it does not work well with tasks with large or even infinite label space.This limitation signals a new direction for icl research: we need better selection methods to adapt icl methods to more tasks.\",\".Our method faces obstacles when attempting to validate it on datasets other than those previously used in this paper.For example, lrs2 is a widely used dataset in visual language recognition tasks.However, since lrs2 dataset does not provide speaker identification labels, we cannot easily classify speakers into domain-specific and domainindependent sets.Despite the enormous amount of work, re-annotating existing datasets with crowdsourcing or annotating a new real-life dataset with speaker labels is a viable solution.Besides, existing data augmentation methods cannot match the generalization requirements on visual temporal-aligned translation perfectly, which inspires researchers to develop targeted augmentation paradigms based on the study in this paper to cooperate with our meta-learning strategies.\",\"There are two main limitations to the present work.First and foremost is the computational cost associated with the present experiments.We present here results and analyzed gleaned over 10 runs, 7 pretraining regimens, 8 rl gradient propagation variants and 2 data sampling approaches, for a total of 1120 models.While training any one of our models is cheap , the total number of models may pose a challenge for future replication studies and comes at an environmental cost.\",\"The methodology of the constructed corpus is based on ner, making ssi a corpus integrating re with ner.We aimed to enrich our experiments with multi-task learning so as to measure the impact of ner on the extraction of relations.Unfortunately, after conducting statistical analysis on ssi, it showed that, due to its limited size, we did not have enough instances of the same pair of entities participating in one sentence.As this statement would negatively bias our multi-task model, we refrained from conducting these experiments, before proceeding to annotate further instances and add them to our corpus in further work.In terms of annotation, only one annotator has been invested in the current task, as it is a preliminary work including a new dataset for relation extraction, with new entities and relation types.\",\"In this paper, we propose a continued pre-training method to inject knowledge into large pre-trained language models.There are eight v100 gpus involved in each pre-training experiment and the whole pre-training process takes 5 days for the base-size model and 13 days for the large-size model, in primary settings.These numbers in data upscaling settings are significantly greater.Despite its advantage in reducing resource need in inference time, kilm is both time-consuming and computationally resource-consuming during training time.Similar to any model-based generation system, kilm could be prone to generating factually incorrect statements with regard to entities.These statements might also be prone to be biased based on ethnicity, race, and sexual orientation.\",\"Our work seeks to gain insight into what pretraining knowledge is transferred and useful for downstream fine-tuning in nmt using synthetic tasks and data.We note that changes in the data generation methods do require re-running the pretraining stage, which is computationally expensive compared to the fine-tuning stage.Our current synthetic data generation methods are somewhat crude.Although they are designed to encode varying degrees of lexical and structural translation knowledge, they do so in a rather simplistic way.For example, sampling phrases from the normal distribution ignores distributional frequencies which represent information that is likely useful for the synthetic data generation task.In this paper we have presented some interesting initial findings regarding the suitability of synthetic pre-training for nmt.we acknowledge that synthetic pre-training is unlikely to surpass the quality of real-world massively multilingual pre-trained models in performance, especially if synthetic data is the only data used for pre-training.However, good performance can probably be achieved by combining synthetic pretraining and real-data pre-training.Of course, this risks exposing the model to toxic and sensitive or private content.Therefore, concerns of both model quality and data quality should be considered when evaluating the impact and benefits of synthetic pretraining.We view synthetic pre-training as a complimentary approach to finding an optimal balance rather than as a replacement for previous state-ofthe-art nmt pre-training methods.\",\"Authoring transduction rules is relatively easy but may still be labor-intensive for complex domains.our experiments in this paper generated text only in english.It would be interesting to apply the framework to datasets in other languages, e.While our framework is intended to be agnostic to the output language, our notation for response templates might need to be slightly extended to be more convenient to use with morphologically complex languages or free-wordorder languages.In these settings, presumably, the qcfg should systematically generate many inflections or orderings for the lm to choose among.relatedly, we have only tested the proposed approach on dataflow graphs.While qcfg productions were unweighted in this paper, giving them weights would allow the qcfg to express its own preferences about which productions to use for a given input.For example, in a product-of-experts architecture, the probability of a given response y, would be proportional to the lm probability of y times the weights of all productions used in the qcfg derivation of y.Beam search could then be carried out using prefix weights.The weights could be trained using gold responses.Weighting the qcfg raises the possibility that the dataflow transduction rules could encode pragmatic context-dependent policies.For example, a dataflow transduction rule could call a neural network to assess the suitability of applying the rule to a given node in the dataflow graph, and then weight the resulting qcfg production ac- cordingly.However, our approach can still make pragmatically unhelpful omissions, making it potentially risky to deploy in some scenarios.Additionally, we leverage pre-trained neural language models such as codet5, and as such, we acknowledge that our approach might inherit some biases present in these pre-trained models.\",\"Limitation one limitation of styleap is that one extra inference is needed for retrieval.It is mainly due to the monolingual retrieval accuracy is higher than that of crosslingual retrieval.\",\".\\u2022 human labels are currently provided at the sentence level, either a rating of the whole sentence or providing a new sample sentence.However, we have observed that when generating 50-token sentences, often gpt-2 will generate some part of the sentence following the desired attribute\\u002fdistribution while some other part of it not following.\\u2022 our experiments are performed on low quantities of data to demonstrate that our method works under a few-shot setting.Therefore, we do not have evidence on how well our method\\u2019s performance scales when a large number of annotations is available.\",\"A qualitative analysis of distractors generated via mt shows that this method can produce some inadequate candidates.Thus, a human-in-the-loop is needed to ensure the validity of the generated distractors.However, human-in-the-loop is standard practice, when producing language exercises and tests.We therefore believe that the proposed approach does not need to be fully automatic to be useful, as it can still help speed up distractor generation to create advanced vocabulary exercises.The mt method can thus be of huge help to human test developers.The mt approach can be computationally more expensive than the methods proposed in prior work such as bert and word2vec.Although we make use of pre-trained mt systems, the approach can be still costly, as it requires running two mt systems with each pivot, and a bert-based word alignment model to align the carrier sentence with each of its 900 back-translations.In terms of cost comparison, it takes 1-2 hours in a single nvidia tesla a100 gpu to generate 900 translations and produce candidate distractors for a single pivot, versus 0.however, the mt approach can potentially offer advantages that other methods cannot, such as producing a more diverse pool of distractors and, importantly, relating the native language of the learner to the pivot systems used to produce distractors.As our analyses show, each pivot system generates unique distractors.this is because verifying whether tying the pivot to learner\\u2019s native language would be useful would require a human study with a relatively large group of learners of at least 20- 30 students that all share the same first language.In fact, we would need to have several groups of learners, such that students in each group have the same first language background.This would be a large-scale study that is out of the scope of the paper.Note that the current work already presents a human study with 32 students that demonstrates that the automatically generated pivots are of the same difficulty as those created manually.We also note that the method requires relatively good mt systems for generating more difficult distractors.Finally, our study is limited to cloze items that include single words as targets and does not consider fixed expressions, such as phrasal verbs and idioms.In the language testing community, such expressions are typically tested separately from the generic cloze items.The basic approach is to detect them before the carrier sentence is cleared to be used for cloze exercises.Our current work is not focused on carrier sentence selection.But it makes sense to include this consideration in a larger suite of tools for cloze item generation.\",\"In this work, we attempt to extend an existing mnmt model to support new language pairs with an acceptable expense.In addition to the advantages, our method has the following limitations: additional introduced parameters.We utilize the parameter-isolation based method to support new language pairs.The total parameters of the mnmt model have been increased by pluggable modules to achieve better performance than prior studies.The gap between our scenario and the realworld scenario.Our proposed method is a whitebox service in incremental learning.Thus, we train a powerful mnmt model as the original model instead of directly utilizing existing models from the internet.And we only consider eight incremental language pairs due to the limitation of computation resources.We try our best to simulate the realworld scenario and we will apply our proposed method for large-scale pre-trained mnmt models to validate the effectiveness in industrial scenarios.\",\"One potential improvement to this work is the development of a method to evaluate the accuracy of the severity measure component.We have demonstrated the effectiveness of sescore2 with a severity measure through improved kendall correlations for various types of retrieval augmented synthesis in figure 4.however, there is currently no widely accepted way to quantitatively measure the accuracy of the severity labels.This is because there is no existing dataset that can be used to benchmark severity measures.have released mqm annotations with error spans for each segment, these annotations often include compositional errors that prevent the evaluation of individual severity labels without also considering other errors in the sentence.A potential future direction for this research would be to create a benchmark dataset that would allow direct assessment of individual severity estimation or explore alternative methods for evaluating the accuracy of severity measures.Second, we have not been able to test sescore2 on low-resource languages due to the lack of mqm-annotated testing sets in these languages.However, we have demonstrated that sescore2 can still perform well without severity estimation by outperforming top unsupervised metrics such as bertscore, bartscore and prism as shown in figure 4.this suggests that sescore2 may be useful for low-resource languages since parallel corpora are not available for most low-resource language settings.To further verify this, a potential future direction would be to create testing sets with mqm labels for lowresource languages, to test the performance of sescore2 and other learned metrics in such scenarios.Lastly, since sescore2 is based on proximity between reference and model output, its capabilities for open-ended text generation tasks have not yet been fully explored.This presents an opportunity for future research to investigate the potential of this method in such scenarios.\",\"We identify the following limitations of our work.Our current cos\\u2019s reranking expert only learns to rerank single-step results.Thus it can not model the interaction between documents in case of multipassage evidence chains, which might lead to suboptimal performance, e., when we need to rerank the full evidence path for hotpotqa.At the same time, we hypothesize that the capacity of the small model used in our experiments is insufficient for modeling evidence chain reranking.also, our current pretraining setup only includes the three bi-encoder tasks, and thus we can not use the pretrained model out-of-box to solve tasks like end-to-end entity linking.Consequently, the learned skills from self-supervision can not be chained together to perform configurable zero-shot retrieval.It would be interesting to also include the entity span proposal skill in the pretraining stage, which could unleash the full potential of the chain-of-skills inference for zero-shot scenarios.\",\"When using our method, we have to fine-tune the upstream nmt model to construct the downstream nmt model and then datastore for the reviser training.Hence, compared with the current commonlyused knn-mt variant , our method requires more time for training.Nevertheless, it does not introduce additional parameters during inference.\",\"The proposed method ndcr has some limitations as follows: 1) the produced representation of simple proposition sentences in the proposition generator lies in a different space distribution with the image encoding, which affects the performance of their fused representation.Although we introduce the reasoning information of compound proposition text to alleviate this issue, we hope to solve it by improving the text understanding capability of pretrained vlms.In addition, adopting the pretrained textual encoder of vlms to perform proposition decomposition is inadequate due to that they present an inferior understanding for the discourse structure of long texts.2) the performance of samples with highly similar images from video frames is quite different from that of humans.We may improve it from the perspective of image difference modelling.3) the experimental results indicate that our method is effective at logical inference on examples with medium-length descriptions, but there is still room for improvement for longer descriptions.\",\"We have identified several limitations in our work and propose future directions to improve them: the sources for ur-qa in this paper are limited to the document corpus and qa-history, but our unified reader is not restricted to take specific sources.Further research can explore the generalizability of ur-qa to more diverse sources, such as linearized knowledge sources as proposed in.Though it is not the focus of this work to optimize readers, our proposed ur-qa can orthogonally benefit from improvement in retrieval.Further study on the retrieval for ur-qa can be conducted, including the direction to co-optimize the reader and retriever as proposed in.\",\"Due to the high computational costs of the method, we tested it only on a very small set of sentences and larger-scale experiments are needed to confirm the results.Many parameters of the ga algorithm were left unexplored \\u2013 the results could be improved by grid search over the values for mutation and crossover ratios, using a better list of mutation candidates , experimenting with different selection methods, combining more metrics in the fitness function or using multiobjective ga like nsga-ii.In the experiments concerning held-out metrics, we assumed weaknesses of the held-out metrics are not correlated to the weaknesses of the optimization metrics, which is probably not true, due to similar model architectures and training datasets.This means that held-out metrics are not strictly independent, but we believe combining multiple different held-out metrics should mitigate this issue.\",\"Limitation the \\u201cnarratives\\u201d dataset provides a valuable fmri resource, stimulated by language and obtained under naturalistic conditions.Further research opportunities can be pursued with the availability of more detailed datasets.For instance, comparative studies between instances of stuttering and nonstuttering in text stimuli can be conducted, as our experiments demonstrate that the model tends to retain frequently-used filler words as a shortcut for higher accuracy.Meanwhile, the evaluation strategy applied for current research of open-vocabulary brain decoding presents an idealized condition and and serves as a starting point from which further exploration of how existing methods might perform under more real-world scenarios can commence.\",\"Due to the lack of multi-intent text revision datasets, we only conduct experiments on iterater.Although it is a multi-domain dataset, we only use its sentence-level data, and each sentence pair only contains one editing operation.another limitation of our work is that we only made improvements at the model level.Recently improved text revision by leveraging extra data from other text editing tasks and performing editable span detection before revising.\",\"In this study, we assessed the quality of the corpora books-small, medium, and large, by training and evaluating a ner model on them but we did not include the corpus books-huge in our analysis.However, our results on the books-large corpus indicate that there is no substantial benefit to using a corpus larger than books-medium for training ner models.This is consistent with prior research on few-shot training on smaller corpora achieving comparable accuracy to larger, potentially noisy corpora.Given the scarcity of benchmarks for late medieval ner , we were unable to conduct experiments on corpora other than our own.Additionally, we utilized a ner model trained on contemporary texts as our baseline for comparison.Therefore, it is important to note that these results may not generalize to other medieval ner tasks.\",\"By analyzing the error cases, we find that almost all the existing work cannot handle the disorder problem of words well, primarily when the error occurs far from the correct location.For example, there is a correct sentence: \\u2019on my way to school today, i bought a very tasty apple.if the erroneous form is as follows: \\u2019on my way to school apple today, i bought a very tasty.\\u2019, it is hard for the model to understand that the right thing to do is to put apple back at the end of the sentence.\",\"We acknowledge that the methodology used to build dlama-v1 still has limitations related to the information within its relation triples.While directly querying wikidata as a dynamic source of facts provides the flexibility needed to acquire data that is relevant to different cultures , the diversity of the triples that are compiled depends on the availability of a diverse set of facts on wikidata in the first place.For instance, the smaller number of relation triples related to arab countries for the predicates , , and in dlama-v1 demonstrates the difficulty of querying the exact number of facts for both cultures despite using exactly the same queries with the only difference being limiting the region to which the triples belong.Another limitation is the inability to enumerate valid and fine-grained subclasses of objects for specific subjects, if these fine-grained objects are not on wikidata.Steps and of dlama explained in \\u00a74.1 ensure that a possible and more general object is still valid for a specific subject.However, inferring a more specified object from a generic one is impossible.For example, the fact that someone speaks \\u201camerican english\\\" implies that they speak english as well, but knowing that someone speaks \\u201cenglish\\\" is not enough to speculate about their dialect.While the triples within dlama are sampled by picking the ones whose subjects have the largest wikipedia articles\\u2019 sizes, the infeasibility of manually reviewing the large number of diverse facts within dlama-v1 makes it hard to claim that the facts are free of inaccuracies or missing information.More broadly, dlama supports relations predicates that are already part of mlama to fairly compare the results on dlama to those previously reported on mlama.Moreover, we make sure that the subjects and the objects of the relation triples are available in the different languages of interest.Having these constraints might imply that some culturally relevant facts might have been dropped out of dlama-v1.Lastly, we used mlama\\u2019s probing setup in which the models rank a predefined set of objects for each prompt.Their prediction is correct if the top-ranked object is one of the valid labels for the corresponding relation triple used to populate the prompt.Therefore, a model\\u2019s performance is expected to be higher than that achieved by a generative setup in which the model is asked to generate the most probable completions for the masked tokens.\",\"The limitation of citadel mainly shows in two aspects.First, at the beginning of training, the model needs to route each token vector to multiple activated keys for token interaction, which increases the computation cost compared to coil and colbert.This results in slower training speed but it gets better when training approaches the end as more tokens are pruned by the \\u21131 regularization.Another drawback lies in the implementation of citadel, or more generally speaking, most multivector retrieval methods.The token-level retrieval and aggregation make them not compatible with established search libraries such as faiss or pyserini.Moreover, for time and space efficiency, multi-vector retrieval also requires more engineering efforts and low-level optimization.Recently, xtr provides a solution that constrains the document-level retrieval to be consistent with the token-level retrieval during training, which can be used for streamlining citadel.\",\"Although our method is efficient and scalable, we have not conducted pre-training on large-scale corpora due to limited computational resources.The quality and quantity of data are crucial factors for a pre-training model.As our model only covers 36 languages, it cannot provide services for many rare languages.This paper just proposes a new pretraining direction and does not use many training tricks.besides, rtl task is not the only possible tokenalignment task for our dap framework.Other objectives based on token representations are also worth investigating.The best objective form is still under research.\",\"Limitation of a lack of personalized and specific examples in existing datasets, when teaching cognitive techniques.this may serve as the basis for future psychological validation studies of the materials and support future studies of low-intensity self-help interventions.\",\"Our work has several potential limitations.First, given the limited computational budget, we only validate our self-evolution learning on the large and base sizes.It will make our work more convincing if scaling the experiments up to the larger model size and training corpus.On the other hand, besides the improved commonsense knowledge learning ability, we believe that there are still other abilities, e., mathematical word problems, of plms that can be improved by our method, which are not fully explored in this work.\",\"Dungeons dragons is a very complex game to capture completely, and there are certain aspects that fireball does not take into account.For example, fireball\\u2019s scenarios are recorded independently of the overarching narrative context they take place in, do not record players\\u2019 inventory, and do not account for any movement or placement on a map.Our models are not able to play autonomously - but doing so is not the goal.Instead, models can be used to assist and inspire the humans playing.Our models do not take into account the gener- ation of profanity or sensitive topics; these were filtered out post-hoc.Is a game played by players of all ages that often contains violent or profane descriptions, and unfiltered generations may be unsuitable for young players.There are previous instances of roleplaying games that incorporate language models being used to generate sexual content3 that would require age restrictions and content warnings.Gpt-3 may be prohibitively expensive for everyday use; in our experiments, we were unable to use the full set of data we had available for fine-tuning due to budget constraints.\",\"Even though our work improves early exit performance effectively, some limitations are still listed below: \\u2022 our approach focuses on making the intermediate representations of early exit models capable of general linguistic representation learning and task-specific representation extraction.Therefore, we did not fully use the model\\u2019s high-level representation and fuse representations of previous layers, which may restrict the performance of our method.\\u2022 although our early exit method has achieved better performance, we have lost some inference speed due to the introduction of additional adapter modules.\\u2022 in recent years, the parameter size of generative pre-trained models has been continuously increasing, leading to remarkable performance on various nlp tasks.There is an urgent need to develop inference acceleration methods for generative pre-trained models.Unfortunately, our method is limited to discriminative pre-trained models.\",\"It should be noted that our contributions are limited to fanfiction documents.Models trained on our datasets might not transfer to other online content like news articles, websites, or social media posts.Particularly social-media texts are shorter and contain fewer descriptions and more verbal expressions, which is a substantial-enough shift to warrant models explicitly trained in the genre.Similarly, the conclusions of our experiments are limited by the models we used, as well as the genre of the text.Furthermore, the trigger warning scheme we used is a simple structure.Further research should investigate more detailed trigger typologies with a more rich semantics.\",\"In this paper, we propose a new framework that extracts the various aspect of information about given data, relying on the existing model-driven metainformation from the trained models.Hence, if there are some flaws within the used models, such as biased prediction or learning of spurious correlation , then our framework can be directly affected and may have a risk of inheritance or amplification of such problematic behaviors.However, as our framework is not limited to any specific models and metainformation, one can prevent this problem by using the robustly trained models or introducing more specialized meta-information for these problems.In addition, despite the empirical gains we find, our subset selection method is not theoretically guaranteed to be the optimal set of max informativeness, which remains an interesting direction.A further study is necessary showing that selected samples from infoverse could lead to low inter-annotator agreement in manual annotation but provide more accurate information than pseudolabels.Abnormality detection using infoverse, like noisy labels, out-of-distribution, or annotation artifacts, could be interesting future directions.Broader impact and ethical implications our work aims to quantify the data informativeness with multi-perspective for capturing properties that can not be revealed by a single perspective.Especially, infoverse lends some insight into data by models what we have.Thus, infoverse has the potential for guiding the construction of high-quality datasets, e.from these points, it is possible to develop a system or general platform for effectively collecting data like dynabench6 and snorkle7.We anticipate that the general platform of infoverse could be contributing to human-involved machine learning systems.Although our work empirically demonstrates the improvement over various real-world problems, the current version of infoverse has a potential risk to be vulnerable to sample undesirable properties ) in a dataset, as we construct infoverse with metainformation measure do not consider such properties.However, it can be easily alleviated by 6 7 adding various measurements which represent \\u2019fairness\\u2019 thanks to the extensibility of our framework.Hence, we believe that our proposed method can be personalized to the purpose of data collection.\",\".For example, there are still limitations in the current assessment of the structure of topics mined by different models.Examples include assessing the validity of topic hierarchical indicators by topic specialization and the validity of the symmetric structure of topics through clustering as we have demonstrated.All these assessment methods are only a sideways demonstration of the interpretability of the topic structure.Besides, there is still a lot of a prior information available in the field of topic modelling, e.wordnet, and it may help researchers to explore further in the field of topic modelling if they can combine prior human knowledge and information on topic-words obtained from models to define quantitative metrics that are more consistent with human understanding.\",\"We identify the following two limitations of our work: \\u2022 different from raw text, constructing mrcstyle data from wikipedia requires the existence of hyperlinks.This idea works well for resource-rich languages, such as english and chinese.While such an idea is less effective for languages with few hyperlink annotations in wikipedia because a small amount of mrcstyle training data is difficult to guide the learning of nlu capability in those languages.A possible solution is to explore other data resources to automatically construct large-scale mrc data for pre-training.\\u2022 as observed in table 1, the improvements of sequence classification tasks are less significant than those of span extraction tasks.We suggest that the existence of anchors is not a strong relevance indicator between our constructed query and context.therefore, constructing more relevant query-context pairs for sequence classification pre-training can possibly remedy this issue.\",\"Our experiments demonstrate that it is possible to analyze company executive and analyst language during earnings calls and use it to predict future earnings surprises with reasonable accuracy that is well above random chance.We acknowledge that the dataset contains events that result in significant earnings surprises so the performance numbers do not directly translate to a live trading setting in which many events do not result in material surprises.We also note that predicting future earnings surprises is correlated with but not equivalent to predicting future stock returns so more work must be done to translate our results into an actual trading strategy that is out of the scope of this paper.\",\"In this work, we have presented results that help inform us what tasks, methods and metrics are best suited for monitoring as well as methodologies and empirical information about the current set of models.We provide detailed information of how these results can be reproduced, to the extend that research have access to the plms in question, but these results have limitations, in order to reduce costs, many languages were not evaluated which might have left unforeseen patterns not discussed in this work.Moreover, few-shot learning, in particular, could exhibit large variance if different prompts were chosen, or a different set of exemplars chosen.Because of the high costs involved our work does not explore the performance difference when multiple sets of hyper-parameters were chosen.On the conceptual level, we make the assumption that system-level improvements on our tasks translate to downstream usefulness.While prior work suggests that this is the case, tools like chatgpt have significantly expanded the possible application space beyond the realm of \\u201ctypical\\u201d nlp tasks, and we don\\u2019t know how well our findings generalize to this space of tasks.\",\"This study focuses only on improving the speed of knn-mt during decoding; other problems with knn-mt remain.For example, it still demands large amounts of memory and disk space for the target token datastore.In addition, our subset knn-mt requires to construct a sentence datastore; therefore, the memory and disk requirements are increased.For example, the quantized target token datastore has 52gb and our sentence datastore has 2gb in the experiment of wmt\\u201919 de-en.Although subset knn-mt is faster than the original knn-mt in inference, datastore construction is still time-consuming.The decoding latency of our subset knn-mt is still several times slower than base mt for large batch sizes.The experiments reported in this paper evaluated the inference speed of the proposed method on a single computer and single run only; the amount of speed improvement may differ when different computer architectures are used.\",\"The results we present must be viewed in the context of a few limitations.A limitation is that we only perform experiments in english and on one task at a time.To be more comparable to a llm few-shot settings, other languages and a multi-task setup could be explored.Furthermore, in order to replicate the results access to none public models is required and inference must be performed on large amounts of data.Another limitation of our work is that it only explores the original cot prompting approach, but we do not explore subsequent improvements, such a self-consistency.\",\"The largest limitation of this study is that our annotation pipeline is automated.This makes it possible that there are errors in the noise annotations that we base our analysis on.Additionally, since we capture a naturally occurring noise distribution, our findings are coupled to the datasets we study here.Our findings may not generalize to distributions of noise in other datasets.\",\"Our proposed pre-training approaches require access to large gpu resources.Even using 10% of the original pretraining compute, the additional pre-training takes a long time duration to finish.This highlights that this procedure cannot easily be re-done with newer data being made available in an online setting.However the benefit of our approach is that once the pre-training is complete, our released model checkpoints can be directly fine-tuned for the downstream contextual as2 task.For the experiments in this paper, we only consider datasets from the english language, however we conjecture that our techniques should work similarly for other languages with limited morphology.Finally, we believe that the three proposed objectives could be better combined in a multi-task training scenario where the model has to jointly predict the task and the label.At the moment, we only tried using different classification heads for this but the results were worse.\",\"We train a ufet model and then fine-tune it for target fet tasks.In our approach, the ufet training data is the main source of limitations.First, the large size ufet training data are automatically generated, and thus may contain errors.Such errors can propagate to the fine-tuned fet models.Another problem is that, for some entity types, there are not many training examples.Moreover, some types useful in specific domains are not included in the ufet type vocabulary at all.As a result, the ufet model will not be as helpful when applied to fet data that contain such types.\",\"This study is constrained by limited input token length due to hardware memory limitations and lengthy training times.Even with the lsg attention mechanism\\u2019s efficiency, this inadequacy persists in both subtasks.Longer token length could improve summary relevance and factuality.\",\"In our work, when facing long sentences, a large number of synonym candidates can decrease the convergence speed of the lower and upper bounds.Therefore, in the experiments, we set up limitations on the length of the input sentences and the number of synonym candidates.Please note that it is still feasible to process long input sentences because of the anytime nature of our tool, however doing so would increase the unverifiable region, which essentially trades the tightness of bounds for efficiency.\",\"While we achieved preliminary results and created a preliminary projection of the factbank source and target corpus, we do not capture the full source and target nesting in our machine learning experiments.the embedded sources for the coming event are , which translates to \\\"according to the author according to mary according to john, did the coming event happen?\\\" In our experiments and machine learning architecture, we focus on the last nested source, or john in this example.we note that all experiments in this paper were performed using the flan-t5-base model.we are especially curious about framing this task using gpt-3, especially performing tasks on few-shot or in-context learning.Finally, we note that these experiments do not account for potential biases prevalent in fine-tuning large language models.We hypothesize that for some sources in text , there may be biases towards certain labels.\",\"Our approach is proposed based on the intuition that false negative samples should have high similarities with the positive samples that have the same gold entity type, and they also have low similarities with the positive samples of different entity types.However, our proposed approach does not guarantee the selected negatives are true negatives.Furthermore, when the negative samples are hard false negative samples, they are likely to have high similarities with other positive samples as well.However, such hard false negative samples are not prevalent in the datasets.\",\"The small amount of training data provided in this shared task poses a challenge for models that need large amounts of data to reliably learn linguistic patterns.owning to the lack of time and computing resources, we did not fully optimize our transformer models and we did not fully utilize and explore i) speaker-specific information, especially for the transformer models, ii) token frequency information in the corpora, as we assumed extension of morphological patterns is based on type, not token, frequency.Furthermore, we did not experiment with training models with either high-frequency, low-frequency and pseudoword items.It is possible that some speakers\\u2019 high\\u002flow\\u002fpseudoword items would be better served as part of the training set.3 was not able to evaluate the experimental items due to the unattested triphones.This shortcoming can be mitigated by using phonological features.\",\"Currently, to deal with spelling, missing, redundant character errors in chinese text, we jointly pre-train two sub-tasks based on a masked language model with task-specific attention mechanism and utilize re-tagging rules to reformulate the length of text during prediction.The proposed model might be less effective in more complex scenarios: word-level case according to the structure of our model, it could theoretically handle errors that are not limited to character-level, such as redundant or missing words.However, the currently used self-attention matrix works between tokens instead of spans of tokens.A novel attention mask strategy might be considered.If the problem is solved, then our model would be able to handle both chinese spelling correction and some kinds of grammatical error correction tasks at the same time.Task-specific backbone case the backbone of the proposed model is bert that is not taskspecific, while some errors in sighan happened in entities that might need priori knowledge to solve.To improve the performance of our model in more complicated applications, backbones that learn more task-specific knowledge should be considered.Languages mixture case in real world ocr or asr applications, a chinese character might be confused not only by another chinese character, but also by an english character due to their similar pronunciation or shape.Furthermore, a same character of simplified chinese and traditional chinese might be visually different.High efficiency case industrial applications often require the prediction time in milliseconds-level under controlled usage of gpus, which would bring troubles to large models.Distillation or truncating strategies might be a way to improve the proposed model.\",\"This paper proposes a pre-trained language model with prompts for temporal knowledge graph completion.However, there are some limitations in our method: 1) our prompts in the temporal knowledge graphs, especially the time-prompts, are built manually.It needs to be reconstructed manually for different knowledge graphs.We are exploring a way to build prompts in temporal knowledge graphs automatically.2) our model uses a random sampling method, which suffers from the problem of few high-quality training samples and high sam- ple noise.\",\"Despite memory and gpu limitations presenting significant obstacles for our project, we were still able to create high-quality fake scientific papers.Nonetheless, we believe there is room for improvement in addressing such limitations.Moreover, processing entire publications would require substantial computational efforts.Beyond detectgpt , other zero-shot text detectors such as gptzero7 present promising solutions worth testing on our benchmark dataset.However, at the time of writing, such solutions are not available for experiments at scale.\",\"Unfortunately, we cannot access most sighan2008 bakeoff datasets, which were proprietary but used by many previous works.This makes the comparison in table 2 a little unfair.We argue that we replaced these non-accessible datasets with the ones publicly accessible.thus they also replaced datasets just as we did, which makes them the only directly comparable work to ours.\",\"Long documents, intuitively, have more possible translations than short documents, so a dynamic number of generated translations may be a better choice when augmenting the data, which balances the training cost and the performance gain.Such dynamic sampling and matching could potentially be used to increase training efficiency.target-side augmentation can potentially be applied to other seq2seq tasks, where the data sparsity is a problem.\",\"Despite the simplicity and strong empirical results, residual prompt tuning still has few limitations.First, its performance is still not on par with fine-tuning on.Also, our method uses slightly more parameters than prompt tuning to train the reparameterization network.However, this is not a significant limitation given the full language model size.We have tried to cover several model architectures, but so far we have focused on encoder-decoder and encoder-only models.another limitation is that our method strives to reduce the number of trainable parameters, but uses a longer sequence than the original input text.\",\".3, our proposed method fails to make a significant improvement on span boundary identification.For one thing, the annotation inconsistency in the dataset hinders the model\\u2019s understanding.For another, our span proposal module leverages the contextual information alone with implicit training signals for span boundary information.secondly, though tara saves up to 56% inference time compared to the previous amr-guided work, its entire training requires more than 7h on 4 tesla t4 gpus.The bottleneck is the incongruity of pre-trained language models and non-pre-trained gnns.finally, arguments on wikievents and rams are still relatively close to its event trigger , and thus connecting sentencelevel amr graphs is enough to model the longdistance dependency.Otherwise, document-level amr graphs with coreference resolution are in demand.\",\".We release any data, code, and models produced during this study publicly for further research by the community.We license this release under cc-by-nc-sa 4.we plan to perform similar experiments on a large dataset while benchmarking and comparing our current models\\u2019 performance.We also plan to investigate online or active learning for the same.Finally, we also aim to expand on the theoretical underpinnings of sublime aggression and offense by attempting to identify these within other more tangential domains, viz.\",\"Cooking recipes constitute a single genre within procdocqa, with a well-grounded task and large range in risk of harm and user expertise.Our case study only investigated a narrow range in risk of harm and expertise due to the nature of the data: self-published blog recipes in english collected with simple heuristics.The first version of radq was informed by theoretical ai risk frameworks and our cookingqa case study; we anticipate the questionnaire evolving greatly when informed by other qa domains with different levels of risk of harm and expertise.This work only considers immediate risks to humans; longitudinal risks such as the propagation of information are an open research topic.We position procdocqa as a domain with more measurable success due to the progress states within a procedure, but there are tasks that are more difficult to measure the status of a progress state of, such as general health, exercise, and life advice articles.This work contributes to risk mitigation by concretizing risks in user-aware scenarios.Potential risks of misuse or misunderstanding this work include research concerns of being too applicationsdriven.\",\"There are a few limitations to the current framework.Firstly, verify-and-edit works the best for open-domain question-answering tasks that require complex reasoning.Less complex datasets or commonsense datasets that do not require knowledge retrieval may not result in high improvements.Secondly, it is most ideal to edit a group of mostly incorrect samples, which we try to select by using consistency.Thus, our method is reliant on the consistency method\\u2019s performance and its abilities to separate correct and incorrect predictions.Most often, it can demonstrate a larger improvement with a more challenging set of examples.To address these limitations, we plan to work on reducing the noise brought in the rationale-editing stage and utilize more knowledge resources, such as knowledge bases, as a follow-up.\",\"Limitation of our datasets, we concatenate one reference and model-generated response, which are then fed to the encoder.Employing rade when the reference response is not available.Considering the reference is not always available in real-world scenarios, we design two alternatives to enable rade, i., constructing a pseudo-reference via retrieval or generative method.\",\"The proposed model, recurrent attention network , effectively models long sequential data by propagating information window-by-window through the sequence via its well-designed recurrent architecture.However, the multi-head selfattention applied to each window is still limited to local attention, which prevents it from providing a global dependency relationship for the entire sequence.This limitation restricts ran\\u2019s application in scenarios where a global dependency relationship is necessary, such as visualizing attention weights for the entire document via a heatmap.This limitation potentially reduces the interpretability of the model, although it does not affect the model\\u2019s performance.Hence, exploring ways to incorporate global attention mechanisms into the ran architecture is a promising research direction to improve its interpretability and expand its range of applications.\",\"In this exploratory study, we focused on englishlanguage resources.Further, we examined only one social media platform, twitter.As any other platform, twitter has a biased demographic representation of users in terms of language, location, ethnicity, gender, age, socio-economic status, and other characteristics.In particular, twitter is predominantly used in the united states.10 as a result, user attitudes examined in this study primarily represent western views and may differ significantly from views common in other regions of the world.Future studies on aporophobia need to include other languages and world regions and consider cultural differences while measuring and mitigating this type of social bias.When searching for aporophobia-related texts, we excluded derogatory terms and slurs associated with the group \\u2018poor\\u2019 as such explicit forms of online abuse tend to be easier to detect by human 10 umber-of-active-twitter-users-in-selected-count ries\\u002f annotators and nlp models.Nevertheless, when designing tools for measuring and mitigating aporophobia such explicit expressions need to be taken into account.Furthermore, there is a wide variety of linguistic expressions referring to poor and homeless people, and sometimes this target group is not even mentioned at all, but could be inferred from the context.To effectively confront aporophobia, nlp resources need to have a wide coverage of explicit and implicit linguistic expressions of the phenomenon.However, many social media posts combine text with other types of data, such as images and videos.Recent techniques for modeling multi-modal data can be employed to ensure a better coverage of various types of social media posts.future research on this topic should comply with trustworthy ai principles of transparency, justice and fairness, non-maleficence, responsibility, and privacy.Special attention should be paid to involving all legitimate stakeholders in the identification and definition of actions to counteract aporophobia, including the affected communities, non-governmental organizations and government officials working on poverty mitigation.In particular, the views and needs of the communities from both the global north and the global south should be included.\",\"This paper only considers analyzing contrastive learning in the fine-tuning stage, but we note that with isotropy being a desiderata for pre-trained language models , recent works have considered incorporating contrastive objectives in the pre-training stage.we further note that the analysis in this work focuses on theoretical properties occurred during contrastive srl , thus only focuses on semantic textual similarity data as a proof of concept.However, with the growing attention on contrastive learning, we argue that the typical sts-b is perhaps no longer sufficient for revealing the full ability of models trained with newer contrastive srl frameworks.We call for a standard practice that the performance of contrastive srl should be assessed on both semantic textual similarity and information retrieval tasks ).We leave analysis on information retrieval tasks leveraging our analysis pipeline for future studies.For example, how high intra-sentence similarity is related to the learned attention towards tokens that enable document retrieval with better performance.\",\"The performance of kpe is also related to the used pre-trained language model , in addition to the proposed framework.Kpe could suffer from unsatisfactory performance when the base plm is not strong enough.Applying our proposed kpe to stronger plms, such as deberta, may lead to further improvements.\",\"We acknowledge a range of limitations of our work.we do not view this as a limitation per se, again pointing to the recent literature on the value of human label variance pointing at a potential loss of valuable information if we overly focus on arriving at a single gold label per instance, with high confidence.Future modeling work involving entity labels should, however, carefully inspect the role label variation, and potentially remove or aggregate selected annotations, before incorporating the labels as signal into predictive models.We explicitly refrained from training model in this paper to avoid the risk of training a predictor on an unfavorable noise-to-signal ratio.Our data set focuses on english-language news reports, sampled from 2017 to 2019 in mainstream media outlets in the us and uk, and as such focuses on cultures and communities which are already well-resourced and well studied.we explicitly caution against projecting annotations across languages without careful validation as we expect the manifestation of framing, views on entities to vary widely across countries and communities.a related current limitation is the focus on just a single issue and validation of our narrative framing framework for other issues is an important direction for the future.Finally, the annotation process was slow and costly, relying on trained, highly educated annotators with constant monitoring, rendering larger scale annotations challenging, on the one hand.we hired four local annotators who were paid an hourly rate of $53 au in line with the casual research assistant hourly rates set up in the university of melbourne collective agreement.We will release the narrative framing corpus comprising of 428 news articles annotated with frame labels, entities, their narrative roles and stakeholder categories.our data set builds on news articles from the nela corpora 2017-2019, which were released to the public domain.14 we release our code and narrative frames corpus under a mit license.\",\"While our approach inherits the linear runtime complexity of the backpropagation algorithm, runtime concerns should not be fully neglected.Firstly, the linear runtime is only an analytical result, not an empirical measure.This means that the actual runtime of the backpropagation and thus our algorithm depend heavily on their implementation.For instance, some deep learning frameworks do a better job at reusing and parallelizing computations than others.Indeed, our code is optimized for good readability and extensibility at the expense of speed, which hints at another limitation of our approach: our approach requires deep integration with the framework as it needs access to all model weights and the computation graph.For this reason, our approach cannot be easily packaged and wrapped around any existing model or framework and we instead developed our own jax-based reverse-mode autodifferentiation library, based on the numpy-based brunoflow library.While we release our library to enable other researchers to analyze models through their gradient graphs, it faces some computational and memory constraints.In our experiments, running the three semirings together on a single sentence can take several minutes using google\\u002fbert_uncased_l-6_h-512_a-8, the 6-layered pretrained bert from huggingface , totaling our experimentation time on our datasets at about 10 cpu-hours.For improved adoption of this method, we encourage the direct integration of semiring implementations into the most popular deep learning frameworks.Our final point pertains not only to our study but to most interpretability approaches: one has to be careful when drawing conclusions from gradient paths.Cognitive biases, wrong expectations, and omitted confounds may lead to misinterpretation of results.our work aims to make the inner workings of neural network models more interpretable.On this account, we hope to contribute to reducing biases inherent in model architectures, pre-trained model weights, and tasks by increasing overall transparency.\",\"We showed that our proposed method can greatly improve the performance of parameter efficient tuning on diverse nlu tasks and three different pre- trained models.However, we acknowledge the following limitations: the more super-sized pretrained models with tens of billions of or more parameters were not studied due to limited computation resources.Other tasks in natural language processing, like the text generation tasks, were also not considered.But our framework can be easily transferred to other backbone architectures and different types of tasks.It would be of interest to investigate if the superiority of our method holds for other backbone models and types of tasks.\",\"Demonstration selection methods we assume that diversity can be obtained by choosing demonstrations with different program structures.This is based on previous work that demonstrated the importance of diversifying program structures in semantic parsing tasks.We also try to diversify utterance words or program symbols but do not consider more complex utterance features that could be applied to a wider range of language understating tasks.We also assume that recall matters more than precision when designing cover-ls algorithm.That means we aim to choose a set of demonstrations that covers every predicted local structure in sy\\u0303test , since it has the potential to be a correct one.We do not predict whether a specific structure should be covered.Furthermore, our approach for increasing gold structure coverage by using additional beam candidates could be improved by employing search methods specifically targeted for diversity.Retrievers we used different retrievers for noft and ft setups based on the retriever that worked best on the development set.Future research should be conducted to understand why different retrievers are preferred in different setups.\",\"Our proposed taxonomy is subject to extension, and we expect new phenomena to be included into its scope as the field progresses and as more document sources are considered.Using a taxonomy as an organizational basis for the proposed schema is dictated by our aim to keep the schema simple.The design of future, formalized reporting schemata might adopt an onthology-based approach as it affords more flexibility, and take into account interoperability with the existing proposals in the linked open data community.While source analysis is only one of our contributions and is thus limited in scope, we have observed that increasing the number of documents from the same source yields diminishing value: if a source uses a certain non-linguistic textual element, it does so consistently.This suggests that the future qualitative studies of document sources used in nlp should be conducted in a breadthfirst fashion, with few documents samples from many sources, unless quantitative measurement is desired or unless a source is known to accommodate a wide variety of document types with different publication and formatting standards.We do not provide specific details on documenting the text production environment, which represents a promising future research avenue.The study of how the texts in nlp are created is a critical research direction: due to the increased applied use of pre-trained generative language models, documenting the text form and origin is a pressing need.\",\"Presenting significant obstacles for our project, we were still able to create high-quality fake scientific papers.Nonetheless, we believe there is room for improvement in addressing such limitations.\",\"The limitations of this work mostly come from our assumptions: 1) a randomly initialized and frozen tlm, and 2) input tokens are all different and randomly sampled.These two assumptions obviously do not hold true for human languages and pre-trained tlms.Therefore, we attempted to empirically verify the existence of lemmas and properties on a pre-trained tlm without positional embeddings in \\u00a75.That being said, several methods could be attempted to remove these assumptions.Firstly, we can analyze the training dynamics of a tlm to shed light on the model parameter distribution after pretraining.Secondly, zipf\\u2019s law or a simple n-gram language model could be used to quantify the degree of input token duplication in human languages.This might give us a more accurate estimate of the variance at different positions.\",\"We identify the following limitations in our work: first, our ftt framework captures and models topic-level temporal patterns for forecasting temporal trends.Though the forecasts bring better temporal generalizability, ftt could hardly forecast the emergence of events in new topics.Second, ftt considers temporal patterns based on the topic-wise frequency sequences to identify patterns such as decrease, periodicity, and approximate stationery.There might be diverse patterns that could not be reflected by frequency sequences.Third, limited by the scarcity of the dataset that satisfies our evaluation requirements , we only performed the experiments on a chinese text-only dataset.Our method should be further examined on datasets of other languages and multi-modal ones.\",\"One of the advantages of the fusion-in-decoder approach is that it uses the off-the-shelf t5 architecture with publicly available checkpoints.The proposed fido modifications strongly improve performance and inference speed for retrieval-augmented question-answering, but require pre-training from scratch.It is in general preferable to have a small number of checkpoints that can be fine-tuned for any application.For example, it may not be feasible to train different giant language models for use in the retrieval-augmented setting.Instead, the architectures for such large models may need to be a compromise for different use cases.\",\"More work is needed to uncover the causes of the inconsistent performance across randomly initialized models in experiment 1.although the bias toward forward function application implemented in experiment 2 was effective in our experiments, it is unlikely to work as a general-purpose method, since languages vary in their branching characteristics and in the contexts in which they apply forward and backward function application.\",\"Symbolictom assumes stories are written chronologically, which may not hold for some human-written stories.This may be alleviated using time-stamping models like faghihi and kordjamshidi.Furthermore, since we use off-theshelf models and openie ) to create and update the graphs, the presented approach may propagate errors as revealed in the linguistic diversity experiments.However, these issues can be largely alle- 5as a part of out-of-domain testing, we also create a more challenging version of the available tom datasets, available at along with a corrected version of tomi.Viated by using more sophisticated models, even the llms like gpt3 themselves.We do not experiment with them due to budgetary restrictions.Currently, all nlp datasets available for theory of mind reasoning describe sally-anne tests.In these datasets, the concept of large distances is absent, meaning that anyone specified to be in a location is assumed to be a witness of the actions that occur there.This assumption can be violated in realistic settings.For example, \\u201canne is in the usa\\u201d does not imply she is a witness to every action happening in the usa.we could also refine the witness detection algorithm by sampling paths between the inserted edge and each node referring to a person, to query an lm directly on that substory by asking if the person witnessed the action.To be able to test both of these ideas, we would need to obtain new theory of mind datasets with significantly more types of interactions and physical commonsense in the stories.\",\"In this work, we propose a self-training method which requires unlabeled data in target languages.Recall that we remove gold labels from readily available target-language training data from the same public ner dataset, and use them as unlabeled data in our experiments.However, this might not perfectly simulate a real-life application scenario.Firstly, most free text in target languages might not contain any predefined named entities.This requires careful data cleaning and preprocessing to produce unlabeled data ready for use.Secondly, there might be a domain shift between labeled source-language data and unlabeled targetlanguage data, which poses a question on the effectiveness of our method.Furthermore, the ner datasets used in this work contain only a few entity types and different entity classes are relatively balanced.However, on datasets with a larger number of classes, each class will be underrepresented in a batch and a larger batch size might be required for contrastive selftraining to work satisfactorily.Also, if the entity type distribution is long-tailed, prototypes for those rare entity types might be inaccurate, and this affects the efficacy of prototype-based pseudolabeling.Lastly, as we observe slight drops of pseudo label quality at the end of training for some languages, the pseudo label update strategy can be refined for further improvement.\",\".In this study, we opted for the minimal context approach for practical reasons, such as budget, time constraints, and copy rights of the source corpora.The minimal context approach allowed the annotation sample to represent as many writers as possible for better generalization with relatively small sample sizes.However, future research should use longer units of analysis to enhance the quality of manual annotation.Despite this limitation, the results of the current study indicated that the current approach is a promising direction for further research on automated analyses of rhetorical features.\",\"And advantages of both methodologies for the two scenarios.According to our results, the supervised models show good id performance at the price of a significant drop in ood performance.In contrast, unsupervised zero-shot systems excel in ood settings but do not outperform supervised models in id contexts.A reasonable compromise between the two methodologies is the nli-fine-tuned method, which improves odd results compared to supervised systems and achieves good performance compared to the zero-shot approach in an id setting.In a situation where limited training data are available, the fine-tuned nli system has the advantage of achieving a good trade-off between id and ood performance, with less training data than supervised models.Using a zero-shot nli-based system is preferable in situations where the final data distribution is unknown.Furthermore, it requires less implementation time and no training dataset.Our experimental analysis is not without limitations.We focused on emotive content , therefore our results might not be extendable to other domains.Emotions have a certain degree of subjectivity that can affect the annotation process by making data annotator-dependent.In other fields, this might not be the case.Moreover, our analysis is limited to ten datasets that we believe are representative of the work in this field.However, many other datasets are available, especially in the hate speech domain, and a wider evaluation might lead to a more definitive\",\"Our method depends on a large-scale paraphrasing corpus.We only test our method on the english ls task.Excluding english, other languages have large-scale paraphrasing datasets available, e.our method can be easily extended to these languages.But, for some languages that cannot obtain enough paraphrasing datasets, our proposed method cannot be used.Another limitation is that our method may struggle to generate substitutions for rare or unusual words and phrases, as they may not have encountered sufficient examples of these words in the training paraphrase data.\",\"The results of this work may be limited in reliability and replicability due to some hard-to-avoid aspects of the low-resource setting.Our numerical results have low statistical power, as illustrated by the wide bleurt confidence intervals in tables 4 and 6.without a large test set, most differences are not statistically significant at the accepted level.They should be treated as trends which can motivate further investigation rather than solid\",\"One limitation of the proposed method, infinity, is that it is currently limited to fully unsupervised and not incorporating any parallel data which may lead to performance deterioration in uncommon scenarios.Furthermore, it only works well with languages having limited morphology such as english and may not perform as well on languages with complex morphology.Finally, the method may have low scalability to long text as it requires large gpu resources.These limitations inspire further investigation to improve the performance and applicability of the method.\",\"Because we focus on off-the-shelf tools in this work, we are necessarily constrained by the availability of such tools in different languages and contexts.In addition, we are constrained by the current state of the art for amr parsing and, more challengingly, alignment.Amr parsing continues to improve, but alignment has only recently attracted interest again as a problem, such as in.Additionally, this work, in evaluating six fewshot settings across six pairs of datasets and a number of seeds suffers from a combinatorial problem in terms of the necessary compute infrastructure.As discussed in the paper, our work consumed roughly a month of gpu time.Combined with the size of the models, this limits the accessibility of this vein of research.More effort understanding how to narrow down the choice of datasets before studying transfer would go a long way towards alleviating this issue.\",\"We think the following three points are the limitations of this work.3, the computational cost of our distillation approach increases linearly with the number of gd steps and the distilled data size.to optimize the distilled dataset through the gradient method, we utilized word embedding vectors instead of directly optimizing the text as in the existing work.Therefore, the distilled dataset we obtained cannot be applied to models with different word embeddings, such as other pretrained models or full-scratch training.In our experiments, we evaluated our approach only on text classification tasks.However, our approach can also be applied to text generation tasks as well by applying the attention labels to all input tokens and using vocabulary-wise soft labels.\",\"While our dataset shows promise based on the models we train with it, at about 1000 annotated examples in the training partition, it is relatively small.Before working on increasing the size of the dataset, we need to work on improving the guidelines provided to mechanical turk workers and finding more robust ways of excluding bad faith annotators., polarity, beliefs involving out-of-sentence coreference resolution, as well as believer and belief span annotations.\",null],\"marker\":{\"color\":\"#CFD8DC\",\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"other\",\"showlegend\":false,\"x\":[13.370918273925781,13.129385948181152,12.053235054016113,9.176992416381836,10.318509101867676,14.660441398620605,12.036805152893066,11.488358497619629,13.44868278503418,9.715017318725586,13.432887077331543,14.630691528320312,12.291668891906738,13.063830375671387,10.116581916809082,10.615960121154785,13.515088081359863,7.377702236175537,11.478253364562988,13.423752784729004,14.666478157043457,13.49186897277832,11.215673446655273,13.94821548461914,10.371855735778809,11.708295822143555,10.533160209655762,11.076578140258789,13.269871711730957,11.707043647766113,10.0802640914917,9.736185073852539,11.962668418884277,13.256695747375488,13.812932014465332,11.750669479370117,10.6912841796875,10.505718231201172,12.046316146850586,12.784538269042969,13.00548267364502,10.709975242614746,9.86646842956543,11.018011093139648,9.934183120727539,11.81261157989502,13.272284507751465,12.062804222106934,12.036822319030762,9.941655158996582,12.115907669067383,12.13844108581543,11.646269798278809,13.103399276733398,11.448694229125977,12.129443168640137,8.949137687683105,13.368173599243164,11.18526554107666,10.853765487670898,11.767834663391113,13.02305793762207,10.467449188232422,10.801329612731934,13.266253471374512,12.536324501037598,14.27286434173584,11.332353591918945,11.572169303894043,12.032867431640625,11.311735153198242,12.767190933227539,10.60842227935791,14.037425994873047,12.93950080871582,10.924229621887207,13.24350643157959,14.072285652160645,12.314461708068848,13.187835693359375,7.383323669433594,12.919633865356445,10.894185066223145,9.330490112304688,13.7048921585083,12.746039390563965,14.053203582763672,9.755548477172852,12.508106231689453,13.076180458068848,12.40343189239502,10.736129760742188,10.055603981018066,12.742337226867676,13.24665641784668,11.690393447875977,9.409212112426758,12.181706428527832,13.359391212463379,11.715903282165527,11.687442779541016,10.953727722167969,11.664239883422852,10.497753143310547,11.341198921203613,11.107361793518066,13.427091598510742,13.029200553894043,12.824102401733398,12.373723030090332,9.016769409179688,12.40432357788086,10.832151412963867,9.639232635498047,13.352508544921875,13.435611724853516,10.298675537109375,13.144866943359375,12.115567207336426,12.133270263671875,11.85163688659668,11.77834701538086,11.117612838745117,10.14604663848877,8.616178512573242,12.53096866607666,10.493721961975098,13.521265983581543,11.526937484741211,13.583752632141113,12.534470558166504,11.991167068481445,12.56511116027832,10.793107032775879,11.644185066223145,13.822220802307129,9.958580017089844,11.499373435974121,13.148080825805664,11.043363571166992,11.389510154724121,10.509968757629395,12.611907005310059,12.922550201416016,13.833020210266113,12.031327247619629,14.373254776000977,11.09329891204834,9.891433715820312,12.794100761413574,14.891044616699219,12.780519485473633,10.511573791503906,13.316277503967285,11.823945999145508,11.110907554626465,13.379473686218262,10.610578536987305,12.701107025146484,9.472084999084473,11.930293083190918,12.070284843444824,12.41022777557373,10.926301956176758,12.063966751098633,12.479164123535156,14.537768363952637,12.767754554748535,13.96374797821045,9.236536026000977,13.109007835388184,12.913456916809082,13.412822723388672,12.305880546569824,9.295774459838867,12.24936580657959,11.983236312866211,14.529646873474121,12.193758964538574,14.511344909667969,9.693695068359375,9.689292907714844,13.491232872009277,12.556979179382324,9.91010856628418,13.157806396484375,12.249117851257324,11.007497787475586,13.452848434448242,11.795064926147461,14.598572731018066,14.020545959472656,12.533883094787598,12.988822937011719,8.690645217895508,11.20370101928711,12.743304252624512,12.631157875061035,13.051981925964355,13.683547019958496,9.644620895385742,13.422666549682617,11.53226089477539,12.461970329284668,12.160737991333008,10.95163345336914,12.298341751098633,11.086115837097168,10.601217269897461,13.229673385620117,11.309616088867188,10.2756929397583,11.683799743652344,13.17288589477539,10.630250930786133,13.20805549621582,9.034151077270508,12.288163185119629,13.169450759887695,13.984204292297363,13.00132942199707,13.423680305480957,12.81550121307373,12.839468955993652,10.526449203491211,10.9198637008667,12.89007568359375,11.531067848205566,9.991304397583008,8.934313774108887,11.857382774353027,12.960347175598145,10.920783042907715,12.908148765563965,10.689255714416504,12.134014129638672,11.946715354919434,13.094372749328613,13.344542503356934,9.364677429199219,13.49194622039795,14.036178588867188,12.116925239562988,13.29471492767334,12.33942699432373,12.893332481384277,9.700372695922852,13.170490264892578,11.947681427001953,11.31263542175293,11.299057960510254,12.82114315032959,12.427443504333496,12.122262001037598,12.631631851196289,9.690023422241211,12.20253849029541,14.330798149108887,11.15615463256836,13.041043281555176,9.891294479370117,12.32588005065918,13.160538673400879,10.817564010620117,10.43750286102295,12.549263000488281,11.956257820129395,13.00434684753418,12.208963394165039,12.81181812286377,12.09424877166748,8.458510398864746,10.7952880859375,11.499159812927246,11.329635620117188,10.129556655883789,13.141329765319824,12.8485689163208,10.520578384399414,11.795881271362305,8.96495246887207,12.5131196975708,12.250031471252441,11.987841606140137,11.976017951965332,13.291985511779785,10.665505409240723,12.349231719970703,10.726137161254883,13.433174133300781,13.509625434875488,10.083642959594727,12.546037673950195,11.583261489868164,13.774648666381836,13.708701133728027,12.732115745544434,13.01210880279541,13.521023750305176,10.25667667388916,9.469538688659668,12.784807205200195,11.801420211791992,13.419061660766602,11.960289001464844,10.684747695922852,12.027467727661133,12.509485244750977,11.927572250366211,10.848775863647461,13.290578842163086,10.033879280090332,10.719996452331543,9.741799354553223,11.728726387023926,11.378923416137695,12.638176918029785,14.252902030944824,10.768924713134766,11.95698356628418,12.317420959472656,14.0381498336792,12.051312446594238,11.523858070373535,11.390949249267578,14.203413963317871,11.359419822692871,10.83908748626709,10.67739200592041,13.598803520202637,12.156889915466309,9.579813003540039,9.706430435180664,11.3063325881958,10.948480606079102,9.552821159362793,7.37657356262207,10.074501037597656,9.88803482055664,10.70562744140625,10.096357345581055,9.514257431030273,11.869437217712402,13.360132217407227,11.943005561828613,10.056159019470215,11.603903770446777,10.491911888122559,12.131387710571289,12.572060585021973,12.586329460144043,10.879775047302246,12.275635719299316,12.378890037536621,8.452288627624512,13.219350814819336,11.117956161499023,12.842188835144043,10.343193054199219,11.458611488342285,12.163107872009277,13.409536361694336,10.6093168258667,13.104722023010254,12.859635353088379,12.956547737121582,13.932485580444336,12.793530464172363,11.9458589553833,9.92127799987793,10.339512825012207,12.970459938049316,9.584610939025879,8.958019256591797,13.689056396484375,12.96224308013916,14.674088478088379,11.41112995147705,12.209307670593262,10.849552154541016,13.547089576721191,12.167088508605957,12.126701354980469,13.948671340942383,9.636033058166504,13.134465217590332,10.130858421325684,9.457054138183594,13.184537887573242,12.007427215576172,14.142720222473145,11.775663375854492,12.841741561889648,12.635038375854492,14.809099197387695,12.089456558227539,13.998213768005371,10.508298873901367,12.154494285583496,12.126131057739258,13.441146850585938,12.448594093322754,10.68613052368164,8.45823860168457,11.580342292785645,13.59794807434082,11.304731369018555,12.631580352783203,14.237119674682617,12.596136093139648,10.8030366897583,11.617314338684082,11.043274879455566,10.333301544189453,12.893991470336914,9.347188949584961,13.065559387207031,10.869949340820312,13.831562042236328,11.349591255187988,12.7172269821167,11.51797866821289,12.078713417053223,11.099891662597656,12.576135635375977,12.677592277526855,10.388179779052734,13.457932472229004,11.858072280883789,13.203768730163574,11.344524383544922,8.479249000549316,8.982152938842773,13.389139175415039,13.135329246520996,12.502355575561523,11.706390380859375,11.148998260498047,11.823305130004883,12.208524703979492,14.230873107910156,9.648701667785645,13.797762870788574,10.910603523254395,11.247138977050781,13.599188804626465,13.398489952087402,13.84194278717041,10.38425350189209,11.436079978942871,10.977278709411621,11.01844596862793,12.142057418823242,10.788090705871582,12.737088203430176,12.740315437316895,10.911097526550293,13.118880271911621,11.962443351745605,11.761299133300781,12.381904602050781,12.655721664428711,13.278562545776367,11.541192054748535,11.960105895996094,12.06750202178955,11.582871437072754,14.70927619934082,7.377814769744873,10.94760513305664,12.688840866088867,11.298312187194824,13.155099868774414,12.096270561218262,11.54802131652832,11.816882133483887,11.602961540222168,10.128204345703125,12.829883575439453,12.828737258911133,12.097203254699707,14.084954261779785,12.921464920043945,12.660555839538574,13.109821319580078,11.833967208862305,13.37723445892334,8.874615669250488,12.980786323547363,11.48216724395752,12.063813209533691,14.066935539245605,11.25208568572998,13.40531063079834,9.185076713562012,13.347719192504883,12.043641090393066,12.218363761901855,12.661172866821289,12.27241325378418,12.85143756866455,10.461129188537598,13.48518180847168,12.783422470092773,14.80752944946289,12.707290649414062,12.919700622558594,11.145163536071777,11.602890968322754,12.870562553405762,11.294469833374023,12.610368728637695,11.885260581970215,9.098445892333984,11.59610366821289,13.457826614379883,12.116194725036621,13.550716400146484,11.596869468688965,11.818647384643555,12.577539443969727,12.904958724975586,13.415432929992676,11.693746566772461,12.703805923461914,11.318652153015137,10.021062850952148,13.7789945602417,13.180118560791016,12.55765151977539,12.013988494873047,12.515264511108398,10.183858871459961,14.2123384475708,12.814457893371582,9.758732795715332,13.05067253112793,12.469999313354492,11.045397758483887,12.057945251464844,13.240927696228027,12.5480375289917,13.035054206848145,14.163431167602539,8.444754600524902,12.125141143798828,13.466667175292969,13.045393943786621,13.28132438659668,12.379011154174805,12.005993843078613,12.326628684997559,14.027329444885254,11.669485092163086,13.26653003692627,11.979989051818848,12.070928573608398,13.135702133178711,10.559443473815918,11.85631275177002,11.457378387451172,11.508805274963379,11.165696144104004,11.695890426635742,12.20832633972168,14.070244789123535,13.204364776611328,12.635759353637695,11.135004043579102,13.794519424438477,12.209294319152832,10.991649627685547,12.64378833770752,10.4540376663208,9.809300422668457,14.669600486755371,12.910438537597656,12.634286880493164,12.7755765914917,9.527647972106934,11.112241744995117,13.294198989868164,12.223224639892578,12.5613374710083,11.990836143493652,10.525680541992188,9.056483268737793,12.034914016723633,12.488144874572754,12.864766120910645,12.785632133483887,12.839990615844727,9.995662689208984,9.610950469970703,13.077008247375488,12.132308006286621,11.857993125915527,10.093789100646973,11.51099967956543,12.600337982177734,12.553210258483887,11.959351539611816,12.222475051879883,9.127450942993164,9.365364074707031,14.669790267944336,13.126644134521484,10.997435569763184,12.34501838684082,10.386831283569336,12.14587116241455,10.011884689331055,9.44206714630127,10.085397720336914,11.6487455368042,12.214672088623047,11.695626258850098,12.45635986328125,12.627851486206055,11.547647476196289,12.308051109313965,11.6715669631958,11.371240615844727,12.161478996276855,13.121188163757324,11.109047889709473,11.772663116455078,12.54744815826416,12.980629920959473,12.664565086364746,12.965189933776855,12.633472442626953,12.656468391418457,12.25086784362793,13.538830757141113,10.942832946777344,13.075262069702148,12.82534408569336,12.897477149963379,11.071670532226562,11.009050369262695,12.54705810546875,11.65331745147705,10.980219841003418,13.186958312988281,10.817108154296875,11.360865592956543,11.340490341186523,13.352699279785156,12.394287109375,12.175155639648438,10.401163101196289,13.399001121520996,13.054078102111816,13.234932899475098,11.95809555053711,12.755426406860352,12.919485092163086,13.180474281311035,11.549281120300293,13.677132606506348,11.384831428527832,12.252188682556152,12.253417015075684,13.167330741882324,14.483746528625488,13.036728858947754,12.167168617248535,12.13988971710205,11.9710693359375,9.591856002807617,10.987706184387207,9.776192665100098,10.615574836730957,10.822587966918945,13.627337455749512,10.616890907287598,11.942363739013672,10.806941032409668,10.799115180969238,13.354889869689941,12.175363540649414,9.722004890441895,13.387664794921875,13.105334281921387,12.9909086227417,11.80427074432373,12.60688304901123,12.61043930053711,13.366689682006836,12.573036193847656,12.040092468261719,12.823715209960938,13.407584190368652,13.11092758178711,11.295598983764648,7.378913879394531,11.239259719848633,12.489267349243164,11.566067695617676,11.74985122680664,11.563640594482422,10.14163875579834,11.251019477844238,13.265475273132324,10.76784896850586,13.086859703063965,12.890549659729004,12.1961030960083,12.911227226257324,12.589641571044922,11.419158935546875,11.553984642028809,11.924825668334961,12.792930603027344,10.807540893554688,10.163413047790527,10.99303913116455,10.524809837341309,12.769533157348633,12.77454948425293,9.693831443786621,13.233333587646484,12.034689903259277,12.91292667388916,12.678542137145996,9.521735191345215,12.912367820739746,12.97086238861084,13.459036827087402,11.487768173217773,9.833525657653809,11.14234733581543,13.480935096740723,12.078130722045898,9.960002899169922,11.432868003845215,10.628692626953125,12.750417709350586,9.476323127746582,9.342026710510254,13.423416137695312,9.424540519714355,7.376262664794922,13.130826950073242,10.568912506103516,9.721040725708008,10.81435489654541,11.788331031799316,8.900181770324707,9.627016067504883,13.162022590637207,9.992947578430176,11.717680931091309,12.325252532958984,12.809517860412598,11.34546184539795,12.711065292358398,14.245421409606934,9.580374717712402,12.038093566894531,13.186022758483887,13.563688278198242,11.28506088256836,9.374109268188477,11.792803764343262,12.840495109558105,11.823312759399414,13.311737060546875,11.878796577453613,12.821337699890137,9.985834121704102,12.948945045471191,11.017476081848145,11.796930313110352,10.58350658416748,13.194756507873535,8.45887565612793,13.758413314819336,9.69582462310791,10.071861267089844,11.633549690246582,12.814940452575684,12.761361122131348,12.414091110229492,13.483663558959961,9.704805374145508,12.544641494750977,11.504131317138672,12.886697769165039,14.717803001403809,11.598715782165527,13.620635986328125,13.214582443237305,12.648982048034668,12.450103759765625,7.375244617462158,12.124582290649414,12.046445846557617,11.397716522216797,13.368813514709473,12.67910385131836,11.729294776916504,11.161080360412598,12.529500961303711,11.859766006469727,12.55379867553711,12.254478454589844,14.675714492797852,11.61102294921875,12.732407569885254,11.747307777404785,12.72380256652832,11.885045051574707,9.670793533325195,9.662339210510254,10.813069343566895,11.373141288757324,10.591872215270996,11.289769172668457,11.833226203918457,13.864373207092285,12.610286712646484,13.52210807800293,11.664884567260742,9.719606399536133,10.962550163269043,13.613184928894043,12.796432495117188,12.644097328186035,11.871627807617188,10.188582420349121,11.939449310302734,11.065254211425781,9.725427627563477,11.014341354370117,12.2175874710083,14.669583320617676,13.134209632873535,11.935638427734375,10.6268892288208,11.170269966125488,12.729118347167969,13.090108871459961,10.759147644042969,12.635734558105469,12.343104362487793,9.57535171508789,9.709870338439941,12.569595336914062,12.355887413024902,12.963922500610352,10.949813842773438,11.966733932495117,12.615514755249023,13.138450622558594,12.275333404541016,13.219440460205078,12.31989574432373,12.160449981689453,12.557188034057617,12.105088233947754,10.996682167053223,11.097118377685547,12.50152587890625,11.729860305786133,9.074333190917969,10.838717460632324,12.407896995544434,10.486307144165039,13.131638526916504,12.889549255371094,12.425800323486328,11.791596412658691,12.584457397460938,9.637909889221191,12.318270683288574,13.225953102111816,13.791191101074219,10.274442672729492,13.676323890686035,10.931281089782715,14.494017601013184,10.064900398254395,12.920795440673828,11.843405723571777,9.891942977905273,11.050874710083008,11.260539054870605,12.36363697052002,10.923389434814453,12.935821533203125,11.364681243896484,12.573941230773926,10.97855281829834,11.816873550415039,11.916487693786621,13.592192649841309,12.541654586791992,12.330859184265137,12.914822578430176,13.55208683013916,12.122734069824219,14.642132759094238,11.791476249694824,11.278715133666992,13.6757230758667,9.828176498413086,11.6958646774292,12.56118106842041,11.088314056396484,10.421248435974121,11.153604507446289,13.880023002624512,12.242606163024902,13.280402183532715,11.438232421875,12.856744766235352,12.171623229980469,13.646942138671875,8.872869491577148,12.179302215576172,11.183941841125488,13.047853469848633,14.351018905639648,11.455432891845703,11.737250328063965,12.385884284973145,10.355916023254395,12.673069953918457,12.902497291564941,11.19161319732666,12.572444915771484,12.974329948425293,11.89826774597168,12.428143501281738,11.910385131835938,11.693814277648926,11.054215431213379,13.56718921661377,13.411906242370605,10.083378791809082,9.965200424194336,11.210006713867188,12.447346687316895,12.735180854797363,12.82627010345459,8.879070281982422,10.942899703979492,11.053876876831055,13.271478652954102,12.52558422088623,14.039688110351562,12.695708274841309,13.217219352722168,10.924018859863281,12.666877746582031,12.446202278137207,11.24301815032959,10.556553840637207,9.895024299621582,13.207962036132812,10.674429893493652,10.300735473632812,13.919363021850586,10.819778442382812,12.895362854003906,10.641951560974121,12.555561065673828,12.560113906860352,10.98803997039795,13.43642807006836,13.290969848632812,12.706768035888672,9.741263389587402,11.930825233459473],\"y\":[1.507306456565857,2.4207963943481445,4.316191673278809,0.5102812051773071,4.351354122161865,0.3226845860481262,0.8982223272323608,3.3303349018096924,3.1168205738067627,3.2457163333892822,1.3116782903671265,0.3398812711238861,2.627408266067505,3.5863447189331055,0.4591532349586487,1.8721373081207275,2.59684681892395,2.0959103107452393,3.149441719055176,0.7273111343383789,0.404301255941391,1.1538777351379395,0.47859829664230347,1.1893887519836426,0.8482530117034912,4.480172157287598,0.6037070155143738,3.999026298522949,2.4619247913360596,1.4505866765975952,2.0469107627868652,0.38434460759162903,4.18442964553833,1.5416126251220703,2.1589250564575195,-0.5393324494361877,0.6223673224449158,3.841806650161743,2.211869716644287,2.199165105819702,2.1696276664733887,2.352555751800537,2.728048086166382,4.350765705108643,2.1546812057495117,0.42506060004234314,2.4407355785369873,1.1690542697906494,3.641746997833252,-0.11202618479728699,1.8165446519851685,2.0864055156707764,1.4754703044891357,3.142125129699707,1.5970345735549927,3.721360206604004,3.9236321449279785,2.404432535171509,4.0007243156433105,-0.22485516965389252,2.375478506088257,3.2460124492645264,3.945085048675537,2.4788665771484375,-0.4101865887641907,3.377387285232544,2.1252009868621826,-0.5785242915153503,3.719083547592163,-0.05052642896771431,1.4751540422439575,-0.7022446990013123,1.8287338018417358,0.8024368286132812,1.9997283220291138,1.4606341123580933,2.604973316192627,1.4613151550292969,2.347259521484375,4.493715763092041,2.096546173095703,3.7118804454803467,1.250640630722046,4.5775580406188965,1.7460253238677979,0.34211650490760803,2.7896816730499268,1.718787670135498,2.7310361862182617,1.3947547674179077,2.1304163932800293,3.6700267791748047,2.089962959289551,2.4383544921875,3.5687668323516846,0.9181693196296692,0.09192104637622833,2.192847490310669,2.6576716899871826,3.3588621616363525,2.991515874862671,2.097174644470215,0.9715812802314758,3.851013660430908,4.002720832824707,0.7822956442832947,3.132732391357422,0.6646859049797058,1.5552555322647095,4.182912826538086,0.6196082234382629,2.095290422439575,-0.18723732233047485,-0.16546708345413208,1.3244792222976685,1.4611302614212036,3.9873270988464355,3.0334932804107666,2.8467254638671875,2.8810019493103027,0.8321359157562256,1.8536946773529053,0.2910764217376709,0.042055580765008926,0.5279823541641235,3.661564350128174,-0.07724453508853912,1.9331488609313965,0.3603086471557617,3.272552967071533,1.8510955572128296,1.678562045097351,3.3658502101898193,3.623711347579956,0.8364458680152893,1.8635711669921875,0.683874785900116,0.22363659739494324,3.6560378074645996,-0.0354466550052166,2.7863786220550537,1.860956072807312,4.096649169921875,1.4631794691085815,2.8087878227233887,1.2081469297409058,2.045775890350342,1.1945379972457886,0.21957427263259888,0.3339994251728058,1.9521361589431763,2.6870803833007812,1.30488121509552,1.622704029083252,3.5201311111450195,-0.6378558278083801,3.2248761653900146,0.6754312515258789,2.846132755279541,3.357870101928711,3.1381354331970215,3.9575700759887695,3.349735736846924,2.0364859104156494,1.1525269746780396,2.4633357524871826,1.5299075841903687,1.5564807653427124,1.8753632307052612,-0.004554279148578644,3.2524967193603516,2.307678461074829,0.5151564478874207,2.3165690898895264,0.5424305200576782,1.8063089847564697,-0.18732231855392456,3.3321328163146973,4.82322359085083,3.327812433242798,1.8366180658340454,3.3768088817596436,1.5901246070861816,3.344776153564453,1.4118517637252808,1.7958210706710815,2.6258456707000732,-0.21800321340560913,3.9406492710113525,3.0026063919067383,0.35503560304641724,0.8777638077735901,3.209075689315796,1.7807344198226929,0.30633625388145447,1.7140427827835083,4.0319600105285645,4.727674961090088,2.032911777496338,2.2430472373962402,4.297402858734131,1.2354248762130737,3.343724250793457,4.549727439880371,2.7123260498046875,4.471125602722168,1.5851975679397583,-0.008239027112722397,-0.03579096123576164,0.6962659955024719,1.642042636871338,0.007765602320432663,4.5641021728515625,1.7181318998336792,0.48780158162117004,0.9258548021316528,2.6752874851226807,0.7018755078315735,0.8958325982093811,1.0525304079055786,0.4012320041656494,3.158207654953003,1.0357576608657837,3.606261968612671,0.2348896861076355,1.8230359554290771,1.1030339002609253,3.2201130390167236,-0.3748437166213989,4.405543327331543,1.2899785041809082,2.427333116531372,4.321866512298584,3.178401470184326,0.6197330355644226,2.4651403427124023,1.7135303020477295,1.7823418378829956,-0.32073596119880676,4.584204196929932,1.3968353271484375,2.4077961444854736,4.2302327156066895,-0.4107261598110199,1.9505587816238403,2.854809284210205,4.066312789916992,2.4553749561309814,3.721719264984131,1.065097689628601,0.5373880863189697,3.21931791305542,2.3772096633911133,4.333065509796143,1.168684959411621,1.2294330596923828,0.9634657502174377,2.135991334915161,3.255793571472168,1.47713041305542,1.4053293466567993,4.438722610473633,3.3417510986328125,0.77984219789505,-0.6577997803688049,3.335097312927246,0.9028550982475281,1.784332036972046,2.5739457607269287,1.5630472898483276,4.255022048950195,0.47621768712997437,1.224573016166687,1.8064907789230347,3.777756452560425,0.7913487553596497,2.4245057106018066,1.8494513034820557,1.212245225906372,2.4078309535980225,0.07685303688049316,3.6952083110809326,2.5941503047943115,1.4024100303649902,2.794158935546875,-0.41550371050834656,0.5075252056121826,0.681168794631958,-0.05566178262233734,3.0398170948028564,1.661298394203186,2.1198036670684814,2.772426128387451,0.803010880947113,2.135280132293701,4.008086681365967,3.425002336502075,0.9121075868606567,2.340266704559326,2.2921903133392334,0.7950373888015747,2.8389360904693604,3.4929821491241455,1.3534882068634033,3.183328628540039,0.15177598595619202,3.9541072845458984,3.2621312141418457,3.404259443283081,-0.559249758720398,0.9637823104858398,0.44517797231674194,2.066474199295044,0.656364381313324,1.019322156906128,1.2479918003082275,2.7956390380859375,3.532331943511963,2.1427114009857178,3.68852162361145,1.9364203214645386,0.8520002961158752,4.028737545013428,3.204054117202759,1.8958054780960083,3.148305892944336,-0.886218786239624,-0.6291579604148865,0.633914589881897,2.6274592876434326,-0.4008409082889557,4.128902912139893,-0.1953260898590088,4.234408855438232,4.257920265197754,4.404775619506836,2.095197916030884,0.3217342495918274,0.5947314500808716,2.322157621383667,2.109975576400757,2.7286033630371094,-0.14018361270427704,0.6630464196205139,2.2010860443115234,1.8108470439910889,4.259799957275391,3.8510754108428955,3.0663845539093018,2.656661033630371,2.48760724067688,3.7310667037963867,1.9937796592712402,1.41938054561615,0.45107322931289673,1.5648137331008911,0.12592341005802155,1.5279438495635986,4.089201927185059,3.1535937786102295,1.3887526988983154,0.5217064619064331,1.4126652479171753,2.135779857635498,1.741749882698059,2.457362651824951,1.4273675680160522,1.8354824781417847,0.4084896147251129,2.0181241035461426,1.2267817258834839,3.4286208152770996,0.8427394032478333,0.13242918252944946,2.8376529216766357,1.7971707582473755,0.3357393443584442,4.240313529968262,1.584699273109436,2.9798004627227783,1.5480058193206787,1.2376223802566528,2.065072536468506,2.7111775875091553,3.430250644683838,3.1784985065460205,0.10957737267017365,0.7182363271713257,1.5753724575042725,3.613825798034668,0.644492506980896,1.4867182970046997,3.257819414138794,-0.23451361060142517,1.8473373651504517,3.5510706901550293,2.667003870010376,-0.24549369513988495,1.2712122201919556,3.8507349491119385,1.7514121532440186,3.47365665435791,0.6185303926467896,0.4896754026412964,0.6338416337966919,1.7970584630966187,4.714752674102783,1.9585669040679932,2.050154447555542,2.4020185470581055,1.1077927350997925,4.563567161560059,2.365028142929077,1.183559775352478,1.8685791492462158,0.007654814049601555,1.7238655090332031,0.71434086561203,2.3655507564544678,1.2656222581863403,4.71193265914917,3.3010201454162598,2.1121249198913574,0.881807804107666,3.71816086769104,1.335047960281372,0.8696401119232178,0.8078348636627197,4.2869462966918945,2.5819921493530273,2.3216233253479004,0.4906277656555176,3.9840564727783203,0.5017974376678467,3.305938959121704,2.6380319595336914,4.027563571929932,-0.6155979037284851,0.05338341370224953,4.055395603179932,2.1313276290893555,1.042435884475708,3.3332130908966064,3.71514892578125,4.351900577545166,1.3762791156768799,1.4845874309539795,2.76271390914917,1.207015037536621,0.1028292328119278,2.1501920223236084,-0.03169238939881325,1.1814765930175781,3.1704180240631104,1.8639248609542847,1.8560513257980347,2.1845180988311768,2.6598308086395264,3.6082026958465576,4.112246513366699,3.368616819381714,2.0284316539764404,3.3800017833709717,0.497557133436203,2.4005062580108643,1.1722631454467773,3.6082603931427,2.114776134490967,2.094217300415039,1.133660912513733,1.6386992931365967,0.6620090007781982,4.639134407043457,1.7188031673431396,1.5438989400863647,4.13488245010376,3.4239821434020996,2.109323501586914,3.8099565505981445,-0.459151029586792,2.0242533683776855,3.185093641281128,1.0645835399627686,2.188619375228882,3.042832612991333,1.7089647054672241,2.3003664016723633,0.06863885372877121,2.3870468139648438,2.420334577560425,1.460945725440979,0.7886748909950256,0.5657801628112793,1.2295072078704834,4.129209995269775,3.443634033203125,3.9732279777526855,1.3301258087158203,4.066137313842773,2.4340717792510986,1.55797278881073,1.1940613985061646,2.6406619548797607,-0.6998453140258789,1.918683409690857,3.7310338020324707,0.5560200810432434,1.2003809213638306,1.1483811140060425,1.1745744943618774,1.8612607717514038,3.3504059314727783,0.8137112259864807,0.7124507427215576,3.289857864379883,3.145576238632202,3.347191333770752,1.2169671058654785,0.47211408615112305,2.3507843017578125,3.275209665298462,2.514665365219116,0.5469618439674377,0.7065786719322205,2.3119328022003174,0.6099385023117065,1.8535757064819336,2.4495136737823486,2.088813066482544,0.8910029530525208,3.5951640605926514,3.686134099960327,4.302435874938965,3.964002847671509,0.8079436421394348,1.2681810855865479,1.2933151721954346,-0.14534351229667664,5.0020976066589355,3.9586925506591797,4.059518337249756,0.9012271761894226,3.477648973464966,4.18040657043457,0.48795831203460693,1.0922242403030396,0.7489666938781738,3.7521135807037354,3.6949756145477295,1.1970937252044678,3.5207836627960205,3.760321617126465,2.4009201526641846,4.091020584106445,0.9433523416519165,3.1957523822784424,3.2947237491607666,3.152250051498413,-0.2378460019826889,1.8129217624664307,3.1509947776794434,3.550290822982788,1.1265699863433838,2.999068260192871,3.7556638717651367,1.5922778844833374,1.6005226373672485,1.5406572818756104,-0.6036825180053711,2.701667547225952,3.56705379486084,1.5186060667037964,4.578021049499512,0.01953813061118126,1.088356852531433,0.3284533619880676,3.6783876419067383,0.8866409659385681,2.4810380935668945,0.33492255210876465,3.3736114501953125,-0.405870646238327,2.4493350982666016,4.545297145843506,1.0316461324691772,1.9033293724060059,0.3944714367389679,3.973219633102417,1.7597594261169434,2.2158846855163574,3.185065269470215,1.1302825212478638,3.109597682952881,0.19515058398246765,2.0682809352874756,2.876065492630005,-0.18958845734596252,2.090435266494751,4.313929080963135,1.4099946022033691,1.0712666511535645,3.206789970397949,2.3332583904266357,0.616935133934021,0.045256759971380234,0.3346382975578308,1.9457918405532837,2.1948914527893066,1.207532525062561,1.1902925968170166,3.6382687091827393,2.132603406906128,-0.11839228868484497,2.1287736892700195,1.535379409790039,1.5416932106018066,1.2641938924789429,1.4660247564315796,2.3681626319885254,3.477397918701172,1.4667547941207886,4.119585990905762,0.587958812713623,1.7789968252182007,2.066115140914917,1.200212001800537,2.4594759941101074,1.5469329357147217,3.1930742263793945,0.28759753704071045,2.9408297538757324,3.8315396308898926,0.2728513777256012,0.3454519212245941,1.8637174367904663,3.7971911430358887,4.260958194732666,-0.46089687943458557,3.6099464893341064,3.0379638671875,3.783876895904541,1.911690592765808,-0.21531720459461212,4.40946626663208,2.313239574432373,2.931300640106201,0.062370218336582184,2.3163368701934814,1.6380000114440918,1.7225641012191772,-0.14954404532909393,-0.15820349752902985,0.5775056481361389,2.6082892417907715,2.0907576084136963,2.4120962619781494,0.7721648812294006,2.374150514602661,2.6680495738983154,2.651606321334839,1.8734396696090698,1.6551371812820435,4.269644737243652,2.521723508834839,2.497095823287964,2.0968716144561768,3.759312391281128,1.206268548965454,1.6847782135009766,2.475590944290161,1.4060825109481812,3.887336492538452,2.6748037338256836,3.162585496902466,1.4088122844696045,1.9695817232131958,0.17320869863033295,1.3509252071380615,2.2542996406555176,0.4900657832622528,3.1276168823242188,1.0616177320480347,1.8853658437728882,0.8292788863182068,2.4289257526397705,1.766862154006958,3.4001266956329346,1.4613251686096191,1.471782922744751,3.169278621673584,1.956749439239502,0.0855627954006195,2.9822747707366943,0.9547541737556458,2.0180280208587646,-0.04501548781991005,2.0928401947021484,3.5595529079437256,1.9442869424819946,3.335322380065918,4.311258316040039,1.4239171743392944,0.49572691321372986,0.5796030163764954,-0.39457839727401733,2.5927977561950684,3.885775566101074,1.8760502338409424,3.672818899154663,3.811131477355957,2.2307136058807373,1.2452229261398315,2.8910441398620605,1.6013931035995483,2.911939859390259,3.1904914379119873,0.6303874254226685,4.384883880615234,0.03447128087282181,2.4231209754943848,4.214849948883057,4.0666961669921875,0.5230627059936523,1.6741764545440674,1.9067773818969727,2.3787779808044434,3.8618264198303223,2.158189535140991,0.32392844557762146,1.2169387340545654,0.2558423578739166,0.5831639170646667,3.8040060997009277,1.3385379314422607,0.2092827409505844,0.46686291694641113,2.349745750427246,0.5954152941703796,2.0960755348205566,-0.16126614809036255,-0.07722871005535126,2.7550952434539795,3.611664056777954,2.0955708026885986,3.6054015159606934,1.2939273118972778,1.1260716915130615,2.53818678855896,1.5179544687271118,0.054284702986478806,3.2599973678588867,2.3959834575653076,-0.37340179085731506,3.978621006011963,2.136821985244751,1.6384094953536987,2.3025758266448975,2.7206883430480957,3.687239408493042,1.0926687717437744,3.4023232460021973,2.823194980621338,3.406859874725342,2.9801926612854004,-0.09126182645559311,3.4523773193359375,2.531829595565796,1.7842382192611694,-0.38307350873947144,2.3132567405700684,1.0727715492248535,2.2265701293945312,2.884035587310791,0.8918060064315796,2.2678780555725098,3.153651475906372,1.7186988592147827,0.4921904504299164,2.6329410076141357,3.408735990524292,2.0680081844329834,1.132338285446167,1.1508996486663818,2.939976453781128,0.1860128790140152,1.4579036235809326,-0.2155604511499405,0.8905779123306274,2.728846311569214,3.017880916595459,2.785245656967163,0.96174156665802,1.8349359035491943,3.3027775287628174,0.27016976475715637,2.6169066429138184,2.09425950050354,1.2668203115463257,1.700421690940857,0.632376492023468,1.6025913953781128,1.8996806144714355,1.4422632455825806,0.8881476521492004,2.7764129638671875,3.9408488273620605,0.884432852268219,4.3370232582092285,0.32823315262794495,4.430941581726074,2.468080520629883,1.7292624711990356,0.8392194509506226,3.514183759689331,1.8507956266403198,0.6195991039276123,1.103644609451294,1.363840103149414,3.132805347442627,3.5263781547546387,1.8544836044311523,4.157810211181641,1.3948109149932861,3.0018434524536133,1.0341068506240845,4.0839691162109375,4.423365592956543,3.8808939456939697,4.312247276306152,2.7022247314453125,2.9562642574310303,2.1795754432678223,0.9443203806877136,0.18180550634860992,3.294416666030884,3.0668399333953857,1.4725055694580078,2.145565986633301,3.066848039627075,4.339686393737793,1.2442132234573364,-0.9585033059120178,3.049936294555664,2.498439311981201,1.325722336769104,2.141326665878296,3.1884922981262207,4.342645168304443,0.7013605237007141,3.6197595596313477,2.488260507583618,2.500720739364624,1.4350630044937134,1.7070918083190918,2.853356122970581,3.2090325355529785,2.4210517406463623,1.6098593473434448,3.2205097675323486,1.7933086156845093,1.540576696395874,4.076273441314697,0.8919649124145508,3.616443395614624,4.142409324645996,4.539780139923096,0.8967981338500977,-0.21308939158916473,3.5775766372680664,1.1900606155395508,3.003845453262329,3.6587460041046143,2.6246864795684814,4.147606372833252,4.718569278717041,1.0721962451934814,0.7037444710731506,0.9321343302726746,2.3884334564208984,2.419309616088867,4.287731647491455,0.36042705178260803,1.8216502666473389,-0.40917572379112244,1.1244007349014282,0.5408831238746643,0.7277372479438782,3.796677827835083,2.5551393032073975,2.7562975883483887,1.133357048034668,1.3652997016906738,1.2907882928848267,1.6792508363723755,-0.21056237816810608,-0.2080443650484085,2.374600648880005,2.0530881881713867,3.6230456829071045,0.8871607184410095,2.687720537185669,2.4690792560577393,1.202226161956787,0.3940191864967346,1.757133960723877,1.6551944017410278,2.5789742469787598,3.1955454349517822,2.931424617767334,3.534515380859375,0.5360407829284668,-0.38213884830474854,2.394369125366211,3.4015698432922363,1.4856414794921875,1.0046844482421875,1.2732312679290771,0.24245475232601166,2.760497808456421,1.8938589096069336,2.0846047401428223,2.3730387687683105,3.886180877685547,2.847752332687378,0.5136028528213501,4.742417812347412,2.383831262588501,1.17144775390625,0.8523836135864258,4.428389072418213,2.886436939239502,1.6782050132751465,1.1377261877059937,3.2932605743408203,3.049024820327759,2.691157817840576,1.0521892309188843,1.0721131563186646,3.640789270401001,1.907788634300232,1.8878467082977295,0.14270752668380737,1.8628921508789062,3.7283854484558105,2.8242907524108887,4.02400016784668,1.674302339553833,3.5064127445220947,3.1503617763519287,0.9654422402381897,3.631971836090088,2.7282071113586426,0.7885450720787048,2.0672452449798584,2.948789596557617,3.2313883304595947,4.064459323883057,2.236391305923462,1.638910174369812,0.7208170890808105,1.9330946207046509,2.7430360317230225,1.960645318031311,-0.1620693802833557,2.491590738296509,2.1218008995056152,1.3523684740066528,0.674596905708313,4.787007808685303,0.8735396265983582,3.8860464096069336,2.8781120777130127,-0.42245930433273315,2.501638412475586,2.7132835388183594,2.0535812377929688],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"In this work we explored transfer learning approaches only by using pretrained word embeddings.some of the segmentation methods have their own hyperparameters which are usually obtained for high-resourced languages and might be suboptimal in our case.finally, token-free pretrained models fine-tuned on our data should be investigated.It is costly and difficult to acquire human translations, due to the limited number of speakers and exclusive lrl communities; moreover, the fact that we are not way\\u00faunaiki speakers limited our qualitative assessment.\",\"The dataset used in this work is in italian and there may be language-specific limitations in the model performance.Geppetto is the only candidate for auto-regressive models for the italian language at the time of this research.Therefore, its performance may be limited due to the small number of parameters.We were unable to experiment with it5-large model due to computation power limitations.\",\"Biases may be present in the data annotator as well as in the data the models were pretrained on.Furthermore, we only include english-language data in our benchmark and analysis.Recent work has noted that language models may be susceptible to learning such data biases , thus we request that the users be aware of potential issues in downstream use cases.1, we take measures to ensure a high quality benchmark.There will inevitably be noise in the dataset collection process, either in the acu writing or matching step, and high agreement of annotations does not necessarily coincide with correctness.However, we believe that the steps taken to spot check acu writing and filter workers for acu matching allow us to curate a high-quality benchmark.Furthermore, we encourage the community to analyze and improve rose in the spirit of evolving, living benchmarks.For reference-based evaluation, questions about reference quality arise naturally.We also note that the original pyramid protocol was designed for multi-reference evaluation and weighting of semantic content units, while we do not weight acus during aggregation.As discussed above, we argue that our benchmark and analysis are still valuable given the purpose of studying conditional generation and evaluating automatic metrics for semantic overlap in targeted evaluation.\",\"First, in this work, we assume that clustering in the encoder and the decoder is only related to the source and target languages, respectively.Actually, both parameters in the encoder and the decoder are influenced by source and target languages simultaneously.Therefore, our assumption may lead to a performance drop.moreover, our adapter-families method depends on prior linguistic knowledge.Its actual effectiveness can be affected by the distribution of language families\\u002fgroups in clients.Our methods mainly apply to comparably uniform language distribution.In addition, the effectiveness of our methods on other plms needs to be verified.However, it is easy to transfer our methods to other models so it will not be a challenging problem.\",\"While dims makes the cross-entropy based family competitive with alignment based variants, it still falls behind one some cases.Moreover, dims can improve the performance of models trained on raw data, but the best performance is still achieved when dims is applied on distilled datasets.Therefore, dims still depends on an auto-regressive model for the best translation quality.\",\"The fst does not yet have an extensive coverage of the lushootseed vocabulary, so it does not work on all domains of text.Also, writing an fst takes a lot of time and requires special knowledge of the language.The neural model is limited to nouns only, but it can work on out-of-vocabulary words unlike the fst, however, we have only tested its accuracy using the words that are known to the fst, which means that words that follow very different inflection patterns will, most likely, not be analyzed correctly.Furthermore, the neural model was not trained on derivational morphology, which means that word derivations might also result in erroneous predictions.\",\"There are several limitations to this study that should be considered.First, a key limitation is the lack of a variety of language-specific jad.Here, we have four different languages namely en, da, fr, and de.This means that our analysis is based on a limited subset of languages and may not be representative of jad data outside of these four languages.In turn, the second limitation is that the esco taxonomy used as pre-training data only covers europe and the datasets used in this work also covers mostly europe.The results may not be generalizable to other regions.However, we see a slight improvement in the bhola dataset, the data of which comes from singapore, which hints that it could generalize to other cultures.The esco relation prediction task aims for learning the relations between elements of the esco taxonomy.We acknowledge that we do not evaluate the effectiveness of the pre-training objective in relation-centered tasks.Unfortunately, to the best of our knowledge, there is no job-related dataset containing relations between skill\\u002foccupation concepts to benchmark our model on.finally, we did not conduct an ablation study on the erp pre-training objective, i.as the accuracy of the objective is 60%, we are unable to determine which sampling method is detrimental to this accuracy.However, we suspect that the linked sampling approach might be the hardest to predict correctly.For example, many occupations have a lot of necessary and optional skills, thus it is harder to determine if some skill truly belongs to a specific occupation.Nevertheless, we see that adding the erp objective improves over regular mlm domain-adaptive pre-training.Despite these limitations, we believe that this study provides valuable resources and insights into the use of escoxlm-r for analyzing jad and suggests directions for future research.Future studies could address the limitations of this study by using a larger, more diverse datasets and by conducting ablation studies on the language model to better understand which parts contribute to the results.\",\"Although our proposed aligner has surpassed the existing lm-based alignment extraction methods in most of the datasets, it could not make any improvement for the en-fr language pair, as shown in table 1.this suggests that our proposed method might be only beneficial for more distant languages.On the other hand, for similar languages, it not only cannot add any information to the similarity matrix, but also its estimation for the alignment probabilities might add noise to the alignment extraction method.another limitation of our method, as well as other lm-based aligners, is that they first extract subword-level alignments, and then heuristically map them to word-level.By observing the aligner outputs, we realize that many errors occur when the pre-trained lm can not efficiently split words into meaningful subwords.This happens more often for low-resource languages or far languages from english.Thus, achieving better subword tokenization in pre-trained lms or applicable methods to convert subword-level representations into word-level could help improve the quality of lm-based aligners.\",\"While we do include multilingual datasets in our experiments, our error analysis is limited to languages of the indo-european family, specifically english, french, and spanish, as these are the languages covered by our datasets which we can confidently analyze.In addition, it is possible to question some of the assumptions made in our theory, which should be kept in mind when considering our work.For example, we assume that, for each content word token in a discourse, there exists a single concept which that word is intended by the sender to express, regardless of whether it appears unambiguous to the receiver.However, unlike in mathematics, theoretical assumptions may not always hold in practice; for example, puns often exploit multiple meanings of a word for humorous effect.While such cases are not frequently considered in lexical semantics, we can expect exceptions to almost any assumption or\",\"One of the limitations of our dataset is the lower number of wordforms belonging to a closeclass partofspeech as we chiefly focus on nouns, verbs and ad jectives.On the other hand, we only include in flectional morphology without paradigms of word formation.Furthermore, we only address the mor phology of the standard variety of central kurdish, i.we plan to extend our work to include other varieties of central kurdish along with derivational morphology.Given that central kurdish lacks a treebank, it will be com pelling to bridge central kurdish morphology and syntax as well.Another limitation of the current work is due to the unimorph schema.Using the lgspec tag is not recommended for features that are found across languages but for those that are limited to specific languages.\\u2019s hierarchical model, is needed for languages with rich morphology like kurdish.\",\"While we observe marked improvements in the proposed multilingual language transfer with adapters, we recognize that there are several limitations still in the experiments.The first limitation is that the entity translation remains difficult, which is especially severe in the generated responses in the setting.on the other hand, we think that while knowledge of language is one aspect for the transfer, the structural information of the semantic representation is also another important aspect \\u2013 models need to acquire the important semantic structural information on top of the language-specific syntactic information.We think that this would further improve the resulting performance.\",\"For the dataset, although we adopt several methods to assure a high quality of the dataset, mislabeled data still exist due to the subjectivity of the annotators.directly translating safeconv to other languages with translation tools may induce erroneous labels due to syntactic and cultural differences between languages.We call for endeavors to fix it, such as annotating similar datasets in other languages or improving translation strategies.however, there are large chatbots that we do not include in the evaluation due to the limitation of computing resources, such as eva-xlarge with up to 2.8b parameters, on which the detoxifying results will lead to more comprehensive results.Secondly, as shown in table 8, the overall contextual coherence and informativeness of the responses from current state-of-the-art chatbots in chinese are still not satisfying.\",\"Our methodology is currently tested with only english.We conjecture that the methodology should be applicable to other languages, but may be limited by the capacity of llms in those specific languages.It is possible that value-aligned knowledge distillation may be more difficult with languages from countries and regions that do not have a complete set of human value definitions.Thus, exploring the value-aligned task in different languages other than english is a promising research direction.Our main experimental results are based on a 175b parameter model, which requires large gpu resources or access through an api.This may hinder other researchers from reproducing experimental results.Additionally, we explored different sizes of llm including 1b and 6b models, which do not require large gpu resources, and showed they can achieve comparable results.We hope they can be possible alternative options for researchers who may not have access to 100b+ models.Although sexism is a suitable case study for us to investigate the feasibility of the value alignment task as we have shown throughout this work, it is still one domain.Further expansion to different domains or value-aligned classification tasks such as the detection of racism, toxicity, other than sexism, are needed.\",\".Although we strive to include tasks in all arabic varieties, available downstream datasets from certain countries such as mauritania and djibouti are almost nonexistent and so are not covered in orca.In addition, there is a need in the community to create more datasets for several arabic dialects.This includes, for example, dialects such as iraqi, sudanese, and yemeni.With the introduction of more datasets for such dialects, orca\\u2019s coverage can be further extended.1 shows, orca datasets are quite diverse from a geographical perspective.Although orca currently covers both dialectal arabic and msa, it does not pay as much attention to the classical variety of arabic due to historical reasons.That is, the community did not invest as much efforts creating and releasing datasets involving ca.However, as more unlabeled datasets become available and with an undergoing positive change in the culture around data sharing, this is likely to change in the near future.Although benchmarks in general are useful in encouraging standardize evaluations and meaningful comparisons, and can help motivate progress within the community, they also run the risk of contributing to a culture of leaderboard chasing that is not necessarily useful.That is, although scientific research advances due to competition, it also thrives through partnerships and collaborations that bring the best from diverse groups.It is in the context of this collaborative culture that we hope orca will be perceived and used.\",\"We identify the following limitations for our work: 1.Due to limited access to a wide network of native speakers from the majority of languages, we were able to manually inspect only a subset of languages present in our pretraining data.Specifically, we could only manually evaluate afrikaans, yor\\u00f9b\\u00e1, igbo, hausa, luganda, kinyarwanda, chichewa, shona, somali, swahili, xhosa, bemba, and zulu.we believe automatic analyses are not sufficient before development of models that get deployed in particular applications.another limitation is related to our inability to perform extensive analysis of biases and hateful speech present in our pretraining data.Again, this is due to relatively restricted access to native speakers to perform this analysis.As a result, we cannot fully ensure that our models is free from biases and socially undesirable effects.Therefore, it is important that these models be used with care and caution, and be analyzed for biases and socially undesirable effects before use.additionally, due to unavailability of sufficient computing resources, we were unable to evaluate large language models such as bloom, even though it covers 22 african languages.Finally, even though afronlu has diverse tasks at the word and sentence level, these tasks only cover few african languages.We therefore encourage the creation of more datasets for downstream nlu tasks in more african languages.We believe broader benchmarks will continue to be important for future progress in african nlp.\",\"Our framework currently only supports english, thus not allowing us to complete a cross-lingual study.our method is evaluated on a 16 dataset stance benchmark, where some domains bear similarities.The benchmark should be extended and analyzed further to find independent datasets with varying domains and minimal similarities, allowing for a more granular out-ofdomain evaluation.\",\"Our dataset and model only covers 201 languages: the ones we were able to test with the flores-200 evaluation benchmark.In addition, because our test set consists of sentences from a single domain , performance on this test set may not reflect how well our classifier works in other domains.finally, most of the data was not audited by native speakers as would be ideal.Future versions of this dataset should have more languages verified by native speakers, with a focus on the least resourced languages.\",\".First, the system\\u2019s performance is constrained by the use of automated systems to produce pseudo-parallel data.Errors in translation and morpheme labeling on the high-resource side propagate to the output and cause mistakes in target side labeling.We have not yet performed the extensive error analyses needed to understand how much error propagation might be affecting the system.Second, we have not yet tested the system in an actual documentation project.When working on nlp with endangered and\\u002for indigenous languages in mind, there is a clear risk of perpetuating existing oppression.We hope to avoid some of these harms by using data from a wide range of non-threatened languages first, waiting to involve language community members and documentary linguists until we have a system with good enough results that we expect it could actually be helpful in real world contexts.We have already developed collaborations with several speakers of endangered languages and linguists working on documentation projects, and we look forward to continuing this work with their guidance and involvement.\",\"Limitation by collecting an addendum to big-c using images taken locally in zambia.In addition, we plan to further expand to other zambian languages such as tonga, tumbuka, chewa, or lozi, by translating the existing dataset and by direct data collection.Further down the roan we plan to study the dialectal varieties of bemba and the other languages, by collecting contrastive datasets from different regions of the country.7note that zambia is a land-locked country.Limitations we observe the following limitations with the dataset: \\u2022 language diversity: in terms of number of languages, the presented dataset only covers two languages; bemba and english.\\u2022 image diversity all the images used in this dataset were obtained from flickr30k image dataset.Therefore, in terms image composition, our dataset is limited to the image diversity in the flickr30k dataset.It mostly lacks images that could be considered as \\\"culturally relevant\\\" ones for the zambian or generally sub-saharan african context.\",\"Our work sheds light on understanding the training dynamics of cross-lingual transfer learning of multilingual lms.In our work, we selected to use english as the source of cross-lingual transfer following previous work.We acknowledge that using other languages as the source language can provide benefits depending on the task.Our work does not focus on choosing source language to maximize downstream performance but instead focuses on the difference between classification tasks and generation tasks in cross-lingual transfer.Secondly, we acknowledge that some of the datasets used in our work are created by machine translation and human annotation.Previous studies have pointed out that translationese in datasets affects cross-lingual transfer performance.We believe that translationese in datasets also have impact on xlrs.\",\".4, we already discussed the limitations and challenges of the model proposed.In the following, we concentrate on the limitations that refer to the scope of this work.we experimented with a limited number of languages and datasets.We do not have experimental evidence that our method can work for other languages, including languages with a richer morphology.Still, our system has been built without any language-specific constraints or resources, other than the t5 checkpoints and the manually annotated training set.Our method can be applied to any other language for which these resources are available, or by applying crosslingual techniques and resources to transfer to other languages zero-shot.In those cases, the expected quality is lower, but the efficiency advantage of diable remains.this is due to the restriction on our computational budget to be both economically- and environmentally-friendly, which made it infeasible to conduct thorough experiments using larger-scale language models.However, we re-emphasise that diable allows one to easily plug and play arbitrary language models and the efficiency advantage of diable remains.we experimented with three different versions of the multi- woz dataset.Although this is the current benchmark for dst accepted by the community, and we followed the standard evaluation methodology and metrics, we are aware that the results presented might not be directly generalisable to other datasets or real-world scenarios with a considerable data shift with respect to multiwoz.Additionally, multiwoz has a certain level of noise and this can have an impact on the evaluation and the generalisation capabilities of the model trained.\",\"First of all, our study is limited to languages that use the latin script.Still, the 4 languages are from different language families and are typologically diverse.as mentioned in 3, in order to carry out the experiments the languages involved were required to have enough monolingual data to train lms, as well as available evaluation datasets for nlu tasks.The source of the pre-training corpora for swahili and finnish is not completely comparable with the corpora used for basque and spanish , due to the unavailability of a large curated corpus for swahili, and the lack of big news corpora for finnish with an open license that allowed us to share freely the pre-training data.16 our study is limited to 3 language model sizes and 3 pre-training corpora sizes.Including other model sizes like a bert-large or a model between 51m and 16m , and adding more pre-training corpora sizes were out of the scope of this work.In addition, we use the default hyperparameters that are commonly used for bert-base for the pre-training and fine-tuning of the bert51m and bert16m models without any hyperparameter tuning.\",\"The annotation scheme proposed in this work is designed to focus on non-identidy coreference, cut, and is not able to handle some complex linguistics phenomena.That includes com- plex temporal ordering, vp or np ellipsis under conjunction and\\u002for disjunction, event negation.As a result, during data selection process, we had to look for those linguistic features and excluded documents with them from the data set.Specifically, to limit the scope of the research, we intentionally limited our analysis to data that: \\u2022 is temporally linear \\u2022 has a single terminal state \\u2022 has a high density of object transformations referred to explicitly throughout the text we chose to work within the cooking recipe domain because it easily satisfies criteria.However, procedural text in general satisfies these three conditions, and our current model is therefore compatible with a broader range of domains than strictly recipes.during the manual curation of 100-document subset, we did not encounter any annotation of nominal events, and therefore this work ipso facto involves only events extracted from verbs.Although event recognition is not the primary research focus of this work, being able to additionally identify different types of lexical trigger of events is indeed important when considering broader domains.\",\"In this work, we focused our exploration of lsls on the encoder.Although we ran some initial explorations on the decoder side, further investigation is needed.Another venue for research is how lsls affect language expansion.Since our approach tries to limit the language-specific weights to just a few layers, in theory, it should be possible to add new languages by only expanding and training the lsls.However, blindly doing so might not work well and the interactions between languages from different families needs further studying.Lastly, it is unclear whether our argmax approach to selecting where to place lsls is optimal, how dataset dependent it is, and if there exist alternative approaches that can lead to better results.The fact that it does not take model complexity into account can be a disadvantage in practice.\",\"One limitation of our approach is that in-sample curriculum learning methods always incur extra overhead during training compared with the vanilla model shown in table 7.nevertheless, the inference time of different approaches is the same as the vanilla model.In a word, it\\u2019s worthwhile because icl-sc can perform significantly better than both baselines without additional computational requirements during inference in real applications; icl-sc doesn\\u2019t rely on task-specific expertise and has strong generalization ability.Due to the limited computational resources, we were unable to do experiments on machine translation., all of their machine translation experiments were done on 32g nvidia v100 gpus which are much more powerful than a single rtx 3090.even for the low resource setting with around 133k to 612k training samples, they used dynamic batching with 4096 maximum tokens and trained for 60 epochs.This will either lead to an out-of-memory error or take us several weeks or even months to get the results of a single run on our machine.Instead, we tried our best to cover a range of representative natural language generation tasks and corresponding datasets with different characteristics, such as sizes and output lengths.\",\"Softmv is specifically designed for cross-lingual natural language inference.We believe that some of the ideas in our paper can be used in other tasks of xlu, which remains to be further investigated by subsequent research.In addition, we conduct experiments on the xnli dataset which consists of 15 languages.Softmv outperforms the baseline methods under the cross-lingual transfer settings.\",\"In our research, we have focused on one language pair, english\\u2194hokkien, and experimenting in both directions.our approach leverages parallel speech-to-text data between the unwritten language and a linguistically similar written language.There remains a question of whether there are unwritten languages without similar written languages.\",\"The dataset used in this work is in italian and the plms are pre-trained for the italian language.The performance of the models and the results may be influenced by language-specific properties.To reduce the ecs sparsity and, therefore, better modelling the inter-dependency between ec and valence prediction tasks, particularly in the experiments ec\\u2192 val., a larger dataset with more narratives per narrator is needed.\",\"Aside from the reverse models used in backtranslation , we only studied translation of language pairs into english.Using data augmentation techniques like back-translation where english is not the target language, or is neither the source or target language is certainly worthy of study, but was out of scope in the present work.We did however, include many source languages that are typologically different from english.In order to study the effectiveness of bt in a large number of languages we relied on extant multilingual datasets, namely flores101 and tico19.The direction of human translation when building these datasets was from english into another language.We did not run repeated trials on our experiments.Many models required training for a couple of gpu-weeks on v100s, and additional trials would have added significant computational expense.\",\"Our system has been trained on everyday conversations from spanish-english bilinguals and may not be applicable to other domains.Additionally, the accuracy of the classifier varies depending on the label type.We use human-created transcripts, so results may not apply for automatic transcripts.There is a risk that incorrect conclusions can be drawn if the system does not meet the performance requirements.\",\".we rely on them as they are validated by dialect speakers and have been shown to be predictive of performance on gold dialect data.However, they were designed as stress tests of robustness which isolates morphology and syntax.We are therefore unsure how tada performs when it faces the topical and register shifts which often are associated with naturally occurring dialects.These limitations are similar to localization issues in translated benchmarks.In this work, we evaluate tada on only encoder-only llms.Increasingly, both encoderdecoder and decoder-only models are seeing widescale use due to their flexibility.\",\"One of the limitations is the selection of hyperparameters.At present, we determine the optimal hyperparameters based on the performance of the selection methods on an existing bilingual dataset.For example, to identify the appropriate utterances to be translated from english to german, we would adjust the hyperparameters based on the performance of the methods on existing datasets in english and thai.However, this approach may not always be feasible as such a dataset is not always available, and different languages possess distinct characteristics.As a result, the process of tuning hyperparameters on english-thai datasets may not guarantee optimal performance on english-german datasets.As a future direction, we intend to investigate and develop more effective methods for hyperparameter tuning to address this limitation.\",\"Although these experiments were performed on only one dataset, it is indeed large with data from 82 participants.That said, it will be nice to perform experiments with more listening datasets.We experiment with a linear encoder \\u2013 ridge regression.This work was done on data related to english stories only.Several other languages belong to the same language family as english.While we can expect the insights and learnings to hold across languages in the same language family as english, empirical validation needs to be done.For languages in other language families, syntactic structure may be very different from english.Hence, more work needs to be done to check which of these insights hold for datasets in other language families.This work was conducted on a dataset where the participants were involved in the listening task.However, the stimuli was represented in the text form.We believe that an audio form of the stimuli can lead to improved insights.Thus, more work needs to be done to design representations for auditory stimuli.\",\"Although we demonstrated that the proposed metric peculiarity is useful for selecting candidates for few-shot cross-lingual transfer, our current work has the following limitations.Lack of evaluations to argue the usefulness of peculiarity.We demonstrated that peculiarity selects candidates to efficiently enhance few-shot cross-lingual performance in several tasks and languages.In addition, peculiarity is robust for hyperparameter k.however, further verification is required to evaluate the usefulness of peculiarity.In this study, we only used xlm-r as the mmlm in the experiments, because previous works have demonstrated that mbert and xlm-r show the same trend and xlm-r achieves better zeroshot and few-shot cross-lingual performance.However, it is not obvious that peculiarity will work well in mbert.proposed xglm, a pre-trained multilingual causal language model, that demonstrates strong multilingual capabilities.We would like to experiment using these pre-trained multilingual models to show the usefulness of peculiarity regardless of models.We fine-tuned the mmlm using a standard training objective, predicting true labels or tags for inputs.On the contrary, zhao and sch\\u00fctze revealed that fine-tuning in a prompting format encourages better zero-shot and few-shot crosslingual transfer than the standard fine-tuning.It is worthwhile to examine few-shot cross-lingual transfer performance when fine-tuning the mmlm with high peculiarity examples in a prompting format because it may be possible to achieve higher accuracy in the target languages with a smaller amount of examples.We experimented using english as the source language.However, if possible, it is better to use a language that is linguistically close to the target language as the source language.In our experiments, we did not show that peculiarity works well regardless of source languages.Therefore, verifying this aspect is also a remaining challenge.in this study, we defined annotation cost in terms of the number of candidates following previous studies.However, a small number of candidates does not necessarily mean less work for annotators.If a candidate length is long or hard, it is considered to take longer to understand.On the other hand, if the candidate length is short or easy, annotation time per candidate will be shorter, and the annotators can annotate more candidates in the same time.Therefore, we should evaluate candidate selection methods based on total time required for annotation.In addition, aligning the cross-lingual representations between source and target languages using bilingual data is one approach to enhance accuracy for the target languages.To align the representations, we should create bilingual data through a human or automatic translator.Verification whether labeling or translating is less labor intensive and further boosting performance is one of the future goals.Developing a better peculiarity-based candidate selection method.In this study, we used the bos hidden states to measure peculiarity; in other words, it measures example-level peculiarity.In classification tasks, using example-level peculiarity to select candidates is intuitive because we predict labels based on the bos hidden states.On the other hand, in the sequence-tagging tasks, we predict token tags based on hidden states of each token.In addition, we consider that it is necessary to fine-tune ms with peculiar tokens, tokens that are not covered by the source language, to ensure that the model predicts tags of these tokens correctly.Therefore, we will attempt to select candidates that contain peculiar tokens by using token-level peculiarity and conduct few-shot cross-lingual transfer in the sequence-tagging tasks.We observed that peculiarity selects more redundant candidates compared to the km-based methods and argued that this aspect is the reason that peculiarity does not work in the \\u201cpoor\\u201d group.We consider the possibility of other reasons for this behavior.Several studies have suggested that if only a small amount of examples can be used for training, it is important to use not only hard examples but also some easy examples for training in order to improve model performance.In terms of few-shot crosslingual transfer using peculiarity, we should finetune ms with the both highest and lowest peculiarity examples.In addition, using typical examples selected by km instead of the lowest peculiarity examples is one of the approaches.\",\"While we carried out our research in four language pairs , we recognise that these are mainly european languages and each pair is from or into english.who used a different domain suggest so.\",\"Other ways of reducing the amount of required supervision could be attempted, but we do not expect that these would change the outcomes significantly.Self-supervised learning via masking \\u002f denoising objectives, either in the form of an auxiliary task or via the use of pretrained models, is one such approach.iterative backtranslation might offer an additional boost for data-scarce settings, but is very computationally intensive, complex, and any gains would almost certainly apply to models trained with the addition of seed data too.The seed datasets that we release bring about large translation performance gains for a number of low-resource languages.We note that, due to budgetary and complexity constraints, the source data we used was sourced from english wikipedia only.This is likely to have two effects.First, translating english-original data leads to so-called translationese effects one the low-resource side , leading to decreased effectiveness for directions that target low-resource languages.Second, the data is unlikely to adequately cover diverse content from multiple cultures.An interesting avenue for future research would therefore involve studying the effects of seed parallel data that is originally translated from low-resource languages.\",\"Our work provides an effective solution to augment st when source transcripts are unavailable, which could benefit many unwritten languages.However, limited by the publicly available st datasets, we use english as an unwritten language for experiments, which may slightly differ from realworld unwritten languages.Since we never use transcripts in our approach, we believe our work can shed some light on st for real-world unwritten languages.\",\"One of the limitations of the current study is the lack of annotated data for all languages.This is also the case of machine translation for which data could only be found for kashmiri, sorani and sindhi, while other languages do not have much parallel data yet.On the other hand, the notion of noisy data is limited to the replacement of the missing characters in a script when compared to another one, i.as an ablation study, injecting other types of noise, beyond those discussed in this paper, may improve the performance of the models to tackle not only script normalization but several related tasks such as spelling error correction and may also increase the robustness of the models for morphologically rich languages or languages with versatile word boundaries using zwnj.Although we did our best to filter out code-switched data in the corpora, our datasets may contain data in other languages.we also suggest evaluating the impact of script normalization on more downstream tasks, especially transliteration and tokenization.\",\"Although peit is an end-to-end approach to image translation, in the current form, it needs to be pre-trained in two stages with mt and synthesized data and fine-tuned on the curated image translation data.The training procedure is longer than the standard mt task due to the lack of training data and the cross-modality challenge.For the created ecoit dataset, we used online mt to automatically generate translations and then manually post-edited translations via crowd-sourcing.This significantly reduces the cost of building a large-scale image translation dataset from scratch but may introduce translation noise and \\u201cmachine translationese\\u201d in comparison to professional human translation.\",\"Although we have shown the robustness of multisource nmt to transcription errors in a fullsentence and simultaneous settings, our work has the following limitations: \\u2022 our work does not address the case where the additional source, typically interpreted, is available after a delay.A delayed source may reduce the gains seen by multi-sourcing.\\u2022 we have only focused on the local agreement approach for simultaneous translation and exploration of other simultaneous approaches such as wait-k remains.\\u2022 human evaluation of translations is pending.\\u2022 evaluation on other language pairs is pending.\",\"The data that we analyzed are limited to only one english-german language pair, 5 sst systems from iwslt 2022, and three domains.All the systems were trained in the standard supervised fashion on parallel texts.They do not aim to mimic interpretation with shortening, summarization or redundancy reduction, and they do not use document context.The used mt metrics are good for evaluating individual sentence translations and that is an important, but not the only subtask of sst.We assume that some future systems created with a different approach may show divergence of cr and the offline mt metrics.Furthermore, we used only one example of human interpreting.A precise in-depth study of human interpretations is needed to re-assess the recommendation of translation or interpreting as reference in sst.\",\"Of glosses and common datasets, as well as a standardized evaluation method.In order to make future research on gloss translation more meaningful, we make practical recommendations for the field.We urge researchers to spell out limitations of gloss translation approaches, e.finally, we also caution that researchers should consider whether gloss translation is worthwhile, and if time and effort would be better spent on basic linguistic tools , creating training corpora or translation methods that do not rely on glosses.Limitations our approach to surveying the research literature has limitations.Firstly, some characterizations of the published works we survey are subjective.For example, it is somewhat subjective whether a paper \\u201cincludes an adequate\",\"Our work is focused on just a single case study of language identification of romanized text.another limitation was our choice to focus on already existing pre-trained models, rather than directly controlling the training data that is input to each model.\",\".First, this study is limited to englishlanguage tasks, due to english being the common language of the annotators.further, the datasets and models used may contain biases reflecting the culture of the english-speaking population, as well as biases relating to gender, race, age, and other socioeconomic factors.we rewrite the original natural language definitions into triplets after extracting key information in it and observe improved performance.However, a complementary perspective is to write such a triplet from scratch, by filling in the blanks in triplet templates and seeing whether the improvements still hold.This directly reflects whether such an organizing method works.Our approach serves as a starting point to demonstrate the effectiveness of using a structured and condensed definition.Third, larger language models can be tested.The largest model we adopt is a t5 model with 3b parameters.As we observe variant behavior as model size grows, later work can further extend our analysis to larger models.Also, new emergent ability of lms might be discovered with larger models, like mathematical reasoning with larger models following instructions.That is beyond the scope of this paper.Last, some observations cannot be easily explained in this paper.For example, we saw that removing label information for classification tasks during training eventually also affects the model performance on generation tasks, which can be counter-intuitive and requires further exploration.Later work can pick a few points in the paper and provide deeper analysis on them.\",\"Since this is one of the first studies on understanding the effects of continued finetuning of multilingual models, the focus of this paper was to lay the groundwork by establishing the experimental setting on a set of representative nlp tasks and languages.The resulting set of languages chosen in our setup for evaluation , although diverse, are still relatively higher resource.\",\".This makes it difficult to scale\\u002fslower to train with translation models of large size.secondly, even though our method gives a significant boost to translation quality in the early latencies, it relies on the mma policy that has some limitations in terms of latency because of a suboptimal decision making using multiple heads.While our policy shows improvement, it could be further optimized, for instance, in following reference alignments more closely which would have a positive effect on latency.\",\"Although form similarity is demonstrably responsible for slower translation processing, we are unable to ascertain if it is the primary reason.The work also reveals one shortcoming of alignment distributions \\u2014 the measure tends to be biased towards translations with similar forms and does not always make accurate predictions about cognates.\",\"Lavs is proposed to overcome the off-target problem among languages that share alphabets because those languages tend to have more sharing tokens after the sub-word tokenization process.As for language pair that does not have shared tokens, lavs might not have a direct influence on the zero-shot translation though it can also increase the overall performance for those languages, which might need further exploration.\",\"The main limitation of this work is our use of automatic metrics rather than human evaluation.First, the score distribution produced by a metric is not guaranteed to be similar to one produced by human annotators, which could influence results.Secondly, the metrics we examined do not incorporate context.Motivated by evidence that document-level information is becoming necessary to distinguish between human translations and high quality machine translation , recent wmt evaluations have incorporated context.Since the human annotations are influenced by the context in which they appear and the automatic metrics are not , additional study may be necessary to answer questions such as whether additional preceding source context should be displayed to annotators ), to determine how much additional time reading this context would take , or to determine whether human annotator behavior may differ based on where in a document the snippet comes from.We also do not directly address issues such as the best interfaces for human annotation; a problem that is mostly orthogonal to the question of what data should be annotated.In this work, we also follow the approach in the wmt metrics shared task of treating the scores assigned to systems as full rankings of systems, rather than as clusters of systems.In practice, this may mean that statistically insignificant differences between systems are considered on par with statistically significant ones when we examine reorderings that occur based on different sampling procedures.While this is a major concern in human annotation , it is less of a concern in this setting where the annotation is guaranteed to be consistent.\",\"In this work, we limit our experiments to the most commonly used document-level system architec- 6the precision and recall are roughly the same for the f1 scores reported in table 4.other approaches exist, which might exhibit a different behavior in decoding.Two out of the three document-level translation tasks we use in this work are low resource with less than 500k sentence-pairs as training data.We chose these tasks due to computational limitations and to be better comparable to other works, but higher resource scenarios are more realistic for actual applications.We limit the analysis of pronoun translation to the english-german language pair.Also, there are other aspects of documentlevel nmt, like consistent translation of entities, which we did not consider in our analysis.\",\"Limitation indicates that the transformer model\\u2019s performance is highly dependent on the availability of extensive parallel corpora for effective machine translation.On the other hand, the transfer learning approach showed more promising results for low-resource indigenous languages.We observed that models pretrained on high-resource languages, such as spanish, and fine-tuned on the indigenous languages improved translation quality.However, even with transfer learning, the performance was not satisfactory, and there were errors that persisted across all three approaches.The general error that all three approaches failed to address adequately was the translation of domain-specific and culturally specific terms in mazatec and mixtec.These languages have unique vocabulary and cultural nuances that require a deeper understanding and context to ensure accurate translation.The limited availability of domain-specific parallel corpora for these languages hampered the models\\u2019 ability to capture and translate such terms effectively.\",\"Potential limitations to our work can mainly be attributed to two factors: 1) the fact that we run our experiments using the icelandic language, and 2) inherent biases in the corpora we use.as such, it is both germanic and indo-european.While we are fairly confident that our results hold for these languages, different results may hold for other languages, particularly those not using latin script or those using logograms, such as chinese characters.The curated datasets we use only represent a fairly small proportion of all possible demographics and users of the icelandic language.In particular, annotations are performed by a handful of university students, bringing in their biases to the annotated data.Even so, the data should serve well to compare the relative differences.The resources we use to develop the models consist of a few high-performing gpus.While these are powerful, this is a relatively low requirement compared to many industry or academic use cases.Finally, it is worth re-iterating that the byt5 model we use is slow compared to subword-based models for texts of similar length.Inference in our setting was around 2.3x slower on average than for mt5.As such, production use of these methods may be better suited to offline processing, particularly for longer documents.\",\"The coco-crola benchmark generating procedure is intended to yield multilingual evaluations that can be scaled to even larger sets of concepts and languages without experienced annotators.In the interests of both concept and language quantity scale, we opted for an automated procedure which leverages machine translation systems, which can introduce translation errors.Furthermore, variation in the nuance or normative meaning of concepts, particularly culturally contested ones such as \\u201cface,\\u201d \\u201cperson,\\u201d or \\u201cman\\u201d will inevitably drive some variance in expected outputs by users across language communities.This cultural variation will place an unavoidable upper bound on the performance of inherently cross-cultural benchmarks such as coco-crola.Additionally, typological variation between languages can introduce complications in applying our framework.For example, while simple template filling for prompting is straightforward in chinese, which requires no word-dependent articles, in english phonological properties of the word govern the preceding article, and in spanish and german grammatical gender do the same.Hebrew has gendered nouns, adjectives, and verbs but not articles, on the other hand.while doing so aids in the scalability of the approach, using clip as a feature extractor for computing the metrics, particularly correctness xc and wc, potentially introduces biases due to the english-primary data that clip is pretrained on.\",\"The scores reported in the si test were lower than those in the offline test.Reporting results on other si data would support seeing the effectiveness of our method.To our knowledge, this is the first work to use si data as speech translation data.There are no other language pairs si data than englishjapanese pairs those source speech and target text aligned.\",\"Mt5, compared with previous hebrew lms, is bigger, pretrained on more multiligual data, and learning to segment and tag in an end-to-end manner.While larger mt5 models perform better than available lms, they require more powerful hardware accelerators and take longer to train and infer.However, this is a reasonable trade-off from pretraining designated monolingual models from scratch, a more expensive task by itself.Additionally, the inclusion of data from 101 languages in the training of mt5 may have negatively impacted its performance on hebrew, as some of the data may not have been relevant or beneficial to this particular language.an inherent risk in sequence-to-sequence models is that they can generate inconsistent text with respect to the input text.While potentially sensitive in different applications, a number of evaluation frameworks have been suggested to reduce the number of such \\u201challucinations\\\".Another limitation of our evaluation framework is that, for lack of available datasets, we did not evaluate mt5 on purely generative tasks such as summarization and paraphrasing.\",\"The approach of language model integration for neural machine translation is analyzed and compared to the de-facto standard method of backtranslation.Due to constrained resources, this work has several limitations.We focus on translation of text in a single domain, namely news-articles.for the back-translation experiments, we use beam search to create the synthetic data, other methods like sampling were not considered.When combining the synthetic and real parallel data, there are additional methods like tagging and block-wise batching, which we did not utilize in this work.Finally, we compare against the most commonly used lm fusion approach, i.there exist other lm fusion techniques which might exhibit different behaviour when used in combination with ilm neutralization.\",\"While we have done our best to create high-quality evaluation data, there are limitations that should be kept in mind when using these datasets.It is known that creating translations by post-editing may bias data towards the output of the mt systems used for initial translations; however, many transcription and translation vendors now exclusively use postediting rather than translation from scratch and so direct translation may not be an option in all cases.This could influence metrics toward similar mt systems.The presented evaluation sets are moderately sized compared to datasets in other domains with plentiful mined data, and may be best used in conjunction by reporting on both the development and evaluation sets for statistical significance.The evaluation sets also have a necessarily limited set of speakers which may not be fully representative.Systems which tune to the development set run the risk of over-fitting to specific speakers or content.We do not perform a comparison to human evaluation here, but refer interested readers to the iwslt\\u201923 evaluation campaign findings paper which runs this comparison for a variety of systems with the acl 60\\u002f60 data.\",\"Our machine translation experiments revealed that optimal generalization performance is obtained with small interval values.However, r-dangle with small intervals still runs much slower than an equivalent transformer model.Despite our modifications, large r-dangle models with small intervals on large datasets remain computationally expensive.In this paper, we only explored a sim- ple periodic re-encoding strategy.However, more complex and flexible ways of re-encoding could be used to further improve computational efficiency.For instance, we could adopt a dynamic strategy which learns when re-encoding is necessary.\",\"Lexical variation is not our focus because it is not well-described by systematic, scalable, and generalizable rules.One can derive lexical distributions from data, but many low-resource dialects lack corpora on which to base these insights.This is an important problem for future research.Multi-value\\u2019s strength is its extensive coverage of english morphosyntacic patterns that have been documented in ewave by over 80 linguists.Such comprehensive resources are not available for other languages, but we encourage continued collaborations between computer scientists and linguists to build these resources for dialect-robust nlp systems across languages.As it stands, the current iteration of multi-value provides global value by serving a global contact language, english, and its 50 most documented varieties.Despite the scope and precision of ewave for english, its catalog ultimately derives from linguists\\u2019 oral interviews with native speakers, and here we can identify some additional limitations.First, the orthographic conventions that linguists use to encode spoken dialect may not always align with the speakers\\u2019 own writing conventions and usage.Second, our approach can only cover the variation that linguists observe frequently enough to document, and in canonical forms in which they are documented.This means we may not fully capture variation within each feature.Finally, dialects should not be treated like deterministic speech patterns, but rather like a range of grammatical options or switches that may be turned on and off and adjusted for frequency in various social and personal contexts.Dialects do not always fit into nicely prescribed categories.\",\"While the vector representations obtained using xl-lexeme for different languages are potentially comparable, lying on the same geometric space, the evaluation of cross-lingual semantic changes cannot be performed for lacking crosslingual lsc detection resources.Semeval 2020 task 1 datasets consist of small sets of target words, i., the number of target words for english, german, latin, and swedish is 37, 48, 40, and 31, respectively.The example of the latin language highlights that xl-lexeme can perform poorly on languages that are underrepresented in the training set of xlm-r and not covered by the wic dataset.Generally, at the moment is not possible to state precisely how and how much xl-lexeme performance is affected by the language distribution in the xlm-r training set and the wic dataset.\",\"To create a clean and diverse corpus, we have chosen to crawl news articles as our primary data sources.Since all the articles are crawled from public domains, the data could potentially encompass the biases which propagate in public channels.Currently, the models trained on such data sources could model the inherent biases present within the data.8 9 10 language identification tools are restricted to a limited number of languages and unavailable for some of the very low-resource languages like bodo, dogri, khasi, etc.We made our best effort to clean the corpus using unicode spans, but it is possible that the data sources could have some issues.from our ablation studies, we see that models are benefited by using in-language training and\\u002for development sets.We call upon the community to work together to create more in-language data resources.Finally, there is still work required in terms of building datasets for hundreds of extremely low-resource languages not represented in this work.\",\"In each contribution of this work, we can isolate several potential limitations.In creating c-xnli, the mt model for the formation of the train set was chosen based on the results from a single dataset.Additionally, an assumption that the model is plagued with typical issues that affect mt models was investigated on a small dataset.Although we are skeptical of the mt model\\u2019s performance and perform qe scoring of the small dataset by a group of annotators and analysis to ascertain its performance, we are only comparing croatian machinetranslation results to results from a single language , assuming that results would hold for other high-resource languages.Also, for some mt evaluations, we use a single metric known to have many problems but only generally considered to correlate with human judgment.Our hyperparameter optimizations are of limited scope.All hyperparameters are fixed, except the learning rate with four possible values we search over.we could not perfectly reproduce the results outlined in the paper of our baseline xlm-r base model, partly due to a lack of elucidation in the original 4cc by-nc 4.0 5 paper and partly due to limited hyperparameter optimizations.\",\".as such, there is a risk of an anglocentric bias in the created dataset.Nonetheless, the goal of this study is to explore the potential of leveraging colexifications to bootstrap cross-lingual datasets in as many languages as possible, including a lot of low-resource languages.\",\"We highlight three limitations of our work.The first is that xsim++ is automatically constructed.There could be noisy sentences leading to errors that are irrelevant to the quality of encoders.The second is that xsim++ applies transformations solely to english sentences.Generalizing it to non-english language pairs requires additional research.Finally, we have experimented with the two most popular multilingual encoders: laser and labse.There are other available approaches which would be interesting to also validate xsim++ against.\",\".For example, none of the models give a special treatment to cognates which are normally considered easier to acquire.Another concern has been the lack of transparency in the classifier\\u2019s predictions, a direct consequence of the dense representations we favoured over the more interpretable linguistic features.Finally, a contextual classifier may predict different levels for occurrences of the same lexeme in different contexts.Those limitations underline the need for human validation of the output of such systems.\",\"This work focuses on using computational tools to determine dialect based on a small quantity of writings of a spoken language, using a writing system that was adapted recently, rather than one that evolved alongside the language for thousands of years.This limitation in orthography leads to differences in character usage, frequently between dialects , but there is also variation also within dialects depend- ing on the author.For example, different writers will use different methods of transcribing a nasal sound; eastern tends to use nh for nasal sounds in the middle of a word, although some writers will use a capital n.southwestern anishinaabemowin tends to use ny, ns or nz for these same nasal sounds within words.As noted by valentine , there are variations in language within dialects, including age-stratified language proficiency, where older speakers tend to be more fluent than younger ones, largely due to differences in opportunities to learn the language.These differences might be detected and interpreted as dialect differences if the diversity in writers is not comparable between the two sets of texts being compared.Additionally, an individual\\u2019s word choice may change depending on their gender or occupation , and having only a small sample of writings does not allow us to capture these differences well.To test how much our model is learning author preferences over dialects, using some writings from authors not included in the training data would provide some insight.only small quantities of text were used for each dialect, which was limiting in terms of methods that could be used.The methods utilized in this paper could be easily applied to minority languages that do not have large quantities of written text available, of course, with permission from and in collaboration with indigenous language keepers.ethical statement some of the texts used for our samples were transcribed aadizookaanan, a type of traditional story highly revered by ojibwe.These particular stories are not to be spoken out loud during non-winter months without snow on the ground.There are particular spiritual reasons for this, and unfortunate things can happen to individuals telling or hearing these stories when there is no nearby snow.Therefore, we will not write out the stories here and we strongly encourage citation followers to heed precaution.For more information, bring your tobacco and questions to a trusted anishinaabe knowledge keeper.Indigenous peoples have experienced a long his- tory of colonialism, including by well-meaning researchers.Please remember that indigenous peoples must maintain sovereignty over their languages, traditional stories, and other knowledge.All research involving indigenous knowledge, including that for the development of generative ai, should be done ethically in reciprocal relationships with indigenous peoples.The research should also meet their needs and wants, as described by the given indigenous peoples.\",\"Naturally, our work comes with a number of limitations: for instance, we restrict ourselves to testing eight pronoun sets out of the rich plethora of existing options.To ensure diversity, we resort to one or two sets per pronoun group\\u2014we hope that individuals feel represented by our choices.Similarly, we only translate single sentences and don\\u2019t investigate translations of larger and possibly more complex texts and we only translate to a number of languages none of which is resource-lean.Our study demonstrates that simpler and shorter texts already exhibit fundamental problems in their translations, even to resource-rich languages.\",\".2, sometimes the metrics are unnecessarily penalised due to errors made by the end task models.Filtering these cases would require checking every example in every task manually.We hope our results can provide conclusive trends to the metric developers focusing on segment-level mt evaluation.We included three tasks to cover different types of errors in machine translations and different types of contexts in which an online mt metric is required.Naturally, this regime can be extended to other datasets, other tasks, and other languages.Further, our tasks used stricter evaluation metrics such as exact match.we have covered 37 language pairs across the tasks which majorly use english as one of the languages.Most of the language pairs in this study are high-resource languages.Similarly, the examples in multilingual datasets are likely to exhibit translationese - unnatural artefacts from the task language present in the test language during manual translation; which tend to overestimate the performance of the various tasks.We hope to explore the effect of translationese on mt evaluation and extrinsic tasks in future.The choice of metrics in this work is not exhaustive and is dependent on the availability and ease of use of the metric provided by the authors.\",\"This study is restricted to the americas.Therefore the results from this paper can not be generalized, as different indigenous communities or nations might have different pasts.Also, all opinions expressed by the interviewed people are exclusively personal and in should not be interpreted as the general stand of the communities.As discussed in the paper, the main aim of this work is not to provide a normative for mt researchers.We rather provide a set of questions and open topics that should be considered when performing mt work with indigenous languages.Nevertheless, we also provide general and broad non-normative recommendations that should be carefully applied to the concrete case of each community.\",\"This work applies to languages that have a modest amount of data in parallel with english and are represented in pre-trained language models.These are typically high to mid-resource languages.Very low-resource languages might not have enough parallel corpora to extract sufficient ner training data.With limited parallel data and\\u002for limited representation in pre-trained lms, it will be difficult to get high-quality word alignments for projection.We use span-based annotation projection to alleviate word alignment errors to some extent.\",\"In this paper, we build the optimal translation policy under all latency by simply setting the search interval, achieving high performance.However, we think that the performance of our method can be further improved by exploring more interval settings.Additionally, although we train the agent using a simple architecture and achieve good performance, there exists a performance gap between the learned policy and the searched optimal policy under low latency.\",\"There are several limitations to this work which should be kept in mind.First and foremost, the datasets for evaluating the measurement of semantic change are relatively small, meaning that any estimates of correlation with human judgements will be relatively high variance.In addition, although the semeval data includes text from four languages, there is no guarantee that these methods will work as well as they do on other languages or other time periods.Moreover, our approach depends on the use of pretrained language models, and the quality of these and other relevant resources will vary by language.In addition, like all methods, our approach involves numerous small choices, such as the number of background terms to sample, the number of samples taken, and the value of k in choosing top substitutes.We have kept our choices for these consistent across all five datasets, and these values have not been tuned.As such, different choices could result in better or worse correlation with human judgements.It is also worth noting that the human judgements collected by the creators of these datasets may involve errors or noise.It is possible that a different sample of data, or having different people evaluate the same data, would produce different judgements.Directly, with the only differences being that we mask terms of interest , and do not combine multiple forms of lemmas when getting the top-k terms.We adopt this approach because it is especially easy to combine with our own work, but different methods for word sense induction might lead to different\",\"Train dataset for muril-simcse: while we try to minimize the hateful samples in this dataset by removing all the toxic\\u002fhateful samples of the respective datasets used to form this dataset, there could be samples containing certain biases like gender bias and racial bias.Also the dataset contains the respective languages written in the roman script, so the results might not be transferable to the respective native scripts of the languages.Muril-simcse: the model was trained on a single tesla p100 gpu for 9 hrs.We could have trained further and on more data, but we could not due to resource and economic constraints.\",\"1) on plm capability for transferring to new language, in this work, we used metaai\\u2019s wmt21 multilingual pre-trained language models as our test-base for the knowledge transfer into an external language fine-tuning and translation.This new-language ability is much dependent on the mplms we used, such as wmt21fb as a huge size model, a conditional generation from meta-ai\\u2019s massive m2m-100 model.If we try to fine-tune a bilingual model on an external language that the plm did not see, it will not be that good because for smaller-sized models such fine-tuning would be too much of a change, and the model will lose generalisation which leads to problems.For huge multilingual plm models, the 250k of fine-tuning data is a small set of numbers, and that\\u2019s why the model does not lose generalisation and captures new data well without losing linguistic knowledge of other languages that it was trained on.2) on the impact of language families, the mmplm wmt21fb we deployed has both alphabetic languages and cjk character languages, as well as slavic language.This might make it easier to transfer to a new language, e.however, in situations when the mplms did not include any of the language scripts that belong to the language family of the target one, it can be much harder for it to transfer to the new target language.\",\"Algorithmic limitations: the current approach assumes each phoneme \\u002f grapheme corruption is independent of the surrounding phonemes \\u002f graphemes, which can be relaxed to get further insights and model any contextual phonetic shifts.The relative importance between grapheme and phoneme corruptions could also be explored as a hyperparameter to personalize more to the type of errors of a community.Other limitations : our coverage analysis is conservative since it does not cover the user generated data from various social media where such l1-l2 phonetic misspellings are bound to be more common.The coverage analysis also relies on the context not being corrupted.However, this might not necessarily hold and the analysis could benefit from a careful formulation of a relaxed matching criteria that also considers cases with corrupted contexts.With transliteration playing a major role in our solution, it is difficult to immediately extend the work to low-resource languages that do not have models or appropriate datasets to build transliteration modules.\",\"Our study is limited to the adaptation of mlms to new languages.While we believe that our proposed approach could also be applied more broadly , further experiments are necessary to empirically verify this.In addition, we observe a considerable variance across languages , the reasons for which are not entirely clear.Ideally, we would have a broader set of languages to better study this, as our language set is limited and skewed towards the indo-european family.Finally, we average results over 5 finetuning runs, but computational restrictions prevented us from also averaging over multiple pretraining runs.5, we observed a non-negligible variance over pretraining runs in a preliminary experiment, but a more systematic exploration is necessary to better understand its impact.\",\"Our approach is heavily dependent on the quality of the pre-processed orthographic-phonemic transcription data as it provides the ground-truth for unsupervised alignment objectives.Generating phonemic transcriptions and aligning them correctly with orthographic representations can be costly.Despite our significant efforts, the alignment is still far from perfect optimality.Secondly, our approach might not be effective in improving performance on randomly chosen language pairs.As our framework aims to exploit phonemic similarities of languages with different orthographic representations, the methods are only effective in cross-lingual transfer between lexically similar languages in terms of phonology such as cjkv languages.Languages that do not fall into this category might observer little to no performance gains with our proposed framework.\",\"In this work, we confirm the effectiveness of the proposed method only on the english-german translation tasks using the multi30k dataset, the most commonly used dataset in the mnmt reserach area.It is not clear whether the proposed method is effective for translation for language pairs other than english and german or translation when a larger training dataset is used.the proposed method has improved translation performance of mt, but the performance is not perfect and translation results could include translation errors.Accordingly, there still remains a possibility that translation results by the proposed method could convey incorrect information.The proposed method requires an additional process for transforming images, compared with conventional mnmt models.The experiment, including model training and testing, on the proposed model mnmt took about 20 hours longer than that on the baseline mnmt model mnmt when using rtx3090 gpu \\u00d7 1.\",\"Our findings should be interpreted considering a series of problem definitions and design choices.First, our quantitative results on measuring incidental bilingualism at scale are subject to language identification, sentence splitting, and mining errors.Our qualitative analysis for the english-french language pair revealed that those errors are reasonably small.However, we expect the accuracy of our tools to vary across languages and, crucially, exhibit unanticipated failure modes on web text and low-resource languages.Second, our findings are restricted to quantifying bilingualism and translations within a limited set of language pairs and only paired with english.Thus, by problem definition, we are limited to computing a lower-bound estimate on incidental bilingualism of palm.The above limitations should also be taken into consideration when interpreting our ablation results.Although we attempted to remove most bilingual signals in our series of mt experiments, it is still possible that bilingualism slips through due to either model errors or due to bilin- gual signals beyond our focus set of languages.Finally, any results and findings of our work are restricted to palm; the single llm studied in this work.However, our finer-grained analysis reveals that incidental bilingualism, including translation signals, is observed across various data sources that are commonly included in the training data of other popular llms.\",\"One of the limitations of our work is the unavailability of context data and unavailability of phrasebased annotations for all languages except hindi.The unavailability of phrase-based annotations prevents the usage of universal tags because markings that are present on a single word in highly agglutinative languages like marathi or tamil get expressed on 2\\u20133 words in isolating or fusional languages like hindi or bengali.The benefits of using phrase level morphology over token level morphology have been discussed in goldman and tsarfaty.For example, the word \\u2018sochega\\u2019 in hindi will have msd tags: future tense and male gender, while in english, it would take two words, \\u2018he will think\\u2019 to express the same amount of morphological information.The presence of contextual data can also help to disambiguate msd tags.The other limitation of our work is the mismatch between the languages for which pretrained models are available and the languages for which we have the annotated data.For example, unimorph dataset contains annotated examples for assamese and sanskrit, but we do not have multilingual pretrained encoder-decoder models for these languages.\",\"The general performance of clicotea could be improved with a better mplm than mbert, such as xlm-r which has a larger token vocabulary and has been pre-trained on a much larger dataset.Our approach is currently not applicable to generation tasks where a multilingual text decoder is needed to generate text in unseen languages., current multilingual vl models still do not surpass the translate-test baseline of the tasks from iglue benchmark.The performance of clicotea is promising but the best scores are still obtained when translating everything to english and using the albef model.The smallest difference in accuracy on marvl dataset between clicotea and albef with translate-test is obtained in swahili , while the gap is much larger for the other languages.Outperforming the translate-test achieved by albef still remains an open challenge, especially for high-resource languages.\",\"One limitation of this work is the focus on englishto-many and many-to-english settings, while previous studies also went beyond english-centric translation.Second, we experiment with a wmt based benchmark that has a total of 15 languages and 200m training examples, when translation models were also trained on larger datasets.additionally, the data collected from high resource languages may be of higher quality compared to that collected from low resource languages.Further research is needed to determine the impact of low quality training data on interference and synergy.Finally, while we explore trends when scaling models width, deeper models might help mitigating interference even further.\",\"Although wspalign successfully outperforms all existing baselines, it is still limited to the accessibility of low-resource language information.For example, the collection of pre-training data requires multi-lingual pos tagging tools to identify which words are common or not.It also requires a multilingual plm and wikipedia hyperlinks to make the alignments, which could be inaccessible for an exceptional minority language.But note that we showed wspalign\\u2019s cross-lingual ability in \\u00a75.1, which implies that this issue can potentially be addressed in the direction of pre-training on large-scale monolingual data with our future effort.Besides, this paper lacks evaluation on real low-resource language benchmarks because there is no existing test set.\",\"This benchmark compiled and analyzed existing resources collected from diverse methods and domains.Although we demonstrated how careful use of these resources could transfer well to other resources, along with a manual analysis of a varied set of corpora, we cannot guarantee the quality of each resource or validate the methods that the original authors used to create them.However, we encourage a deeper exploration of the quality of individual resources by researchers that speak the 12 languages included in this benchmark and corresponding data loaders.Additionally, the human evaluation performed in this study was limited in scope and served primarily to validate the findings by automatic metrics.A more extensive evaluation with more annotators evaluating more sentences would be beneficial in order to draw further conclusions.Furthermore, some of the resources discussed in this paper were automatically aligned.Although neural crf models in english have been shown to yield high-quality alignments , other alignment algorithms such as tf-idf scoring have been shown to result in a high number of false positives.we will continue updating this benchmark as updates are made to the underlying datasets, and new multilingual resources are released.\",\".The hubert model quality is critical to speech-to-speech translation performance, as its extracted units are used by both speech-tounit model and vocoder.We have not explored the optimal strategy of multilingual hubert training.One research question is how to choose a group of languages so that a multilingual hubert model could be well trained.For example, it is arguable whether lithuanian should be included in slavic or uralic family.Other questions could be whether a larger hubert with more model capacity should be used and how we should deal with language imbalance in multilingual training.We provide benchmark results of bilingual speech translation with mined data selected by heuristics.One of our future directions is to come up with a better strategy of mined data selection to improve translation performance and training efficiency.As mentioned in our results analysis, the reported bleu scores are heavily dependent on the asr quality, which may not reflect the speech translation performance accurately.Future directions could be improving asr quality or exploring other evaluation metrics without reliance on asr models.as a technology used for speech generation, the presented speech translation models or the translation models that will be trained with speechmatrix dataset might have systemic bias or produce inappropriate outputs.\",\"We do not propose a solution for extremely lowresource languages, where neither unlabeled text for building language models, nor native speakers are readily available.Examples of such languages include muscogee, with about 4500 native speakers, and 325 articles in the muscogee language wikipedia, and arapaho with about 1000 speakers and no wikipedia articles.In such cases, finding even a single expert annotator might be difficult.The development of resources in such languages, however, do not necessarily rest purely on technological factors.On the technical side, directprobe relies on the fact that a representation can be generated for the instance to be annotated.However, obtaining an effective representation for structured annotations is nontrivial.While this is a problem, this is orthogonal to our contributions.\",\"In this work, we evaluated neural networks on the task of back-translating mathematical formulae from the pl latex to semantic cls..Moreover, we observed that the perplexity of the translation of mathematical formulae behaves differently from the perplexity of the translation between natural languages.Our evaluation shows that our model outperforms the mathematica software on the task of interpreting latex produced by mathematica while inferring the semantic information from the context within the formula.A general limitation of neural networks is that trained models inherit biases from training data.For a successful formula translation, this means that the set of symbols, as well as the style in which the formulae are written, has to be present in the training data.Mathematica exports into a very common flavor \\u002f convention of latex, while semantic latex, translated by latexml, yields many unconventional latex expressions.In both cases, however, the flavor \\u002f conventions of latex are constant and do not allow variation as it is produced by a rule-based translator.Because of the limited vocabularies as well as limited set of latex conventions in the data sets, the translation of mathematical latex expressions of different flavors is not possible.In addition, we can see that a shift to a more difficult domain, such as special functions in the dlmf, produces a drop in performance but still generates very promising results.as an example, a random choice between multiple ways to express a mathematica expression in latex could be added.For semantic latex, the performance on real-world data could be improved by using multiple macro definitions for each macro.Ideal would be a data set of hand-written equivalents between the pls and cls.This could allow learning translations and tokens that are not present in the training data for the respective language pair.Further, mathematical language-independent concepts could support a shared internal representation.Another limitation is that data sets of mathematical formulae are not publicly available due to copyright and licensing.We will attempt to mitigate this issue by providing the data sets to interested researchers.Note that this work does not use information from the context around a formula.Integrating such context information would aid the translation as it can solve ambiguities.For example, for interpreting the expression n, information about the specific field of mathematics is essential.Further, context information can include custom mathematical definitions.In real-world applications, building on such additional information could be important for reliable translations.\",\"The approach to collect our dataset is expensive and laborious.This along with the dependence on expert annotators makes the transfer of such an approach challenging for other low-resource languages.We however, find this a necessary endeavor to develop initial resources that can help provide a starting point to extend access to more languages and iteratively improve research, technologies and services across languages.\",\"In this paper, we mainly focus on evaluating our approach on two english-centric corpora, iwslt17 and pc32.Future research could consider more multilingual machine translation benchmarks with different number of languages and training samples and conduct experiments on more challenging training scenarios such as chain configurations where we have multiple bridge languages and different zero-shot distances.\",\"Since this work relies on the in-context learning ability of large language models, the challenges associated with computational resources to load an llm ensue.Due to resource constraints, we could not use larger or commercially available llms to validate if the advantages of x-insta translate to those models as well.5, the static nature of the aligners poses a limitation on x-insta.therefore, task-specific, trial-and-error style manual intervention is needed.We believe a better understanding of the pretraining distribution of the multilingual llms can pave the way toward better automated alignment methods.There are multiple shortcomings of monolingual icl that entail its cross-lingual counterpart and x-insta does not address them; issues like knowledge hallucination, limited common-sense reasoning, inconsistency in retrieving factual associations, etc.\",\"Our method requires access to a large set of labeled examples for the memory bank\\u2014ideally with some relevance to the evaluation tasks.This limits the languages and tasks that are optimal for this method: there does not exist a large variety of training examples for low-resource language varieties, nor for certain much more specific tasks\\u2014as in, for example, industry applications with domainspecific customer data.And while multilingual models could leverage cross-lingual transfer, it is unclear how well this model would generalize into low-resource languages when using multilingual bart.When using the full demonstration memory, meta-training does not run on a 16gb gpu using our current implementation.While this does exclude more common gpus, our approach could still run quickly on a 32gb gpu in a few hours, thus costing far less than pre-training a language model of comparable few-shot performance from scratch.\",\"Our work presents a new dataset based on text data scraped from the internet.Hence, the quality of the text depends on the quality of the available websites.Most of our data stems from the three websites apo, koe and mdr providing a rich vocabulary in our corpus.While this vocabulary covers a variety of mixed topics, we cannot rule out any negative side effects of data imbalance.Moreover, our dataset can only represent topics that were considered relevant to be translated into simple german by the respective website.2 we presented the different guis that we used to either manually align the sentence pairs or evaluate a sample of sentence alignments.One drawback of the tool for the second evaluation method is that it focuses solely on the matched sentences and presents them isolated from their contexts.One can argue that evaluators using the tool would have to see the context in which the sentences appear in order to correctly classify partial matches.Also, providing more information to the annotators might enable them to also correctly classify additional explanatory sentences.a higher granularity of language difficulty could be achieved by incorporating texts originally directed at language learners that are rated, e.our work presents a parallel corpus for german and simple german and should be continuously expanded.Not only to increase its size, but mainly to increase the number of topics covered in the corpus.Yet, as there are no efforts to start a single big corpus like a simple german wikipedia, web scraping from various sources stays the method of choice for the future.An additional option is to compute sentence alignments for existing article aligned corpora to include them in the dataset.As for the sentence alignment algorithms, various extensions are imaginable.Firstly, it might be interesting to allow one simple german sentence to be matched to multiple german sentences.Also, the assumption of the mst-lis about the order of information is very strong, and recall might be improved by softening this assumption, e.by allowing matches that are at most n sentences away.Other alignment algorithms that impose different biases on sentence order are interesting for further extensions.Our dataset can be used to train automatic text simplification systems which then should produce text with properties of simple german.Direct use cases for such simplification systems are support systems for human translators or browser plugins to simplify web pages.Further research has shown that text simplification as a pre-processing step may increase performance in downstream natural language processing tasks such as information extraction , relation extraction , or machine translation.It remains an interesting direction for future research if simple german can help to further increase performance on such tasks.\",\"First, neither feldermodell nor doppelbaum has obtained complete concurrence among linguists.Also, we limit our scope to the english\\u2013 german language pair and the it domain using the wmt 2019 training, validation, and test data sets.A broader scope would not provide confidence in the validity of conducted experiments because there are hardly any standard setups for experimental research.In addition, the conducted experiment should take into consideration the effect of randomness that is attended in the process of training artificial neural networks; different techniques, different hyperparameters, and multiple runs of optimizers may present different results.However, as previous studies , including the study on the baseline model , do not consider the effect of randomness, we also do not investigate the effect of randomness further, considering that training multiple models to obtain good estimators will cost a lot.\",\"A major limitation of the current work is the absence of gold alignments for evaluating the different methods.Gold alignments would also enable us to provide more reliable estimates of the prevalence of the evaluated phenomena in the three datasets.We are not aware of any other similar corpora that come with gold character alignments.furthermore, our work currently only covers european languages in latin script.Some of the presented techniques also assume identical writing systems in the transcribed and normalized layers.Our setup may therefore not generalize well to the dialectal variation and writing systems present in other parts of the world.For example, the v-c proportion cannot be easily determined in scripts that do not specify all vowels.Although there is an extensive amount of research in particular on arabic and japanese dialects and their normalization , we currently limit our experiments to data written in latin script.\",\"For cls, giving a more faithful solution.Limitations the limitation of this paper can be stated from three perspectives.First, although using our cls annotation protocol can label more faithful data, the annotation cost is higher because annotators need to comprehend the full source text instead of only the source summary.Second, convsumx only covers 3 typical languages, while languages from different language families and have different morphology and lexical-\\u002fsyntactic rules require further investigation.Third, although the proposed 2-step method is effective, we simply concatenate the source input text and mono-lingual summary at the token level as the model input but do not make further exploration.\",\".Nlp research can also be used as a political instrument of power, where we can observe mutual relationships between language, society, and the individual that \\u201care also the source for the societal impact factors of nlp\\u201d.In this way, nlp translation can be applied as an instrument to changing the culture of minorities as in traditional translation.So colonizers used translation as means of imperial control and expropriation.The asymmetry of power is the cause of domination, where subaltern cultures being flooded with \\u201cforeign materials and foreign language impositions\\u201d is a real danger for minority cultures.Schwartz discuss the need to decolonize the scientific approach of the nlp community as a whole, expressing the need for researchers to be cognizant of the history and the cultural aspects of the communities which use the languages they are working with.Additionally, he proposes that our research should have an obligation to provide some benefit from our studies to the communities, an obligation of accountability , and an obligation of non-maleficence.The fact that many translation systems nowadays are multilingual8 also result in more multi-cultural challenges.Finally, we also want to highlight the importance of discussing mt systems in a text-to-text setup.The usage of text is constrained to certain topics and varies from community to community.For instance, wixarika and quechua, languages that are spoken across all generations, are used in a written fashion mostly in private messaging apps but also have a prolific meme and facebook publication generation9.Even if a certain community does not widely adopt the written tradition, there are, at minimum legal obligations of the states towards indigenous languages.For example, some constitutions recognize indigenous languages as national languages , 8multilingual systems refer in nlp to systems capable of translating a set of languages from and to english.In some cases, they are also able to translate between languages where english is not involved.com\\u002fmemeswixarika2019, quechua speaking group: 711230846397383\\u002f binding the state to the responsibility to translate all official pages, documents, laws, etc.this has not been implemented, and this case is a highly valuable application case for machine translation to assist human translation.However, our findings also apply to speech-to-text translation and speech-to-speech tasks that would cover all languages, even with no written tradition.\",\"One limitation of our dataset, elqa, is that the corpus only contains questions in english and about english.However, stack exchange has sites with questions about other languages and our main data extraction scripts are general enough that they can be used to create corpora for other sites on stack exchange.Of course, language-specific processing steps, quality assurance and analysis must be applied before releasing such data.Most importantly, the models we have presented here are intended only as baselines for future research, not for deployment.Potential biases reflecting the demographics of authors represented in the training data also need to be considered if models are deployed for different target populations.Moreover, many of these types of questions are found on the web, and a lot of the same topics are brought up by many users, so a model\\u2019s ability to generate correct answers cannot necessarily be attributed to abstract reasoning.\",\"Our experimental results should be interpreted with the following limitations in mind.First, our experiments involved relatively small datasets in english only.The performance of the model should also be evaluated on other languages and larger datasets.Second, the improvement observed in our best models depends on both the efficacy of the linguistic features and on the strength of the neural model itself.As neural models continue to improve and effective linguistic features are identified, the best methods for combining may also need to be updated.\",\"Our study includes some limitations that must be addressed.Some test examples might have wrong predictions made by the homograph disambiguation module.Specifically, in positive examples where lexical constraints should be imposed, its errors result in wrong corrections.Table 12 shows how these erroneous corrections affect the results.We can observe an overall decline in csr; however, it does not hurt the translation quality.We verify that the differences in bleu resulting from wrong corrections are not statistically significant for all the methods.Considering the gain achieved in negative examples, as seen in fig.2, our proposed homograph disambiguation might serve as a useful starting point to address homographs in lnmt; however, there is still room for improvement.Our current homograph disambiguation module is designed as a stand-alone system outside the lnmt.\",\"All samples used in this work are in english, thus to apply the model to other languages, it will require training data on the specified language or using multilingual language backbones.Moreover, we are aware that it remains an open problem to mitigate biases in human stancetaking.Of course, current models and laboratory experiments are always limited in this or similar ways.We do not foresee any unethical uses of our proposed methods or their underlying tools, but hope that it will contribute to reducing incorrect system outputs.\",\".In terms of method generalization, the proposed method depends on multi-lingual neural machine translation models to generate trans-lingual definitions, and hence limits its application scope to those languages rarely supported by translation models.Moreover, our findings are based on three languages, but different families of languages may exhibit distinct phenomenon that even challenges our\",\"The work described here is part of an ongoing project, and our results, while promising, should be viewed as preliminary.We only report results for the writing systems of two languages, which is a major limitation for a study focusing on typology and cross-writing system variation; past studies in this vein ; sproat and gutkin ; rosati ) have rightly considered a wider range of languages.While the systems we consider provide good points of comparison, the results would be strengthened by considering a wider range of writing systems.Finally, it should be noted that the morphological parsing done on the data used in this study may be imperfect, despite the first author\\u2019s best efforts.Limitations in modern understanding of sumerian result in some cases that should perhaps be viewed with some caution.Similarly, for japanese, the treatment of all jukugo words as bimorphemic may or may not accurately reflect how such words should be analyzed in modern japanese.It\\u2019s also possible that some non-jukugo two-kanji words were accidentally categorized and parsed as if they were jukugo.Certain non-jukugo compounds may have also escaped detection.\",\"Perhaps the main limitation of this work is that we only explore the approach within the context of machine translation benchmarks, although we conduct extensive experiments within this task that cover different training data scales and diverse pairs of languages, including low-resource ones.Nevertheless, we remark that the proposed approach is entirely general-purpose, and can be applied to any other language generation or even any neural classification tasks.Furthermore, we have not yet explored how this technique would interact with other modelling choices, such as different optimizers, training objectives, or subword tokenisation algorithms.Lastly, our unigram initialisation of the bias term is currently done at the level of subword units, which do not always correspond to lexically or morphologically meaningful linguistic units.\",\"We worked with only five language pairs, all involving english and another language: arabic, spanish, french, russian and chinese.This is due to using the multiun dataset for evaluating alignment and performing realignment.This narrow choice of language limits our ability to understand why realignment methods work well for some languages and not others.And we believe that making a similar analysis with many language pairs, not necessarily involving english, would be a good lead for further research investigating the link between the success of the realignment method and how two languages relate to each other.We chose a strong alignment objective with contrastive learning for our realignment task.Several other objectives could have been tried, like learning an orthogonal mapping between representations or simply using a \\u21132-loss to collapse representations together , but both methods require an extra regularization step since they do not leverage any negative samples.For the sake of simplicity, we focused on a contrastive loss, as trying different methods would have led to an explosion in the number of runs for the controlled experiment.This also explains why we used the same hyperparameters and pre-processing steps of wu and dredze.A more thorough search for the optimal parameters, and realignment loss, might lead to better results.\",\"There are several limitations of this work.First, while we have observed positive correlations between ft-scores and rtt-scores and conducted experiments to predict ft-scores using rtt-scores, their relations could be complicated and non-linear.We encourage future research to investigate various rtt-score features and more complex machine learning models for better prediction models.Second, we have examined the prediction models on low-resource languages in flores-101, but have not tested those very lowresource languages out of these 101 languages.We suggest auditing ft-score prediction models on a small validation dataset for any new low-resource languages in future applications.Third, our assessment has been systematic and thorough, utilizing datasets such as flores-101, wmt2020-news, and wmt2020-bio.Despite this, the nature of our study is constrained by the timeline of the data utilized.The wmt data we used is from 2020, opening up the possibility that more recently proposed metrics could potentially outperform the ones proposed in this work.\",\"This study utilizes a monolingual pre-trained language model in the english language.Although the weaklysupervised classification methods are not limited to a particular language, we have not explored applying the method to another language.Social media language use may differ significantly from the data used to train the plm.Moreover, the presence of code-switching may also degrade a monolingual plm\\u2019s performance.We explored a roberta checkpoint continually trained with 60m english tweets.16 however, it does not yield better performance than bert.We have not investigated whether it is due to the training regime or the dataset.Moreover, in this work, we focus on classifying hate speech categories\\u002ftarget groups instead of hs detection.due to limited space, we prioritized in-depth analysis instead of a comprehensive evaluation.we are working in parallel on extending this work to a longer-form journal article to cover more datasets and experimental results.Recent work on large language models demonstrated that when the parameters scale to a certain level, language models exhibit a drastically-increased performance in zero-shot classification.We reported the performance of a moderately-sized bert-large-uncased zero-shot model because of limited computational resources and lack of access to commercial apis.Larger language models will likely perform much better than this baseline.Lastly, understanding hs sometimes requires cultural understanding or background knowledge.It may be difficult to determine the presence and category of hs when we take the post out of its context.For example, many \\u201csexist\\u201d posts in waseem dataset are tweets related to the australian tv show my kitchen rules , and below is a tweet labeled as \\u201csexist\\u201d: everyone else, despite our commentary, has fought hard too.\",\"Currently, we build a vocabulary from the original one used in mbart-50, and only conduct downstream experiments across 6 languages.Although we could involve more languages, it would require a larger cuda memory that might go beyond our device capacity.Hence, we merely select the above languages that have sufficient overlap with our pre-training datasets.In addition, for fair comparisons, we only use the strictly-aligned multilingual multi-modal dataset provided in , which is augmented through machine translation.It is unclear how the quality of strictly-aligned dataset would affect model performance.Meanwhile, the length of texts in our weakly-aligned multilingual multi-modal dataset is generally very long.As a result, we truncate textual inputs before feeding them into the encoder, possibly bringing information loss to some extent.\",\".First, it only considers and collects language instructions for the floor plan domain.second, it is limited in the scope of languages where we only collect instructions written in english.third, although generating floor plan designs from languages exhibit diversity, we do not consider improving generation diversity at this moment.\",\".For authors, the editing of posts would only be possible for output languages that they can read and write in.Furthermore, authors\\u2019 ability to edit the translations of their posts could result in ethical concerns, as this feature may be used to create language-specific misinformation.A solution to this problem is to present the reader with both the original machine translation and the user-edited version.Alternative solutions include allowing users to disallow the automatic translation of posts on a case-by-case basis, or to filter the audience of a post based on its original language.This aligns with the fact that users\\u2019 language choices are intentional, and depend heavily on the topic of the post and the target audience [13]: many purposely write posts in a certain language as a means to target those who speak that language.Overall, we see a substantial benefit in making sns users aware of how their posts are translated and shared with others, and in allowing them to remove the translation if the post was meant to be targeted to a specific audience or correcting the translation to preserve the original meaning of the post.Together, these design solutions would ensure users that their information is conveyed accurately and only to their intended audiences.\",\"While we aim for a comprehensive analysis of existing methods and model types for ancient greek and other classical languages, there are limits to exhaustively exploring the full space of variations and rigorously evaluating their impact on model performance.For example, we could not comprehensively evaluate the effects of the pre-training corpora, as we did not re-train a bert model for ancient greek, to pin down the exact difference between prior bert models and our own models, which are based on inherently stronger model types; similarly, we did not induce latin roberta and t5 models, to confirm the differences between mono- and multilingual models for language-specific latin tasks.In a similar vein, we did not compare different model sizes.However, we studied prior work and scaling laws and believe that the base model is appropriate for the size of our training data.Further factors of this type concern hyperparameter settings and other factors in isolation.Not only do we miss sufficient computational resources to perform such manifold ablations and comparative assessments, we also considered the carbon footprint that such experiments cause and which does not stand up to the insights that could possibly be gained from more experiments.For these reasons, we focused on two selected dimensions of variants that we believe to be valuable for a community interested in classical languages: we tried to answer questions as to when multilingual models can be profitably used, and aimed to showcase various potential advantages of encoder-decoder models, which by now have not been considered in studies on classical languages.Another clear limitation lies in the size of the demonstrated semantic and knowledge probing tasks.They are of small size, and we cannot, therefore, draw firm conclusions as to, e.also, the synonym\\u002fantonym disambiguation task is presumably the most difficult one.As a counter-balance, we used a more tangible task for knowledge probing, by choosing family relationships, which we expect to be frequently found in the pre-training corpora.A further limitation we find for the knowledge probing tasks resides in the size of our trained models and the underlying pretraining data.This limitation could be one that is not easy to overcome.But we still encourage the community to create similar probing task datasets.\",\"The morpho-syntactic parameters used in this study are just a fraction of various other linguistic parameters that have been proposed in theoretical syntax.A set of optimal language parameters for language clustering may vary depending on the target task.It remains to be seen whether and how various parameters in theoretical linguistics could improve different nlp tasks.For example, cross-lingual transfer learning may be performed more effectively by carefully tailoring the linguistic parameters to a particular task, like what we have done for ner.Related to the above point, one limitation of our approach would be the fact that some languages have not yet been investigated well in theoretical linguistics, particularly some underdocumented or endangered languages.Even as for welldocumented languages in theoretical linguistics, some parameters still remain controversial, such as the so-called np\\u002fdp parameter.Thus, our approach proceeds in tandem with the advancement of theoretical linguistics.\",\"This paper\\u2019s aim is to give an introduction to researchers, students, of interested community indigenous community members to the topic of machine translation for indigenous languages of the americas.Therefore, this paper is not an in-depth survey of the literature on indigenous languages nor a more technical survey of low-resource machine translation.We would point the reader to more specific surveys on these aspects.Ethical statement we could not find any specific ethical issue for this paper or potential danger.Nevertheless, we want to point to the reader that working with indigenous languages implies a set of ethical questions that are important to handle.For a deeper understanding of the matter, we suggest specialized literature to the reader.\",\"Our work depends mainly on parallel data.Although tasks focusing on language abilities can leverage machine translation to obtain parallel data , it is much harder for tasks about knowledge and facts.Using parallel data to train cross-lingual model editors is like doing full supervision, while we need to leverage weakly labeled data to mitigate data scarcity.On the other hand, whether monolingual or crosslingual, model editing still struggles with the continual learning problem.In the real world, knowledge constantly emerges and fades, disabling the stop of learning.However, most studies, including our work, focus on a single or a batch of inputs.Thus, an effective solution of continuously updating a series of inputs is necessary before model editing becomes a practical technic.Note that our work focuses on the editor\\u2019s generalized cross-lingual editing ability.We expect the editor to perform the editing honestly.This target potentially offers the possibility to modify model behavior maliciously.Though editing may not soon become a practical technic, the potential risk does exist.\",\"Similar to the limitations of existing selection methods, our method needs a reasonable feature embedding for accent representation in order to effectively target accents.Mfcc features are not the best choice to represent accent information.Some accents may be more difficult to represent than others.This also lowers fairness scores for such accents.For instance, in one of our experiments where manipuri accent was paired with rajasthani or assamese accents, we observe that acquiring a fair subset using any selection strategy is challenging.Although, flmi was able to achieve a higher tf score than others, it was relatively lower than other accent pairs.This is due to the fact that the pairwise similarity scores of utterances within the manipuri accent are lower than other accents.The lower pairwise similarity scores lead to lower marginal gains during greedy maximization and are a consequence of poor feature representations due to insufficient information being encoded about the manipuri accent.On another note, a risk associated with the targeting ability of ditto is that it could be misused to create models that are unfair to certain populations.\",\"Our model is evaluated in standard english datasets for classification.As we stated earlier we plan to investigate the cross lingual setting in the next step.The iterative nature of self-training imposes a high cost on the experiments.This has led to a few common practices.Most existing studies employ one underlying classifier to carry out the experiments\\u2013i.this practice albeit limiting, is justified by the argument that if an algorithm does not make any assumption about the underlying structure of the classifier, then one can safely select the best available classifier and use it in the experiments.another limitation is that, which is again stemmed from the high cost of self-training, one is typically forced to select a few sample sizes as labeled sets to carry out the experiments\\u2013e.this is in contrast to similar research areas, such as active learning, when one can usually afford to report a learning curve to illustrate the performance with a few training examples all the way to using the full labeled dataset.Given that we have 10 baselines, we reported the performance with 300 and 500 labeled examples.\",\"Measurement of translation literalness is neither well studied nor well understood.We rely on a combined interpretation of multiple measurements to investigate our hypothesis and its implications.This limits the extent to which we can make strong claims, since in the absence of a highly correlated metric for translation literalness, it is hard to compare systems.We could only claim that our investigation indicates the presence of a tendency towards non-literalness in gpt translations, but a stronger result would have been preferred to further disambiguate the translation characteristics.Further, we only compare gpt translations in the standard zero-shot and few-shot settings and it is quite conceivable that more specific verbose instructions could steer the llms to produce translations with different characteristics.\",\"Proto-elamite is undeciphered, which means that our results on this script cannot be compared to any known ground truth.We attempt to ground our results by situating them relative to current assyriological scholarship instead.Writing systems exhibit considerable variation in terms of the number of characters used, the visual complexity of those characters, and the degree to which they represent phonetic information.Although we try to cover a range of alphabetic and non-alphabetic scripts in our evaluations, we cannot cover all possible cases, and focus on those which have some similarity to the proto-elamite script which is the main concern of our work.\",\".following is the non-exhaustive list of possible covariates that we do not control in this work.The adapted model size, the size of pre-training data, pretraining configuration parameters, but also the broad variance of adapted language pair; the variance of mutual similarity of languages within the pair, and hence the difficulty of training the translation model.The evaluation of our experiments did not consider the effect of randomness of the training process.Despite the fact that our experiments were run with a fixed random seed and initial value, making our results deterministically reproducible, the variance of the results among the experiments of different random seeds was not investigated due to the related infrastructural costs.1 might have blind spots; for instance, in the cases utilizing decontextualized embeddings, where both the hypothesis and reference contain multiple occurrences of the same word, the alignment scheme will make the prediction of the same target token equally good, regardless of the position.\",\"Our experiments are based on the most standard adapter architecture for adapter-based cross-lingual transfer and beyond, which also facilitates comparisons to prior work in this area.However, we again note that there are other emerging parameter-efficient modular methods, including different adapter architectures , that could be used with the same conceptual idea.our evaluation relies on the currently available standard multilingual benchmarks, and in particular those targeted towards low-resource languages.While the development of better models for underrepresented languages is possible mostly owing to such benchmarks, it is also inherently constrained by their quality and availability.Even though our experiments have been conducted over 35 different target languages and across several different tasks, we mostly focus on generally consistent trends across multiple languages.Delving deeper into finer-grained qualitative and linguistically oriented analyses over particular low-resource languages would require access to native speakers of those languages, and it is very challenging to conduct such analyses for many languages in our language sample.Due to a large number of experiments across many tasks and languages, we report all our results based on a single run.Averages over multiple runs conducted on a subset of languages and tasks confirm all the core findings; for simplicity, we eventually chose to report the results for all languages and tasks in the same setup.Finally, training language adapters is typically computationally expensive; however, owing to the modular design of our framework with respect to language adapters, these are trained only once per language and reused across different evaluations.\",\"Limitation our paper presents a pilot exploration of investigating a new setting in code-switched text synthesis \\u2014 we allow the target language pair selection not limited to those for which we already have training data.Although we have shown the strength of gloss qualitatively and quantitatively, our experimental setting is still confined due to the dataset restriction \\u2014 all the input text is in english.additionally, due to the computational restriction, in gloss, we only explore mbart50-mmt and an augment-mmt as our pmmtm.From the experimental results, we do observe the benefit of having a more stable pmmtm in gloss.We anticipate the models\\u2019 performance can be further improved by leveraging more stronger pmmtm, and the exploration is left for the future.Broader impacts our proposed models are based on a model that is pre-trained on a large scale of multilingual machine translation data.It is known that the machine translation model could capture the bias reflecting the training data.Therefore, our models can potentially generate code-switched text containing offensive or biased content.We suggest that for deploying our model in any real-world applications, careful examination of the potential bias is an essential step.\",\".For the ease of the analysis, we conducted experiments using only the english dataset in this study.Although our proposed method can be applied to any language, its performance must be evaluated on languages other than english.For example, the semeval2020 task 1 dataset includes latin, german, and swedish language datasets, in addition to english, and can be used for this purpose.In particular, our proposed method requires only pretrained mlms and does not require additional training data for the target languages, which makes it easily scalable to many languages.Availability of mlms for the target language.Experimental results show that the quality of the mlm is an important factor determining the performance of the proposed method.For example, the proposed method reports good performance with vanilla bert model in table 2 but further gains in performance can be obtained with the fine-tuned bert model on masked time stamps.However, since our method assumes the availability of pretrained mlms, a problem arises when trying to adapt our method to minor languages where no pretrained mlms are available.This limitation could be mitigated to an extent by using multilingual mlms.For example, arefyev and zhikov demonstrated that satisfactory levels of accuracies can be obtained for semantic change detection by using multilingual mlms.Our proposed method can further benefit from the fact that new and larger mlms are being publicly released for many languages in the nlp community.\",\".First, our method requires additional entity resources, which may be difficult to obtain for certain language pairs.With the development of multilingual entity datasets like paranames , we are optimistic such resources will be more accessible in the near future.4, extracting translation candidates increases inference time.\",\".1 at two variants based on the compute budget , we only tested our methods on the 1.3b variant with a fixed number of experts e=64.These methods can potentially show larger improvements on larger models and marginal impacts on smaller models that do not suffer from severe over-fitting.The second limitation of this work is that our methods are only validated on a single multilingual mt benchmark.another limitation of this work, and most other works on multilingual machine translation, is the evaluation metrics and how to aggregate them.We report in this paper chrf++ scores and we average across three subsets of directions and three resource levels.This makes it difficult to highlight the impact in some challenging directions on which our methods can lead to \\u00b13chrf++ differential in quality.We did not report other metrics for the sake of brevity, and since we are not comparing to previously published results, chrf++ is a reliable metric for comparing and contrasting our methods.\",\"While our study shows that easyproject can effectively translate the source sentences with special markers inserted to the target languages, using the google translation and nllb model, it is unclear whether all translation models can work well when special markers are inserted.To generalize this approach to future mt systems, we design a simple and computationally efficient approach to improve the robustness of mt systems in handling special markers.However, the translation quality for the marker-inserted text still falls behind the original text.\",\"We used xlm-r for the baseline model to train with our dataset in our experiments because we wanted to make experimental settings as close as the previous study of codebert but for multilingual data.Since codebert is based on roberta, we chose xlm-r, which is also roberta-based and already trained with multilingual data.\",\"This work is mainly dedicated to the curation of a new multilingual dataset for indic languages, many of which are low-resource languages.During data collection, we face several limitations that can potentially result in ethical concerns.Some of the important ones are mentioned below: \\u2022 our dataset contains only those articles written by dailyhunt\\u2019s partner publishers.This has the potential to result in a bias towards a particular narrative or ideology that can affect the representativeness and diversity of the dataset.\\u2022 another limitation is the languages represented in va\\u0304rta.Out of 22 languages with official status in india, our dataset has only 13.there are 122 major languages spoken by at least 10,000 people and 159 other languages which are extremely low-resourced.14 none of these languages are represented in our dataset.\\u2022 we do not perform any kind of debiasing on va\\u0304rta.This means that societal and cultural biases may exist in the dataset, which can adversely affect the fairness and inclusivity of the models trained on it.\",\"Models are often developed with specific datasets in mind.Some papers introducing new models also introduce new training sets such as copyattention , spanoie , and imojie which may influence model assumptions.Spanoie also introduces its own manually annotated benchmark, which may have informed the assumptions spanoie makes.The lack of consensus on how to label openie makes it difficult to perform apples-to-apples comparisons because certain models can not extract some relations due to the assumptions they make.Openie has also largely been limited to english.Milie makes assumptions that allow for different extraction methods depending on the language, but other openie models that support multilingual extraction largely treat extraction from other languages the same as extraction from english.Multilingual openie remains an open field of study.\",\"One of the main limitations of our approach is the use of machine translation to create the iesemparse suite.However, we showed that the overall quality of our dataset is comparable to samanantar, a human-verified translation dataset.have shown the effectiveness of quality estimation in referenceless settings.Lastly, we have also extensively evaluated our dataset with the help of 3 human evaluators for each language as described in \\u00a73.We can further take help of gpt4 in future to evaluate the translations in a scaled manner.The second point of discussion focuses on the motivation for preserving logical form slot values in english.We explore the use cases where querying data in english is crucial, and how this approach can enhance models by reducing latency, limiting vocabulary size, and handling system redundancy.While open-source tools currently cannot achieve this, it would be valuable to evaluate the effectiveness of this task by comparing it with the other two discussed approaches.To accomplish this, we suggest using a dialogue manager and scoring the performance of its responses on the three top approaches outlined in the paper.Another potential limitation of our dataset is that it may contain biases and flaws inherited from the original top datasets.However, we contend that spoken utterances are generally simpler and more universal than written ones, which mitigates the risk of cultural mismatches in ie-semparse dataset.Furthermore, our work is confined only to the indo-dravidian language family of indic languages due to our familiarity with them and the availability of high-quality resources from previous research.Nonetheless, our approach is easily extendable to other languages with effective translation models, enabling broader applications in various languages worldwide.\",\"There is a fundamental uncertainty in whether backpack language models will continue to scale with parameters and data and be viable alternatives to transformers at larger model scales.In a similar vein, we do not verify that backpack language models perform well across multiple languages., finetuning backpacks on other tasks, or masked language modeling\\u2014there is a wide range of possible uses that remain to be verified.One potential obstacle to the use of backpacks that we do not study is the effect of tokenization in languages with richer morphological structure than english\\u2014will the backpack structure be amenable to modeling those languages? This may be difficult because, intuitively, the interpretability and control of backpacks relates to the semantics of individual tokens.Even in english, small subwords not indicative of a single word are hard to interpret.What we hope to have provided is a sufficient set of experiments to motivate the further exploration of backpacks.\",\"Psgd is a straightforward constrained-decoding algorithm for the translation suggestion task.How- ever, the early-stopping mechanism involves extra time costs.Though psgd is more efficient than vdba in the scene of ts, where only two constraints appear, it could be slower than vdba if there were more short constraints.Besides, even if we take both prefix and suffix constraints into consideration for emphasis on the whole translation generation, the decoding process is still auto-regressive from left to right.The algorithm could be improved if we made better use of the information of suffix constraints.\",\"In this work, we transliterate all maltese words in the same manner.Given the hybrid nature of maltese, it might be optimal to handle words which do not have an arabic origin in a different way.Similarly, we do not treat named-entities any differently.Moreover, we assume that the maltese text is written using the standard orthographic rules.In turn, the system might produce spurious transliterations for cases with spelling errors.This issue also exists when the text is in raw form, but may be further exacerbated with transliteration.The character mappings could be expanded to handle dropped maltese diacritics, such as writing c instead of c\\u0307, but there are other cases where silent letters such as gh\\u0304 are dropped altogether, making the problem non-trivial.\",\"Our comix approach assumes availability of parallel bilingual corpora and mature tools for pos tagging and phonetic transcription for both the embedded and matrix languages which does not hold true for every language.second, our current choice of guiding function for attention fdkga and mixing probability pmix are based on limited knowledge of the linguistic structure specific to english and indic languages, and might need to be adapted for other language families.lastly, as with large language models, our comix models are also vulnerable to biases inherent in the training corpus.\",\"Choice of languages our choice of languages for wikiann and ud probing evaluations were intended to strike a balance between being being typologically diverse and having data in our chosen benchmarks.However, there are major language families and geographical regions not represented in our languages.While we expect the trends in our results to continue to hold for other languages, we believe that further investigation is necessary on more languages to confirm our hypothesis.Choice of evaluation tasks one notable omission from our evaluation suite are sentence-level tasks, such xnli , xglue and crosslingual retrieval tasks.One reason is that previous work has shown that character-level models already perform well on these evaluations.In our work, we were particularly interested in situations where prior work showed character-level models underperforming subword-based models.In particular, canine underperformed at ner, especially in the high-resource conll 2003 ner dataset.Therefore, we chose to focus specifically on ner and extractive qa as typical use cases of encoder-only models.\",\"The benchmark for language identification for the most part contains clean sentences.Data from the real world might be noisy.A better representative benchmark might be useful for such use cases.However, the use cases captured by this benchmark should suffice for the collection of clean monolingual corpora.This also represents a first step for many languages where no lid benchmark exists.The use of synthetic training data seems to create a gap in performance due to divergence in train\\u002ftest data distributions.Acquisition of original native romanized text and methods to generate better romanized text are needed.Note that the romanized lid model does not support dogri since the indicxlit transliteration model does not support dogri.However, since dogri is written in the devanagari script using the transliterator for hindi which uses the same script might be a good approximation to generate synthetic training data.this work is limited to the 22 languages listed in the 8th schedule of the indian constitution.Further work is needed to extend the benchmark to many more widely used languages in india.\",\"We note several methodological limitations with our experiments.First, since the evaluation materials were manually crafted, there is a rather small number of items.Small evaluation sets can introduce issues of statistical power and introduce bias based on lexical items.\\u2019s experiments; and in practice, there is enough signal to distinguish between the tested models.Second, we only evaluate models on englishlanguage materials, and some of the tasks were designed based on norms of communication and social interaction in western cultures.third, aside from the openai api models, we were only able to test models with \\u226411b parameters due to limited computational resources.Models with parameter sizes between 11b and the size of text-davinci-002 could exhibit qualitatively different behaviors.Finally, we emphasize that it is impossible to predict how models will respond to an arbitrary input.Therefore, we caution against extrapolating from our results and expecting that models will behave \\u201cpragmatically\\u201d in downstream applications.This is especially true for models behind the openai api, and text-davinci-002 in particular, for which very little is publicly known about the training protocol.\",\"A large limitation of this work is the ubiquity of english.With the exception of the afriberta , the remaining plms in this study all included english in the pretraining data.As a result, it is difficult to disentangle the benefits of including relevant languages in the pretraining data, from the general benefits of including english in the pretraining data, for processing code-mixed text.In a similar vein, our work is limited in that we did not try other non-english monolingual plms.For the indic languages, this is because monolingual indic plms typically use the devanagari script, but the datasets in this paper are constrained to using the latin script.For naija, we likewise did not experiment with monolingual models for the other relevant nigerian languages; to our knowledge, most publicly available plms for hausa, yoruba, and igbo seem to be created through continued pretraining with monolingual data over existing multilingual plms.Thus, experimenting with these models still does not strictly control for english and other languages.Beyond plms, another limitation of this work pertains to the error analysis, which hinges upon currently available lid technologies., most lid technologies operate on a document level, and thus intra-utterance lid is still an open problem.For code-mixed language, the lack of robust lid puts limits us to coarser-grained analysis of the data.Ideally, a finer-grained partition of the data could be useful in determining the extent to which a plm\\u2019s knowledge of english enables performance on downstream tasks.\",\"One limitation of this work is the lack of a more comprehensive study of langid methods, which could impact slightly the results.Another limitation is the number of non-bil languages, which can be increased to more than 1,000 languages with the datasets proposed in.Furthermore, the use of wikipedia data limits the search of samples, since all pages are supposedly written in portuguese.So, relying on a broader set can bring a more realistic estimate on the in-the-wild search for data.In addition, a major limitation of this work is the lack of inspection of the results with native speakers.We are already engaging with one mbya guarani community, but it is quite difficult to extend such engagement to other communities.\",\"Limitation, our work focuses on dialect robustness and only briefly evaluates dialect awareness.our encouraging preliminary results lead us to urge researchers to consider and improve the dialect diversity during pretraining.Limitations besides the limited size of the evaluation corpora and a brevity of the exploration of dialect awareness that we point out as limitations in \\u00a78, we again acknowledge the data acquisition strategy as another limitation of our work.Our data acquisition of dialects requires country codes, which exclude many dialects.Build a dataset of tweets that are likely to include a high density of african-american english by linking geolocated twitter data with demographic data from the u.however, this approach is limited to dialects that have strong geographic associations within the united states and which correlate with census demographics like race.build a dataset of city-level arabic dialects, again relying on twitter geolocation.An alternative approach that does not rely on geolocation is to translate existing corpora into multiple dialects.However, this is labor intensive and therefore difficult to scale up to the amount of data needed for pretraining.\",\"A key limitation of this work is the dependence on a machine translation system to get highquality translations and annotation projections of the dataset.Depending on the availability of language resources and the mt model quality for a given language pair, the translations we use for training and evaluation may be inaccurate, or be affected by translationese, possibly leading to overly optimistic estimates of model performance.In addition, since the annotation projection for relation arguments is completely automatic, any alignment errors of the mt system will yield inaccurate instances.Alignment is at the token-level, rendering it inadequate for e.due to the significant resource requirements of constructing adequately-sized test sets, another limitation is the lack of evaluation on original-language test instances.While we manually validate and analyze sample translations in each target language for an initial exploration of mt effects, these efforts should be extended to larger samples or the complete test sets.Finally, we limited this work to a single dataset, which was constructed with a specific set of target relations , from news and web text sources.These text types and the corresponding relation expressions may be well reflected in the training data of current mt systems, and thus easier to translate than relation extraction datasets from other domains , or other text types.The translated examples also reflect the source language\\u2019s view of the world, not how the relations would necessarily be formulated in the target language.\",\"We note a few limitations of our work: a) while we systematically investigate the choice of in-context examples for both in- and out-of-domain settings for higher-resource language pairs , it is unclear how this in-context ability of the plm varies for the lowerresourced language pairs; b) we only experimented with one pre-trained language model, xglm.5b to result in better translation quality than bloom7b under the same settings.However, further investigation is required to understand how these results vary across different model scales; c) we analyze different orderings for the few-shot task-level prompts but only examine limited sets of ordering for the example-specific prompts.\",\"The similarity of tms is an important factor influencing the translations of tmplm.However, high-similarity tms are not always available in practical applications.It is worth studying methods to make use of relatively low-similarity translations in llm-based translation systems.\",\"We strived to make this work as accessible and applicable as possible.However, as with any other research effort, it suffers from several limitations stemming from preconceived assumptions.We believe that the most important limitation of our work is the assumption of the existence of a pre-trained multilingual language model, to be used as an encoder, that supports both the desired source and target languages.Though most modern multilingual language models support over a hundred languages, with over 7000 spoken languages in the world, the vast majority of languages remain unsupported.That being said, language models are trained in an unsupervised manner, meaning that only unlabeled data is required for training purposes.As such, a suitable encoder could be trained provided there is access to enough unlabeled data.This leads to what we consider to be the second biggest limitation of our work: the assumption of the availability of unlabeled target-language data.In general, raw unlabeled data is easy to obtain for most languages.However, it can represent a challenge for extremely low-resource languages.In these special cases, training an effective encoder can be an impossibility which, in turn, limits the applicability of our approach.Other limitations stem from our constrained time and computational resources.Our method requires a gpu with a largeenough memory to fit the transformer-based encoder which is usually more than what a personal computer gpu provides.Depending on the dataset and selected batch size, our model requires between 15 and 32 gb of gpu memory.We performed all our experiments on a tesla v100 gpu with 32gb.Finally, additional experiments on a more diverse set of source\\u002ftarget language pairs could certainly provide a more comprehensive overview of our method\\u2019s strengths and weaknesses.\",\"Although multilingual, the constructed open kb is limited to the sampling of the chosen six languages.We do not know how well the system will generalize to various language families that have not been considered here.Further, even among the languages considered, the performance of even the best-performing systems, as measured through is still in the low 20\\u2019s.Therefore the models are not yet ready to be deployed for real-world applications.\",\"This work has certain limitations in terms of the scope of the experiments and what can be reliably inferred from them.Our dataset contains notes that are predominantly from anglophone countries.However, there are less than 20% of the rows that originate from non-english speaking regions.They might contain words in other languages , and although the disease names are usually rendered similarly as english, our models are pretrained on english and their ability to process other languages is therefore limited.Another issue is the relatively short length of these notes.While some notes span a few sentences, most are very short and no more than 4\\u2212 5 tokens in length.This hampers the ability of a contextualised model to derive meaning from the context around each word and limits the power of attention-based architectures that are well-suited for larger contexts.In this preliminary study we only targeted one condition and looked at binary classification.The natural step towards a more inclusive experiment would be to consider other conditions and also use multi-class classification setups where a more finegrained scheme is used to classify a condition.we did have access to multi-class annotations for our current training set, however, one major issue is that the cancer-positive cases are a small percentage of the entire rows and among the cancer types themselves, there are types that occur only once or twice and the rest belong to more frequent classes.This would make it harder for the model to learn infrequent classes.We plan to augment the annotations over time to be able to conduct experiments in scenarios beyond binary classification and cancer alone.The issue of negation was further complicated in this work by a few cases where the note had been classified as cancer positive because the doctor had identified a history of this condition in the patient but had ruled out or downplayed the possibility of cancer at the present time.Distinguishing a current co-morbidity of cancer from a past history of cancer would introduce further complexity and this work does not attempt to address that.\",\"All experiments are conducted on data containing exclusively english language.Consequently, the results may differ in particular on morphology-rich languages and\\u002for non-inflectional languages.Similarly, all presented techniques expect languages to use latin characters.Therefore, our method first needs to be adapted in order to be used on languages using different characters, such as cyrillic languages, korean or persian.Using the bert-classifier in conjunction with al is resource intensive.Loading the \\\"bert-baseduncased\\\" model from huggingface along with one of the datasets with a batch size of 24 requires around 22gb of gpu memory.We train for a maximum of 15 epochs, requiring up to 1 gpu hour, depending on the size of the dataset and the length of individual inputs.As such, a single al experiment requires approximately 20 gpu hours to complete.In addition, computing the al strategies requires up to 2 gpu hours for the alps strategy and up to 1 gpu hour for the dal strategy, depending on the dataset.The subword strategy is calculated only once and used up to 1 gpu hour.In total, our ral experiments take around 60 to 100 gpu hours to complete.It is important to note that the rl itself is not expensive and does not require gpu, therefore ral can easily be adapted to scenarios with low computational resources by employing a different classification model as well as using al strategies that do not rely on large pre-trained language models.\",\"While the multilingual models employed in this study are capable of processing a range of languages, their performance is restricted when it comes to code-mixed sentences that feature a combination of roman urdu and english.This limitation suggests that the models may yield comparable results when dealing with similar language pairs.Additionally, the effectiveness of utilizing chatgpt\\u2019s api to translate code-mixed sen- tences into english has not been conclusively established, and thus, it remains uncertain whether this approach represents the optimal solution.\",\"The presented results only apply to the english language.Both our benchmark dataset and the baseline model target the english language exclusively.Special text sources such as instant messaging or speech-to-text are likely under-represented in our benchmark test set; therefore, we did not evaluate classification performance in those domains.Since we used roberta as the base model, our model inherits the same limitations.Specifically, the length of input sequences is limited to 512 bpe tokens, and additional pre- and post-processing is necessary to run predictions on longer inputs.However, we did not evaluate prediction aggregation methods or classification performance.\",\"This work complemented previous analyses on the link between the linguistic and psychological accuracy of a neural language model by expanding the language sample to ten typologically distinct languages.However, our sample of neural language models was limited with respect to the literature focusing exclusively on english.This problem cannot be overcome at the present state of affairs, since there are very few available massively multilingual auto-regressive language models, and the only one with sufficient coverage of our language sample was xglm.This problem is an expression of a general difficulty in nlp to conduct experimental research on low-resource languages, due to the extreme skewness in the distribution of available resources.However, we are confident that future developments in natural language engineering will support an additional test of our hypotheses with a more representative sample of models.\",\"Our work focuses on the problem of spelling variation in japanese.The japanese writing system is the most complex of any modern writing system and presents a unique range of issues that impact speech and language technology, one of which is the spelling variation discussed in this paper.this may be particularly an issue in lan- guages that do not have a standardized writing system\\u2014e.colloquial arabic dialects\\u2014and where a large amount of spelling variation is often observed.However we have not evaluated the approach on this sort of data.Our evaluation system is not open-sourced due to the propriety lexical resources, text normalizer and kana\\u002fkanji translators.The text normalizer could probably be replaced with, e., the open-source mecab system, though we expect that performance would be degraded.Similarly our lexical resources could potentially be replaced with publicly available japanese dictionaries such as jmdict , but again performance would probably suffer.Note in particular that unlike cjki\\u2019s japanese orthographic dictionary, jmdict entries have not been carefully curated to indicate which spellings are interchangeable, and which are, rather, words with the same reading but distinct meanings.An informal manual evaluation we performed on potential spelling variant pairs that were extracted from jmdict entries nominally representing the same word sense, revealed that about 92% were valid variant spellings, but that the rest were either wrong, or at least unclear.\",\"First, most articles are crawled from the us and uk presses.This means the crawled data is englishonly and regionally biased, limiting the scope and the diversity of issues.Extending our work to other languages and more regionally-diverse presses will be helpful for reducing such bias in our dataset.Second, we suspect that there will be a nontrivial annotation bias in our dataset.We are concerned with the fact that all of our in-house annotators share the same cultural background and similar personal interest.Furthermore, given that claimdiff-w is aiming to catch the subtle differences in the nuances of these professional news articles, it is very challenging for different annotators to have a common view, especially compared to claimdiff-s.Third, since claimdiff is a sentence-level comparison task, it currently does not give information about the surrounding context of each sentence.This means inter-sentence dependency such as coreference often cannot be resolved.One way to work around this is to give an access to the full articles for each claim pair, but we have refrained from it in this work for simplicity.Fourth, the size of claimdiff is relatively small compared to other fact verficiation datasets.This is mainly because its annotation process is quite challenging and requires a substantial amount of time.\",\"\\u2022 our work focuses on data from one platform, kialo, which contains cleaner and higher quality arguments from a diverse range of topics and domains.\\u2022 the vast majority of data available is english which makes conducting and evaluating multilingual experiments not feasible even with language transfer.\\u2022 the dataset used in the training and evaluation has only one correct position although there might be multiple suitable parents.Given the large scale of the data and the huge number of nodes per tree, annotating all suitable parents would\\u2019ve require a very-large-scale unfeasible annotation.This could be investigated in future-work with the support of our models.\\u2022 the design of our annotation study does not take into consideration the structure of the tree.This might have made the task more challenging for the annotators.Reconstructing or representing the tree structure without revealing the actual parent is challenging when limiting the candidate parents to 10.\\u2022 although small models are shown to perform relatively well and are recommended to use when computation resources are limited, the models that perform, in our experiments, on par with humans are large models that are costly to train.Employing parameter efficient fine-tuning methods might be of interest here.\\u2022 we use only manually designed templates as a simple approach that required no extra training or engineering.Including prompt-based fine-tuning might be also of interest to investigate in combination with contrastive training although language modeling training would require more computational resources.\\u2022 our task definition excludes the prediction of pro\\u002fcon relation as less important, but the pro\\u002fcon template information might be useful for this.More evaluation and analysis is needed to verify that.\\u2022 extra analysis that was out-of-scope to include in this paper might be of interest: e.the effect of topic, the degree of a node, and semantic similarity to siblings on model or human performance.\",\"Our training schedule introduces a language discriminator loss to impose constraints on the intermediate translation in the back-translation period.The experimental results suggest that our method can alleviate the copying problem when the involved languages are distant language pairs or lack training data.However, for language pairs that are not distant, and especially high-resource languages, our model does not show improvement over the baseline.Due to time and resource limitations, we do not further explore whether the optimal weight for the language discriminator loss can have a connection with the size of the dataset and the involved language pairs.For example, for wmt en-de or en-fr pairs, the languages are not distant language pairs and therefore we might obtain better results if the weights are slightly smaller.We believe that future research could explore this direction: to adapt the weight to different language pairs and the size of the training data.In addition, we do not conduct hyperparameter search for other hyperparameters, instead directly using suggested values.In this work, we propose a novel training schedule that tries to address the copying problem, which is common among distant language pairs in unmt.We experiment with high-resource languages english, german, french, russian and chinese, and low-resource languages including gujarati and kazakh.The training data we use is monolingual text extracted from online newspapers and released for the wmt series of shared tasks.As far as we know, all the monolingual corpora do not contain any metadata and therefore it would be unlikely that anyone can use the concerned data to attribute to specific individuals.\",\"One limitation of our work is the experimentation only with languages with shallow orthographies, i.the results might vary for deeper-orthographies languages.Although we took extra care to verify our conversions are correct and complete, and designed the rules to be as comprehensive as possible, automatic rule-based processes in languages may not be 100% perfect and some corner cases may introduce errors.These errors may propagate to affect the numerical results.To mitigate this issue, when ambiguities in determining a target phoneme in a given language occur, we purposefully select the values that occur more frequently in the unimorph data of that particular language.\",\"We would have liked to evaluate the generalization of our cross-lingual approach on more languages.For instance, we partially rely on machine translation models for chinese-to-english translation.Available translation models for other language pairs, especially from\\u002fto low-resource languages have much lower quality, and it would be desirable to measure the effect of that in our experiments.The ontology used for new languages is derived by translating the chinese ontology.As a result, the entities are not localized.Creating local ontology requires manual effort as one would need to identify websites or databases for scraping or collecting the entities.Once the local entities are collected, we can automatically replace translated entities with local ones to localize the dataset.Another limitation is the lack of human evaluation for agent responses.Bleu score does not correlate well with human judgment , and ser only accounts for the factuality of the response but not grammar or fluency.\",\"This paper covers only four nlp tasks.Certain other tasks requiring more background knowledge may show different results.We suggest recruiting language learners when native speakers are not available, but recruiting learners may also be difficult for languages that are not popular for learners.Our results are based on a relatively low number of participants, as we chose to cover three different languages to show generalizability across languages.Many factors that may contribute to the results remain, such as the order of the batch of annotation questions with respect to the question difficulty level.\",\"Some language families in africa not covered for example, khoisan and austronesian.We performed extensive analysis and experiments on niger-congo languages but we only covered one language each in the afro-asiatic and nilo-saharan families.News domain our annotated dataset belong to the news domain, which is a popular domain in ud.However, the pos dataset and models may not generalize to other domains like speech transcript, conversation data etc.Transfer results may not generalize to all nlp tasks we have only experimented with pos task, the best transfer language e.e wolof, may not be the same for other nlp tasks.\",\"While our proposed projection techniques often improve cross-lingual transfer, the choice of the projection layer and the projection probability in the case of random projection are hyperparameters that vary across tasks and languages.Our ongoing work involves identifying a mechanism via which we can parameterize these quantities, enabling the model to directly learn the optimal layer and probability values for projection.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"0_translation_languages_multilingual\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"0_translation_languages_multilingual\"],\"textfont\":{\"size\":12},\"x\":[14.64406967163086,13.7753267288208,13.644251823425293,14.512482643127441,14.60434341430664,13.799180030822754,14.048680305480957,14.41437816619873,13.986284255981445,14.437543869018555,14.585538864135742,13.614507675170898,14.448748588562012,14.403486251831055,14.175288200378418,14.02754020690918,14.002740859985352,14.032350540161133,14.266409873962402,14.584813117980957,13.954750061035156,14.19575023651123,13.402664184570312,14.2051362991333,14.63054084777832,14.159987449645996,14.246871948242188,13.800273895263672,14.770191192626953,14.00381088256836,14.436531066894531,14.6339111328125,14.181230545043945,14.662309646606445,14.04929256439209,14.544872283935547,14.021499633789062,14.338593482971191,14.909297943115234,14.868453025817871,14.918893814086914,14.942896842956543,13.970205307006836,13.473339080810547,14.13626766204834,14.841829299926758,14.80643081665039,14.689885139465332,14.94958782196045,14.839753150939941,14.74835205078125,13.815553665161133,14.83078670501709,14.848963737487793,13.969526290893555,14.714621543884277,14.896309852600098,14.788846015930176,14.296158790588379,14.050692558288574,13.496996879577637,14.152851104736328,14.123931884765625,14.418935775756836,13.916388511657715,14.507061004638672,14.854994773864746,14.925897598266602,14.27055835723877,14.141298294067383,14.836803436279297,13.794408798217773,14.0542573928833,14.582378387451172,14.146570205688477,14.172537803649902,14.365836143493652,14.916523933410645,14.0220365524292,13.935368537902832,14.667534828186035,14.786911964416504,14.262618064880371,13.670228004455566,14.467340469360352,13.881732940673828,14.696356773376465,13.77252197265625,14.885176658630371,14.33213996887207,14.57826042175293,13.699742317199707,13.82735538482666,14.47417163848877,13.721490859985352,14.148058891296387,13.662017822265625,13.737567901611328,14.845695495605469,14.041550636291504,14.721123695373535,14.513903617858887,14.733917236328125,14.465664863586426,14.059483528137207,13.665475845336914,14.07945728302002,14.077699661254883,14.077261924743652,13.860182762145996,14.536134719848633,14.264437675476074,14.510618209838867,14.624363899230957,13.919905662536621,14.858956336975098,14.54011344909668,14.703385353088379,14.601969718933105,14.70383071899414,13.997807502746582,14.017738342285156,14.842256546020508,14.818700790405273,14.187997817993164,14.124794006347656,13.664078712463379,14.893455505371094,14.17477798461914,14.874664306640625,14.487317085266113,14.1564302444458,14.04804801940918,13.998133659362793,13.853947639465332,14.29643726348877,14.126228332519531,14.256382942199707,14.869417190551758,14.676034927368164,14.86535358428955,14.1512451171875,14.179442405700684,13.952130317687988,13.88955020904541,14.368393898010254,13.788641929626465,13.843708992004395,14.487235069274902,13.517558097839355,13.635135650634766,14.743097305297852,14.233297348022461,14.423409461975098,13.556717872619629,14.206621170043945,14.614326477050781,14.29736042022705],\"y\":[2.1206278800964355,2.0459847450256348,1.0821186304092407,1.9061638116836548,2.2898871898651123,2.1502137184143066,1.4573640823364258,1.72158682346344,0.9691416621208191,1.3624459505081177,1.9967553615570068,0.9485470056533813,1.5997060537338257,1.2797060012817383,1.4270786046981812,1.17060387134552,1.54963219165802,1.0400104522705078,1.258195400238037,1.9615994691848755,1.6879578828811646,1.416810154914856,0.8636229038238525,1.8551007509231567,2.149442434310913,1.4932076930999756,1.4632320404052734,1.7913613319396973,2.2355804443359375,1.6492196321487427,1.5885026454925537,1.8786247968673706,1.840674877166748,1.8817319869995117,1.0723499059677124,2.2649857997894287,1.5397127866744995,1.5384314060211182,2.2257626056671143,2.1094021797180176,2.1217234134674072,2.009085178375244,1.541926622390747,0.8392106890678406,1.7269001007080078,2.2394025325775146,1.8812440633773804,1.9169468879699707,2.0169363021850586,2.048081159591675,2.2320406436920166,0.9665337204933167,1.8138630390167236,2.1290907859802246,2.0803792476654053,2.2612500190734863,2.1202735900878906,2.3251454830169678,1.2251992225646973,1.5076688528060913,0.9428068399429321,2.305506706237793,1.163865566253662,1.9106602668762207,1.7201085090637207,1.3695008754730225,1.9992640018463135,2.080075740814209,1.0548354387283325,1.5940330028533936,2.3740813732147217,1.2510448694229126,2.3582088947296143,2.0957019329071045,1.8701030015945435,2.137420892715454,1.7418690919876099,2.157238006591797,0.9063822627067566,1.5669087171554565,2.1826133728027344,2.1559159755706787,1.5395188331604004,1.0523676872253418,2.4036147594451904,1.1422173976898193,2.2715327739715576,1.0667442083358765,2.1383821964263916,1.6993260383605957,1.95530366897583,1.0322710275650024,2.1408193111419678,1.4082081317901611,1.0292414426803589,0.9214536547660828,1.0053393840789795,2.0513033866882324,1.8732420206069946,1.4551175832748413,2.235825300216675,1.3804298639297485,2.203396797180176,1.7070815563201904,1.7912077903747559,1.9604227542877197,1.4727799892425537,1.1691219806671143,0.8451208472251892,1.8480911254882812,1.920552134513855,1.013845443725586,2.1904807090759277,1.8552864789962769,1.6870639324188232,1.9570977687835693,1.39228093624115,2.2386980056762695,1.9726945161819458,2.1459968090057373,1.7665742635726929,1.2387127876281738,2.1155152320861816,2.139789581298828,2.1029434204101562,1.1331547498703003,1.4177722930908203,2.106031656265259,1.8243802785873413,2.249870538711548,1.4184317588806152,1.6527878046035767,1.0737468004226685,1.5156282186508179,1.2490074634552002,1.4748045206069946,1.2277036905288696,1.2507392168045044,1.9383478164672852,2.058555841445923,1.9631752967834473,1.845572829246521,1.6127843856811523,1.7056900262832642,1.8626819849014282,1.4776885509490967,1.8465851545333862,2.1766676902770996,1.410274624824524,0.8846659064292908,0.9953603148460388,2.210970401763916,1.8267228603363037,1.8168041706085205,0.9271969199180603,1.3551487922668457,1.9063857793807983,1.6928439140319824],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Although our dataset presents a significant advancement over previous benchmarks, it is still limited in that it only contains entities already known to wikidata.One could argue that the very long tail is what is even beyond wikidata.In the second stage, our method harnesses an lm pre-trained for entity disambiguation.Therefore, our methodology, in its current form, cannot predict objects that are not already known to that lm and its underlying kb.\",\"A limitation of this approach is the trade-off between completeness and noise in the training data.While our method using keywords to extract text from wikipedia is effective, implication likely contains redundant sentences that cannot improve the model\\u2019s logical reasoning capability.A better rule-based or neural model might be able to extract a better corpus with potentially higher computational costs.Additionally, using pos tagging limits the application of this approach to languages with well-defined pos taggers.Switching to a more universal semantic tagging system can potentially alleviate this.\",\"In this paper, we focus on efficiently and accurately predicting missing links in kgs using low-dimensional features and binary classifiers.Greenkgc can achieve impressive efficiency during the inference stage and can be applied to various platforms with memory constraints because of its superior performance in low-dimensional space.However, the whole training process of greenkgc still requires high-dimensional pre-trained embeddings as initial features.Therefore, it may hinder greenkgc from being trained on resourceconstrained platforms from scratch.In addition, the current greenkgc model is proposed under a transductive setting, where we focus on a fixed entity and relation set.The generalizability of the few-shot learning capability on greenkgc is yet to be explored.The above-mentioned two limitations can be addressed by leveraging textual information in kgs.In recent years, text-based kgc models , which take advantage of entities\\u2019 names and descriptions to obtain features, are more and more popular.We may extend greenkgc using word embeddings from pretrained language models as initial features to overcome the current limitations.In addition, continual learning on the classifiers , which aims at learning new training samples without forgetting the old training samples, i.catastrophic forgetting, is also an active research topic.Thus, greenkgc can incorporate such techniques to improve its generalizability to new data.\",\".5 performs considerably better than gpt-3 in answering factual questions about object location changes.for other tasks for which pretraining on code seems to be beneficial.Establish a fully causal link between entity tracking capacities and high performance on our task.Entity tracking is a high-level linguistic behavior and many other capacities are necessary for achieving high accuracy on our task.Therefore, we cannot rule out that differences in some other capacity, such as interpreting sentences compositionally , are the main driver for the differences in behavior we see across models.A possible criticism of our setup is that it requires short-term memory capacities that exceed the memory capacities of most, if not all, humans.That is, if we presented humans with the same input as the model, we would not expect them to be able to keep track of the contents of all 7 boxes due to memory limitations.Therefore we are potentially expecting models to do super-human entity tracking, a setup that has been criticized for model evaluations of other linguistic abilities.We nevertheless believe that our task is justified given the architecture of the evaluated models.Transformer-based models can look back to any token in the entire input sequence within their context window, so a proper comparison between humans and models would be to present humans with the full description in written form and let them re-read the description after being prompted to state the contents of a box.While we did not formally evaluate whether humans have this ability on a larger population, we personally did not have any trouble tracking the contents of boxes when we had access to the written description.Relatedly, we designed our task such that the entire description fits within the context window of pretrained language models.However, as we mentioned in the introduction, entity tracking is an important ability for understanding long contexts and given the limited context window, our results do not apply to texts whose length exceeds a model\\u2019s context window, and likely different model architectures will be necessary to perform proper entity tracking for longer texts.5 models as well as finetuned t5 models can track entities in our task with higher accuracy than a strong random baseline, our results also indicate that this behavior is not very stable once several operations act on an entity.Our results should therefore not be taken as justification for using these models for critical applications where high accuracy is needed.Lastly, we only evaluated english models in this work.Given that we showed that even without high lexical overlap between the training and evaluation examples, models can keep track of entities to some extent, it seems likely that our results also apply to other languages.However, whether this actually the case remains an open question.\",\"First, as we do not rely on specific corpora and avoid the shortcomings of extractive methods, we also lose their advantages.The typed egs generated by our tp-egg is strongly related to the seed predicates and training data of generation modules, while extractive egs can generate domainindependent egs from large corpora and do not require supervised training data to a considerable degree.Second, the edge calculator w is time- consuming even we can control the scales of output egs, as the edge num |e| will be relatively large for tp-egg to generate powerful egs.Furthermore, how to effectively select seed predicates still remains a difficult problem which has not been discussed thoroughly in this work by using the validation datasets.\",\"The main limitation of our work is that we can not use a unified model to complete the zero-shot entity and relationship extraction tasks.Specifically, our method trains two models, dsp-zsner and dspzsrc, to extract the entities in the text first and then classify the relation of each pair of entities.This method needs to train and store two models, which is troublesome to maintain in practical applications.In addition, although our method has dramatically improved the inference speed of the previous prompt method, the method still affects the reasoning speed of the model.In the followup works, we will be committed to solving this problem.\",\"While the proposed methods offer promising results in improving the performance of knowledge graph-based language representation models, there are some limitations to this work that should be noted.Firstly, the experiments were conducted on a limited number of datasets, and the results may not be generalized to other datasets or domains.Therefore, further experiments are needed to validate the effectiveness of the proposed methods on a broader range of datasets and nlp tasks.Secondly, the proposed methods require additional computation and may increase the complexity of the models.Therefore, it is important to consider the trade-off between performance improvement and computational cost when applying these methods in real-world applications.Lastly, while the proposed methods address some of the limitations of existing knowledge graph-based language representation models, they still may not capture all the contextual relationships and nuances of natural language, leading to potential semantic errors and reduced accuracy.Therefore, it is essential to continue exploring new approaches and techniques to further improve the performance of nlp models.\",\"A limitation of the proposed method is that our gazetteer is constructed only by dataset annotations.And it affects the gazetteer coverage in unseen cases., we will construct a larger gazetteer using external resources such as wikipedia or knowledge bases.another limitation is that the gazetteer contains many spans that are associated with multiple entity types.1 for example, the span \\\"london\\\" has type locationgpe in most cases, while it is sometimes labeled as type art-music.However, in our current design, given a named entity, there is no way to explicitly distinguish between different types.\",\"In this paper, we conducted investigation on mpnn-based kgc models.Mpnn-based models learn the node representations through aggregating from the local neighborhood, which differ from some recent path-based works that learn pair-wise representations by integrating the path information between the node pair.Moreover, we mainly focus on the kgc task which is based on knowledge graph, and thus other types of graph are not considered.Therefore, our findings and observations might not be applicable for other non-mpnn-based models and non-kgc task.\",\"Domain kgs are the premise of knowledgeda, while open and high-quality domain kgs may be rare in some domains.Therefore, the method will be limited in the domains without suitable kgs.Besides, we use a similarity-based method to map entity mentions in the text to the corresponding entities in the kg.Although this method performs efficiently, it ignores the problem of entity ambigu- ity.For instance, the abbreviation, cat, can stand for \\u2018catalase\\u2019 or \\u2018copd assessment test\\u2019 in healthcare.To address this problem, it is necessary to use contextual information to clarify the specific meaning of the mention.Last but not least, knowledgeda may be not good at tasks of paragraph-level texts and the efficiency will reduce.Because long texts probably contain more entity mentions and have more complex syntax, it is more difficult to retrieve the entities and acquire their relations from the kg.\",\"This work introduces a pseudo-entity recognition task to supervise the model learning overlap knowledge.Since no additional entity annotation is available, we manually create a mapping functionm, which maps each argument role r to an entity type.With the help of the mapping functionm, the eae label can be converted to the per label.However, because the annotation of the eae task is complicated, it is hard to avoid a few exceptional samples in the prior mapping function.Some entity words may be attached to impertinent entity types.For example, there is a triple of argument role, event type, and argument in rams\\u2019s movement.the \\\"artifact\\\" argument is mapped to \\\"object\\\" inm, but we expect \\\"two pilots\\\" can be mapped to \\\"person or organization\\\".We tolerate such exceptional samples, and the occasional noise has not affected the training of ape.\",\"Our experiments focus solely on english-language entity linking.Similar models have been trained to perform entity linking in multiple languages , but we do not consider performance beyond english.The issues faced in other languages are likely to be similar, but the multilingual element of other models might lead to different results.Further, how to select keywords in the multilingual setting is unclear.In addition, we are limited by the available annotated entity linking datasets.Given that we need a large amount of data to train these models, they are inherently reliant on wikipedia.These entity linking datasets are skewed towards specific types of matches, including ones that are frequently exact matches.The effectiveness of this model might change when trained on a dataset with different characteristics, even with a large amount of data.Finally, the computational resources required to train these models are large, and our final results do not reflect numerous other preliminary experiments.This restricts our ability to run multiple experiments, train models from scratch easily, and potentially leads to underfitting of our final models.\",\"Gdmm establishes a compelling starting point for dmel research.In spite of this, the proposed approach has several shortcomings.First, gdmm currently generates entity name within the entity candidate set, however, we saw how retrieval errors limit entity linking performance.Thus, how to work collectively with the retrieval system to diminish errors takes appropriate action.Second, how to handle large tables still remains under-explored.It is infeasible to represent a huge database with the table flattening technique.In practice, it is possible to filter out less likely candidates to compress the search space, but a more promising approach is to represent the table more efficiently.Gdmm also enables studies on more diversemodal tasks.New tasks can be easily framed based on the proposed architecture, such as visual question answering, grounded generation, and diversemodal commonsense reasoning.We believe that with more follow-up work on diverse tasks, this approach will turn out to be a more comprehensive generative diverse-modal framework.\",\"Despite showing impressive performance, our graph-based approach still has several limitations.The first one is related to the construction of the sentence graph.At present, we consider two sentences to be semantically related if they share similar nouns.But coherence can be achieved not only by describing similar entities but also by discourse relations.So it will be an exciting direction to incorporate discourse relations into the construction of a graph.The second one is that we implemented our method using only a plain gcn.Recent work has pointed out that the original gcn can be further improved with more advanced aggregation functions or attention mechanisms.So another interesting direction is to explore the benefits of more powerful graph neural networks for our method, which we leave for future study.\",\"We discuss here the limitations of the proposed promptner.First, although promptner performs well on flat and nested ner, it cannot recognize discontinuous entities.The discontinuous entity can be divided into multiple fragments, while each position slot of promptner can only fill one.A simple alternative is to expand the position slots in prompts to accommodate discontinuous entities.Second, named entity recognition requires pretrained language models with the essential ability to sense the structure and semantics of entities, which can enhance entity locating and entity typing in lowresource scenarios.However, since plms prefer to learn semantic rather than structured information in the pre-training stage, promptner needs to be warmed up by wiki training when applied to low-resource scenarios.Finally, since the number of prompts is determined during training, there is a limit to the number of entities that the model can recognize.If the number of entities in a sentence exceeds the pre-specified value when testing, promptner will perform poorly.\",\"This paper are based on the assumption that universal information extraction models have limitations, particularly with regards to over-reliance on label span boundaries and inflexible attention span length.Therefore, the proposed framework may be computationally and spatially expensive as it requires a more complex attention mechanism and additional computing power for training.Nevertheless, this limitation of the span-based uie model can be overlooked in comparison to that of the generative uie model, which uses a stronger language model.Additionally, the probability density functions explored in fsl are limited; thus, further research is needed to develop a more targeted strategy for adjusting the correct information distribution.\",\"Although our model improves upon state-of-the-art methods of bam by incorporating entity coreference and co-occurrence information, there are still some limitations to our model.First, it is not easy to apply our model to other domains where no coreference resolution tool is available.Second, the number of nodes and edges of the generated heterogeneous graph will become enormous if the documents are long and many entities are extracted, which requires more gpu resources.\",\"The proposed asymmetric shapley interaction value could estimate asymmetric feature interaction in explaining the prediction of deep models.There are two major concerns regarding the time complexity: estimation of marginal contribution and construction of hypergraphs.In computing value function we have to consider more permutations to reduce approximation errors.Also, before estimating the contribution of asymmetric interaction, interaction graph with different orders should be constructed.We could resort to effective approximation methods in computing marginal contribution and prior knowledge in building hypergraph.\",\"Although our proposal enjoys the advantages of validity and generality, there are still two major limitations.First, vlp cannot directly generalize to the inductive setting, since vlp is defined based on the score functions of transductive embedding models.One potential direction is to design an inductive reference selector for emerging entities.Second, how to efficiently select more helpful references for prediction is still an open challenge.We expect future studies to mitigate these issues.\",\".Firstly, our focus is confined to evaluating the specificity of predictions made by pre-trained language models for entity relations.despite this restriction, we consider this work as an initial attempt to highlight the concept of language model specificity.We believe it will stimulate further research into this crucial, yet under-explored, area.A second limitation is the scale of the models evaluated in this work.Given the swift evolution of large language models concurrent with the drafting of this paper, the models we examined are comparatively small., large language models may fail to answer a problem at the appropriate level of specificity.We thus encourage future investigations to delve into the specificity of these rapidly evolving, larger language models.\",\"Despite the remarkable improvement on complicated information extraction, there are still some limits of our method.First, due to the multi-round argument extraction modeling, we discard the parallelism in element extraction.Furthermore, the mdp process interacting with the dqn further increases the computational load of the extraction process.So compare to other methods, our framework is relative slow at the inference stage.Second, though our framework can be easily adapted to different extraction task with different schema, we still need an extra module helping identifying the relations in the instance beforehand.Because of the difference task definition and modeling , although recognizing them potentially implies order decision making, they are beyond the scope of this paper.\",\"This paper aims to investigate a more efficient and effective framework to incorporate the heterogeneous features of both text and graph knowledge.The extensive experiments demonstrate our framework has a superior performance in capturing semantics of input knowledge, thus beating all sota models.However, due to the time and resource limit, we could not conduct further experimentation to compare with promising frameworks in similar areas.we also cannot exclude some other factors which may affect performance.For example, we select bart as the base language model in this paper.In practical use, the latest language models may have better performance in this task.We have to leave the analysis of these factors to future study.\",\"There are two main limitations of our work: our approach requires a set of previously aligned predicate pairs as training data to achieve predicate alignment between different kgs, which limits the generalization ability of our method.In our experiments, since we manually aligned a set of equivalent predicates with arguments of types person and location between the english and chinese egs, we can only perform predicate alignment and entailment graph enhancement between the \\u003cperson,location\\u003e subgraphs of two egs.it might not be robust enough when dealing with entailment graphs of poor quality.\",\"In this work we have tested our approach using spanbert, a relatively small model when compared to, say, deberta_large or gpt.Spanbert has been reported to obtain state-of-the-art performance for relation extraction , but it is unclear if a larger lm would improve this semi-supervised learning setting.We use both surface patterns format) and syntactic patterns ) as training seeds, but our approach can only produce syntactic patterns as outputs.This is not ideal, since there is empirical evidence showing that the mixed representation for rules may provide better performance.For example, we can easily capture per_title relation with a surface rule such as \\u201c{obj_title} {subj_person}\\u201d, which simply looks for the two entities being adjacent.\",\".First, we introduce multiple knowledge sources to construct hkg, and encoding this knowledge through lm consumes more gpu resources.Second, some useful knowledge may be removed when retrieving knowledge from key entities optimized by dictionary vocabulary.Then, we experimentally demonstrate that the paraphrase descriptions are effective in improving the reasoning ability of the model, but due to resource constraints, we are unable to incorporate the paraphrases of all entities into hkg.Finally, our method uses the simpler transe algorithm when optimizing the knowledge representation using krl due to gpu constraints, which may not be able to model the complex relationships in hkg well.\",\"There are two limitations of the current must model.First, although pre-trained language models can potentially boost the performance in web information extraction, pre-train a must on web documents has its unique challenges.There are several possibilities for our future exploration.For example, we plan to pretrain a must model by incorporating html-specific tasks, such as masking dom nodes and predicting the relations between dom nodes.Second, our model focuses on web pages with single-object, where each target field only has exactly one answer.a movie listing page, there are different movie names corresponding to different movies on the page.However, methods like repeated patterns can be applied.\",\"This paper lacks a formalized analysis of the relationship among perturbed contexts\\u002fpretraining\\u002fmodel generalization.The validation of the perturbed context is limited to relation extraction.a more systematic evaluation on different nlp tasks is still excepted.\",\"Similar to many knowledge graph embedding models, our proposed method is yet to handle link prediction under inductive settings.One possible future extension is to leverage entity description information to generate textual features and use compounde as a decoder to handle unseen entities.Also, the affine operators we use are limited to translation, rotation, and scaling and this may limit the number of different relation patterns we can handle.also, because we use 2d givens rotation matrix, the embedding dimension setting needs to be a factor of 2.we can explore higher dimensional transformations such as 3d transformations and compare the modeling power.\",\"For hkg one-position link prediction tasks, hahe shows the best performance in all three datasets.However, because hahe is based on hypergraph learning, it improves more on the wd50k high quality hyper-relational knowledge graph link prediction dataset, and less on the wikipeople dataset where triples are the majority, so hahe prefers the fact with more arity numbers.for hkg multi-position link prediction tasks, it can be seen that our model is effective when predicting multiple missing auxiliary information, which is a frequent situation in practical applications.However, the prediction accuracy of our model needs to be further improved in the case of missing primary relations.\",\"May extend to other domains in this paper, we present a generic framework and evaluate the effectiveness of our proposed model jointprop on three public datasets.We may further extend the framework to various datasets in different domains.For example, ace05 in social networks, journalism, and broadcasting, as well as genia corpus in biomedical research.May extend to other nlp tasks our proposed model focus on two tasks, namely ner and re.We may extend our framework to include more information extraction tasks, such as coreference resolution and event extraction.Moreover, we may contract knowledge graphs from extracted structural information.\",\"We discuss the limitations of our method from three perspectives.First, our method is based on pre-trained language models, so compared to rule-based data augmentation methods , our method requires higher time complexity.Second, the entity matching process will discard sentences which cannot match entities in the entity list, which will affect the utilization of data.Third, our data augmentation method based on the pre-trained language models, whose generalization ability is limited since the augmented knowledge comes from the pre-trained language models.However, the knowledge in pre-trained language models is limited and not domain-specific.How to improve the generalization ability of the data augmentation methods is a future research work.\",\"Kgt5-context relies on the textual mentions of entities and relations.Therefore, it is only applicable to kgs that provide such information.Kgt5-context may be able to handle some entities without textual features when well-described by their neighborhood; we did not investigate this though.To use kgt5-context for prediction, the kg has to be queried to obtain context information, i., the one-hop neighborhood of the query entity.Kgt5context thus cannot be used without the underlying kg.The verbalized neighborhood of the query entity leads to long input sequences, which in turn may induce higher memory consumption and higher computational cost during training.Overall, training kgt5-context is typically more expensive than training traditional kge models, which can be tuned and trained efficiently.For inference, kgt5-context first samples relation-neighbor pairs for contextualization, and then samples possible answers from the decoder.These sampling steps can lead to variance in predictive performance.We found this effect to be negligible on wikidata5m, but it may be larger on other datasets.\",\"In principle our method is applicable in many domains, for example, one could use a biomedical knowledge graph instead of conceptnet in a relevant domain.However, in this paper we only evaluate the quality of our approach in argumentative tasks which require commonsense knowledge.Our approach is unsupervised, but its performance depends on the quality of the used knowledge graph and sbert model.Similarly, we only evaluate cckgs for english data, although our approach is not limited to english if one uses multilingual sbert models or a multilingual knowledge graph.Finally, our approach is purely extractive and hence, is limited by the coverage and quality of knowledge graphs.However, improving knowledge graphs is an active field of research and hence, high-quality and high-coverage knowledge graphs are to be expected.Furthermore, our extracted cckgs could be augmented with generative models if coverage in the knowledge graph is not sufficient.However, that would reduce the interpretability that our approach provides.\",\"The open information extraction methods may amplify the bias of the corpus by extracting any relation occurring in the data.The models with deep learning may learn the relation bias from the training corpus and extract those biased statements.To mitigate the effect of data bias, we try to balance the relations in constrained tuples and the ratio of constraints when constructing the cteb dataset.In addition, the utilization of external auxiliary information increases additional computation time.Our ian model has still achieved superior performance when the external auxiliary information is removed.\",\"Our model is developed to tackle the structural difference between the ontology and instance views of a kg.However, many modern kgs are multilingual, where different portions of a kg may not only differ in structure but also differ in the text semantics.How to jointly capture these differences remain unsolved.Also since box embeddings naturally provide interpretability to the granularity of the learned concepts, how to use the current model to discover unknown concepts from these embeddings is also challenging.\",\".To make our rhgh model effective on the kg with millions of entities, it is desirable to apply some graph chunking techniques, such as cluster-gcn , to reduce the size of the kg for our rhgh model to improve computational efficiency.Currently, our rhgh model treats each relation individually.However, relation paths consisting of multiple relations will contain more complex semantic information in kgs.Relation paths enable entities to obtain higher-order neighbor information, but it is also more difficult to align relational paths in different knowledge graphs.\",\"While the proposed methods are attractive due to their efficiency, explainability and not needing training data, the limitations are also manifold: the pipeline nature propagates all errors that occur.For instance, the dependency parser in use performs rather poorly on informal texts such as tweets.Further, our definition of positive and negative effect relations is quite shallow and does not always live up to the real world\\u2019s complexity.We only capture effect relations that are formulated explicitly within one sentence, and only one effect relation per sentence.Requiring the nodes to link to wikipedia might be too restrictive while not even truly solving the problem of filtering non-sense nodes.Both the low inter-annotator-agreement in our effect graph evaluation as well as the discrepancy of the crowds\\u2019 and the expert\\u2019s annotations make it hard to assess the correctness of the extracted effect relations.And lastly, while we showcase some generated explanations, we did not properly evaluate how reliable the approach is in finding reasonable explanations.Indeed, first results suggest that this approach of generating explanations works rather inconsistently, though the ranking helps to a certain degree.What one might consider another limitation is that we do not check the effect relations for factual correctness, which ultimately leads to contradictions and inconsistencies in the effect graph.While fact checking is a difficult and controversial task, we also purposefully decided against any form of fact or consistency checking.Each edge in the effect graph is meant to represent one effect relation exactly as it was expressed.Including critical effect relations in the graph allows for identifying, analyzing, and potentially disproving them.\",\".Problem modeling: new concepts appear yearly in the real world, but the current system cannot generate new concepts.Generally, the emergence of new concepts often comes from the fusion of mature technologies.Thus, we model the idea exploration as link prediction.Note that it is not the only pathway to brew new ideas, but we have verified the effectiveness and rationality of this approach in the experiments.In addition, plm can be taken as an implicit knowledge graph , which is capable of tackling uncovered concepts in the evolving concept graphs.We will continue exploring the potential of plm in knowledge discovery and innovation.Logic, correctness, and concreteness: although the verbalized ideas can deceive many experts, they may still lack logic, correctness, and details, especially in natural and exact sciences.It is also a challenge for natural language generation.We plan to use more academic corpus and introduce constraint to alleviate such problems.Temporal information: in plm-lp, we simply take the year information as a token in the input sequence.We conduct additional experiments to show that the temporal information is not sensitive to plm-lp, which can be attributed to the negative sampling and the nature of the strictly evolving network.Two birds one stone: the current system employs two different plms for link prediction and idea verbalization, respectively.The development of prompt learning reveals that most nlp problems can be regarded as generation problems.\",\".Rada framework only considers textual domain corpus as the datastore, although this has greatly improve the coverage of domain knowledge as texts are always relatively easy to collect.However, it is widely investigated that structured knowledge such as knowledge base can also serve similar purpose.And such resources are generally in higher quality and are easier to match.Therefore, it would be benefiting to further integrate such resources at certain scenario where kb is available.The other limitation regards to the scale of the rada implementation.As large language models have becoming increasingly powerful, they have demonstrated quite impressive capability in memorizing and recalling a wide range of background knowledge existed in the massive corpora they have been pretrained on.ethical statement we evaluate the proposed method on established and publicly available datasets.There is also no human evaluation involved.This paper is not concerned with the above ethical risks.When the proposed framework is deployed into domain specific production, the domain adapted language models might express ethical-related outputs, but just as any other language models do , and should be treated with according techniques to eliminate ethical risks such as bias, stereotypes.\",\"One limitation of the proposed method is that it does not consider domain-specific information to evaluate informativeness.The phraseness module has access to domain-specific knowledge, which are the phrases that occur in similar contexts, i.on the other hand, the informativeness module only employs a domain-general sentence embedding model to measure informativeness of phrases.Therefore, the integration of both domain-specific and domain-general information for the evaluation of informativeness may be worth further investigation.Another limitation of this work is that we only tested the proposed method on short texts.Therefore, it is uncertain of the proposed framework\\u2019s performance on long text documents.Handling long texts could be significantly more difficult than short text, as long texts contain much more information.The final limitation of this work is the absence of experiments on using different sentence embedding models to construct the informativeness module.Therefore, it might be useful to explore the impact of different sentence embedding models on keyphrase generation performance.\",\"A potential limitation of our experiments is the use of oracle validation labels instead of human manual annotation as in the real-world setting.However, all validation sets we used in our experiments were collected based on the manually defined seed set of entities and relations, carefully cleaned and augmented with manually labeled negative samples.another limitation of experiments that use established data sets and focus on isolated aspects of knowledge-graph construction is their detachment from the real-world scenarios.Indeed, in reality knowledge graph completion is done in a much more complicated environment, that involves a variety of stakeholders and aspects, such as data verification, requirements consideration, user management and so on.Nevertheless, we do believe that our method, even if studied initially in isolation, can be useful as one component in real world knowledge graph construction.\",\"The deep neural networks in rho uses feature extraction and vectorization to represent the texts.The model only detects the statistical regularities and quantitative relationships among the variables but can not see qualitative relationships, such as causality, hierarchy, and other abstractions.Although we leverage the response re-ranking technique, which improves the explainability of rho, the neural networks are undoubtedly still \\u201cblack boxes\\u201d to humans.Therefore, the faithfulness of generated responses can not be fully guaranteed.\",\"Entity knowledge propagation focuses on updating lms\\u2019 knowledge about emerging entities.However, there might be cases where knowledge about existing entities needs to be updated.We intentionally exclude these cases since they can easily become intractable due to their complexity.For example, an organization changing its name could theoretically reflect a large number of entities that have relations to that organization.By investigating model behavior when a lm encounters new information which is completely unseen during pretraining, we can experiment in a controlled environment.We find ample challenges unaddressed by current research even in this setting.Our experiments are conducted on english language models only.While we believe the results can generalize to multilingual models, it is conceivable that the internal representations of these models make them more or less amenable to the sorts of updating explored here.More work is needed to benchmark these techniques in broader settings such as with larger language models and newer parameter-tuning approaches.\",\"Although our qaar achieves better performance on inductive relation prediction, it still suffers from some limitations.First, for a give query we extract a k-hop subgraph without using any sampling method, which will require large gpu memory when the extracted subgraph is large.Second, our qaar does not leverage logical rules to enhance the performance which has shown useful in previous methods.We believe that our method can be further improved by incorporating logical rules.\",\"Of the existing ea method, the lack of interaction and heterogeneous embedding spaces, we propose a unified textual entailment framework for entity alignment called tea.We transform the origin relational triples and attribute triples of an entity into textual sequences and model the ea task as a bi-directional textual entailment task between the sequences of cross-kg entities.We propose two kinds of plm-based aligners to capture the fine-grained correlation between entities with two kinds of sequences in a unified manner.The entailment probability is used for measuring entity similarity and ranking the entity candidates.Experiment results on five cross-lingual datasets show that tea outperforms existing ea methods and enables the mutual enhancement between the heterogeneous information.Limitations despite that tea achieves some gains for ea, tea still has the following limitations: first, tea has a higher computation cost than the embedding-based ea methods in the re-ranking phase, since tea process entity-pair input for modeling the interaction between them.For reducing time costs, we adopt the confidence-aware reranking strategy to reduce the number of re-ranking samples and candidates.However, the inference time cost is still higher than the embedding-based methods.In addition, the candidate selection may be limited in some corner cases if the ground truth entity is not ranked in the top |c| similar entities calculated by entity embeddings.We will further explore efficient approaches which could cover the corner cases.Second, the alignment of relational information of tea requires the entity names to construct sequences.However, the entity names are not always available in some ea datasets, such as the wikidata kg in openea benchmark.In that case, tea can use the attribute sequences without entity names for entity alignment.Though tea w\\u002fo t r can achieve competitive performance as shown in table 3, it still limits the application of tea.We will further explore plm-based ap- proaches to align the relational information without the requirement of entity names.\",\"Currently, roht framework is restricted to incorporating kbs and text.In addition, a device with large storage space and memory is needed for the storage and usage of wikipeida and wikidata.\",\"Our work aims to uncover how the distribution of factual and contextual errors in referring expression generation varies based on the familiarity of entities.Our proposed experiments uncover this effect using pragmatics-driven heuristics.We need a more general deep-dive into what models \\u201cknow\\\" to estimate how language models handle known and unknown information differently, in a way that might even escape human scrutiny.\",\"We inject the medical knowledge graph into local texts for entity span representations enhancement.However, unlike most joint extraction methods, the proposed model is hard to be trained in a parallel way.Therefore, it is time-consuming to obtain a well-trained model.moreover, our model is adapted to chinese medical texts where a token usually means a character.Hence, there will be errors when aligning the entities from the knowledge graph with mentions from local texts.\",\"Our method cannot discover entities that are mentioned without disappearing contexts since we utilize such disappearance signals.Although our experiments focused solely on wikipedia entities, they did not sufficiently cover certain entities, such as stores and food products, for which disappearance is important for us.Therefore, it is necessary to collect and conduct experiments specifically for these entities.\",\"Since the unified graph is very large, it will take more time to construct the subgraphs before the first training.But after saving these subgraphs, there is no need to rebuild the subgraphs in the subsequent training process.On the other hand, the aligned entities among different kgs is a necessary condition for our proposed framework and otherwise, our model can not conduct knowledge transfer among the given kgs without an alignment model or other techniques.\",\"Of relying on lms\\u2019 parameters to memorize factual knowledge and to understand what factors affect factual knowledge memorization.Our results show that memorization has a strong correlation with entity popularity and that scaling up models on long-tail distributions may only provide marginal improvements.We also demonstrate that non-parametric memories can greatly aid lms on these long-tail distributions, but can also mislead lms on questions about well-known entities, as powerful lms have already memorized them in their parameters.Based on those findings, we devise simple-yet-effective adaptive retrieval, which only retrieves when necessary, using a heuristic based on entity popularity and relationship types.Our experimental results show that this method is not only more powerful than lms or previous retrieval-augmented lms but also more efficient.Limitations this work focuses on entity-centric factual knowledge and demonstrates that lms\\u2019 memorization is heavily affected by the popularity of the entities and the aspect of the entities being asked in the questions.It is important to emphasize that for running controlled experiments, we have relied on two synthetic datasets, and the extent to which our results apply to naturally occurring factual knowledge has not been firmly established.While we can be fairly confident about the relationship between scaling, retrieval, popularity, relationship type, and performance for the kinds of knowledge studied here, the effectiveness of adaptive retrieval will depend on many details of the question answering pipeline.Moreover, our work depends on a definition of popularity that is time-dependent and may not perfectly reflect how frequently entities are discussed on the web.To improve the effectiveness of adaptive retrieval.It is an open question if the same findings are applicable to other types of world knowledge such as commonsense.\",\"Obviously, the work presented in this paper is limited to transcripts of spontaneous conversations in english.Since we are investigating the problem of named entity recognition, we have to point out that there are practically no datasets of human conversations annotated with entity spans apart from swne, ontonotes and earnings-21, the three datasets used in our paper.These datasets are relatively small, and the distribution of the frequency of appearance of entity classes is extremely skewed, with several entity classes represented by a handful of examples.Another significant limitation of the results reported in this paper is the choice of metric.Following the common practice in the nlp community, we have chosen the f1 score as the primary metric of entity recognition.However, this metric is questionable in the context of ner recognition in asr transcripts because it is highly dependent on two factors: the wer produced by the asr and the definition of span alignment.Consider a gold transcript annotation \\\"johnb-person f.i-person kennedyi-person\\\" and the asr output with \\\"f.\\\" transcribed as \\\"eh\\\" annotated as follows: \\\"johnb-person eh kennedyb-person.\\\"\",\"Limitation, currently, we mainly evaluate mocl under the single-source cross-domain setting.We plan to further extend it to multi-source cross-domain settings.limitations we propose a sequence-level contrastive learningbased model-agnostic framework mocl to enhance entity type classification in cross-domain named entity recognition.in addition, due to the hierarchical structure of entity types between the source domain and the target domain, it would also be beneficial to adopt non-euclidean space to represent words for better learning the relative hierarchical relationship between entities.\",\"Regarding the wikipedia articles used for creating our dataset wikipedia table and image generation , some infoboxes may not follow the defined format and rules.This is because various users can freely edit infoboxes.Moreover, the html dump data published by english wikipedia is not based on recent information., our image generation task requires generating a cropped fixed-size square image instead of the original aspect ratio.In addition, a table in an infobox may contain cells unrelated to image generation, and thus it may be redundant for image generation.\",\"The paper has only focused on graphs with multitype relations.When magnn shows improvement over baselines, someone may doubt if ma-gnn will do well on single-type relation graphs.The limitations of the representational power of the ma-gnn model should be discussed more deeply.\",\".First, our model requires the retrieval of relevant structured and unstructured knowledge from different knowledge sources, which can be time-consuming.Using cosine similarity over question and fact embeddings can be a bottleneck for the model performance.Second, our model focuses on rich background knowledge but might ignore some inferential knowledge, which can be acquired from other sources such as atomic.Third, our model might not be applicable to low resources languages where knowledge graphs are not available.\",\"Our proposed kalm has two limitations: \\u2022 kalm relies on existing knowledge graphs to facilitate knowledge-aware long document understanding.While knowledge graphs are effective and prevalent tools for modeling real-world symbolic knowledge, they are often sparse and hardly exhaustive.In addition, external knowledge is not only limited to knowledge graphs but also exists in textual, visual, and other symbolic forms.\\u2022 kalm leverages tagme to identify entity mentions and build the three knowledge-aware contexts.While tagme and other entity identification tools are effective, they are not 100% correct, resulting in potentially omitted entities and external knowledge.In addition, running tagme on hundreds of thousands of long documents is time-consuming and resource-consuming even if processed in parallel.\",\"The theoretical results and the algorithm should be applicable for other knowledge integration models which encode target sentences and associated textual knowledge descriptions in mini-batches.However, this paper does not extensively apply the proposed method to various knowledge integration models to explore its efficiency and effectiveness.\",\"In our paper, we simply evaluate the performance of models to address the task of entity linking in simulated scenarios with noisy data.We cannot think of any potential risks of our work.\",\"Elaborated relation descriptions are the foundation of the matching-based methods to achieve superior performance.Although we have proposed some ways to enrich the entity information in the descriptions, it is still a promising direction to explore more diversified and effective ways to enrich relation description.\",\"Regarding our work, we summarize the following three limitations.Tldt is proposed under the assumption that entities in a sentence are independent of each other, that is, they do not overlap.In the case of overlapping entities, tldt cannot capture the label dependency of these entities.We tested tldt on four public datasets and these datasets contain a part of the same labels.This may help tldt to learn the general knowledge of the label dependency.We did not give experimental proof that if the label sets of the datasets all totally different, whether the tldt can maintain good robustness.To overcome the label discrepancy problem and generate adjustable hyperparameters, we model for the individual transition score in the specific task.However, the mechanism neglects the correlation between parameters, which limits the ability to update rule learning.\",\".1, although our annotated dataset enables the possibility of learning an extractive model that can be trained to predict the span of the text segments of interest from scratch, we focus on the more essential actioncondition dependency linkage inference task as we find that the srl extraction heuristic currently applied sufficiently reliable.more specifically, the extractive module can be supervised and\\u002for evaluated against with our human annotations on the text segment start-end positions of an article.The current system is only trained on unimodal and english instruction resources.Multilingual and multimodal versions of our work could be as well an interesting future endeavors to make.In this work, we mostly consider instructions from physical works.While certain conditions and actions can still be defined within more social domain of data.As a result, we do not really guarantee the performance of our models when applied to data from these less physicaloriented domains.\",\"We conducted extensive experiments with three datasets from different domains to substantiate the results thoroughly.We observe the best performance when we also leverage the body of the articles.So, we did not evaluate the performance on the datasets that do not have the full text of the articles.In our work, we provide a comprehensive analysis and present data augmentation strategies specifically to address keyphrase generation in purely resource-constrained domains.We do not expect any direct ethical concern from our work.\",\"As mentioned in chapter 2, since data is annotated by the author in person, it may include some human error in labeling.However, this can be resolved through a review of the labels and the publication of an amended version.In addition, the sentence data is obtained from the universal dependencies japanese-gsd data set, this is just one of eight ud japanese corpora and other ones could be used to expand the data set.Also, this research treats the single kanji homographs as the target, and the work could undoubtedly be improved by expanding the research to kanji combinations.As discussed in chapter 1, japanese is one of the languages that lack word boundaries.Therefore, the first interesting point will be that when a kanji homograph is in a kanji combination or phrase, which kanji homographs will tie together to make a word to create a word boundary with other kanji homographs.Then the second point is how the new kanji combination can affect the pronunciation selection of those kanji homographs.An anonymous reviewer suggests that we compare against the kanji disambiguation system embedded in mecab.Finally, due to time constraints, this research extracts n-gram features for the target kanji homographs.There will be other features that help analyze the context to improve the model performance.\",\"We only investigate representative methods of three widely-used el paradigms.furthermore, more auxiliary information in the biomedical domain can be introduced to address the nil issue we identify in this work.For example, a hierarchical structure exists for concepts in kbs in the biomedical domain.Therefore, nil may be solved by linking them to hypernym concepts in the partial kbs.We consider the hierarchical mapping between nils and in-kb concepts as a potential solution for performance degradation in partial kb inference.Users can obtain different entity-linking results based on their own kbs which have the potential risk of missing important clinical information from the texts.\",\"In this work we propose an uncertainty-aware bootstrap learning framework for joint extraction.Though it achieves state-of-the-art performance compared to other denoising techniques, unbed requires large training resources considering the ensemble loss calculated between two large plms and the probability variance calculated on the plm joint extraction model.we will also consider more complex relations between entities, e., relations beyond the sentence boundary, to fit in real-world information extraction scenarios.\",\"Due to the nature of deep learning, our method is less explainable than path-finding-based kg completion methods , which provide a concrete reasoning path to the target entity.Composing the path with multiple queries might be an applicable strategy that is worthwhile to investigate in order to extend our work on the kg reasoning task.For the link prediction task, we adapt the \\u201crecall and re-ranking\\u201d strategy from pkgc , which brings a trade-off between prediction efficiency and accuracy.as a common issue of existing kg completion models, the performance of our model also degrades when the input kg contains noisy data.The advantage of our approach in addressing this issue is that it can use both corpus-based textual information and implicit plm knowledge to reduce noise.\",\"While our approach effectively predicts the relationships between entities in a knowledge graph, there are limitations in the scope of knowledge graph resources that can be modeled.The knowledge graph contains a vast array of resources, including attributes, descriptions, and images, which are not easily captured by embedding-based methods, but can be effectively modeled using plms.To improve the compatibility of kgc with actual needs, it is necessary to consider a broader range of data types in the knowledge graph and develop complementary methods to effectively incorporate them.\",\"Our work still exist some limitations.First, we choose an entity typing system on the base of wikidata tags, however, the granularity of the typing system remains to be discussed.A system with too many types would introduce noise to long-tail types, while insufficient types would weaken the disambiguation ability of type similarity.Thus, building a type system with adequate granularity remains a challenge.Second, we combine the entity typing task with plm-based semantic encoders, which require a fixed type system and further finetuning.Integrating the entity typing task into the pretraining process may enhance the transferability of the model and remove the dependency on a fixed type system.our proposed dataset nel centers on ambiguous entities, whose type distribution may not remain the same with other datasets.A potential risk is that the model trained on nel may experience under-exposure of other entity types, which would damage their transferability and lead to undesired outputs on other datasets.\",\"We discuss here the limitations of the proposed diffusionner.First, as a latent generative model, diffusionner relies on sampling from a gaussian distribution to produce noisy spans, which leads to a random characteristic of entity generation.Second, diffusionner converges slowly due to the denoising training and matching-based loss over a large noise timestep.Finally, since discontinuous named entities often contain multiple fragments, diffusionner currently lacks the ability to generate such entities.We can design a simple classifier on top of diffusionner, which is used to combine entity fragments and thus solve the problem of discontinuous ner.\",\"One limitation of our work is that manner only explicitly utilizes the memory to enhance the performance of the entity typing module in target domain.However, we argue that the memory could also implicitly enhances the span detection module through the shared pretrained language model with entity typing module.\",\"While there is a lot of work on creating and making available large pre-trained language models for a range of languages, there is to our knowledge not that many knowledge graphs for other languages than english \\u2014 especially general knowledge ones, like conceptnet.This is a major limitation, as it restricts research to one single language and the structured representation of knowledge found in the culture associated with that specific group of language users.Creating commonsense kgs from unstructured text is a costly process that requires financial resources for annotation as well as available corpora to extract the graph from.\",\"Although our geometric embedding approach can handle a complete set of basic fol operators , the modeling of negation operator cannot narrow down the predicted answers to relevant topics of atomic queries.For example, one can expect the answers of this negation question\\u002fquery \\u201clist argentina players who are not lionel messi in world cup 2022?\\u201d to be any teammates of lionel messi.However, the current model is designed to return all elements in the entire entity set except for lionel messi, which have redundant objects.This is a common limitation not only in geometric-based models but in others using fuzzy sets representation.This is due to the fact that the modeling of negation operator is assumed to be the complement set of a questionable entity w.our hypothesis is that the expected answers should be narrowed into the complement set w.this special case is inevitable in a system using geometric representation that is closed under negation and conjunction, but not for disjunction.\",\"In this paper, we suggest incorporating textual and visual data from search engines for multimodal relation extraction.Despite the fact that the proposed model yields competitive results on the benchmark, it still has several limitations.Firstly, using a search engine is a feasible way to obtain related knowledge, but it also brings the issue of noisy evidence.Unrelated visual and textual evidence returned by the search engine may lead to incorrect predictions from the model.Additionally, not all the retrieved evidence is equally reliable, and sometimes sources may contradict each other.On the other hand, retrieval-augmented methods are slower than content-based counterparts, since retrieving evidence from the internet requires extra time.Therefore, it may not satisfy some of the time-sensitive scenarios.Lastly, evidence may be presented in different forms other than texts and images.For instance, structural information such as tables, info lists, and knowledge graphs also provide important contexts for identifying semantic relations.Humans are able to extract relevant information from these heterogeneous sources for inference, while our relation extraction system can only model and reason over textual and visual evidence.\",\"In this paper, we propose a novel set-wise framework to extract keyphrases globally.To verify the effectiveness of the new framework, we design simple yield effective neural networks for both the neural keyphrase set function and the keyphrase set extractor agent modules.In general, a complex neural network should yield better performance.Moreover, for the sake of fairness, our model adopts the same pre-trained language model as the recent state-of-the-art baselines.Actually, other pre-trained language models can be applied to our model, such as roberta.These pre-trained language models may yield better results, which also demonstrates that there is much room for improvement in our proposed framework.Therefore, we believe the power of this set-wise framework has not been fully exploited.\",\"Risks limitations: in this work we present a novel method to address data scarcity issue for relation extraction.first, similar to previous work , the current method assumes golden entity mentions to perform re that might not be the case in different applications.It is thus helpful to explore the method in a more realistic setting where entity mentions are predicted, e., using joint inference models to simultaneously extract entity mentions and relations in an end-to-end fashion.Second, our method is currently evaluated only for sentence-level re.finally, our method requires the generative gpt-2 model for data generation.To perform well, gpt-2 needs to be trained on large unlabeled datasets that might not be readily available for low-resource languages.As such, it is important to further evaluate our method on low-resource languages to better reveal its effectiveness.Risks: in this work, we employ gpt-2 to generate new training samples for the task of re.Although gpt-2 is publicly available and the datasets employed in this work to fine-tune gpt-2 for re are also publicly available, a generative language model might produce biased sentences, insulting texts or reveal private information.As such, it is necessary to take further measures before publicly releasing the automatically generated labeled data.To this end, we inspect the data employed for finetuning to exclude any offensive text and identity information.The generated data will also be inspected for purpose before publicly releasing the data.\",\"The limitations of this work are: in this work, we expect to consider more realistic and more applicable settings for class-incremental ner.Therefore, we consider the unlabeled entity problem and provide a more realistic benchmark based on 66 fine-grained entity types.However, there remain some more serious situations unsolved in this work.First, the entity classes in each step might not be disjoint.For example, a new entity type \\\"director\\\" might be included in an old entity type \\\"person\\\".This problem is referred to as the coarse-to-fine problem existing in emerging types of ner.Second, the amount of data or labeled data introduced in each step can also be limited, referring to the few-shot class-incremental problem.Therefore, the proposed method can be further improved to solve these problems.Third, the current version of the proposed method cannot handle the nested ner or contiguous ner problems.In the current version, we simply followed typical works in ner and adopted the sequence labeling scheme to model the ner task, which is not suitable for more complicated ner tasks.The proposed method is a rehearsal-based method that requires keeping exemplar sets for each class.Although the number of exemplars for each class is really small, we believe there can be more data-efficient solutions that totally avoid the need of memorizing data and also achieve good results.The proposed method includes several hyper-parameters such as the entity threshold tentity, relabeling threshold t hnn and t hproto.Although we have shown that the choice of thresholds is relatively robust , it still requires efforts to explore the most suitable thresholds when applied to other datasets or situations.There can be further work to improve this problem by formulating an automatic threshold searching strategy.\",\"We present kg-flex, an end-to-end model that can access new relations at test-time without retraining.Our current kg-flex model does not perform entity resolution, and so we rely on resolved entities provided by the datasets.However, resolved entities may not always be available, so tools such as automatic entity recognition may be necessary.While it is possible for end-to-end models to jointly learn to resolve entities in questions before relation following , we consider this outside the scope of this focused work.Additionally, kg-flex is limited in the kinds of reasoning it can do over a knowledge graph.further, we test kg-flex on popular datasets representing possible real human questions.However, we do not deeply investigate the semantic properties of these questions.Notably, mckenna and steedman show that searching for similar relations in embedding space may work better for paraphrastic inference, and only in certain cases for directional inference where semantic precision matters, e.Defeat entails play, but play does not entail defeat.\",\"While our ndd metric has demonstrated its effectiveness in measuring the semantic distance between overlapped sentences, there are still some limitations to consider.Firstly, the calculation efficiency of ndd may become a bottleneck when dealing with large amounts of data.The mask-andpredict strategy requires the generation of a large number of predictions for each word in the lcs, which can be computationally expensive.Therefore, for large-scale applications, more efficient algorithms or hardware acceleration may be necessary to speed up the calculation of ndd.Secondly, our method currently cannot selectively compress certain parts of the text.The mask-and-predict strategy compresses the entire overlapped segment, which may not always be desirable.For example, in some cases, it may be more desirable to compress only the less relevant portion of the text while retaining the most informative content.While ndd has an advantage over supervised compressors in controlling compression ratio, it still cannot control the compression orders.Future research may investigate techniques to allow for more fine-grained control over the compression process.Overall, while ndd shows great promise in improving the evaluation of semantic similarity and text compression, further research is needed to address these limitations and improve the compression rate controlling ability and versatility of the method.\",\"Currently, the main goal of shrinke is to model inference patterns directly in the embedding space for hyper-relational kgs and we do not explore more advanced training strategies that have recently been proposed.For example, recent works have demonstrated that adding auxiliary training tasks, e., the task of predicting qualifier entities, can further improve the overall performance.another limitation of shrinke, though rarely happens, is that when dealing with semantically opaque contexts, the monotonicity assumption might not hold.In that case, we need ad-hoc solutions.One simple way is to explicitly distinguish semantically transparent and semantically opaque contexts.\",\"Due to the massive combination of relations and times on tkgs, balancing the model performance and efficiency is challenging.techs is a two-step approach that can be further improved if we can fuse logical reasoning in the graph encoder like conglr.The model will be more efficient for computational space and time.\",\"One major limitation of our work is that our experiments are only conducted on docred and re-docred that consist of documents from general domain.Yet, information extraction has many broader applications in specific domains, e.besides, since tag consists of a number of modules and use plm as encoder, the training process takes relatively more time and computational resources than dedicated docre model that only extract relations.We concern that it may affect the scalability with larger amount of either data or parameters.\",\"Limitation this paper presents the mvp-tuning framework, which combines multi-view knowledge retrieval with prompt tuning and incorporates retrieved knowledge in a simple kg-encoder-free paradigm.However, there are limitations to our approach.Firstly, multi-view knowledge consists of self-view and consensus-view knowledge, which are one-hop triplets in the knowledge graph.However, not all question-choice pairs have one-hop triplets, leading to null knowledge being retrieved.Additionally, excessive consensus-view knowledge can lead to noisy retrieved knowledge.Therefore, our knowledge retrieval system needs further improvement to obtain sufficient, high-quality knowledge.Secondly, we focus on the empirical study of prompt tuning in commonsense reasoning tasks.Although we conduct extensive experiments, including initialization schemes and prefix token length, we do not fully understand the mechanism behind prompt tuning and sometimes experience unstable performance.Although prompt tuning has been proven to be an efficient tuning paradigm for commonsense reasoning tasks, it requires further exploration.\",\"We acknowledge that our dataset is not huge compared to other sentence-level relation extraction datasets.However, histred is the first bilingual re dataset at the document level on the historical corpus.In addition, we constructed 5,816 data instances, and our bilingual model trained on histred achieved an f1 score of 63.This reveals that our dataset is sufficient for finetuning the pretrained language models.Also, because yeonhaengnok is a collection of travel records, the domain is not as expansive as other joseon dynasty records.Additional research on massive corpora covering a broader domain is required in future studies.\",\"We identify the following limitations of our work: longer output sequences while outputting the reasoning path as a single short sequence makes the model more interpretable, it increases the challenge of producing a long \\u002fcoherent sequence when the question is complex.Producing a longer sequence also increases the inference time.Simplifying this output while not sacrificing interpretability is a good future direction entity identification our method needs wikipedia outlinks or a entity linker to construct a localized graph for every question.Generalizing this step by pretraining the model to do entity linking might eliminate the need to use an external module.\",\"Our constructed dataset, fine, has limitations in terms of entity category balance.Some categories have a lower number of online passages and less user attention, resulting in an unbalanced distribution of entities across categories.We aimed to simulate a real-world situation by sampling passages based on their click rates.But this may have contributed to the imbalance.Additionally, our proposed method, softfine, is specifically designed for fine-grained chinese named entity recognition with hierarchical categories, and thus has its own limitations.The model is kept simple in structure, with most efforts focused on developing supervision methods, which result in more hyperparameters and require a grid search to find the optimal hyperparameters.however, it has fast inference speed that is comparable to the bert baseline in real-world applications.To address these limitations, future research could explore methods to automatically balance the loss ratio and dynamically score relevance between flattened hierarchical labels.\",\"We acknowledge that the randomness of our noise generation procedure may generate a new entity span that can be considered unnatural.Such aspect of our method may have some impact on the performance levels measured, as distinct types of annotation mistakes can affect model performance differently.In this case, added noisy words that are improbable to be part of the entity may be more easily \\u201cignored\\u201d by the model, while ambiguous additions can lead to a mistake.in addition, the constraint of expanding the mention boundary by a single token should be also taken into consideration.The reason for this design choice was not only based on narrowing the analysis spectrum by reducing the amount of data we had to investigate but also on time constraints, as the training procedure of multiple models on an even larger number of noisy dataset instances would escalate quickly.However, now conscious of the behavior relaxed annotation has on model behavior, it would be interesting to evaluate how this tendency is transformed by introducing even more unnecessary adjacent context into the annotations.A last limitation that can be pointed out is that we only evaluated the noise effect in a single dataset.There are other widely adopted benchmarks for el, such as msnbc and clueweb , which could be used in this work.However, we feel it would be more interesting to juxtapose with other textual domains, especially those with specific jargon and nes, such as the medical domain.\",\"The construction of the reasoning tree may be affected by the kg quality since the connection operations are variant with the kg structure.Hence the unsolved problem in knowledge graph such as incompleteness or noise could disturb the reasoning process.\",\".\\u2022 since the knowledge retrieval is not the focus of this work, we did not spend much space on discussing the choice of different retrieval methods.As shown in table 5, there is still much room for improving the knowledge retrieval from a large scale knowledge graph.It is also worth studying more efficient retrieval methods for retrieving knowledge from a densely connected kg.\\u2022 the proposed method requires an additional mental health related knowledge graph constructed by experts or knowledgeable workers, which is probably difficult to obtain in some applications.However, different from other knowledgeintensive tasks that can be benefited from opendomain knowledge , it attaches great importance in the professionals of the knowledge for building a helpful and safe esc system.\",\"There are some limitations in the use of gpda.\\u2022 the label propagation procedure requires anchor matching in the light of annotation precision, which limits the unlabeled data source.However, wikipedia is a open-domain easyto-fetch corpus with anchor links, which can somehow mitigate the issue.\\u2022 augmented data generated by gpda provide more diversity.But for some datasets, simple modifications on the original words performs better.We are investigating a hybrid approach to apply gpda and nerda in the same framework.\",\"This work utilizes generative models trained on large volumes of data, to generate supplemental training data for named entity recognition systems.We do not address any biases, or filter generations of the underlying paraphrasers when using their generated data.This can bias the fine tuned models towards underlying biases of the generative system.While we do not test or correct the paraphrasing systems for biases, we do not find any evidence for the models deviating unfairly from the underlying training data in any of our human evaluations of the paraphrases.We recommend human review, and automatic filtering of the generations when applying techniques based on generative models to critical applications, to ensure the black box paraphrasing does not introduce, or exacerbate the biases in existing training datasets.\",\"We have applied our neighborhood knowledge graphs to the pubmedbert and pubmedbert+bran models and show the effectiveness of the graphs on the pubmedbert model.We have not deeply investigated how our approach cooperates with other enhancements, and the performance is lower than the state-of-the-art model.\",\"The limitations of sendir include the following two points: it has not extended to documentlevel entity-centric relations tasks.document-level entity-centric re needs to consider multiple mentions of an entity and different relations in different directions of the same entity pair.It does not bring in external commonsense knowledge.Knowledge can be used to enrich events and improve the accurate ere.\",\"While interdapt can be used as a strategy to reduce the negative impact of weak labeling in realworld use cases, it is difficult to understand the magnitude of performance improvements that can be achieved using interdapt as these improvements are highly dependent on how noisy the target sub-domain datasets are and how robust the target entity labels are.Despite potentially reducing data annotation costs, interdapt still has data requirements that are domain-specific to some extent.While such data can be obtained from publicly available datasets, those tend to be less noisy than real-world data.As this work does not explore the impact of the amount and nature of noise in data, it is unclear at this time how this framework would perform when cleaner datasets are used in prior stages of training.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"1_entity_entities_knowledge\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"1_entity_entities_knowledge\"],\"textfont\":{\"size\":12},\"x\":[12.3629732131958,12.46400260925293,12.411046028137207,12.120960235595703,12.373078346252441,12.460407257080078,12.178906440734863,12.435386657714844,12.330207824707031,12.392333030700684,12.34096908569336,12.352035522460938,12.367571830749512,11.686519622802734,12.151520729064941,12.322314262390137,12.428262710571289,12.386985778808594,12.28312873840332,12.329534530639648,12.560233116149902,12.173877716064453,12.222665786743164,12.502204895019531,12.608271598815918,12.131986618041992,12.506386756896973,12.24795913696289,12.360870361328125,12.383989334106445,12.3296480178833,12.381264686584473,12.124107360839844,12.404587745666504,12.232998847961426,12.314001083374023,11.95212173461914,12.067059516906738,12.207422256469727,12.019486427307129,12.26590347290039,12.237849235534668,12.227936744689941,12.443282127380371,12.28659439086914,12.361428260803223,12.298815727233887,12.33173656463623,12.309762001037598,12.335761070251465,12.049247741699219,12.44062614440918,12.326705932617188,12.738941192626953,12.386312484741211,12.12714958190918,12.35537052154541,12.200736045837402,12.352761268615723,12.491442680358887,11.702459335327148,12.274547576904297,12.592436790466309,12.163750648498535,12.349433898925781,12.421330451965332,12.385663032531738,12.285418510437012,12.36113452911377,12.437851905822754,12.245623588562012,12.141519546508789,11.971447944641113,12.38232421875,12.5885648727417,12.485875129699707,12.4918212890625,12.299614906311035,11.63685417175293,12.400733947753906,12.169035911560059,12.443724632263184,12.016213417053223,12.606990814208984,12.152653694152832,12.355424880981445,12.488000869750977,12.313587188720703,12.00426959991455,12.234464645385742,12.478883743286133,12.207159042358398,11.748417854309082,12.449972152709961,12.295371055603027],\"y\":[0.13386249542236328,0.25808826088905334,-0.2965281307697296,0.287521630525589,0.3051694631576538,0.15899531543254852,-0.17661967873573303,0.14149795472621918,-0.3507627546787262,0.03646150231361389,0.26060226559638977,0.18384552001953125,0.14787447452545166,0.7003435492515564,0.29496851563453674,0.440884530544281,0.10975825786590576,-0.5035728216171265,0.1564674973487854,0.2634607255458832,-0.4880813956260681,-0.18651525676250458,0.023380732163786888,0.24710264801979065,0.6273831725120544,-0.08986400067806244,0.2333255410194397,-0.0515044741332531,-0.33173230290412903,0.13606040179729462,0.2849443554878235,-0.25463342666625977,-0.1956845074892044,0.6914575099945068,-0.2713533043861389,-0.33146753907203674,0.5135630965232849,-0.12578335404396057,-0.1660788357257843,0.7374831438064575,-0.1570054590702057,0.7398234605789185,0.05956769734621048,-0.3250463008880615,0.0517817847430706,0.006290052551776171,0.6064624786376953,0.05520648509263992,0.1639324575662613,-0.3262576460838318,0.03674279525876045,0.551723301410675,0.4133778214454651,0.08496306091547012,-0.4450061619281769,-0.1940920650959015,0.02685621567070484,-0.11029358953237534,0.21369819343090057,0.19037456810474396,0.9753171801567078,0.5499284267425537,0.7666923403739929,0.8375871777534485,0.060298021882772446,0.18252740800380707,-0.3403187096118927,-0.2955140769481659,0.14478255808353424,0.7462948560714722,0.2624155282974243,-0.1922467201948166,-0.1587277203798294,0.32049012184143066,0.7979783415794373,0.3585462272167206,0.5144309997558594,-0.0593014620244503,1.069823980331421,-0.22503827512264252,-0.3750602602958679,0.2507076859474182,-0.10358821600675583,0.375628262758255,-0.03457413613796234,0.40788400173187256,0.7708780169487,-0.34409299492836,-0.202552929520607,0.7343758940696716,0.656299889087677,-0.2788761258125305,0.8513343334197998,0.256462424993515,0.15402354300022125],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Despite the significant advancements made by the proposed factual-mr representation in addressing the limitations of current scene graph parsing datasets, there remain several areas for future research.However, the limitations still remain due to the ambiguity of language.second, there is currently no explicit alignment between objects represented within factualmr and the corresponding bounding boxes in the image.To fully utilize multi-modal information, collecting such alignments may be necessary.Third, the proposed method utilizes oracle scene graphs of the image, however, in practical applications, extracting a scene graph from an image remains a challenging problem.Further research is required to determine if utilizing a visual scene graph parsing model to extract scene graphs from images would negatively impact image retrieval performance.Lastly, our current approach utilizes a large pretrained language model to train the parser.However, the issue of robustness in parsers has always been a significant concern.The captions in the vg dataset mainly consist of short sentences with simple patterns.It remains unclear whether the parser is robust enough to handle sentences with more complex linguistic variations, which calls for further investigation.\",\"Of the existing methods, thus improving the accuracy of the kb-rec task.limitations to better understand the limitations of the proposed method, we conducted an error analysis by randomly selecting 100 incorrect predictions and categorizing their error types.The results revealed that 32% of errors were caused by grounding issues, specifically, an inability to distinguish between multiple objects of the same category, despite having knowledge category of the referent object.The results indicate that there is a need for improvement in the ability to discriminate visual objects, especially for object categories with long-tailed distributions.Additionally, the results show that 20% of errors are due to imprecise object detection, particularly for small objects.This highlights the need for optimization of the visual encoder and loss function.Moreover, 14% of errors are attributed to incorrect knowledge retrieval.To address this, incorporating more fine-grained information in expressions for retrieval should be considered as a future research direction.Furthermore, 34% of incorrect predictions can be attributed to issues with the ground-truth annotations, which may negatively impact the model\\u2019s learning process.\",\"Since our model involves an additional step of ocr, it is less efficient than the end-to-end tit model, although it can achieve significantly better performance.Besides, with the incorporation of image information, our model is still unable to completely address the issue of error propagation caused by ocr.\",\".The pentomino domain can only serve as an abstraction for referring expression generations in visual domains.The amount of objects is limited to 9 different shapes and the number of colors is reduced to 6 as well.The positions are chosen to be discrete and absolute while real-world references might include spatial relations.Furthermore, the pieces show no texture or naturalness, but are drawn with a solid color fill.We choose this simplified domain to focus on the interaction between the follower and the teacher and left the evaluation of the proposed models on more realistic looking scenes for further work.Nevertheless, we think our approach can also be applied to photo-realistic environments.Limits on variability of the referring expressions.We only explored expressions that are generate by the incremental algorithm.Moreover, we choose a fix property value order for the realisation of the template\\u2019s surface structure and left the exploration for a higher variability to further work.Limits on variability of the feedback signal.In this work we used a heuristic teacher with a fixed behavior to provide the intermediate feedback to the follower.We choose this oracle speaker for better control over the experiments and to focus on the research questions of which feedback is most helpful and how it should be presented.We are aware that in natural interaction the teacher\\u2019s responses might be more dynamic and can be potentially learnt in a much more complex multi-agent rl settings which would go beyond our focused contribution here.Still this is an interesting prospect for future research.\",\"This work focuses on informative image captioning evaluation, including an overall score, vision recall, text precision and token-level scores.The effectiveness of our metric is validated on standard image captioning benchmarks.Infometic in this work may not perform well in other captioning tasks due to domain gap, but we contend that our general framework can be adapted to other domains such as text-aware image captioning.For example, for textaware image captioning which focuses more on scene texts in images, we could further encode text regions besides the existing object regions for better comparison with captions.\",\"The proposed method has several limitations: 1) the current approach achieves hunky context reasoning performance in the cross-modal scene of a single text clue and image, but the context reasoning capability in the scene containing multiple textual and visual clues still needs to be further explored, such as video and long text.2) from the experimental results, we observed that the visual prefix length greatly impacts the stability of language models infused with visual information.Hence, we still need to explore effective and stable vision-aided language models for natural language processing and multi-modal scenarios.3) we also hope this work could spark further research on improving the long context reasoning capability of pretrained vision-language models.Acknowledge we thank the anonymous reviewers for their constructive comments, and gratefully acknowledge the support of natural science foundation of china and the stable support program for higher education institutions of shenzhen.\",\"We discover that for datasets with a relatively large number of categories, our method requires a more delicate setting of epoch under different shots.Figure 5 shows the average results on sun397 and imagenet of different epochs.It can be observed that for datasets with a large number of categories , as the number of shots decreases, the performance deteriorates with an increase in the number of epochs, which is not evident on the datasets with a small number of cat- egories.We will delve further into this problem to find the reason and solution.\",\"In this work, we take the sufficient advantages of the external semantic and syntactic structure knowledge to improve our focused problem.But this could be a double-edged sword to use such features.Specifically, our paper has the following two potential limitations.First of all, our method closely relies on the availability of the resources of scene graph structures and syntax structures.While most of the languages come with these structure annotations to train good-performing structure parsers , some minor languages may not have structure resources.That being said, our idea still works well even in the absence of the targetside structure annotations.With only the structure annotations at pivot-side language , we can still achieve much better performances than those baselines without using the structural features.Besides, our method will be subject to the quality of the external structure parsers.When the parsed structures of scene graphs and syntax trees are with much noise, the helpfulness of our methods will be hurt.Fortunately, the existing external semantic and syntactic structure parsers have already achieved satisfactory performances, which can meet our demands.\",\".Though the ensemble method can alleviate this problem to some extent, how to automatically map the text label into the corresponding image is an interesting research question to investigate.Since clip was pre-trained on noisy web-crawled data on the internet, our approaches are limited by pre-training data distribution of clip.Therefore, a potential future direction is to further pre-train clip on more general downstream task datasets.\",\"Although our proposed multiemo framework has achieved state-of-the-art performances on both iemocap and meld, there are some limitations with this work: \\u2022 our proposed visual feature extractor visextnet does not distinguish between speakers and irrelevant people in the scene, which can be problematic in some scenarios.For instance, one scene in meld is the cafeteria, where a lot of background actors sit and drink coffee.The facial expressions of these background people have no impact on the emotion of the speaker since they do not participant in the conversation.However, visextnet captures visual features of everyone appeared in the cafeteria with no differentiation, which may lead to a wrong comprehension of the speaker\\u2019s emotional tendency due to the effects of facial expressions from irrelevant people.\\u2022 the effects of hyperparameters in the swfc loss on model performances have not been fully studied, which will be thoroughly analyzed in our future research.\\u2022 due to the class imbalanced issue with meld, the swfc loss requires a large batch size on meld to ensure that for each training sample there exists at least one positive pair in the batch, which can be computationally expensive.We will investigate effective approaches to tackle this challenge in our future research.\\u2022 even though multiemo has achieved remarkable improvements in minority emotion categories, the performances of multiemo in minority emotions are still worse than majority classes.\",\"There are several limitations that can be considered for future improvements: in multimodal alignment and fusion, we only consider a single image for each sample, whereas multiple images can be available.A more flexible visual encoding architecture that can digest an indefinite number of input images can improve the visual information coverage; 2) the empirical results in this work focus on three attribute extraction datasets that can clearly benefit from visual perspectives, while there are also various attribute types that rely more on the textual input.Different traits of attributes may influence the preferred modalities during the modeling, which is out of scope for this work but serves as a natural extension of this study; 3) currently there is no specific design to improve the efficiency based on the visual question answering architecture.It can be not scalable as the number of attributes increases.There could be a dual-use regarding the attentionpruning mechanism, which can be a potential risk of this work that could arise and harm the result.The attention-pruning mechanism encourages the model to focus on the task-relevant foreground on the given image selected with category supervision, which can improve the prediction precision given the input image is visually rich and contains noisy context background.While for some types of images, such as infographics, there may be helpful text information on the images or intentionally attached by providers.These additional texts may be overlooked by the attention-pruning mechanism, resulting in potential information losses.A possible mitigation strategy is to add an ocr component along with the visual encoder to extract potential text information from given images.\",\"Our method has the following limitations: datasets: in multi-modal pre-training, we rely on downstream datasets to evaluate the performance of pre-trained models.The commonly used entity extraction datasets are relatively small and lack diversity, so the proposed method may not generalize well to real word scenarios.Lack of image modality: in layoutmask, we focus on text-layout interactions, leaving the image modality unexplored.However, documents in the real world contain many elements that can not be described by text and layout modalities, like figures and lines, so incorporating image modality is important in building a universal multi-modal pre-training model for document understanding.\",\"In this paper, we pre-train cclm with moderate multi-modal data, e.cc3m, to make a fair comparison with previous work such as m3p and uc2.We leverage large-scale vision language pretraining simply by utilizing the pre-trained weights of x2-vlm which has been pre-trained on billionscale image-text pairs in english.Collecting more image-text pairs in different languages will very likely lead to further performance improvements.Moreover, there exists larger public available multilingual datasets, such as multiun and opus.Leveraging more multi-lingual datasets for pre-training should also yield a more powerful multi-lingual multi-modal model.As for social impact, multi-modal pre-trained models can be used in applications that help people with disability in one modality.Our work makes these applications applicable to minority people speaking non-english, and potentially low-resource languages.In sum, our work potentially enables deep learning technology to benefit more people, and is unlikely to have direct negative social impact.\",\"To facilitate future research, we analyze the difficulties and possible solutions in this new area.As we present extensive empirical results and address the weakness of clip on vocabulary expansion, its theoretical risk on open tasks is urged to be investigated.The current evaluation protocol is an approximation of the real open world.An evolving benchmark could facilitate future research.For various visual categories, their degree of abstraction, the ease of describing them in natural language, and their density in the data distribution can also influence the extensibility and stability of models, which are worth studying.\",\"While our solar has demonstrated its superior performance on three benchmarks, it still has several limitations.Firstly, solar relies on accurate recognition of image caption and object-attribute detection models.If the features of these two parts are not correctly recognized, it will cause subsequent cascading errors.lastly, the experimental results do not delve deeper into which cases a unified language representation is better and in which cases a multimodal model performs better.We speculate that an integration of language models and multi-modal models will yield better results.\",\"Considering modality heterogeneity can promote many related multimodal applications, it is worth continually exploring.In this paper, we propose text-guided fusion module equipped with sparse-attention to integrate different modalities in representation aspects, which is an implicit way to build the relations of fine-grained features, such as visual objects, and textual words.Previous work has proven that graph convolutional network shows advantages in modeling the relations among visual and textual elements.Inspired by these works, we argue that explicitly introducing the relationship of fine-grained features via gcn can better guide the model to eliminate redundant features.Thus it can further narrow the modalities gap and facilitate fusion for multimodal content understanding.\",\"Since dall\\u2022e mini is trained on english-language material, and since our input text is english only, our proposed methods will only be able to measure the imageability of english isolated words and connected text.The text-to-image model we use, dall\\u2022e mini, requires gpus or tpus to generate images.While we used 4 gpus to obtain the results in this paper, we were able to use a single gpu to successfully run the same experiments with longer runtime.\",\"An important limitation of our approach vawi is the need for extracting visually-hungry words as the trigger to inject visual knowledge into plms.In real-world applications, it is hard to obtain the annotations of vh-words.Therefore, we propose three vh-words extraction strategies.However, the three strategies may be not always proper for all nlp tasks, and we rely on the experimental results to select the best one among them.Besides, we adopt the text encoder of clip as the vl-ptm for generating the visually-aligned representation.As a pre-trained model, clip also may contain biases learned from the pre-training corpus, which may result in improper biased prediction on some nlp tasks.\",\"Although introducing chart value prediction objective, it only provides minor improvement to the model\\u2019s performance on doing complex reasoning.There is still a large room to improve the model\\u2019s capability in math calculation.Our model also suffers from the noisy ocr prediction of off-the-shelf object detector, whose performance will depend highly on the extracted ocr text qualities.Another possible limitation of our approach is the quality of the pre-training data, which only contains synthetic images.Although the proposed model works fairly well on the chartqa dataset, it is unclear if the improved performance can be generalized to other realistic chart images.\",\"Limitations of data collection our proposed dataset only targets english language tasks.in addition, our current dataset mainly focuses on vision-language tasks.Datasets from more diverse modalities should be considered such as audio and video.While we have built a novel multimodal instruction dataset containing 62 tasks, the number of tasks and associated instructions remains limited.To address this, future research could consider utilizing crowd-sourcing or automatic generation and augmentation techniques to increase the variety of instructions available.Limitations of experiments and evaluation our work is the first to explore instruction tuning on multimodal tasks and shows improved performance compared to baseline methods.However, there is still room for improvement, specifically in utilizing text-only instruction datasets.Future research could explore alternative architectures and stronger vision-language pre-trained models, or develop additional training loss functions to better utilize these unimodal instruction datasets.Additionally, we only used ofa as the baseline model as it was the largest open-source multimodal pretrained model available when we conducted this research.As more and stronger multimodal pretrained models being publicly available, it would be interesting to conduct a thorough comparison between models with different sizes.Finally, we take the first step to define sensitivity as a metric to evaluate the robustness of the models on understanding and following human-written instructions, which can be a potential standard metric for all the following instruction-tuning studies.However, it\\u2019s only based on the variation of model performance across different instructions for the same task., the model\\u2019s capability to understand different instructions for different tasks , to further improve the sensitivity metric for instruction tuning.\",\"The main novelty of our proposed mir-gan is refining frame-level modality-invariant representations via adversarial learning.It is promising to combine this approach with the popular selfsupervised pre-training to learn unified multimodal representations.In this work, we only load pretrained av-hubert for the front-ends and speech recognition model, while the proposed modules are still trained from scratch.In future, we may include the entire mir-gan into self-supervised learning scheme, together with the adversarial learning to refine better multimodal representations.\",\"Though cacr shows significant gains in compositional performance, results are limited in their exploration of only one pre-trained model and compositionality dataset.A significant risk of models is their tendency to be biased by distributions in their training data; vision-language models are not free from this flaw, but we see our work as teaching vlms to learn better structured representations rather than memorizing spurious correlations in data.We remain far from solving the vision-language compositionality problem, so biases must continue to be actively mitigated.\",\"We identify a few limitations of the current work.Our approach still suffers from biases in the training data and may produce incorrect output or lead to an inaccurate understanding of multi-modal content.And a large-scale audio-visual pre-trained model is a promising direction toward more advanced and cheaper approaches for transfer learning, which we leave for future study.\",\"Given that many special properties of childdirected speech are not present in text, we would have liked to work on a multimodal dataset, where both visual and speech information would be present.More specifically, we would have liked to test the effect of the following: \\u2022 grounding the language models in vision to test the effect of joint attention.Joint attention refers to the phenomena where the caregiver\\u2019s and the child\\u2019s coordinated attention to each other to a third object or an event.\\u2022 child-directed speech is known to have special prosodic properties such as higher variability in pitch , lengthening of vowels and pauses , context-specific intonational contours.These properties have been suggested by many researchers to serve as a mechanism for getting the infants attention.This attentive role may be considered to be beneficial for language development in children.As our models only take text as the input, we were unable to test the relationship the between these properties and language acquisition in neural network based models have.\\u2022 caregivers give a lot of feedback when young children are first producing and acquiring language.Our current mainstream language models are not interactive.Therefore, it is difficult to incorporate the feedback loop and the test the effect of the same in models\\u2019 language acquisition.As it is, our findings suggest that many of the most important facilitative features of childdirected speech are relevant to precisely those formal and conceptual aspects of language acquisition that are not captured by text-based language models.In this paper, we have tested the effect of native cds in l2 acquisition with 5 typologically diverse languages.However, there is enormous scope to test the effect of the same with many more different languages, which may lead to more pointed implications and conclusions than the findings offered here.\",\"The most important limitation of our work lies in the size of the havqa dataset.However, substantial further funding would be needed to resolve this.For the baseline multimodal experiments, we did not use the image directly but resorted to extracting textual tags and including them in the text-only translation input.A tighter fusion technique may give better performance.\",\"Deplot\\u2019s strength is highly dependent on the accuracy of plot-to-text conversion.To obtain effective plot-to-text conversion, large amounts of diverse and in-domain plot-table parallel data are usually needed.It is unknown to which extent deplot can work for out-of-domain plotto-text conversion.beyond, deplot does not work for visual language that does not have a clear latent textual representation such as textbook figures where the visual illustrations are created using specialized software and do not have clear structured representations.Another limitation of the current deplot approach is that we ignore any layout information such as orientation and color of the visual elements\\u002fobjects.\",\"Although the proposed multicapclip can generate multilingual zero-shot visual captions without any labeled vision-caption training pairs.We still need the independent set of text for training\\u002ftranslating, which may still be difficult to collect for some lowresource languages.besides, our approach uses clip to measure text-text similarities for retrieving concept prompts and conducting input augmentation during training.Considering that clip is optimized by image-text global contrast and intra-modal retrieval of such a model is not as well as its cross-modal retrieval , an improvement direction of our approach is using a vision-language pre-trained model that measures intra-modal and inter-modal semantic similarities well.\",\"Despite the promising results of our iml pipeline for image captioning, our work has some limitations.Firstly, the experiments were conducted on a domain-specific dataset, vizwiz, and may not generalize to other datasets or domains.Secondly, our approach may not be suitable for scenarios where user feedback is sparse or unreliable, as the effectiveness of iml heavily depends on the quality and quantity of the feedback.Thirdly, our use of episodic memory to retain knowledge from previously seen clusters may not scale well to smaller datasets and other methods may be required.Lastly, our approach does not address the challenge of bias in the data, which can lead to biased models.Ethical statement as of now, we do not see ethical concerns with the study presented in this paper.We used a dataset that is publicly available.The study is currently not applied to human subjects with personal data; in this case, the use of user feedback in the training process could potentially introduce biases if the feedback is not diverse or representative of the population.Lastly, our approach may be used to develop image captioning models that generate harmful or inappropriate content, such as captions that perpetuate harmful stereotypes or stigmatize certain groups of people.\",\"Though we have injected math reasoning skills to matcha, error analysis shows that there is still room for improvement on queries requiring complex reasoning.Besides, it remains debatable whether doing math calculation in weight space in a purely end-to-end manner is the most promising path forward.9 besides math reasoning, figure 2 shows that plot attributes is an area where matcha underperforms pali.We conjecture that it is due to matcha\\u2019s lack of massive scale grounded imagetext pretraining with rich semantics.While chartto-code pretraining provides certain level of plot attribute grounding, such plot features are mostly using default options in plotting packages but not explicitly written out in code.In terms of experimental setup, the reported number is result of a single run.Pretraining is extremely costly especially when there exists more than twenty ablation setups and downstream evaluation tasks.We have collected pretraining and evaluation data points from multiple aspects on various scenarios to verify the robustness of matcha.However, we do acknowledge that the paper can benefit from reporting multiple runs given sufficient compute.Last but not least, it is also worth noting that visual language is an umbrella term.There are other visual language systems beyond the ones discussed in this paper.As an example, comics\\u002fmanga have their distinct visual lexicon or even grammars.\",\"We only use pre-trained video transformers off-theshelf to encode the videos, while more nuanced and specific utilization of other models can be explored to further improve the performance.There are also valuable egocentric videos and demographic statistics along with the ego4d dataset that we have not yet incorporated in our approach.Due to the difficulty and cost of collecting videos with transcriptions and voting outcome annotations, the total number of games is insufficient to train a deep neural network for voting outcome deduction, though data augmentation techniques can be explored to mitigate this limitation.\",\"There are several limitations in this work.First, we have not explored how to make use of compositionbased augmentations in the supervised setting.A second limitation is a lack of theoretical grounding in the impact of our latent space composition.Finally, we have not explored interoperability with other training objectives.\",\"Since hypothesis sentences were created and labeled by medical experts, the size of our current dataset is small.In particular, the number of examples of contradiction is small because the hypothesis sentences were created based on captions to efficiently construct our dataset.However, we can increase the number of examples of contradiction by rewriting phrases in the hypothesis sentences.The claim of this study is that we can relatively efficiently create a vte dataset in the medical domain from the existing image caption dataset, and can empirically demonstrate the challenges of current vision-and-language models on the vte dataset.Although increasing the data size is an important next step, it is beyond the scope of this paper.\",\"First, we again emphasise that the lack of highquality non-english image-caption pairs is a primary obstacle to wider-scale multilingual and cross-lingual tti investigations.second, our work uses 512-dim \\u2018xlm-r large vit-b\\u002f32\\u2019 mclip22 and is based on the stylegan2 framework.Since the main focus of our work is to realise multilingual and cross-lingual tti and enable fair comparisons across different models and approaches, we compare all proposed and baseline methods with the same mclip text encoder and the gan framework.However, for readers and potential users interested in \\u2018chasing\\u2019 stronger absolute fid scores, we speculate that the larger 640-dim \\u2018xlm-r large vitb\\u002f16+\\u2019 mclip text encoder and the more recent stylegan3 can be helpful.also use clip to condition image generation on text input.This means that we could be able to derive multilingual diffusion models for mtti also by replacing clip with mclip and enhance the mtti performance with our proposed ensad.fourth, the ensad boosts cross-lingual transfer for tti by combining the knowledge from multiple translations, which can mitigate potential translation errors.Our work does not demonstrate if ensad is applicable and adaptable to downstream cross-lingual tasks besides tti.It is because 1) downstream tasks other than tti are out of the scope of this work and 2) adapting ensad to different tasks will require redesign of model structures and losses catering to the characteristics of each downstream task, making us believe it is not proper to expand the topic and include everything in a single piece of work.\",\"Although we introduce a new gmner task and propose a number of baseline systems and an hindex framework, there are still some limitations in this work.First, our gmner task only requires identifying the visual regions that are correspondent to named entities mentioned in text.However, for each image, many visual regions may contain real-world entities that are not mentioned in text.second, our work is a preliminary exploration of the gmner task, and the proposed approaches are primarily based on previous representative ner or mner methods.We hope this work can encourage more research to apply the recent advanced techniques from both the nlp and computer vision communities to improve its performance.\",\"In this paper, we limit the proposed whitenedcse for sentence embedding learning.Conceptually, whitenedcse is potential to benefit contrastive learning on some other tasks, e., self-supervised image representation learning and self-supervised vision-language contrastive learning.However, we did not investigate the self-supervised image representation learning because this domain is currently dominated by masked image modeling.We will consider extending whitenedcse for visionlanguage contrastive learning when we have sufficient training resources for the extraordinary largescale text-image pairs.\",\"The main limitation of the presented work is the need for significant computing resources to train multimodal models using dynamic uda.It should be noted that the proposed methods, mmbv and dynamic uda, require fewer computational resources than the original version of uda.\",\"During the creation of the remuq dataset, we simply remove the words in the question that are duplicated in the image caption \\u2013 in some cases, this may result in grammatical errors in the text query.We performed the experiments for studying optimal masking ratio on a subset of the pretraining data, due to resource constraints.\",\"Our work overcomes visual noise data that limit extraction performance, incorporating multi-modal knowledge of different levels.Empirical experiments demonstrate that our method avoids noise data misleading the mmre model.However, there are still some limitations of our approach can be summarized as follows: \\u2022 due to the limitation of the existing mmre datasets, we only experiment on two modalities to explore the influence of image features.\\u2022 our method neglects the multiple relations for an input, which may not consider the multiple semantics of entities.\",\"Although our proposed curriculum can be applied to any multimodal architecture, curriculum aware loss requires modifications for use with dual encoder architectures that don\\u2019t use cross-modal attention.Additionally, we use an off-the-shelf partof-speech tagger to divide the data into different phases.As such, the correctness of this division is dependent on the quality of tagger.A poor tagger can negatively impact the curriculum design.Moreover, our approach doesn\\u2019t apply to possible image-captions dataset which contain only short captions, containing possibly only one noun.\",\"In this work, we first formulate the scene-robust nlvl problem and propose our solution.However, our generalizable nlvl model is still tested on existing close-world datasets, and the actual performance in real-world scenarios needs to be further explored.A real-world, large-scale dataset is required to develop a practical, generalized, openworld query-based video retrieval model.\",\"Lilgym uses synthetic visual stimuli, which does not reflect the complexity or characteristics of realistic visual observations.This is critical for our ability to control the environment and provide a lightweight and accessible rl benchmark.Our goal is not to provide a resource for the development of methods that aim to handle realistic visual input, and lilgym is not suitable for this purpose.The limited number of colors, shapes, and sizes used limits the visual and lexical complexity of the data.The synthetic nature of the data and the modular library of functions we use allow to relatively easily extend the environment.This will require collecting additional natural language data.In this work, we opted to rely on the nlvr data without further expanding it.Some annotators of the original nlvr data adopted annotation strategies that led to repetition of some common phrases.Showed that nlvr demonstrates high semantic diversity and compositionality.translating the data is a feasible low-cost solution, because the program annotations will not require updating.The rate was determined by the workers.The lilgym environment and data as is are intended to be used for research, including algorithm development and evaluation, and not for development of models to be deployed.\",\"Dataset utilization we have collected 137 datasets, yet we have only conducted experiments over a minority of these , leaving the remaining datasets unexplored.In this work, we do not experiment on image-text datasets for two reasons: all of the image-text datasets are translated from english versions; and there is no large lm available for zero-shot image-to-text generation.Experiments we did not attempt few-shot or fully-supervised learning experiments in nusacrowd since prior work has explored these approaches on some of the datasets.We specifically conduct our experiments on zero-shot methods to explore the generalization of zero-shot cross-lingual and zero-shot prompting approaches to extremely lowresource languages.Task diversity the tasks represented in nusacrowd are skewed towards mt, sentiment, abusive text classification, and asr.Many other tasks remain unexplored for indonesian and regional languages.Furthermore, most asr work come from the same authors or research groups.While these topics are prevalent among indonesian researchers, it is also important to expand to other tasks.Domain diversity the datasets in nusacrowd are primarily from the domains of social media, news, and other general domain sources.Despite having a huge potential, narrow-domain datasets, such as clinical, biomedical, legal, financial, and educational datasets remain underrepresented for indonesian and regional languages.Exploration of domain-specific data and use cases for indonesian and regional languages is critical.Language diversity there are 700+ languages in indonesia.However, we have only focused on a small fraction of these languages.In addition, there are also other regional languages similar to the two sinitic languages in nusacrowd, i.more focus on under-represented languages is an interesting future direction.Multimodality the datasets in nusacrowd are mainly in the text modality.Exploration of speech, image, and other modalities for indonesian and regional languages is still limited, and there are potentially exciting opportunities to capture locallyrelevant indonesian culture in such modalities.Utilization of datasets there are 137 datasets contained in nusacrowd.While we showcased three different use cases for the datasets , there is much greater potential to use the datasets in nusacrowd.Potential areas of focus include experimenting with various approaches and analyses over multiple datasets, such as multi-task learning, continual learning, or few-shot learning.\",\"Limitations and bias of pre-trained models: our work uses detected objects and their attributes in the images to introduce novel insertions in the corresponding text.To this end, it is important to address the limitations of the state-of-the-art object and attribute detection methods.The undesired artifacts of these methods could be categorized as inaccurate or biased.The detected objects could be incorrect, but since we only consider objects that are also mentioned in the text, the effect of incorrect object detections is non-existent in our augmentations.However, we notice that some of the detected attributes in images and bert predictions reflect stereotypical associations and have been documented in prior works.We acknowledge that the current state of deep learning research is limited, and the consequential shortcomings are reflected in our augmentations to some extent.Broader social impact: the authors do not foresee any negative social impacts of this work.We believe our cross-modal augmentations will enable an exhaustive evaluation of the robustness of visionand-language models, leading to more reliable multimodal systems.We release the code for our experiments to aid reproducibility and enable future research on this topic.Annotations, irb approval, and datasets: the annotators for evaluations done in this study were recruited via amazon mechanical turk.We specifically recruited \\u2018master\\u2019 annotators located in the united states; and paid them at an hourly rate of 12 usd for their annotations.The human evaluation experiments were approved by the institutional review board at the authors\\u2019 institution.The datasets used in this study are publicly available and were curated by previous research.We abide by their terms of use.\",\"Our work has the following limitations.First, our proposed facialmmt approach is a two-stage framework that is not fully end-to-end.second, this work primarily focuses on the visual modality, and has not yet delved into other aspects of the mermc task.\",\"The two videoqa datasets used in experiments are associated with relatively short videos.Therefore it would be better if more experiments could be conducted on videoqa datasets with long videos to verify the effectiveness of our approach on a wider range of videoqa tasks.Although the proposed approach in this paper can also be used in other video-language tasks, our experiments focuses on a specific video-language task - videoqa.Experiments on more video-language tasks are needed to show that our approach are also effective in other video-language tasks.\",\"While the proposed single-frame training approach shows strong performance on various videolanguage datasets, it does not work well on true temporal tasks like the new ssv2 tasks.Compared to multi-frame models, our single-frame model also has a higher demand for pre-training data.\",\".Our work mainly study the setting where each dataset serves as an independent domain.However, the adopted datasets for query-based image segmentation are mostly collected on ms-coco and have limited domain gap between visual modality.The findings could inspire the researchers to explore other settings, e.each class serves as an independent domain.\",\"Limitation first, there are studies claiming visual information only serves as regularization.In our ablation study, we find the adversarial setting of fusion-based approach outperforms the plain transformer.Combined with observations from previous studies, we suggest that fusion-based architectures may apply some images information as regularization terms, yet the further quantitative analysis is needed to confirm this phenomenon.Second, though our testset is carefully selected to ensure the textual ambiguity without image data, we encounter difficulties in designing a suitable metric for quantifying the degree to which the models are able to resolve the ambiguity.Specifically, we find that conventional metrics, such as wordlevel entity translation accuracy, exhibit significant fluctuations and do not effectively quantify the extent to which the model effectively resolves ambiguity.we acknowledge that the evaluation of multimodal ambiguity remains an open problem and an area for future research.In addition, there are some details regarding the dataset that we need to clarify: the dataset is collected after covid-19, so some commodities will be associated with the pandemic.We collect data by category in order to cover various products to reduce the impact of the epidemic on product types.\",\"In this work, we evaluate the proposed models for nlp tasks only.However, tasks in other fields such as computer vision may present a very different input inductive bias, thus affecting the performance.Moreover, our models are trained from scratch, hence it is unknown whether the same divide-andconquer strategy works for pre-trained models.\",\"A lot of recent work especially in computer vision has leveraged the unsupervised methods or unpaired multi-modality data to pre-trained crossmodal language model.Applying the same idea into speech language model is also discussed in some recent research works.To compare fairly with previous works in st area, we do not build our model on top of such frameworks and discuss how to utilize the raw audio.In terms of the model training, multi-tasks may affect each other due to uneven data distribution, and we have just scratched the surface of this part of the analysis.\",\"In this paper, we focused on english comics only because of their ease of availability.Although we have not experimented with non-english text, we expect the proposed model to work well in multilingual settings if we replace gpt-2 decoder with other decoders like bloom.\",\"The findings of this study have to be seen in light of some limitations.It is non-trivial to extend our model for generation tasks.Since the main focus of this work is to improve both effectiveness and efficiency of the dual-encoders, text-decoder is not considered in model design.there may be disadvantages of the model in region-level vl tasks such as object detection.The reason is that these tasks require images in high resolution and fine-grained annotations of bounding boxes, which are non-trivial in generic vlp settings.\",\"One limitation is that the importance and visual salience of character instances are not measured directly.another limitation of our work is that we only evaluate our visual coherence loss on a single dataset.Whether the vcl can generalize to other datasets remains unexplored.The reason is that many other datasets are collected in a way to exhibit less visual coherence.The vist dataset contains fewer human characters per story than vwp.Also, some of the features we are using for character reidentification may not be suitable to other datasets to the same extent.\",\"Our experiments are conducted on transformerbased models with the same multi-modality fea- tures.Considering the importance of entity information in textual context and specific regions of images, it is also important to investigate whether the performance of the model promotes with different methods of extracting multi-modality features.We use the bpe technique to encode all entities in input articles which may separate the whole entity word into several sub-word tokens and may affect the impact of vision features.There are still a lot of entities of captions that don\\u2019t appear in articles from datasets on our experiments.\",\"As this work is mainly focused on weakly supervised vision-and-language pre-training, we do not fully explore the factors that may influence the performance of relative representations, such as the use of different unimodal encoders and the source of the anchors.Besides, we only validate the effectiveness of relative representations in a weakly supervised setting, while it remains to be explored whether it is also useful for standard vlp and multimodal learning in other modalities.\",\"For image captioning, we used the pre-trained ofa model for zero-shot inference.We did not explore every state-of-the-art model or fine-tune ofa specifically on the imsitu dataset.Other image captioning systems could yield better results.The gap between automatic object recognition and using gold nouns confirms that correctly identifying the objects in an image is very important for activity recognition.Also, we are not certain that mapping the jiang and riloff function frames to the imsitu frames is strictly necessary.\",\"Although our proposed nuwa-xl improves the quality of long video generation and accelerates the inference speed, there are still several limitations: first, due to the unavailability of open-domain long videos , we only validate the effectiveness of nuwa-xl on public available cartoon flintstones.second, direct training on long videos reduces the training-inference gap but poses a great challenge to data.Third, although nuwa-xl can accelerate the inference speed, this part of the gain requires reasonable gpu resources to support parallel inference.\",\"Limitation of existing visual document classification models by modeling a document as a graph and learning its embeddings using a graph attention network.By defining two types of edges , we leverage the benefit of layout information while minimizing the effects of the errors from ocr reading order.Thus, effectively embracing coarse and fine-grained layout information, gvdoc generalizes better for different layouts.While most visual document classifiers tend to perform well on in-distribution data, they fail or struggle on outof-distribution data; our model does not drop its performance on ood data.Through experiments, we demonstrate the generalization of our model on out-of-distribution data.\",\"To better analyze the limitations of pace, we carry out an analysis of the errors made by pace on the photochat and simmc2.we reveal several reasons for the errors, which can be divided into the following categories.First, since there are many similar images in the datasets, pace fail to distinguish some gold image from similar candidates.This may be because we do not design an explicit fine-grained reasoning module to capture the details of images and texts.For example, for the context mentions \\u201ci and my dad both have a camera\\u201d, our model can capture the entity \\u201ccamera\\u201d, but fails to reason the fact that there should be two cameras.One possible solution is to introduce a deep reasoning and comprehension strategy to empower the model with excellent reasoning ability.Second, due to the lack of fine-grained structural understanding of the images, the sentences generated by pace suffer from identifying the relative positions of entities.For example, pace may have difficulties recognizing the fact that the right side of a yellow shirt is black pants.This issue is particularly severe in simmc as there are many entities in the pictures and spatial descriptions of entities in the responses.One possible idea is to extract the relative positions of objects mentioned in the conversation as auxiliary data to guide the model\\u2019s generation.\",\".Though ofa-ocr achieves high accuracy on multiple text recognition datasets, its costs are larger than the non-transformer baselines.In practice, it is difficult to deploy such large models.\",\"Our work explores a dimension of context understanding by voice assistants but it is only a small step.Firstly, we only consider 5 categories, while screens have a myriad of other texts and visual content.We do not include image context into our reference understanding models.But users could use them when formulating references to texts near them.Using image captions or some pixels would improve coverage.Our system leverages entities extracted by upstream and hence is bounded by the performance of that.Also our model evaluates each entity separately while there may be benefit in considering the entire screen holistically.\",\"Currently, cone is mainly implemented for proposal-based models as they can generate ex- plicit moment proposals for the introduced interwindow contrastive learning.In contrast, proposalfree methods ) directly predict the start\\u002fend timestamps without explicit proposals.furthermore, cone falls short on the groundtruth moment case whose duration is longer than the adopted video window duration.\",\".Our work mainly leverages a pre-training scheme to enhance the encoding of speech for video grounding.However, the adopted audio data for pre-training are different from the one in the grounding dataset.This could lead to performance degradation due to the domain gap.The findings could inspire the researchers to explore a better pre-training strategy to learn domain-invariant and effective speech representations for grounding.\",\".Through the avoidance of conflicting weight updates, exssnet not only improves performance but also eliminates forgetting, striking a delicate balance.Moreover, the inclusion of the knowledge transfer module propels the learning process, utilizing previously acquired knowledge to expedite and enhance the learning of new tasks.The efficacy of exssnet is substantiated by its superior performance in both nlp and vision domains, its particular proficiency for sparse masks, and its scalability up to a hundred tasks.\",\"Our methods are currently trained and tested on two svhr datasets.Gender biases and unethical hashtags could exist in the datasets, which may cause the model trained on these datasets to generate these biases.Besides, although our methods are not language-specific, we only choose the english dataset due to its rich resource.Furthermore, we regard the user tags as the hashtags for sfvd2 in our experiments and there are small differences between the user tags and hashtags.as an initial work for svhr, in our task formulation, our model only take the video and its description as input to predict hashtags and ignore user preference in hashtag recommendations.we used up to eight a100 gpus per experiment and it took more than one day to run experiments on sfvd2.More efficient models are needed for real-world applications.\",\"Despite promising, the current work still has limitations.First, the current model mainly focuses on understanding problems.The generation ability of our model has not yet been investigated.It is unclear whether our weakly supervised framework also fits generative models and transfers strong generation capability across languages.Secondly, the current work explores multilingual corpora and overlooks the domain gaps in existing image resources.As argued in , the visual appearances of objects are diverse across cultures.Bias naturally exists in the distribution of images in existing v-l corpora.To develop a truly generalized multilingual multimodal model, the gap between visual distributions in different cultures should be considered.\",\"First, inference efficiency is one of the main limitations of this work.The bart model takes about 14 minutes to complete the inference on our dataset, while our utged needs 92 minutes.The reason for the slow inference is that utged requires heavy computation to update the gradient to the encoder\\u2019s states and decoder\\u2019s states.second, the lack of multimodal content in the published tweets would result in another limitation.The images contained in the published tweets are ignored in this work.However, due to the complicated relationships between images and texts in a multimodal tweet, images might provide complementary content and complete the meanings of the message.Therefore, future studies might explore selfintroduction generation using multimodal tweets to indicate personal interests.\",\"This work only explores the multilingual vlp model for the image-text retrieval task.at the same time, our proposed method relies on a well-pretrained vision transformer and a multilingual text encoder.Its performance is heavily influenced by the performance of the visual and textual backbones.This hinders the mclip from further improvements with the given backbones.\",\"In this work, we studied the effects of large pretrained models in the temporal video grounding task and investigated the applicability of nlp adapters for a parameter-efficient integration.While we believe our results show the efficacy of incorporating better language models in tvg models, it is important to note that we primarily focused on proposal-free tvg models and thus have no evidence to suggest such improvement would be observed in proposal-based models.Furthermore, as our main goal was to investigate how the chosen models\\u2019 performance varied when only changing the text encoding models, we compared state-of-the-art models using different visual features.While it would be interesting and insightful to check their performance when using the same features as our chosen models , such experiments are out of the scope of this study.Moreover, although language adapters can be stacked before a task adapter for training on the task in a new language, we have only experimented with queries in english.It would be interesting to investigate if language adapters could be applied to tvg in different languages.Finally, as for hardware requirements, our experiments were performed on a single 40-gb nvidia a100 gpu from a large cluster, and we spent about 400 usd on our experimental setup.While experiments with excl and tmlga can be run on smaller gpus with no significant increase in training time , for dori, due to the size of the input features and number of training parameters, we recommend using a gpu with at least 32gb of memory.\",\".As the main idea proposed in this work heavily relies on the external 3d scene extractor, the quality of extractor on our used vsd images largely influences the task performance.However, we reveal in analysis that although suffering from the domain shift issue by the out-of-domain 3d scene extractor, our method still improves the vsd task.We show that when handling the in-domain vsd images as used for training the 3d scene extractor, the vsd performance has been boosted remarkedly.Thus, with better a 3d scene extractor, it can be expected that our system will exhibit much stronger capability and advance the vsd task more significantly.\",\"While imaginary concepts are encouraged in stylized visual storytelling task, it would be better if these literary imaginations are more related to visual contents.In order to improve semantic relevance, we could restrain models from generating visually unrelated descriptions, or make pseudo images more related to stylized stories.However, the former solution is likely to harm the style expression by decreasing stylistic imaginations.For the latter scheme, we have tried to generate pseudo visual inputs with pre-trained text2image model , however, there is a domain gap between photos in vist and images generated with stylized sentences.\",\"In this work, we focused on en\\u2192{fr,de,cs} multimodal mt.At the time of writing, our method can only be applied for en\\u2192x mmt.It is indeed necessary to have access to a modulated object detector in the source language to extract the features and the image-text relationship exploited by our model.This type of modulated object detector is only available in english for the moment.moreover, our method requires large amount of captioning data to perform well.\",\".3 characterizes the effect of general lhomomorphisms, lexsym specifically produces single-token swaps.In images represented as discrete symbol sequences, if a single symbol simultaneously encodes multiple visual features , these features will remain entangled in synthesized examples.It will not exchange substructures larger than a single token, and thus will not synthesize examples longer than those already present in the training set.This is because lexsym targets compositionality but not recursion, which is also required to model the full range of human-like generalizations in sequence learning problems.Lexsym is also sensitive to the nature of the tokenization scheme itself.In morphologically rich languages, for example, lexsym may need to be applied not on top of words or segments, but instead canonicalized morphemes produced by learned morphological analyzers.Finally, lexsym does not induce some of the generalizations obtained other methods for improv- ing compositional generalization, especially those that exploit extra structure in the semantic parsing domain.It might serve as a platform for future versions of those methods that offer greater generality and formal guarantees.\",\"Bigvideo is collected from two video platforms xigua and youtube.However, some videos may contain user information or other sensitive information.Similar to vatex and how2, we will release our test set annotation and the code to reproduce our dataset.For videos without copyright or sensitive issues, we will make them public but limit for research, and non-commercial use.For videos with copyright or sensitive risks, we will provide ids, which can be used to download the video.This step will be done under the instruction of professional lawyers.Though we show that our model with video inputs helps disambiguation, we find that our model could yield incorrect translation due to the lack of world knowledge.For example, model can not distinguish famous table tennis player fan zhengdong and give correct translation.We find this is due to video pretrained models are often trained on action dataset ) and hardly learn such world knowledge.In this work, we do not further study methods that leverage world knowledge.\",\"Although our proposed method is effective in three vision-language tasks, we still have some limitations.Firstly, we utilize t5 to convert the questionanswering format into the declarative sentence in vqa and it works well in most cases, but it still faces out-of-coverage problems, which will affect the following zero-shot prediction of clip.We need to design more rules for these special cases for better conversion.Secondly, our clustering algorithm for snli-ve can achieve strong zero-shot performance, but the clustering centroids are close to each other and the algorithm is sensitive to these centroids.The robustness of this algorithm should be improved.What\\u2019s more, we leverage fasterrcnn in visual fine-grained information extraction, so the detectable object attributes and classes are constrained in a relatively limited object set of faster-rcnn, which may hinder further improvement from visual fine-grained information.The faster-rcnn can be replaced with a better vision module.\",\"There are two major limitations of the proposed tfsgc.The first one is that the effectiveness of tfsgc depends on the quality of the scene graph.Since mscoco does not have sg annotations, we evaluate the parsers in visual genome: for butd\\u002fpatch and vinvl, the of relation\\u002fattribute are respectively 65.we use vinvl\\u2019s sgs in tfsgc and cider improves from 132.if the scene graph quality is poor, then tfsgc will not achieve good performance.When an incorrect node in the scene graph, it also affects the output of the caption.4 , the correct object label should be \\\"surfboard\\\" instead of \\\"train\\\".the second limitation of tfsgc is that if the visual features contain abundant attribute or relation knowledge, then the improvement of tfsgc compared with the classic transformer will be weakened.For example, compared with the butd feature case where the relative improvement of ciderd is 3.6 , the vinvl feature is more powerful since it is trained by much more data samples with more semantic labels, thus the relative improvement is lower, which is 2.\",\"It\\u2019s worth noting that this study has certain limitations.One of the limitations is the limited scope of the training data employed.The altclip model is trained on open-source parallel corpora and publicly available unfiltered text-image pairs.A more careful study of the training data, i.filtering textimage pairs by relevance and text\\u002fimage quality may help to further improve the overall performance of the model.Another limitation is the challenge of evaluating the model in a multilingual setting.Despite our best efforts to include as many benchmarks as possible and to translate from english datasets, the evaluation of the model\\u2019s performance in other languages is not as comprehensive as it is in english.For example, there may be fewer tasks available such as ocr or action recognition in videos in other languages.In addition, the use of machine translation may introduce biases that could affect performance.Future research should focus on creating a more robust and scientifically rigorous multilingual evaluation framework.\",\"Although our approach, mime is empirically observed to outperform several other competitive baselines, we do observe some limitations in the modeling capacity towards memex.As depicted in table 6, there are three possible scenarios of ineffective detection \\u2013 no predictions, partial match, and incorrect predictions.The key challenges stem from the limitations in modeling the complex level of abstractions that a meme exhibits.These are primarily encountered in either of the following potential scenarios: \\u2022 a critical, yet a cryptic piece of information within memes, comes from the visuals, which typically requires some systematic integration of factual knowledge, that currently lacks in mime.\\u2022 insufficient textual cues pose challenges for mime, for learning the required contextual associativity.\\u2022 potentially spurious pieces of evidence being picked up due to the lexical biasing within the related context.\",\"Limitations on the evaluated language models and obtained results: the presented model architecture utilises various pre-trained language or image models.The main limitation of the experimental evaluation is not using other language models.Due to the limited budget and processing power, we have included the language models that have been shown to perform better based on the previous work.Another limitation is that we excluded language models that exceeded the 80 gb memory of an nvidia a100 gpu.it can be explained by using different methods for converting images to textual representations and slightly varying prompting structures.Limitations on the used image models: the limitation concerning the pre-trained image models is that we selected a handful of methods based on their success for related tasks.Including other pretrained models would increase the parameter space and thus increase the budget for the study.Limitations on the selected datasets: all datasets are multimodal tasks where the underlying text is only in english.The choice of the dataset is related to the fact that there are limited multimodal datasets in other languages.The evaluation metric for the ok-vqa dataset requires the output to match exactly one of the expected answers.It counts as a wrong answer even if a slight change in the answer or another paraphrase is given as an output, e.\",\"A limitation of this work is that it is only evaluated on synthesized datasets of cartoons with limited characters and scenes.In the real world application, there might be many different scenes\\u002fcharacters, posing new challenges to the proposed approach.Another limitation is the requirement of supervised training data and resources.Despite the number of trainable parameters of our approach is less than ar-ldm , the model still needs many story-level training data and computing resources.\",\"Keeping narration coherent within a movie is crucial for visually impaired people to enjoy the movie.In this work, we move a step forward for this target by setting the ground-truth texts in the movie clip narrating task as narration paragraphs and providing longer video clips as inputs.However, how to ensure description coherence across different clips within a movie has not been studied in this work.This requires a higher-level comprehending ability of models to process the whole movie and connect different plots.We leave this to our future investigation.\",\"We identify the potential limitations of our work as follow: distant labels may not be available in every application domain , although domain adaptation can be applied in these scenarios.We also believe that distantly supervised contrastive learning can be exploited in tasks involving image and video where surrogate labels are abundant.We also acknowledge that the offline npmi matrix of our proposed ccl method depends on a dataset labeled with multiple classes in each sample.This weighting approach achieves sizable improvement over roberta on 16 in-domain datasets, though it underperforms our npmi-based approach.Our framework does not always work on tasks outside sm.For example, our model underperforms self-supervised cl models, i.as we showed, however, our framework exhibits promising performance on some other tasks.\",\"The limitation of this paper are twofold.First, our method does not provide a recipe for data imbalancement in nlvl task.Thus, our method does not guarantee the effectiveness on edge cases.Second, the choice of feature extractor is considered relatively outdated.Our model does not benefit from the recent development of pre-trained visionlanguage models.On the other hand, using pretrained vision-language models remains in its early stage in nlvl tasks.Not using pre-trained features makes a fair comparison between our model with existing baselines.\",\"Unlike the traditional multimodal contrastive loss focusing more on building the direct link between paired modalities, our proposed unis-mmc aims to leverage inter-modality relationships and potential effectiveness among modalities to create more trustworthy and complementary multimodal representations.It means that unis-mmc is not applied to all multimodal problems.It can achieve competitive performance in tasks that rely on the quantity of the joint representation, such as the multimodal classification task.It is not suitable for tasks that rely purely on correspondence between modalities, such as the cross-modal retrieval task.\",\"This work focused on assessing multimodal degree for recent english vl models.we only evaluated a limited number of models in a zero-shot setting using their image-sentence alignment and vqa heads.this work applied mm-shap to vl encoders.we only applied ml-shap to vl models.computing all possible coalitions between input tokens for shapley values is infeasible because their number is exponential in the number of tokens.Therefore we perform monte carlo approxi- mation by randomly sub-sampling 2p+1 coalitions.This results in approximate mm-shap scores per sample.We argue that as an alternative, one can simply increase the number of sampled coalitions for more exact measurements \\u2013 at the cost of increasing the environmental footprint.But it is not necessary to increase the number of samples when estimating mm-shap at dataset level, because the number of coalitions has very little effect on a data-set wide range \\u2013 given that approximation fluctuations average out.To compute mm-shap at data-set level, one needs to run models in inference mode 2p+1 times, where p is the number of tokens to mask.On an nvidia titan x gpu, computing mm-shap for one image-caption pair can take 2 seconds for albef, 3 seconds for clip.Lxmert is the most expensive and needs 15 seconds, because it computes image features with a cnn backbone for every masking configuration.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"2_visual_image_multimodal\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"2_visual_image_multimodal\"],\"textfont\":{\"size\":12},\"x\":[14.011492729187012,13.563758850097656,13.73110580444336,14.07259750366211,14.02968692779541,14.0562744140625,13.35315990447998,14.050606727600098,14.021933555603027,14.011385917663574,13.969324111938477,13.968438148498535,14.1489896774292,14.056147575378418,13.965522766113281,13.974627494812012,14.124675750732422,14.045980453491211,13.686330795288086,14.145655632019043,14.324444770812988,13.98792839050293,14.263077735900879,14.169135093688965,13.970939636230469,13.828200340270996,14.093598365783691,13.873458862304688,10.968879699707031,14.37747573852539,13.579143524169922,13.789421081542969,14.156478881835938,13.926226615905762,13.995336532592773,14.178508758544922,13.818201065063477,13.945860862731934,14.1910982131958,14.070268630981445,14.139464378356934,13.390596389770508,13.799216270446777,13.970854759216309,14.364916801452637,14.385557174682617,13.927231788635254,13.86274242401123,13.988593101501465,14.218914985656738,14.180549621582031,14.034582138061523,14.023658752441406,14.03663444519043,14.054633140563965,13.892982482910156,14.38626766204834,13.632023811340332,13.851221084594727,13.661580085754395,8.933733940124512,14.392317771911621,14.336225509643555,13.02872371673584,14.435759544372559,14.108428955078125,9.00529956817627,14.123286247253418,14.372445106506348,13.978010177612305,14.071572303771973,14.037449836730957,13.838614463806152,14.43516731262207,13.865060806274414,14.042671203613281,14.068052291870117,13.744613647460938,14.208438873291016,14.224749565124512,14.216421127319336,14.042305946350098,14.06617546081543,14.113310813903809,14.69426441192627,13.86683177947998],\"y\":[4.874200344085693,4.552572250366211,4.483399391174316,4.690883159637451,4.7714691162109375,4.694427967071533,4.671432018280029,4.879252910614014,4.680064678192139,4.778570175170898,4.7350311279296875,4.740472793579102,4.485082626342773,4.601826190948486,4.756874084472656,4.492321014404297,4.590670585632324,4.559532165527344,4.576777935028076,4.198574542999268,4.17182731628418,4.322980880737305,4.142038345336914,4.177412986755371,4.6757001876831055,4.500733852386475,4.656324863433838,4.820201396942139,-0.649021565914154,4.147455215454102,4.175715923309326,4.869722843170166,4.654445171356201,4.8008222579956055,4.4214301109313965,4.311507225036621,4.7635345458984375,4.799968719482422,4.217967510223389,4.445092678070068,4.409996509552002,4.715231418609619,4.436208248138428,4.729881286621094,4.129871368408203,4.125178337097168,4.72091817855835,4.489636421203613,4.324191093444824,4.209523677825928,4.515107154846191,4.31076717376709,4.400446891784668,4.71837854385376,4.383558750152588,4.815260410308838,4.137305736541748,4.64137077331543,4.830683708190918,4.617709159851074,0.2879605293273926,4.125056266784668,4.003645420074463,4.526096820831299,4.089608192443848,4.649999141693115,3.4402499198913574,4.622204780578613,4.1278839111328125,4.613137245178223,4.696290493011475,4.694088459014893,4.1305341720581055,4.096492290496826,4.633578777313232,4.8661274909973145,4.641563415527344,4.831003189086914,4.23732852935791,4.471971035003662,4.26488733291626,4.431140422821045,4.369754314422607,4.313553333282471,2.4336347579956055,4.358803749084473],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"As mentioned above, the current study is limited to the question of whether conditioning turn-taking prediction on the response improves the performance.It does not yet show how the model could be incorporated in a spoken dialogue system.Moreover, this study focuses only on written conversations without incorporating spoken dialogues.Thus, the interpretations can be limited to dialogues that are relatively \\u2018formal\\u2019 without hesitations, repetitions, etc.\",\"We point out potential limitations and ethical concerns of this work.Limitation: data and modeling the dialogues in our dataset are made by playwright, which are slightly different from daily chat.Second, the automatic evaluation metrics for the response generation task can not perfectly reflect the interactiveness of dialogue system.Lastly, our autoregressive generative model simply add the segment embedding to the inputs.Similar to the position encoding in transformer, our coarse method does not make good use of the segmentation, and lacks interpretability.its collection and annotation procedure is designed for videogrouned dialogue understanding and generation purpose, and does not involve privacy issues.Following lsmdc and movienet , we will polish an agreement and release tv shows content under very strict conditions but will open-source all the scrawling code, pretrained features and sampled images.\",\"Despite the contributions of our work, there are also unavoidable limitations of it.First, our method is based on the setting that each utterance in the dialogue except the first one has exactly one addressee.This setting holds tightly in online forums such as twitter or reddit, yet has its limit in group chats or meetings, where an utterance can reply to multiple or no addressees.However, this scenario is relatively rare in multiparty conversations.Considering this scenario is challenging and complicated since the one-to-many reply-to relations can cause the single-turn em algorithm intractable.second, the ubuntu irc benchmark of response generation task is extracted from the ubuntu chat corpus , where people discuss the technical issues on the ubuntu operating system.Due to the lack of human annotators with knowledge of linux and ubuntu, we do not con- duct human evaluations on this dataset.However, we do provide the generated responses in our supplementary materials for those who are interested in the human evaluations.\",\"We discuss three limitations of this work as follows.The first one is the instability of reinforcement learning.Reward-driven policy learning is an essential advantage of this work because it is better equipped with the positive emotion-driven process of esc than existing works and can model flexible esc expression beyond the training data.However, this flexibility also suffers from instability, which calls for additional knowledge or strategies to refine the learning process.The second one is the need for further reference to psychological theory.An advantage of our work is to learn posterior esc patterns integrating the dialogue context and future feedback in the form of rewards.However, there is still other valuable prior knowledge to be referred from psychology studies, e.this kind of prior knowledge can be used as additional knowledge to refine the learning process as mentioned in the first limitation.The third one is that the reward design can be further optimized.The ideal case is to construct a high-quality dataset with human-feedback labels for training reward model.At the same time, the larger parameter of the reward model, the more conducive it is to learn a robust policy and avoid it overfitting to the reward function.However, such optimizations need a trade-off with cost.\",\"Our limitations are as follow: \\u2022 data scale: this paper only employ the wikipedia of wizard dataset, a small scale and well-established knowledge conversation dataset, and lack of the validation on largescale dataset.\\u2022 backbones: this paper lacks the evaluating of other knowledge dialogue model on the proposed method.Actually, we have two reasons to employ the plato.First, the plato can better handle the one-to-many phenomenon, which is suitable for learning our expansion samples.Second, the plato is a pre-trained dialogue model, and its performance on knowledge dialogue generation task has been proved.\\u2022 knowledge expansion methods: this paper only use the synonym and antonym to construct the noised knowledge, which lacks of the comparison of using other data augment method.Indeed, we use two tokenlevel data augmentation methods to prove our statements on hallucination problem in knowledgedialogue generation task.Based on this study, we believe that incorporating other data augmentation methods will also mitigate the hallucinations.\\u2022 manual prompts and responses: this paper designed five prefix prompts, four post-prompts and nineteen euphemistic responses.For ak-more method, we simply randomly choose one prefix-prompt and one post-prompt and concatenate them with the ground-truth response.as for ck method, we randomly select one euphemistic response for the incorrect knowledge.However, we found that the response may not coherent with the query.\",\"The training process of mars needs to rely on manually annotated belief states and action states as semantic states to explicitly model the relationship between dialog context and semantic state representations through contrastive learning methods.We propose mars in the research community and hope it can be better applied to real-world scenarios in the industry.However, the annotated data is expensive, which makes our methods have some limitations in the landing process of real scenarios.\",\"We train the task-optimized adapters based on the pre-trained weights of dialogue lm.Therefore, if applied to other dialogue tasks such as chit-chat and conversational qa system, the performance could be lower than that shown in our research.2 dataset, which is expected that our model does not overfit to the confused labels.Our model inferences in on the end-to-end manner, but trains like modular system for each task.we could adapt the multitask end-to-end learning to our method, which may lead to the better performance.Also, we could analyze the inner working of task-optimized adapters applying xai technologies.\",\".First, we focus on single teacher and single student setup to study guidance generation whereas in real life there often are multiple teachers and students.We plan to extend to multi-party goal-driven communication and also provides a proper testbed to study this problem.Second, there are more nuances in guidance: railroading direct guidance and subtle indirect guidance.We did include them in our human labeling and evaluation interface but did not specifically distinguish them during modeling.Third, due to the constraints on input sizes for most lms, we have to set a context window to study dialogue generation in however, both dm and players have a long-term memory about the comprehensive story progression which might influence how they communicate.As a next step, we plan to use summarization models and adventure books as narrative backgrounds to ground our g4c task with a larger world setting.\",\".The synthetic dialogues sometimes do not express knowledge with sufficient accuracy.Also, some of the synthetic dialogues are less coherent and diverse than the human-written ones.We believe that these issues can be mitigated in two aspects.First, similar to , employing larger lms can help generate utterances with higher quality.Second, introducing knowledge graph and textual reasoning techniques to produce better dialogue flows.In addition, using large lms inevitably requires more computational resources.However, it is still a cheaper and promising alternative to hiring expensive labor.\",\"The main limitation of this work is the usage of explicit knowledge in the knowledge graph.Although using knowledge graphs is a common advantage of most current target-oriented dialogue studies, and explicit relations between entities help to effective and reliable reasoning for the recommendation, there is still a large amount of implicit knowledge in unstructured resources that cannot be extracted as explicit triplets, e., the multidimensional similarity between entities, but can be a further extra supplement to dialog context.In this work, we involve implicit knowledge by generating a path as a natural language sentence, but the knowledge graph is still necessary.\",\"Training data our pre-training data is sourced from 19 existing dialogue datasets.However, it\\u2019s important to note that these datasets may contain noise, such as harmful content, irrelevant file names, and url links.Despite utilizing multiple automatic tools to filter out this content during preprocessing, there is still a chance that some noise may be present in our pre-training data.This could potentially impact the performance of dionysus, making it important to monitor and improve the pre-processing steps continuously.We also know the potential drawbacks of constructing pseudo summaries using the gsg method, which may lead to unnatural summaries for dialogue data.1, which is specifically trained on two dialogue summarization datasets containing natural summaries.This approach enables more realistic pseudo-summaries and enhances zero-shot performance.Although we employ top-m turns as an additional source of pseudo summaries, figure 4 illustrates that gsg+ contributes a minor portion of the pseudo summary, with a 0.3 ratio between generated and topm turns.Our method thus minimizes referent and pronoun confusion, ensuring better coherence than solely employing the standard gsg technique.Training resource to improve our model\\u2019s performance, we employ the \\u201cbetter rouge\\u201d strategy, which calculates the rouge score for both candidates and selects the best one as the final training objective.This data pre-processing process can be pretty time-consuming, taking approximately one day to complete for our pre-training data when utilizing 100 threads.Additionally, we utilize 16 nvidia v100 gpus to train our models, which may not be accessible or reproducible for all researchers.This could present a significant obstacle for those looking to replicate or build upon our work.Test data another potential concern is the test datasets used to evaluate dionysus.The test set size is relatively small, which may not fully represent the breadth of dialogue types that a general dialogue summarization model should be able to handle.This could lead to the model performing well on the test set but not generalizing to other unseen dialogue types.Further, our analysis did not include the assessment of long dialogue summarization, such as lengthy meetings or screenplays.However, our study\\u2019s approach has the potential to handle these scenarios, even though it was not specifically designed for them.By incorporating longt5 or dialoglm , which are known for their ability to process extended input sequences, we expect that they could efficiently tackle this task.\",\"While our proposed method demonstrates promising results and outperforms several state-of-the-art techniques, it is important to acknowledge certain limitations.\\u2022 dependence on pre-trained llms: our method relies heavily on the pre-trained llm\\u2019s quality and the knowledge it has captured.As a result, any biases, inaccuracies, or limitations present in the llm may directly impact the performance of our evaluation metric.\\u2022 lack of diversity in the dataset: the fed dataset, which we use for evaluation, is primarily derived from conversations with the meena and mitsuku chatbots.Consequently, it is possible that our evaluation might not have better correlation with human ratings for other dialogue systems or more diverse conversational contexts.\\u2022 adaptability to new evaluation dimensions: our method currently focuses on eight turnlevel metrics.Extending the method to incorporate additional or novel evaluation dimensions might require further investigation and calibration.\\u2022 computational cost: the current implementation of our approach is around twice as slow as the baseline nll-based method due to multiple times of the inferences of the language model.\\u2022 subjectivity in human judgments: our evaluation metric\\u2019s correlation with human judgments serves as a key performance indicator.However, human judgments are inherently subjective, which could lead to inconsistencies or discrepancies in the evaluation results.Despite these limitations, our proposed method presents a significant step forward in dialogue evaluation, offering a model-agnostic, unreferenced, and training-free approach that captures the human and the system interaction.\",\"In this work, we generate diverse responses through large-scale sampling in the oversampling stage.Although we use the compression and distillation models to speed up, the problem of generation speed still exists.Thus, one of the limitations of this work is the additional time cost when generating large-scale candidate responses.In addition, we use existing dialogue models for dialogue generation, mainly used in short text generation and unsuitable for long text generation, which is another limitation of this work.\",\"In this paper, we present a comprehensive framework for measuring the quality of a dialogue system dedicated to activities of daily living assessments.We have created a new high-quality dataset of human-written questions and answers with corresponding profile information.We are currently working on expanding the dataset by adding more profiles and removing any factual inconsistencies resulting from human error.Although more complex models showed better query classification performances, we need to consider the trade-offs between model size and generation time in the deployment environment to ensure a smooth user experience.We also identify areas where llm performance can be augmented by a knowledge base filled with human written natural language facts, and that this augmentation need not come at a penalty to sensibleness, specificity, or the realistic quality of conversation.General conclusions based on our initial work here may not be possible given the limited number of evaluators and small amount of evaluated dialogues, and this is a major limitation of our contribution.Such an evaluation will need to include larger numbers of human raters to improve the statistical power of the surveys.Recent automatic evaluations may also help improve development efforts, as a sufficiently powerful llm such as gpt-4 may be able to monitor the chatbot for regressions in its ability to speak fluently, sensibly or specifically.This assessment, known informally as the \\\"vicuna assessment\\\", cannot give an evaluation of the chatbot\\u2019s fit-for-purpose, but could be used to compare short conversations from several versions of the same chatbot.This could free up more human resources to evaluate the knowledgegroundedness and fit-for-purpose of future versions.In addition, given more computing budget and more time to engineer prompts, larger language models beyond llama 7b could be further studied or fine-tuned while experimenting with fine-tuning datasets and process.There are also many thresholds and parameters that could be further tested in the development of the knowledge-grounding system, wherein similarity measures inform the system\\u2019s decision to answer using a generative model versus responding with language directly from the knowledge base.\",\"Though our proposed method exhibits superior performance, we also recognize its limitations and discuss potential solutions.Our proposed method for goal-directed dialogue generation suffers from error propagation since the three stages perform in a pipeline manner.After analyzing those generated utterances with low human evaluation scores, we find that the performance of dialogue generation is prone to drop when our color model fails to plan an appropriate dialogue path.We intend to alleviate this issue by introducing some techniques in the cascaded generation, such as noisy channel models.In addition, other issues, such as how to make existing goal-directed dialogue systems more engaging and personalized, are worth further exploring.\",\"Limitation training computation overheads: although having the same inference complexity as any other two-stage retrieval-based dialogue system, our approach requires more computation resources during training as it needs to optimize the two modules in the meantime.Static negatives: we train both modules with a fixed number of random negative samples for a fair comparison with baselines.Actually, more effective negatives can be dynamically sampled by the fast retriever to the smart reranker to further improve its performance.Ethical statement our paper primarily aims to enhance the training method for constructing retrieval-based dialogue systems that exhibit improved effectiveness.The training corpora we utilize, such as the ubuntu corpus and the response selection track of the dialog system technology challenge, are openly accessible and do not give rise to any privacy concerns.Furthermore, the algorithm we propose is designed to be free from ethical or social bias, ensuring fairness and unbiased performance.\",\"Although our work can effectively model the variability issue in dialogue, we acknowledge some limitations of our study.Firstly, our study can work well on the approaches based on rnn, but cannot be employed to sequence models based on transformer, which limits the generality of our approach.The reasons we analyze are as follows.Transformer is not a good architecture for finegrained diversity.The diversity of dialogue includes three granularities of discourse level, utterance level and word level.To model diversity, models will be required to utilize the representation at time t and the relationship between the representation at time t and time t+1 to determine the representation at time t+1.If we only consider discourse-level diversity, our approach and variational mechanisms are easily transferable to transformer architectures.Because we can use the transformer model to encode the entire historical dialogue sequence.Latent variables or summarizing variables only exist between the entire historical sequence and the responses.This will not destroy the parallel structure of the transformer.If we employ a transformer to model diversity at the utterance and word granularity, this will seriously damage the parallelism of the transformer.There are great limitations in the variational transformer models.The transformer and variational thinking is not a good match, which leads to less relevant research.The transformer baselines we compared in the manuscript cover most of the current transformer models that combine variations.Although svt, gvt, plato and dialogved incorporate variational ideas, these models connect all the dialogue history utterances into a consecutive sequence.It is inadvisable to model the finegrained diversity relationship in a parallel structure.Secondly, although our methods can improve the diversity and relevence of responses, there are still gaps in fluency compared with other baselines.\",\"One of the main limitations is that we focus solely on the ietf.Consequently, we can never be completely sure how well our findings generalize to other similar organizations without further annotation.We are also limited by not conducting a hyperparameter search on our models.We omit this step as the main goal is not maximizing performance, but rather data annotation and analysis.In a similar vein, it is likely possible to increase performance by using a more advanced model that is either trained on dialogue-like data or is specifically designed to exploit phenomena specific to dialogue.We also acknowledge that many emails are longer than 512 tokens which is the limit of our bert model and thus might have been cut short.However, most of the emails do fit into this limit.\",\"The dialogue generated for the player exhibits a higher degree of repetition and has a tendency to- wards looping.This limitation exists as we did not focus on generating player dialogue as that is a different problem of its own.To account for this limitation, both the self-diagnosis and the turing quest only evaluate the npc\\u2019s dialogue.Currently, the maximum context window for the dialogue history portion is limited by the max tokens of a given model minus the tokens required for the npc header.Despite being a rare occurrence, it is possible that the dialogue history becomes so long that the model may not be able to generate any responses as there is no more remaining space.We did not experience this problem; however, a workaround would be to discard the oldest dialogue history entry as needed.This approach however may cause the npc to lose out on information that it would otherwise be able to leverage in dialogue.\",\".In particular, its test set can support segment-level metrics, but is not large enough to support reliable dialogue-level evaluation metrics.Due to resource constraints, we also do not report inter-annotator agreement measurements.While we made effort to make our interface low-friction, the demonstration setting still differs from the test-time scenario it is meant to emulate, and such a mismatch may also result in undesired data biases.Because our dialogues were collected before having a trained interpretation model, trajectories always follow gold interpretations.Because of this, the main sources of errors are asr misdetections or user speech errors.In particular, tertius contains data on: 1.misdetections and speech errors in transcription, and how to fix them through commands, 2.misdetections and speech errors in edits, and what intent they correspond to.Some of these limitations can be addressed by incorporating trained models into the demonstration interface, which will allow faster demonstration, and capture trajectories that include actual system interpretations.Though the trained system runs, we have not done user studies with it because it is not production-ready.The t5-base models are efficient enough, but the prompted gpt3 model is too slow for a responsive interactive experience.Neither model is accurate enough at interpretation.We welcome more research on this task! When a human dictates to another human, interleaved corrections and commands are often marked prosodically.we also haven\\u2019t considered how to make use of speech lattices or n-best lists, but they could be very useful if the user is correcting our mistranscription\\u2014both to figure out what text the user is referring to, and to fix it.\",\"Our approach builds on a task schema that characterizes a task-oriented dialogue system\\u2019s domain.For example, the schema captures various attributes of the task.For some domains, when a schema is not pre-defined, it first needs to be extracted, e.in this paper, we used bert as our lm to be comparable with related work, but more advanced models could further improve the performance.A limitation of our task attribute importance scoring method is that it currently produces a static set of weights, reflecting the domain.\",\"A limitation of our work is that our graph traversal algorithm is a heuristic and unlearned algorithm.This leads to a number of nodes after being selected by this algorithm are not suitable for the model to generate conversational questions, and are eventually filtered out by other modules.Furthermore, our algorithm to select the relevant turns in the conversational history to generate the conversational questions is a heuristic of selecting a maximum of three previous turns.\",\"Despite we largely improve the performance of the existing conversational search method, the mechanism of the self-supervised tasks in our ssp is simple and intuitive.Additionally, our post-training method relies on the external query reformulation dataset, which is a compromise under the scarcity of conversational search data.However, the essential contribution of this work is that we point out the significance of modeling dialogue structure , and the phenomenon of contextual semantic vanishing in conversational search for the first time.\",\"While our approach is able to optimise over the retrieved shortlist of replies, it does not improve the initial retrieval from the candidate pool, which still scores individual candidates, rather than reply sets, using the matching model.This is a limitation that is shared with prior baseline methods.A further limitation is that we only consider the monolingual setting, whereas many deployed sr applications have an international footprint.Learning a multilingual matching model in sr is known to have additional challenges.Another limitation is that our model is only tested on public dialogue datasets, due to actual conversations on platforms using sr being proprietary.Therefore, while our techniques should work well in the instant messaging setting, our methods have not been directly tested in the email setting.\",\"Besides its merits, this work still has limitations that could be further explored.On the one hand, we collect source data of mmdialog with only english language.Thus the applicability to other languages would be restricted.On the other hand, we get rid of gifs and video-modality elements in mmdialog.\",\".Based on our empirical observation, we reveal several limitations, which can be divided into two primary categories.First, our proposed spectra method relies on large-scale spoken dialog corpora with explicit word-level speech-text alignment annotation, such as spotify100k.This limits the generality of our model on more spoken dialog corpora.second, our method is mainly designed for speech-text understanding and has not been fully explored for generative tasks.We plan to devise dialog generation per-training objective to empower the model with better generation ability.Third, the work only involves speech and text modalities.We are interested in handling more modalities, such as images or videos, to enrich cross-modal information in joint representations.\",\"One major limitation of uni-encoder is its suitability only for generation-based dialogue systems in which the number of responses is small.A twostage approach is necessary for retrieval-based systems: context-independent encoding methods like poly-encoder first filter out a small set of candidates from the large pool, then uni-encoder can pick out the best response from the pre-filtered collection.However, the increasing research of pure generation methods with alignments bakedin may gradually replace the sft+rl method.Consequently, uni-encoder will have a smaller and smaller impact in terms of application.Nevertheless, because uni-encoder unified all other ranking paradigms, we believe it remains helpful even as a theoretical framework.\",\"As our models are trained on customer-agent conversations in english, they might not be suitable to be used in other domains, types of inputs , or languages.Moreover, as we demonstrated in the paper that the model has limitations in certain question types, the user needs to decide which question types to be used when deploying the system in production.Though the dialogled model performs better, it requires higher computing resources.On the contrary, even though the distilbert model consumes lower memory, its performance is poorer than the dialogled model.\",\"Although our dualgats simultaneously consider the complementarity of discourse structure and speaker-aware context for more accurate erc, it requires more computation and a longer training time.The performance of discourse parsing could be more satisfying in the current stage.Moreover, we directly utilize pre-trained deep sequential models to parse dialogues in erc datasets, which does not address the domain gap problem well.\",\"This study suffers from four limitations.The first limitation is that even though we annotated 850 dialogues manually, which includes almost 200,000 dependencies, there is still room for improvement in the total number of labeled dialogues.The second one is that our parsing method of inter-edu in the inter-utterance situation is simplistic and straightforward, and it can not cover certain difficult labels.It is desirable to propose a more elegant and comprehensive approach.The third is somewhat analogous to the second.the last one is about our pseudo-labeled data selection method.It could be interesting to investigate the iterative process.\",\".In particular, the current fip methods cause task performance degradation.Moreover, the computational cost needed for the injection of prompts and the storage required to store the parameters of every injected model have not been extensively considered.For example, when considering previous conversation history as the prompt to be injected in a long-term conversation setting, fast injection may also be a requirement for real-world application.Updating or adding a relatively small number of parameters may be a potential avenue for addressing the problems.\",\"Although our model achieves competitive results with baseline models, some limitations are summarized as follows.The process of extracting data distributional signatures is time-consuming, especially for datasets with more diverse dialogue patterns.The process of calculating adjacent n-grams is slow.In addition, repeated string manipulation for long texts also needs to be optimized 2.The experiment results are easily affected by the fluctuation of hyper-parameters, especially the signature block hidden size.There is some noise in the distributional signatures.Under different hyper-parameters, noise may have different effects and directly affect experiment results.our model performs poorly when the training set is too small.The distributional signatures of small data interfere with the model.\",\"Although our proposed method asap is able to outperform baseline estimators, an important factor it ignores is the subjectivity of user satisfaction.In practice, different users may have different degrees of satisfaction with the same dialogue.This implies that asap may be effective for some users, but it may also fail to predict true satisfaction for others.In order to adequately simulate a user, it is essential to take the issue of subjectivity into account.\",\"And ethical concerns of this work.Limitation: data and modeling the dialogues in our dataset are made by playwright, which are slightly different from daily chat.Second, the automatic evaluation metrics for the response generation task can not perfectly reflect the interactiveness of dialogue system.Lastly, our autoregressive generative model simply add the segment embedding to the inputs.Similar to the position encoding in transformer, our coarse method does not make good use of the segmentation, and lacks interpretability.\",\"The current dialogue system is mainly based on deep neural network, like transformer structure, which often requires a large number of data sets for training model.However, there are still some deficiencies in our dataset.We will further label and create more dataset to train model.In addition, in order to improve the quality of dialogue, our model parameters are relatively large, which affect the speed of dialogue generation to some extent.We will explore some methods, such as knowledge distillation, to reduce model parameters to improve the speed of dialogue generation on the premise of keeping the quality of dialogue generation unchanged.\",\"Although our proposed method performs well in evaluating the open-domain dialogue systems, it also has some limitations.Our method identifies the dependencies between context and response., human-evaluated metrics can contain a variety of attributes whilst we only identify the large-scale dependencies of semantics and do not disentangle the texts into the attributes of human-evaluated metrics.\",\"The simulated dialogues constructed by kidg are a powerful source of training data for retrieval-free knowledge-grounded dialogue systems.However, there is a clear style difference between the generated utterance and the original document sentences: one is the oral expression and the other is a more formal style.But as shown in table 5, the pdms trained on kidial appear to be more proactive and knowledgeable during conversations.The generated utterances serve as a type of prompt to help the model understand the knowledge.In the meanwhile, our kidg embeds the knowledge into different contexts, alleviating the one-to-many problem in some degree.Although generating dialogues needs to cost gpu resources, it is still a cheaper and quicker way to acquire large-scale knowledge-intensive dialogues.\",\".We believe existing augmentation methods will benefit further improving performance.We design a simple technique of constructing the teacher.More complicated methods should be considered, such as multi-teacher and large teacher.Futuretod in this paper cares about dialogue understanding tasks like intent detection, dialogue state tracking, etc.We hope to extend the similar idea to the generative dialogue pre-trained models and larger tod corpus.Besides, exploiting limited dialogue labels is also valuable to explore.\",\"First, our model is a method of approximating clustering by contrastive learning, but due to the limitations of the model structure, we cannot directly explore the performance of past clustering algorithms on this task.Secondly, due to the large scale of the experiment, our dialogue generator only considers gpt-2.Although the ablation study proves the effectiveness of our model, it is a limitation.Finally, this paper proposes a complete evaluation framework for personalized dialogue generation.It is very effective, but the specific indicators in it still need to be discussed and further studied.In addition, the model assumes that response and persona are independent gaussian distributions in cvae.Although it performs well in the experiment, it does not conform to realistic cognition.\",\"Limitation of the current static graph-based dialogue summarization methods and propose a static-dynamic graphbased dialogue summarization method.It contains two modules, a static graph module and a dynamic graph module.The former injects human prior into the summarization model and the latter encodes the implicit knowledge from a pretrained language model.By fusing these two kinds of graphs with a fine-grained 1\\u00d71 convolution, sdds could adaptively adjust the graph weight and learn the graph structure in an end-to-end learning fashion from the supervision of the summarization task.To validate the effectiveness of sdds, we conduct extensive experiments on three public dialogue summarization datasets and observe significant improvement over strong baselines.We also carefully examine each key component and gives a detailed analysis of sdds for future research.Limitations we discuss the limitations of sdds as follows: although we propose a general framework for dialogue summarization by incorporating both static and dynamic graphs, we only adopt four static graphs to model the dialogue structure.Since dialogue structure modeling is still an active research direction, we believe future advances would further benefit our framework.Despite the strong performance achieved by sdds across three dialogue summarization datasets, we use a pre-trained language model as the backbone of our proposed method, as a consequence, we can not go beyond the limitation of the maximum sequence length of the plm for the dialog summarization scenario like meeting summarization so it remains a future challenge for dialog summarization in the extremely long format.\",\"Our system employs a modified sequence-tosequence architecture to implement the response generator.Since the length of dialogue context increases as the dialogue continues, the generator needs to input multiple long dialogue contexts to the encoder simultaneously, each for a retrieved entity.This may cause redundancy in the input and lowers the proportion of kb-related information.\",\"Increase in the number of dialogue turns the dsd dataset has a higher average number of turns compared to the sgd dataset.This is a limitation in terms of completing a task with fewer dialogue turns, one of the objectives of the tod system.This is because dsd was created by extracting and augmenting the target turns of sgd.However, assuming that the tod agent trained with the dsd is applied to real-world scenarios, we expect that the agent will play a role in reducing the number of user rejections by expanding the range of choices to users through compare-based disambiguation.\",\"Graph models require four or more utterances to form meaningful conversation connections and model their dynamics.In some cases, conversations that derail are not sufficiently long and may be best modeled by simpler sequential models.Any of these models will work best with asynchronous conversations where there is a time lag between the turns to allow for moderation after forecasting.\",\"In this paper, we propose a method named autoconv, which means automatically generating information-seeking conversations with large language models.Though it has achieved great performance on both quac and coqa , there are still some limitations that should be noticed.in our experiments, we use opt-13b as the llm for generating synthetic conversations due to the limited computational resources.Larger models should be considered to further understand the potential ability of autoconv, e., gpt-3 , opt-175b , bloom-176b , and glm-130b etc.8, there is still a gap between our synthetic dialogues and human dialogues.It is important to improve the quality of synthetic dialogues so that we can further alleviate the dependence on human annotation.\",\"One trade-off of limiting the prediction space using an extractive pointer module is that it does not support prediction of multiple slot values which is necessary for some dialogues in the multiwoz 2.to keep the architecture simple we do not consider cases in which slots take multiple values in this work, but we can effectively adapt our model for this setting by introducing sequential query tokens for each slot.Another limitation is that the span representation requires a computation of o complexity where n and lans represent the length of context and answer span, respectively.For very long answers this might occur significant computational costs compared to exist- ing span prediction approaches which have o complexity.However, this can be alleviated by adding a simple sampling and filtering step during training and prediction.\",\"We acknowledge the following limitations of our work.first, we only collect 1k human-to-human conversations with 14.6k utterances due to the high cost of the annotation process.This brings difficulties for the learning of news grounded dialogue generation.Second, each conversation in newsdialogues is grounded on one news article, which may have limited knowledge for real-world applications.1, the image information in the news article is neglected in this version, which requires further exploration.large language models have shown great few-shot learning ability and generation capacity on various tasks, e., gpt-3 , opt-175b and bloom-176b etc.It is important to investigate the performance of llm on newsdialogues, while this has been neglected in this work due to the limited computational resources.\",\"The main limitation is that the in-domain noise is hard to recognize in noisy multi-party conversations.Though our proposed rarm achieves the best performance compared to all baselines, we find that if the content of the noise is close to the multi-party conversation\\u2019s content, the average accuracy of all methods is not high, how to improve the performance on these hard samples is worthy of further study.\",\"Enabling dialogue agents to join multi-party conversations naturally is undoubtedly a crucial step towards building human-like conversational ai, especially as such technology becomes more affordable and portable.More crucially, research on multi-party conversations has the promising potential to improve the interactive experience between humans and machines.Although the proposed method has shown great performance and generalization ability across various models and tasks, however, we never lose the sight of the other side of the coin.The proposed method requires full interactions among utterances in multi- head attention of transformers.Therefore, computational complexity and inference latency may be worth considering when deploying to online dialogue systems.Aside from the well-known difficulties in deployment, the proposed method was only evaluated on the domain-specific datasets, i., ubuntu irc, considering the constraints of dataset resources.\",\"This work focuses on mitigating the negative transfer and catastrophic forgetting issue in multi-task dialogue generation.All technologies built upon the large-scale plm more or less inherit their potential harms.Besides, we acknowledge some specific limitations within our methods: 1.the construction of pseudo labels requires dependency parsing with spacy, which is timeconsuming.But we only construct pseudo labels offline in the training processing and it causes no latency at inference.we instantiate our modular framework using minilm as the backbone of the reader within the programmer, and t5 as the backbone for the content operators and linguistic operators.We did not try other instantiations although the modular framework does not depend on the specific initialization choice of modules.Theoretically, any generative plm could be the backbone of these linguistic and content modules.we aim at decomposing the response generation into relatively independent and composable operators.Currently, the division of dialogue skills and module functions is in a heuristic way inspired by linguistics.Thus it remains a future research question about how to design modular architecture in a more data-driven way.\",\"First, due to the lack of datasets to evaluate the mpdrg task, we perform our experiments only on the ubuntu irc benchmark and pre-train our model only on the domain of ubuntu chats.However, the potential of our approach goes far beyond that since it is applicable to any open-domain multi-party dialogue dataset.additionally, the pre-training process solely relies on the addressee information of individual turns, disregarding the reply-to relations within the dialogue history.This oversight prevents the model from benefiting from valuable contextual cues necessary for a comprehensive understanding of the multi-party dialogue.\",\"There exist some limitations in our work.Livechat is a chinese-originated dataset involving unique cultures and abundant replying styles.However, this intensifies the difficulty of fully understand- ing the content of this dataset.Fortunately, the same data construction pipeline can be applied to streaming platforms of other languages, like tiktok.And currently, our livechat is only sourced from 351 streamers on douyin, not sufficient to train a general chatbot.We believe that livechat helps get one\\u2019s foot in the door to the wonderful and diversified live scenarios and a dialogue model pre-trained on the considerable amount of videosourced dialogue data among cross-platforms is promising.Besides, livechat contains some noisy spoken language segments that are not easy to read after transcribing from the asr tool.The upper bound data quality is limited by such third-party tools.as for the dialogue-matching method, we simply implement a combination of bow and bert for semantic matching, which needs further optimization.Other limitations from the training perspective can also be highlighted.For example, contextual background information is not considered in our modeling.That includes history dialogues in multiturn settings and information from other modalities, like the streamer eating in front of the camera.In addition, we have not explored enough of our annotated basic profiles.In our primary experiments, we found that directly adding basic information such as age, gender, location, and other room information has limited influence on the model performance.We account for the fact that these basic profiles have limited connections with reply styles and contents in livechat.Also, note that we remove the repetition part of a streamer\\u2019s response before training, while it is useful to maintain this pattern in practical application.\",\"Application to other benchmarks a central limitation of our work is that the main experiments are based on a single task-oriented dialogue benchmark.While there are multiple other natural language understanding benchmarks like xnli, xquad, mlqa, and paws-x that can also be used to back up our claims, we argue that this is outside the scope of this paper.The main objectives of this paper are to first come up with a new definition of a crosslingual continual learning challenge and then to give an example using a comprehensive and realistic benchmark like task-oriented dialogue to catalyze more research in that direction.Choice of realistic permutations for more realistic setups of continual learning, we need to come up with an approach to define continual learning annotation scenarios of languages.Rather than using brute force with all possible ways the languages could be annotated at different stages, a principled way would be more desired.Since it is hard to tell if there is any logic or pattern in the annotation process itself and given the sheer amount of realistic scenarios, we chose one scenario experienced by some of the users: a model is built for a user, then the user reveals that more languages are desired.We test in our work the plausibility of continual learning approaches where the sequence moves from one language to another without repetition of the same language.Working on scenarios where the data from different languages are integrated as soon as they are annotated, implying different languages for different hops, is out of the scope of this paper.Data and model size analysis in this paper, we pick certain model expansion approach variations to analyze the effect of model components and two data distribution scenarios.However, analyzing extensively the effect of the scale of data and model size is beyond the scope of our work.We agree that different data sizes can be used and it is interesting to analyze different supervision levels such as using different proportions of the data for each language and simulating few-shot scenarios.We believe that for lowresource scenarios we need to investigate specific approaches to continual learning like meta-learning.Application to other transformers another possible limitation of our work is the restriction of the evaluation to a base model on top of m-bert transformers.With the advent of transformer-based encoders as strong pillars for transfer-learning, several transformers such as xlm-r have been proposed more recently.Although those models have been shown to outperform m-bert on numerous downstream applications especially on low-resource languages , m-bert is still largely used due to its reduced number of parameters.In our specific continual learning challenge, efficiency is a top concern as we are training in multiple hops and benchmarking on different models.So, m-bert has been feasible in our use case.\",\"The current dialogue system still has some limitations.For example, although the current crg model can make the output contain the key concept words in the knowledge path, due to the large scale of the pre-training model, the output semantics of the current method are still not very interpretable and controllable.A feasible way is to explore new fine-tuning methods to approach high-level semantic style control.In addition, our current dialogue system lacks human qualities such as empathy, factual correctness judgment, and moral common sense representation.A key breakthrough is to explore a goal-oriented dialogue dataset with richer dimensions.\",\"Similar to other augmentation methods, dialogps demands high requirements for computing resources.The training is performed on up to 8 v100 gpus.On dailydialog: a vanilla transformer only needs 50 minutes while a non-pretrained dialogps takes about 80 minutes when k = 1.other baselines take about the same amount of time as dialogps k = 1.but when dialogps achieves its performance peak , the training takes 4 hours.Most of time cost comes from sampling which is difficult to be accelerated by gpus.\",\".Current directions with promising results include using llms for conversation synthesis , where high-quality multi-party conversations are synthesized through prompting, and the conversations can be grounded in specific characters or personas.Such synthesized conversations may also help adapt methods for conversation analysis and response generation to rarer domains that may not be well-represented in natural corpora.\",\"This paper mainly focuses on the generalized intent discovery task in task-oriented dialogue systems.Our proposed decoupled prototype learning framework well decouple pseudo label disambiguation and representation learning through protopical contrastive learning and prototype-based label disambiguation, and achieves sota performance on three gid benchmark datasets.However, our work also have several limitations: we only verified the effectiveness of our dpl framework on gid task, but the adaptability of dpl in more unsupervised \\u002f semi-supervised settings, such as unsupervised clustering and ood intent discovery, is worth further exploration.We follow standard experiment settings as previous work, and assume that each ood sample must belong to a corresponding intent cluster.However, a more realistic scenario is that there may be noise samples in the ood data.These noise samples do not actually belong to any cluster\\u002fcategory and are some outliers.\",\"Although we use data from dialogues, we do not model collaborative reference, i., we do not model continual mutual adaptation.Instead, we focus on the speaker\\u2019s adaptation to the listener in a single turn, which is certainly a simplified setup.Furthermore, our plug-and-play approach still requires the training of simulators per listener type.However, as we keep the speaker and listener models frozen and use the output obtained from them to train the simulators, this allows us to reduce the required amounts of training.We train the models from scratch using photobook data and do not make use of state-of-the-art large pretrained visionand-language models that are nowadays commonly based on transformers, which could be considered a limitation.We opted for this setup as it is more aligned with our research questions, allowing us to control the domain-specificity of the models.We also acknowledge the imbalance in the set sizes of the domains, as well as the possible lexical and visual overlaps in the samples across domains.The overlaps may facilitate the adaptation of certain sentences from one domain to another , and this is not uncommon in human communication.\",\"In this work, we investigate the use of pre-trained language models for long-term english conversations.While we expect a modular approach may be effective for other languages when given a capable language model, it should also be noted that further research is needed to confirm the applicability of our findings to other languages.For instance, though bloom is trained as a multilingual language model, we only implement mpcbloom in english and evaluate its english capability as a open-domain dialogue agent.Meanwhile, a modular system can create additional inference overhead or error accumulation.The system performance would become much bet- ter if we optimally choose the lm for each module.For example, we could use gpt-3 td2 for the memory processor, while we employ opt-175b for the utterance generator.In terms of evaluation methodology, our human evaluations of mpc and its analysis face the same challenges as previous studies on evaluating interactive conversational tasks., there is currently no definitive evaluation method for determining the best chatbot model.Additionally, there are several factors that must be taken into account during data collection and interpretation, such as annotator subjectivity, instruction bias, and crowdworker working conditions.to some extent, this evaluation setup reduces cultural bias and platform homogeneity compared to using mturk workers alone.However, the limitations of this approach should be acknowledged and this may further complicate the analysis when controlling for mpc\\u2019s performance on different subgroups.Lastly, we note that running mpc requires at least as much memory as its underlying language model, making mpc infeasible to even load on a single node for heavy models such as bloom176b.\",\"While we show that applying gap can result in a significant improvement in the generalization capability of lms, especially for dialogue tasks, we are only able to show 300 gap runs for each lm size in this work.furthermore, a separate validation set of the tasks at interest are needed in order to choose the best checkpoint when performing gap.\",\"We hereby discuss the current limitations of our work: the simmc-vr dataset, similar to the simmc 2.0 version, focuses on shopping scenarios , one of the most common everyday activities that virtual reality could enable users to do from anywhere, anytime.We have not tested whether the models would generalize to domains outside of the shopping experiences, thus we cannot speak to the transferability of our results to environments with very different visual properties than what our virtual environments provide.In this dataset, we hand-design several possible dialog acts that we assume are common for human buyers, as well as their associated scenarios.This may not exhaust all the possible interactions a shopper can do with the assistant.However, we emphasize that the coverage should be sufficient for common shopping experiences.Additionally, although most of our proposed subtasks should be modeling generic user-assistant multimodal dialogue interaction and thus could be transferred well to other domains, the domain specific mm-dst may not generalize as much.Nevertheless, they should still be transferable to similar environments.The audio of the simmc-vr videos are generated by automatic tts, which may fall short to represent the natural human speech.However, we do not foresee this causing problems for multimodal dialog modeling, which this work mostly focuses on.\",\"Natcs is partially annotated with dialogue acts, intents, and slots, which are annotated independently from the initial collection of the conversations.While decoupling annotations from collection was intended to facilitate natural and diverse dialogues, the methodology is more timeconsuming and expensive than previous approaches that use pre-structured conversation templates to avoid the need for manual annotation.In particular, natcsspoke requires multiple participants engaging in synchronous conversations, followed by independent manual transcriptions and annotations, making the approach particularly time-consuming and difficult to apply for large collections.Furthermore, this decoupling of annotations from collection has greater potential for annotator disagreement.While the complexity types and annotations are mostly language-agnostic, natcs is restricted to en-us customer-initiated customer service conversations between a single agent and customer in a limited number of domains.The annotations included are primarily intended for applications related to task-oriented dialogue systems.Further, we note that natcs closes the gap from real conversations along many metrics, but still falls short along some dimensions.We find that real conversations are more verbose, more believable, and less predictable.We also note that comparisons in our paper focused on a limited number of taskoriented dialogue datasets with different collection approaches, and did not exhaustively include all pre-existing dialogue datasets for comparison.\",\"Although llm-eval has shown promising results in assessing open-domain conversations, it is crucial to acknowledge its limitations.Firstly, the performance of our method relies heavily on the large language models underlying it, which may exhibit biases or generate unexpected outputs.If the language model misinterprets the evaluation schema or prompt instructions, it could lead to inaccurate evaluation scores.Secondly, the choice of llm significantly influences the evaluation results, as demonstrated in our analysis.While dialogue-optimized llms produce better performance, this selection may limit llm-eval\\u2019s applicability for particular tasks or dialogue systems.Thirdly, our approach employs single-number scoring for each evaluation dimension, which may fail to capture the subtleties of human judgments, particularly for subjective aspects like engagement, creativity, or humor.Lastly, the effectiveness of llm-eval hinges on the quality and clarity of the prompts and evaluation schemas.Creating such prompts and schemas may require domain expertise and knowledge of llm behavior, posing challenges for non-experts.To overcome these limitations, future research can focus on exploring alternative prompt designs, refining evaluation schemas, and expanding the method to cover a wider range of evaluation dimensions and dialogue system types.\",\"The introduced dataset has a moderate scale, as it is currently designed for fine-tuning instead of large model pretraining.Our proposed collection scheme can be futher applied to enlarge the dataset.Moreover, as we focus on english, the data source has multiple language versions written by experts.Hence, extending causaldialogue to multilingual is straightforward.With reward labeling, the dataset can be more intuitively used for offline rl.Meanwhile, the dataset includes personality descriptions that can be used for personalized dialogue generation, even though is not the focus in this paper.Finally, training a generative model on dialogue domain can require various computational costs, depending on the aspects such as lengths of input and output texts and number of model parameters, as well as special designs to prevent misuses.\",\"Our proposed approach is designed explicitly for evaluation of task-oriented dialog systems, and is hence unlikely to generalize well to chitchat systems.Most traffic to our platform comes in the form of task-oriented interactions.User turns in the traffic we analyze tend to be quite short and direct, so our model is unlikely to perform as well on dialogs driven by long-form user utterances.\",\"We note several important limitations of this work.Perhaps most importantly, our dataset is \\\"naturalistic,\\\" but not actually \\\"natural\\\" in the sense of independently occurring in the world.Though the interactions between our participants are real, the task itself is ultimately artificially constructed.In a real-world negotiation over something as valuable and significant as a house, the negotiating parties will be much more invested in the outcome than our experimental participants, whose actions change their outcome to the order of a few dollars.This difference in turn could lead real-world negotiating parties to speak differently and possibly employ substantially different strategies than we observe.Methodologically, our study has a few important limitations.Firstly our analyses are based entirely on language that has been automatically transcribed , and while this helps with expense and scale, these transcripts could be missing important subtleties that influence the outcome.uncover an important limitation of these systems, finding significant racial disparities in the quality of asr transcriptions.The linguistic feature analysis we perform should be treated as largely exploratory, and provides suggestive and correlational rather than causal evidence for the relationship between language in the interactions and negotiation outcomes.Lastly, there are further linguistic and interactional phenomena at play that we have not yet integrated into the analysis presented here.For one, we have access to the audio channel of participants\\u2019 actual speech, but we have not analyzed it in this work.There could very well be acoustic cues in participants\\u2019 speech that are as significant to the interactions as the textual features analyzed here, particularly speech prosody which has been shown to communicate social meanings that could be highly relevant to negotiation like friendliness.This particularly extends to more interactional questions of not simply who said what, but what was said in response to what and in what way.For instance, existing research has shown that acoustic entrainment in dialog has important social associations with dialogue success.broader impacts this research, collectively with prior and future related work, has the potential to advance our understanding of negotiation, a ubiquitous human activity.Our dataset can enable future research into the dynamics of human bargaining as well as interpersonal interactions more broadly.By employing the findings and insights gained from such research, individuals may be able to enhance their ability to negotiate effectively in various settings, such as salary negotiations, personal relationships, and community initiatives.Meanwhile, we must acknowledge that while a better understanding of language as an instrument in social interaction can be empowering, it may also be used as a tool for manipulation.\",\"The main limitation of our work is its focus on english datasets.While this was due to their popularity and extensive usage , it overlooks datasets like duconv and naturalconv which employ more explicit annotation instructions regarding dialog \\u2018path\\u2019 and topic transitions.Studying the way these restrictions affect conversational attributes, is necessary for a more comprehensive understanding of the problem.Another limitation is the lack of an empirical investigation on how\\u002fif these artefacts and biases affect the final objective of kgd modeling, i.This of course is not easy in the absence of a less biased dataset, but synthetic datasets \\u2013which have become much better in quality and flexibility thanks to large language models\\u2013 can probably provide reliable estimations, which we plan to explore in future studies.\",\"Our proposed metric is mainly designed for a turnlevel evaluation of dialogue systems.We recognize that our metric may not generalize to other evaluation scenarios directly, such as dialogue-level evaluation or human-chatbot interactive setups.5, the easiest way to extend our metric to a multi-turn dialogue evaluation is by evaluating every turn in a dialogue individually, and then aggregating their scores.However, as the dialogue-level evaluation is not considered during the development process of our metric, it is not clear whether such a simple extension would be applicable without a decrease in performance.Nevertheless, as turn-level evaluation is a fundamental component to build a holistic evaluation framework for a dialogue, we believe that it is an important task to investigate better evaluation metrics for individual responses.\",\".This work specifically proposes improvements to the controllable generation portion of mixed-initiative dialogue systems.However, dialogue policy planning is still an important problem to consider.In order to evaluate generation improvements, we hold dialogue policies fixed \\u2014 in the static evaluation, we condition on ground truth dialogue intents, and in the interactive evaluation, we follow the same dialogue intents prescribed by the rap system.To this end, a mixed-initiative dialogue system cannot consist solely of a generation module powered by prompting.There needs to be a set of rules or models that govern how a system can regain control of a conversation; the generation module is just a means of enacting these rules.due to these limitations, we did not conduct an interactive evaluation in the esc setting.Emotional support conversations are highly personal, as circumstances vary across individuals.It would have required having study participants pretend to require support regarding a fixed scenario, or for participants to disclose their personal issues, which can raise other ethical concerns.Moreover, dialogue policy planning is not straightforward for emotional support, due to this highly variable nature.Effective support strategy planning requires expert knowledge.However, deploying prompt-based systems may be less useful for the purpose of setting new benchmarks on existing leaderboards with a plethora of data.Such setting already have plenty of well-annotated conversations and simple fine-tuned models can often achieve strong performance.Proper guardrails should be put inplace prior to productionization of any dialogue system, prompt-driven or not.While we witness strong overall response quality both in terms of human evaluation and automatic metrics, language models can generate contradictions.System builders may consider employing guardrails for dialogue consistency ) and coherence ), among others.As with any training set, instructgpt and other llms have been trained on finite amounts of data.Instructgpt has not been trained on data after 2021.This is also true of training corpora such as p4g or esc; these corpora were published in 2019 and 2021, respectively.rap attempted to remedy this by incorporating retrieval for factual questions, which we also embedded into our prompting approach, but this knowledge base is also finite.a possible solution is internet retrieval , but search engines can also yield misinformation, which leads to hallucination.llms are computationally expensive, and in the case of models such as instructgpt, they are not open source.However, in this study, we did not have access to equally powerful open-source models such as opt 175b, nor the appropriate hardware to load such a model.We performed initial experiments with much smaller models which fit our hardware constraints such as gpt- j 6b, but there was much higher variance in performance.This is supported by the fact that many reasoning capabilities do not seem possible with models smaller than 175b parameters.Given our limited budget for human evaluation, we opted to use the best performing llm we had access to, instructgpt.Prompt optimality it is possible that we do not use an \\u201coptimal\\u201d set of prompts as we did not mine prompts or perform soft prompting.However, prompt optimality itself is a problem in dialogue generation, because open-ended dialogue evaluation is a difficult task.Most automatic evaluation metrics do not align well with human ratings in dialogue.This makes it suboptimal to use as a discriminator in soft prompting, for instance.Most existing work that does search for optimal prompts or tunes prompts works with tasks that have clearly defined automatic evaluation, such as sentiment analysis or table-to-text generation.Moreover, human ratings are expensive and not scalable for systematic optimization.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"3_dialogue_conversations_dialog\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"3_dialogue_conversations_dialog\"],\"textfont\":{\"size\":12},\"x\":[8.803038597106934,8.769072532653809,8.925328254699707,8.684524536132812,8.694107055664062,8.759001731872559,8.789107322692871,8.966529846191406,8.691180229187012,8.710948944091797,8.838711738586426,9.0755615234375,8.690327644348145,9.25385570526123,8.703097343444824,8.779243469238281,8.796465873718262,9.109729766845703,8.839390754699707,8.972161293029785,8.741108894348145,9.015848159790039,8.751465797424316,8.950371742248535,8.851380348205566,8.758508682250977,8.85625171661377,9.206896781921387,8.847291946411133,8.792131423950195,8.730589866638184,9.0468111038208,9.094619750976562,8.79432201385498,8.662964820861816,8.8854341506958,8.728361129760742,8.741266250610352,8.638940811157227,8.726975440979004,8.831901550292969,8.810755729675293,8.988513946533203,9.162132263183594,9.231175422668457,8.780074119567871,8.968523025512695,8.869771957397461,8.717719078063965,8.880363464355469,8.922595024108887,8.691802978515625,8.704130172729492,8.781693458557129,8.918852806091309,8.66059398651123,8.916647911071777,9.051214218139648,8.925167083740234,9.172795295715332,8.833559036254883,9.051694869995117,8.618342399597168,8.804460525512695,9.080790519714355,8.850582122802734,8.826745986938477,8.784867286682129,8.86044692993164],\"y\":[0.3727523982524872,0.05632997676730156,0.394091933965683,0.19148652255535126,-0.009454028680920601,0.005956251174211502,0.3555779457092285,0.21972744166851044,-0.03304301202297211,0.008733333088457584,0.2580350339412689,0.6955015063285828,0.06579966843128204,-0.009113936685025692,0.03157553821802139,0.18641433119773865,0.039921835064888,0.5575271844863892,0.19171081483364105,0.4250294268131256,0.2824844717979431,0.09242776781320572,0.17986507713794708,0.25310683250427246,0.1789131909608841,-0.014649180695414543,0.003779833670705557,0.3528074026107788,0.028545565903186798,0.02607499435544014,0.3580016791820526,0.47089460492134094,0.5960489511489868,0.0596415214240551,0.06160072982311249,0.4254792630672455,-0.009132357314229012,0.11286314576864243,0.117836132645607,0.04742314666509628,0.004075799603015184,0.4368227422237396,0.25250595808029175,0.12979598343372345,-0.02086646296083927,0.028482384979724884,0.3782106041908264,0.4245314598083496,0.27685776352882385,0.36287516355514526,0.5386481881141663,0.28749150037765503,0.08521866798400879,0.3535066246986389,0.05651787668466568,0.10208811610937119,0.3506891429424286,0.2675573527812958,0.46138542890548706,0.28489193320274353,0.04522198811173439,0.5281559824943542,0.21968725323677063,0.39311379194259644,0.47858789563179016,0.304879754781723,0.48809051513671875,0.056824080646038055,0.22359442710876465],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"As pe remains largely undeciphered, our results can only be evaluated on the small subset of numerals which we have manually disambiguated.As noted in the paper, these may be easier than the rest of the corpus, meaning our evaluation can only give an upper bound on model performance.We use a feature-based classifier to give interpretable results which can more easily be shared and discussed with non-technical experts in assyriology.A limitation of this approach is that model performance depends on the choice of input features, and features which are effective can sometimes seem arbitrary.We attempt to justify the features used by our model by explaining in table 2 which aspects of the script each is intended to capture.Lastly, pe numerals are just one aspect of a complex and multifarious decipherment problem.Our results alone cannot paint a complete picture of this script, and must be interpreted in relation to results from outside of computer science.\",\"Some limitations of this work are listed below: \\u2022 the architecture and configuration for the custom plm are the same as bsc-bio-ehr-es.Another architecture and configuration could obtain better results.\\u2022 the textual data come from just one provider.Using data from several providers could help with generalization.\\u2022 the custom plm has not been compared with other plms in language tasks such as named entity recognition or question answering.This comparison can help to understand if the custom plm can outperform available plms in other types of tasks.The data was extracted from administrative and clinical records from an insurance and health provider that specialized in labor accidents.Within this data, it is possible to find personal and sensitive information such as personal and company names, addresses, health information, pre-existing conditions, and diagnoses, among others.An anonymization process was not carried out since the model will be used for internal purposes and will not be released.As a process of memorization can occur in the plm, we believe it is best to keep the model private because privacy attacks can extract personal and sensitive information.We did not test the models for any bias under any protected field.Therefore, the trained models could benefit certain patients or accidents over others in the insurance decision.If a biased model is deployed in this provider\\u2019s systems, it could harm patients with their insurance coverage decisions.\",\"Due to strict data protection regulations and a high annotation workload in the clinical domain, obtaining more diverse target tasks to validate our approach is a challenge.In this work, we focused on only two use cases in german clinical applications and need to extend our experiments to english or other non-english languages in the field.\",\"The methods described in this paper do not leverage any external medical knowledge, a technique that has been shown to be effective by other studies.And like other methods based on large language models, in theory, our models are also prone to hallucinations and omission of key-clinical concepts.Although the impact of the task c model as a data augmentation tool is undoubtedly positive , qualitative error analysis of patientdoctor conversations produced by the model showed that the output contained a small number of dialogue turns, and each individual turn was too long, packed with information.Producing conversations with a more natural flow should yield an even better boost on downstream tasks, and we leave exploring such methods to future experimentation.We also recognize that n-pass summarization for task b with higher values of n should be able to cover the entirety of the input conversations in the task b datasets, albeit with diminishing returns as n increases.We hope to evaluate them in future iterations of similar shared tasks.\",\"Although our method delivers optimal results, it doesn\\u2019t comply with data protection regulations like hipaa, even though azure offers a hipaacompliant option.From a privacy standpoint, deploying a local model such as led might be preferable, but our findings indicate that further work is needed for this method to achieve satisfactory performance.Regardless, when creating automated conversation-generation systems, healthcare providers and developers must ensure that the entire system\\u2014including text-to-dialogue, data transmission and storage, and model inference\\u2014complies with privacy and security standards to maintain trust and avoid privacy breaches in clinical environments.Hence, developing an automated conversation generation system from clinical note entails several\",\"Biodlm adopts dlms as backbone models.Compared to plms with other training objectives, dlms may miss language modeling benefits and squeeze representation space.Besides, our benchmarks can be extended to more biomedical discriminative tasks, such as relation extraction, document classification, and entity disambiguation.\",\"In the experiments conducted by the team, several limitations were observed.Firstly, the handling of the semi-structured structure was not accomplished perfectly.additionally, addressing the clinical domain properly was not achieved.\",\"Although our proposed framework beats several baseline methods for medical dialogue generation, there is still room for progress.We exploit an entity flow and a dialogue act flow to improve dialogue understanding and guide response generation.However, our summarized dialogue acts are limited in the types and granularity of functions they denote.We can manually annotate more medical-related dialogue acts in our future research following the soap notes.Besides, more medical knowledge with different formats, such as medical articles and medical examination reports, can be incorporated.Finally, it is crucial to recognize the potential risks associated with system utilization and the possibility of patient privacy leakage.A collaborative approach involving both dialogue systems and medical professionals should be considered.This will ensure that responses are endorsed by physicians and stringently overseen by reliable authorities.\",\"Although our models performed well in all downstream tasks, the models also have some limitations.One of the limitations is the lack of varied biomedical corpus.Hence, we plan to work on integrating clinical documents e.ehr data, specifically physician notes, to make the model more robust to various kind of biomedical documents.The models can also be enlarged by using continual learning strategy from well-known french pre-trained language models.Camembert can be used as a base model and the training can be continued using our biomedical corpus, like biobert and others did.Moreover, our models used 512 sequence of tokens and more longer sequence lengths can be used as seen in the long language models like bigbird.We are currently working on a new version of alibert with more data and a greater diversity of corpora that include text from ehr and medical notes in our corpora.Finally, we also plan to train alibert to generate biomedical texts for different purposes.A reasonable amount of computational resources was used to conduct this study, since approximately 20,160 hours of gpu computation were used to create the three pre-trained models presented above.The total environmental cost according to green algorithm 12 is equivalent to 1.this computational cost and environmental impact should be taken into consideration when training such a model.\",\".A challenge we faced was that mimiciii was unevenly distributed across the races for the patients represented.We had significantly more white and black patients than any other race of people and even still many more white than black patients.Therefore we continue to express the need for more representative, inclusive, and balanced datasets.Further, the dataset did include ethnic breakdowns, but due to the lack of patients present in those ethnic groups we could not include caribbean or middle eastern patients as well as many other subgroups in our analysis.If we had more time, we would like to partner with a medical facility that regularly serves marginalized and non-marginalized groups, steadily, to develop a dataset which captures more features that could reveal some bias and ensure they are more descriptive to get higher quality data.Better feature selection and using more demographic features.To ensure the quality of the aforementioned data, we will perform a causal analysis to identify the specific features that cause testimonial injustice.We anticipate that variables such as age and education level of patients need be included, as these factors have been shown to affect how patients are treated, particularly in the medical field.\",\"In our work we faced numerous types of limitations that fall under different categories.Data our relatively small dataset size limits our analysis, especially with the use of language models.Furthermore, the label distribution is skewed across the different specialties , which affects model performance, robustness and generalizability.The differences in distribution might be the result of how the data was collected, which was not in light of the anchor words, or due to the domain\\u2019s nature and\\u002for the medical providers\\u2019 language of that specialty.Furthermore, the time frame that the data was sampled from might manifest certain biases that are different from other time frames.Finally, our datasets are only representative of a small number of specialties from two medical institutions.Patient populations and providers may vary greatly across medical fields and additional institutions.Task the formulation of the labels for our task imposes limitations and challenges.Stigmatizing language is subjective and can vary between the perspective of the patient and the medical provider.As a result, we are aware that our medical experts\\u2019 annotations might impose a bias.Additionally, the negative connotations of language might be ambiguous and can change depending on a medical expert\\u2019s identity, background and specialty, which creates a bias that is hard to mitigate.Computational resources we only used irbapproved servers to access the dataset and perform the experiments.Because these platforms had limited computational capacity and lacked the specifications required to build more complex neural models, we were not able to include more recent language models in our experiments that might have yielded better performance.\",\"In this work, we adopt three gcns to exploit the inter-code relations under different levels.However, this may bring extra training difficulty and the risk of over-parameterization to the model.Besides, during the preprocessing stage, we adopt a wordlevel tokenizer and cbow to obtain their embeddings for mimic-iii texts and code descriptions.However, this might not be enough to represent the words since medical documents have some special characteristics, but we do not take them into consideration.We tried in our work with other pretraining strategies, such as clinicalbert , bert , biobert and biowordvec.We added as well the bpe tokenizer in order to capture the meaningful medical sub-word units.However, the results are all far from satisfactory.\",\"There are several limitations to our framework.Specifically, since observations are introduced as guiding information, our framework requires observation extraction tools to label the training set in advance.Then, the nodes contained in the observation graph are mined from the training data.As a result, the mined n-grams could be biased when the overall size of the training set is small.In addition, our framework is a pipeline, and the report genera- tion performance highly relies on the performance of observation planning.Thus, errors could accumulate through the pipeline, especially for small datasets.Finally, our framework is designed for radiology report generation targeting chest x-ray images.However, there are other types of medical images that our framework needs to examine.\",\"Since our approach, tm-hgnn, aggregates every note during icu stays for patient representation learning, it is inappropriate for time-series prediction tasks.We look forward to further study that adopts and applies our approach to time-series prediction tasks.\",\"The experiments in this paper were performed using openai\\u2019s gpt-3 api.While running locally does not require a large amount of computational resources, the server-side service cannot be easily replicated and requires a large amount of computational resources.Additionally, given the inherently restrictive nature of medical text, we can only evaluate our approach on a small corpus of english-language dialogues taken from the dataset of a single company\\u2019s medical service, which we cannot release due to privacy concerns.Finally, given summarization is a challenging task to evaluate, we rely on a small number of expert human annotators and automatic metrics.However, additional annotations may be helpful and it may also help to study and report labeler agreement when reporting human preferences.\",\"Evaluation of generated text is difficult evaluating automatically generated text, including clinical notes, is generally hard due to the inherently subjective nature of many aspects of output quality.Automatic evaluation metrics such as rouge and bertscore are imperfect and may not correlate with aspects of expert judgment.However, they are frequently used to evaluate model-generated clinical notes and do correlate with certain aspects of quality.To further validate our findings, we also conducted a human evaluation with three expert physicians.As noted previously , even human evaluation of clinical notes is far from perfect; inter-annotator agreement is generally low, likely because physicians have differing opinions on the importance of each patient statement and whether it should be included in a consultation note.We also found low interannotator agreement in our human evaluation and speculate this is partially due to differences in specialties among the physicians.Physicians 1 and 3, both from family medicine, had high agreement with each other but low agreement with physician 2.Investigating better automatic metrics and best practices for evaluating clinical notes is an active field of research.data privacy while our gpt-4 based solution achieves the best performance, it is not compliant with data protection regulations such as hipaa; although azure does advertise a hipaa-compliant option.13 from a privacy perspective, locally deploying a model such as led may be preferred; however, our results suggest that more work is needed for this approach to reach acceptable performance.In either case, when implementing automated clinical note-generation systems, healthcare providers and developers should ensure that the whole system \\u2014 including text-tospeech, data transmission storage, and model inference \\u2014 adheres to privacy and security requirements to maintain trust and prevent privacy violations in the clinical setting.\",\"When evaluating our model in a cross-dataset adaptation setting, our experiments indicate the importance of using a retrieval dataset.It is challenging to procure high-quality and volume retrieval datasets, especially in low-resource domains such as the medical field.Fortunately, the vqa-rad and slake datasets we evaluate on contain professionally annotated medical images.We also overcome the lack of data by creating a synthetic dataset from the medical roco image-captioning dataset.Additionally, our model struggles with questions requiring multi-step reasoning, such as knowledge graph, abnormality, and position questions.\",\"Such as time and resource constraints.However, incorporating the expertise of medical professionals during the development process may provide valuable insights into the clinical implications of the generated summaries, resulting in the creation of more clinically relevant and useful models.Given the effectiveness of incorporating human feedback in various nlp tasks , we recommend future research to explore the performance of involving medical experts in the development and evaluation through a human-in-the-loop approach.\",\"Some potential limitations of this work include a relatively small sample size, which may limit the generalizability of the results.Hindi is spoken differently across india, hence the translations made by the three translators may not be representative.Study did not examine the potential impact of regional dialects or variations in hindi, on the accuracy of the diagnosis.Finally, the study focused solely on the use of speech narratives and did not explore the other types of data, e., imaging or genetic data, which could be important for the diagnosis of ad.\",\".Firstly, minimal prompt engineering was used, and context could have been provided in the form of few-shot or chainof-thought examples, which have been shown to increase accuracy.Strategies like self-consistency decoding and retrieval augmentation are also promising for healthcare where varying factual content of responses from each model even to the same prompt poses a clinical risk.Additionally, we did not compare the llm responses to those of human experts, which is an important comparison for appropriateness and safety.\",\"As established, medical coding is an important task for the healthcare industry.Efforts toward its successful automation have wide-ranging implications, from increasing the speed and efficiency of clinical coders while reducing their errors, saving expenses for hospitals, and ultimately improving the quality of care for patients.However, with this goal in mind, our study presented here should be contextualized by the two main limitations that we identify and outline below.As with other data-driven approaches, the evaluation performance discussed in our paper is limited by the choice of the mimic-iii data set.This data set could be seen as lacking diversity, as it only features a fraction of all possible icd-9 codes and contains medical notes collected in english from a specific department of patients belonging to a specific demographic.While our approach does not make any explicit assumptions about the nature of the data other than the hierarchy of labels, in absence of formal guarantees, we cannot make rigorous statements about the efficacy of our approaches on clinical data gathered in different settings, such as other languages, countries or departments.The second limitation is of a more practical nature, since 2015 the icd-9 coding system is being phased out in favor of the more expressive icd-10 system, thus icd-9 coding has limited applications in practice.However, as with its predecessor, the icd-10 codes are organized in an even richer hierarchy, which should enable the successful application of our proposed approach.\",\"There are several limitations to the experiments conducted in this project that should be acknowledged: \\u2022 selection of the best pre-trained language model for prompt-based learning: the evaluation method used to compare the performance of bert, gpt-2, and t5 in the context of manual templates and manual verbalizers may not be entirely accurate.The performance of these models did not show significant differences, making it difficult to determine the best model for prompt-based learning.Furthermore, other domain-specific plms, such as bio-bert, which may be better suited for handling clinical data, were not considered in this project.\\u2022 limited exploration of templates: the experiments utilized a limited number of templates, particularly for soft and mixed templates.These templates were primarily based on prompts derived from manual templates.Further experimentation is needed to explore different patterns, such as varying the position and length of soft token sequences or using soft tokens in mixed templates to replace manual tokens.\\u2022 comparison with frozen plms: the experiments did not include a comparison between fine-tuned and frozen plms, as done in taylor\\u2019s study.This comparison could provide valuable insights into the performance trade-offs between these two approaches.\\u2022 addressing the effects of imbalanced datasets, several strategies have gained popularity.1) re-sampling techniques, for example, monte carlo simulation analysis, can be used to balance class distribution by oversampling the minority class, undersampling the majority class, or the combination of these two.2) data augmentation techniques, such as the use of generative adversarial networks , can generate new examples for the minority class by applying transformations to existing data.3) furthermore, machine learning approaches like bagging and bootstrapping can reduce variances by implementing a \\\"voting system\\\" that enables models to make better decisions.\\u2022 finally, it would be advantageous to develop a post-processing step that generates a table displaying all treatments along with their corresponding temporal information.This would create an end-to-end system that physicians could use as a practical tool.Future research should address these limitations by exploring a broader range of plms, templates, and experimental setups to provide a more comprehensive understanding of the performance characteristics of prompt-based learning methods in the clinical domain.Application to some more powerful computational resources will also extend this work.\",\".Finally, we have tested the model on proprietary ner datasets and radling-kg has yielded 0.93 macro f1 on less represented anatomies like neck, while for the highly represented anatomies, f1 is as high as 0.this actually shows the potential of using radlex in radiology pretraining.Thus, in a real world setting with high imbalances in datasets, radling-kg is more robust.In future we would like to explore more ways to infuse knowledge by using text description of context like , retrieving context from biomedical knowledge graphs like snomed 4 and umls, and more robust knowledge embedding methods.We would like to experiment with larger datasets and models, and work with more downstream radiology applications.\",\"The limitation of this work was the use of rouge-l as the evaluation metric.Given the many acronyms and synonyms in medical writing, rouge-l, based on the longest common sequence, does not capture the many nuances in its score.Researchers have shown concerns for the rouge score and have developed metrics for summarization that are more semantically aware of the ground truth , but their usability is yet to be validated.Training large language models from scratch uses a considerable amount of carbon footprint.Fine-tuning large language models for downstream tasks is one way to reduce carbon footprint but still needs to be cost-effective.As the ai community progresses in this field, developing a cost-effective and carbon-friendly solution is needed.The nlp field is moving towards prompt-based methods with larger llms , so the next step for this research is to experiment with soft prompting approaches to address low resource settings and leverage prompt tuning in llms for the problem summarization task.\",\"The use of auxiliary classifiers at every node of the decision tree is not feasible when the hierarchical tree is huge, such as the large hierarchical terminologies for medical literature indexing.Besides, in table 1, even though the integration of the hierarchical information shows a consistent improvement in both the baseline and mimlroberta models, these improvements are still within one standard deviation of micro-f1.Lastly, it is worth noting that we do not focus on large language models since our approach is to explore improvements on a published state-of-theart model.While they might improve accuracy, a careful exploration of those on a new task is beyond the scope.\",\"This study is based on a single clinical cohort consisted of 1479 patients, which may limit the generalizability of the results to other clinical cohorts.This specific cohort of patients may not be representative enough of the general population, which may inject certain level of bias brought by the dissimilar distributions of gender, age, race, etc.While we envision the generalization capability of the language models is applicable to other clinical prediction tasks, the focus of this work is majorly about prognostic prediction of cancer immunotherapy, and we hereby have not provided solid evidence to prove that the success can also be extended to other relevant trials.Additionally, we have yet only compared a limited set of transformers and language models, and it is possible that other models may perform better on the tasks evaluated in this study.Finally, it is important to note that while the models in this study achieve high accuracy in clinical prediction, the ultimate value of these models in improving patient outcomes will depend on how well they are integrated into clinical decision-making processes and the impact they have on patient care.\",\"There are a few limitations in our work: first, the rules developed are totally based on frequency filtering and not further checked by medical experts, we are not sure whether there are any hidden template or patterns in the clincal note summary.Second, due to time limitation, we did not conduct second time pre-training for the language models, t5 was originally trained from generic text of which the genre is quite different from the clinical domain.765 training examples may not be enough for the model to learn.Third, we would like to give a full test of the more recent large language model , but we cannot fine tune it with the open free api.\",\"We acknowledge three limitations in our experiments.First, in our second experiment, we fixed the window size for each type of note to be n\\u002f2.A more comprehensive investigation could also search for the optimal window size for each note type.Second, although we explored one fragmented window configuration p = both, we did not explore other fragmented window configurations due to resource constraints.Lastly, we did not investigate more types of clinical notes because mimic-iii has limited examples for other note types.\",\"The mimic-iii corpus includes a discharge summary for each admission.However, it is limited to patient\\u2019s time in the intensive care unit , meaning that the patient\\u2019s history for any time after transfer from the department is lost.2 we discuss the statistics that justify this conclusion.\",\"Our best results were obtained using a few-shot prompt of the text-davinci-003 model from openai.While the technical barriers to this method are very low due to the ease of use of the model api, the cost of querying the api can become prohibitively expensive and this limited our own experiments with the model10.This cost would rise significantly more if the text-davinci-003 was finetuned to perform these lay summaries.If this is a concern, then using the open-sourced biogpt models may be beneficial.It should be noted that performing the fine-tuning process is itself expensive and requires access to high-end gpus.Practitioners should investigate parameter-efficient finetuning techniques if access to these gpus is an issue.\",\"Professional coders are trained to find sufficient, as opposed to exhaustive, evidence for each code.Our profee coders were instructed to find all the evidence for each code.However, given the large number of notes in some mimic encounters, they might only manage to annotate most of the evidence.For inpatient, there might be more bias among coders towards finding sufficient evidence: namely, there were many cases in which one coder found evidence that another had not, but during the adjudication process, both coders agreed it should be included.Thus, although we have opened the door to automatic evaluation of evidence extraction systems, some metrics, such as recall on our dataset, might underestimate the true recall of a system.We observed inconsistencies and human errors while cleaning up the data.Coders sometimes only annotated partial evidence, leaving out modifiers like \\\"acute\\\", \\\"moderate\\\" and \\\"bilateral\\\".For example, we consider \\\"bilateral pleural effusions\\\" as the correct evidence but only \\\"effusions\\\" was highlighted, and for \\\"weakness in his lower extremities\\\", only \\\"weakness\\\" was highlighted.Another source of error is due to the limitation of the annotation tool which does not support highlighting and linking discontinuous spans of text as a single evidence for a code.As a result, some evidence may contain extra tokens between the correct evidence tokens and others may miss part of the evidence when the supporting text spans are far apart.We tried our best to fix these issues, but some errors likely remain in the dataset.\",\"And boundary conditions of our models in order to make more accurate, informed, and measured claims about their clinical outcomes.For each patient transcript, we generate 9 versions of the original text that amplify types of language errors in both semantic and syntactic categories.Table 2 compares the aphasia and dysarthria models\\u2019 performance on all categories of errorinfused transcripts, with f1 as the evaluation metric.\",\"In our work, we manually annotated small portions of corpora.Such limited size is justified by the time-consuming task of temporal annotations and the requirement of expertise for toxicity event annotations.Although our temporal representation seems to perform well with other clinical reports containing information about a different type of cancer from that on which it was trained , such results must be validated on clinical reports containing information about additional cancer types.Additional experiments are also needed to validate the generalizability of our event-independent representation, such as evaluating it on other hospital or data warehouse clinical reports with various structures and evaluating it on other extraction tasks with different event definitions.\",\"The paper does not cover all types of possible methods and models for the generation of clinical notes.The challenge datasets are also limited in terms of size and medical specialities.Further experiments and evaluations are needed to validate the best performing methods on other datasets and scenarios.\",\"The use of machine learning models in clinical decision-making requires an understanding of the reasoning behind model predictions.Our study focuses on improving the performance of smaller models using context and subtask blocks.While the subtask state labels provide some interpretability, we have not explored its impact on trust among medical professionals.\",\"Although our findings suggest that biomedical language model pre-training is quite robust to suboptimal tokenization, we note that our work has a few potential limitations that should be explored further.The use of a biomedically relevant subset of the sigmorphon shared task dataset for evaluating biomedical term tokenization is a straight-forward and reasonable strategy, however, it is important to highlight that the resource was not created for this purpose and might not be perfectly aligned with ideal biomedical tokenization.Additionally, we would like to point out that even though our biovocabbert tokenizer outperforms other equivalent tokenizers like pubmedbert\\u2019s, it severly underperforms the best possible segmentation accuracy.It is therefore possible, although unexpected, that a tokenizer which performs biomedical tokenization at even higher levels could lead to sudden improvements in the pre-training process.Finally, we note that the effects of the biovocabbert\\u2019s much larger vocabulary size, almost three times larger than pubmedbert\\u2019s, on the pre-training process were not explored in depth.Nevertheless, given that some previous work argues that larger vocabularies lead to slight improvements in downstream tasks, our main conclusions are likely to hold.\",\".In this paper, we follow previous studies and only utilize english radiology report datasets to verify the effectiveness of our proposed model, which is limited in verification in other languages.The main reason is that most publicly available radiology report datasets center on english.In addition, our model needs relatively more parameters than the models only using findings to generate impressions.\",\"In the evaluation datasets, as well as small model sizes relative to the large pretraining corpus.We make the scholarbert models available on huggingface.While we cannot share the full public resource dataset, we have provided a sample of open-access articles from the dataset in both the original pdf and extracted txt formats to illustrate the quality of the pdf-to-text preprocessing.Limitations our 12 labeled test datasets are from just five domains ; five of the 12 are from biomedicine.This imbalance, which reflects the varied adoption of nlp methods across domains, means that our evaluation dataset is necessarily limited.Our largest model, with 770m parameters, may not be sufficiently large to demonstrate scaling laws for language models.We also aim to extend our experiments to tasks other than ner, relation extraction, and text classification, such as question-answering and textual entailment in scientific domains.\",\"This paper is limited to the study of clinical ner models using an encoder-only architecture.however such methods require a careful prompt selection strategy and cannot be directly compared to supervised models.This paper is also limited to cross-lingual transfer to french and german.Do in their general-domain comparison of strategies for crosslingual transfer.However evaluation datasets are lacking for that purpose in the clinical domain.\",\"We finetuned pre-trained nli models for tdg parsing.Both data sets we used were in english.To apply this model to other languages and to get the best results, pre-trained nli models or nli data sets might be required for the new language.Templates to verbalize the temporal relations in the new language are also required.The clinical data set we used in this work only contains ehrs from one institution: mayo clinic.Clinicians from different hospitals can have different writing style or use different templates when writing the notes.\",\"It is important to mention some limitations of our work.Firstly, it would be wise to evaluate the impact of the tokenizer on the performance of the models to ensure that this is not the main reason for the observed performance gains.Furthermore, we can not affirm in this study whether the medical domain transfer observed from english to french using continual pre-training on pubmedbert can be generalized to other languages or other domains.Finally, it is possible that training a chubert model with more diverse private clinical data and in a larger quantity could have brought notable performance gains on private tasks.A considerable amount of computational resources was used to conduct this study, since approximately 18,000 hours of gpu computation were used to create the 7 models presented here, as well as about 7,500 hours of gpu for debugging due to technical issues related to model configurations and poor performance, for a total of 25,500 hours.The total environmental cost, according to the jean zay supercomputer documentation4 is equivalent to 6,604,500 wh or 376.45 kg co2eq based on the carbon intensity of the energy grid mention by bloom environmental cost study also made on jean zay.This makes the present study difficult to reproduce and to transpose to other languages when limited material resources are available.\",\"The authors want to use the opportunity given by this column to highlight the fact that the definitions generated by this procedure do not all meet the standards required for presentation to users, or for reasoning-required scenarios, due to their imperfect quality.We release this dataset for building retreival-based systems, and evaluate large biomedical language models on the definition-generation task.In addition to the imperfect quality of the generated definitions and the presence of hurtful definitions in the dataset, it might also be useful to consider the bias induced by the choice of snomedct as our source of knowledge.While extensive, snomedct does not cover all possible relationships between concepts, and by biasing the output towards relationships present in snomedct, we might perpetuate existing biases in the data.Another limitation is that we only evaluate the generated definitions on three metrics, but more could be relevant depending on the application.Finally, our rating of what is considered acceptable insight was biased towards what could possibly be condensed in short definitions , but longer definitions might sometimes be required to express the full range of nuance required by biomedical concepts.It is however difficult to estimate the value of omitted information.\",\"One limitation of our study is the small size of the test set, which may impact the generalizability of our results.Finally, the e3c guidelines have been designed for clinical entity extraction and entity-linking via umls entities.After the first step of manual annotation, some spans of the entities have been modified to fit as close as possible to the semantical concepts found in umls.For instance, clinical entities could be split into separate disorder concepts, and the extent of a disorder candidate could be reduced to fit with a concept.These biases could induce additional difficulties in finding the correct span for a given model.\",\".For classification, we will experiment with model configurations and explore alternative machine learning algorithms.For summarization, we will refine prompt strategies, incorporate domain-specific knowledge, and investigate various fine-tuning techniques.Lastly, conducting user studies with medical professionals will provide valuable feedback to assess the utility and accuracy of our generated summaries in real-world clinical settings and further refine our approach.\",\"While our research and empirical results support specific evaluation metrics for the task of clinical note generation according to a given evaluation criteria, more results, including testing on additional datasets are needed to further validate these findings.Our manual annotations followed clear and structured guidelines, but could still contain some level of annotator bias and have an average pearson inter-annotator-agreement of 0.\",\"Our findings are limited to medication entities, the only semantic class that is annotated in all available corpora.Moreover, we had to exclude bronco150 for long-span experiments due to a mismatch of entity definitions.Although the label alignment decisions are somewhat subjective, they are made based on a thorough inspection of definitions and samples.The differences in annotation quality and biases may be playing an uncertain role in the models.However, making statements on the impact of the annotation quality is challenging, since each work followed a different annotation protocol and reports different measures of annotator agreement.This is another area where harmonization efforts might be warranted for future research.Furthermore, exploring different hyperparameter configurations lied out of scope for our work, but could have a substantial impact.Mainly, the results from the transformers comparison could shed different\",\"In this work, we have identified two key limitations of coad that can be further examined in future research.The first limitation is that coad only allows for the querying of one symptom at a time, making it unsuitable for scenarios where multiple symptoms are present.However, coad has superior performance in the main metrics for automatic diagnosis.To relieve this limitation, potential solutions include relaxing symptom feedback conditions and allowing the model to produce symptoms sequentially until a stop signal is encountered or querying the top k symptoms in a single turn.Additionally, coad has some restrictions on input format, requiring standardized symptoms and values.To make it more applicable to end-to-end settings, an natural language understanding module is required to parse plain text and obtain the input symptom sequence, and a natural language generation module is needed to translate the predicted symptom or disease to text.The ultimate goal of automatic diagnosis is to support the dialogue between doctors and patients, after coad determines the symptom or disease, rulebased nlu and nlg modules can help to achieve the text to text communication.\",\"There are a few limitations pertaining to the training data we used, some of which are listed below.our domain adaptation of llms was performed on english reports only; therefore, it may not work out of the box in a multilingual setting.The paper utilizes the mimic-iv dataset for dapt training, which might include overlapping data from mimic-iii and mimiccxr.Consequently, there is a potential risk of information leak in this method.There is a data imbalance concerning imaging modalities and anatomies covered by our training data.For example, regions such as extremities, neck, spine, and shoulder are underrepresented in the dataset, and report summarization related to those regions needs thorough evaluation.a study is needed to examine the diversity of patients represented in the data and how it impacts the model\\u2019s performance for underrepresented communities.Different radiologists and radiology departments have distinct preferences and styles for writing reports.Moreover, clinical referrals occasionally dictate the extent to which certain details are documented in the report.No study has been conducted on the consistency, uncertainty, or information richness of the report.Aside from the training data, the model\\u2019s space and time throughputs may render them unsuitable for on-premise and\\u002for at-the-edge applications.This aspect presents an opportunity for further research on how to best quantize and deploy radbloomz within the clinical workflow to enhance efficiency for radiologists.Additionally, the paper utilizes the mimic-iv dataset for dapt training, which could contain overlapping data from mimic-iii and mimiccxr.Consequently, there is a potential risk of information leak in this method.\",\"One limitation of our method is that when training larger models, it requires more computation resources, whose cost is relatively high.However, after pre-training, we will release our models so that readers can directly use them without pre-training again.Broader impacts we provide a new generative pre-trained model on molecules and text.On one hand, the model can be used to speed up scientific discovery, like molecule design, drug optimization, etc.On the other hand, once the model is trained on clinical data , it might lead to personal information leaky.We will enhance data filtration to anonymize all personal information, and will design new models to protect the information.\",\"The pre-training methodology used in this work applies a masking process at the sentence level that requires scoring the relevance of each sentence within the text.Therefore, this implies additional computational costs, limiting the scalability of our approach.Due to time restrictions, the appearance of hallucinations in the generated radiology reports by our models has not been measured.\",\"The major limitations of this work are: \\u2022 we show results on two public datasets, from bio-medical and bio-chemical domains.These results may not generalize to other domains.\\u2022 our results indicate benefit in low resource settings, while no appreciable benefit is seen for medium or high resource settings.\\u2022 our method relies on gpt-2, a large language model that needs humongous compute resources and a long training time.It takes about 2 hours to generate 50 samples, versus the baselines like vanilla gpt-2 taking 30 mins or entinj taking about 10 mins to generate same number of examples with much less memory requirements.\\u2022 we use quantitative measures to evaluate the quality of text generation, which might not be enough to capture the quality of generated text.Gold standard of measuring the quality is human evaluation, which is expensive.\",\"Data unbalancing is one of the limitations of our work.for our experiments, we have used all samples from the iu x-ray dataset.But from the mimic-cxr dataset, we have used only 44578 reports out of 227827 reports.Our results for the mimic-cxr dataset might differ when we use the whole dataset.To evaluate the generated pathological descriptions, we consider the pathological descriptions that we extract from original reports as ground truth.To evaluate the generated full reports, we generate templated reports by replacing ground truth pathological description in normal report template and consider it as ground truth.So it considers the abnormalities from the original reports and the normal sentences from the normal report template.Automatic generation of chest x-ray reports will make it easier for radiologists to diagnose and write reports.Our model achieved comparable performance with state-of-the-art models on chest x-ray report generation.In realistic scenarios, it is still a long way from being used clinically.\",\"Due to time and budget constraints, this work remains limited in a number of important ways.The relatively small size of our corpus and its specificity to covid-19 necessitates the development of systems with richer inductive biases and the ability to effectively transfer knowledge from related corpora like fever.Additionally, due to the large size of cord-19, the database of scientific literature from which we draw the abstracts in check-covid, it is difficult to evaluate abstract retrieval components like vespa with our annotations.There are likely many abstracts in cord-19 in addition to the ones we\\u2019ve selected that contain evidence relevant to any given claim, precluding both measurements of abstract precision and the evaluation of notenoughinfo in the full fact-checking pipeline setting.Finally, as the claims in checkcovid are drawn from western, english language news sources and annotators, they are likely unrepresentative of the full range of covid-related information in need of fact-checking online.\",\"On the adoption of ask an expert pertains to certain domains where the system must be deployed locally to uphold privacy concerns, such as mental health systems aiming to safeguard patient data.In such instances, relying on external api services becomes less feasible.However, it is not always necessary to utilize all the knowledge of large expert models.And for specific domain use cases, such as mental health, it is unlikely that the full size of the model is indispensable.\",\"We identify three limitations to our approach.First, parsing research claims and automatically classifying a sentence\\u2019s purpose are not solved problems.It is more prudent to surface novel claims supported by original research than an author\\u2019s allusion to other research as background context.Second, the domain of biomedical scientific text is complicated by wordy prose, hedging, and long-distance anaphora.These aspects make natural language understanding challenging and present implementational challenges for tokenization, including truncating long sentences and extracting meaning from out-of-vocabulary tokens.Third, commonsense reasoning for detecting contradictions in biomedical text requires expert background knowledge and a working definition of when contexts are sufficiently aligned such that two claims are called contradictory, which may differ depending on the use case.We believe that context sensitivity and interpretability analysis of llms for nli in challenging domains like this using attention mechanisms or frameworks such as maieutic prompting are particularly fruitful research directions.\",\"While this dataset is unique and pioneering, its size is limited, and it involves specific patients.To enhance the generalizability of the findings, a larger dataset may be required.for instance, we observed a higher frequency of the \\\"intervention in- formation\\\" category within cognitive engagement, likely because the intervention predominantly follows a q a format.We hope that the coding scheme established in this study can aid future research in refining this category with finer granularity, based on specific intervention theories.\",\"During the manual labeling process of the notes, we searched for keywords to select for the namedentity-recognition and compared them to those appearing on the summary.For the system to work effectively, the list of problems and diagnoses should appear in its entirety in the original text columns.most of the examples had summaries with additional diagnoses and problems with words that could not be extracted from the original text.In addition to these cases, having an inconsistent use of acronyms leads to different versions of the same term in the original text and the summary.Finally, when comparing both lists of tokens, the items found in both segments had a distinct order of appearance, having examples score low for only counting one of them as a match in rouge-l, where the list of words is the same but in a different order.\",\"Limitation of this work is the small speech database used in this paper.We are actively building a detailed annotated large speech and language database from hundreds of patients with post-stroke aphasia, with the aim of training and developing asr for pathological speech.We expect that such work will promote greater confidence in the use of ai and specifically nlp for healthcare intervention.\",\"There are a few limitations pertaining to the training data we used.radling has been trained on english reports only, and therefore will not work out of the box in a multilingual setting.There is data imbalance with respect to imaging modalities and anatomies covered by our training data.For example, regions like extremities, neck, spine and shoulder are underrepresented in the dataset, and expected understanding of observations related to those regions may be limited.There needs to be a study on the diversity of the patients and radiologist expertise represented in the data, and how it impacts the performance of the model for underrepresented communities.Different radiologists have different preferences and styles of writing reports.In addition, clinical referrals sometimes dictate to what extent some details are documented the report e.There was no study on the consistency, uncertainty or information richness of the report.Asides from the training data, there may be space and time throughputs of the model which could make them unsuitable for at-the-edge applications with limited bandwidth.\",\"Generating clinical notes or summaries of clinical conversations using nlp technology is a rapidly developing field with great potential.However, there are several limitations to this technology that must be considered.Firstly, nlp models rely on high-quality data to achieve accurate results.In the medical field, obtaining such data can be challenging due to privacy concerns and regulations.Secondly, the complex and technical nature of medical language poses a challenge to nlp models, which may struggle to understand and interpret medical terminology and abbreviations accurately.Additionally, clinical conversations often involve sensitive information that requires careful handling, making it important to ensure the security and privacy of generated clinical notes.This field is considered a safety critical area, where high precision is expected, therefore, the use of nlp models in such clinical settings must be performed with caution and under medical professionals\\u2019 supervision to ensure the generated notes\\u2019 accuracy and reliability.\",\"This method for designing and annotating clinical text for a specific clinical use case can be beneficial for researchers needing to annotate a corpus.first, the experiences are based on a specific clinical case and focus on the qualitative aspects.Details of certain parts of the design and annotation process will likely need to be adjusted based on resources available to other researchers.This can include the data selected for annotation, the number of annotators available, and the annotators\\u2019 level of expertise.For instance, the use case in the design process is based on using 8 annotators to annotate 100 synthetic ae notes over 5 sessions.Second, expertise and additional time are required to generate synthetic notes for annotation.\",\"Limitation of the existing medical ner models is their poor performance on nonus and eu prescriptions due to bias in the training data, which is almost exclusively based on useu centric medical content and vocabulary.In our approach, we have deliberately chosen to have explicit dependence on aspects that vary across geographical regions , which enhances the applicability of our approach.To further limit the model bias and minimize distributional differences between training and production settings, we have trained our models on prescription images that are randomly sampled from customer uploads.These often include low resolution and improperly positioned images.In future, as the scope of deployment changes, we plan to periodically retrain the model with training images by sampling from the production data.Health safety: one of the primary concerns in prescription digitisation is the impact of errors on patient health and adherence to health regulations.To alleviate adverse outcomes, we have multiple guardrails.First, we present the top three suggestions along with scores for each medication for two-fold review by customer and pharmacist.Second, to avoid prescription abuse and comply with regulations, there are additional checks based on the prescription date, patient purchase history, and recommended limits on medication quantities.Usage for a limited scope: our proprietary system has been trained for a specific-use case, i., prescription digitization with acceptable performance on primarily english printed prescriptions for india region.We plan to use the model within this limited scope and expand usage only after adequate benchmarking.To limit the risks of misuse, we do not plan to release this system externally.\",\"Our prescription digitization approach has a few limitations but is still effective for a broad enough application domain and permits future enhancements that address these limitations.First, our system uses an off-the-shelf text extraction tool that provides accurate extractions on printed prescriptions but has variable performance on hand written data depending on the legibility of the handwriting.In future, we plan to build a specialized extraction model trained to recognise medical practitioner\\u2019s handwriting to replace aws textextract.Further, multiple components in our approach have been trained on primarily english transcriptions.Extension to other language prescriptions requires access to medical vocabulary and training data in those languages.Note that aws textract supports multiple languages and can be readily paired with an automated translator to convert the content to english.We did not consider this option since multilingual prescriptions in india tend to have mixed content with medications written in english itself.Lastly, the performance of multiple tasks such as advice block detection, medication attribute extraction and matching-canonicalization depends on the coverage of the available medical catalog.\",\"Our experiments only used the abstracts of the articles and we did not consider the full text.Actually, the full text might provide valuable information for the decision about the similarity, while making it harder at the same time, since the text is much longer.However, full text is usually not available for many of the articles in pubmed, which would result in even shorter datasets.We did not train any model based on the available datasets, as carried out by.Indeed, our goal was to evaluate the dataset based on general-purpose text similarity algorithms that had not been fine-tuned for a particular dataset.For the annotation of the facets, we simply asked the annotators to select sentences that were part of the research goal.However, it might not have been carried exactly as in the annotation of the smafira-c dataset.\",\"The limitation of our study is that we only evaluated our model on a limited set of spoken language transcripts.We believe that additional attention should be given to features specific to ad patients, such as pauses and filler words in speech.Furthermore, the lack of diversity in the data may also adversely affect the model\\u2019s performance on unseen samples.\",\"We note two limitations of our paper.First, our work does not extensively evaluate all the available pre-trained models that could be suitable for this task, e., electra , biolinkbert , gatortron , radbert , and pubmedbert.The aim of this work is not to report the strongest possible score but rather to address weaknesses of existing radiology report summarization studies.second, although f1-radgraph seems like an appropriate metric to evaluate our new modalities and anatomies , it has only been evaluated subjectively and not systematically.\",\"There are a few limitations of this study: first, the patient sample size for the validation cohorts was limited to 954 patients from a local hospital.As annotation in the medical setting is expensive and time consuming, we only get patient level labels and cannot pay the effort for document level annotations.The size and diversity of the data sample could be improved by collecting clinical notes for patients from other hospitals in different age groups and of similar clinical complexity.We did not perform cross label check for the sampled patients, as there is a large number of uncertain patients, among those patients there are still ones who suffer from dementia but not diagnosed.Second, more statistical models can bee developed.At the moment we only tried a keyword based model and a deep neural models.Traditional statistical models like logistic regression with biomedical concept features can also be considered.Furthermore, our study would have benefited from more model interpretability and human error analysis on the classifier predictions.We have plans to extend our current work with the above mentioned directions.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"4_clinical_medical_patients\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"4_clinical_medical_patients\"],\"textfont\":{\"size\":12},\"x\":[9.2066650390625,8.931243896484375,8.718754768371582,8.811614990234375,8.818126678466797,9.215078353881836,8.634231567382812,8.810436248779297,8.962774276733398,8.620264053344727,8.624359130859375,8.992632865905762,8.339010238647461,8.710030555725098,9.13064956665039,8.88737678527832,8.435333251953125,8.88728141784668,8.595454216003418,8.879843711853027,8.76127815246582,8.932909965515137,8.340548515319824,9.0595121383667,9.390219688415527,8.576577186584473,8.626944541931152,8.770554542541504,8.673125267028809,9.417387008666992,8.833999633789062,8.673982620239258,8.871368408203125,8.812541007995605,8.768672943115234,9.208149909973145,8.408467292785645,9.281914710998535,8.574034690856934,8.901947021484375,8.878828048706055,9.038740158081055,8.887786865234375,8.839777946472168,8.792658805847168,8.935977935791016,8.762090682983398,8.396232604980469,9.267548561096191,8.340608596801758,9.281201362609863,8.318047523498535,9.28376579284668,8.870061874389648,9.219091415405273,8.744048118591309,9.037554740905762,8.759321212768555,8.345552444458008,8.816793441772461,8.771828651428223,8.424758911132812,8.831013679504395,9.512537002563477,8.6851806640625,8.343867301940918,8.643104553222656,8.822735786437988],\"y\":[1.8315868377685547,1.6842083930969238,1.7153187990188599,1.6466460227966309,1.6088554859161377,1.846671223640442,1.817840576171875,1.5949522256851196,1.8366872072219849,1.8080229759216309,1.8136519193649292,1.8492003679275513,1.881683111190796,1.8451755046844482,1.5829432010650635,1.7488118410110474,1.9083462953567505,1.6942230463027954,1.8388307094573975,1.6626039743423462,1.781146764755249,1.6610102653503418,1.8827035427093506,1.7777349948883057,1.86476731300354,1.831426739692688,1.8791848421096802,1.795546054840088,1.804989218711853,1.7646400928497314,1.766296625137329,1.794590950012207,1.7221964597702026,1.7658400535583496,1.7550983428955078,1.8672630786895752,1.8414140939712524,1.8870724439620972,1.90692138671875,1.7184346914291382,1.8760617971420288,1.7256691455841064,1.6792510747909546,1.7804630994796753,1.7875505685806274,1.6775743961334229,1.8047919273376465,1.8876463174819946,1.8851972818374634,1.841961145401001,1.8576124906539917,1.9455969333648682,1.8503170013427734,1.400510549545288,1.803425669670105,1.8914700746536255,1.7915892601013184,1.7482833862304688,1.881922960281372,1.6751737594604492,1.7312091588974,1.9011576175689697,1.7691088914871216,1.8939422369003296,1.8027747869491577,1.8829442262649536,1.7896842956542969,1.7887675762176514],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Despite achieving superior performance, our proposed method requires manual selection for hyperparameters to decide the number of tasks, i., the number of virtual relations v and the number of synthetic tasks nv for each virtual relation.besides, although we adopt a task selector to adaptively select beneficial tasks, it is still inevitable to bring noisy tasks in the meta-training stage.We will explore the strategy to achieve better denoising.\",\"Despite our efforts to collect as many generation tasks and datasets as possible, we only evaluate the generation quality and generality of our models on a small number of tasks and datasets.The interpretability and robustness of our models require further analysis.Besides, there exists subjectivity when collecting downstream tasks and intratask datasets, albeit our attempts to employ widelyrecognized categorizations from the literature.Due to the limitation of computing power, we do not study the performance of our method at different model scales.The effectiveness of multi-task pretraining from scratch, similar to ext5 , also merits an in-depth study.Broader impacts in this paper, we pre-trained a language model mvp using labeled nlg datasets.According to the research , plms tend to \\u201cremember\\u201d what they have \\u201cseen\\u201d in the pre-training corpus.This could result in the reproduction of undesirable biases from pretraining data on downstream tasks.Training data intervention could be a solution to alleviate this issue.It is also interesting to investigate whether supervised pre-training produces fewer biases than unsupervised pre-training.Environmental impact is another factor we should consider.In contrast to large plms with tens of billions of parameters, such as t5 and gpt-3 , we pre-train only a small model with hundreds of millions of parameters.In addition, we utilize supervised pretraining data and initialize our model with pretrained bart, both of which improve the convergence of our model.Ultimately, our model is pretrained for about 20, 000 steps, whereas the bart of the same size is pre-trained for 500, 000 steps.\",\"There may be some potential limitations to this work: \\u2022 due to the maximum input length limitations and cumbersome deployments in most plms , we limited our input lengths with a specific selector ) and searched hyperparameters in a limited range, especially in batch sizes.Theoretically, better experimental results can be reported; however, we reimplemented comparative methods and conducted all analytical experiments in the same environments with the same settings, ensuring fairness in performance comparison and problem addressing.\\u2022 due to the characteristics of the applications in our work and the existing dg methods that are difficult to directly apply to our tasks, we only simulate the performance of plain-text models as dg benchmarks.However, textual information is inherent in up-invariant signals for dg performance to some extent, and a comparative experiment indeed leverages the proposed method for better performance in the same ood scenarios; therefore, it is reasonable for evaluating the performances.To further address these limitations, we will explore more dg strategies to adapt feasible dg methods applied to our personalized sentiment analysis or more complex scenarios with the external introduction of inherent domain shifts in texts such as topics.\\u2022 last but not least, in this paper, the proposed dg method is only evaluated on personalized sentiment analysis tasks.However, more applications can be applied to our method, where domain shifts occur due to explicit knowledge injection or fi can be augmented and exposed.\",\"Some of our experiments, specifically those in the ablations with large batch sizes, required significant computational resources.We trained these models on google cloud tpuv3 pod slice with 128 chips for a few days.This experiment is important, as otherwise there would be questions on how the models compare at large batch sizes where contrastive models are known to work better.Due to training costs and in the interest of open research, we will open source our code and model checkpoints for the community to use and build upon.Secondly, vmsst and bitranslation require decoding which which means they need more memory for the decoder and are slower during training.However one advantage of these models is that they can be trained with gradient checkpointing greatly reducing their memory requirements, which cannot be used for the contrastive models as that would reduce the effective batch size for finding negative examples.Moreover, during inference, there is no difference in the memory or speed requirements in contrastive, bitranslation, or vmsst as only a single encoder is used in inference and there is no decoding.\",\"The main limitation of our work is the high memory and computation cost.As both our methods estimate the importance of training examples based on the prompt-performance statistics, we first need to run in-context learning on the dev set multiple times with different prompts in dicl.Although icl does not require any parameter updates, llms still require a large amount of memory footprint during inference, especially when the model size is large and the average sequence length is long.For each setup, our dicl contains around 50,000 prompts and 50 dev examples per class, so the most expensive setup costs more than 500 gpu hours on an rtxa6000 gpu.Our preliminary study shows that the proposed methods need the statistics of at least 10,000 randomly sampled prompts to perform well.we also release our collected data of every setup in to support future studies on icl.In this paper, we only study classification tasks, for the sake of easy evaluation.due to hardware constraints, we do not study llms of sizes over 13b, and we fix the number of shots and prompt templates for simplicity.In independent work, nguyen and wong complement these limitations of our paper, showing that similar approaches work well on larger models and a diverse number of shots for in-context example selection.Still, the influence of in-context examples for gigantic llms larger than 100b parameters has not been studied.Due to emergent abilities of llms , it is unclear whether our methods and findings would still apply when prompting these gigantic llms.\",\"A major limitation of activeaed is that it is significantly more compute-intensive than other scoringbased aed methods such as aum or dm.This is inherent to the proposed method because the ensemble requires training of multiple models and, after receiving human feedback, the full ensemble has to be re-trained.Also, the ensembling of activeaed requires more training runs than trainingdynamics-based aed methods.However, most model-based methods require a cross-validation scheme.The ensembling component of activeaed is more data-efficient than these approaches, because it makes use of the training dynamics captured during cross-validation instead of discarding them., they represent only a fraction of the scoring-based aed methods described in the literature.Finally, our evaluation is limited to a single language model and it would be interesting to investigate how activeaed interacts with larger language models than distilroberta.\",\"Our hyde method relies on real-time generation from llms and therefore may not be suitable for tasks that demand high throughput or low latency.However, over the years we have seen the cost of hardware decrease and model compression techniques advance, which may help improve the efficiency of llm inference.\",\".the large-scale model size can bring promising performance while it also consumes more training time and releases more carbon dioxide, which may be inconsistent with the theme of green ai.\",\"Our pareto-md doubles computational cost due to training two models simultaneously, which can be a limitation of our approach.However, paretomd obtains significant improvement that is hard to achieve for previous methods of training individual models, thus worthy.moreover, pareto-md does not affect inference efficiency.\",\"It seems to be difficult to evaluate the efficiency of odqa models fairly and impartially due to multiple factors that should be considered and need to be traded off.On the one hand, it is not enough to only use accuracy, memory, and processing time to evaluate effectiveness.It is also important to establish what resource, e., money, power consumption, carbon emissions, etc.on the other hand, how to deploy models and what tools model implementation relies on contributes to inequity growth.It is extremely challenging to unify the deployment of all models and the tools they rely on and to achieve a truly fair and unbiased effectiveness comparison.\",\"Limitation the unstructured sparse patterns we introduce are not as hardware-friendly as the structured patterns, suggesting the speedup of using unstructured patterns maybe limited due to the implementation.The number of parameters of models we are studying are only at the level of 100 \\u223c 300m, and the datasets are focus on glue, e2e, webnlg, and dart.\",\"Though our method does not require iterative retraining, pruning, and rewinding process, one question still remains under-explored: how to selfadaptively find the optimal sparsity instead of trial training, which can boost the training efficiency.Also, we plan to further investigate the effectiveness of lottery prompt tuning in other scenarios, including the multi-task learning , prompt ensembling , etc.Furthermore, the proposed learning method should be compatible with other parameter-efficient finetuning methods, such as adapter tuning and lora.\",\"While the results have shown the effectiveness of our framework in ie without using any additional resources, we did not explore the potential enhancement by utilizing existing resources in the easy-tohard learning process.On one hand, we can build the easy stage with the help of existing data of simpler tasks.On the other hand, the data of harder tasks can be used for the hard stage.To enhance the e2h framework via effectively using existing resources is an interesting and promising direction.Another limitation is that we did not extensively explore the possible skill sets for each task.Exploring more approaches to obtain the skill sets is also open for future research.\",\"In the current work, we adapt one-step gradient descent training for the outer loop based on our bi-level optimization framework.Since this outer loop optimization doesn\\u2019t have a closed-form solution, determining how many steps to perform for the outer loop for better outer optimization is still important to explore.\",\"Task configurations are entangled with the full model parameters.In our ablation study of task configurations at training time , we see that when training without task type, the model fails to generalize to fetaqa.Upon examining the model output, we find that although we change the output configuration to \\u201clong answer\\u201d, the model still produces a short-form answer.This indicates that model behaviors are not always aligned with a single configuration, leading us to question the extent to which each individual configuration influences the model.In order to have better and more interpretable control over the models, one potential avenue for future research is to develop pluggable task configurations, where each configuration controls a more atomic function of the model and can be plugged, unplugged, and combined to yield different model behaviors.Our exploration scope is limited to table-to-text tasks.Due to the constraints of the computational resources, we haven\\u2019t explored joint training with a broader range of other nlp tasks.We think with some modifications, such as the inclusion of dataset domains in the configuration set, it would be possible to extend our approach to additional datasets and tasks.\",\"The primary weakness of \\u2018fairly\\u2019 averaging model weights for xlt is that sensible checkpoints need to be averaged.This manifests, for instance, in hyperparameter ablation for zs-xlt on tydiqagoldp.Tydiqa-goldp is a complex task with merely 3,696 training instances that observes unusual training dynamics.On such a dataset, the early checkpoints often underperform models that have converged, especially if training utilizes low learning rates with schedulers.Here, srcdev could be used to weed out underperforming checkpoints, such that ca then always exceeds the baseline that performs model selection on sourcelanguage validation data.Whenever the english training portion is sizable \\u2013 like in our other tasks \\u2013 checkpoint averaging is consistently beneficial.Our experiments also demonstrate that xlt behaves differently by task.Averaging checkpoints consequently might affect other tasks differently like, for instance, document classification that reason about long contexts or retrieval tasks like tatoeba that jointly require sequence- and word-level semantics.Another dimension we did not explore further due to a limited compute budget is how to ensure best that monolingual models are aligned for run averaging.For instance, it may not be required or even desirable to keep classifiers frozen throughout the second step of our proposed training curriculum , as we would ideally also want to average out idiosyncratic noise of the original classifier.\",\"Our study used t5-base due to the capacity of our computational resources.Thus, it is unclear whether our method is also effective for larger models, such as t5-xl\\u002fxxl.argues that continuous prompts are particularly effective for large t5 models.Following their results, our instruction embedder is also expected to be effective for larger models.As shown in figure 3, instruction optimization is slightly unstable to converge.Some studies tackled the unstable convergence of bilevel optimization by l2-normalization, early stopping , or perturbation of hyperparameters.These methods might be effective in stabilizing the instruction optimization.\",\"Compared to vanilla prototypes, the advantage of hyperproto would also rely on the additional radius parameter.Under the 1-shot setting, however, hypersphere prototypes will face challenges in estimating the radius in support sets, this is because the initial radius may be biased by the randomness of sampling.When the radius is set to exactly 0, the model will resemble a traditional prototypical network.Nevertheless, although not as large as the boost in the multi-shot setting, we find that having a consistently optimizable radius parameter at the training stage in the 1-shot scenario still delivers non-trivial results and exceeds most baselines.This further points to the positive influence of the added radius parameter to learning prototype representation and hints on the possible research direction in learning a transferable radius in 1-shot scenario.\",\"While we conducted extensive experiments and demonstrated the effectiveness of the suggested approach for controlled bandit learning in the context of the skill routing problem, there are multiple directions of improvement for future studies.We believe one of the limitations of the suggested constrained optimization framework is that it relies on expert-defined conditions on an arbitrary segmentation of samples.It entails the need for human intervention and manual constraint definition\\u002foptimization which can be challenging.Another limitation we faced was during our experiments which showed additional compute overhead of between 2 to 3 times for different constrained optimization methods due to additional optimization objectives, inner loops, and backward passes.\",\"Like lots of deep learning algorithms, our work also needs gpu resources.In common learning problems, models will be trained once on the existing training datasets, using dev sets for tuning the models.Then the trained model would be ready for use.In contrast, in active learning, we need to train the model several times , which increases the need for gpu resources.However, the need for gpu, is not related to our proposed method and it is due to the nature of active learning.In addition, one can run the active learning method once for building an acceptable dataset.It should be noted that we have designed the algorithm in a way to be independent of the target language and utilized model.However, we only tested our method on egyptian arabic dialect and the accuracy of the model should be investigated on other languages and dialects using different learning models in further studies.\",\"The proposed model is computationally demanding.Recent work on parameter-efficient fine-tuning methods, such as lora , suggests that they can significantly reduce the number of trainable parameters at a minimal performance cost, which may help further democratise the development of domain- and task-specific models.In addition, as we continued to pretrain, to obtain the pulsar models, their tokenizer was inherited from corresponding flan-t5 model.Thus it does not contain domain-specific terminology, which may be a limitation in terms of representation density.\",\".Our proposed memory-efficient optimizer introduces additional computation costs for the nonnegative matrix factorization of the instability matrix in comparison with adafactor.We observe, however, that the training time of came increases only slightly in our experiments.Meanwhile, it is possible to conduct further experiments on other models in other fields, such as computer vision and reinforcement learning, thereby exploring the effectiveness of came training under more application scenarios.As a final point, it would be much more helpful to provide an in-depth theoretical analysis of came to improve comprehensiveness of the paper.\",\"This paper fundamentally considers a scenario in which practitioners rent cloud gpus.In the case of hosting gpus by themselves, the two strategies explored in this study would not be simply comparable.However, in practice, when training a large model , we conjecture that renting gpus could be preferred in many cases as scaling compute powers is not trivial and prohibitively expensive.on the other hand, human annotation cost is likely to be the same at least or even more expensive.With cost changes in such a direction, the same\",\"The main limitation of mccl is the requirement of a sufficiently large batch size in training , leading to a need for large gpu memory.This is because mccl uses in-batch entity pairs for contrastive learning, and a small batch size does not provide enough instances to form multiple clusters.In addition, we need to store the entity pair embedding of the whole training set for knn-based inference, which is less memory-efficient than ce.\",\"Collapsed fine-tuning runs mostly occur in the low resource scenario where plms may easily overfit to the small data.The improvement with the proposed technique becomes marginal when the amount of training data scales up, as shown in table 2.The other limitation is that hype introduces two new hyper-parameters: the noise distribution form and the scale of variance.To achieve the best performance, we may need to search for different combinations of hyper-parameters.\",\"Our approaches that are developed in the parameterefficient weight ensembling framework, and experiments have the following limitations.First of all, our framework cannot efficiently extract information from the parameters of the trained lightweight objects, resulting in relatively unsatisfactory performance of the approach resorting to the information from the weights, i.Furthermore, the modules that we focus on in our analysis of module importance are only blocks and sub-layers of the blocks.We have not probed finer modules, in which we speculate more precise information about transferring lightweight objects across tasks is concealed.Last, all tasks in our experiments are formulated into the text-to-text format, and we have not conducted analysis on tasks in other formats.\",\"We found that prefix tuning takes much longer to converge when compared to fine tuning, and for t5-base, it takes around 10 days on a 48 gb gpu to complete tuning for a single setting in table 1.Due to limitation of resources and with an aim to save energy, we did not conduct experiments with larger models such as t5-large, t5-xl etc.We also did not perform experiments with smaller splits of the same datasets, which could have given further insights on how model performance varies when training data size is less.\",\"The current method works with a large prompt search space t , which means a tremendous number of inference api calls are required.Though figure 2 shows that the average cost of finding a lottery prompt for each instance is low, the searching process is highly randomized and there is no guarantee of the performance of searched prompts over the test dataset.Therefore, finding strong prompts over the training set can still be laborious.How to use plm inference calls more efficiently and leverage the generalization ability of t \\u2217 within a reasonable cost is of future research interest.another aspect is that not all strong prompts are interpretable as presented in 2.while recently emerged larger models like chatgpt demonstrate superb language understanding ability and can almost always answer yes or no questions correctly given a human-interpretable prompt.This gap observed between small plms like roberta and large language models like chatgpt is yet another interesting research topic.\",\"While we conducted an extensive set of experiments to gain a broad picture of whether modeling improvements hold between benchmarks, it is always possible to investigate more settings.While our study covers a representative set of 20 nonpretrained and pre-trained modeling approaches, it is conceivable that evaluating more modeling approaches on additional benchmarks would have led to different results.Furthermore, although we evaluate each modeling approach on each benchmark with the same training hyperparameters used for squad, as well as 5 additional randomly sampled hyperparameter settings , it is possible that the squad hyperparameters for some modeling approaches happen to be more general than other modeling approaches.Ideally, each modeling approach would be individually tuned to maximize performance on every benchmark, but doing so requires prohibitive amounts of compute and researcher effort\\u2014we believe that our experiments have enough coverage with respect to hyperparameter optimization.\",\"We acknowledge the main limitation of this work is that we only evaluate our methods on some tasks from the glue and superglue benchmarks due to limited computation resources.And all tasks are not in a realistic few-shot setting, where the number of training samples is less than a few hundred and development sets are not offered.The benefit of peft methods could come from an exhaustive search of hyper-parameters for the development sets, while the realistic few-shot setting could solve this issue and shed more light on peft.It would be interesting to see how our methods and other baselines perform on a wide range of few-shot tasks.In addition, current frameworks are not friendly for sparse fine-tuning methods.Most works still need to calculate a full gradient of all parameters and selectively update the masked parameters, which makes it cost the same training time as full fine-tuning.Last but not least, we only estimate our methods on one single complex task, i.\",\"To address the low-resource data in the training of tau-dr, we apply two heuristics, dataset enrichment and generation from different checkpoints.Despite being effective, they require additional computational time that might be challenging in applications with low-computational resources.A possible approach to reduce the computational time might be to average the checkpoints.We believe that this might lead to competitive results, with a significant reduction in computational time, since checkpoint averaging proved to be an effective approach in low-resource settings.Another limitation is when the original dataset is in a highlyspecialized domain that might contain domainspecific phrases that were most likely not included in the pre-training data of the language model.The results obtained by existing data augmentation approaches will most likely exhibit only marginal improvement.\",\"Firstly, the transferability between different tasks within an mtl system is not well measured in current work.We also find such transferability is asymmetric and thus hard to quantify using symmetric measurements such as cosine similarity between task embeddings or gradients: for example, tsa positively transfers to sc, but sc negatively transfers to tsa ; fsrl positively transfer to all other tasks, but other tasks negatively affect fsrl.\",\"Model sizes and comparability as we have pointed out in the paper, due to computational and time constraints on the hardware we had at our disposal, we found it was unfeasible to train larger architectures.Nevertheless, we believe our comparisons between dmlm and its direct competitor, mlm, have been fair, as we have done our best to set a level playing field between the two.Thus, while we understand that this is a significant limitation in terms of comparability to larger models, we still think the results we have obtained could pave the way for further exploration in this direction.Moreover, we have performed architecture scaling experiments to show that it is important to continue research in this direction, and test dmlm\\u2019s capabilities on larger networks, while we did not perform a similar comparison with mlm because several works have already explored how mlm scales with network size.Applying dmlm only half of the time although we acknowledge that our choice to apply dmlm to only half of the sentences can be seen as arbitrary, we argue that it is a sound choice given the nature of our objective.Indeed, we did not want our models to rely too much on the definitions provided, or they would have required them at inference time.Such a requirement is mostly unfeasible, as it would demand running a wsd pipeline before the model\\u2019s inference, and this is incompatible or unnecessary with most downstream settings.Nevertheless, we plan on training other architectures with different frequencies, so as to better assess how impactful this hyperparameter is.Training corpus domain our models are trained on a sense-tagged version of wikitext-103, which only contains text coming from wikipedia, and thus is very descriptive in style.While many other works have based their pre-training corpora on wikipedia, we do recognize that this might be a limitation, especially for downstream tasks.Training on longer sequences in this work, we trained language models on sentences, as opposed to what is commonly done in the literature, i., longer sequences of text which are usually concatenated sentences.We see a limitation here in that, in its current formulation, dmlm does not support training on longer sequences as we have no way of discerning between multiple definitions appended to our input sequence.Nonetheless, while we performed wsd at the sentence level, the corpus can be brought back to full documents, which would make sequence-level training feasible with the available data, provided that an extension to dmlm that supports multiple definitions is designed.Scaling to multiple languages our formulation of descriptive masked language modeling can be applied to, as far as we know, virtually any language.Moreover, we argue that it might be possible, in a multilingual setting, that definitions of the same sense could help in aligning the output representations of the trained models for words sharing the same sense.Nevertheless, having said this, it is worth noting that there might be two impediments to achieving multilinguality.First, in our work, we leveraged english word sense disambiguation, which, despite its recent advancements, is still far from performing the task equally well on other, even high-resource, languages ).Second, we decided to employ definitions coming from sense inventories which, at least in english, cover a wide number of senses with meaningful descriptions, but this might not be the case for other languages, especially low-resource or endangered ones, with babelnet being the largest resource providing textual definitions in hundreds of languages.Reproducibility we acknowledge that, even by releasing the code and dataset on which our models are trained, it might be hard for other interested entities to reproduce this work, as our training runs lasted up to 8.\",\"More generators bring significant benefits to the model performance to our mgr, but the training cost is also increased with the number of generators growing.Although we have verified that we only need to keep one generator during test, there is no denying that the training cost is still an important problem.\",\"Comparing downstream performance of pretraining objectives with large-scale models is prohibitively expensive.Because of this, we employ scaled-down models that closely resemble the architectures and training procedures of popular plms.In doing so, we assume that our findings are transferable to some larger publicly available models., ctxaug offers an interesting alternative to prompting generative lms that are significantly smaller than those that typically exhibit few- and zero-shot capabilities.\\u2019s claim and our assumption in preliminary and supplementary experiments with select plms , these experiments are still performed on models of up to 140m parameters.Therefore, we stop short of concluding that our findings generalise to llms, which dwarf these models in comparison.Additionally, the number and types of target attributes that a user may want to control for in various downstream text generation tasks are potentially endless.However, our study focuses on only two possible target attributes, namely, inquisitiveness and positive sentiment, for the task of conversational dialogue modelling.\",\"Although comprehensive, our study of mpt in this work has couple of limitations: \\u2022 as mentioned in \\u00a75.3, because of infeasiblity to search for optimal hyperparameters for each of the meta learning methods in each of the ten settings, we choose to use the r\\u2192r setting as our main representative setting.This could be one of the reasons for mpt underperforming mtl in some non-classification tasks.\\u2022 we mainly focus on how upstream meta learning can improve the performance on target tasks.However, meta learning also enables faster convergence.aside from that, meta prompt tuning as a method has a limitation that it is memory-intensive.Optimization-based meta learning methods, especially maml, are memory-intensive, which limits the tuning of the inner batch size and inner update steps.One potential solution is to build more memory-efficient meta learning libraries.\",\"It is worth noting that the supportive pretraining data we investigated throughout the work is w.the current lm, such that a perturbative continued pretraining with the supportive data would improve the final lm checkpoint deployed to downstream tasks.It is possible that for some data which we did not determine as supportive, they had been supportive w.additionally, another significant limitation of our work is the amount of involved computing resource.The orca-icl method is gradient-based that requires back-propagation.Since we iterate through a large size of pretraining data, the cost of computation is similar to training a language model with a batch size of 1 on the considered pretraining data.On our 4 nodes each consists of 8 nvidia v100 gpus, finding the supportive pretraining data for each source task in our experiment would take about a week.One mitigating aspect of such computation is that the gradient calculation can be done asynchronously, therefore enabling the use of idle, leftover gpus scattered across a cluster of nodes.\",\"Even though our proposed methodology, tgtss, was able to significantly reduce model instability, there is still a gap in performance with the gold standard ensembling techniques.More work needs to be done to bridge this gap.In our empirical analysis, we used two open source datasets, massive and clinc150.Both these datasets are small and may not represent the complexity in real world production datasets which may contain substantially large noise.In our proposed methodology, we train a pair of models successively, a teacher and a student, which is significantly better than ensembling in terms of computational cost.However, this setup may still be challenging in many sophisticated real world production nlu systems.More work needs to be done to reduce the computational complexity of training and inference for these systems.\",\"On training efficiency and inference efficiency.First, since metaretriever will first retrieve taskspecific knowledge and then make predictions in the inference phase, such a retrieve-then-extract manner will take longer inference time than nonretrieve methods unavoidably.We conduct experiments on the test set of conll04 dataset to compare overall inference time of metaretriever with uie.All hyper-parameters are set to be the same for a fair comparison.Experimental results are shown in table 9 and we can find that metaretriever cost nearly twice time as uie to make predictions.As metaretriever works in a retrieve-than-extract manner, such a time cost is reasonable.Second, our proposed meta-pretraining algorithm is based on bi-level optimization.In the pretraining phase, it needs to calculate high-order gradients to optimize parameters and calculating high-order gradient requires more time.Therefore, it takes longer time to pretrain metaretriever.To illustrate the time cost, we conduct experiments on 10k instances to compare the pretraining time of metaretriever with simpleretriever which is pretrained without meta-pretraining algorithm.All hyper-parameters are set to be the same for a fair comparison.from it, we can obverse that compared with simpleretriever, metaretriever takes about 1\\u002f4 longer time than simpleretriever.Finally, we spent 56 hours to pretrain metaretriever on filtered pretraining corpus.\",\".3, conal still is slightly more computationally expensive than vanilla training.It also requires access to a pretrained llm with which to generate novel examples.To achieve optimal performance, usage of the openai api is required, which poses some concerns around transparency, as details around gpt-3 training and data are not publicly released.Finally, performance varies across datasets, suggesting that types of outliers that are unexpected to llms might still confuse a conaltrained model.\",\".2, we show that our training mixture with multiple-choice qa tasks, although small, is highly effective for multitask training.However, it is still unclear why multiple-choice qa tasks are particularly effective.Identifying the key factors towards positive or negative transfer from different tasks in the multitask training mixture would greatly help improve zero-shot task generalization.\",\"Perhaps the most important limitation regarding cold fusion is its deployment.This paper presents a method for multitasking, not a platform.In that sense it solves both multitask learning goals under the constraints resulting from collaboration.However, using cold fusion in practice might require much more effort \\u2013 it would require a place to host the models, a way to make sure no malicious or erroneous model was sent, and other aspects of a platform to support this training.This is the first method to tackle collaborative multitasking and we scaled it to 35 datasets.However, future methods may be found more efficient or scale better with the amount of data and computation.Cold fusion with many iterations and models might require more computational effort for a given amount of data than regular multitask learning.As a result, while our bottom line performance is encouraging, cold fusion might not be the preferred way under every possible scenario.While this paper studied the impact of various cold fusion parameters, it is unclear how finetuning or even pretraining parameters affect results.However, we do have a reason to believe the method is relatively robust to these refactors through our initial results and the replication on another architecture.Another limitation is the assumption that the weights of the model change.Some adaptation methods assume the model is frozen and only its inputs change.In those cases, the model would not be improved by use.Still, even in such cases, multitask learning might be applied on the inputs, or the same model might be used in different ways, where some also adapt parts of it.In those cases, the method might still prove useful, even if it benefits only from some of the contributions.As mentioned before, another concern is a possible harmful update done by a contributor.Handling it would require monitoring the updates by regularly evaluating the model, or measuring the updates diff to identify noisy models.\",\".moreover, although our ddg outperforms most of the state-of-the-art methods, the performance is still a long way from humans.\",\"Our monotonic kd approach requires searching for a hyper-parameter k to strike a balance between monotonicity and translation quality for generating pseudo-targets.The current process requires substantial computational resources to determine the optimal value, which may be different depending on the dataset.More studies are needed to establish an efficient method.\",\"Hyper-parameters selection some hyperparameters still acquire careful selection for wecheck, e.also, using different set of hyper-parameters for different tasks and datasets will further boost performance.Thus, we need to train the model several time and select the best performing parameters based on validation.End-to-end training wecheck applies the weak annotation and noise-aware fine-tuning twostep pipeline, where the noises in the first step will greatly affect the performance of the second step.By modifying the overall framework into end-toend training will solve this problem.\",\"The drawbacks of our method are the same as those of lora: it is tricky to batch inputs to many tasks with varying a and b in a single forward pass, and the rank may be greater for tasks that are more challenging.Moreover, we believe that weights obtained during a single task may be used for a better initialisation.Finally, the use of a different sampling policy on a different dataset may also be appropriate, however this choice is not obvious.\",\".With recent advances in huge lm such as gpt4 and their extraordinary generation capabilities, one may wonder about the relevance of this work which mainly focuses on a medium size fine-tuned teacher.Although we show the distillation of a huge lm , it is often infeasible.First, when the data cannot be sent to external servers because of privacy constraints or when the domain is unique or specific , huge lms that cannot be fine-tuned may be less effective.Second, we have distinguished between two types of costs: computational and financial.While training a student model with a medium-size finetuned teacher may take a few days, the entire process is feasible since training time is typically not a limited resource.In contrast, generating pts with a huge lm like gpt-4 can easily cost dozens of thousands of dollars.This financial cost is often prohibitive, particularly when training a general high-quality student or several domain-specific ones.While it is possible to utilize a huge lm to obtain a limited number of labeled examples, relying on it for generating pts for abundant unlabeled data is not feasible.Therefore, a medium size teacher is needed.Furthermore, research suggests that using mediator\\u002fassistant teachers aids the distillation process , as might be the case in distillation from a huge lm to a medium size fine-tuned teacher, and finally to a small student.Considering the aforementioned reasons, our study holds significant relevance as it emphasizes the importance of the distillation process with a medium size teacher, regardless of whether the data is generated manually or by a huge lm.While our results demonstrate the effectiveness of kd for various english-to-english nlg tasks, for the tasks that were part of the study, the output length is relatively short compared to the input or has a similar length.The results may differ for tasks with much longer output lengths or for non-english-to-english tasks such as nmt, data-to-text , multilingual, or multi-modality tasks.In addition, the results are applicable to our realistic task-specific setups, and some findings may vary in high-resource scenarios or when unlabeled data is unavailable.Although these scenarios may be less relevant to nlp application developers, they are commonly studied in academic research.another limitation of our research is that we did not consider the computational costs of the kd stages.The training time comparison between the methods was therefore overlooked.This is because we assumed that one-time resource usage for training could be neglected compared to the accumulated inference cost of a deployed model.However, it is worth noting that generating pts with the teacher for all the training and unlabeled examples is computationally expensive.Furthermore, joint-teaching can also be computationally heavier than other kd methods, as the student generates pts during the training process.In addition, different training objectives also have different costs, with some methods being more computationally intensive than others.Finally, the distillation process can be long, and multiple epochs are required until the student converges - in some setups, we trained the student for more than a few days.certain limitations arise in our extreme setup, which involves the costly utilization of huge lms provided by external companies like openai.First, the comparison with the joint-teaching method is not conducted due to the need for repeated costly querying of the teacher model to extract its logits every time a pt is generated with the student.Nevertheless, extracting the logits of the teacher pts and generating multiple pts is approximately equivalent to generating a single pt.This is because the prompt, consisting of many tokens, is processed only once, and the marginal cost of generating multiple pts is low.Another limitation arises from relying on external companies to enable logit extraction and there is no assurance that this feature will be supported.For instance, in the chat versions: chatgpt and gpt-4, logits are not accessible.In this work, we rely on an internal version of gpt-4, which allows us to extract its logits.Fortunately, we demonstrate that even without logits kd, achieving a strong student model is possible.\",\"Limitation one limitation of the mcce models is that the number of candidates during training and inference should be the same, otherwise, the performance drops severely.One simple potential solution is to divide or pad the candidates during inference to match the number of candidates during training.For example, divide 128 candidates into two sets with 64 candidates and apply twice forward passes of a filter model if it is trained on 64 candidates and required to filter 128 candidates during inference.\",\".Second, as shown in table 5, although the time cost of fpt is lower than that of gedi, it is higher than those of other prefix tuning-based methods and grows approximately linearly by a factor of 2 \\u00d7 number of attributes.\",\"Our work follows the general assumption that the training and test set contain the same list of predefined entities.Without additional or necessary modifications, the few-shot or zero-shot capability of the model is expected to be limited.\",\".In our experiments, we observe that the performance of prequant is highly correlated with the data size.When fine-tuning with very limited data, prequant may not meet expectation to preserve the performance of plms.Moreover, our model performance also depends on the number of parameters restored in the fine-tuning stage.This hyperparameter controls the trade-off between model performance and parameter efficiency.The optimal choice of the hyper-parameter for different tasks requires further investigation.\",\"Our getmtl needs to compute the gi for each task i at each iteration and requires a backwardpropagation procedure over the model parameters.Every iteration requires one forward-propagation followed by t backward-propagation procedure and computation of backward-propagation is typically more expensive than the forward-propagation.Here, we define the time of one forward pass and one backward pass as ef and eb, respectively.The time of optimization process is defined as eo.Therefore, the total time e of getmtl is defined, e = ef + teb + eo \\u2248 teb + eo for few-task learning scenario , usually eo eb and getmtl still works fine.However, for large-scale task set , usually eo eb or eo teb.Consequently, our getmtl may get stuck in the optimization and backward-propagation process at each iteration.Therefore, the major limitation of our work is that it can not be applied to scenarios with large-scale task sets.\",\"While we do explore a range of models in the 1- 20m parameter space, our work does not constitute a complete study of downscaling.In this work, we aimed to explore the more fundamental components of model shape, model size, and input data.However, our findings may not generalize to other models with alternative applications of downscaling methods.Considering it to be out of scope for this study\\u2019s assessment of pre-training effects, we did not compare our results to knowledge distillation methods of similar model shape and size.Furthermore, our exploration of model shape and size was limited to a model\\u2019s hidden size, number of hidden layers, embedding size, and intermediate size, and number of attention heads as these are the most commonly-tuned hyperparameters.Our usage of vocabulary filtration as a means of downscaling input data size may not be the most effective means of limiting input data.While shown to be effective, alternative approaches for input data manipulation such as curriculum learning, and data pruning merit study beyond the scope of this paper.by exploring models in the space of 1-20m parameters, our findings can inform language modeling work for those without access to large, gpu-enabled environments.This is important as it can encourage further research work in this space by those who are otherwise unable to work with sota llms.We acknowledge that our resources enabled the breadth of study in this paper; most of this study was conducted using a single gpu.This consideration underscores our commitment to improving accessibility for under-resourced technologists throughout the world.Furthermore, in working with downscaled llms, we hope to encourage methods that reduce overall carbon footprint and bolster sustainable practices in nlp.These considerations are especially important given the particular burden placed on those with limited access to electricity and technology.The cost of running and experimenting with these models may prove quite costly in terms of person-hours and compute resources.As such, we hope our work at smaller scale can help lessen these burdens, and positively impact the lives of technologists, and others.Any model from the study can be trained in less than a day on a single consumer-grade gpu.\",\"That are revealed in our work, such as multiple training procedures or hyperparameter tuning for each method.Limitations although our method well estimates the ambiguity without additional resources as well as boosting model latency significantly, there are a few limitations.First, our method requires additional training procedures, such as training the internal classifiers and kd.For this, we may fine-tune the original model and internal classifiers simultaneously.Another limitation is in setting the hyperparameters.Finally, we validated our method with a limited number of benchmarks since most of datasets have been released with only aggregated gold labels.\",\"The limitations of this work can be concluded into three points: 1) the data in the test set is relatively small, so it cannot more accurately reflect the effectiveness of the method proposed in this paper.We believe that tuning the model on a larger dataset can help improve the performance of the model.2) due to device performance limitations,we did not experiment with larger models.In our experiment,we only tested the method with models like xlm-roberta, mbert and bert.Its performance with larger models is not known.3) we did not perform an extensive hyperparameter search, which might further improve the model\\u2019s performance.\",\"We did not perform any comprehensive hyperparameter search, which would have further consolidated our results.This decision was made due to the high cost of training multiple models.Compared to current very large models, glot500-m is comparatively small.Although we have tried to minimize the amount of noise in our data, some noise is still present.\",\"While working with the data of the iemocap data set, we realised that some of the examples are of very low quality which can negatively affect the performance of the models.This, however, does only apply to a small part of the data set.Additionally, it is not possible to correctly asses the influence of the gender on the performance.The same applies to a possible influence of the way, the data is generated: scripted or improvised.In some models we use selu activation function, which is still not widely used, therefore, it is possible that there are problems that are not that well known.In general, there are limitations based on the data set.It only contains scripted and improvised recordings, by actors, which might not be representative of naturally occurring emotions.Also, as the data set is recorded in english, any generalizations outside this language are not possible.\",\"Rl4f is primarily targeted at improving final performance.While we have found that the critiques learned by rl4f remain natural, we do not introduce any explicit restraints preventing semantic drift.As though it may raise end-task performance, semantic drift would also hinder interpretability.lastly, we limit our analysis to gpt-3 and focus on a scenario where it is inefficient or impossible to train the task model while this may be a conservative assumption for other settings.\",\"While we show that c2a successfully improves the effectiveness and efficiency of peft in fl, we have mainly focused on improving the effectiveness of the vanilla adapter.However, it is an open question whether our framework can improve other peft approaches, such as prompt tuning, and lora.Although we didn\\u2019t analyze whether our framework can generate parameters for alternative peft, one recent approach reveals that hypernetworks can generate parameters for various types of peft in multi-task learning.Likewise, as c2a generates parameters with hypernetwork, we believe that c2a is highly expected to improve the performance of any alternative peft modules.at the same time, there are a number of ethical concerns with plms in general, including concerns regarding the generation of biased or discriminative text , the leakage of private information from training data , and the environmental impact of training or tuning them.Our framework attempts to train plms with minimal changes made to their pre-existing parameters in fl scenarios.Our work is believed to bring some insights into the two ethical dimensions: privacy and environment.First, with respect to private information leakage, although our work has not addressed address the privacy issue in the pre-train process, our fl framework can mitigate the data privacy issues in the fine-tuning stages.In addition, with respect to environmental impact, our work may obviate the need for full fine-tuning, which may also significantly reduce the cost in terms of memory or deployed servers.\",\"While in-context learning methods for dst are promising in their data efficiency and flexibility to new domains, they typically require very large models to perform effectively.At 175 billion parameters, openai codex is much larger than some of the fine-tuned approaches to dst, though with better performance and ability to adapt to new domains without re-training.Despite our advances, there are still significant errors when applying icl for dst.As such, icl may not necessarily be relied on in safety-critical settings.\",\"Although the proposed method has shown great performance to alleviate the issues of biased learning and deficient interaction, which are common problems among electra-style pre-training models, we should realize that the proposed method still can be further improved.more about mission design regarding with this issue is worth studying.\",\"For the top k operation in target relation calculation, we set k\\u2019s three hyperparameters to determine the number of the related targets.To explore the influence of the selection of k on model performance, a grid search on these three hyperparameters needs to be conducted to iterate each combination.However, due to the time and resource limits, we explore the impact of one hyperparameter in k by controlling the other two hyperparameters.Based on the empirical findings from this, we then set the value of k so as to achieve an appropriate performance.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"5_hyperparameters_metaretriever_pts\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"5_hyperparameters_metaretriever_pts\"],\"textfont\":{\"size\":12},\"x\":[12.365973472595215,12.361464500427246,12.316726684570312,12.361103057861328,12.689732551574707,12.468236923217773,12.344058990478516,12.262201309204102,12.78281021118164,12.997715950012207,12.412834167480469,12.471392631530762,12.441023826599121,12.562270164489746,12.393527030944824,12.554333686828613,12.417521476745605,12.495306015014648,12.32705307006836,12.493281364440918,12.453723907470703,12.46026611328125,12.932080268859863,12.690991401672363,12.313637733459473,12.374321937561035,12.297930717468262,12.703873634338379,12.32208251953125,12.435154914855957,12.624835014343262,12.367938995361328,12.367415428161621,12.076888084411621,12.45442008972168,12.50022029876709,12.607337951660156,12.236982345581055,12.371963500976562,12.690954208374023,12.424464225769043,12.51205062866211,12.113851547241211,12.21579647064209,12.242761611938477,12.382984161376953,12.486370086669922,12.664587020874023,12.339261054992676,12.466328620910645,12.334747314453125,12.530769348144531,12.185464859008789,12.12268352508545,12.309247016906738,12.318441390991211,12.067307472229004,12.405698776245117,12.428974151611328,12.704882621765137,12.465496063232422,12.207571983337402,12.431151390075684],\"y\":[4.467864036560059,4.4283127784729,4.176626682281494,3.6315269470214844,3.7712368965148926,4.1582417488098145,3.713595390319824,4.053158760070801,3.538978099822998,3.7569966316223145,3.8396971225738525,4.230458736419678,4.466948509216309,4.055192947387695,4.458680629730225,3.8698818683624268,4.063071250915527,4.443406105041504,4.220512390136719,4.113996982574463,4.100529670715332,4.252491474151611,3.3314239978790283,3.738828420639038,4.1650190353393555,4.295022487640381,3.9913742542266846,4.182713508605957,4.18434476852417,4.394583225250244,3.7805087566375732,4.4270806312561035,3.944108247756958,4.04191255569458,4.068378448486328,4.522586822509766,3.7324962615966797,4.043066024780273,4.464474678039551,3.798755407333374,4.623950004577637,4.597942352294922,3.9044251441955566,3.9132699966430664,4.262228488922119,4.278287410736084,3.7673232555389404,3.806206226348877,3.99015212059021,4.547479152679443,4.1720404624938965,4.178450107574463,3.971133232116699,4.178406715393066,4.180454730987549,4.170754432678223,4.111559867858887,2.660550117492676,4.441583633422852,3.8240549564361572,4.13341760635376,3.9501519203186035,4.073901653289795],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"In experiments we only take plms into account because of their prevalence, hence the transferability to non-pretrained models is still unknown.However, due to the generality of plms, this can be a minor point in practical scenarios.Moreover, although we successfully transfer adversarial attack methods in cv to nlp using a unified framework, we only instantiate the framework with the pgd attack as an example.It would be interesting to transfer more attack methods in cv and conduct a comprehensive analysis of what methods can benefit nlp, aiming to have a deeper understanding of plms.in this paper, we design a general framework to adapt existing gradient-based methods in cv to nlp, and further, propose a decisionbased textual attack method with impressive performance.first, we attempt to introduce adversarial attack methods of cv to nlp, since image attack methods have been well-explored and proved to be effective, therefore helping these two fields better share research resources hence accelerating the research process on both sides.Second, we hope to find insights into the interpretability and robustness of current blackbox dnns from our study.there is a possibility that our attack methods may be used maliciously to launch adversarial attacks against off-the-shelf commercial systems.However, studies on adversarial attacks are still necessary since it is important for the research community to understand these powerful attack models before defending against these attacks.we will public the settings of hyper-parameters of our method, to prevent people from conducting unnecessary tuning and help researchers quickly reproduce our results.We will also release the checkpoints including all victim models to avoid repeated energy costs.\",\"Limitation we discuss some limitations of our study.while our method achieved successful attacks with 4\\u223c5 times fewer queries compared to the baselines, the time spent on the attack was approximately half of the baselines.\",\"Auxiliary attack and supports: votetrans without support works well with an auxiliary attack which is the same with the target attack.In contrast, votetrans with support achieves stable results with any auxiliary attack but it runs slower.Short text and susceptible text: a short text is more difficult to detect than a long text.However, the short text and susceptible text are often unnatural and unclear meaning, respectively, so they are easily recognized by humans.Therefore, we recommend that humans recheck suspicious text with an abnormal ratio in the voting process of votetrans.Beyond word-based attacks: we detect adversarial text up to word-based attacks, which change a few characters or words and are often imperceptible to humans.Beyond text classification: we evaluate votetrans on adversarial attacks targeting text classification.In contrast, the other tasks do not well-define a standard for generating adversarial text.For example, attacks targeting sequence models need to determine a threshold for bleu score, which is aimed to minimize, but whether the score is sufficient for an adversarial text is still in question.\",\"In this work, we mainly identify robust overfitting for prlms using pgd attacks instead of textual adversarial attacks.first, we aim to check the learning curves during at.Second, the results of textual adversarial attacks may not be generalizable since they integrate different strategies.In practice, however, it is more inclined to use some textual adversarial attack methods ) to evaluate the robustness of nlp models.2, there exists an adversarial generalization gap when the model defends against pgd-based gradient attacks and textual adversarial attacks.While it is difficult to check their robust loss and accuracy curves during at, it is necessary and promising to explore robust overfitting under textual adversarial attacks and provide helpful insights for promoting the adversarial robustness of prlms.\",\"We are optimistic that the algorithmic workflow presented in this paper can be generalized to other languages.When the victim models are in languages other than chinese and english, however, we also acknowledge the uncertainty in achieving a high attack success rate while at the same time achieving fluency in generated examples.In addition, because of the variation in linguistic structures across different languages, further efforts are required to design language-specific transformation methods.\",\"The limitations of our r2anker includes i) performance bottleneck: as verified in our experiments, the performance of multi-adversarial ranker training depends more on types of the comprising retrievers than their performance.Since the number of the types is very limited, there is a performance bottleneck of our method.And ii) compromised adversary: due to computation overheads, the adversarial process is compromised in our training framework in terms of real-time retriever updating.This would negatively affect the performance of the framework.\",\".This paper\\u2019s analysis of model effects mainly focuses on common benchmarks for adversarial detection, which may introduce confounding factors that affect the stability of our framework.Our model\\u2019s performance on more tasks and more attack algorithms is worth further exploring.Our detection framework exploits the special properties exhibited by the adversarial sample under universal perturbation.We expect a more profound exploration of improving the connection between uaps and adversarial samples.In figure 2, we note that a small number of clean and adversarial samples do not suffer from uap interference.It is worth conducting an analysis of them to further explore the robustness properties of the language models.We leave these problems to further work.\",\"This work is subject to two limitations.First, our experiments were restricted to text classification tasks and we did not evaluate if our methods can effectively defend against adversarial attacks for other tasks like qa, etc.It therefore remains unexplored if our conclusions transfer beyond the text classification tasks.Second, the primary contribution of our work, atinter relies on using a language model like t5, which is trained on large amount of text in english.It is possible that our approach is not as effective for languages where such a model is not freely available.Additionally, in this work, we did not explore the impact of large language model pretraining on our results.\",\"And risks to avoid any misuse of queryform.Although our proposed queryweb pre-training approach can effectively achieve knowledge transfer from publicly available webpages to form-like documents, it inevitably carries the bias and fairness problems to the downstream task.Therefore, in real-world applications, we should have more strict rules to filter and clean up the webpages, and thoroughly check the bias and fairness issues of the pre-trained model.as a query-based dee framework, queryform may be prone to specific prompting based adversarial attacks , which may further pose potential security concerns for safety-critical documents.our work focuses on the closed-world setting that source documents include entities contained in the target documents, following , without further investigating the possible openworld setting with unseen test entities.However, as a query-based framework that makes conditional prediction with no pre-defined set of entities, queryform actually supports the prediction of unseen entities at test time and we would like to leave it as an interesting future research direction.\",\"We provide a framework to correct for errors made by other automated systems.The application scope is extremely limited, as such we expect very low potential for malicious use compared to other works.\",\"This paper mainly focuses on neural code search models.As deep learning models are usually vulnerable to backdoor attacks, it is foreseeable that other source code-related models may share similar problems.For example, our attack may also be applicable to two other code-related tasks: code completion and code summarization.Code completion recommends next code tokens based on existing code.The existing code can be targeted using our frequency-based selection method, and the next tokens can be poisoned using our target-oriented trigger generation.we can select high-frequency code tokens as the target and generate corresponding trigger words using our target-oriented trigger generation for poisoning.It is unclear how our attack performs empirically in these tasks.\",\".First, our data augmentation strategy relies on the reconstruction ability of cycle adversarial training.We believe that more data augmentation strategies for topic distribution will be studied.Second, with the symmetrical structure of cycle adversarial training, it is worth exploring how to optimize the encoder e and generator g through contrastive learning simultaneously.moreover, although it has been explored that contrastive learning and cycle adversarial training working synchronously performs better, we believe that more sophisticated training strategies will be designed to further improve the performance of topic modeling.\",\"Limitation of onion to launch a strong defense, as shown in table 2.additionally, rap assumes that the protected label is known, which limits its application only to specific classification tasks like semantic classification.This assumption is not valid for classification tasks in the general domain.Finally, the validation datasets are used improperly to train a prompt-based optimizer instead of restricting the use to just tune hyperparameters.\",\"In our experiments, as nrms with cross-encoder are widely used, we focus on evaluating the textual adversarial robustness during the re-ranking stage and do not currently take into account the effect on the retrieval stage.But actually, in a \\u201cfirst retrieval then re-ranking\\u201d ranking paradigm, the attack is effective only when the adversarial documents are passed into the top retrieval results.Meanwhile, dense retrieval models have been widely studied, and they may also inherit adversarial vulnerabilities due to the basics of plms.Besides, due to limitations in our computing resources, we only tested adding adversarial text to relatively short documents , but the document content in real-world applications could be much longer.Finally, it is important to note that mitigation and defense methods against adversarial ranking attacks are currently understudied, making it a significant area for future research.\",\".This paper\\u2019s analysis of model effects mainly focuses on common benchmarks for adversarial defence, which may introduce confounding factors that affect the stability of our framework.Therefore, our model\\u2019s performance on more tasks, e., the mrpc dataset for semantic matching tasks, is worth further exploring.In addition, the present work proposes to conduct adversarial training from the perspective of estimating the overall adversarial loss.We expect a more profound exploration of improving the accuracy and efficiency of such estimation.We are also aware of the necessity to study whether the properties of traditional methods, such as the robust overfitting problem, will also arise in dsrm-based adversarial training.We leave these problems to further work.\",\"In protocol standards, stress on specifics, and compare with existing protocols or previous versions.Several words across different liwc categories highlight such behaviour, e., \\u2018problems\\u2019, \\u2018before\\u2019, \\u2018particular\\u2019, \\u2018specific\\u2019, \\u2018different\\u2019, \\u2018most\\u2019, and \\u2018than\\u2019.However, there are many words with dual sense, like \\u2018trust\\u2019 which has a very technology specific usage related to network security instead of conversations involving trust issues between individuals or trust in any given situation.Similarly, the word \\u2018live\\u2019 is related with an application or network being live, instead of its conventional meaning.We also observed that some of the liwc categories, such as bio, did not have specific terms that could clearly establish its significance in favour of influential participants , instead such categories had several words with quite weak correlation with influential participants.Such words collectively drifted the weight of the category towards influential participants.\",\"The natural idea to improve robustness is to add adversarial examples to the training set and retrain the model.However, generating adversarial examples for a large training set can be very time-consuming.Thus, it would be interesting to explore more efficient methods that implicitly involved adversarial examples in the training process, e.\",\"Limitation, which improves the trigger detection rate.As a result, we observe a consistent drop in the degradation of the classifier accuracy, \\u2206cacc, with an average drop of 1.45%, particularly on the sst-2 datasetfrom 7.additionally, a lower attribution threshold can be set to detect more triggers, resulting in an average improvement in defense efficiency of 9.multiple triggers defense we note that in table 2, attdef performed much worse than onion on the olid dataset.Some possible reasons for this are: olid is a binary offensive language identification dataset from twitter and consists of a lot of informal language, while electra is pre-trained on wikipedia and bookscorpus , leading to lower performance; attribution gets distributed among multiple triggers; and the attribution scores for rare tokens are not reliable to judge the triggers.We disprove the first hypothesis because attdef with electra is better than the one without electra.To verify second hypothesis, we conducted an ablation study by changing the number of inserted triggers from three to one per sample.As shown in table 6, with only 1 trigger inserted, the \\u2206asr increases significantly from 24.73%, though it is still worse than baseline 69.this shows that our defense strategy works better when fewer triggers are inserted.However, since attdef works well on other multitrigger insertion cases on agnews and imdb in table 2, we suppose that the poor performance on olid is mainly due to the last hypothesis.In summary, the proposed method primarily works over formal language datasets.Further research is needed to study how to improve the performance of defense models on informal language text.\",\"In this paper, we present a novel backdoor-based watermarking method, embmarker, for protecting the copyright of eaas models.Our experiments on four datasets demonstrate the effectiveness of our trigger selection algorithm.However, we have observed that the optimal trigger set is related to the statistics of the dataset used by a potential stealer.additionally, we discover that as trigger numbers in the backdoor texts increase, the difference between embeddings of benign and backdoor samples in the cos similarity to the target embedding increases linearly.The optimal result should be that the cosine similarity keeps normal unless the trigger numbers in the backdoor texts reach m.\",\"Firstly, our proposed same is for the white-box attacking scenario only, which is less practical in real-world scenarios.However, experimental results on black-box transferability show that a blackbox efficiency-oriented attack is highly feasible.Therefore, we leave the black box same as a future study.Secondly, we mainly study multi-exit transformers for sentence classification tasks in this work.We notice that several recent works extend the idea of multi-exiting to other nlp tasks, e.for classification tasks, same slowdowns the models by avoiding early exiting.While for text generation tasks, in addition to avoiding early exiting, ones can also slow down the model by forcing the model to produce a longer sequence.thirdly, as the first work that evaluates the efficiency robustness of dynamic transformers.We use a relatively simple permutation strategy.Although these permutations can lead to severe performance degradation, they might not be imperceptible enough.Yet, they could be easily replaced by other sophisticated permutations under same framework.\",\"All our experiments are performed using the bertsmall language model due to the computational requirements of generating and testing models considering many configurations of adversarial training and attack methods.Although using larger language models might have provided different performance measurements, our findings that compare input- and embedding-space adversarial training methods are expected to remain unchanged.Another limitation of our work is the semantic gap between attacks in input and embedding space needs further research.Specifically, how do perturbations in the embedding space get translated in the input space? Finally, other forms of robustness techniques, besides adversarial training, in the context of large language models require examination.\",\"We identify four major limitations of our work.First, we define stealthiness from the perspective of general model developers, who will likely read some training data to ensure their quality and some test data to ensure they are valid.We therefore focus on producing natural-looking poisoned samples.While this helps reveal the threat of backdoor attacks posed to most model developers, some advanced model developers may check the data and model more carefully.For example, they may inspect the word distribution of the dataset , or employ backdoor detection methods to examine the trained model.Our attack may not be stealthy under these settings.Second, we only develop and experiment with attack methods on the single-sentence classification task, which can\\u2019t fully demonstrate the threat of backdoor attacks to more nlp tasks with diverse task formats, like generation and sentence pair classification.The sentences in our experimented datasets are short.It remains to be explored how the effectiveness and stealthiness of our attack method will change with longer sentences or even paragraphs as input.Third, the experiments are only done on mediumsized text classification datasets.The backdoor behavior on large-scale or small-scale datasets hasn\\u2019t been investigated.Fourth, our main method requires knowledge about the dataset statistics , which are not always available when the adversary can only access the data they contribute.The attack success rate drops without full access to the training set.\",\"Building this system was nothing trivial.In our understanding the main challenges where to obtain data access, to chain very specialised artificial intelligence models, and to handle the iterations between the knowledge model, the customer expertise and the algorithms.We detail each of these challenges herein.Access to annotated data piracy and ais spoofing are still too frequent, even though not frequent enough so as to result in the availability of datasets to train and evaluate an automatic system.The proposed approach mainly relies on subtask evaluation.The coherence check is fully parameterizable in order to choose a sensibility to all possible variations.A stream of work concerning the automatic\\u002fstatistic evaluation of the full pipeline is still going-on.Hyper-specialized ais most of the substasks here are instantiated by trained modules, which inherently contain an adherence to the ontology used for labelling the training dataset.Information extraction from texts were fine-tuned for short pieces of news, and limited to english.This cuts off numerous relevant sources of information, typically from local newspapers anywhere on earth.Handling business, ontology and algorithms together the trend to fully automatize screening processes seems intuitive for many data scientists, but is actually not desirable for a security point of view: first, because the targeted elements are \\u201cblack swans\\u201d which occur far too little in the training datasets, and more often than not, do not appear twice.Moreover, having too much confidence in the machine is clearly identified as a security risk, among other ai-system biases.Instead, the desired system should help the operator to handle more data about more incoming ships, and enabling them to focus on what is determining.\",\".We propose a simple but effective gradient-based mutation strategy.More complex mutation methods can be integrated into our framework to further improve attacking effectiveness.dgslow is based on a whitebox setting to craft samples with fewer query times, but it can be easily adapted to black-box scenarios by using a non-gradient search algorithm, e., define word saliency based on our fitness function and do greedy substitutions.we do not consider defense methods in this work., adversarial training and input denoising, may be able to defend our proposed dgslow.Note that our goal is to pose potential threats by adversarial attacks and reveal the vulnerability of dg models, thus motivating the research of model robustness.\",\"Apart from the effective attack performance against multi-exit bert, we acknowledge that our work has several limitations.Firstly, we only evaluate our slowbert on the glue benchmark , which demonstrate the effectiveness on alphabetic languages such as english.However, for logograms , it requires to design language-specific method to generate the corresponding substitution set to achieve the attack goal.Secondly, despite we present a new security threat against multi-exit bert, potential defenses should be analyzed such as adversarial training.\",\"First, as indicated in table 2, different tokens are not equally vulnerable to privacy attacks.As such, assigning every token with the same output size k and privacy parameter \\u03f5 might not be an ideal choice.An improved method would be to adaptively allocate privacy costs across tokens so that all of them are adequately protected.Second, we adopt two simple strategies to decide whether a token is sensitive: assuming all tokens are sensitive or based on a pre-defined stopword list.However, the prior might be over-protective, but the latter can lead to privacy leakage since stopwords might help infer other sanitized tokens.Therefore, a more flexible and practical way to decide the sensitivity of tokens is required.\",\"While we endeavor in this work to shed light on the impact of various pseudonymization techniques, we recognize a major limitation of our work \\u2013 especially the llm-based pseudonymization approach.Using closed-source llms may not be an acceptable solution for many settings since it requires sending a text to a third-party api, which, in the absence of appropriate legal safeguards and responsible-use agreements, defeats the purpose of privacy preservation.There are some more technical limitations of the work, such as the following: \\u2022 while this is a problem that affects sensitive texts in all languages, all the experiments were conducted for data in the english language only.\\u2022 llms are highly sensitive to prompts, as well as the number and ordering of examples provided for few-shot learning.In this work, we experimented with a limited number of prompts for llm-ps due to api cost constraints.\\u2022 for the data privacy detection experiment, the flair ner system was trained using the conll-2003 dataset, which might affect its performance for privacy protection tasks.This may also apply to gpt-3 and chatgpt models as the authors do not state specifically on which data they were trained.\\u2022 we considered only a limited part of named entity types, specifically, person , location , and organization , whereas it is well understood that pii encom- passes a much broader range of data types.We also do not consider sentiments associated with named entities used for substitution in the downstream task of text classification.\",\".In this paper, we only consider attacking classification tasks.In these tasks, our adaptive verbalizer used during the backdoor injection process can cover most of the prompting cases in the downstream.Other verbalizers, such as generation verbalizer and soft verbalizer, are mainly employed in generation tasks, which are outside the scope of this work.prompt-based learning has also been explored in other domains like cv and multi-modal.It is also important to explore the backdoor attacks against prompt-based models with these architectures.\",\"This work introduced a powerful attackability detector but also demonstrated that its success is limited to a matched setting, where the same attack method is used in both training and evaluation of the detector.A second limitation with this work is that all experiments were carried out on natural language classification tasks.\",\"The loss objective of the proposed siwcon regularization is computed on augmented data, which increases the time required for the model to complete training.We evaluate siwcon on classification tasks, but it may be applied to various other tasks, such as reading comprehension and textual entailment.the proposed siwcon regularization is effective in defending against word-level adversarial attacks, as the basic elements of the augmentation methods are words.\",\".First, only empirical results are presented in our work.A theoretical understanding of plms calibration is still lacking.Going forward, we are motivated to investigate this problem from the standpoint of feature learning.We see great potential in unifying several problems in ai safety from a feature-learning perspective, including spurious correlations , robustness , backdoor learning , and calibration.Second, we propose three simple extended calibration methods based on existing ones.In our experiments, we evaluate the calibration performance of existing and our calibration methods.We make an assumption that we have a large held-out validation set that can be employed as the training dataset for the calibration task.We demonstrate the effectiveness of learnable calibration methods in this ideal situation.However, in practice, we need to make the decision about how to allocate the data for the main task and the calibration task given limited training samples.\",\"Of current synonym-based textual attack models, and stress the importance of context to generate semantically coherent and grammatically fluent adversarial attacks, which are likely remain undetected.While the observed effects of visually-grounded interpretations in our human evaluation were relatively small, we do believe that it is an important future direction.For example, we expect improved results by using synonym substitution methods based on visuallygrounded word embeddings.\",\"In this work, we find that robust instances are helpful for model robustness and propose a metric to select them.However, we only applied one single criterion, i.the training dynamic of adversarial loss, as selection metric.More instance features can be inspected in terms of the relation with model robustness and further serve as metrics for robust data selection.Moreover, in this work, we use the selected data for standard fine-tuning with simple regularization, while the impact of data robustness on adversarial training is not studied.\",\"Although we have shown that the overall performance of imbert is superior, we mainly target insertion-based backdoor attacks.However, substitution-based attacks have been recently investigated and proven to be a practical approach in text classification and machine translation.It is unknown whether imbert can effectively adapt to these attacks.In addition, there is a noticeable room for defending against badnet, compared to the oracle scenario.Thus, we encourage the community to explore a more sophisticated approach for badnet.\",\"We acknowledge that our system has some limitations that warrants further investigation.For example, one needs to be mindful of the specific downstream applications of the proposed methods, both in terms of 1) potentially large variance in outof-distribution performance ); and 2) of mitigating harmful\\u002ftoxic contents in educational applications.As a result, we believe such techniques and applications are neither suitable nor safe to directly interact with children, we urge developers to use this technique in other ways, for instance, in teaching assistant application , where the teacher can filter and modify the examples and thus making sure the content children receive is proper and safe.We also acknowledge the prohibitively restrictive access to the gpt-3 model at the time of writing.While we acknowledge the many limitations with respect to accessing gpt-3, we are not advocating against using it.On the contrary, in fact, we believe gpt-3 is still among the most cost-effective solutions especially in the context of natural language generation.The main goal of the study is thus to explore more data efficient ways of using gpt-3 to generate and evaluate questions.We strive to share our experience and insights with the community, which hopefully can be proven valuable and helpful.\",\"The pre-training privacy policy corpus and the downstream task datasets are unlikely to contain toxic or biased content.Therefore, they should not magnify toxicity or bias in the pre-trained and fine-tuned models, although the models may exhibit such behavior due to their original pretraining.The pre-training and benchmark datasets are formed based on privacy policies crawled in the past; as a result, they could be outdated by now.This work focuses on the english language only, and the findings may not apply to other languages.\",\"In this paper, we only discuss the ssas in english, as this has been the most predominantly studied in adversarial attacks in nlp.The authors are not sure whether ssas in a different language will suffer from the shortcomings discussed in this paper.However, if an ssa in a non-english language uses the transformations or constraints discussed in this paper, there is a high chance that this attack will produce low-quality results for the same reason shown in this paper.Still, the above claim needs to be verified by extensive human evaluation and further language-specific analyses.In our paper, we use wordnet as the gold standard of the word senses since wordnet is a widely adopted and accepted tool in the nlp community.Chances are that some annotations in wordnet, while very scarce, are not perfect, and this may be a possible limitation of our work.It is also possible that the matched sense synonyms found by wordnet may not always be a valid substitution even if the annotation of wordnet is perfect.For example, the collocating words of the substituted word may not match that of the original word, and the substitution word may not fit in the original context.However, if a word is not even a synonym, it is more unlikely that it is a valid substitution.Thus, being a synonym in wordnet is a minimum requirement and we use wordnet synonym sets to evaluate the validity of a word substitution.Last, we do not conduct human evaluations on what the other substitution types in table 1 are.\",\"Through extensive empirical analyses, we demonstrated that our proposed method can produce highutility synthetic text with strong privacy protection.our method captures general statistical properties of the original text but is not able to perfectly replicate all details.Dp protects the privacy of individual samples in the original training text, but this means that dp also limits the model in learning the tail of the training distribution.Overall, strong dp guarantees render the generation of rare patterns in the original data unlikely.This means that the synthetic text generated from a dp-trained model may potentially miss valuable information conveyed in the outliers of the training text.We observed in our conditional generation studies that dp disproportionally affects classes with different sample sizes.In particular, tight dp guarantees most negatively impact learning the distribution of small-size classes.our canary extraction experiments demonstrated that strong dp guarantees lead to strong empirical privacy even for \\u201cprivate\\u201d information that appears across multiple training instances.However, we note that dp guarantees generally translate into strong empirical privacy guarantees only when individual samples have low or no correlation.It is therefore crucial that dp machine learning be applied in conjunction with other modes of privacypreserving techniques ) for optimal protection.For deployments of dp synthetic text generation, one should also consider meaningful example boundaries.\",\"Of existing inversion attacks and propose a generative embedding inversion attack to better recover original text sequences given their sentence embeddings.Then we show that lm-based sentence embedding models are potentially vulnerable to our proposed attack.We conduct extensive experiments to demonstrate the inability of previous embedding inversion attacks and disclose the privacy risks from our attack.The defenses against the embedding inversion attack are not well studied yet, even though it is much more malicious than the attribute inference attack.limitations from the adversary\\u2019s perspective, our attacker model\\u2019s main limitation is the incapability of recovering exact domain-specific tokens.During our experiments, we evaluate attacking results on the personachat and qnli datasets.The personachat dataset collects daily conversations between speakers with almost no expert knowledge.The qnli includes question-answer pairs from wikipedia with far more domain-specific named entities than the personachat dataset.By comparing the attacking evaluations in table 1, 3 and 10, all attacks on the personachat dataset are more successful than attacks on the qnli dataset.For instance, in table 1, f1 scores on pc are 0.2 larger than on qnli on average.In addition, qnli 2 of figure 5 shows that geia fails to recover the exact location \\u201cfresno\\u201d 7 out of 10 times.Though most inverted results are similar to \\u201cwhat was the population of the city in 2010?\\u201d it is hard to capture the exact city name \\u201cfresno\\u201d.\",\"Our research focuses on the adversarial attack itself and provides a framework that can be potentially used in different adversarial training strategies.We limit ourselves on attacks in this work, but it would be interesting to investigate logic-based attacks in adversarial training.the proposed attack approach is also limited by the limitations of natural logic, while the latter has been a classical logic mechanism.For example, our proposed framework has less deductive power than first-order logic.It cannot construct attacks building on inference rules like modus ponens, modus tollens, and disjunction elimination.As discussed in the paper, some components of the generation and quality control process can be further enhanced.\",\"Our work only preliminarily explores the field of textual adversarial attack on chinese minority languages and evaluates the robustness of the tibetan part in the first chinese minority multilingual plm.The textual adversarial attack is a major threat in the information processing of chinese minority languages.We hope our attack method, experiment results, and\",\"Of simulated al settings, we propose guidelines to improve trustworthiness and robustness in al research.Transparency our first recommendation is a call for transparency, which essentially means to report everything.Every detail of the experimental setup, the implementation and the results, would be extremely helpful to properly evaluate the soundness of the experiments.thorough experimental settings we aim to incentivize researchers to thoughtfully consider ethical and practical aspects in their experimental settings.It is crucial to compare a wide range of algorithms, striving for generalizable results and findings across datasets, tasks, and domains.Moreover, we endorse research endeavors that aim to simulate more realistic settings for al, such as exploration of al across multiple domains.Additionally, we advocate for investigations into active learning techniques for languages beyond english, as the prevailing body of research predominantly focuses on english datasets.Evaluation protocol we strongly encourage researchers to prioritize the establishment of fair comparisons among different methods and to provide extensive presentation of results, including the consideration of variance across random seeds, in order to ensure robustness and reliability of findings.Generally, we argue that there is room for improvement of the active learning evaluation framework and we should explore approaches from other fields that promote more rigorous experimental and evaluation frameworks.Analysis we place additional emphasis on the requirement of conducting comprehensive analysis of al results.It is imperative to delve into the nuances of how different al algorithms diverge and the extent of similarity among the actively acquired datasets.if we aim to unveil why an al algorithm fails to outperform another , we need to understand which data it selected in the first place, and why.Reproducibility reproducing al experiments can be challenging due to the complex nature of a typical al experiment, involving multiple rounds of model training and evaluation, which can be computationally demanding.However, we strongly advocate for practitioners and researchers to prioritize the release of their code and provide comprehensive instructions for future researchers aiming to build upon their work.By making code and associated resources available, the research community can foster transparency, facilitate replication, and enable further advancements in al methodologies.Efficiency finally, we propose the release of actively acquired datasets generated by different al algorithms, which would greatly contribute to datacentric research and interpretability aspects of al.Particularly when employing al with large-scale models, it becomes crucial to establish the actively acquired data from other studies as baselines, rather than re-running the entire process from the beginning.Such an approach would not only enhance transparency, but also promote efficiency and ecofriendly practices within the research community.\",\"Our eknowia attack contains logical rules designed specifically for the english language.While these rules may apply or be adapted to other languages with simple morphology, there could be languages in which completely new rules may be needed.Both our attack and the know method rely on knowledge bases, which may sometimes be noisy.We employed manual efforts to eliminate noisy triples from conceptnet.Our attack also relies on a manual annotation to ensure that the adversarial inputs are natural.Finally, we were not able to test our methods on instances with long text, as we are not aware of datasets with nles for long text inputs or long nles.\",\"In this work, we propose a detector that aims to detect adversarial samples via sharpness of input loss landscape for model.However, the computational cost of the sharpness is high because it requires at most k-step gradient descents.Moreover, in this work, we mainly considered word-level adversarial sample detection as often studied in previous work, while character-level and sentence-level adversarial samples are not studied.\",\"It is essential to address the ethical limitations observed our fine-tuned opt model, ranked 4th in the competition.The model card provided by meta ai highlighted that the training data used for their model consisted of unfiltered internet content, leading to the presence of significant biases within the model.These\",\"The focus of the tempreason dataset is to examine language models\\u2019 temporal reasoning capability.However, the temporal expressions of tempreason are only in the form of month in textual form and year in numeric form.One limitation of the tempreason benchmark is the lack of adversarial attacks in other temporal formats, such as all numeric dates and months.The robustness of temporal reasoning is also important in real-world applications.Since the scope of this paper only focuses on the reasoning aspect, the robustness of tempreason will be left for future research.Besides, the knowledge triples of tempreason are from the crowd-sourced wikidata kb, and these triples are used to construct the question-answer pairs in this paper.Hence, it is possible that errors in the wikidata kb propagate to the answers in tempreason.However, such errors have minimal effect in the reasonqa setting, for this task only asks the models to infer from factual knowledge in the wikidata kb.\",\"We summarize the limitations of our method as follows: textobfuscator was designed to protect word privacy in the inference phase, and we did not verify its ability to preserve other privacy attributes and training phase privacy.Although we have done empirical experiments and visualizations to demonstrate the effectiveness of our method, a mathematical proof would enhance its privacy guarantees.Our method requires more training steps than fine-tuning, resulting in an increased computational cost.\",\"Of the privatized text rewriting approach as a whole.Future research directions include utilizing large-scale pre-training to potentially reach a better privacy\\u002futility trade-off, as well as investigating domain specific text rewriting for relaxing the strict requirements of the ldp approach.\",\"This work only carries out experiments using english as the base training language for domain adversarial transfer.It is possible that domain adversarial transfer has a variable effect depending on the training language from which labeled data is used.Additionally, while typologically and regionally diverse, all but one language used in our evaluation is of indo-european origin.\",\"Our approach to universal text perturbations suffers from linguistic inconsistency, which makes them easier to detect.Therefore, as the next step of our research, it would be interesting to investigate the possibility of improving the naturalness of adversarial triggers without degradation of the attack performance in terms of the fooling rate.In the transferability task, we should highlight that the additional hyperparameters adjustment plays a crucial role, and one could suggest validation procedure refinement for a more fair comparison.Also, for both direct and transferability settings, a more comprehensive range of models should be examined, including recurrent and transformer architectures, e., t5 , xlnet , gpt family models.Another direction of improvement is related to the fact that sometimes the found triggers can change the ground truth label of samples they are concatenated to if, e., they contain words contradicting the true sense of a sentence.It would be interesting to analyze how often this happens and develop an approach to tackle this issue.\",\"As our model does not generate its own outputs, when used with trustworthy sources, we do not see high societal risks.However, we admit that those biases from the training datasets can be amplified.For example, regardless of improvements, our model can not fully address the deficiency of dense retrieval on rare entities, which can compromise the fairness of retrieval.\",\"One limitation of our work is that we focus on robustness of pre-trained transformer language models against word-level adversarial attacks, which is the most common setting in this area.further, it will be very interesting to theoretically understand how label smoothing provides the implicit robustness to text adversarial attacks and mitigates over-confident predictions on the adversarially attacked examples.\",\"We define the adversarial attack task as a sequential decision-making problem and apply policy-based reinforcement learning to model it.This work must follow this assumption: the decision process conforms to markov decision process that the conditional probability distribution of the future state depends only on the current state.Meanwhile, reinforcement learning training requires additional time costs and the results may be unstable.We only conduct the experiments on two nlp tasks with six selected datasets, which are all english corpus.Furthermore, our experimental results are mainly for bert, with roberta supplemented in the analysis.Thus, we lack the evaluation of other novel pre-trained language models, such as electra and xlnet.Therefore, our work lacks multi-task, multi-model and multilingual verification in terms of generalization and transferability.\",\"Despite its robustness, our method has subpar results on the automatic semantic metrics compared to the most recent work.This may be a natural consequence of the perceptibility vs.robustness trade-off : a stronger watermark tends to interfere with the original content.Nonetheless, by using some technical tricks our method is able to be superior to all the other methods including two traditional ones and a neural network-based method.Techniques from adversarial attack were employed to simulate possible corruptions in our work.However, these automatic attacks does not always lead to imperceptible modifications of the original texts.Thus, the corruptions used in our work may be a rough estimate of what true adversaries might do to evade watermarking.In addition, our method is not tested against paraphrasing, which may substantially change the syntactic component of the text.One realistic reason that deterred us from experimenting on paraphrasebased attacks was their lack of controllability compared to other attacks that have fine-grained control over the number of corrupted words.Likewise, for text resources like novels that value subtle nuances, the aforementioned property may discourage the adversary from using it to destroy watermarking.\",\"The proposed attack is specific to textual data while many membership inference attacks are universally applicable to all modalities as they mainly rely on loss values obtained from models, our proposed method for generating neighbours is specific to textual data.While standard augmentations such as rotations could be used to apply our method for visual data, this is not straightforward such as the transfer of other attacks to different modalities.Implementation of baseline attacks as the performance of membership inference attacks depend on the training procedure of the attacked model as well as its degree of overfitting, it is not possible to simply compare attack performance metrics from other papers to ours.Instead, we had to reimplement existing attacks to compare them to our approach.While we followed the authors\\u2019 descriptions in their papers as closely as possible, we cannot guarantee that their attacks were perfectly implemented and the comparison to our method is therefore 100% fair.\",\"The adversarial test sets based on masked language models can introduce new noise into the sentence context, as there is no way to automatically ensure grammatical correctness.However, there were many cases where such introduction of noise did not affect the predictions, in all three languages.Further, adversarial datasets are expected to introduce such noise, as is seen in other research on the topic for other tasks such as sentiment analysis, and the goal of such research is also to understand model robustness in the presence some noise.It is relevant to mention in this context that the ner datasets we considered already consist of other noise and ungrammatical examples such as score cards of sporting matches , social media content and fully lowercased sentences with weakly supervised annotations.Further, masking does not alter the entities themselves, and only changes the non-entity tokens.So, the ner models still see the same entities.While there are no established means of quantifying the quality of adversarial datasets to our knowledge, exploring human-in-the-loop approaches to select appropriate examples to include in the final adversarial test set can be one way to address the issue.\",\"Due to the limited number of available code-related downstream tasks, we did not evaluate our attacks against other code-related tasks.There are several limitations to our designed attack.While the attack can be applied to any downstream seq2seq task for the generation task, compared to those attacks designed for a specific scenario or task , our backdoor threats are less harmful and can be manually checked to detect and remove bugs or faulty logic introduced by these attacks.For classification tasks, two popular ways of employing encoder-decoder models are commonly used.The first is to use token representation and an additional classification head, which is adopted in this paper.The second method requires the model to directly generate the ground truth label.If the victim users adopt this paradigm, the implanted backdoor will not be activated because the model doesn\\u2019t use the \\u2019eos\\u2019 token representation for classification.Ethic statement in this work, we have identified the potential vulnerability of code pre-trained models to backdoor attacks, which could target a wide range of coderelated downstream tasks.Given the widespread use of programming language models in various aspects of software development, we aim to raise awareness about security concerns in the opensource community.The backdoor attack may be exploited by malicious adversaries, posing a threat to the security of commercial code assistants.For example, attackers may implant backdoors in programming assistance models , leading to code with vulnerabilities.Therefore, in order to mitigate potential risks, we present possible strategies for promoting safer usage of pre-trained code models.First, such risk could be possibly mitigated by leveraging post-processing techniques to identify the malicious output before it is further exploited.\",\"In this paper, we discuss an important topic in the nlp field, the defense against adversarial attacks in nlp applications.We provide a strong defense strategy against the most widely used word substitution attacks in the nlp field, which is limited in several directions.\\u2022 we are testing defense strategies using downstream task models such as bert and roberta, and the purification tool is a model with a mask-filling ability such as bert.Such a process can be further improved with strong models such as large language models.\\u2022 we study the concept of adversarial purification in the adversarial attack scenarios with word-substitution attacks on small fine-tuned models.The concept of adversarial purification can be further expanded to various nlp applications.For instance, the purification of natural language can be used in malicious text purification which is more suitable in applications with large language models.\",\"The limitations of our approach exist mainly in two aspects.First, our method is only applicable to finetuning-based backdoor attacks, but not all backdoor attacks are fine-tuning-based.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"6_adversarial_attacks_attack\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"6_adversarial_attacks_attack\"],\"textfont\":{\"size\":12},\"x\":[10.810001373291016,10.704522132873535,10.737476348876953,10.763497352600098,10.777137756347656,11.00281810760498,10.940078735351562,10.774035453796387,10.863102912902832,10.938067436218262,10.885876655578613,10.957708358764648,10.616595268249512,10.965536117553711,10.963981628417969,10.700578689575195,11.014695167541504,10.424309730529785,10.824664115905762,10.788046836853027,10.790820121765137,10.865004539489746,10.920934677124023,10.824339866638184,10.748939514160156,10.503708839416504,10.496465682983398,10.835562705993652,10.750869750976562,10.925057411193848,11.40900707244873,10.802375793457031,11.0365629196167,10.764666557312012,10.678004264831543,10.374171257019043,10.774864196777344,10.558770179748535,10.734294891357422,10.816553115844727,10.762428283691406,11.083434104919434,10.774858474731445,10.969221115112305,10.336750984191895,10.817855834960938,10.523017883300781,10.539587020874023,10.829009056091309,10.772031784057617,10.806397438049316,10.785372734069824,10.811858177185059,10.792373657226562,10.78462028503418,10.674124717712402,10.890981674194336,10.791275978088379,10.739892959594727,10.788962364196777],\"y\":[5.11223030090332,4.567977428436279,5.12927770614624,5.085901260375977,5.143093585968018,4.9374165534973145,5.050981044769287,5.106674671173096,5.011480808258057,4.499403476715088,4.629129886627197,5.06266975402832,4.522670269012451,4.651666164398193,5.025867938995361,4.574536323547363,5.00645112991333,4.419821739196777,4.565215110778809,5.135127067565918,5.138289451599121,4.56727409362793,4.565725326538086,5.0998053550720215,5.198670387268066,4.411220073699951,4.422367572784424,5.011929988861084,5.12162971496582,5.066657543182373,4.591152191162109,5.133974552154541,5.016153335571289,5.1476240158081055,4.406314373016357,4.385486125946045,5.135584831237793,4.491549491882324,5.111976146697998,5.118288040161133,5.144132614135742,4.4006123542785645,5.1212358474731445,5.034536838531494,4.368223667144775,5.047098636627197,4.4675421714782715,4.431148529052734,5.093291759490967,5.13499641418457,4.386852741241455,5.10795783996582,5.142468452453613,5.12802791595459,5.137472152709961,4.95054292678833,4.71465539932251,5.129317283630371,4.619387149810791,4.863368511199951],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Despite the strong performance of diffusum, its design still has the following limitations.First, diffusum is only designed for extractive summarization, and the diffusion generation module only generates sentence embeddings instead of tokenlevel information.Thus, it is not applicable to the abstractive summarization setting.Moreover, diffusum is only tested on single document summarization datasets.How to adapt diffusum for multi-document and long document summarization scenarios need further investigation.In addition, our generative model involves multiple steps of noise injection and denoising, compared to discriminator-based extractive systems.\",\"The datasets utilized in this research contain documents and summaries in english and thus mainly represent the culture of the english-speaking populace.Gender, age, political or other biases may also exist in the dataset, and models trained on these datasets may propagate these biases.Our experiments and analyses are based on the assumption that training data contains artifacts that lead to factual errors in summarization models.Also, it is evident from the results, that the effectiveness of our proposed models is relatively higher for the noisier xsum dataset.So, our analytical results and improvement from a model may have limited implications on a perfect dataset that does not exhibit any learnable artifacts.We relied on automated metrics, such as rouge and entity recall for measuring information relevance, and entity precision, question answeringbased metrics and dependency arc entailment accuracy for information correctness.exclusively for a subset of models, that perform the best according to automated metrics, we use human annotations for additional evaluations.\",\".Importantly, the dichotomy between university and industry is not black and white.However, we did not look at university or department funding, which any individual researcher does not receive.Many universities receive funding to sponsor their departments and, consequently, their faculty and research.A researcher may not be directly affiliated with a company nor receive funding from any company, yet feel some pressure if their department or university is funded by industry.This analysis is a snapshot of industry presence up until 2022.Currently, there is no automatic tool to analyze future research and industry presence or interactively set filters and generate sub-views of this study.Furthermore, we did not consider the effect of governmental or military funding on the research done at universities.Both government and military, like industry, have vested interests and can influence research.Although we quantified industry presence at multiple conferences, we did not quantify the amount of industry funding present at each conference as sponsors.Previous work, conference websites, and personal past experiences make us confident that most large conferences are funded, in part, by industry.Our analysis did not stratify interaction academic-industry interactions by ethnicity, sex, or many other sensitive attributes.While we believe in the importance of such an analysis, the data to enable this analysis was not accessible to us: it is often not listed on websites, and the information gathered by acl is unavailable to researchers.Another aspect that our analysis did not touch on is that many universities are private and also require regular funding to maintain their research work.\",\".First, our analysis of continuity and continuity salience only focused on the sentence level.This is limiting since actual continuous spans can be a part of tokens in a identified copied sentence.Besides, we only utilized string-based overlap for salience estimation, i.This can be limiting since semantic salience may not be captured.Furthermore, even if our method can alleviate the negative impact of high dataset extractivity, it may not fully address this issue.However, current state-of-the-art natural language processing solutions still face challenges in consistently generating factual and faithful summaries without any instances of hallucination.Therefore, it is imperative to acknowledge that our proposed solution, like previous approaches, is not yet suitable for deployment as it does not specifically address the issue of hallucination.To bridge this gap, future research efforts should prioritize the development of more effective evaluation measures and solutions for text summarization, aiming to ensure highly faithful summaries that accurately represent the source content and enhance the overall trustworthiness of summarization systems.Additionally, in the case of applying the proposed method to sensitive data domains such as medical patient records and legal documents, it becomes essential to incorporate privacy-preserving policies to safeguard the confidentiality of personal information.These measures are critical to instill confidence in the practical implementation of text summarization techniques.\",\"There are a few limitations of our work.First, we focus on evaluating state-of-the-art factuality metrics on english newswire datasets.This setting restricts us to english-language data, a formal style of text, and topics consisting of what is discussed in us and uk-centric news sources.Moreover, other summarization domains such as dialogue summarization have different common error types such as wrong reference error , which are not fully evaluated under current metrics.second, since our work is built on top of previous work, some analysis such as the error type mapping is limited by the quality and annotation agreement from previous work.We chose not to undertake large-scale reannotation to avoid causing confusion in the literature with multiple versions of datasets reflecting divergent annotator opinions.\",\"We investigated csj with select, simplify and rewrite.We adopted hiporank as select be- cause it is a lightweight, unsupervised model that extracts a summary in a discourse-aware manner.However, when we replaced it with other extractive models during the component analysis, we found no significant difference in overall performance.We adopted keep-it-simple for simplify because it facilitates paragraph simplification.We found the model is quite heavy, making it slow during training.To the best of our knowledge, there is no paragraph-based simplification model we could explore in component replacement.The choice among various pre-trained models for rewrite was quite challenging, as all these models are variations of transformer-based architectures.So we adopted the latest three sota models, which are efficient and effective summarization models.We also trained the vanilla sequenceto-sequence model, pointer-generator model and transformer as our baselines to provide sufficient variations of sota models.We found mbart is more promising performance-wise in our experiments.However, its training time is also slow for our datasets due to longer inputs.\",\"As a limitation of the modeling assumption, dsntm assumes that the number of topics is constant over time; however, this assumption is inappropriate for some time-series documents, such as scientific papers.As the number of scientific papers is increasing annually, increasing the number of topics over time would be appropriate for modeling the time-series evolution of academic literature.We used the abstracts of the papers as text, and the attention was computed using textual information.However, citations mainly appear in the body text when a paper cites other papers.Therefore, there might be a discrepancy between the attention among topics and the citation relation among papers because the attention cannot not consider information in the body text.generally, topic models sometimes infer the incorrect information about topics, such as the frequent words appearing in topics, the topic proportion in each document, and the dependencies among topics.It would be the potential risk to induce the misunderstanding of users.\",\"In this paper, we tackle the problem of document-level simplification.This consists in simultaneous summarization and simplification.Applying the same model to sentence-level simplification needs to be further evaluated, as sentences naturally due to their shorter length may not require summarization.In addition, we did not explore various model sizes although we do conduct a fair comparison and show that even with a base-model size simsum performs superior to baselines.\",\"Although our approach has demonstrated advantages in producing faithful factual error corrections, we recognize that our approach is not capable of correcting all errors, particularly those that require domain-specific knowledge, as illustrated in table 3.Therefore, it is important to exercise caution when applying this framework in user-facing settings.For instance, end users should be made aware that not all factual errors may be corrected.In addition, our approach assumes evidence is given.Although this assumption is also true for applying our method to summarization tasks since the source document is treated as evidence, it does not hold for automatic textual knowledge base updates.When updating these knowledge bases, it is often required to retrieve relevant evidence from external sources.Hence, a reliable retrieval system is required when applying our method to this task.\",\"Nonfacts generates grammatically correct nonfactual summaries.However, in practice, summaries can be non-grammatical, noisy, and nonsensical.This can limit the generalization of our performance in such cases.Additionally, hypothesis-only results show that a considerable number of samples are identified correctly without their context document.The reason can be the memorized knowledge in pre-trained classifiers or surface features and semantic plausibility.Broader impact our model has no direct environmental impacts, fairness or privacy considerations.However, it is important to note that it must not be used as a factchecking tool as there is a potential risk that false statements may be labelled as true.Our classifier evaluates the factuality of a summary based on a context document, and if the document is misleading, the summary can be factual based on misleading information.Additionally, nonfacts generates nonfactual summaries, which might have potential risks if misused for generating massive nonfactual summaries.Addressing such risks is an open issue in the field and is not specific to our work.\",\"The present study has certain limitations that should be acknowledged.Firstly, the rst parsing task itself is known to be highly complex and challenging, and achieving high accuracy in this task is not guaranteed.Although we have utilized the most high-performing parser, there is still room for further improvement in the rst parsing performance, which could potentially enhance the downstream summarization task.Another limitation pertains to the size of the data used for human evaluation.Due to the nature of long document summarization and the length of the original texts , scaling up the evaluation process, such as through crowd-sourcing, becomes difficult.Consequently, we are only able to evaluate a limited number of 10 documents, which may not be fully representative of the entire dataset.Furthermore, another potential risk in our study is the limitation in obtaining an unlimited number of training samples.The data samples investigated are often small subsets of real-world data or may exhibit certain biases, which may not accurately reflect the distribution of real-world data.Although we have verified the effectiveness of our model using highly diverse and heterogeneous datasets from different domains, it is important to note that the model\\u2019s performance on the specific dataset of interest may not be as robust as its performance on unseen real-world data.Finally, both training and evaluating the models require significant computational resources.Despite our attempts to optimize the computation by replacing the original attention calculation with the rst attention tensor , we have not achieved satisfactory results.The high computational costs pose a challenge, as they result in increased human and material resources required for the model.\",\"Our work focuses on domain generalization for abstractive summarization through prefix averaging.However, we do not experiment with larger backbone models due to computational constraints.Based on previous works we expect our approach\\u2019s performance to improve with model size.Also, a larger sequence length for prefix tuning increases the computational costs at inference.Another limitation of our work is that we do not test it on natural language understanding tasks.\",\"The underlying assumption of our method is that abstract reflects the entire article, creating an unbiased summary of the paper.However, abstract does not guarantee an objective representation of the paper, can often emphasize the main findings while discarding details that the authors deem insignificant.This can lead to potential inaccuracies in paper representations, affecting the results of paper retrieval and recommendation.Also, in this work we did not exhaust all possible training settings and evaluation strategies due to limited resources.We perform evaluation using three different standards.While we selected the most relevant evaluation tasks, it would be interesting to assess the quality of representations in other ways, such as citation graph reconstruction, predicting reader activity and other clustering-based evaluations.Additionally, with the emergence of large-scale language models, another interesting direction for future research is to investigate the relationship between model size and final performance.\",\"While many studies show that the architectures of the deep learning models significantly influence the results, we perform experiments with several base architectures because of the constrained hardware.Furthermore, there has not been a vietnamese benchmark summarization dataset, which is both sizable and of high quality.The existing summarization datasets are derived from online magazines, which usually contain misspelled words and grammatical errors.In addition, the reference summaries might not convey the main content of the corresponding articles.Therefore, selecting and developing efficient summarization models for vietnamese still present numerous challenges.\",\"The conditional independence assumptions are a limitation for the applicability of our multiset tagging model.For example, the independence assumptions are too strong to apply it to natural language generation tasks such as summarization.From a technical point of view, the independence assumptions are important to be able to induce the latent assignment of output tokens to multisets efficiently.while our method for predicting permutations is comparatively fast and only has a memory requirement of o, inference on long sequences, e.with more than 100 tokens, remains somewhat slow.regarding the importance of trees for compositional generalization, our model has no explicit structural inductive bias towards trees.However, we do not exclude that the pretrained roberta model that we use as a component implicitly captures trees or tree-like structures to a certain degree.\",\"First of all, our oasum inevitably contains inappropriate summaries not strongly correlated with certain aspects since it is automatically curated.The model trained on it could furthermore hold such misinformation and affect other downstream tasks.But we hope the large-scale training can alleviate such effects to a minimum.At the current stage, we are not responsible for any products directly built on our results.secondly, we only opt for end-to-end extraction, which requires large computational memory and cost that may not be afforded by everyone.Thus, a meaningful direction would be investigating other extract-then-summarize two-step methods for dealing with long document summarization.Besides, our vanilla dataset contains millions of summaries that are difficult for certain researchers with limited computational resources to directly reproduce results on.We recommend using a small subset of our corpus if enough computational capability is not immediately available.Finally, we only explore a simple strategy for controlling the summarization based on input aspects.However, we find it can not always guarantee aspect-focused generation.How to efficiently and accurately generate specific summaries by confining aspects is not only challenging for model design but also difficult for humans to evaluate.\",\"The omission problem is critical in dialogue summarization, but even if this problem is solved, we still cannot guarantee a candidate is appropriate because it might bring hallucination content that is not presented by the source dialogue.Previous works also concluded that factual inconsistency is a critical problem in dialogue summarization, and it is not easy to distinguish.\",\"Candidate summaries dependency while we mainly investigate a training objective to select the best summary among a set of candidates, we find that our model has been dependent on those obtained from the generation model.Recently, several works have been presented to improve language generation.improve decoding methods to generate diverse outputs.It will be beneficial when applying our method to these approaches.One-sentence summary our approach can fail to capture the information from an extremely short summary.Since table 2 shows that our approach has a smaller improvement than cnn\\u002fdm, we plan to investigate that our model aims to capture more detailed features from an input text.\",\"Gumsum is designed to constrain summaries to one sentence for all 12 genres, which raises the question of whether one-sentence summaries are useful for all possible genres or long-document summarization.This is a complex topic that needs in-depth investigation.Moreover, in analyzing human evaluators\\u2019 responses to two open-ended questions , we noticed that virtually all evaluators mentioned that limiting the summary to one-sentence is very difficult and that some genres were easier than others.For example, one evaluator who was given a vlog and a travel guide commented that, \\u201cthe travel guide was much more difficult than the vlog, likely because it was longer and denser.] the travel guide packed a lot more information into its pages and within each sentence.\\u201d this indicates that genre differences at the summary-level is not trivial due to the style of the original text.Additionally, this paper examined a specific subset of pre-trained systems and one version of gpt3\\u2019s pretrained language model , producing findings which may not generalize to other settings.While it is inevitable that more data would lead to different results, we do not believe that system rankings or overall findings would be substantially different, so long as the guidelines and genres examined here remain stable.Finally, we must raise a further limitation involving text type and language: our study encompasses 12 specific written and spoken genres available in the ud english gum corpus, but does not capture findings for other genres, or indeed other languages, which deserve more attention in future studies.\",\"The primary constraints encountered in our research result from our dependence on a single dataset for experimentation and computing resource limitations.Despite these, we postulate that our ranking-based methodology can be utilized for any summarization task that necessitates robust correspondence with a specific structure within the input.To validate this hypothesis, further experimentation is required to assess the generalizability of our technique to alternative datasets and domains.In addition, our limited computational resources prevented us from experimenting with other long document encoder-decoder models such as bigbird and longt5 as well as using higher beam widths during decoding.Furthermore, the cost and complexity of procuring expert evaluators within the legal domain resulted in using automatic metrics alone.\",\"The annotation task we proposed in this work, i., detecting factual errors in summaries and providing human demonstrations and feedback for correcting the identified errors, can be complicated and timeconsuming.During our recruiting phase for mturk annotators, we found that the ratio of annotators who were qualified after finishing the qualification test was relatively low.Therefore, it can be difficult to scale up the annotated dataset given the time and budget limitations.As a result, our dataset is of a relatively small scale and we only used one summarization dataset and one base summarization model.In this work, we view summary factual consistency as an example of user-expected quality to study leveraging natural language feedback for aligning system outputs with user preferences.However, user preferences can be diverse and personal and some user-expected output quality will be less well-defined and objective than summary factual consistency, which further increases the difficulty and ambiguity of data annotation and model evaluation.\",\"The study presented in this paper has three main limitations.While the design of the framework does not prohibit the utilization of longer textual forms, the two case studies presented deal with short texts.When dealing with longer text forms, we need to consider the cognitive load of having experts look at groups of instances.In our ongoing work, we employ strategies such as summarization, highlighting and other visualization techniques to deal with these challenges.In the studies presented, qualitative researchers worked in groups to identify themes.Our goal in comparing two independent groups of researchers was to evaluate the degree of subjectivity by observing if the themes identified by the two groups would diverge.This setup might not always be realistic, as a lot of times qualitative researchers work independently or asynchronously.Finally, we did not include a comprehensive user study to gather input from the experts about their experience with our framework.We consider this to be an important next step and we are actively working in this direction.\",\".5 to generate additional training examples and showed that it helps improve the relevance of the generated summary.However, we did not explicitly analyse the quality of generated examples to check whether or not they are faithful and factually correct which could lead to the same problem in generated summaries.To obtain better training examples, we could use faithful or factuality metrics to assess generated training examples and then use the post-editing method or human evaluation to remove unfaithful content, which we leave for future research.\",\"Since our contrastive framework requires the probability mass of various summaries given a source document, there is an extremely large consumption of gpu memory even if the batch size is small, which limits the scale of contrastive data and suppresses the potential of our method.Meanwhile, due to limited sample points , our bootstrap re-sampling procedure is susceptible to outliers and cannot fully take advantage of this algorithm.In addition, like most abstractive summarization systems, our model does not attach importance to controllable text generation , which means that the generated text might contain redundant and incorrect information.\",\"Our user study tests students on only two short lecture videos which are pre-recorded and carefully edited.Overall, our experiments compare three different conditions.Adding other conditions might have shed light on the relative value of automatic summaries.For instance, if we limit the time available for participants to prepare before taking the quiz, and at the same time track the amount of time spent on summaries and\\u002for videos, then that could give better insights into how students would utilize the two sources differently with limited time constraints.Finally, we could also contrast the usefulness of summaries versus transcripts.\",\"Due to the absence of large evaluation databases, we only evaluated our method on three publicly available datasets that can be used for the msmo task., coin and howto100m datasets, can not be used in our task, since they lack narrations and key-step annotation.So a large evaluation database is highly needed for evaluating the performance of msmo approaches.As the nature of the summarization task, human preference has an inevitable influence on the performance, since the ground-truth labels were provided by human annotators.It\\u2019s somehow difficult to quantitatively specify the quality of the summarization result, and current widely used evaluation metrics may not reflect the performance of the results very well.So we are seeking some new directions to find another idea for quality evaluation.The current setting is short videos short documents, due to the constrain of available data.To extend the current msmo to a more general setting, i., much longer videos or documents, new datasets should be collected.However, this requires huge human effort in annotating and organizing a high-value dataset, which is extremely time-consuming and labor-intensive.Nevertheless, we believe the msmo task is promising and can provide valuable solutions to many real-world problems.So if such a dataset is collected, we believe it could significantly boost the research in this field.\",\"This study is our first exploration of this research topic.We use pre-trained primera, pegasus, and bart-longformer models and fine-tune them for technical abstract and lay summary generation.Novelty in the techniques is the main limitation.We downloaded all pre-trained from hugginface, which may be inappropriate for this summarization task.How to determine an excellent pre-trained model doesn\\u2019t include in this study.In addition, we only fine-tuned all pre-trained models over the task-given datasets without collecting other related summarization data to enhance the model performance.\",\"This paper mainly focuses on the query-focused meeting summarization task.Besides, we have explored the performance of the rankergenerator framework on the long-input summarization task.But the results do not show a significant improvement.Although qmsum dataset is also faced with the long-input challenge, the qfms task only summarizes specific parts of the original text, so it can take these parts as the input.While the goal of the long-input summarization task is to generate an overall summary, which needs to have a global view on the original text.So we think the extract-then-generate framework is unsuitable for the long-input summarization task.The previous work summn is more suitable for the long-input summarization task.In addition, the multi-stage approach has a performance disadvantage over the end-to-end approach.However, the computational complexity of the multi-stage approach is much lower than that of the end-to-end approach.The multi-stage approach can balance experimental performance and computational complexity.So it is worthy of exploration as well as the end-to-end approach.\",\"Our study is limited in scope, studying only english question-answering data.We also acknowledge that the long-form answers we study are not always factually correct, as they can be outdated or incorrect as they are crawled from web forums.Further, our user study is limited in its scale, evaluating 175 instances, and does not carefully study potentially diverging interpretations from annotators of different demographics.We also do not extensively explore all summarization models, such as the extract-and-abstract approaches mentioned in related work.\",\"Considering that english is the most widely spoken language, we select it as the high-resource monolingual language in this study.While across is a general summarization framework not limited to a certain target language, it deserves an in-depth exploration of how across works on other highresource languages.Additionally, we employ mt5 as our backbone because it supports most languages in crosssum.our model is less likely to generate controversial content since the model is trained on a dataset from the bbc news domain.Data in the news domain is often scrutinized before being published, and thus the model is not likely to generate controversial data.we use the amazon mechanical turk crowdsourcing platform to evaluate three artificial indicators.For investigators, all sensitive user data is desensitized by the platform.Therefore, we also do not have access to sensitive user information.\",\"Below, we outline several limitations of our work.our claims are only valid for the datasets accessed in our study.We use the microsoft academic graph and s2orc, which is larger than other publiclyavailable scientific text corpora.However, these sources can differ from other collections of scientific text, because which journal\\u002fvenues, sources, and resource types constitute \\u201cscience\\u201d differs across academic literature search systems and databases.In particular, since a substantial portion of s2orc comes from scrapes of arxiv and pubmed, its coverage of computer science and medicine is better than that of other fields.Also, our coverage is limited to english articles.Past work has shown that citation-based metrics of impact favor articles written in english, and articles from non-englishspeaking countries have different citation patterns compared to others.Finally, we recognize that mag field of study labels are contestable and imperfect.For example, less than twothirds of acl articles are labeled as natural language processing, and the most popular subfield in icml is mathematics rather than machine learning.another limitation of our study is that many scholarly terms are not single words or tokens, but rather phrases.Phrases are somewhat accounted for by measuring words\\u2019 senses, since senses induced by language models reflect words\\u2019 in-context use, including their use in discipline-specific phrases.For example, table 3 shows that title has a sense specific to stereochemistry, and in abstracts, this word often occurs in the phrases title reaction or title compound.Phrases containing distinctive words are also somewhat accounted for by measuring individual words in the phrase.science of science is interdisciplinary and involves a range of organizations and institutions.Not all researchers will have easy access to the computuational resources needed to replicate our study or apply our approach to data of the same scale.The most resource intensive step of our pipeline is when scholarbert predicts each instance of a vocabulary word\\u2019s top 5 substitutes across contemporary s2orc and wikisample.This took approximately 90 gpu hours split across nvidia rtx a6000 and quadro rtx 8000 gpus.Scholarbert itself is a 770m-parameter bert model , and generally our compute infrastructure included machines with 64 to 128 cores and 512 to 1024 gb of ram.2, we define \\u201csuccess\\u201d in two ways, both of which are based on citations.However, though citations are an important currency in science, they are imperfect signals of credit or impact.One article may cite another for reasons that span a range of significance, from brief mentions of related background to core motivation.In addition, associations between jargon use and scientific success may differ as success is redefined using indicators beyond citations.For example, success could be defined beyond scientific communities, such as findings that lead to societal change, products, and use.Finally, our study on the relationship between jargon and success is not causal, but associational and descriptive.\",\"While our work covers a large number of languages, it is focused on a specific source and style of summaries.Our experiments focus exclusively on the xlsum dataset which is based on bbc articles where the opening sentence serves as a summary.It would be interesting to explore our methods on additional datasets and text generation tasks, e., where the summaries are longer, or there are multiple input documents.\",\"The limitation of unisumm can be stated from three perspectives.First, the multi-task pre-training of unisumm can be time and cost consuming, which requires large gpu resources.Second, the current framework uses prefixes of a fixed length for both multi-task training and few-shot prefixtuning.However, different summarization task may prefer different size of prefixes.Third, in this work, we focus on summarization tasks in english.The performance of unisumm for languages that have a different morphology or syntactic structures from english needs further exploration.\",\".We employed human evaluation for assessing factual consistency, and this analysis has been conducted over a larger set of automatic metrics to provide a more comprehensive picture.Furthermore, we demonstrate that further optimizing the model using reinforcement learn- ing with the metric as a reward can result in significant improvements in factual consistency.Our contributions include a simple yet effective approach for two medical summarization tasks, validation of several automatic evaluation metrics for their correlation with expert-assessed factualness, and the identification of the best-correlating metric to guide generation models toward enhanced summary correctness.This work lays the foundation for the development of more robust clinical trial summarization systems, facilitating the efficient dissemination of medical knowledge to practitioners and researchers.\",\"In this paper, we focus on open-ended text generation and demonstrate the effectiveness of contrastive decoding.We would like contrastive decoding to also work well for task-oriented generation settings such as summarization and machine translation.However, the idea of contrasting models across different scales is not directly applicable, because the modes of both amateur lm and expert lm are of high quality.Empirically, having a smaller summaization model as the amateur lm yields lower rouge score than employing a uniform distribution as the amateur lm, which is equivalent to beam search based on log-probabilities.\",\"Though we include 6 systems in our annotation which reflect the current state-of-the-art, all of the models are transformer-based and fine-tuned on just the cochrane dataset, which may limit the diversity of our generated summaries.Additionally, none of the systems are generating summaries that approach the accuracy of human-written summaries.As a consequence, though the summaries in our dataset span the spectrum of quality, they may have less coverage on the higher end of quality.Our analysis of evaluation metrics also assumes the existence of reference summaries.In many real-world summarization scenarios, reference summaries do not exist, and reference-free evaluation metrics are needed for assessment.our notions of summary quality also do not necessarily correspond to clinical utility.As with anything in the medical setting, it is of utmost importance to verify correctness and the quality of evidence before using any generated text to make or guide clinical decisions.\",\"A limitation of this work is that the dataset has not yet been thoroughly vetted by native speakers of the languages contained in the dataset.we hope to do more manual review of lr-sum and other summarization datasets in the near future.\",\"In this paper, we propose the evaluation model umse which can be used to evaluate the summary quality in three typical scenarios.However, in the summarization task, different annotators have different writing styles, and there might exist more than one good summary for one document.Moreover, there can be summaries that concentrate on different aspects of a document.\",\"Retrieval augmentation adds complexity to natural language generation requiring a separate retrieval module before the text generation step can begin.Additionally, retrieval augmentation possibly introduces more input text than the original input which is problematic for many neural network architectures with limited input space especially in the case of summarizing entire scientific papers.Finally, retrieval augmentation itself could introduce factual or relevancy errors if the retrieved documents are irrelevant or incorrect and they end up being used in the generated summary.\",\".Moreover, our exploration of the automatic factuality evaluation in cross-lingual settings illustrates its challenging nature.Limitations the scenarios we studied are limited to chinese to english and english to chinese.For other languages, the factual characteristics may be different.The genre of the source documents we study is news or blog post.For other genres, such as dialogue, our\",\"As mentioned in the main paper, one of the limitations of our centrum model is that it tends to produce longer outputs in comparison to primera.This necessitates controlling the length of the summary by truncating to a desired length.This constraint significantly reduces the utilizable corpus size, leading us to work with roughly 45% of the corpus size used by primera.\",\"We propose a set prediction network for the extractive summarization task, which has worked well on some datasets but still has some limitations.Firstly, due to the use of pre-train bert in the document encoder, our method is inadequate for long text summarization tasks.In general, the text length of a long document is much longer, so the model needs to be more capable to capture the dependency.Next, we will extend the method to long document summarization tasks.Secondly, the queries in the decoder are initialized with a normal distribution.\",\"Newsela dataset one limitation to this study is our use of the newsela dataset.Because this requires a license to access, researchers cannot fully reproduce our work without first obtaining permission from newsela inc.Unfortunately there is currently no other large dataset offering high quality aligned documents for simplification under an open source license.The only other datasets so far used for document-level simplification are based on wikilarge, which has very poor and inconsistent alignments at the document-level.Paragraph-level human evaluation in order to reduce complexity, our human evaluation was performed on paragraphs rather than full documents.As a result, there is a potential limit to the accuracy of human judgements when certain discourse phenomena are present.For example, important information may be excluded from a specific output paragraph , but this could actually be present in a different part of the true simplified document.Monolinguality this study focused entirely on simplification for english-language documents.Reproducing the proposed systems for use on other languages would require dedicated datasets of similar scale, along with sentence\\u002fparagraph alignments and operation labels.Further, the nature of simplification in other languages may differ quite a lot from english with respect to the types of operations that are performed, potentially reducing the suitability of the proposed framework.Generalised target audience we approach this study with our definition of \\\"simplification\\\" being based on that of a generalised audience, following the standard set out by the assigned reading-levels of the newsela dataset.Existing works often outline the intent for their systems to be used to simultaneously assist a wide array of different target users, such as those with cognitive impairments, non-native speakers, and children.However, they rarely go into any detail about which simplification strategies work for each of these different groups or perform human evaluation with annotators from the same target demographics.As such, we acknowledge that using our systems for a specific demographic might prove insufficient to enable their consumption of media without first making further revisions to support their precise needs.\",\"Even though we performed a rigorous literature search to try to cover all existing work on scientific fact-checking, there is possibly work that was left uncovered due to different keywords, naming conventions.Whenever possible, we tried covering all related work and all relevant cited papers.All approaches for automated scientific factchecking described in this work are still not safe for widespread adoption in practice due to constraints to their performance.Having deployed automated fact-checking systems that would produce incorrect verdicts could lead to mistrust in their usefulness and the process of fact-checking itself, including the work of dedicated manual fact-checkers.\",\"Downstream tasks in this work, we focused on long-document summarization as we believe it is the task where controllable summarization is most needed.to handle long document input we could not use the bart model with socratic pretraining adaptation directly.Instead, we applied the segenc architecture on top of bart.This adaptation of the pretrained model may have dampened some of the few-shot performance of socratic pretraining.We thus believe that tasks with shorter input documents for which the segenc architecture is not necessary would see even greater benefits in the low-resource setting.Base model throughout this work, we restricted our analysis to one model architecture the segenc architecture with the bart base model.Previous work extensively studied the impact of different architectures for long-document query-focused summarization.These primarily differ in how they model long documents.The authors found segenc, a simple sliding window adaptation of bart, to perform best on qmsum.While the results presented here are specific to segenc and bart, our approach is agnostic to the underlying model architecture and is orthogonal to longdocument modeling.Evaluation metrics as discussed in prior work , there are limitations with the current automated evaluation metrics which do not strongly correlate with human judgments.Our results from these metrics should therefore be interpreted with caution and in combination with the human evaluation we performed to support them.One area in which automated metrics have been reported to perform poorly is factuality.Moreover, current factuality metrics have been designed and tested in the news domain and their performance in the out-of-domain setting was not systematically evaluated and is hard to interpret.In this work, we therefore choose not to report any factuality metric results.Qg efficiency we did not optimize the efficiency of the qg component of socratic pretraining and, consequently, it is computationally expensive.Currently, given equal amounts of resources for qg and pretraining, it takes us about the same time to perform the qg phase and pretraining phase on the same amount of data.We note, however, that in low-resource scenarios, the additional compute can lead to significant benefits, as shown in our results.In addition, we did not experiment with efficient sampling strategies, and believe that improving the efficiency of the qg model inference, for example through model distillation , could lead to significant efficiency gains.Dataset biases the datasets for pretraining and finetuning used in this work are in english and thus mainly represent the culture of the englishspeaking populace.Political or gender biases may also exist in the dataset, and models trained on these datasets may propagate these biases.Additionally, the pretrained bart model carries biases from the data it was pretrained on.We did not stress test these models for biases and request that the users be aware of these potential issues in applying the models presented.Misuse potential and failure mode when properly used, the summarization models described in this paper can be time-saving.However, the current model outputs may be factually inconsistent with the input documents, and in such a case could contribute to misinformation on the internet.This issue is present among all current abstractive summarization models and is an area of active research.\",\"In this paper, we conducted an experiment on code summarization using two benchmark datasets, the java dataset and the python dataset.Blocsum may need to be tested for its generalizability to other program languages.We chose two program languages that were easily parsed to map the block position of code and ast.We believe that since other programming languages have similar syntactic structures, blocsum should be able to achieve similar performance on them as well.\",\"While our approach shows promising results in both automatic and human evaluation, it relies on two significant pillars: a strong entailment model and a strong initial summarization model.The nli model implicitly encodes the biases and other data regularities that were part of the nli training set into the generated summaries of our policy.This is well demonstrated by the gap between human attribution judgements and the automatic nli metric.Our rl policies cannot improve on factual consistency errors if they are undetectable by the nli reward.Hopefully, as nli capabilities get better, so will the efficacy of rlef and the abilities to automatically flag hallucinations and contradictions.Secondly, a strong summarization model is essential for our method in two ways: as an initialized starting point for rl exploration and as an anchor point to a policy.While our rl training does not require any reference data and opens the possibility to use more un-summarized documents, it would probably not succeed as well without initializing from a high-quality supervised model.Another limitation is that our experiments suggest that model size is important when using rlef : both our summarization and nli models are 11b parameters models.We believe it is important to further understand how to make our approach more robust to smaller models, to increase its computational efficiency and availability.\",\"In this paper, we introduce z-code++, a robust pre-trained model tailored for summarization tasks.However, it should be noted that there are certain limitations to our model.Firstly, the model is not versatile enough as it is specifically designed for summarization.It is unclear whether it performs well on other natural language tasks.Secondly, while fie can handle document summarization, there are still significant potential for improving cost efficiency.Lastly, the evaluation of multilingual summarization is not thorough enough due to the limitations of available datasets.\",\"By retrieving pseudo captions from summaries, one limitation is that the most relevant sentence for a specific image may not be in the summary.However, it has a trivial impact on the overall msmo performance.If this happens, most of the time, the image will not be the salient image to select, and its caption will provide no helpful information for the text summary.In this situation, selecting a pseudo caption from summary sentences will not hinder the overall performance, though it may not be the best for the specific image.Besides, even though our task setting strictly follows three previous works , another possible limitation is that only one msmo benchmark is used.We believe providing more diversified datasets and investigating more about the rationale under the task setting are critical to pushing forward the multimodal summarization community, although they are out of the scope of this work.\",\"We discuss the limitations of our framework as follows: in this paper, we take an initial step on the robustness of the summarization system by focusing on word-level perturbations in the input document.However, in practice, the robustness of the summarization models is reflected in many other aspects.For example, the summarization performance towards sentence-level or document-level perturbations is also a kind of robustness.Although dasum greatly improves the generation quality compared with other augmentationbased models, it requires more computational resources with respect to the augmented dataset construction process.For large-scale datasets with long text ), it is worth considering the time complexity of transformer architecture.\",\"Some of the summarization datasets annotated for faithfulness are relatively small, which makes score estimates uncertain.Furthermore, many datasets contain only output from a limited number of generation systems, which makes it hard to properly account for potential biases towards certain generation systems that may confound scores ).These concerns are, however, alleviated to some extent since we study trends across many independently created datasets, which makes it less likely for a single bias to persist in all of them.Furthermore the availability of generation and thus annotated faithfulness data limits our experiments to english.Finally, it remains unclear whether our results would still provide advantages when applied to larger models such as t5-11b, whose parameter count makes experimentation infeasible on the hardware available to us.\",\"In this work, we collect extensive and comprehensive human feedback with high qualities to facilitate our human-in-the-loop conversation summarization framework.While the learned rewards and models are showing good generalization abilities, further attention is still needed to deeply understand what types of feedback or what amount of feedback is necessary.Our current work only considers human feedback collected using the required forms.Furthermore, we mainly focus on conversation summarization with human feedback in this work, and other types of summarization tasks could be further explored to incorporate human knowledge.\",\"While the proposed method performs well on four benchmarks, we discuss some of its limitations.On one hand, as discussed in \\u00a75.5, our method is not accurate enough to predict sentences at the end of the documents.There may be some better strategies to construct training samples so that the model can better take into account each part of the documents and make more accurate predictions.On the other hand, our model is not pre-trained with more diverse domains and larger scale data.Our datasets are limited to two types, i., paper abstracts and short stories, both of which have comparatively obvious order characteristics.In addition, we do not use some larger scale datasets, such as nsf abstracts and arxiv abstracts, because of computation and time constraints.With more diverse and larger data, the performance of our model should be further improved.\",\"Our proposed summarization model is pretrained exclusively on news datasets, however, our experiments and analysis were conducted on biographical narratives.We only studied english summarization and our processes and in particular relevance findings are likely not entirely applicable to long multi-lingual documents.Moreover, single-domain trained models may propagate inductive biases rooted in the data they were pretrained on.This was evidenced in finetuning on our target dataset as the model demonstrated a moderate degree of transferability in adapting the newswire domain to our biographical discourse genre.Our work studies generated summaries for long narrative text.While our taxonomy appears generalizable to other domains, investigating summarization quality of large-scale datasets, such as scientific articles, patent documents, government reports or meeting discourses was confined to the scope of baseline performance comparison.\",\"We present a new dataset for meeting summarization that has the potential to improve the efficiency and effectiveness of meetings.However, we note that the dataset is limited to city council meetings from u.cities over the past decade and licensing issues have restricted our ability to include certain city council meetings in the dataset.For example, we contacted the city council of san francisco and were informed that they do not allow the redistribution of meeting minutes.Moreover, our dataset does not include non-verbal cues such as eye gazes, gestures and facial expressions, which may make it less suitable for developing summarization systems that rely on these cues.Despite these limitations, we believe that the dataset is of high quality and will be a valuable resource for the development of meeting summarization systems.\",\"Of existing book summarization resources, such as booksum.Indeed, previous datasets for full-text book summarization are, i) limited in size, and, ii) monolingual, i.in addition, we leveraged echoes to bring to light the unsatisfying capabilities of current approaches to generalize to book summarization.Finally, to mitigate this issue, we proposed a new extractivethen-abstractive baseline for book summarization, which outperforms its purely-abstractive counterpart on echo-wiki and echo-xsum, achieving results on the standard booksum test set that are comparable with the current state of the art while using a number of parameters that is only 0.limitations despite the multilinguality of our resource, there is still a strong bias towards the english language, as the majority of books are in english and many translations are from english.This may result in the values of english literature being reflected, and these may differ from those of other cultures; summarizing literature from different cultures and regions may not be fully accurate, as every region has had its own historical development.Language models used in the experiments can inherit biases from the training data and the tools, such as the ones used for preprocessing, and have limitations that have not been fully evaluated and could impact the results of this study.This study includes the use of web data, which \\u2013 while marked as public domain \\u2013 may be subject to copyright laws.The data used in this study was collected for research purposes and was not intended for any other use.Additionally, it is worth noting that the majority of books used in our resource are copyright-free, and therefore, old.While this allowed us to include a large number of texts in our dataset, it also means that our resource may not fully capture contemporary literature and may not be representative of current linguistic trends and cultural values.\",\"Though our systems achieve promising results in solving the summarization task for long documents, we believe that we can gain more improvement with the following further considerations.The current explicit key information selection strategy is somehow heuristics.We can alternatively try extractive summarization methods.Also, this lay summarization is interesting which helps nonexpert readers can understand scientific articles.However, specific strategies focusing on this aspect such as using non-expert vocabulary, or mapping to general knowledge, are yet applied.Some minor parameters such as the sequence lengths , or tuning the sota brio model also need to be investigated more deeply.\",\"\\u2022 we are unable to test the proposed model\\u2019s performance on other datasets due to the unavailability of public multi-reference headline generation datasets.\\u2022 our dataset is created over a period of 6 months and contains around 3000 examples.Although there are several commonly used benchmark datasets with a similar number of examples: e., r4c reading comprehension dataset , fire-lid , iiithner datasets in gluecos benchmark , wnli , rte and mrpc datasets in glue benchmark , nope corpus , we believe that it will be better to have a larger dataset for this challenging task.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"7_summarization_summaries_summary\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"7_summarization_summaries_summary\"],\"textfont\":{\"size\":12},\"x\":[10.46885871887207,10.417335510253906,9.801658630371094,10.504593849182129,11.025761604309082,10.67697811126709,10.63868236541748,10.55656909942627,11.050668716430664,10.38466739654541,10.419771194458008,10.527435302734375,9.9822998046875,10.487809181213379,11.463507652282715,10.417495727539062,10.893223762512207,10.527936935424805,10.248513221740723,10.463409423828125,9.78425121307373,10.218131065368652,10.273455619812012,10.452314376831055,9.991952896118164,10.171281814575195,10.429539680480957,10.381532669067383,10.382804870605469,10.451255798339844,9.703696250915527,10.452698707580566,10.563027381896973,10.054858207702637,10.54006290435791,9.739104270935059,10.417299270629883,10.090603828430176,10.871346473693848,11.186309814453125,10.432827949523926,10.490466117858887,10.773506164550781,11.049448013305664,10.517618179321289,10.411752700805664,10.040364265441895,10.459213256835938,13.886174201965332,10.444337844848633,10.474603652954102,10.167675018310547,10.518391609191895,10.471736907958984,10.290996551513672,10.451930046081543,10.540450096130371,10.635761260986328,10.502930641174316],\"y\":[1.5483983755111694,1.574286937713623,2.0428271293640137,1.6300437450408936,1.0924763679504395,1.4956433773040771,1.9527541399002075,1.555073857307434,1.008413314819336,1.6000016927719116,1.5501397848129272,1.638960838317871,2.1314897537231445,1.6307250261306763,1.7099614143371582,1.6005139350891113,0.9336033463478088,1.5558160543441772,1.64090895652771,1.6608587503433228,2.4892735481262207,1.6811091899871826,1.6087567806243896,1.5615038871765137,1.4788529872894287,1.7149635553359985,1.5489226579666138,1.6497856378555298,0.13952073454856873,1.5861998796463013,1.9647458791732788,1.5432921648025513,1.627739667892456,1.714582920074463,1.4409414529800415,1.8705264329910278,1.588223934173584,1.8075412511825562,1.8841729164123535,1.1102265119552612,1.4960795640945435,1.6475757360458374,1.4638429880142212,0.906802773475647,1.6220431327819824,1.5191891193389893,1.9472516775131226,1.5821754932403564,4.8460283279418945,1.58230721950531,1.601049780845642,1.389208436012268,1.8752634525299072,1.6162720918655396,1.6667171716690063,1.5765962600708008,1.757663369178772,1.5596264600753784,1.6417148113250732],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Despite the promising results obtained in our model, there are still several areas for improvement.Firstly, when dealing with a large corpus, the online retrieval function becomes challenging as it requires a significant amount of computational resources and time.Additionally, creating a vectorized corpus dynamically every time becomes difficult.Secondly, the process of collecting a large number of reviews from users raises privacy concerns.The collection of data, especially from private and non-public sources, may pose difficulties.\",\"Our methodology provides a novel approach to predicting emotional reactions to social posts.While our proposed methodology has shown promising results, further research is necessary to address some limitations and expand the scope of our findings.Firstly, our model\\u2019s reliance on only textual content and metadata may limit prediction accuracy, as other factors such as user demographics or multimedia content may also impact emotional reactions.secondly, this study focuses on predicting emotional reactions to posts, rather than the reasons behind them.While our method provides textual explanations, they may not fully capture the complexity of user emotional responses.Lastly, our approach was evaluated on meta posts, but may not generalize well to other social networks or domains.Further evaluations on different datasets are needed to confirm the applicability of our methodology across different contexts.\",\"It should be noted that, as our model and the baseline models in this study were trained using texts from social media and the experiments were conducted on online text, the results may not accurately reflect the performance in a clinical setting.A proper diagnosis by clinical experts necessitates a comprehensive analysis of various factors, including the number of manifested symptoms, the onset and history of symptoms, developmental background, lifestyle, and recent life changes, in order to gain a comprehensive understanding of the patient\\u2019s condition.However, it is still challenging to capture detailed information such as personal secrets through online text, as these texts are often composed of fragments of daily life, episodic experiences, and emotive expressions rather than providing a comprehensive view of an individual\\u2019s life.Despite the domain-specific limitations imposed by the fragmentary text, we hope that our model may still serve as a valuable aid for clinical experts in their decision-making process.Furthermore, future research should aim to move beyond predicting psychological symptoms and disorders solely based on linguistic styles and expressions, and instead seek to uncover the underlying features that contribute to these expressions as our model does.\",\"We see two main limitations of this work.The first one concerns the diversity of the language models and datasets used.Bert, distilbert and roberta have similar architecture, and sst2, imdb and rotten tomatoes are datasets designed to evaluate the sentiment of english text.It would therefore be interesting to validate the robustness of our results on more diverse languages, tasks and language models.In this short paper, we decided for brevity to follow the experiment design of sanyal and ren , while being aware of its inherent limitations.The second limitation of this work concerns the time complexity of sig.As it needs to compute explanations for each word individually, this method can become very computationally expensive when applied on large text data.To alleviate this issue, we first made it possible to compute gradients in parallel, using an internal batch size similar to how captum implemented the integrated gradients method.2, it is possible to reduce the number of interpolated points, which makes the computation faster while retaining better performance than the original ig.In this work, we ran our experiments on a machine with 16 cpus, and one nvidia tesla t4 gpu.With this setting, computing sig on sst2 and rotten tomatoes takes around one hour for each model.On the larger imdb, computing sig, on 2000 randomly sampled inputs, takes around 5 days for bert and roberta, and 2 days for distilbert.\",\".Wer has the main advantage of being simple and consistent.Unlike sentiment or text embeddings, there are not multiple models.The main limitation of wer is that, because it is not based on any understanding or model of the language, there are severe errors that have a relatively low wer, and vice versa, there are non-severe errors that have a high wer such as a multivitamin vs.Sentiment has strong limitations due to the fact that these algorithms are designed to only measure how positive or negative a text is.Sentiment proved to be sensitive to misses in disfluencies like um or uh.This is highlighted in the example, uhm it started last night vs and it started last night, where there was a strong difference in sentiment of 1.This can be an advantage or a limitation depending on the scenario.Many asr systems overlook disfluencies, but, for example in human robot interactions, spoken dialogue systems, or in the prediction of dementia status, disfluencies can be vital to understanding and performance.There is the also a limitation on the accuracy of the model.In the examples any previous surgeries vs.any previous surgery or uh i smoke about a pack a day vs.uh smoke about a pack of day, there is a high difference in sentiment yet the only difference is in missing the pronoun i or the plural of surgery, which should not affect sentiment greatly.Despite these limitations, sentiment is able to catch some severe errors where the wer is relatively low.In the example where crystal meth becomes crystal mud or where chest pain becoming chatting the wer is 0.333 respectively, but the difference in sentiment is very high at 1.Text embeddings are limited by the performance of the model, like sentiment, yet capture more than just polarity of a given text.Knowing that many of these models are trained in a self-supervised manor using the context in the training text, we can see how the embeddings in the example of my parents and our friends would be similar.Both of these phrases could occur in with similar surrounding text; they have the same grammatical structure and parents and friends are both human relationships.Another limitation on these models is the amount of text they can handle.Anything above the model\\u2019s limit gets truncated, and consequently, loses the meaning of truncated text.Although utterances are commonly short in asr training data, the character limitation on these models could affect performance on longer utterances.Despite these limitations, text embeddings were able to capture well the differences in meaning.Text embeddings were able to give a high score to the examples where crystal meth becomes for sunlight and where chest pain becomes testing when wer and sentiment scores were relatively low.Text embeddings were also able to give low ratings for different writings of the word okay and numbers when wer were high.\",\".First, we have relied on evaluation against self-reported assessment of anxiety.Selfreporting the degree of anxiety on a survey instrument is not entirely dependable in diagnostic accuracy.However, it has shown reliable associations with diagnoses, serving clinical assessment treatment purposes beyond diagnosis.For example, anxiety scores from selfreported surveys have been robustly associated with consequential real-world outcomes such as mortality.Clinical evaluation of the assessments proposed in this work should be evaluated against clinical outcomes.Furthermore, the sample may not fully reflect the language use of the general population as it is skewed towards young and female4 and only focused on english spoken by those from the u., although previous work suggests this dataset contains a diverse representation of socioeconomic status.Additionally, we do not focus on actual utilization of discourse relations in assessing anxiety, as the scope of this work limits us to showing the viability of modeling anxiety on a continuous scale and the importance of discourse information towards modeling it.Lastly, the strong associations of theoretical discourse relations come from models that themselves are not perfect, with f1 scores ranging from 0.868 for causal explanations, though one might expect this error to lead to underestimates of correlation with anxiety.With nlp increasingly working towards better human-focused applications , we are presented with increasing considerations for human privacy as a trade-off with considerations for open data sharing.In this case, the data used was shared with consent only for academic research use.Open sharing of such data violates trust with research participants.While it would be ideal to 4the self-reported user age averaged 22.6 , and over half marked their gender as female.Release everything and preserve privacy, in this situation, we believe the fact that the unprecedented data is not universally available suggests an imperative for those with access to openly share our work as best possible within ethical guidelines.We are thus releasing aggregated anonymized features from the secondary evaluation dataset that allows one to qualitatively replicate the associations in our results while preserving the privacy of participants.\",\"And should acknowledge and\\u002for try to mitigate them to the extent possible.Our work strictly follows the task definition and evaluation protocols of the original esc paper , where the support is provided through social interactions rather than professional counseling.As mentioned in , further efforts are still needed to probe the ethical extent to which dialogue models can or should provide support.These protocols should also not be used directly in fields other than the esc task that require the guidance of professional researchers, such as psychological counseling.We also ethically conducted the human evaluation.We transparently communicated with the participants of our study intent and explicitly informed them of the disclaimers before they participated.We paid the participants at the hourly wage above $10\\u002fhour, going well beyond the local labor compensation standard.We acknowledge that the results of human evaluation could be affected by the participants\\u2019 demographic and geographic characteristics.This work has obtained study approval from the institutional review board.\",\"In the design of this study offer directions for further work to improve performance.Since the redditmh dataset uses posts from general mental health subreddits, the binary-choice qa model was not exposed to finer-grained data from illness-specific subreddits, which could lead to better assessments.Also, the multiple-choice qa model forces an answer from the list of five diseases even if there are no distinct language markers of any disease.two-stage qa models will serve as effective screening tools to ultimately provide care to individuals who need it the most.\",\"This study aims to detect signs of anorexia, selfharm, and depression in users of social media environments through a double-domain adaptation of a language model.This study presents some limitations, mainly because these datasets are observational studies and we do not have access to the personal and medical information that is often considered in risk assessment studies.For example, we cannot discard that some users who publicly expressed that they have been diagnosed with anorexia are actually non-anorexia cases.However, the identification of positive users from selfexpressions of diagnosis is a common practice in this area , and the test collections built in this way are regarded as solid experimental benchmarks.There are also some limitations given by the nature of the data, as the users in these datasets might differ from users at risk who do not have exposure to social media.\",\"While our proposed confede method has shown promising results in multimodal sentiment analysis, there are some limitations to consider.Firstly, our method is designed for multimodal sentiment analysis that includes three modalities: vision, audio, and text.The performance of the model when one of these modalities is missing is not considered.Additionally, as the number of training samples increases, our custom-designed sampling method may require more processing time.However, the similarity calculation can be pre-processed between the unimodal training stage and the multimodal training stage.Therefore, it may not consume a significant amount of time.\",\"Although our proposed method achieves the stateof-art performance, it still has a few limitations.Firstly, we only consider the dependency between aspect and opinion in the target text yet ignoring the order influence in the input text, which may bring more improvements.Secondly, there are three label types for aste, including aspect, opinion, and sentiment.Currently, we only utilize the aspect and opinion markers in the marker-oriented sequence labeling module.We believe that the specific design for the sentiment marker can further improve the performance, which can be a future direction.\",\"We note that there are several limitations with such a sentiment knowledge enhanced self-supervised learning approach.First, the preprocessing of massive videos is time-consuming and laborious.Second, the pre-training of our model has relatively large requirements on the gpu resources.Finally, we argue that there should not be too many videos without sentimental words, so as to avoid the model having a large bias and not learning any sentiment knowledge.\",\"We present a novel pair extraction task for understanding the stance of interesting topics in the wild.There are two potential limitations to our work.First, the mapping module requires a predefined list of targets.Without the predefined list of targets, it is very difficult to understand the correctness of stance labels for the predicted targets in the absence of gold labels.On the other hand, the predefined list of targets makes the entire system end-to-end and automatically evaluable.Second, the process of mapping might become too slow if the number of targets of interest grows bigger.However, the primary contribution of the work is not to present a fully robust pipeline model but to present a novel, interesting, and challenging task to the community working in stance detection.\",\"The method introduced in this paper applies to a specific type of sentiment analysis task, where the item to be analysed is a review, the author of the review and the product\\u002fservice being reviewed are known and uniquely identified, and the author and product information is available for all reviews in the training set.While our approach is expected to perform well on other languages beyond english, the experimental results do not necessarily support that since our evaluation is only carried out on english data.\",\"In prior research , several challenges have been identified in this field, such as noisy or low-quality data, semantic ambiguity, absence of standards, social desirability bias, and the requirement for human intervention.Our study aimed to tackle the challenge of detecting pain points and devised various strategies for managing noisy real-world reviews.Nonetheless, to fully unlock the potential of the painsight, additional research is necessary to explore the wide range of emotional polarities beyond the generic \\u2018negative\\u2019 sentiment.Furthermore, customer reviews often show mixed sentiments, which calls for addressing semantic ambiguity.Lastly, the performance of painsight assessment was constrained to five product categories, highlighting the need for a comprehensive, high-quality benchmark encompassing diverse domains and performance evaluations across distinct categories.\",\"There is no dataset currently available specific for fine-grained emotional paraphrasing.For our study, we have to utilize publicly available paraphrase datasets, google paws, mrpc, and quora and augment their text pairs with emotions labels.These datasets may not be best suited for study- ing this new task.Therefore, new datasets that are particularly developed for fine-grained emotional paraphrasing are needed.Furthermore, it is also desirable to evaluate the proposed methods in alternative application scenarios other than lowering sentiment intensity.When using goemotions as our fine-grained emotion classifier, we selected the emotion with the dominant confidence score above the threshold of 0.as the authors of goemotions have pointed out, there is still much room to improve on the classification accuracy of goemotions.Although the confidence score threshold of 0.5 worked well in our experiments, how to set this threshold still requires more studies.Similarly we utilized nltk\\u2019s vader scores to place emotions into high, low, and neutral intensity groups.The vader score thresholds for this grouping were selected empirically.Further studies are needed for setting the thresholds or developing better ways for intensity grouping.In the evaluation of our fine-grained emotional paraphrasing models, we utilized two sets of metrics for emotion transition and paraphrasing respectively.\",\"Firstly, from the technical perspective, we have advocated the advantages of our proposed listwise loss for the mrhp task in terms of generalization capacity.Nevertheless, there are other various listwise discrimination functions that may prove beneficial for the mrhp model training, for example neuralndcg , listmle , etc.Moreover, despite the novelty of our proposed gradient-boosted tree in partitioning product reviews into helpful and unhelpful groups, our method does not employ prior contrastive representation learning, whose objective is also to segregate helpful and unhelpful input reviews.The contrastive technique might discriminate reviews of distinctive helpfulness features to bring further performance gain to multimodal review helpfulness prediction.At the moment, we leave the exploration of different listwise discrimination functions and contrastive learning as our prospective future research direction.Secondly, our study can be extended to other problems which involve ranking operations.For instance, in recommendation, there is a need to rank the items according to their appropriateness to present to the customers in a rational order.Our gradient-boosted decision tree could divide items into corresponding partitions in order for us to recommend products to the customer from the highly appropriate partition to the less appropriate one.\",\"We propose tokencluster, a sentence extraction algorithm that can automatically identify aspects in user reviews and leverage that for performing extractive summarization.Being a data-driven approach, it is susceptible to noisy data.Therefore, a limitation of tokencluster is that the clusters of aspect-related words can be noisy and imbalanced if data is noisy.This can possibly be achieved using external data.Another limitation is that tokencluster is computationally more expensive compared to more simple sentence extraction approaches.This is because tokencluster clusters aspect-related words during inference, which can be restrictive when the review set is large.\",\"Limitation although experiments on two public datasets show the effectiveness of our proposed method compared with other state-of-the-art methods, we notice that our proposed model fails to distinguish similar emotions effectively going through the prediction results, as frustrated and anger, happy and excited ).Moreover, our proposed model tends to misclassify samples of other emotions to neutral on meld due to the majority proportion of neutral samples in these datasets.\",\".6, we found that the choice of different templates and the order of generating content will both lead to performance variation.It is worthwhile to conduct a detailed investigation on this interesting problem, however, due to the limit of pages, we only experimented with limited alternative templates.Second, our proposed aqe task shares some similarities with some tasks in other domains, which means that it is possible to adapt our proposed framework to other tasks, such as relation extraction and sentiment analysis.We will leave this for future research and demonstrate its effectiveness in other domains.Last, subject to both the economic and time cost of dataset annotation, we only expand one existing dataset for our proposed aqe task.\",\"For modeling simplicity, we adopt the classic lda methods to get the topic id for each video segment.We plan to investigate more advanced topic clustering methods and check how it can be applied to multilingual cases.Also, we propose a twostage framework that first extract topic and style features, based on which the emotion classifier will be trained.\",\"Our work is the first study of generative asqp task from the view of what not to generate.firstly, implicit information is still challenging for uaul.6 demonstrate that tough cases require in-depth semantic understanding.Though uaul achieves wide improvements in the generation paradigm, it struggles to deal with implicit cases.Secondly, in this work, we only design tokenlevel marginalized unlikelihood learning.Since aspect sentiment quadruplets contain four types of information, considering span-level and whole sequence-level negative sample learning may attain further gains.Thirdly, uaul increases the training time, as shown in table 8.we optimize the implementation by parallel computation.Meanwhile, mc dropout is only adopted in the last dropout layer.The training time is still significantly enlarged.Nevertheless, our method does not require additional human labor, which has obvious advantages in real applications.\",\"We analyze the limitations of this study from the following perspectives: \\u2022 the aste task extracts the sentiment triplets from a review, while the aspect sentiment quad prediction task adds an aspect category based on the triplets and provides more comprehensive information.Defining the aspect category for each domain is also hard work.\\u2022 all the models are evaluated by f1 score, in which only exact matching can be considered correct.This metric can not differentiate between partially matching and completely mismatching and is not the best choice for a challenging dataset like dmaste.\\u2022 there is no specifically designed method for cross-domain aste.But we analyze the challenges of this task in detail.We are planning to design a new method for cross-domain aste based on the analysis results.\",\"Although the proposed framework yields promising results on two fine-grained emotion datasets\\u2014 goemotions and empathetic dialogues\\u2014there remain limitations, including: to the best of our knowledge, there is no such fine-grained emotion dataset in other languages.Although theoretically, our method should work fine on languages other than english, we can only show the results in english.The proposed method works best when the label structure contains hierarchy, especially when the semantics of some labels are close and difficult to distinguish.When the label structure is flat and independent, our method may backoff to a conventional classification model.\",\"Key point hierarchies may be valuable for summarizing opinions and views in multiple domains, including reviews, survey responses, customer feedback, political debates etc.also, we only attempted to create kphs for english reviews, for which an abundance of resources is available, including a huge number of written reviews and high-quality trained models, e.applying these methods to low-resource languages is expected to be far more challenging.Finally, the quality of the resulting kphs depends on the quality of the extracted key points provided as input, which may vary across different domains.To alleviate this problem in thinkp, we manually filtered out problematic key points from the dataset.\",\"First, our method needs to check all spans in the given sentence and build a table for each sentiment polarity, and is therefore difficult to handle too long sentences.Another limitation of our work is that for the different aspects in the same sentence, we need to rebuild the tables.\",\".3, our model achieves relatively slight performance improvement on sentiment analysis task.For reasons, our model may be dependent on the scale of datasets to learn noise and redundancy patterns in video, which needs to be further improved and studied.\",\"This paper aims to make advancements toward automatically identifying fine-grained depressive symptoms from memes shared on social media.Although we used only those memes shared by users who self-declared themselves as depressive, we did not conduct any further clinical assessment to judge whether the user was depressive or not, nor we clinically evaluated their depression severity.Therefore, deploying this system without expert advice could compromise patient safety and lead to undesirable outcomes.We further acknowledge that determining the depression symptoms based on the visual and textual cues present in the meme can be subjective, and therefore the created gold-standard dataset may contain explicit and demographic biases.In this study, we focused on training the models using only the social media data, leaving their performance unchecked if tested on other medical data sources.Finally, our study is not indented to provide any diagnosis; instead, we envision the methods we provide being used as aids by healthcare professionals.\",\"There are three points to discuss and they may inspire further investigation.First, since the length of empathetic conversations in the current benchmark dataset empatheticdialogues is relatively short, the theory of selfother awareness could be explored under the circumstance of long conversations to maintain the self-awareness of chatbots for the long run.Second, for the better comprehension of self-other awareness, it is helpful to introduce more commonsense knowledge of higher quality.Finally, current automatic evaluation metrics are still not rational and proper to measure the ability of empathy.It is desirable to build better evaluation metrics for empathetic responses.\",\"Of the dataset: the handling of example complexity, the coverage of the dataset in terms of domains and entity types, the number of included languages, and the quality of pretrained language models.6 shows that the number of entities per example has an important influence on results.For now, the handling of examples with more than one entity includes the detection of the mention, but does not consider other criteria.It would be useful to adapt the tsc methods in order to determine whether the sentiment about all entities is expressed by the same part of the example or not.If sentiment is expressed in different parts of the example, a splitting of the example into parts which express the sentiment about each entity would prove useful.Despite a more diversified coverage of the political domain compared to newsmtsc, madtsc remains focused on politics.It would be interesting to include other major news domains, such as environment, business, culture, technology, sports, etc.Equally important, all targets from mad-tsc are person names.It would be useful to also include sentiment expressed about other types of named entities , as well as other polarization-prone concepts in each domain.Such extensions of the dataset would provide a more complete view of tsc performance.Ultimately, they would make the analysis of sentiments expressed in news articles more comprehensive and reliable.While mad-tsc is the first multilingual aligned dataset designed for tsc in news, it would benefit from the inclusion of more languages, including non-european ones.This limitation is due to the unavailability of massively multilingual and aligned news datasets which could be used to include more languages.A potential solution to overcome these limitations would be to enrich the dataset with manual translations in other languages.finally, the comparison of results between languages is biased because the effectiveness of available pretrained language models is variable.This is a limitation which is shared by most nlp works which focus on multilingual datasets and reuse pretrained models, themselves trained on whatever datasets available for each language.\",\"In this paper, we only explore binary sentiment classification, as it is enables cross-lingual experiments to be somewhat comparable.However, this is a simplified task, which should be taken into account when interpreting the results.Our multilingual datasets also come from various domains and, although we try to control for this in english, this does lead to some effect in the results.Finally, for emotion detection, we only experiment in english.We also chose only a few representative methods for each approach.This was a necessary simplification given the large number of available models, and care was given to choose truly representative methods for each approach.However, some relevant methods may not be represented here.Finally, we only report the results for a single run for the supervised models, rather than the average of 5-10 runs as is common.We compensate by averaging over results on several datasets and across several methods.\",\"We discuss two limitations of this work as follows: one limitation of our work is the lack of taskspecific automatic metrics to evaluate the empathy of generated responses.Therefore, the evaluation of empathy relies more on human evaluation.Although human evaluation is a golden standard, automatic metrics help to conduct large-scale investigations.This is also a common limitation in current works on empathetic dialogue.The second limitation is the passive response to the user\\u2019s cognition and affection.In many scenarios, empathy is used as a strategy for emotional support by responding to the user\\u2019s cognition and affection.\",\"Our proposed method is an offline system in which the input is a dialogue containing all utterances rather than a single utterance input in chronological order.An online system for emotion recognition can be applied in real-time conference systems or human-computer interaction, so the online system has potential value for future research.Our method can be built into online systems by creating buffer systems such as history windows.However, all the baseline methods in the past are offline systems, such as cogmen, dialoguernn, etc.In addition, the form of datasets also leads us to construct an offline system for training and testing.On the other hand, the offline system also has application scenarios such as analyzing emotions of posted videos, opinion mining in social media, etc.Therefore, our method only builds an offline system under the offline experimental setting that can be compared and evaluated.Besides, the input of our method is feature-based.The original text, audio, and video files will first pass through feature extractors to obtain multimodal features, which may cause information loss and hurt performance.We focus on feature-based training methods because training based on the original files costs a lot.For example, training a video encoder generally requires several v100 gpus and days of training time.Therefore, we, including the baseline methods we compare, adapt the feature-based training methods.with feature-based training methods, different baseline methods use feature extractors to obtain features, leading to a lack of fairness in method comparison.In this regard, we reimplemented all open-source methods and compared them using a unified feature file to ensure the fairness of the experimental results.At the same time, we also conducted evaluations with different signature files to verify the generalization of the method.\",\".\\u2022 we create few-shot datasets from the perspective of the combination of sentiment categories without considering the distribution of aspect items, such as the number of aspects in each sample.It may affect the performance of the model on the task of extracting aspects.We should create more efficient datasets for mabsa in the few-shot setting.\\u2022 as we put more emphasis on the performance of the main task, the performance of the subtask of predicting the number of aspect terms in each example may suffer.\\u2022 we roughly exploit initial image features and do not perform alignment between text and image modalities.\",\"This paper proposes five novel contrastive losses for multi-label text classification tasks.However, our method has the following limitations: 1.we only selected the multi-label emotion classification task and multi-label news classification as the representative of the multi-label text classification tasks.we only conduct experiments on the single modal of text, and have not extended to multimodal tasks.our method chooses the spanemo model as the backbone, lacking attempts to more models.\",\"One limitation in this work is the metrics employed in the automatic evaluation.The metrics mainly focus on the quality of generated response and the accuracy of emotion recognition, while automatic evaluation lacks a comprehensive method to evaluate empathy.Another limitation comes from the utilization of the dataset designed for open-domain dialogue system, so that the generated response from the proposed framework is not task-oriented.\",\"Because of the nature of our framework design, our work requires a diverse set of targets during training, which is important for target prediction and therefore the stance detection method.besides, the model is trained on news-related debate corpus, so it may need further domain adaptation if applying the model to other domains such as social media.We are using an auto-regressive generation framework, which will also require extra inference time to generate the whole output sequence compared to the classification model.We would encourage readers to compare it with classification methods for efficiency when it will be applied in a time-sensitive scenario.\",\"As this is the first large-scale analysis of client reactions in online mental health counseling, there is huge room for future improvement.Here we only list a few problems that we would like to address in the short future.First, although our annotation framework is comprehensive, the data labeled is quite imbalanced.Therefore, our analysis mostly focuses on the extending and defending behaviors.We will label more data so that rare cases can be better understood and classified more accurately.The accuracy of a classifier is important for real-life applications because it has the potential to mislead counselors.Second, we only have one short post-survey, which limits our coarse-scale analysis.We are adding more and richer post-surveys.Third, while we hope that the lessons learned can be applied to everyday conversations, our analysis has only been limited to psycho-counseling.The lessons learned will be tested against a wider range of use cases.It is important, however, not to overgeneralize our findings as this may harm the naturalness of our daily conversations.After all, the psycho-counseling process is a very special type of conversation.\",\"Though our proposed method outperforms current state-of-the-art methods, there are still many challenges we should overcome in future research.First, for colloquial expression which confuses current dependency tree parser, we should come up with new solutions.Second, emotional prediction of tweet posts describing current issues needs external knowledge, which is absent in existing research.\",\"Although our proposed method exhibits great performance to generate more smooth and natural emotional support than baseline models, we argue that the research on this field still has a long way to go.We conclude three aspects that may inspire further exploration.First, the automatically annotated emotion labels may be a little bit coarse and may not accurately manifest the emotional states of the seeker.Second, since various types of commonsense knowledge are not introduced, the current chatbots always generate general and safe responses, failing to provide specific and personalized suggestions to help the seeker get over the dilemma.Finally, current automatic evaluation metrics are still not rational and proper to measure the ability of chabots to provide emotional support.It is desirable to build better evaluation metrics for this.\",\"Despite obtaining promising results, our proposed approach still has the following limitations.First, although our da2lm approach can generate a large amount of target-domain data with high diversity, the generated words are still limited by the source-domain labeled data and target-domain unlabeled data.second, our da2lm model is primarily proposed for the absa and ae tasks, which are not directly applicable for the other information extraction tasks with more than two elements, such as aspect sentiment triplet extraction.Therefore, cross-domain data augmentation for multiple-element information extraction tasks may be a promising followup direction.\",\"We presented a method to automatically generate and label affective events by co-prompting with large language models.The data generation process does not involve creating or training new language models.There are some limitations to our approach.One limitation is that language models are not guaranteed to generate truthful or sensible information, which could introduce noisy information to our model.For example, we observed that the emotion prompt sometimes generates highly unlikely polarity labels for some events.Language models can also produce biased results, which could introduce biased information to our model.Another limitation is that it may be non-trivial for researchers who want to apply our method to other nlp problems to design prompts that are effective for their task.We believe that this method should be fairly general, but it has not yet been evaluated for other tasks.Lastly, our method requires a moderate amount of computational resources, including gpu cards with substantial memory and access to large language models.As a result, groups with limited resources might find our method too computationally intensive.\",\"First, our work essentially relies upon a generative language model to understand the relationships between the sentiment elements in contrast to discriminative\\u002fextractive models which make structured predictions by design.As a result, our model is susceptible to usual anomalies suffered by generative models e.we recover the quadruples from the model\\u2019s output sequence using regular expression based matching with fixed templates, as a result, an end-user will never receive any irrelevant text generated by the model.However, the accuracy will still be impacted in such cases nevertheless.Second, input sequences in user-generated content can be arbitrarily long and that might result in increased decoding time because of the underlying generative model.Last but not the least, all the instruction templates we provide in this work are designed solely for english.It would be interesting to explore systematic ways to be more language inclusive for instruction tuning based absa.\",\"In this paper, we present a supervised adversarial contrastive learning framework with contextual adversarial training to learn class-spread structured representations for context-dependent emotion classification.to more comprehensively evaluate the generalization of sacl, it is necessary to test its transferability in low-resource and out-of-distribution scenarios, and evaluate its performance across a wider range of tasks.Additionally, it would be beneficial to explore the theoretical underpinnings and potential applications of the framework in greater depth.The aforementioned limitations will be left for future research.\",\"Our method is first limited by the proposed grammar that doesn\\u2019t cover all the realistic cases.As shown in table 1, there are still a few cases in the randomly sampled 100 examples that none of the defined rules can explain.Secondly, the time complexity of our method is the cube of the sentence length, limiting its direct applications on long documents.So we have to classify the document based on classification of individual sentences, which might be problematic since the sentiment of different sentences in the document may affect each other.All the experiments in this paper are conducted on public available datasets, which has no data privacy concerns.Meanwhile, this paper doesn\\u2019t involve human annotations, so there are no related ethical concerns.\",\"Our approach relies on the pre-trained language model performance.Although using a graph neural network with the user-product graph helps improve the performance in sentiment analysis, the pre-trained language model still plays an important role in the task.If the pre-trained language model cannot obtain good results, it will affect the performance as discussed on the imdb dataset.Furthermore, the graph density could affect the performance of gnnlm-gnns, as discussed in the experimental results.Since gnnlm is built on top of gnnlm-gnns, gnnlm is also affected by the sparsity problem.As already reported, the density of the user-product graph on the imdb, yelp-2013, and yelp-2014 datasets are 0.the greater the value is, the denser the graph is.Comparing gnnlm with gnnlm-lm, we found that the improvements we could obtain on the imdb, yelp-2013, and yelp2014 datasets are 6.the trend of improvement conforms with the density of the graph.Therefore, if the user-product graph is very sparse, it would greatly affect the performance of gnnlm.\",\"Although we have shown the potential of performing automatic emotion cause extraction on social media without human annotation, there are still several limitations in our work.Firstly, our work only considers the ece task in chinese microblogs.It might be interesting to investigate the effectiveness of our framework in social media platforms in other languages.Secondly, we only focus on extracting the emotion cause expressed in the current post., 37% of the emotion causes exist in the original or historical posts in a conversation thread.\",\"The limitations of our work can be stated from two perspectives.First, the proposed context-free opinion grammar is designed manually.secondly, we focus on opinion tree parsing in one major language.The performance of other languages remains unknown.\",\"Of our work as follows: \\u2022 we only validate the effectiveness of the setmatching strategy for generative models on the coqe task.\\u2022 we observe that the scale of the coqe datasets is quite small and has caused the model\\u2019s overfitting problem.\\u2022 utilize unsupervised data to better help the models mine comparative opinion information.\\u2022 design data augmentation methods to relieve the data sparsity problem.\",\": the duration task focused only on data with a positive happiness label, but it would be interesting to see whether the framework generalizes to a complete dataset and more sophisticated problem definitions.The need for annotations limits the generalizability of our approach, but the bert-psyam framework is effective even with labels generated through semi-supervised methods and other metadata.\",\"Our analysis is limited to non-autoregressive transformer-based models, fine-tuned with the same set of hyperparameters.Hyperparameter optimization would undoubtedly lead to better performance for some models, but we fine-tuned each model with standard hyperparameter values for solving sentiment analysis tasks to reduce resource consumption.\",\".Our findings indicate that bert yields the best performance in terms of sentiment classification accuracy.In combination with shap, it offers a global view of feature importance, which helps detecting spurious features and bias.We think that end users will profit from xai methods which allow to get an aggregated view of feature importance for a particular topic category, or based on a specific time frame.However, due to the high dimension of our data, local explanations are overall not very plausible, regardless of the underlying ml model and explainability method.Moreover, the bert model is less faithful than cnn and lstm, due to high complexity of the model.For our use case, the computational time for generating explanations with lime, ig or saliency would be acceptable in a real-time application, except for shap which suffers from a long computational time.\",\"Since our approach identifies common opinions based on frequency of sentence encodings, we require a relatively large number of input sentences.We were not able to experiment with other popular datasets like amazon , yelp or rotten tomatoes since these datasets only include a small number of input reviews.The abstractive summaries are generated solely based on the latent encoding, and our model does not include a copy mechanism or attend to the original inputs when decoding.It therefore does not always generalize well to new domains.However, this limitation is mitigated by not requiring any labelled data during training: hercules can easily be retrained on a new domain.Generating output based only on latent encodings means that the model is also susceptible to hallucinating, since the output is less directly linked to the inputs.However, unlike other methods, hercules provides evidence sets alongside the generated summaries, making it easier to check whether the output is faithful.Finally, hercules generates summary sentences independently, leading to summaries that are less coherent than approaches that model the summary as a single sequence.we do not anticipate any significant risks resulting from this work.\",\"There are also several potential limitations.First, the severity of the identification of depression is subjective, which inevitably leads to annotation bias, and we cannot verify the actual diagnosis.Thus, our model is not intended to be used as a psychiatric diagnosis tool but an estimate of depression level for users, which can be utilized to direct intervention and treatment for non-clinical use.Second, the datasets used are collected from a single social media platform and are imbalanced.despite these limitations, we believe that our work will facilitate depression severity detection.\",\"While we strongly argue against empathetic conversational systems, there may be use cases \\u2013 such as psychotherapy or educational chatbots \\u2013 where validating a user\\u2019s emotions is, if not required, helpful in terms of their goal.In addition, while a lot of the work on empathetic responses we have discussed is intentional, generative models like chatgpt produce relatively uncontrolled responses that may well be unintentionally empathetic.As with toxic outputs, care should be taken to prevent these models from validating users\\u2019 emotions that cannot be understood.\",\"We conducted our randomized field study on a single platform and in a single language.However, mha is a particularly popular source for mental health resources with over ten million yearly visitors.In addition, we note that a range of socio-cultural factors might influence how negative thoughts should be reframed and how lms assisting this process should be developed.Conducting studies on specific communities, including underrepresented communities and minorities, was beyond the scope of this research.Ensuring equitable access of these tools and adapting them to various socio-cultural contexts requires further investigation.Not all cognitive reframing implementations elicit situations, but we believed it was essential for making the reframe personally relatable.due to privacy concerns, we presently do not gather information to link multiple sessions.However, with appropriate\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"8_sentiment_emotion_emotional\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"8_sentiment_emotion_emotional\"],\"textfont\":{\"size\":12},\"x\":[10.546685218811035,8.933537483215332,8.629233360290527,10.453145980834961,10.184593200683594,8.665473937988281,8.753068923950195,8.574503898620605,8.615254402160645,9.188872337341309,10.181867599487305,9.888319969177246,9.965624809265137,10.0517578125,9.283114433288574,8.923332214355469,10.200927734375,10.279436111450195,8.949695587158203,10.313004493713379,9.019885063171387,10.293429374694824,10.19305419921875,8.967026710510254,10.239083290100098,10.128541946411133,9.998832702636719,8.638582229614258,8.729537010192871,10.27481460571289,9.057855606079102,8.747272491455078,8.863886833190918,12.717002868652344,9.050850868225098,8.738780975341797,9.922455787658691,8.585773468017578,8.966815948486328,8.78267765045166,10.312349319458008,8.925681114196777,10.245230674743652,9.011608123779297,11.712776184082031,10.069964408874512,8.916767120361328,10.13510513305664,10.301738739013672,8.944499015808105,10.056870460510254,10.128914833068848,10.25405216217041,8.579861640930176,8.76407527923584,8.584954261779785,9.561464309692383],\"y\":[2.5053822994232178,2.8836863040924072,2.576442241668701,2.545474052429199,2.6953108310699463,2.627883195877075,2.578399419784546,2.5909879207611084,2.688103199005127,2.8789470195770264,2.7634565830230713,2.7427725791931152,3.025644063949585,2.736769199371338,3.0056042671203613,2.8591957092285156,2.6647017002105713,2.6294443607330322,2.816439390182495,2.6638405323028564,2.800039291381836,2.700221300125122,2.752380609512329,2.813877582550049,2.6803252696990967,2.8086023330688477,2.7831740379333496,2.7159535884857178,2.5877389907836914,2.6702466011047363,2.809302806854248,2.579326629638672,2.7270395755767822,4.976442813873291,2.788383722305298,2.5822267532348633,3.1121985912323,2.619410991668701,2.8334507942199707,2.6743509769439697,2.6706361770629883,2.8363358974456787,2.7369515895843506,2.8198864459991455,1.8662251234054565,2.730687379837036,2.8481643199920654,2.7277233600616455,2.6023385524749756,2.812340259552002,2.7737162113189697,2.69439435005188,2.6643013954162598,2.714836835861206,2.6397342681884766,2.6081674098968506,2.7542788982391357],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"While our theoretical work is broadly applicable to any protected attribute and any dialogue task, our empirical study has primarily tested gender bias on the guesswhat?! Task.Continued experimental study on a wider range of protected attributes and tasks can better support our mathematical findings.Also, users of our theory should verify the assumptions of our theory when using it to draw insights on new datasets.Specifically, as the type of data bias changes, it is possible the assumptions of thm.users of our theory should take care in ensuring context-awareness and context-preservation, for example, are reasonable assumptions on new data, prior to applying the insights of \\u00a7 3.lastly, while all of our gender annotations come from human annotators, only a smaller subset come from annotators primed to 14 judge correctness\\u002fequity of gender reference.So, more in-depth human evaluation can better support our theoretical results as well.\",\".4, authors and annotators of mpchat are primarily in the us, uk, new zealand, and australia.These demographic and geographic biases mean that mpchat may not equally represent all groups.reported that preprocessing data with clip can cause gender-bias issues.We use clip to measure image-text similarity in the pre-processing for data collection, so this problem may exist in our dataset.Users of our dataset should be aware of these risks.To comply with the reddit api terms of use and to protect the privacy of reddit users, commer- cial and for-profit use of our data is limited.It must be available for academic purposes only.\",\"Limitation regarding positive predictive power, there is always a risk with research on social biases that it can give practitioners a false sense of security.It is absolutely possible to evaluate on our corpus and get no bias, and still end up causing harm to racial or gender demographics, since they do not cover all biases or all domains.This should be kept in mind whenever applying this research.\",\"We acknowledge that our benchmark dataset does not cover all the existing ambiguities and that ambiguities related to fairness do not cover all the possibilities.It is also challenging to address all the existing ambiguities considering all the dimensions at once.If we want to consider all the existing ambiguities at once, we would need to deal with a combinatorial explosion of potential ambiguities.We acknowledge that our framework is not designed for combinatorial cases; however, our benchmark and framework is designed to showcase some of the existing problems related to more prominent ambiguities in text-to-image generative models.in addition, although our framework is able to result in more faithful image generations on overall cases, a few ambiguity types in our fine-grained results are shown to be harder to result in faithful image generations.\",\"Our analyses were based around one metric of uniform dif, z.The benefits of z are that it is commonly used in practice, it is highly interpretable with well-established effect sizes, and it is easy to aggregate across items and focal groups.One of the drawbacks, however, is that it does not capture non-uniform dif, and it is not ideal in terms of statistical power.Consistent with other analyses of dif, our study struggles to identify sources of dif.Although it is outside the scope of this study, a finegrained analysis of examinees\\u2019 language, especially based on l1, could provide insight.Additionally, it could be beneficial to explore the possibility of modifying bert using debiasing techniques.These techniques could potentially reveal sources of dif and reduce dif.Follow-up analyses along these lines of inquiry may be found in kwako.\",\"Due to clinical and financial constraints, both the patient and the mturk sample sizes of our study are still relatively small.This means that we cannot afford to set aside patient data as a hold-out test set, and have to use the validation set for model evaluation.As we work towards enrolling more patients and recruiting more healthy volunteers to 429 improve model generalizability, we hope to expand the scope of our pipeline beyond english to serve non-native speaker patients.One major limitation of the cookie theft picture description task is its lack of equitable assessment for an increasingly diverse patient population.identify gender as a particularly fraught aspect of the picture\\u2019s expected response, as the rubrics of the initial nihss were established from a male-only corpus.Although there is no alternative picture or stroke patient corpus available to our study, we try to ensure the equity of our models by maintaining a gender balance in our patient set, with 136 female patients and 132 male patients.On our patient-only evaluation set, our aphasia model performs significantly better on female patients compared to male patients , while the dysarthria model exhibits better performance on male patients than female patients.At present, we are unable to draw any definitive\",\"One limitation that we observed in relation to the language model concerns the possibility that a debiasing method may depend on specific lm characteristics and may not be universally adaptable.It is crucial to clearly acknowledge this limitation.This became apparent during the development of the instructive debiasing method, which relies on an lm\\u2019s comprehension of context and polarity for its functioning.Interestingly, while gpt-3 exhibited these capabilities, gpt-2 seemed 4 to lack them.If a debiasing method is found to be inconsistent on the current lm, transitioning to more advanced lms is a critical subsequent step.A unique advantage of instructive debiasing over other, more complex debiasing methods that modify the lm\\u2019s mechanisms, is its ease of application to closed-source models such as gpt-3, as demonstrated in our paper.We are currently exploring its applicability to successors and counterparts such as palm and chatgpt.Our preliminary experiments with chatgpt using the instructive debiasing approach have yielded intriguing results, with the model persistently refusing to follow the instruction to continue the text.This behavior might represent the most significant leap in debiasing capabilities to date; after all, you can\\u2019t say something bad if you say nothing at all.Another model limitation is the observed tendency of a language model to repeat itself when given a prompt.This was especially prominent in gpt-2 outputs but less so for gpt-3, however certain prompts did still elicit gpt-3 to show the same behaviour.Table 13 reveals outputs for given inputs to gpt-3 and debiased using instructive debiasing with nonsensical specifications, it can be clearly seen that gpt-3 will sometimes repeat the input as well.\",\"Although, we uncover and collate a broad-range of stereotypes, it is not without limitations.Firstly, we generate stereotypes using seeds which influence and skew the output stereotypes retrieved.Our coverage could thus be greatly affected and potentially increased with different or more seed stereotypes.Secondly, stereotypes are inherently subjective in nature and even though we do get 6 annotations from annotators residing in different regions, they have a limited world view and might not be aware of all the existing stereotypes.Additionally, certain stereotypes make sense only in context.For example the stereotype is not offensive by itself but becomes problematic when we compare or rank asians with other social groups.Moreover, the stereotype exists in tandem with the former stereotype which is offensive.Although we do capture regional sensitivity of stereotypes, our work does not capture the contextual information around these stereotypes.For capturing in-region vs out-region stereotypes, we only select annotators from north america but the out-region annotators can belong to any of the other regions as well.That is outside the scope of this work.Additionally, we emphasise that this work is not a replacement to the more participatory work done directly with different communities to understand the societal context and the associated stereotypes.The complementary usage of our method with more community engaged methods can lead to broader coverage of evaluations of harm.\",\"As noted, this work is limited in that it does not address neopronouns.We speculate that the augmentation techniques deployed in this work may extend to these pronouns as well, we recognize that they do not have the same linguistic reality as he\\u002fshe\\u002fthey pronouns.Neopronouns may be similar to singular they in being relatively infrequent in a naturalistic corpus, but they are also different in that they don\\u2019t overlap with a frequent morphologically-identical paradigm like plural they.Additionally, the singular they augmentation technique we propose is specific to english and distributional facts about english pronouns.For one, english singular they morphologically overlaps with a plural pronoun, which is the primary motivation for using coreference information to identify contexts where they would have a primarily singular interpretation.This is often not the case for other languages, as in swedish where the gender-neutral hen is functionally similar to singular they but morphologically and distributionally dissimilar in that it does not overlap with a plural pronoun.\",\"The most notable limitation of our work is the lack of external context.Consideration of external contexts that may be relevant for the classification task in our current models, such as the profile bio, user gender, post history, current and past political scenarios of the concerned region, and so on, might prove beneficial for the results in this field.Our research now focuses majorly on only six types of social biases rather than all conceivable degrees of prejudice.We also focused on utilising hindi, english, korean, and italian in our study, and the hindi dataset is primarily from the indian context.The limited scope of concern can be further explored with our presented experiments to prove to be fruitful for a wider range of audiences by covering datasets bias annotations pertaining to other low-resource languages.We show the effectiveness of few-shot transfer learning using language models with relatively fewer parameters as compared to recent state-of-the-art language models.\",\"As this is a position paper, i mainly provide thoughts here, and do not include any experiments or actions myself.Although the goal of this paper is to combat biases in aer, it is limited to discussing the dominance of english.Other biases, like the bias towards social media texts, or the tendency to ignore neurodiversity and conditions like alexithymia and autism spectrum disorder, are not addressed in this paper.The counting study i performed to demonstrate that the number of papers dealing with other languages than english does not increase \\u2013 contrary to the number of languages that are addressed, which does show an upward trend \\u2013 is only based on papers presented at the workshop on computational approaches to subjectivity and sentiment analysis.Maybe other patterns could be discovered when analyzing the papers of other venues.\",\"In this paper, we developed an approach for evaluating how large language models encode social attitudes about gender, and we applied that approach to evaluate bert-base-uncased.Because the goal of this paper was ethical in nature, limitations on the generalizability of our approach and findings entail ethical risks.we first discuss limitations related to data, and then discuss those related to models and tasks.For both data and models\\u002ftasks, we consider general limitations of our approach, as well as more specific limitations of how we applied the approach here.\",\"Incomplete representation of all demographic groups we highlight that the names used in our study are not close to a complete representation of every demographic group in the united states or world.In our study, we adopt the definition of race\\u002fethnicity from the us census survey, using us-centric racial and ethnic categorizations that may be less applicable in other countries.We adopt a binary model of gender , based on the ssa dataset, which is derived from statistics on baby names and assigned sex at birth; this approach limits our ability to study chosen first names, or to study fairness with respect to nonbinary and transgender people.For race\\u002fethnicity, our study is limited to us census categories of white, black, hispanic, and asian.We are unable to include american indian or alaska native in our study, for instance, as we were unable to identify any names from this group that met our inclusion criteria of \\u003e 50% membership according to our name data source.Furthermore, by using first names as a proxy for demographic attributes, we are only able to study certain demographic attributes that plausibly correlate with names but not other demographic attributes that are likely harder to infer from names.Other demographic attributes that may be discernible to varying degrees from first names were excluded from the scope of this study.Assumption: invariance under name substitution invariance under name substitution, while a valuable fairness criterion for social iqa, may not hold in all other task settings.For example, a factoid qa system should provide different answers to the questions \\u201cwhat year was adam smith born?\\u201d and \\u201cwhat year was bessie smith born?\\u201d.Extended evaluation time and heavy computational costs due to the huge number of mcq instances we construct for evaluation and a diverse set of names to cover multiple demographic identities, it takes a considerably large amount of time and computational resources to obtain the analysis results.however, it is worth noting that the extensive analysis on a wide range of mcq instances and names makes our observations more statistically robust.A future research direction may be optimizing the implementation of sodapop framework, which we use as a major experiment setup to obtain the analysis, for more efficient evaluation.Effectiveness of counter-factual data augmentation it is worth noting that the ineffective result we obtained is not surprising because sodapop has demonstrated that models that are trained with existing state-of-the-art debiasing algorithms continue to treat names differently.Although we find that controlling the name distribution in the finetuning dataset to be rather ineffective in mitigating the disparate treatment of names, it is an open question if applying cda to the pre-training corpus would be more effective.A recent work proposes to apply cda to the pre-training corpus , and it will likely be a great source to use for investigating our open question here.\",\"The findings of this work are limited and dependent on the presented experiments.The image dataset may be biased since the gender, ethnicity, and age were estimated by the dex algorithm and checked by the authors.Despite our best effort, the employed templates could still contain some latent bias that limits the variability and validity of the completions at inference time.Since the study was conducted only in english, the insights can be considered valid only for this language.Ethical statement one main concern with bias in vl is the potential harm it can cause to marginalized communities.Biased vl models can perpetuate and amplify existing societal inequalities and injustices.This can result in discrimination against certain groups of people, such as racial and gender minorities, people with disabilities, and more.In particular, we are concerned about the use of vl in areas such as content moderation, hiring decisions, and criminal justice.Biased models used in these contexts can have serious consequences, such as wrongful censorship or discrimination against certain job applicants.While we acknowledge that the specific harms we fear may not always be likely to occur, we believe it is important to prioritize\",\"In this work, we focus on debiasing the gender bias for plms.in addition, we also plan to extend our debiasing method to more language models, such as natural language generation models.\",\"Due to the restrictions of the adopted benchmarks and resources, our evaluation bears the following limitations: we only focus on social biases in the english language and north american cultures.This is due to the fact that both crowspairs and stereoset are generated by crowd workers from north america.the corresponding resources such as the french crows-pairs and multilingual weat.crows-pairs, stereoset, and winobias all focus on stereotyping, a kind of representational harm, while others like allocational harms are untouched.Developing methods to measure these harms generally requires in-depth interactions between technologists and customers.also point out several conceputalization and operationalization pitfalls in the above three bias benchmarks, which limits the validity of the results evaluated on them.Due to the incomplete bias attribute word lists, our cda-based debiasing method is by no means fair enough to cover all the minority groups.we recommend more complete resources such as the gender-inclusive word list in for real-world scenarios.\",\"In this paper, we only focus on investigating and improving gender fairness of pre-trained language models and didn\\u2019t touch other fairness issues given the length of the paper.However, we would like to note that with the investigation of other fairness issues in human language more deeply conducted, if the biased words regarding other fairness issues can be more specifically concluded, geep can be directly applied to address other fairness problems in pre-trained large language models.\",\"The proposed method has limitations in its dependence on the accuracy and performance of the probe classifier as noted in , and may be limited in scenarios where the dataset is small or lacks sufficient information about the protected attribute.Additionally, this approach increases inference time due to the use of a sequential debiasing classifiers.finally, the proposed method aims to eliminate information about a protected attribute in neural representations.While it may align with fairness metrics such as demographic parity, it is not specifically designed to ensure them.\",\"An important limitation of our work concerns the definition of the protected attributes in the datasets used for evaluation.In particular, gender in bios and pan16 is limited to the binary female\\u002fmale, lacking an inclusive and nuanced definition of gender.Similarly in fdcl18, we consider only two dialects of african american and white american, while clearly this definition is limited and noninclusive.Furthermore as in previous work , the labels of this protected attribute are assigned through a probabilistic model, and hence the dataset might not represent the nuances and traits of the real-world.The second limitation regards reaching strong conclusions on the generalizability of the multiattribute setting for moddiffy over any possible number of protected attributes or subset of them.Our multi-attribute experiments are conducted on one dataset with two attributes of gender and age, particularly due to the lack of available suitable datasets.Hence, further studies are required for achieving a more comprehensive picture on the topic.Finally, we should also highlight two general limitations, shared with the other related studies in the area of model bias mitigation.First, we should consider that the aim of representation disentanglement optimizations is to reduce the existing correlations in the model with the protected attributes based on the observed data.These data-oriented approaches might lack effective generalization, particularly when the model is evaluated in other domains or out-of-distribution data.Second, our bias mitigation evaluation is grounded in the notion of fairness through blindness, and the debiasing optimization methods are designed to support this form of fairness.\",\"When applying causal intervention to remove psycholinguistic bias, we utilize the liwc dictionary to construct the confounder dictionary du.We argue that the debiasing performance could be affected by the quality of the constructed confounder dictionary.\",\".Due to this, the predictions generated by clarifydelphi are currently limited to representing only the perspectives of western culture.Overcoming the western-centric bias is a compelling direction for future research.On defeasibility we rely upon delphi to produce acceptable judgments given a situation and the modifying context as a measure of defeasibility.We recognize that, however, delphi is not perfect and is characterized by a variety of limitations such as limited cultural awareness and inconsistent predictions.Investigating improved methods for identifying answer divergences that will better capture defeasibility is a topic for future investigation.\",\"The main concern is that debiased techniques may remove simple biased features.However, to our knowledge, most debiased techniques can only remove biases across a concept subspace in the embedding space., requires hypothesized biases to train biased models and is limited to tasks with known hypothesized biases.Also, they remove biased examples rather than identify biased symbols.\",\"We acknowledge that the stereoset and crows datasets and metrics are not ideal evaluation measures for debiasing work for more details about their pitfalls).We advise practitioners to conduct careful analysis for their specific use case rather than interpreting the scores from our experiments as clear measures of bias mitigation or removal.Furthermore, we realize that in discussion of harms, we should also ensure that allocative harms do not arise from dependency on a pcgu-debiased model.In this paper, we do not report experiments on models finetuned for other downstream tasks, as finetuning is generally more prone to spurious correlations and accidentally encoding bias, so evaluating such models obfuscates the procedure\\u2019s effect on the pretrained model.Instead, we focused only on the masked language modeling task such that intrinsic and extrinsic evaluations both use the pretrained model directly and only.In the modern age of large language models, this is arguably more applicable, but this setting doesn\\u2019t take into account the effects of prompts on the prediction distribution.An interesting extension of this study would be to debias using some form of pcgu in the pure generation setting and evaluating with high quality generation-based resources such as holisticbias.However, the base form of pcgu is not directly applicable due to the difficulty in attaining and using minimal pairs\\u002ftuples in generations.Another related limitation is that our experiments were only conducted in english.However, many languages, such as spanish or other romance languages, have a much richer concept of grammatical\\u002flexical gender sometimes affecting multiple words per sentence.Unfortunately, a fundamental problem with interpretability arises if we wish to evaluate the language model\\u2019s bias implicitly.For example, the prediction in figure 2 suggests that the debiased model is less biased than a model predicting the full probability mass for the female term.Discrete metrics fail to account for this behavior, so better evaluation metrics would also give us a better sense of the efficacy of our proposed method.We further note that gender, which has historically been treated as a binary construct, is likely to be a relatively easy domain to work with.Other more complicated social biases like racism and classism are similarly harmful, and an ideal debiasing procedure should work for all of them.Similar questions may arise about if we can ever comprehensively cover all domains without a better way to generalize across domains.a natural thought would be to attempt to generate training data for pcgu.We attempted this but found that the generations were not reliable in terms of providing signal for what constituted bias.By using a templated dataset like winogender, we can ensure that every instance in the training set encodes bias by an assumption of gender based on only the profession.Obviously, partitioning at the most granular level where each single parameter is its own part would make our directional comparison meaningless.However, we did not extensively study how important the specific partitioning method was.An interesting class of experiments would be using some sort of random partitioning, where each individual parameter is assigned to its group of parameters not according to any architectural reason but according to some sort of randomness.Our implementation of this made the gradient selection extremely expensive because it required too much indexing into tensors as opposed to a full replacement of specific dimensions.A better implementation or experiment would be needed to draw actionable conclusions about different partitioning methods.However, our baseline experiments for this matched with the intuition that sampling each weight as being a bias or non-bias weight using a bernoulli distribution yields a similar effect as regular training with dropout, similar to the k=all experiments in table 2.\",\"Our work is limited in capturing the unintended dependencies of attributes.It is possible that maximizing certain attributes like positive sentiment may maximize attributes like gender bias.A formal study to capture the dependency of the bias with varied attribute control is an important future direction.The efficacy automated metrics used to measure the linguistic qualities and attribute alignment of the generations is limited.Devising more exhaustive and explainable metrics is also an important future-work.\",\"We acknowledge the underlying assumptions of the social bias benchmarks used in our study.While the presented study aims to point out a key limitation of currently accepted methodologies, the presented investigation could benefit from more diversification.also, the bias benchmarks themselves imbibe the notion of fairness with the western value system , and future explorations of benchmarks should diversify culturally as well.Last but not least, we acknowledge the harm of binary treatment of genders in one of the target benchmarks.The purpose of this work was to bring light to a broader problem regarding the reliability of social benchmark metrics, with the hypothesis that the main idea of this paper would hold for a wider range of datasets with other assumptions or notions of fairness.We also acknowledge that there are larger models that we were not able to train and evaluate due to the limitations on our computational budget.The current study was focused on benchmarks with templated instances.This is no coincidence: the dominant majority of the social bias benchmarking literature relies on sentences with some degree of known structure, even in those collected from the wild.Such structural assumptions in datasets are necessary for defining and extracting quantifiable measures of social bias, which as we argue, are the reason behind the brittleness of their decisions.broader impact bias evaluating benchmarks play a very significant role in helping identify potential risks of language technologies.While a large body of work evolves in this area of work, there is growing concern about the ability of the different benchmarks to accurately quantify and identify social biases.We emphasize these concerns by evaluating how robust the benchmarks are to alternate constructions based on simple linguistic properties.It is important to note how inaccurate measurements of social biases can be problematic by underestimating or misdiagnosing the potential harm from language models.We hope our work helps identify such pitfalls.\",\"Just as it is not possible to create a single benchmark for all language understanding , it is not possible to create a single, definitive dataset that relates language choices to social attitudes.Human experimental data is always limited by practical considerations and cannot test every condition of theoretical interest; e., in the role nouns dataset, there were no conditions with gender neutral names, while in the singular they dataset, there was no comparison to neopronouns.Additionally, because past work has found that model preferences may vary across similar linguistic contexts , it may be the case that bert\\u2019s predictions would correlate differently with human responses on other variations on the stimuli.Relating model preferences to human behaviour will always be limited by the amount of human data that can be obtained.Moreover, datasets are always situated in a perspective, emphasizing some people or views over others.For example, both datasets we consider focus on first language english speakers from the united states, and the specific relationship between social attitudes and linguistic choices captured by those datasets may not generalize outside that context.Languages other than english may have extensive grammatical gender systems, or classification systems that include social roles, among other linguistic devices, which interact to yield rich mechanisms for expressing social attitudes around gender.Even within english speakers in the us, how language signals social attitudes about gender may vary across groups and social contexts.Found that republicans with progressive social attitudes about gender did not use more gender neutral forms the way democrats did; other, more fine-grained differences likely also exist.) Additionally, relating social attitudes about gen- der to linguistic choices requires some method for measuring social attitudes.Since conceptions of gender are so diverse and culturally variable, no single measurement would be appropriate for all contexts.For example, in one of the datasets we used, a survey for measuring social attitudes about gender asks participants to evaluate statements about stereotypical social roles of men and women, which are likely culturally specific.In evaluating language technology, a focus on associations between linguistic choices and social attitudes limited to particular linguistic and cultural contexts risks prioritizing the social knowledge from those communities, and imposing that in other communities when language technology is deployed.To support the creation of inclusive technology, the research community will need to prioritize generation of datasets like the two we drew on here \\u2013 i., ones explicitly connecting linguistic choices to social views \\u2013 across more languages and cultural contexts.\",\".5, limitations of this work also include the issue of data imbalances that some attributes may have imbalance distributions.For example, we may find significantly more profiles with the country of citizenship as united states than any other countries, which may have a negative impact on generalization, especially when the distributions of training and inference diverge.5 indicate that the prediction results for non-celebrity distributions should be carefully adjudicated.The degraded performances on low-resource attributes also indicate that the prediction results may be unreliable when performing inference on attributes without enough training data.In this paper, we assume that the attributes are already given.However, many wikidata attributes are not applicable to everyone.For example, attributes such as \\u201cposition played on team\\u201d may be specific to athletes.Therefore, it is also important to investigate how to automatically detect applicable attributes for certain users.In this work, we use at most 100 recent tweets and aggressively create training and inference examples between each attribute and those tweets.Since we use sliding window on the collected tweets, involving more tweets in training or inference may significantly increase the time cost.\",\".Gender dependency: our approach does not account for sentences that only make sense for a single gender.For example, sentences like \\\"she needs to see a gynecologist\\\" would not be captured by our method.This is a common problem encountered by most debiasing algorithms as it is difficult to distinguish these.finite wordlist: the wordlist does not contain all gender-based words as the language con- tinues to evolve.blunt substitution: the phrase substitution method is an improvement over direct word substitution, but there are still plenty of instances where the new sentence might be semantically incorrect.This does not have any major implication on inference as we are only doing few-shot learning, but it should not be extended to the entire dataset.binary gender: the method only focuses on the male and female gender.It does not consider non-binary or gender-neutral pronouns such as \\\"ze\\u002fhir.\\\" This can be solved by using an updated wordlist, but the authors could not come across one at the time of writing.downstream analyses: while our work proposes methods that show reduced gender bias as per a set of metrics, the work in no way claims to reduce gender bias in general, especially on downstream tasks.However, we strongly believe that this technique holds potential to reduce gender bias on downstream tasks as well since we adopt a regular finetuning approach and focus mainly on better data interventions.Moreover, recent research has shown that fine-tuning-based debiasing approaches do not damage a model\\u2019s internal representations to a critical extent.Overall, these limitations suggest that our approach may not be suitable for use in contexts where gender-specific or non-binary language is prevalent, and the underlying wordlist should be frequently updated.\",\"The data from the pitt dataset , while useful for our paper, contains many images and annotations that may perpetuate harmful stereotypes according to sensitive characteristics such as gender and carry the risk of amplification by machine learning models.We plan to collaborate with ai robustness researchers to identify such examples and develop methods for improving ml models in terms of robustness and reliability.\",\".In turn, fairprism provides a richer lens for diagnosing the types of fairness-related harms that ai text generation systems cause, and the potential limitations of mitigation methods.The process we followed to develop fairprism offers a recipe for building improved datasets for measuring and mitigating harms caused by ai systems.limitations fairprism is limited to fairness-related harms relating to gender and sexuality.It contains only english text, primarily represents varieties of english used in the u., and the annotators who labeled the examples were from the u.as a result, it is less well suited to measuring or mitigating harms relating to other demographic groups, harms specific to other countries, and harms in other languages.In addition, the social bias frames dataset, from which we obtained some of the human inputs, consists of text from social media sites, so it may not reflect typical interactions with ai text generation systems.Some of the constructs we attempted to operationalize have competing definitions, which may affect the range of harms covered by fairprism.For example, our definitions of stereotyping and demeaning harms may have caused annotators to label some stereotypes, demeaning content, or forms of phrasing as harmful more easily than others.Annotators may also have used implicit criteria when labeling examples.In addition, our focus on stereotyping and demeaning harms excludes other types of harms.For example, allocation and quality-of-service harms are not covered by fairprism, nor are harms that stem from the use of ai text generation systems more broadly, such as questions of power and agency that relate to who is able to design or use these systems.Unintended uses as a result of fairprism\\u2019s limitations, we do not intend it to be used for any of the purposes outlined below.Access to fairprism is restricted as a preventative measure.To request access, please send an email to detailing your desired use case for us to review.As training data for generating hate speech.Illintentioned actors could train models on fairprism for the purpose of generating hate speech.directly using fairprism to train classifiers for mitigating fairness-related harms prevents it from being useful as a measurement instrument.Furthermore, fairprism is not sufficiently large or comprehensive to be effective for training mitigation methods.\\u201d if ai systems are repeatedly trained to improve on any single aggregate metric calculated using fairprism, this will result in overfitting to the dataset, which will make the dataset less useful for measurement and may lead to a greater proliferation of harms that it does not cover due to a false sense of complete coverage.fairprism contains examples of text generated in both reply scenarios and continuation scenarios.Its efficacy will therefore lessen for applications that are further removed from these scenarios and for applications that are highly specific.Fairprism is also less well suited to measuring or mitigating harms relating to demographic groups other than those based on gender and sexuality, harms specific to countries other than the u.and canada, and harms in languages other than english.\",\"In capturing all dimensions of fairness.Further research is needed to develop comprehensive extrinsic metrics across diverse tasks.Although our work has been centered around fairness in allocation-based applications, addressing fairness concerns in other types of language models, such as natural language generation models, is necessary.In generative tasks, the measurement of unfair outcomes would be distinct from the methods we have used.\",\"Although the bias metrics and debiasing methods we study work well, they certainly have limitations.Limitations of this paper are given below: we are aware that defining a bias in terms of target-attribute pairs can be incomplete and somewhat subjective.our dataset contains multiple bias categories, but they are still defined in advance and limited.It is feasible to explicitly define the different bias categories separately, but this also means that we need to use the corresponding subsets of the dataset when studying the different biases.Therefore, a mechanism that can automatically classify biases is necessary.\",\"This paper evaluates language models for their ability to use gender-neutral pronouns and neopronouns using a template-based dataset, misgendered.While this approach is helpful in assessing bias, the measurements can be sensitive to the choice of templates.Consequently, our findings should not be considered as the definitive verdict on the phenomenon of misgendering by language models.There are other limitations to our work that should be considered as well.We also only conduct an upstream evaluation on language models and do not assess downstream applications.Our evaluation is also limited to a western conception of gender and restricted to english only.We only consider names and genders assigned at birth in the united states.Subsequent changes in names or genders are not taken into account in our analysis.Furthermore, our work does not take into account individuals who use multiple sets of pronouns, such as she\\u002fthey combinations , nor does it consider the full range of nonbinary pronouns as the list continues to expand.However, additional names and neo-pronouns can be directly used with our framework to further evaluate llms.We release our full code dataset to make this easier.Lastly, there are larger models that were not evaluated due to limitations in our computational budget.Further research needs to be done to address these limitations for the complete assessment of accurate preferred pronoun usage by language models.\",\"Although our research led to improvements in the translation of subject pronouns, object pronouns, and possessive adjectives and pronouns, these improvements did not cover non-binary-associated pronouns, such as they\\u002fthem\\u002ftheirs, xe\\u002fxem\\u002fxyr and ze\\u002fhir\\u002fhirs.The large underrepresentation of non-binary genders in textual and visual data contributes to propagating the misrepresentation of non-binary people by language models.\",\".However, small variations to the prompt structure yield dramatically different results.We also do not explore how different cot prompts affect stereotypes, focusing only on the sota \\u201clet\\u2019s think step by step.for example, priming cot generation with \\u201clet\\u2019s think about how to answer the question in a way that avoids bias or stereotyping\\u201d may reduce biased outputs.We also do not explore bias in few-shot settings.Models are very sensitive to few-shot exemplars ; furthermore, exemplars trivialize intrinsic bias benchmarks, and are similar to finetuning.Limitations of bias benchmarks prior work has shown flaws in existing fairness benchmarks; measuring fairness is itself an open problem.Benchmarks often-time have differing conceptualizations of bias , leading to contradictory results.We ran our analysis across 3 separate benchmarks, including an extrinsic evaluation of bias in question answering.We also conduct a manual, qualitative analysis of failures to tie our quantitative findings to examples of representational harm against protected groups.We believe the general agreement across our analyses mitigates the flaws of each individual benchmark, but the limitations and stated goals of each should be carefully considered when interpreting results.\",\"As we cannot control for all confounding variables when examining the correlates of the most effective contrast sets, we only claim to identify trends, not causality, between calibration set characteristics and downstream performance.For instance, the top beams, on average, have higher relevance.As such, for each strategy, we record all key set characteristics and focus our analysis on observing trends between set characteristic values and downstream performance across all experiments, not simply within each selection type.\",\"In this work, we are the first to uncover the social bias problem in the text-to-sql task.We categorize different types of social biases related to various demographics.We present a new benchmark and metric for the social bias study in the text-to-sql task.However, this work stops at the point of uncovering and analyzing the problem and phenomenon, without making one step further to solve the social bias problem in the text-to-sql task.Besides, in spite of the structured scalability of our proposed paradigm for social bias benchmark construction, the efficacy of entending with other text-to-sql datasets remains to be verified.\",\"A limitation of our conclusions is that although it is necessary to use several fairness metrics to be able to properly quantify the bias, this is not enough.These metrics must be well chosen according to the context and the task being looked at.The expertise of a person working in the field is therefore always necessary to have the most complete possible interpretation of the bias.More specifically, the different fairness metrics measure distinct properties, and the fact that they are often incompatible has been a core part of the fair ml conversation from the beginning.Thus, suggesting to choose a different metric depending on the sample size may sometimes be inappropriate, since this choice may depend on the meaning of the metric in a given application.We must therefore be very careful and see the notion of robustness as additional necessary information and not as a replacement for the metric\\u2019s meaning.We also did not reduce the bias using advanced strategies because this paper focuses more on the analysis intended for a population closer to the law than to machine learning.In this vein, it is interesting to note that more and more tools are available to reduce bias.In particular, makes it possible to reduce the bias according to several fairness metrics, therefore remaining in our logic of taking several metrics.The main problem raised by our article comes from the fact that fairness indices are not stable when they are calculated.We should consider them as random variables and look at their law.The first step is to look at the mean and the variance as done in this paper but having the full distribution would be more interesting.Works that compute the asymptotic law can be taken as an example like.This work uses different experiments and fairness metrics to shed light on the shortcomings of these metrics with respect to gender bias made by ml algorithms on textual data.We believe that transmitting knowledge from research to industry on a subject like fairness is essential to make the field of ml more ethical.Hence, this work focuses on issues that most affect the industrial ml landscape and contains a clear message to them on how they should change their current practices.\",\".The small sample size we collected makes it difficult to assess the generalizability of our findings.This also prevented us from running any analysis of internal structure or differential item functioning using methods from factor analysis or item response theory, as these models require large sample sizes.we also did not examine the diversity of the generated items, in other words, how thoroughly the model explored the construct space.It is a well-known problem in psychometrics that having too many similarly worded items can inflate the reliability and reduce the validity of a measure , and our results may have been susceptible to this.A related problem is ensuring that the distribution of labels in the generated items remains balanced, and while we took steps to account for this, we did find that the distribution of gpt-3 items was somewhat unbalanced.For example, there were far fewer neutral items than either entailment or contradiction.our experiment with gpt-4, while disappointing, was also quite limited and should be expanded upon.We deliberately kept the prompt design as similar as possible between the two models, to avoid possible confounds.Making effective use of the system query and changing the structure of the prompts to suit a conversational style could lead to much better results, however.llms have the potential to greatly ease the burden of scale development, and transform educational and psychological measurement.Our results contribute to the growing field of llm-based automated item generation, and demonstrate the potential these methods have for generating valid and reliable items at a scale that would have previously been impossible.Further research, combining our approach with more advanced prompting strategies, or zero-shot parameter estimation, could conceivably lead to a system that generates high-quality items in a fully autonomous fashion, which would transform the practice of writing and validating test items.Limitations we emphasize that our research is exploratory and the generated items we produced should not be used for making critical evaluations of cognitive skillsets in either humans or llms.\",\".First, demographics may not be the best construct for positionality, as there may be variability of beliefs within demographic groups.Assuming that there is homogeneity within demographic groups is reductionist and limited.study annotators could also purposefully answer untruthfully, producing low-quality annotations.We address this risk by using labinthewild.Labinthewild has been shown to produce highquality data because participants are intrinsically motivated to participate by learning something about themselves.However, as is the case for all online recruiting methods, our sample of participants is not representative of the world\\u2019s population due to the necessity of having access to the internet.In addition, there is likely a selection bias in who decides to participate in a labinthewild study.Pearson\\u2019s r may not fully capture alignment as it does not consider interaction effects between different demographics.Thus, there may be additional mediating or moderating variables that may explain the results that our analysis does not consider.We also took the average of the annotations per group, which could mask individual variations.Also, having a low number of participants from specific demographic groups may limit how well the results generalize to the entire group; further, it may risk tokenizing already marginalized communities.As part of our study, we apply nlpositionality to only two tasks which have relatively straightforward annotation schemes.It may be difficult to generalize to other nlp tasks which have harder annotation schemes, especially ones that require a lot of explanation to the annotators, for example, natural language inference tasks.Our approach is evaluated and works the best for classification tasks and classifiers.Generation tasks would need more careful annotator training which is difficult to achieve on a voluntary platform without adequate incentives.Having annotators use one likert scale to rate the social acceptability and toxicity of a situation or text may not be a sufficient measure to represent these complex social phenomena.To reduce this threat, we provide detailed instructions that describe how to provide annotations and followed the original annotation setup as closely as possible.\",\"One potential limitation of this review is our selection bias which may affect the representativeness of the included papers.Another limitation is the potential differences in methodologies across the papers we reviewed, which makes it difficult to draw generalizable conclusions.Different studies use different experimental settings and methods for measuring feature importance, which could also impact the comparability of the findings across the included studies.Furthermore, we acknowledge the potential publication bias which might lead to an overestimation of the impact of different factors, as studies with statistically significant results may be more likely to be published than those with non-significant results.\",\"We replicate three well-known experiments in the gender bias literature, where bias is measured according to a binary female vs.this choice ignores other views of gender but eases the presentation of the frameworks.We only use two corpora and three datasets which by no means capture the biases of all the people speaking or writing in the english language.Moreover, we don\\u2019t experiment with different corpus sizes, a more diversified set of corpora or more bias types.the hyperparameters of the models have not been varied, using their default values.This replicates the standard experimental setting used in the literature.Since there are no ground truths when measuring biases , hyperparameters are usually set to their default values.\",\"In this part, we show limitations of our work by categorizing wrong predictions outputed by our method clever into two groups.The first type of error is induced by the unconspicuous biased features of claims.For example, the claim scandinavia includes the remote norwegian islands of svalbard and jan mayen.Does not contain obvious biases so that the output of claimonly model cannot represent the biased distribution.Therefore, subtracting such output fails to mitigate biases but reduces the beneficial claim information instead.the second type of error occurs when high-level reasoning is required, e., mathematical computation and multi-hop reasoning, which drops into the scope of model reasoning ability.This work mainly focuses on debiasing fact-checking models that make them concentrate on the intrinsic evidential information.After debiasing, how to enhance the reasoning ability over such information is a promising future direction.\",\"Our findings about gender bias in the field of language vision are based on two datasets, one task , one language , a mostly western population , and one computational model.Moreover, in the third analysis, due to the characteristics of the test set of manynames, the sample size was small.Additionally, the bias around naming choices concerns the domain of sports only.Regarding our most novel finding , given the basic function of naming in language and the fact that western englishspeaking societies are not known to be more genderbiased than most non-western and\\u002for non-englishspeaking societies, it is plausible that the identified bias extends to other tasks such as image captioning, referring expression generation, or visual question answering.Furthermore, given previous work on bias in our field, it is plausible that the identified bias in the model extends to other models.It is, however, not clear whether the bias will be amplified or simply reproduced.Testing further models on the same naming data that we used is straightforward; checking for biases in some other tasks for english should be feasible at least to some extent, since some datasets provide multiple annotations per image.Instead, analyzing other languages and populations, such as nonbinary individuals, will in most cases require further data collection due to the scarcity of non-weird data in our field.In this study, gender was operationalized in a bi- nary manner in order to most effectively investigate the stated hypothesis.Furthermore, there is a lack of nonbinary labels within the datasets used , and the resources required to reflect the reality of the gender landscape currently do not exist.This indicates a separate but related issue regarding a lack of representation of nonbinary individuals within vision datasets and how to conduct ethically inclusive studies on gender.Finally, this work solely concerns the identification of biases; further work should focus on how to deal with them in terms of data collection, curation, and modeling.\",\"The word embeddings and language models used in this work are trained on contemporary english language, and our social contexts overly contain explicit stereotypes encoded in english.Stereotypes for a specific group can be quite different depending on the language and culture.Although out of the scope of the present work, cross-societal differences in human stereotyping have been shown to be explainable using the scm framework.Thus, it is fair to posit that our scmbased framework generalizes to social group biases beyond those in english.Future research is encouraged to replicate our study in non-english languages.Furthermore, we would like to point out that there exists a catalogue of bias measurements for word embeddings and language models in the field.additionally, some of these measures have been shown to fail robustness checks.Although our current work uses some of the most recently developed ect and eqt, we believe that few, if any, of these measurements are completely sound nor complete.In our experiments for language models, we tried to measure bias for the same social group or attribute using multiple benchmarks but still found some substantial differences in results across benchmarks.Therefore, we caution against interpreting low bias measurements as evidence of complete bias removal.While developing a new bias measurement scale is not within the scope of this work, we are optimistic that the social psychological theory in which our approach is grounded provides the bedrock for the current evidence of scm efficacy to hold on future benchmarks.Unlike bias mitigation methods for static word embeddings, such as partial projection, the post hoc methods of debiasing for large language models can\\u2019t be trivially applied to mitigate biases for multiple social attributes simultaneously.For dpce, the formulation allows for mitigating biases on multiple social attributes, but collecting enough sentences from each attribute that do not include any words from other attributes or neutral words was not possible with the corpora we experimented with.This problem is exacerbated as the number of social attributes grow due to the mutual exclusivity condition.For adept on the other hand, the formulation did not trivially handle multiple dimensions.Hence, we employed a coordinate-descent modification in our experiments to apply adept to scm.\",\"While we consider our approach more easily applicable to new languages than rule-based forward augmentation, it relies on the existence of sufficient original gender-fair text in the language of interest and it is currently unclear what the minimum amount of parallel data is to learn a genderfair rewriting model.Additionally, our survey only targets affinity groups which limits the generalisability of our results to all german speakers.Since people who choose to not use gender-fair language can simply not use a rewriting system, we do not think that this lack of generalisability is a problem in this case.Another limitation is that we use a specific form of gender-fair german in our survey.We made participants aware of this in a disclaimer at the beginning of the survey.It should be stated that there are many different acceptable gender-fair forms in german.While using a different gender-fair form could affect the individual ratings in our survey, we do not expect that it would change our finding that rewriter outputs are rated more gender-fair than the original texts.\",\"Community survey the winoqueer benchmark is necessarily an imperfect representation of the needs of the lgbtq+ community, because our sample of survey participants does not represent the entire queer community.Crowdsourcing, or volunteer sampling, was used for recruiting survey participants in this study as it has its strength in situations where there is a limitation in availability or willingness to participate in research.However, this sampling method has a weakness in terms of generalizability due to selection bias and\\u002for undercoverage bias.We limited our survey population to english-speakers, and the winoqueer benchmark is entirely in english.We also limited our survey population to adults to avoid requiring parental involvement, so queer youth are not represented in our sample.Additionally, because we recruited participants online, younger community members are overrepresented, and queer elders are underrepresented.Compared to the overall demographics of the us, black, hispanic\\u002flatino, and native american individuals are underrepresentend in our survey population.Geographically, our respondents are mostly american, and the global south is heavily underrepresented.These shortcomings are important opportunities for growth and improvement in future participatory research.Finetuning data collection in an effort to balance the amount of linguistic data retrieved from media cloud and twitter respectively, we had to use additional search terms for media cloud as it yielded significantly fewer results than twitter when using the same search terms.Also, news articles from january to may 2022 are excluded from the news article dataset due to media cloud\\u2019s backend api issues.Due to the size our datasets and the inexact nature of sampling based on hashtags, it is likely that there are at least some irrelevant and spam tweets in our sample.Template creation our generated sentences have several limitations and areas for improvement.First, our nine identity subgroups are necessarily broad and may not represent all identities in the queer community.The winoqueer benchmark is limited to biases about gender and sexual orientation.the names used in templates are taken from the us census, so they are generally western european names common among middle-aged white americans.Noneuropean names are not well-represented in the benchmark.Additionally, the benchmark currently only includes he, she, and they personal pronouns; future versions should include a more diverse set of personal pronouns.Finally, sentences are generated from a small set of templates, so they do not represent every possible stereotyping, offensive, or harmful statement about lgbtq+ individuals.A high winoqueer bias score is an indicator that a model encodes homophobic and transphobic stereotypes, but a low bias score does not indicate that these stereotypes are absent.Evaluation and finetuning we used similar, but not identical, scoring functions to evaluate masked and autoregressive lan- guage models.It is possible that the metrics are not perfectly calibrated, and that one category of models may be evaluated more harshly than the other.Additionally, some of our finetuned models scored below the ideal bias score of 50.this means that they are more likely to apply homophobic and transphobic stereotypes to heterosexual and cisgender people than to lgbtq+ people.Many of these stereotypes are toxic and offensive regardless of the target, but others do not carry the same weight when applied to cis and straight individuals.Currently, it is not well-defined what wq scores under 50 mean, in theory or in practice.This definition will need to be developed in consultation with researchers, end users, and the lgbtq+ community.This paper only includes results for a small fraction of available pretrained language models, and our results only represent comparatively small models.We present baseline results for models up to 7.1 billion parameters and finetuned results for models up to 1.5 billion parameters, but many of the models in use today have hundreds of billions of parameters.Finally, our results are limited to opensource models and do not include closed-source or proprietary models.\",\"Although gender-tuning succeeds in reducing the gender bias scores in the pre-trained language models, there are some limitations to performing debiasing.Gender-tuning only works on gender-related words list.Thus gender-tuning cannot cover the probable gender biases that do not exist in its\\u2019 list.We defer the gender-related word list modification to future research.All our experiments ran on english language texts with english gender-word morphology.\",\"Our protocol carries with it assumptions that may not allow it to be applied to all possible debiasing methods.For example, it does not account for debiasing methods that do not use specifications or for those whose specifications do not have corresponding opposites.Moreover, the protocol isn\\u2019t a universal tool; its application is limited to debiasing methods that employ both a mechanism and specifications with polarity.For researchers working on new debiasing methods that either don\\u2019t use specifications or employ more complex specifications that may not represent a specific polarity, we encourage them to leverage our findings and investigate potential adaptations to suit their methodology.The protocol was not only designed to be compatible with appropriate debiasing methods, but also to serve as a foundation for the development of novel protocols.first, the protocol was designed to be used as an evaluation tool for consistency and is far from representing all tests and considerations that must be taken before deploying a debiasing method into public or private use.The use of the protocol is encouraged to gain insight into possible shortcomings of a methodology, but there are risks to this as there may be considerations and inefficiencies that the protocol does not account for.Thus, the protocol is meant for research purposes only and is not meant to be a foolproof ethical check for software deployment.Second, the protocol was built using, and only considering, english with north american definitions of specifications.This means that any results found using the protocol, and even the protocol itself, may not work or be as effective if used with different languages or different social and cultural definitions of specifications.\",\"Our experiments were performed under some limitations.Since our work deals with both privacy and bias, we tried to keep the individual concepts within bounds, and thus only focused on the oftentreated case of gender bias.Other works, however, also consider cases of, for example, stereotypes towards members of the lgbtqia+ community or different religions.Additionally, we adopted the simplified assumption of binary genders without considering other existing identities such as non-binary or trans*7.7 2022-worldwide\\u002f furthermore, our computational resources were limited.Training with dp requires a lot of gpu memory , which is why we could not train the entire gpt-2 medium with dp.Moreover, we could only train with a batch size of 2.Compensating this by increasing the gradient accumulation steps was also only possible to a small extent due to the limited memory.However, it is likely that dp could have a higher effect on some of the evaluation frameworks when applied to all layers of the model.It would have been of great interest to see if the effect on fairness would have been different.Furthermore, the dataset we used for training was relatively small.Due to limited computational resources and the overall good compatibility with opacus , we worked exclusively with gpt-2.In the experiments, we found that both dropout and cda did not provide unambiguously reliable mitigation results.We agree with the finding of other authors that the reliability of seat is not beyond doubt, as no bias with statistical significance is found even in the pre-trained gpt-2 model.For the other two approaches , the model must make predictions with respect to very specific stereotypes, and these predictions may not necessarily be changed by training on a counterfactually expanded data set or increased dropout.Moreover, we evaluated our models on the glue benchmark, without focusing on individual tests.More closely examining this would be interesting scope of future research.\",\"Rather than a complete, systematic probing of the stereotypes and biases related to each demographic group that may occur in the open-ended outputs, our study offers insight into the patterns in the stereotypes that the widespread use of llms may propagate.It is limited in scope, as we only evaluate models available through the openai api.While our approach can be generalized to other contexts, our lexicon and qualitative analysis draw only upon american stereotypes, and we perform the analysis only on english.Beyond the five race\\u002fethnicity and three gender groups we evaluate, there are many other demographic categories and identity markers that we do not yet explore.Another limitation of our method is that it currently requires defining which identities are marked a priori, rather than finding the default\\u002funmarked class in an unsupervised manner.The prompts are marked with the desired demographic attribute, and every persona is produced with an explicit group label.Given these explicit labels, we then compare and analyze the results for marked vs.a potential risk of our paper is that by studying harms to particular demographic groups, we reify these socially constructed categories.Also, by focusing our research on openai\\u2019s models, we contribute to their dominance and widespread use.\",\"There are also several limitations related to the models and tasks considered.First, we evaluated only one model , and more work is needed to understand if and how our specific results generalize to other masked language models.This is especially important given that past findings comparing gender bias in masked language models with different architectures and model sizes are mixed.Additionally, we only considered the task of masked language modeling.We made this choice because psycholinguistic datasets that pair linguistic choices with results of social attitude surveys are rare, and those available to us used language tasks that were most appropriate for evaluation on the task of masked language modeling.However, given that bias on the intrinsic task of masked language modeling may not relate to bias on downstream tasks , our results may or may not carry over to downstream tasks.another limitation has to do with differences in the information considered by language models, as opposed to humans, in choosing to use gendered vs.In both tasks we study, participants and language models evaluate the appropriateness of gender neutral forms based only on contextual cues to the subject\\u2019s gender, especially gender associations of names.However, when deciding what to say, people can also take into account the referential gender of people being referred to.For example, if a person knows that someone named michael uses feminine referential gender, they would likely refer to her with gender neutral or feminine forms but probably not masculine forms.Focusing on evaluation tasks which do not consider information about referential gender risks encouraging the development of language technology that performs worse on data from trans people, and contributing to their erasure.Note that in the michael example there are still linguistic choices , which may reflect social attitudes.Finally, while this work developed an approach for evaluating the social attitudes about gender communicated by language models, it does not propose any approaches for improving language models or adjusting the attitudes they communicate.Past work in nlp has discussed different approaches for how pronouns might be handled in language technology , and has developed gender neutral re-writing tasks , which replace gendered pronouns and words like fireman\\u002ffirewoman with gender neutral variants.Contrasting with standard fairness approaches in nlp that remove information about gender from language technology, work in feminist hci has discussed approaches for the treatment of gender in language generation which are intended to challenge existing norms and stereotypes, and bring about social change.Additionally, work on lan- guage reform has discussed the challenges involved in working towards gender-inclusive language, including how explicitly gendered and gender neutral variants can often take on different meanings.\",\"One limitation of this study is its scope, which covers two downstream tasks and two types of demographics.The binary gender definition we used excludes other genders that do not fall under male or female.In the case of race, we explored only african american race , which excludes biases related to other races, and is a us-centric view of racial bias.We did not investigate other types of bias, such as religious bias.Furthermore, our method was tested on datasets with short texts, and it is unclear how it will perform on longer texts.The experiments were conducted on datasets in english, and it is unclear how our method will work on languages that are morphologically rich.\",\"We now discuss limitations of causal-debias.In consideration of the fairness, we follow the prior bias mitigation work and use human-collected lists of gender and racial pairs for counterfactual data augmentation and intervened distribution generation.It is obvious that the bias word lists are inadequate to cover all the bias-related demographic groups, while we believe the general list is exhaustive.We consider there is a possible model improvement that leverages the perturbation augmentation on bias-related sentences along multiple demographic axes.Another possible improvement would be to generate bias words by using prompts to probe the biases that may lead to a bad effect.Moreover, we also considered the use of external corpora.The external corpora have been significantly investigated in prior works and are utilized as an intervention corpora.used two natural language inference data to produce generalpurpose debiased representations.There are several other corpora including news-commentary-v1 , wikipedia , and wikitext-2.A possible future direction of debiasing is how to mitigate the biases without heavily relying on any corpora and just using internal knowledge.Moreover, in the paper we primarily focus on studying gender and racial bias mitigation.we would also like to note that although causaldebias shows a satisfactory performance on seat tests and crows-pairs, these results should not be interpreted as a complete bias mitigation.the main metrics are mainly against north american social biases and only reflect positive predictive power.They detect the presence of the biases but not their absence.did not use the seat tests and evaluated their model on various metrics.From the perspective of different usage scenarios, we need a more general and reliable debias metric for comparison between different models.The lack of universality and agreement in existing evaluation frameworks is a fundamental challenge in this field.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"9_gender_bias_debiasing\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"9_gender_bias_debiasing\"],\"textfont\":{\"size\":12},\"x\":[9.145990371704102,8.977890968322754,9.214333534240723,9.413126945495605,9.398330688476562,9.037474632263184,9.303510665893555,9.089763641357422,8.957518577575684,9.224438667297363,9.136661529541016,8.973836898803711,9.08991813659668,9.05898666381836,9.171557426452637,9.095734596252441,9.403937339782715,9.312227249145508,9.08510971069336,9.402816772460938,9.370207786560059,9.38476276397705,9.35987377166748,9.252252578735352,9.34296703338623,8.978824615478516,9.326610565185547,9.085888862609863,9.18592357635498,9.506793975830078,9.480792045593262,9.380291938781738,9.005170822143555,8.971504211425781,9.198469161987305,9.458588600158691,9.151717185974121,9.468940734863281,9.441468238830566,8.991645812988281,9.396974563598633,9.102920532226562,9.412025451660156,9.046582221984863,9.031002044677734,9.106647491455078,8.994826316833496,9.08059024810791,9.37531566619873,9.112527847290039,9.036992073059082,8.956121444702148,9.084114074707031,9.339553833007812,9.20207405090332],\"y\":[4.550693035125732,3.9833502769470215,4.495200157165527,4.485685348510742,4.220068454742432,4.3865647315979,4.523543834686279,4.339585781097412,4.6950273513793945,4.428567886352539,4.425017833709717,4.415380001068115,4.463863849639893,4.418537616729736,4.571656227111816,4.405927658081055,4.501498222351074,4.487646102905273,4.535689353942871,4.309328079223633,4.106369495391846,4.467409610748291,4.387577056884766,4.534162521362305,4.4656081199646,4.43990421295166,4.426300048828125,4.619644641876221,4.406549453735352,4.436380386352539,4.4630560874938965,4.466843605041504,4.670903205871582,4.685003757476807,4.216232776641846,4.41265344619751,4.432260513305664,4.457881450653076,4.12162446975708,4.296168804168701,4.363552570343018,4.542664527893066,4.379595756530762,4.563055992126465,4.416816711425781,4.655592441558838,4.341220378875732,4.630074501037598,4.468267440795898,4.541782855987549,4.365445137023926,4.5213470458984375,4.4802374839782715,4.450536727905273,4.442139625549316],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\".Our work is based on simplified settings, i.post-ensemble variance is not evaluated due to the nature of the ens ensemble algorithm.Extended experiments using vanilla bagging ensemble would enable analysis of post-ensemble variance.Further investigation into refining the two stages would help understand the performance of lpms, e.those that are in phase-i but before the peak in figure 1.our results for multirc are based on the instance sampling, however a better sampling technique should be based on stratified sampling based on the ratio of the question types in the multirc set.However, to achieve this, the multirc set needs to be annotated for question types, which is currently missing.Sampling techniques by themselves can become a research topic so that a further decrease of variance due to sampling can be achieved.Although we list these items as limitations, they are also topics for future research within the greater theme of understanding the new bias-prevalence paradigm for lpms.\",\"Since the multi-hop texttableqa problem has only one dataset hybridqa, our model has experimented on only one dataset.This may lead to a lack of generalizability of our model.Transparency and interpretability are important in multi-hop question answering.While our model achieves the best results, the model does not fully predict the reasoning path explicitly and can only predict the row-level path and passage-level path.\",\"Due to the structure of meetingqa, the answers to questions asked by participants are present in the transcript itself, making it an extractive task.Therefore, we do not extensively explore the use of generative models since the predictions do not stick to the sentences in the transcript and could possibly include hallucinations.However, we aim to mitigate hallucinations by using instruction-tuned generative models with suitably designed instructions and enforce a strict exact match criteria for filtering any possible hallucinations.we also do not report zero-shot performance of instructgpt as these models are not freely accessible., but predicting answer spans by classifying each token can be difficult to train leading to slightly lower performance.\",\"There are two major limitations in this work.Firstly, while we showed that query refinement prompts improve the ability of llms to generate long-form answers in a closed book and few-shot settings, open-book systems still perform better even when using a lot less parameters.Doing open-book long-form question answering in llms is currently not trivial due to their input token length limit and the need to use longer prompts when context passages are given.The other major limitation is that human annotators only used the gold-standard answers to check for correctness.5, there can be many ways to disambiguate questions, and therefore the systems can obtain a longform answer that is not different from the gold answers but still should be considered correct.We tried asking annotators to use the internet to check for correctness, however they found it difficult to do so even for a single example.\",\".In addition, we performed a quantitative analysis in terms of efficiency and offered certain suggestions about method selections of open domain question answering.Finally, we discussed possible open challenges and potential future directions of efficient odqa models.\",\"While the previous vqa methods that retrieve from plms all use gpt-3, we do not experiment with gpt-3 in this paper due to the additional cost.We only focus on applying text-generation models as answer selectors, while classification models could also potentially be good answer selectors.The multi-modal clip embedding has already been surpassed by several recent studies and therefore clipcap cannot represent the performance of multi-modal answer selectors.\",\"The main limitation of our methodology is that the training of generative question answering models requires the usage of large gpu resources, which may not be easily available to all researchers.Regarding the performance of the model and quality of the generated answers, our approach can be affected by possible bias induced by the evaluation system we are using.For the experiments in this paper, we only consider datasets from the english language, however, we conjecture that our techniques should work similarly for languages with a similar morphology.Automatic qa evaluation systems do not achieve perfect correlation with human annotations, which indicates a gap with respect to human evaluation.For safety critical applications, human evaluation of generated answers still remains the best means for evaluation.\",\"As shown in table 2, our approach underperforms the state-of-the-art supervised model on the smd dataset, where the supervised sota labels a search instruction for each sample.In addition, the lemonpicked example in table 8 demonstrates that some- times it is challenging for the query generator to learn complicated expressions automatically.Despite our model\\u2019s superiority over all unsupervised methods, these gaps reveal some improvement room of qkconv.another limitation is that our approach suffers from the time-consuming off-the-shelf knowledge selection when given a large dataset and knowledge base.It takes half of the training hours in knowledge selection since it involves heavy computation of retrieval from a large-scale knowledge base and reranking with a cross-encoder.\",\".In this paper, we tested three different architectures - but there are many more to be considered including those that incorporate knowledge graphs which have been shown to improve the richness and semantic correctness of generated questions.There is also room to explore different prompt strategies including a fill-in-the-blank approach which may be more appropriate fo the tqa data.For the attribute models, we used the single task objective of question generation, but it would be worthwhile to explore jointly generating the question attribute and the question itself.Additionally, document level abstract meaning representations with resolved coreferences has been shown to improve the quality of knowledge based question generation.We also recognize that we focused on different context for the input, but not on the wide variety of generation strategies available for this task.On top of the variety of model architectures, we would like to evaluate a greater set of corpora that include additional topics such as history and economics.Reading comprehension is critical to these fields as well and there is limited, if any, research on question generation for these topics.Ideally, we will have educators and students assess the output of our models for factual correctness, relevance, and fluency of the questions generated.This output can then be used to train an instruction fine-tuned model.In order to make a solution that is viable for the classroom, it is critical to think beyond the automated metrics and get real teacher feedback.This preliminary research demonstrates the potential for expanding automated question generation to multiple classroom subjects and the value of incorporating discourse information into different model architectures to produce high quality questions.\",\"In this paper, we focus on the end-to-end accuracy and passage retrieval accuracy for opendomain qa.We have also experimented on the beir benchmark to evaluate our method in the zero-shot document retrieval task.for some tasks, tour obtains significant improvements with a pre-trained document retriever.For example, tour improves the baseline retriever by 11.8% on bioasq and treccovid, respectively, while also outperforming the re-ranker by 2.we plan to better understand why tour performs better specifically on these tasks and further improve it.Tour also requires a set of validation examples for hyperparameter selection.While we only used in-domain validation examples for tour, which were also adopted when training re-rankers, we observed some performance variances depending on the hyperparameters.\",\"Among the six ambiguous and unanswerable problem categories in table 1, our counterfactual example generation approach can not cover the calculation unanswerable and out-of-scope examples generation.The reason is that our approach focuses on the table transformation ways while generating the calculation unanswerable and out-of-scope examples requires conditional nl modification techniques.\",\"In this paper, we introduce the concept of multigranularity temporal question answering and construct a benchmark dataset multitq, which features ample relevant facts and multiple temporal granularities.We also propose a multi-granularity temporal question answering model multiqa, serving as a strong baseline for follow-up research.The main drawback of our data creation protocol is that the question\\u002fanswer pairs were generated automatically, leading the question distribution to be artificial from a semantic perspective.\",\"We showed that our model is efficient in handling conditional qa on long documents with hierarchical reasoning framework.there is promising improvement for our approach by use of more efficient discourse parsers.\",\"A limitation of rata is always assuming the answer is included in the retrieval corpus, which is not always true.When the corpus does not contain the correct answer, the desired behavior is to inform the user that the answer cannot be obtained, but rata will provide a poorly supported answer.This also encourages rata to learn spurious correlations when the retrieved tables coincidentally contain the same value, but does not really support the answer.This problem is especially serious when the answer is very generic and same values by coincidence are common.This is related to the answerable question issue or evidentiality issue for question answering.Course location review date votes beaver island state park grand island, ny ? 0 1 joseph davis state park lewiston, ny 12\\u002f1\\u002f2009 1 0 black diamond dgc south wales, ny 12\\u002f1\\u002f2009 1 1 query: jonnieoh's dgcoursereview profile - disc golf course review gold answer: 12\\u002f1\\u002f2009 bart output: 12\\u002f1\\u002f2009; 12\\u002f2\\u002f2009; 12\\u002f1\\u002f2010; \\u2026 rata output: retrieved table: jchoate7's dgcoursereview profile - disc golf course review for cell-filling on webtables, bart outperforms rata often by either copying values from other rows of the query table or producing values similar to those in other rows.However, as shown in figure 5, rata\\u2019s retrieval is often not helpful.Usually, the information required to fill the query table is not repeated in the corpus, so the retrieved table cannot support the query.As a result, rata is simply retrieving some similar table, and selecting similar values in the tables.\",\"First, our taxonomy of multi-answer mrc instances only considers whether we know the exact number of answers from the questions.In some cases, one might have an imprecise estimate of answer numbers from the question.For example, for the question who are barcelona\\u2019s active players?, one might estimate that there are dozens of active players for this football club.Yet, these estimations are sometimes subjective and difficult to quantify.Therefore, this instance is classified as passage-dependent according to our current taxonomy.second, we did not conduct many experiments with pre-trained models larger than the large-size ones due to limited computational budgets.Generation models of larger sizes show great potential with more parameters and larger pre-training corpora.We encourage more efforts to deal with multi-answer mrc with much larger models, such as gpt-3.\",\"In this paper, we studied paragraph-level qag models, which limits their input up to around 500 tokens, and the same approach cannot be easily applied to longer documents.Also, the answer is an entity or a phrase consisting of a few tokens and the question requires one-hop reasoning, so our models are not able for use in generating longer answers or multi-hop questions.As far as the languages are concerned, the models studies here are english only and to adapt squadshifts qa evaluation in other languages, we need qa datasets to train and evaluate the qag model in those languages.The focus on this paper was on evaluating the quality of generated question-answer pairs.As such, we do not attempt to achieve the best qa model possible, but rather use question answering as an extrinsic evaluation.This extrinsic evaluation could be further enhanced with an intrinsic manual evaluation that we did not perform in this paper.Finally, given computational constraints, our qa evaluation is based on a single model only.Again, the goal here was not to achieve the best qa performance, but we acknowledge than using different models could lead to different results.\",\"Our dqgf still exists some limitations.While our generated data improves performance in diverse questions settings, there is still some noise in the generated data that affects the performance of original single question.In the following, we will give the limitations of our dqgf on its three components.The diversity of the question depends on the diversity of the equations.Our equation generator is based on heuristic rules, resulting that the generated equations are very simple.in the question generator, it can only recognise equations with the operator \\\"+-*\\u002f\\\" due to the limited operator set in our training dataset unbiasedmwp.using the answers of expert model as a criterion for evaluation still exists bias and leads to the noisy data.In fact, we have tried to generate more diverse equations but all are filtered by the current data filter.\",\"Limitation of existing lm prompting schemes, which rely on the static knowledge internalized in parameters; therefore, when such knowledge are incomplete, inaccurate, and outdated, llms may generate factually incorrect answers.To tackle this challenge, we introduced a novel knowledge-augmented language model prompting framework, which augments the knowledge for the input question from kgs directly in the input prompt of llms, with the fact retriever to inject only the relevant knowledge.The proposed framework is completely zero-shot, and versatile with any lms, without additional parameter updates and training datasets.We validated that our kaping yields huge performance gaps from the lm prompting model relying on its internal knowledge, especially with smaller lms, on the kgqa tasks.We believe our new mechanism for augmenting facts from kgs to the lm prompt will bring substantial practical impacts in generating knowledge-grounded answers.\",\"We built buzzer quiz answering systems.However, they do not take into account the time required to respond, and these systems do not have the ability to generate real-time responses, which is essential in actual buzzer quizzes.Additionally, the experiments in this study were conducted only in japanese, and it remains unclear whether similar results would be obtained in other languages.Particularly, english has a significantly different sentence structure compared to japanese, hence further investigation is necessary to confirm whether appropriate results can be achieved.\",\"While good performance has been achieved, there are still limitations in our work.First, though qavgae extracts enhanced features and are fast to train, it is an independent module from the main framework.Second, as a post-processing step, the performance of serr module on simple question is better than that of complex questions.\",\"Though our ultimate goal is to build a versatile qa system that can handle all types of questions, our benchmark mainly focuses on extractive questions \\u2013 those can be explicitly answered by copying from a document in the knowledge source.We start from extractive qa because they cover a wide range of real-world questions and are easier to be automatically evaluated.Although we addressed the issue of long-form qa evaluation with human evaluation and a range of automatic evaluation metrics, there is still much room for improvements in terms of evaluation of long-form text \\u2014 human evaluation can be expensive and non-reproducible while current automatic metrics are not without faults.Furthermore, all questions are in english and possibly collected from english-speaking users.We also use the english wikipedia as our knowledge source.Thus, our models and dataset may under-represent the non-english speakers.\",\"In this study, we used entity type markers to enable a qa model to output multiple answers for re for multiple entity pairs that share an entity in a sentence.However, we have not evaluated other qa models that can output multiple answers.In addition, since entity type markers are incorporated before and after entities, it is difficult to apply the method to nested entity pairs.Qa-based re methods are not computationally efficient since they require multiple qa processes to extract the relation for an entity pair.Furthermore, the question templates in qa-based re affect the performance of re, but in this study, only one template set was used for evaluation, and there is room for improvement.Direct comparison with existing sota is not performed because of the unavailability of the original drugprot test data set.In addition, the sota model is an ensemble of 10 pretrained language models, and we cannot directly compare the performance in a fair setting.\",\"We evaluate hybrank on natural questions, ms marco and trec 2019\\u002f2020 datasets, which focus on english open-domain question answering.Although none of the components in hybrank are specifically designed for english, the verification of hybrank on other languages is limited.Otherwise, there are more general information retrieval tasks involving diversity or broader coverage in the returned results.Considering the possibility of lacking collaborative property, whether hybrank can generalize to these high-coverage retrieval tasks is still inconclusive.As transformer encoder architecture is adopted in the sequence interaction and aggregation, the computation cost would be unacceptable when the length of passage list or number of anchors is too large.This is also the reason why we only conduct experiments with anchor numbers no more than 100.besides, hybrank only uses similarities computed by off-the-shelf retrievers as input features, and thus lacks sufficient interaction between raw inputs.The performance of hybrank may be limited by the capability of upstream retrievers.How to incorporate the interaction of raw inputs into hybrank while avoiding massive computation cost is still an open problem for further investigation.\",\"The proposed clkd is technically applicable to other nlp tasks, but we discuss the effectiveness of the approach for question answering systems, specifically for answer sentence selection tasks.In this study, we put our focus on as2 tasks as the research community has not well discussed or proposed multilingual as2 tasks\\u002fdatasets.We also find that using only english teacher models is another major limitation of this study.However, choices of teacher models in the proposed clkd are not limited to english models.It would be interesting to discuss the generalizability of the proposed clkd beyond as2 tasks, but we note that such\",\"In this paper, we focus on the hybrid qa task, where the answers to most questions can be extracted from cell values in tables and linked passages using a reading comprehension model.Although tacr performs well in cell selection, one of its limitations is that it lacks numerical reasoning ability across different cells, such as counting and comparing.another limitation of tacr is that it shows a strong ability in column selection while performing relatively worse in row selection.\",\"Our dataset creation process - introducing unanswerability into a dataset of answerable kb questions by deleting kb elements - limits the nature of unanswerable questions.All of these become answerable by completing the provided kb.Questions may involve false premise, for example, c.manning works at which european university?, or may not even be relevant for the given kb.Complete training and inference for each model with our dataset size takes 50-60 hours.As a result, generating multiple results for the same models in the same setting was not possible and our results are based on single runs.However, using multiple runs with smaller dataset sizes we have seen that the variance is quite small.Also, the dataset creation involves sampling kb elements for deletion and as such the generated dataset is one sample dataset with unanswerability.This is unfortunately unavoidable when creating one benchmark dataset.\",\"Our work proposes a new qg framework, namely skillqg, to frame the comprehension skill required by a question and generate the corresponding comprehension-oriented questions.The limitations are three-fold: firstly, we propose a new skill-based schema for the comprehension nature of questions and map the existing annotations on narrative elements of the fairytaleqa dataset to it and conduct our experiments.This kind of mapping might not reflect the required skills accurately since a narrative element can cover more than one comprehension types.Furthermore, although our proposed skill-based schema is drawn upon general text comprehension, skillqg is only verified on the fairytaleqa dataset and lacks the analysis on generalizability.However, identifying skills and correlations with comprehension skills on new datasets can be challenging because skillqg may struggle with the input passage with a relatively simple discourse structure, which usually does not contain complicated relations.One remedy to this issue could be collecting a new qa dataset with the annotations following our proposed schema.However, the breakdown analysis of qa performance demonstrates that skillqg can strengthen all of the comprehension capabilities, especially the challenging ones.As a result, generating questions that are matched with the current comprehension capabilities of the qa model and co-evolving the qa system and corresponding qg system, could be two interesting research topics.Last but not least, our skillqg is built on the plms of general domains, ignoring the domainspecific and multilingual application.The backbone plms are also shown a biased representation, such as race and gender.\",\"This paper introduces two heuristic approaches to filter noisy feedback data.Although we showed that these simple methods improve the performance of qa models, they have various limitations and they represent only an initial step for future re- search on real feedback data.The core of the relevance filtering is based on the assumption that correct feedback occur when the model and the user agree on the labels.This approach may introduce a selection bias towards tuples associated with \\\"simpler\\\" q\\u002fa pairs, which are already well understood by the model and thus potentially ineffective for training.Although the model can easily discard q\\u002fa pairs whose feedback are clearly different, the risk is that uncertain pairs close to the classification boundary are penalized and easily filtered as they will receive a reliability score close to 0.Regarding the collaborative approach, the main limitation concerns the clustering strategy adopted to aggregate questions.On one hand, we want to reduce as much as possible the number of clusters such that we have a sufficiently high amount of feedback per cluster.This makes the proximity computation between users and the voted feedback vector robust.On the other hand, the clustering may introduce additional noise by aggregating different and nonequivalent questions into the same cluster.This aspect may reduce the reliability of the voted feedback vector.for this reason, we only described the high-level approach of integrating feedback and we showed the impact on public benchmarks.A harsh limitation is caused by the private nature of the customer data, which cannot be released for public research.\",\".additionally, 3-class nli is not sufficient to capture all the natural language relations that might be needed to verify an answer.As such more challenging datasets in other settings and more granular nli settings should be attempted.Another limitation involves answer ranking and the associated computational cost.The main reason we did not test answer ranking in extractive qa is that we did not generate diverse outputs, but another reason is that such a procedure grows prohibitively expensive as the domain becomes more open.In a fully open domain, ranking would require a quadratic evaluation for each context passage against each reformulated answer candidate.\",\"Inapplicability to reference-free evaluation: since our mre supposes that there is an available reference question to be augmented , it is not applicable to reference-free question evaluations such as qrelscore and rquge.Inapplicability for answer-unconditional qg frameworks: mre can\\u2019t be applied to answerunconditional qg frameworks because it only augments the reference question by paraphrasing without considering other possible questions of supposing other answers.Large computations: to generate multi-reference questions, our method requires inference of large language models, which results in huge computational costs.Therefore, this can become burdensome as the test dataset grows.\",\"There are three main limitations of our approach: our model cannot handle negation operation.the modeling for query representation and logical operators is too simple.the training process cannot be parallelized well, which is a common drawback of qe models, as qe models have to predict node representations one by one.\",\"Limitation, we reformulate re into multiple-choice question answering with the purpose of leveraging a task that is widely cov- ered in instruction-tuning datasets like qa, instead of re, which is barely present in these datasets.Comprehensive experiments demonstrate that our qa4re framework unlocks the power of llms as zero-shot relation extractors, especially for two recent llms.We also conduct thorough experiments to explore the robustness and few-shot effectiveness of our method as well as study in what llm training scenarios it is most effective.additionally, we plan to continue exploring this line of work by leveraging our qa4re framework for other llms such as the opt-series and palm , which are not included in this work due to the limited computational resources and\\u002for access.\",\"., multiple subquestion-solution pairs can be sampled, and using majority voting, all pairs leading to the most frequent answer can be used to distill knowledge into the student models.Also, due to computational budget, we used a single prompt to compare the cot and socratic cot and using more prompts might lead to a fairer comparison and better results.We leave these experiments for the future.\",\"While our work tries to focus around reasoning over both fine- and coarse-grained cross-document relationships, qamden, the resulted pre-trained model, might still suffer from factual consistency errors while generating information given a query, and there is no guarantee that it will always generate factual and reasonable content without any further fine-tuning.The qasem question generation model that we used may also have been a source of these problems.There is a possibility that qasem produces inadequate questions that could harm the pre-training process of the model.An attempt was made to filter out noise using a question model, but the results were inferior to non-filtering.Consequently, if the model is not fine-tuned, inconsistency may occur more frequently.In addition, by using the newshead corpus as the pre-training data source, we assume that it is comprised of high quality documents.We also take into account the fact that newshead is limited to documents in the news domain, while some of the benchmarks used for evaluating qamden include another topics of interest.this is crucial for productizing models like qamden in interactive multi-text applications and semantic search applications which are gaining attraction nowadays.Finally, the resulted model qamden was pretrained on sets of related documents, by answering questions that matched their content.As in an out-of-domain scenario, qamden\\u2019s use over sets of documents that are not related, or over single documents, might be unexpected.\",\"The experimental results suggest the possibility of our mtl-based qe approach being biased towards the word-level qe task, as the jointly trained qe models show better performance improvements for the word-level qe task as compared to the sentencelevel qe task.Further, we also observe that our approach does not work well for language pairs with english as a source language.The qualitative analysis of the english-marathi mtl-based qe model shows that the model performs poorly when inputs are in the passive voice.Our multi-pair setting experiments use all seven language pairs.We do not consider properties like the similarity between the languages, translation directions, etc.so it may be possible to achieve comparable performance using a subset of languages.We choose the nash-mtl approach for mtl-based experiments because it has been compared with around ten other mtl techniques and it has been shown that the nash-mtl approach outperforms them on different combinations of the tasks.In the current work, we have not experimentally analyzed how the nash-mtl approach gives better improvements than the ls-mtl approach.\",\"Limitation of this study is that we used only the squad dataset in our experiments.The squad dataset has often been criticized because it is overly dependent on the similarity of question\\u002fanswer sentences rather than on human-type reasoning, meaning it requires only superficial read- ing skills.Thus, examining the effectiveness of our proposed method by applying it to various other datasets will be an important future task.4, we examined only 60 question\\u2013answer pairs generated through the proposed model from ten randomly selected reading passages.The relatively small scale of the experiment is due to the high workload required for people to carefully evaluate the various properties of a large number of questions.although the present study used only five qa systems, the use of a larger number of qa systems with different characteristics is expected to improve the accuracy of question-difficulty estimation and provide difficulty estimates with finer granularity.Therefore, examining the effects of increasing the number and variability of qa systems will be another future direction of this research.We also need to confirm in greater detail whether qa systems can be substituted for human learners.A comparison between irt-based question difficulties calibrated from the responses of qa systems as well as human learners might be a plausible approach.Another future goal is to develop a method of transforming the scale of the irt-based difficulty, estimated based on qa systems, into a scale appropriate for a population of target learners.Such a scaling adjustment is expected to be achievable by using equating, which is a well-established technique in irt.\",\"Our main focus in this work is limited to factoid information-seeking questions that typically prompt short answers.However, lexical matching is adopted by more complicated forms of qa that require complex reasoning.More precisely, qa tasks such as multi-hop reasoning , discrete reasoning , and causal relations also warrant similar systematic analysis as studied in this paper.\",\"Inappropriate initial user questions can negatively affect ner performance.If they are not proper, the qa model returns incorrect phrases, and the phrase embedding queries generated from them will also be erroneous.The absence of a component for controlling this error cascade in our framework should be addressed in future studies.In addition, our method is dependent on the phrase encoder of densephrases.Because the phrase encoder is a general-purpose model trained on wikipedia-based datasets, its capability may be limited for domain-specific entities.In fewshot ner, the phrase encoder can be sensitive to the quality of given example sentences.Future studies should thoroughly analyze the effect of the phrase encoder\\u2019s performance on the resulting ner datasets and ner performance.\",\"While our work is consistent with the key aspects of questions under discussion, we do not attempt to take into account all aspects of this broad framework.While such relationships are potentially useful, with question stacks, the annotation task becomes much more expensive; currently, no existing dataset is available to train parsers in this fashion.We applaud the development of tools such as treeanno to aid annotation.Additionally, because questions are open-ended, they are inherently subjective, which adds substantial challenge to modeling and evaluating stacks.The subjectivity of qud analysis also means that there is no single \\u201cright\\u201d structure.This is in contrast to coherence structures that more rigorously define their structures and relation taxonomies.to evaluate our parser, we developed a human evaluation scheme.\",\".First, our method needs to decompose questions into a symbolic representation, but such representations are hard for humans to comprehend, and therefore this decomposition mechanism is hard to be trained with human annotation.A promising direction is to leverage pre-trained language models such as chatgpt 2 to automate this decomposition step, leveraging chatgpt\\u2019s internal knowledge of decomposing a complex question into sub-questions.Second, the execution of the zero-shot nmns is conducted in a deterministic manner, leading to high risks of error propagation if the reasoning chain gets longer.\",\"Despite showing non-trivial improvements in the multi-hop capabilities of t5 models, our work has multiple limitations.Restricted to 2-hops first, we chose 2wikihopmultiqa as our primary dataset since it uniquely maps each question to a chain of triples that contain the precise, noiseless single-hop knowledge required to answer the question.However, this comes at the cost of our analyses only being restricted to 2-hops who suggest 3-and4-hop questions to be too convoluted to understand even by native-speakers).Nonetheless, our random walk training method is general by definition, and can be extended to multiple hops, though its effectiveness on qa tasks requiring more than 2-hops of reasoning remains to be measured.Knowledge graph size our focus in this paper was to allow models to chain together their internalized knowledge in order to answer complex 2- hop questions.However, this critically requires them to possess the world knowledge required to answer the questions, for which we had to memorize the kg constructed using the structured triples provided in the dataset.This trade-off between focusing on knowledge composition vs.fully encoding world knowledge restricted our kg to be small in size , which could be impractical in most real-world applications.Lack of diverse qa tasks finally, we were unable to consider popular datasets with cbqa versions such as triviaqa , naturalquestions , etc., due to their lack of links from questions to structured knowledge.additionally, this would also overcome the above limitation , as it would substantially increase the amounts of entities and relations to be encoded within models.Implications for larger models although we show clear improvements in triggering 2-hop reasoning in the largest t5 lm , with 11b parameters, contemporary work has shown that multi-step reasoning capacities naturally emerge in lms that are two or three orders of magnitude larger.However, these lms benefit from examples in-context , and therefore it is unclear whether our methods can improve such models\\u2019 capacities even further.We have not tested such lms in our work, due to resource limitations.\",\"We study a limited scope of long-form answers.The questions are either drawn from search queries or from community forums.In the real world, we will encounter many more diverse forms of long form question answering, such as answering questions in education or commercial settings.We only cover the english language, and thus our questions are topically limited to english-speaking culture.Our evaluation of long-form answers is stationary.Annotators are provided a pre-generated output from the model without being able to interact with the model over multiple rounds.\",\"The pne value decreased in all datasets.While this is not problematic for question generation, where the presence of a named entity is not always necessary, it does pose an issue for nlg tasks where the inclusion of named entities is important.In these cases, we recommend using alternative techniques that we have proposed.Additionally, using delexicalization and over-generation in our approach leads to a high training and inference time.\",\"Our work still has some limitations: 1) due to the lack of research about the bias mitigation of qe, there is only one directly related work in this area, which serves as the main baseline in our experiments., therefore our results can not compete with the best results of the wmt qe evaluation tasks.3) also, our method requires reference as the positive sample.Although most qe data includes reference, there are still chances that the qe data is annotated without the absence of reference, and our method would be hard to apply to such cases.\",\"We see two main limitations in our study.Primarily, given the clear trend of larger generative models producing higher quality answers, an obvious question is to investigate whether this continues to be the case indefinitely, or whether it saturates after a critical amount of parameters.Despite this, due to hardware restrictions, we were unable to experiment with models larger than bart-large.Additionally, considering that the field of ambiguous qa inherently requires complementary pieces of evidence, there is no doubt that diversification methods are bound to yield better results in terms of disambiguation quality.In this work, however, we limited ourselves to using a typical neural retriever, shifting our focus toward the factuality and the fluency of the generated answers.\",\"The study has several clear limitations.Firstly, the training and validation datasets used in this study are still relatively small.A larger dataset would give more robust results for comparing different encoders.Additionally, the experiments were only conducted on the ability to generalize to unseen entities and not on the ability to generalize to unseen sketch types, which is also of key importance when addressing low resource ckbqa.Moreover, the methodology used in this study relies on annotated question-program pairs, which are expensive to collect.Learning only from question-answer pairs or even a question with an indicated difficulty based on whether the model was able to answer the question, could be more easier to collect.While the models achieve high accuracy on most of the knowledge base components, overfitting can occur at different stages during training, leading to high accuracy for one component at one training step but poor accuracy for another component at another step.\",\"There are two known limitations of the squadlike annotation approach we used in this work: it can result in higher lexical-overlap between the context and question pairs.It leads to proportionally fewer truly information-seeking questions.The main reason is that the annotators create questions after reading a paragraph, which can induce bias towards recycling words and phrases observed in the context.Our annotation guidelines advise against this, but it is difficult to avoid entirely.Several approaches have been proposed to mitigate this issue, such as natural questions and tydiqa.However, they tend to be expensive, and comparatively, the squad-like method is resource efficient and a more suitable starting point for low-resourced languages such as tigrinya.\",\"Limitation to this work, which is related to time complexity and speed performance.Since every instance is transformed into multiple qa instances, it may take a relatively long time to process a document.limitations there are two primary limitations of the system presented in this work.First, each set of questions we use for training the qa model is designed specifically for the dataset we trained our model on.While we provide a set of questions for each of the two common trc datasets, we believe that training the model on other datasets may require rewrite of the questions.this may increase the overall inference time and pose a practical limitation which needs to be carefully considered.\",\"As previously mentioned, our study is focused on the generator component of a qa pipeline and ignores the retrieval task.In the experiments presented in the paper, we have used gold facts to report the results.For certain datasets such as hitab and multihiertt which were designed for complex tabular structures, this might simplify the end-to-end challenge.In future studies, we hope to explore whether countercomp can enhance the performance of retrievers.The datasets used in our experiments were curated using enterprise documents such as financial reports or other corporate disclosures.Quantitative qa over these reports often involves multi-step reasoning that is limited to linear arithmetic operations such as addition, division, averaging, etc.A completely open-domain qa engine might need to cover more complex operators.Lastly, we designed countercomp to leverage existing data by sampling from the training set.Nevertheless, combining countercomp with augmentation-focused methods such as cad might lead to more robust models.\",\".5 we demonstrated that our question rephrasing model works well for producing fluent questions that reduce ambiguity.Furthermore, in table 3 we showed that the model\\u2019s representations contain information about the underlying question being asked, even though this information is not directly present in the training data and we do not include any supervision from our dataset.given an ambiguous question identified and a set of answers to it from a vqa model, our model could be used to rephrase the question according to each answer.Just as a presenter will often rephrase a question from the audience, the model might present the user with the rephrased question it is actually answering, which would result in better interpretability.This improved interpretability might teach users how to interact with the model.\",\"Although we carefully designed open-wikitable for complex open-domain table qa, there are some limitations since it is based on the existing datasets.First, ambiguous or erroneous samples from the original wikisql or wikitablequestions dataset may still lie in our training and validation set.2, most of the equivocal samples were attributed to the ambiguity of the original question and excluded from the test set, but not removed.Second, unlike semantic coverage of the questions is extended by decontextualization and paraphrasing, the coverage of the question remains in that the answer and logic to derive the answer in each question is the same.Still, openwikitable demonstrates the potential for further research on open-domain qa over the table.\",\"This work focuses on english qa datasets only.Similar techniques should apply in other languages as well; however, we did not evaluate them.The augmentations generated are difficult to validate for yes\\u002fno questions for the few-shot method.Moreover, it can be challenging to generate these augmentations if access to large lm is unavailable.However, under those scenarios, data in the target domain should be annotated, which ideally would perform better than the few-shot setting.Our models also suffer from similar problems as llms, like hallucinations, misinformation, etc.\",\"We identify the major limitation of this work is its input modality.Specifically, our model only considers textual inputs, ignoring question answering tasks in vision and audio.A multi-modal question answering model under realistic open longtailed scenario is worth further exploration.Fortunately, through multi-modal pre-training models and question answering methods , we can equip our model with multi-modal question answering ability.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"10_qa_question_questions\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"10_qa_question_questions\"],\"textfont\":{\"size\":12},\"x\":[10.398134231567383,10.425617218017578,10.17101764678955,10.027867317199707,10.19906997680664,10.095967292785645,10.271556854248047,10.440425872802734,10.051057815551758,10.39487075805664,10.74063777923584,10.417082786560059,10.351696968078613,10.630949974060059,10.363282203674316,10.320760726928711,10.299310684204102,10.089219093322754,10.177507400512695,10.357734680175781,10.28139591217041,10.258552551269531,10.335556983947754,10.317266464233398,10.378898620605469,10.687399864196777,10.368781089782715,10.475990295410156,10.404289245605469,10.331825256347656,10.349339485168457,10.153546333312988,10.380175590515137,10.131158828735352,10.369507789611816,10.31935977935791,10.389159202575684,10.417340278625488,10.389838218688965,10.25548267364502,10.402857780456543,10.227521896362305,10.06610107421875,10.358951568603516,10.32805061340332,10.307161331176758,10.381868362426758,10.306058883666992,10.432424545288086,10.389165878295898,10.692686080932617,10.279783248901367,10.107221603393555,10.330161094665527],\"y\":[-0.34996992349624634,-0.5050549507141113,0.041329462081193924,0.006683920044451952,-0.3129766881465912,-0.09294040501117706,-0.20105589926242828,-0.31288886070251465,-0.4144154489040375,-0.369363009929657,-0.3336394429206848,-0.5059787034988403,-0.43823927640914917,-0.3083164691925049,-0.37619268894195557,-0.4224071800708771,-0.2887267768383026,0.12244793027639389,-0.057289011776447296,-0.29274410009384155,-0.3609761595726013,-0.35920703411102295,-0.2254236936569214,-0.1507423222064972,-0.5837980508804321,-0.26967307925224304,-0.6088269352912903,-0.34517398476600647,-0.3163365423679352,-0.3466590642929077,-0.335929274559021,-0.09458557516336441,-0.36536186933517456,-0.37174493074417114,-0.2963405251502991,-0.394572377204895,-0.5873947143554688,-0.18053658306598663,-0.1402733027935028,0.023438766598701477,-0.5090862512588501,-0.008045713417232037,-0.3614562153816223,-0.34255579113960266,-0.22186455130577087,0.01887206919491291,-0.2637210190296173,-0.35443785786628723,-0.35504627227783203,-0.13553161919116974,-0.27407190203666687,-0.12353327125310898,-0.13722601532936096,-0.27904829382896423],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"To some extent, dspert pursues performance and interpretability over computational efficiency.The major computational cost of a transformer encoder is on the multihead attention module and ffn.For a t -length input and a d-dimensional transformer encoder, the per-layer complexities of the multihead attention and ffn are of order o and o, respectively.When the maximum span size k \\u226a t , our span transformer brings additional o complexity on the attention module, and o complexity on the ffn.Empirically, training a dspert consumes about five times the time for a shallow model of a same scale.However, this issue can be mitigated if we use fewer layers for the span transformer.As noted, we empirically choose the maximum span size k such that it covers most entities in the training and development splits.From the perspective of f1 score, this heuristic works well, and dspert performs favourably on long-span entities as long as they are covered.However, the entities with extreme lengths beyond k will be theoretically irretrievable.\",\"The major limitations of our work are as follows: we show that the neuron structure of moe reveals the presence of modularity in pre-trained transformers.However, the moe structure is not the only possible modular structure.To better understand the modular structure of transformers, we need to explore more types of structures.For example, the number of neurons in each module could be different, and the modular structure could be hierarchical, where modules are grouped into larger modules.We study three typical functions for language processing: semantic function, knowledge function, and task function.There are many other functions that could be studied, such as the syntactic function, discourse function, etc.Moreover, our categorization of functions may be not suitable for pre-trained transformers because there are some overlaps between studied functions.A new transformer-based function categorization may be needed.We transform t5 into its moe version to study its modular structure while not all dense pre-trained transformers can be studied in this way because the adopted moefication technique can only transform relu-based transformers.Studying the modularity of other dense pre-trained transformers, such as bert, is also important for future research.\",\"Since the proposed method scales probability distribution, not shift, the proposed idea does not change an output when coupled with a greedy decoding strategy.Greedy decoding simply takes argmax of probability distribution at each time step, and hence, the output with or without the local temperature scaling will not be changed within greedy.Therefore, the proposed idea is required to be utilized with a beam search, or a variant of it.\",\"Since our method constructs on the multimodal transformer, it cannot be migrated to the dualstream model.Experiment results show that cfsum can achieve comparable performance with strong baselines.But it still cannot surpass the sota of some dual-stream large models.\",\".First, the performance of the hierarchical transformer retriever is limited since the utterance-level transformer is trained from scratch only on our small-scale dataset due to limited time and computational resources.pre-training can potentially improve the performance of the retriever and further improve 8412 the generation quality.Second, the two types of retrieved responses in the recap-mixed model are encoded with the same encoder.However, intuitively, the two types of responses should contribute to generation in different ways, so treating them the same way might harm generation performance.This is also reflected in our results.Even though the recap-mixed model shows improvement from both types of retrieved responses, the improvement is weaker than that on each separate model.\",\", we think the benefits of our work overshadow its limitations.We provide a simple approach and a new set of tools to interpret transformer models and compare them.The realm of input-independent interpretation methods is still nascent and it might provide a fresh perspective on the internals of the transformer, one that allows to glance intrinsic properties of specific parameters, disentangling their dependence on the input.Moreover, many models are prohibitively large for practitioners to run.Our method requires only a fraction of the compute and memory requirements, and allows interpreting a single parameter in isolation.Importantly, our framework allows us to view parameters from different models as residents of a canonical embedding space, where they can be compared in model-agnostic fashion.\",\"Limitation of not utilizing information beyond the training sequence length.Fortunately, this is overcome by our new relative positional embedding, sandwich, which is simplified from the earliest proposed sinusoidal positional embedding.Finally, sandwich demonstrates a log-decaying temporal bias pattern similar to that previously seen in the design of kerple and t5, and such pattern is likely to be the secret to successful length extrapolation.Together these findings supports more effective design of future extrapolatable transformer language models.Limitations although sandwich, kerple, and t5 use information beyond training sequence length, their receptive fields still highly favor the most recent tokens.While this recency bias is beneficial to the modeling of human-written text, it is problematic in other scenarios.Let us consider the task of parity prediction: a model needs to predict whether a bit string has an even or odd number of ones.For example, the parity of [1, 1, 0, 1] is odd and the parity of [1, 0, 1, 0] is even.Unlike human-written text, every single bit is equally important.Transformer language models with current rpes still struggle on this simple task.Its difficulty can be explained by the recency bias effect that we described.\",\"Although lclr shows great potential for unifying the slu decoding process, existing slu models experiment on a set of predefined labels , and our lclr can handle the case of missing predefined labels in the train.It is interesting to try to perform lclr on a more challenging task of detecting out-of-domain detection where unseen intents\\u002fslots are not available.\",\"In this work, we found that the implications of the geometric properties of layernorm affect mainly small models and are less evident for larger models.We hypothesize that with a large hidden dimension, a transformer model can find other solutions for computing \\u201cmajority\\u201c using gradient descent and is, therefore, less dependent on the projection component.Further, we believe that the scaling component is less useful for high dimensional models, since with higher dimensions, it is less likely to encounter a set of vectors where some of them lie within the convex hull of the others.Therefore, we encourage the community to use layernorm before attention layers, especially for small models that operate on long sequences.\",\"In this paper, we indicated that the vanishing gradient problem, caused by layer normalizations, makes the training of deep post-ln transformers unstable.We proposed the b2t connection to mitigate this vanishing gradient problem.However, the proposed b2t connection does not perfectly prevent the vanishing gradient, as shown in figure 3.therefore, the vanishing gradient might harm the training in extremely deep transformers even if our b2t connection is used.In addition, this study depends on empirical observations.In particular, we provided little theoretical justification of the reason for post-ln outperforming pre-ln when training succeeds.because the behavior of deep transformers in various situations is not fully understood, we believe that it is important to provide empirical findings for our research field to progress.however, we are confident that we conducted sufficient experiments to verify our contributions.\",\".\\u2022 the impact of layernorm, language tags, and residual connection settings on zst was analyzed in this study.However, other factors, such as the number of layers of the transformer model, may also have an effect and should be further investigated.\",\"Limitation of subtask a one significant limitation of our transformer-based model is that it does not directly consider length during its generation process.This often results in the production of overly verbose summaries.In contrast, models like openai\\u2019s gpt-3 and gpt-4 have much longer input and output limitations, allowing them to handle more extensive text samples more effectively.It is worth noting that our model was trained on the samsum dataset, which has longer texts compared to subtask a.consequently, our model struggles to adapt to the shorter length requirements of subtask a.moreover, the training dataset for subtask a is relatively small, which further complicates the model\\u2019s adaptation.Future exploration should look at how to constrain the conciseness of generated summaries, which may involve reconsidering the generation method chosen or examining other techniques to promote brevity.Developing methods to better control the length of generated summaries is essential to improve their relevance, coherence, and usability in real-world applications.Length limitation of subtask b for subtask b, it is challenging to achieve reasonable results using fine-tuned models.In reality, this task is more representative of real-world scenarios, where inputs and outputs are considerably long, and the output is expected to maintain a specific structure and format.Thus, we see the main advantage of using contextual examples lies in their ability to guide the structure, style, and length of the desired output.We believe that openai\\u2019s llm is wellsuited for similar real-life scenarios, provided that it is given an appropriate context.In such cases, its performance will significantly surpass that of fine-tuned transformer-based models.Factual inconsistentcy while our study did not specifically investigate the following issues, we noted several factual errors that occur in summaries.A previous study has shown that llms also exhibit a noticeable occurrence of attribute errors and misinterpretation errors.Prompting we find that prompt template and demonstration example selection both have a substantial impact on results.Using more prompt examples for demonstration improves significantly.We acknowledge that we did not explore different selection strategies, such as semscore, lmscore, and tlength, which involve using top-ranked examples.These strategies have been shown to potentially improve the quality of the generated summaries by selecting more effective prompt examples.While our current approach did not incorporate these strategies, we recognize that exploring and incorporating better prompt examples could potentially yield improved results.This is an area that warrants further investigation and experimentation to enhance the performance of our models in future iterations of the study.Data privacy both gpt-3 and gpt-4 are not local models; we utilize openai\\u2019s api to run these models, which actually violates data protection laws such as hipaa.Ensuring data privacy during fine-tuning or testing is of paramount importance.We have not taken this aspect into consideration.\",\".However, there are also many recent fully transformer vlms achieving comparable or better performance.Therefore, applying our distilling then pruning framework on other state-of-the-art vlms can be interesting.Also, we do not apply quantization or matrix decomposition, which are prevalent model compression techniques.\",\"We list the main limitations of this work as follows: \\u2022 limited nat models.The conclusions in this paper are drawn from two representative nat models, which may be not necessarily well suited for other nat models.The main reason is that experiments on six wmt benchmarks have cost a large number of gpu resources.This work totally costed 40,000 gpu hours , because 1) large numbers of experiments; and 2) scaled neural networks and training data require more gpu resources.However, we hope our empirical results can help other researchers to reduce the expense of redundant model training.\",\"As previously mentioned, teast maps relations onto the corresponding archimedean spiral timeline and transforms the quadruples completion to 3th-order tensor factorization.It is required to store the values and this slightly increase the space requirement and training time in the embedding learning process.Among all the baselines, tcomplex, tntcomplex and telm are all tensor factorization based models.Table 4 compares training time and space requirement between our model and baselines on icews14.Tcomplex is the smallest model and takes the minimum training time.Compared with tcomplex, our model is about 4.6% bigger than tcomplex, and takes 21.\",\"Apart from all the advantages that our work achieves, some limitations still exist.Firstly, in this work, we investigate the efficacy of applying our proposed depa approach on the representative vanilla nat, the highly competitive fully nat model glat and current sota ctc-dslpmt for fully nat models, but we have yet to apply depa to iterative nat models, such as imputer , cmlm , and levenshtein transformer.Hence, the effectiveness of depa on iterative nat models still needs to be verified.Secondly, we have not yet incorporated reranking approaches such as noisy parallel decoding into depa.Thirdly, our proposed method fbd requires multiple additional training phases before nat training, resulting in longer training time and using more gpu resources.last but not least, nat models have limitations on handling long text.They suffer from worse translation quality when translating relatively long text.\",\"Our work is limited as it has not explored the effectiveness of our implicit memory transformer in other tasks outside of simulst, such as asr.We have also not explored the impact of our implicit memory left context on alternative blockprocessing-based transformer models.Furthermore, extensive ablation studies could help showcase the potential of the implicit memory transformer.\",\"Despite the strong performance of the proposed attenwalker.There is still large room for improving efficiency.For example, the time cost of our method is still high.Since we need to search for all transformer layers and heads to find potentially re- lated spans, the dataset construction could be quite time-consuming.\",\"Our work is one of the first to perform a detailed empirical investigation of transformer guided chaining but is clearly preliminary.The following are some key limitations: - evaluation of interpretability: a fair evaluation of interpretability is not straightforward.In this paper, we reported results from a preliminary study with limited human labor.- analysis of negations: logicnli dataset uses negations in the facts, rules and statements but it is difficult to disentangle them for a fair investigation.Hence, we were unable to rigorously analyze the ability in handling negations.- evaluation on real-life data: our reported work focused on a synthetic dataset.For a more rigorous evaluation, it is imperative to consider more datasets including real-life ones.\",\"Our method shows impressive performance but relies entirely on beam search during inference.However, it is well known that beam search is a computationally expensive algorithm.With the beam size of 50, the latency increases from 3.6 times to 7 times compared to greedy decoding.In addition, the re-ranking process causes another latency.Therefore, it may not be suitable for real-world dst scenarios.potential directions may include reducing the current two-step pipeline to an efficient one-step process by employing a novel objective function, using data augmentation, or changing the sequential decoding process to a nonautoregressive approach that can be applied in a parallel manner.\",\"Let us begin with the obvious limitation: axomiyaberta only works on assamese.In addition, since assamese comprises a number of dialects and we trained on internet-sourced data, we have no clear evidence regarding which dialects axomiyaberta is most suited to or if it performs as well on non-standard dialects.Axomiyaberta did not perform all that well on wikipedia title selection, compared to other transformer-based models.Our best result is on par with xlm-r and close to indicbert-base, but well below mbert performance.We hypothesize that the amount of wikipedia training data in mbert is a cause of this, but we find that phonological attention makes a big difference in axomiyaberta\\u2019s performance.Nonetheless, the reasons behind this subpar performance, and whether axomiyaberta can be improved for this task without, say, overfitting to wikipedia, need further investigation.\",\"In our paper, we presented existing and novel training-free nas metrics for rnns and transformers.Benchmarks are required to evaluate the effectiveness of these metrics on various architectures.While there exists a robust benchmark for rnn architectures , there is none for transformer models.Thus, we had to create our own nas benchmark.For our work, we were limited by the computational resources available to us, so we were only able to pretrain and finetune 500 models for our nas bert benchmark.A larger sample size would give a more accurate evaluation of the training-free nas metrics.Furthermore, we only investigated the flexibert search space.While flexibert has a diverse search space, having heterogeneous layers and alternative attention operators, the variation between possible architectures is limited and still dependent on the linear paradigm of bert.Alternative transformer search spaces using cell-based methods, such as those presented in \\u201cprimer\\u201d and \\u201cautobert-zero\\u201d , do not have this limitation.We were ultimately unable to investigate the performance of training-free nas metrics on this type of search space, as there are no available benchmarks for these search spaces, and their greater variability necessitates a copiously large sample size that is well outside our computational capabilities.Another limitation is that we only evaluated the effectiveness of the presented metrics on encoderonly transformer architectures, and not encoderdecoder or decoder-only architectures.Furthermore, while the training-free nas metrics are dataagnostic, the benchmarks they were evaluated on were only trained and evaluated on english datasets and tasks.\",\"Despite the promising results achieved by our proposed method, there are certain limitations that need to be noted.Firstly, our approach relies heavily on the use of transformer models, which can be computationally expensive to train and run.Furthermore, our method is not adaptable to nontransformer architectures, as it relies on the specific properties of transformer-based models to extract alignment information.Lastly, our method is based on the assumption that the decoder will attend to those input tokens that are more relevant to predicting the next one.However, this assumption may not always hold true in practice, which could lead to suboptimal alignments.In conclusion, while our proposed method presents a promising approach for cross-lingual amr alignment, it is important to consider the aforementioned limitations when applying our method to real-world scenarios.Future research could focus on addressing these limitations and exploring ways to improve the performance of our aligner for languages other than english.\",\".Therefore, utilizing our proposed bidirectional transformer reranker to rerank candidates from a pre-trained vanilla seq2seq model requires additional pre-training steps, which cost both time and gpu resources.Because the btr masks and predicts only 15% of the tokens in eq., it requires more training steps than a vanilla seq2seq model.In addition, during fine-tuning, the btr also requires additional atrain negative samples, which makes the fine-tuning longer.Furthermore, tuning atrain will be inefficient if the training is slow.In other words, training an effective btr requires much more time than training a vanilla seq2seq model.As a reranker, the performance of the btr depends on the quality of candidates.There is no room for improvement by the btr if no candidate is more grammatical than the original selection.\",\"Although our approach exhibits great speedups in encoder-only settings, it doesn\\u2019t yield as impressive speedups in encoder-decoder setting.This is due to the autoregresive decoding steps in the decoder, that has to be conducted sequentially.Accelerating that with dct requires to incrementally update dct outputs step by step based on outputs of pre- vious timesteps, which is theoretically possible but not easy to optimize its efficiency.\",\"The proposed algorithms allow to speed up an existing model out-of-the-box, without any modification or retraining.However, there are some considerations to bear in mind when using parallel decoding in order to have a speedup in terms of wall-clock time.Firstly, as the name implies, the method executes the decoding phase in parallel.Therefore, to appreciate the speedup one should be able to run computations in parallel.Using parallel decoding without parallel resources or parallel-optimized software may increase wall-clock time due to overheads, leading to a waste of computation.the reported wall-clock time results are thus to be considered within the scope of the experimental setup proposed in this paper and they may vary depending on the underlying hardware and software.Secondly, the method allows speedup of the decoding by scaling on parallel resources.This implies an additional computational cost during the inference phase to achieve a speedup.While using parallel decoding, one should consider a trade-off between the desired acceleration and the utilization of computational resources.Thirdly, since our method performs the decoding in parallel, as for nat systems, it is difficult to combine it with beam search.Beam search is inherently a dynamic programming algorithm and it is not possible to efficiently maximize the joint probability of the large search space without using sequential intermediate computations.We better explain this aspect in the next paragraph.beam search is widely employed to enhance the translation quality in mt as well as in other domains such as audio.However, it is an inherently sequential procedure that stores partial joint probabilities of the entire sequence while progressing with autoregressive decoding.Determining the maximal joint probability of all sequences in parallel is a challenging task, equivalent to a full maximum a posteriori estimation.This is an open research problem and it is also an issue for nat methods.Nat methods patch up this limitation with sequence-level kd which has the advantage of \\\"not requiring any beam search at test-time\\\" thanks to learning and distillation from large models.Since our method is a decoding algorithm, we cannot use the same approach without learning.Nevertheless, the quality guarantee allows our methods to have performance on par with greedy autoregressive and generally better than a nat model.We think of our method, not as a replacement for beam search, but rather as a way to obtain a speedup at inference time that is a middle ground between autoregressive greedy decoding and nats.\",\"Although based on the transformer model, our methods also apply to various dnn modules, including cnns, poolings, and their compositions.an obvious limitation of this work is that we only verify our algorithm on models activated by relu.This issue can be alleviated because our algorithm is theoretically compatible with any piecewise linear activation function.For other functions in the relu family, such as the gelu used by bert , we replace the activations with relu, then fine-tune on downstream tasks and pretrain tasks.Our algorithms bog down on more complex nonlinear functions.It\\u2019s intuitive to fit these nonlinear functions with relu-activated fnns.However, this leads to additional computational and space complexity, which degrades performance after fitting.\",\".3, we find that wk and w v in transformers both correspond to u in probabilistic transformers.However, if we tie wk and w v in transformers, then we may observe a performance drop on some downstream tasks.The performance of probabilistic transformers lags behind transformers on large datasets , which suggests that our model may not be as scalable as transformers.The way of positional encoding for probabilistic transformers leads to slower training and inference speed.On masked language modeling tasks, our model is about 3 times slower than transformers with either absolute or relative positional encoding, though it has much fewer parameters than transformers.\",\"The limitations of this paper mainly lie in the following folds: we do not provide any theoretical analysis for the correlation between long-range dependencies and repetition loops, as well as solutions to avoid repetition loops with maximizationbased decoding.We do not discuss the source of lms\\u2019 learning bias, which may be caused by multiple factors, such as the transformer architecture , the mle loss, or the auto-regressive generation manner.We conduct experiments based on gpt2 due to resource limitations.The conclusions may differ for extra-large lms.We do not experiment with rnn-based models, which are also shown to prefer repetition.We do not perform the manual evaluation to compare selfcont with baselines since we focus on repetition in this paper, which can be automatically evaluated reliably.Perplexity and mauve scores are also shown to correlate highly with manual evaluation for evaluating fluency and overall quality, respectively.\",\"The proposed approach in this paper also suffers from certain limitations, i.we adapt apt on the encoder model and lack design for the other architectures such as decoder-only and encoder-decoder.In addition, it is better to generalize the key idea to other parameter-efficient learning approaches.\",\"Due to the limitation of computational resources, we have not evaluated the flan-t5xxl whose number of parameters is 11b, and the opt whose number of parameters is greater than 1.since opt and gpt-neo perform poorly in the zero-shot setting and separating attention scores of each document in the input is tedious for decoderonly models, we choose not to use them as source lms.However, we prove that taking the encoderdecoder model flan-t5base as our source lm is also robust to augment decoder-only models.We will explore new methods to annotate lm-preferred documents of decoder-only models based on their inherent signals.\",\"The taed model has slightly more parameters than the corresponding transducer model due to the attention modules to connect the speech encoder and aed decoder.They have similar training time for the offline models.However, the optimization of the streaming model would require more gpu memory and computation time due to the chunk-based rnn-t synchronization scheme described in \\u00a72.In our experiments, the streaming taed model takes about three times more training time than the offline model on the 16 a100 gpu cards, each having 40gb of gpu memory.In this work, we evaluate our streaming st al- gorithms on two translation directions: en\\u2192es and en\\u2192de.The word ordering for english and spanish languages are based on subject-verbobject while german is subject-objectverb.The experiments validate the streaming algorithms on both different word ordering pair and similar word ordering pair.\",\"The main limitation of hrt is that its upper bound on the inference speedup is lower than that of single-iteration nat under the same network architecture.2x on gpu\\u002fcpu, respectively, while that of hrt is 1.to achieve higher acceleration, hrt needs to employ the deep-encodershallow-decoder architecture.Increasing the chunk size is a simple way to reduce the autoregressive cost, yet it results in severe bleu degradation.Further research should be conducted to maintain high translation performance with fewer autoregressive prompts.\",\"Although our methodology is agnostic to the specific sequence-processing models used as encoder and decoder, its main purpose is to be used together with lplm.This clearly limits its use to institutions and users with access to computing facilities able to handle such models.Indeed, although the encoder of hsn can successfully be trained while keeping the weights of bert frozen, a feature which reduces training time, the gpt-based decoder does need to be fine-tuned to learn how to interpret the inferred symbols.This last points further limits the use of our methodology to \\u201cnot-so-large\\u201d decoder models.That being said, fine-tuning hsn decoders via lora is an exciting option that can possibly solve this last issue.We shall explore using lora with hsn in the near future.\",\"A well-known shortcoming of transformers is the computational complexity in self-attention lay- ers.Since the number of required calculations grows quadratically with the length of the input, transformers become prohibitively slow on very long sequences.An unfortunate side effect of processing inputs at the characterlevel is that internal sequences become much longer, so token-free transformers run into these efficiency problems much earlier than subword-based models.Figure 7 illustrates this problem by contrasting the runtime of all poetry generation systems when generating a single quatrain.Even bygpt5 , the smallest model in terms of number of parameters and the fastest token-free transformer, is only marginally faster than gpt-2 , which is almost five times larger.propose a two-stage decoding architecture in which the transformer decoder operates on character blocks that an additional lstm model decodes into individual characters.Another shortcoming is that our poetry generation systems can only generate a single poetic form, i.In general, poetry is a very diverse form of language and stanzas can be of arbitrary length, so this is a serious limitation.In particular, one could encode a rhyme scheme not as a single special token, but as an arbitrary series of letters indicating which verses rhyme with each other.Alternatively, our current systems could be used to generate longer stanzas through a sliding window approach, i., generating one verse at a time with the last three verse as context.Further, our human evaluation has limitations due to its relatively small scope.We only have a limited number of annotators and only consider a subset of all style combinations.lastly, quatrain is limited in that it consists of pseudo-quatrains, which are not real quatrains and often have missing contexts.\",\"Of our work are inference speed and decoding strategy.The efficiency of semiautoregressive inference is lower than that of nonautoregressive algorithms, so currently it cannot be applied to scenarios with high frequency requests.Furthermore, this paper only introduces greedy search as a decoding strategy.\",\"Although our model is more efficient than previous models trained using the mlm objective and the standard transformer architecture, we notice that the models runs around 30% slower.This is due to the disentangled attention mechanism, which is more computationally expensive than the standard attention mechanism.We also note that at the time of writing, the debertav3 tensorflow 2 implementation available on huggingface\\u2019s transformers library experiences heavy slowdowns with tpu backends.Our attempts to solve this issue were unsuccessful, and we were unable to train our model on tpus.\",\"In this paper we compare a shared encoder architecture for slu to a baseline architecture that was chosen based on the specific latency and cost constraints of an industry slu system.Since encoder model sizes were chosen based on specific constraints the results may not be directly comparable to model sizes more commonly used in the literature such as bert-large and bert-base.We expect the general benefit and order of magnitude of accuracy improvements shown in our evaluations to transfer to comparable setups with different parameters.The primary focus of this paper is on accuracy improvements and addressing challenges of realworld slu systems such as distribution drift and feature expansion.We do not elaborate on the details of the computational cost and inference aspects.A detailed analysis of compute cost and benchmarks of cpu and gpu inference would better highlight the infrastructure cost benefits of a shared encoder architecture for slu.Regarding the multi-lingual aspect of the encoder we only tested a single grouping of similar european languages.A more extensive analysis of different language groups would demonstrate that similar trade-offs seen in other works on multi-lingual language models also apply for the shared encoder architecture.\",\"While our work shows promising results in improving the generalization capabilities of transformers to sequences of arbitrary length, some limitations must be considered.First, our evaluation is confined to synthetic algorithmic reasoning tasks, which may not fully capture the complexity and diversity of natural language.We focused on synthetic datasets since they showed clear and somewhat surprising limitations of transformer architec- tures.However, the generalizability of our approach to other tasks and domains remains an open question, and additional research, such as evaluation on scan , cfq , cogs , or the long range arena , is necessary to understand its potential in real-world applications.Second, our approach introduces a new hyperparameter \\u2013 the maximum sequence position l.1 show that our method\\u2019s performance is largely unaffected by the precise value of l, practitioners may still have to tune the parameter depending on their specific problem domains.Third, we only isolate and ameliorate one failure mode of transformer length generalization on synthetic datasets.However, there are other factors contributing to poor length generalization, such as attention becoming less peaked for longer sequences.Overall, we believe that our study\\u2019s limitations offer several interesting directions for future research.\",\"Despite achieving high translation performance on various language pairs, lobef has some limitations, coming from the nature of processing utf-8 byte sequences.It is also worth noting that we use the same amounts of model parameters for a total of 6 transformer encoder layers and 6 transformer decoder layers for all models in comparison.As shown in table 7, byte-based models can effectively reduce the amounts of parameters for the embedding layers comparing to the subword-based models, leading to faster training time as shown in table 8., by adding more encoder layers, we can construct byte-based models with comparable amounts of parameters as subword-based models, and these larger byte-based models still require much longer time for training than subword-based models.Extremely low-resource languages: the performance of byte-based models on extremely lowresource languages is still lower than subword models especially in the multilingual setting.We suspect that byte-based methods require a relatively larger number of training data in order to aggregate information from a combination of byte tokens, comparing to subword-based models that explicitly maintain a subword vocabulary.Extra preprocessing: the byte-wsf model requires an extra preprocessing step that precomputes the attention mask corresponding to the words in each sentence.This adds a slight overhead before training, while training the byte-wsf model is as fast as the byte model, as both model use the same transformer architecture.However, for languages that do not have whitespaces to indicate the word boundary, we may rely on an off-the-shell word segmentation tool to preprocess the text.\",\"Model architecture due to our computational resource constraints, we only used the bert base architecture.We cannot confirm whether our results and observations are transferable to any other transformer-based architectures, especially for larger ones.Randomness we did not run pretraining for multiple times with different random seeds due to the limited computational resources and research budgets, though we fine-tuned models five times each with different random seeds in any downstream tasks.This might affect the overall results shown in the paper.Languages other than english it is uncertain whether any results and\",\"As a preliminary study, our proposed classifier decomposition framework focuses on the first training stage of cre models with a lack of explorations on stage 2.besides, more experiments can be conducted by combining our framework with previous leading cre models, which we leave for future research.In addition, our work only focuses on strategies with the ffn layer.As the bert encoder is the main component of cre models, we call for more attentions to the research of improving encoder representations in the first training stage.\",\"Since there are two transformer-based models in hdld, the major limitation of hdld is the training cost.It costs twice training time than most baselines.Here is the training time comparison on opensubtitles: model minutes per epoch adalabel 7.32 to address this limitation, a potential improvement direction is to fuse the inter-duality approach into one transformer, e., sharing parameters between the fxy and fyx.However, we find the significant performance drop when doing so.besides, like most generative models, if there is malicious information in data, there is no guarantee to avoid bad responses to users, which is a potential risk that needs to pay attention.\",\"Our work has the following limitations.First, we only evaluate generalization on datasets based on english language.next, we also do not study the effect of training data size on structural grokking, and do not investigate whether transformers learn to grok hierarchical structure in low data regimes.\",\"Although our ctc-nast model achieves excellent performance, there are still some underlying challenges that remain in the follow-up of our work.although the proposed cla and clm approaches achieve good results by alleviating the monotonic assumption and relieving the modeling burden, combing them can not bring remarkable improvement.More importantly, these two methods fail to stable improvements in encode-decoder architecture.This drives us to investigate the interference of the optimizations between ctc and cross-entropy.\\u2022 combination with the pre-training or multitask learning.Although our methods bring remarkable gains on both ar and nar models, we do not explore the utilization of external data resources.theoretically, we need to design nar asr and mt models that share the same or similar architectures with the acoustic encoder and textual encoder, respectively.In this way, the nast model bridges the gap between pre-training and fine-tuning and has more potential for better performance.\\u2022 the potential risk for unwritten languages.In our work, we assume that transcription is always available, which is consistent with almost previous studies.Although some datasets have no transcription, we can use a well-trained asr model to generate pseudo labels.However, it is hard to handle speech translation from unwritten source speech.The supervision of source text is very important for our model.Therefore, we need to develop better methods for stable training.\",\"There are mainly two limitations in this study.First, we still do not consider components other than the bias parameters in the prediction head.For example, the weight parameters of the prediction head, i.Second, our findings do not cover the transformer language models other than bert and gpt-2.Consistent findings were obtained for the two main architectures and for various model sizes, although future research is needed to show whether the findings can be generalized to roberta , open pre-trained transformer language models , and other variants.Considering transformer encoder-decoder models, such as neural machine translation models and t5 , would also be an interesting future direction.\",\"There exist so many different transformer models and efficiency methods that it is extremely difficult to conduct exhaustive experiments for all of them.Although our experiments demonstrate nice properties for efficiency operators, the observations are restricted to our experimental setup.Considering the huge space of all combinations of transformer models, efficiency methods, and datasets, our experiments provide understanding for an important but small subspace, and it is possible that the\",\"In theory, the method proposed in this paper can be applied to different types of transformer language models for both pre-training and fine-tuning.Due to limit of computational resource, we currently haven\\u2019t had the chance to test our proposed method in the very promising setting of large-scale language model pre-training yet.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"11_transformer_nat_transformers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"11_transformer_nat_transformers\"],\"textfont\":{\"size\":12},\"x\":[13.698426246643066,13.611259460449219,13.720649719238281,13.695761680603027,9.182255744934082,13.774097442626953,13.65735149383545,13.626344680786133,13.591663360595703,13.70583438873291,13.704154968261719,13.60882568359375,13.726582527160645,13.716678619384766,13.455389022827148,13.744966506958008,13.5873441696167,13.722688674926758,13.76022720336914,13.726358413696289,13.8290376663208,13.794198989868164,13.65772533416748,13.442331314086914,13.729654312133789,13.724740982055664,13.622982025146484,13.591304779052734,13.658035278320312,13.616138458251953,13.604561805725098,13.775439262390137,13.739048957824707,13.413418769836426,13.683755874633789,13.725395202636719,13.600854873657227,13.635581970214844,13.730329513549805,13.78129768371582,13.645476341247559,13.35564136505127,13.673656463623047,13.608367919921875,13.679323196411133,13.673518180847168,13.749641418457031,13.67800521850586,13.571589469909668],\"y\":[3.3129563331604004,3.022742986679077,3.712540626525879,3.55329966545105,0.029000936076045036,3.1600420475006104,3.0915987491607666,3.6394262313842773,3.2410085201263428,3.271327495574951,3.0368428230285645,2.939103126525879,3.2975032329559326,3.6139137744903564,3.2417309284210205,3.5977652072906494,3.0726640224456787,3.250173807144165,3.1405575275421143,3.7147722244262695,2.919217348098755,3.1439151763916016,3.4170010089874268,3.352605104446411,3.6935219764709473,3.658109188079834,3.269308567047119,3.000282049179077,3.499763011932373,3.5356202125549316,3.467377185821533,3.486802577972412,3.683976888656616,3.302610397338867,3.030606746673584,3.708505868911743,3.237048625946045,3.505866050720215,3.1295082569122314,2.9352200031280518,3.0580596923828125,3.327394962310791,3.4722893238067627,2.993152379989624,3.61677885055542,3.1085424423217773,3.227612257003784,2.935831308364868,3.242823362350464],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Our approaches largely focus on explicit mentions of identity terms.This does not capture whether the identity term is the target of hate speech, which would require further analysis.This approach also does not capture attitudes held toward high-profile members of those groups, which play a role in circulating associations with identities.is is a large, popular forum for blackpilled incel discourse, which has a unique and extreme ideology that we argue is under-represented in current hate speech datasets.However, our analysis is limited to this forum, and the trends we identify may not apply to more moderate incel discourse or related online male supremacist movements, such as mgtow and puas.Though these communities are known to have related, but distinct jargon , we emphasize that researchers should recognize these lexical innovations in their annotations for hate speech and include a variety of these communities in training datasets for misogyny.\",\"Explored above, we identify several potential risks with this paper.Some may be offended by the images we include.We tried to mitigate this risk by including a warning in the abstract and not including images featuring genitalia.However we appreciate these images may contribute to the sexualisation and objectification of non-cisgender people, particularly if taken out of context.Though we did not set out to generate offensive images 11), images from the full data set could 11 dalle-mega similarly offend and even be weaponised.a data set of cisgender and non-cisgender images labeled by photorealism and presence of a clear face could feasibly be used to finetune a model to identify non-cisgender people.As such, we make our image data set available only upon request; it is intended to measure the harm done to non-cisgender people, not contribute to it.\",\"Given the subjective nature of our proposed task, this work does have some limitations and challenges.Firstly, the notion of harm or potential to do harm is seldom an objective factor and is also difficult to measure or quantify.Our experiments on inter-annotator agreement use a small dataset, so this study could be expanded with collaboration with social science researchers to better qualify how people perceive the agenda in different articles.Our work is also grounded in the united states, so it may have limited applications to the news in other countries.Secondly, our data and framework can be used to build and train a system to perform posthoc detection of harmful agendas in news articles.However, in a real-world system, this identification would likely need to happen on the fly, so as to make readers aware of these agendas as they are exposed to the articles.Finally, another aspect that we have not addressed in this study is the effect that a platform or community may have on the perceived harm in an article.For example, on dedicated social media channels hosting discussions on alternate theories and contentious topics , a junk science article with dubious claims may not be as \\u201charmful\\\" as opposed to the same article being posted on an open forum where readers may perceive it as scientific fact, thereby making the article more \\u201charmful\\\".The context in which news articles are disseminated may have a profound impact on this perceived harm and this may be an interesting direction for future exploration.\",\"Currently our approach relies on translation to analyze multilingual tweets.we carry out our in-domain optimization on a small validation dataset that was annotated by a different set of raters than the one used for the test dataset, which results in a performance drop in the few-shot mode.Ideally, the availability of a high quality validation dataset would boost the zero-shot performance and further adapt the label mappings to the target domain.We also aim to carry out in house annotations by experts to release a publicly available dataset annotated with emotions in the political domain which would pave the way for further analysis in this domain.\",\"Legislators show political support in multiple ways.In this work, we operationalised political support as active and passive cosponsorship.Active and passive cosponsorship represent a strong signal of support between legislators that has been widely accepted in the political science literature.However, other forms of political support, e., endorsement of public posts on social media, could be considered.Future research might explore the extent to which these forms of support might reveal additional insights about the cooperation between legislators.Our second limitation relates to the estimation of legislator\\u2019s ideology.This means that it cannot be directly measured and no ground-truth data exists.Therefore, to validate that our legislator representations encode ideology, we need to prove their performance in a variety of tasks in which the political science literature suggests ideology is important.In our work, we studied three tasks: active\\u002fpassive cosponsorship prediction, party affiliation recovery, and voting prediction.We argue that this is a representative set of tasks.However, legislators are involved in additional ideology-driven tasks, e.Showing that our representations are also predictive of these additional tasks might be considered an even more robust and convincing validation of our results.Third, in its current form, our model cannot compute predictions for newly elected legislators.This is due to no data being available\\u2014newly elected legislators have not given any speeches, or sponsored any bills.We argue that by applying our model as an online predictor, new information on legislators could be incorporated as soon as it becomes available.However, a full exploration of our model\\u2019s potential for this application was outside the scope of this work.Our final limitation concerns how our model can be extended to other data.In our work, we studied four different u.for these, we obtained consistent and high performance.Therefore, we expect this performance to extend to other congresses.However, having focused exclusively on the u., we cannot make any statements about the applicability of our framework to other legislative systems.Addressing this limitation could contribute to proving the generalizability of our results.for instance, recent works on radicalization can take a similar approach to study the relation between ideology and radicalization.Similarly, studies on international relations can benefit from this approach in order to study latent states between nations such as \\u201cally\\u201d, \\u201cneutral\\u201d, and \\u201cenemy\\u201d.\",\".while its abusive dimension can be considered a good proxy, all fine-tuned models systematically fails on core non-hateful functional tests, indicating limitations in the annotated data.0 with multiple hate speech datasets and further 2these correspond to f18\\u201319, f21\\u201322 in mhc.ethical statement limitations hatecheck-nl is based on mhc and it inherits its limits.to address these limitations, we plan to conduct focused interviews with dutch organizations such as the black archives3.Intended use hatechek-nlis a diagnostic tool for hate speech against specific protected groups.We have shown its functionalities and its impact on the evaluation of models trained both on a different language phenomenon, e., offensive language, and on related and comparable one, e.the results have shown critical weaknesses mainly on the non-hateful tests rather than showing the strengths of the systems\\u002fmodels on the hateful examples.Similarly, op-nl is a dynamic test for offensive language whose use is to help assessing the robustness and portability of models trained for offensive language detection.0 is the only publicly available resource for investigating the behavior of models on offensive and abusive language phenomena in dutch.None of the annotated dimensions in dalc-v2.the results of the fine-tuned models on hatechek-nl for the abusive language dimension indicate a compatibility between abusive language in dalc-v2.the use of offensive training data on hatechek-nl better highlights the limitations of the data, especially as pointed out by the systematic failure on the functions f23\\u201324.At the same time, the results on op-nl for offensive language show a relatively good portability of the models for this language phenomenon.\",\"Limitation of the moraldirection is that it is induced on individual words, and thus longer sentences are a significant challenge for the models.Still, we were able to test them on parallel subtitles data, which contains slightly longer, but predominantly still short, sentences.Problems that showed up repeatedly in this experiment were an over-reliance on key lexical items and a failure to understand compositional phrases, particularly negation.Additionally, typical problems of pmlms, such as disambiguation problems across multiple languages, were noticeable within xlm-r.non-english languages appeared more affected by such issues, despite the fact that all our target languages are relatively high resource.our experiments with the mfq reinforce the\",\"Dataset representativeness our dataset covers a range of topics of public interest as well as media from all sides of the political spectrum.However, it should not be seen as representative of the media in any country, nor should it be seen as perfectly balanced in any specific way.Biases human data annotation involves some degree of subjectivity.To mitigate this, we created a comprehensive 60-page guidelines document , which we updated from time to time to clarify newly arising important cases during the annotation process.We further had quality control steps in the data annotation process, and we have been excluding low-performing annotators.Despite all this, we are aware that some degree of intrinsic subjectivity will inevitably be present in the dataset and will eventually be learned by models trained on it.Baseline models the reported experiments can be seen as strong baselines as they include fairly small encoder-only transformer architectures., fewshot and zero-shot in-context learning, instructionbased evaluation, multitask learning, etc.Model biases we did not explore whether and to what extent our dataset contains unwanted biases.\",\"We consider as profanities words that have highly offensive or vulgar connotations.We acknowledge that readers may have different sensibilities with respect to profanities.Obscene words depend on different factors, such as culture, social or religious background, and more.Consequently, some words may be disturbing for a number of people, and should be obfuscated, while other readers may not have any issue with reading them.Moreover, we should consider that there is typically a hierarchy of offense, whereby some words are more severe than others; for example, f*ck is often socially accepted while the n-word usually is not.\",\"We discuss four limitations of our work: the inclusion of unsafe content, potential biases in data sources, a limited measure of image quality and generalizability to different generative models.\\u2022 inclusion of unsafe images and prompts.We collect images and their prompts from the stable diffusion discord server.The discord server has rules against users generating or sharing harmful or nsfw images.The stable diffusion model used in the server also has an nsfw filter that blurs the generated images if it detects nsfw content.However, we observe that diffusiondb includes some nsfw images that were not detected by the nsfw filter or removed by the server moderators.To mitigate the potential harm, we compute and share the likelihood of an image or a prompt containing unsafe content using the state-of-theart nsfw detectors.In addition, we provide a google form on the diffusiondb website where users can report harmful or inappropriate images and prompts.We will closely monitor this form and remove reported images and prompts from diffusiondb.\\u2022 potential biases of the data source.The 14 million images in diffusiondb have diverse styles and categories.However, discord can be a biased data source.Our images come from channels where early users could use a bot to use stable diffusion before release.As these users had started using stable diffusion before the model was public, we hypothesize that they are ai art enthusiasts and are likely to have experience with other text-to-image generative models.Therefore, the prompting style in diffusiondb might not represent novice users.Similarly, the prompts in diffusiondb might not generalize to domains that require specific knowledge, such as medical images.we use joint text-image clip embeddings between prompts and images to detect generation misalignments.While the clip embedding distance can indicate the degree of alignment between the prompts and generated images, it does not provide a measure of the overall image quality.When constructing our dataset, we have considered including image properties such as entropy, variance, and the most common colors to help users gauge image qualities.However, these metrics do not provide a good measure of the overall image quality as well.To better mea- sure image quality, future researchers can recruit annotators to rate images in diffusiondb.Previous research has shown a prompt that works well on one generative model might not give the optimal result when used in other models.Therefore, different models can need users to write different prompts.For example, many stable diffusion prompts use commas to separate keywords, while this pattern is less seen in prompts for dall-e 2 or midjourney.Thus, we caution researchers that some research findings from diffusiondb might not be generalizable to other text-to-image generative models.\",\"Our work can be considered to have the following possible limitations: 1.The dataset we introduce and use to perform analysis contains 2000 tweets sampled from a specific time frame over a single social media platform.However, we aim to extend this work by collecting more political data across various social media platforms and using it to model aggressive behavior.Please do note that these tweets have been manually filtered from a larger set of 10,000 tweets while manually labelling them and ensuring that they are relevant to the political domain.The number of user handles that we scrape tweets from for this study is around 110.This number might not be reflective of a large political space considering the plethora of politically active personalities in india.However, it is noteworthy that each of these 110 user handles has a minimum of 100, 000 followers on twitter, on the basis of which we consider them to be influential on a social media platform.\",\"Limitation of assigning labels to people as being inherently reductionist.As mentioned in \\u00a77, using a single likert scale for social acceptability and toxicity is not sufficient in capturing the complexities in these phenomena, such as situational context.We note that quantifying positionality of existing systems is not an endorsement of the system.In addition to making sure that language technologies work for all populations, researchers should also continue to examine whether these systems should exist in the first place.Further, we note that understanding a dataset or model\\u2019s positionality does not preclude researchers from the responsibilities of adjusting it further.This study was undertaken following approval from the irb at the university of washington.they were lay people from a wide range of ages and diverse backgrounds.Participants were asked for informed consent to the study procedures as well as the associated risks, such as being exposed to toxic or mature content, prior to beginning the study.Research team positionality we discuss aspects of our positionality below that we believe are most relevant to this research.The research team is comprised of computer scientists who study human-computer interaction and nlp and have a bent for using quantitative methods.Thus, we approach the topic from a perspective that assumes that positionality can be characterized, fixed, and quantified.The entire research team currently resides in the united states.In alphabetical order, the team members originate from belgium and switzerland, france, germany, india, and the united states; and identify as east asian, south asian, and white.These nationalities and ethnicities are overrepresented in the development of nlp technologies.Thus, we acknowledge that our knowledge of how design biases in nlp datasets and models impact people is largely through research, rather than personal experience.\",\"Our work can be considered to have the following limitations: 1.The dataset we introduce contains 10, 000 text instances sampled from a single social media platform.we obtained this dataset by crawling for tweets based on 52 keywords.We acknowledge that these keywords may have limited the domains in which political aggression can occur.That being said, we also hope that task generalizability is not compromised due to the presence of pretrained language models at the helm of our experiments.\",\"Our c-stance data is collected from social media, which may be seen as a limitation, as we may not cover all aspects of formal texts that could be used in essays or news comments.however, this is a limitation of any other datasets that focus on social media content.Ethical statement our dataset does not provide any personally identifiable information.Microblogs are collected using generic keywords instead of user information as queries, therefore our dataset does not have a large collection of microblogs from an individual user.Thus our dataset complies with sina weibo\\u2019s information privacy policy.\",\"Ethical and societal considerations we consider the following limitations and societal considerations of our work.Machine-generated data our analysis is based on gpt-3 generated data., such analysis can provide insights into the nature of social interactions.However, this could induce specific biases, such as skewing towards interpretations of words aligned with gpt-3.5\\u2019s training domains and potentially overlooking more specialized domains or minority speech.The pervasive issue of bias in offensive language detection and in llms more generally requires exercising extra caution.We deliberately generate multiple contexts for every statement as an indirect means of managing the biases.Nevertheless, it is a compelling direction for future research to investigate the nature of biases latent in distilled contexts for harmful speech and further investigate their potential impact.For example, it would be valuable to collect human-annotated data on cobra to compare with the machine-generated data.However, we must also recognize that humans are not immune to biases , and therefore, such investigations should be carefully designed.Limited contextual variables although cobracorpus has rich contexts, capturing the full context of statements is challenging.in this work, we focus on the immediate context of a toxic statement.However, we recognize that the context of a toxic statement can be much longer.We have observed significant effects even in relatively brief contexts, indicating the potential for improved performance when more extended contexts are present.We believe that future research could explore the influence of richer contexts by including other modalities.Limited identity descriptions our work focused on distilling the most salient identity charac- teristics that could affect the implications of toxicity of statements.This often resulted in generic identity labels such as \\u201ca white person\\u201d or \\u201ca black woman\\u201d being generated without social roles., the assumption that all members of a demographic group have inherent qualities and experiences, which can be harmful and perpetuate stereotypical thinking.english only we only look at a us-centric perspective in our investigation.subjectivity in offensiveness not everyone agrees that things are offensive, or has the same interpretation of offensiveness.Our in-context prompts and qualification likely make both our machine-generated explanations and human annotations prescriptive , in contrast to a more descriptive approach where we would examine different interpretations.dual use we aim to combat the negative effects and harms of discriminatory language on already marginalized people.It is possible however that our frames, dataset, and models could be used to perpetuate harm against those very people.We do not endorse the use of our data for those purposes.Risk of suppressing speech our frames, dataset, and models are built with content moderation in mind, as online spaces are increasingly riddled with hate and abuse and content moderators are struggling to sift through all of the content.we do not endorse the use of our system to suppress speech without human oversight and encourage practitioners to take non-censorship-oriented approaches to content moderation ).Harms of exposing workers to toxic content the verification process of cobracorpus and cobracorpus-cf is performed by human annotators.Exposure to such offensive content can be harmful to the annotators.We mitigated these by designing minimum annotation workload, paying workers above minimum wage , and providing them with crisis management resources.Our annotation work is also supervised by an institutional review board.\",\"There are certain limitations that can be concerned for further improvements.Our experiments show that a data-driven estimation based on end-to-end model training produces worse results than a hyperparameter grid search.An automatic prior estimation is desirable for real-world applications.Moreover, in npugraph, we approximate the probability of negative\\u002fpositive facts being collected\\u002funcollected via neural networks, which lacks a degree of interpretability.Ethical impact npugraph neither introduces any social\\u002fethical bias to the model nor amplifies any bias in data.for twitter interaction data, we mask all identity and privacy information for users, where only information related to user interactions with tweets and hashtags is presented.Our model is built upon public libraries in pytorch.We do not foresee any direct social consequences or ethical issues.\",\"The political compass test in this work, we leveraged the political compass test as a test bed to probe the underlying political leaning of pretrained language models.While the political compass test is a widely adopted and straightforward toolkit, it is far from perfect and has several limitations: 1) in addition to a two-axis political spectrum on social and economic values , there are numerous political science theories that support other ways of categorizing political ideologies.2) the political compass test focuses heavily on the ideological issues and debates of the western world, while the political landscape is far from homogeneous around the globe.3) there are several criticisms of the political compass test: unclear scoring schema, libertarian bias, and vague statement formulation.However, we present a general methodology to probe the political leaning of lms that is compatible with any ideological theories, tests, and questionnaires.We encourage readers to use our approach along with other ideological theories and tests for a more well-rounded evaluation.Probing language models for encoder-based language models, our approach of mask in-filling is widely adopted in numerous existing works.For language generation models, we curate prompts, conduct prompted text generation, and employ a bartbased stance detector for response evaluation.An alternative approach would be to explicitly frame it as a multi-choice question in the prompt, forcing pretrained language models to choose from strong agree, agree, disagree, and strong disagree.These two approaches have their respective pros and cons: our approach is compatible with all lms that support text generation and is more interpretable, while the response mapping and the stance detector could be more subjective and rely on empirical hyperparameter settings; multichoice questions offer direct and unequivocal answers, while being less interpretable and does not work well with lms with fewer parameters such as gpt-2.Fine-grained political leaning analysis in this work, we \\\"force\\\" each pretrained lm into its position on a two-dimensional space based on their responses to social and economic issues.However, political leaning could be more fine-grained than two numerical values: being liberal on one issue does not necessarily exclude the possibility of being conservative on another, and vice versa.\",\"Our work is not without its limitations.First of all, our annotated data is relatively small.However, given the relatively straightforward task , and since we are using this data only for evaluations, we believe that this amount of data is sufficient for the research questions we are asking\\u002fanswering in this paper.Second, our data entirely comes from the politics domain and social media, situated in the us context.This choice was driven by our downstream use case of a large scale social science analysis in the us political domain.However, we expect performance degradation with genre or dialectal shifts with substantial differences in syntactic patterns.Third, we have not fully exploited the utility of the dataset in this work.2, our aim in this paper is not to build the best tagger possible, and hence we did not explore state of the art modeling techniques such as few-shot learning.Finally, our work is done entirely on english language data.While we believe that similar approach could work in other languages without vocative markers, more research need to be performed to verify that.While we acknowledge these limitations, we reiterate that these are outside the scope of what could be meaningfully done within this short paper.\",\"We reused data collected by previous work in the literature.Collecting news articles is susceptible to various sampling biases, related to the sources collected, the topics covered, and the time span of the collection, which influences what appears in the articles.In addition, labels given to articles are actually the political orientation of their source in the case of the allsides and politics datasets, which is obviously likely to induce errors.They rely on expertise provided respectively by the allsides11 and ad fontes12 websites.The exact methods are undisclosed, but such labeling has necessarily a subjective aspect, oversimplifying predefined political categories, and can evolve in time.This affects classification reliability when applied to different sources, different times, different topics.This is on top of any specific elements related to the language and cultural background of the sources.This study is not intended to provide an accurate tool for predicting the political orientation of a text, but to provide analyses of the linguistic expression of bias, as seen through a supervised model.\",\"While the training method makes use of user profile description and history, one additional factor that is important is the structure between users and news articles.Knowing a user\\u2019s social circles can often give hints about the user\\u2019s interests and beliefs, which can potentially help the model to infer how a particular persona would respond to an issue.A possible direction is to design a method that explores the social context features via graph-based algorithms.\",\"The sample we collected has limitations shared by most datasets focusing on naturalistic behavior surrounding some event.First, because the starting point in this sample was the quit messages, and not every person posts each week, there are necessarily more target posts than other posts.Quit weeks and weeks immediately surrounding target posts were also more verbose than other weeks and, because work-related concerns were salient at the time, likely included more comments and posts about work or career planning.Second, the reddit sample we analyzed is heterogeneous.In most respects, that is a benefit of these data\\u2014the conversations covered diverse topics and took place in groups with varying social norms, cohesiveness, and cultures.In that way, these messages are more naturalistic than language from controlled experiments or narrowly focused social media research.Yet there are better options than simply averaging over these differences.For example, emotional expressions are both inflated and suppressed by forum norms regarding emotional self-disclosure , and the same terms take on different affective meanings across communities.Future research on these or similar data may benefit from clustering forums into psychologically meaningful groups or developing sentiment lexicons tailored to each forum.Finally, as with any analysis of self-labeled data on social media, we are taking people at their word, accepting the likelihood that some of the messages about quitting in our sample were exaggerated or fabricated.Despite efforts to stringently filter out hypothetical, satirical, fictional, remembered, or otherwise non-literal references to recent quitting, there are also no doubt some remaining false positives.\",\".Enabled us to test models for a range of often overlooked groups , we ultimately modelled only four specific attributes.There are likely to be more factors that could play a role.Dataset are exclusively from the united states of america, so that results do not necessarily hold for other countries or cultures.Specifically perceptions of harmful content online are known to vary across countries.This is mainly due to our strict criteria regarding dataset size and availability of annotator-level labels and sociodemographic information.These characteristics were a prerequisite for our experiments across different attributes with sufficient numbers of annotators.Most datasets which include annotator-level labels and sociodemographic information contain much smaller numbers of annotators and attributes.Nevertheless, with the measuring hate speech corpus there is at least one additional dataset with comparable characteristics that could be used in future experiments.Or hs-brexit which was annotated by 6 annotators, each from one of two sociodemographic groups.We do not study the aggregation of individual predictions or evaluate against majority labels, as these are not directly relevant to our investigation of sociodemographic attributes in models of annotation behaviour.Consequently, we cannot derive a conclusion about performance in those settings from our results.Compare labels aggregated from multi-annotator models against predictions from a standard classifier.For computational reasons, our experiments use a comparatively small pre-trained language model.Thus, results might differ with larger models.\",\"It is important to note that the study is based on a limited set of examples and although it is enough to give a signal if a system is struggling or not in faux pas tests, the number of stories is not sufficient for statistically significant ranking between systems.Ethical statement the study\\u2019s scope did not include the representation of harm toward specific populations.The narratives were evaluated by a clinical psychologist to ensure that they did not contain offensive content.However, it is important to acknowledge the potential value of further research on the representation of harm in relation to culturally sensitive and socially controversial topics.\",\"The main limitation of the proposed study is the relatively small scale of the dataset it is based on.The proposed method is scalable and computationally undemanding , and it is feasible to apply it to other countries in the cmp dataset.However, in order to arrive at interpretable results that could be verified in terms of policy substance based on the experts\\u2019 knowledge of the political spectrum, we had to focus the evaluation part on the materials of a single election cycle in one country.Potentially, the method can be applied to any country whose manifestos have cmp annotations, however, further investigation with data from other countries needs to be carried out to verify that.While most policies are recurrent in manifestos, there may be a few topics appearing in upcoming elections, adding some variability in debate across election years.The policy domain labeller might need to be updated every now and then with current topics of interest.Therefore, the effect of news electoral programs in the classification step requires more investigation namely, the feasibility of further training with new topics of the current debate or the necessity to re-train the whole classifier with new manifestos over again.That being said, the cmp codebook has remained the same for over two decades now.We take this as evidence that the policy domains do not need to change, only the ability of the classifier to correctly identify sentences with unseen topics.\",\".While we present the utterances as constituting natural speech by one speaker , it is likely most congresspeople employ social media teams that help in crafting the language of some of their tweets.However, we believe for the sake of interpersonal group membership, the relationship between the speaker and their target would not be affected.Techniques like inlp extract information that is linearly extractable.Might be able to manipulate more non-linearly encoded properties.Picking these parameters is tricky and we have done it in a manner that preserves information in the language model.It is possible that a different set of settings not explored here could lead to different results.\",\"Since our eap builds its graph representation from social media data, our method may carry inductive biases rooted in this type of data.Moreover, note that the scope of our study is limited to english social media posts and our approach does not consider inputs larger than 512 tokens.Therefore using our approach in long document summarization may be challenging.Finally, the general applicability of eap in a different domain is highly dependent on the existence of high-quality lexicons for the domain in question, which may not be available.\",\"Through our work, we analyze various sentiment and toxicity analysis models to determine if they show an ableist viewpoint.The results depict a statically significant presence of disability bias, and we publish our method for any individual to access and use.This step is crucial in the field of nlp to mention the ramifications a given model can have on society.One limitation of this work is that we analyze models that are trained in the english language.We understand that the social concept of disability can change for various cultures and languages.The scope of this paper for now only looks into one language.Ethical statement the paper provides a method to parameterize ableist bias in nlp models, but we acknowledge that this is not the sole method that can be used for identification.The work is limited only to identification in sentiment analysis and toxicity detection models.There can be other methods of identification that are rapidly being worked on which may not have been included in this process.We also understand the effects various other forms of social biases can have when viewed alongside disability bias.We, therefore, will be working on measuring the combination of social biases through a cultural lens for the future.\",\"The proposed cringe loss can be used to mitigate some of the identified problems of large language models, for example, the use of toxic language or contradictory statements.Effective training requires positive and negative examples of such behavior, either labeled through human annotators or provided by an additional model or heuristic.The quality of the data bounds the success of the training approach.In our experiments, we assume non-adversarial label annotation.In real-world interactions with a chatbot, it is likely to experience at least some \\u201ctrolls\\u201d that provide wrong feedback on purpose.Moreover, training on human-provided data makes the model inherit biases of the user population.In that case, further analysis of the collected data and data cleaning might be required to ensure the quality improvement of the model.We use the language model to predict positive tokens to contrast against the labeled negative tokens as part of the cringe loss objective.Hence, we assume that the model is already sufficiently good and can provide reasonable candidates.We have not fully analyzed how the model is affected by the quality of the language model, for example how scale affects our results \\u2013 although we do experiment with 400m and 3b parameter models, and find performance improvements in both cases.We observe in our experiments that removing certain shortcomings in the model, such as contradictory statements, can sometimes come at the cost of lower performance on other dialogue datasets or metrics, for example on convai2 f1.\",\"Although our datasets are publicly available and gathered from participants in different countries, they cannot entirely represent the moral norms from all the individuals in different cultures over the world or predict how moral norms might change into the future.Additionally, we examine a limited set of moral issues for each country, therefore the current experiments should not be regarded as comprehensive of the space of moral issues that people might encounter in different countries.Moreover, taking the average of moral ratings for each culture is a limitation of our work and reduces the natural distribution of moral values in a culture to a single point.Implementing a framework that incorporates both within-country variation and temporal moral variation is a potential future research direction.Currently, it is not clear whether the difference between eplms\\u2019 estimated moral norms and the empirical moral ratings is due to the lack of cultural moral norms in the pre-training data, or that the cultural moral norms mentioned in the pre-training data represent the perspective of an english-speaking person of another country.For example, a person from the united states could write about the moral norms in another country from a western perspective.A person from a nonwestern country could also write about their own moral views using english.These two cases have different implications and introduce different moral biases into the system.Potential risks we believe that the language models should not be used to prescribe\",\"While promising, our work presents limitations that need to be acknowledged.Firstly, we did not explore the best verbalizers for instruction finetuned language models, which could have further enhanced the performance of the models explored in this study, due to computational cost and the specific goals of the research.Secondly, we selected benchmark datasets based on their popularity and diversity, which might not be representative of all possible datasets in hate speech detection.We also acknowledge that, in addition to the languages examined in this paper, there are a number of other languages that may present unique challenges and characteristics for detecting hate speech.Our decision as to which languages to include in the multilingual experiment was based on a direct comparison with state-of-the-art research.Finally, we utilized the latest open-source language models for our experiments, but we did not explore other recent language models, such as the gpt family, primarily because they are not open and reasonably reproducible3, and therefore the community may encounter challenges in replicating our results.These limitations provide directions for future research to improve and expand upon our work.\",\"Biases human data annotation for a sentimentrelated task, e., aspect-based sentiment analysis, hate speech detection, etc.while we included important quality control steps in the tbo annotation process, this intrinsic subjectivity will inevitably be present in tbo and learned by the models (see also the\",\"The intention classification task is not trivial even for humans, especially when the intention is implicit or disguised.The sample size of our study is small, which makes classification more challenging.Currently, we are extending the dataset to include more samples in each category.We aimed to use this data as a proof of concept to shed light on using questions as a means to attack someone or disguise intention.Future directions involve enlarging the dataset and including a variety of social interactions from different sources such as social media , forums , and spoken conversations to investigate other emerging categories based on context, topics, and events.wikipedia editors should follow strict rules and avoid explicit hostility otherwise get blocked.\",\"This analysis relies on comparing model predictions with human annotations.One limitation of this approach is the following: we are assuming that the human annotated labels represent a reasonable ground truth.However, it\\u2019s likely that the annotations have their own bias issues.A future area of work is to analyze how reliable the annotations are for some of the top keywords surfaced here, especially for reclaimed speech and for tweets with aave.However, because previous work has found that word choice and profanity are likely stronger contributors to bias against aave than linguistic features of aave , we hope that bias mitigation techniques at the keyword level can also help alleviate bias against aave without the use of sensitive racial or dialect classifiers.our methodology is helpful for detecting the most widespread and prevalent problems.However, there may be other serious problems that do not receive the same amount of traffic that still deserve attention.Thus, relying on frameworks that focus on bigger segments of the population poses the risk of missing important harms to smaller communities.In this work, we develop a list of keywords for bias evaluation by analyzing a corpus generated from all english tweets on twitter.However, because english twitter is primarily composed of users from the united states and the united kingdom, our list of keywords for evaluation is likely heavily skewed towards us-centric or western issues.One way to mitigate this would be to repeat the analysis conducted here, but using separate corpora for each country or upsampling tweets from countries with smaller populations of twitter users in order to ensure we are getting appropriate coverage in other countries with smaller user bases.This would help increase coverage for minority groups in the data we use for bias evaluation.Another critical area of work would include expanding the analysis to other languages beyond english.The overemphasis of english has led to the underexposure of other languages in nlp research.This work treats reclaimed uses of slurs as an important facet of the speech of marginalized communities.However, reclamation is not a \\\"bullet-proof\\\" process - some may find reappropriated uses acceptable and others may not.Additionally, reclamation may only be deemed acceptable by in-group members or in certain contexts.Since the marginal abuse model only uses the text of a sin- gle tweet , it is difficult for the model to account for such nuance.Furthermore, because this model is used to moderate all english content on twitter, the model implicitly assumes the same utterance has the same meaning across the world, which is an extreme oversimplification.In other words, the model does not account for local variations in language use.Reclamation can also backfire, for example the hong kong media\\u2019s mocking of the reclaimed use of \\\"tongzhi\\\" by the gay and lesbian community.This example serves to illustrate the essentially contested nature of reclaimed speech and how language ideologies shift over time.With respect to automatic content governance, shifting language ideologies indicate the importance of 1) meaningfully engaging and consulting with affected communities on models used for content governance, 2) the utility of regular audits and model refreshes to account for change in language use over time, and 3) additional user controls to better accommodate for multiple definitions of harmful content.Lastly, there are inherent limitations to fixing socio-technical problems through purely technical means.We hope that our analysis provides an interesting case study of some of the challenges associated with automatic content governance in industry and sparks further discussion.\",\"There is a growing interest in investigating human morality in text.However, like most technologies, morality classification can be misused, especially targeting sensitive features including ethnicity and political orientation.For instance, authorities in non-liberal countries could use tomea to identify repressed minorities by detecting moral language that diverges from the expected moral rhetoric.Ongoing research is investigating such issues, e., by creating methods that mitigate bias and unfairness by design.We discuss three main limitations of our analyses related to the corpus we use.First, mftc is composed of english tweets, and we employ a version of bert that was pre-trained on large-scale english data.Our experiments show that tomea produces insightful results under these conditions.However, the performance of tomea with models pre-trained on smaller datasets, e., datasets for morphologically richer languages, remains to be investigated.Further, the scalability of tomea to longer text formats and different mediums of communication is yet to be explored.Second, the tweets in the mftc were collected using the twitter api, which only yields public posts.Thus, following twitter\\u2019s terms of service, deleted content will not be available.Further, the demographic and cultural distribution of twitter users may not be representative of the general population, in addition, we required the crowd workers involved in the evaluation to be fluent in english, and their demographic distribution is skewed towards europe.These factors could possibly lead to the perpetuation of western values and biases in our analyses.Additional experiments are needed to investigate whether tomea would produce insightful results when applied on a dataset collected on a more extensive slice of the population, with a broader set of linguistical expressions.Third, the mftc is focused on us-centric topics.However, when recruiting annotators for our crowd evaluation, we did not require familiarity with such topics.Even though the annotators were not exposed to the original tweets but to a processed version of the dataset , the potential lack of familiarity may have influenced the evaluation results.Finally, we remind that tomea\\u2019s d-distances measure how similar two domains are, and are thus not a judgment of similarity.Further, two corpora collected in the same domain will likely not have a d-distance of 0.It is left to the user to judge the similarity of the two corpora, supported by tomea\\u2019s quantitative and qualitative metrics.\",\"The presented classifier and dataset are only from english-speaking sources, a major disadvantage in detecting white supremacist content globally.This limits its effectiveness in detecting white supremacist content from other time periods.Though including anti-racist data helps mitigate bias tested by our sample of the hatecheck dataset, an accuracy of 69.there is still a risk of overclassifying posts with marginalized identity mentions as white supremacist.\",\"Despite the fact that some measures have been implemented to minimize bias in labeling, we are still explicitly aware that our dataset may contain mislabeled data due to differences in the subjective understanding of toxic language by the annotators.3, our benchmark of toxic knowledge enhanced is not practical for all types of toxic comments, lacks sufficient background knowledge, and can easily lead to spurious associations.For reasons of intellectual property, we only capture the comments rather than the full text, which affects the actual semantics of the sentence to some extent.Besides that, non-textual features are not taken into account in this work, such as images and meta information about publishers.\",\"Our work is the first step towards unified pretraining for political actor modeling and it is limited in two aspects.In terms of data, we focus on the typical political actors, i., the congress legislators, and their statements, without using a larger corpus like political news.But our method can be easily scaled to a larger corpus, where we can aggregate articles of different media and consider their structure information like page links for pre-training.In terms of method, in order to improve the retrieval efficiency in both pre-training and fine-tuning, we use simple methods rather than dynamic selection methods based on embeddings to query and aggregate statements, leaving much room for future exploration.\",\"Although scl-fish achieves improvement over fish, training scl-fish takes longer time than fish.Empirically, we find that scl-fish is approximately 1.Moreover, we believe that the subjective nature of abusive language affects the annotation process of different datasets and possibly negatively impact performance.\",\"This work used the moral foundations dictionaries to measure the moral content of text produced by gpt-3.While studies have demonstrated correspondence between results from the dictionaries and human labels of moral foundational content , dictionarybased analysis is limited in its ability to detect nuanced moral expressions.Dictionary-based analysis could be complemented with machine-learning approaches as well as human evaluation.This study attempted to control for variations in the prompt phrasing by averaging results over several prompt styles.These prompt variations were chosen by the author.A more principled selection procedure could result in a more diverse set of prompts.The human studies that this study refers to were performed on populations from the united states.The precise political connotations of the terms \\u201cliberal\\u201d and \\u201cconserva- tive\\u201d differ across demographics.Documentation for the datasets used herein indicates that the crowd workers leaned politically left, and morally towards the care\\u002fharm and fairness\\u002fcheating foundations.However, bias in the marginal foundation distribution does not hinder the present analysis, since the present experiments experiments focus primarily on the difference in foundation use resulting from varying political identity.1 relies more heavily on the marginal foundation distribution; a foundationallybalanced dataset was constructed for this experiment.other pre-trained language model families of similar scale and architecture include bloom8, which i was unable to test due to compute budget, and llama , which was released after the experiments for this work concluded.While the opt model weights are available for download, gpt-3 and gpt-3.on the other hand, the hardware required to run openly-available models may be a barrier to experimentation that is not a concern for models hosted via an api.Criticisms of moral foundations theory include disagreements about whether a pluralist theory of morality is parsimonious ; ch.6 of , disagreements about the number and character of the 8 foundations , disagreements about stability of the foundations across cultures , and criticisms suggesting bias in the moral foundations questionnaire.Moral foundations theory was used in this study because it provides established methods to measure moral content in text, and because mft-based analyses have identified relationships between political affiliation and moral biases, offering a way to compare llm and human behavior.work that aims to elicit normative moral or ethical judgement from non-human systems has received criticism.Authors have argued that nonhuman systems lack the autonomy and communicative intent to be moral agents.Criticisms have also been raised about the quality and appropriateness of data used to train such systems.Notably, crowdsourced or repurposed data often reflects a priori opinions of individuals who may not be informed about the topics they are asked to judge, and who may not have had the opportunity for discourse or reflection before responding.Some have argued that systems that aggregate moral judgements from descriptive datasets cannot help but be seen as normative, since their reproduction of the popular or average view tends to be implicitly identified with a sense of correctness.Finally, several authors argue that the use of non-human systems that produce apparent or intended normative judgements sets a dangerous precedent by short-circuiting the discursive process by which moral and ethical progress is made, and by obscuring accountability should such a system cause harm.The present study investigates the apparent moral rationalizations produced by prompted llms.This study does not intend to produce a system for normative judgement, and i would discourage a normative use or interpretation of the methods and results presented here.The recent sea change in natural language processing towards general-purpose llms prompted into specific behaviors enables end users to produce a range of outputs of convincing quality, including apparent normative moral or ethical judgements.Anticipating how these systems will impact end users and society requires studying model behaviors under a variety of prompting inputs.The present study was conducted with this goal in mind, under the belief that the benefit of understanding the moral mimicry phenomenon outweighs the risk of normative interpretation.\",\".Limitations due to pre-trained models: the first limitation is the reliance of our system on thirdparty hatespeech detectors which are reported to have bias towards minority groups.These models tend to overestimate the prevalence of toxicity in texts having mentions of minority or protected groups due to sampling bias, or just spurious correlations.Also, these models suffer from low agreement in annotations partially due to annotator identity influencing their perception of hate speech and differences in annotation task setup.Please note that we aim to overcome this unintended bias problem by using principles of causality but still don\\u2019t claim to have completely eliminated the problem.Limitations due to training corpus: we are limited by the distributions of our training corpora in terms of what the model can learn and infer.Further, owtc dataset used in our perplexity evaluations is a subset extracted from openai-wt which contains a lot reddit and news data, where reliability and factual accuracy is a known issue.Limitations due to language: our experiments are conducted experiments only on english language which could be further extended to other languages.Limitations due to model evaluation: previous studies have shown that detoxification approaches optimized for automatic toxicity metrics might not perform equally well on human evaluations.A future direction of work may be to include human evaluations as part of the data.Limitations due to distribution shift: there are three different datasets that are in use.The first is the dataset used to train the ate scores.The second dataset is the set of prompts used to finetune the model.The third dataset is the dataset that is used during testing.A distribution shift between datasets may have an adverse affect on our model.For instance, there may be words which occur in the test set that are neither in the ate training set, nor in the fine-tuning set.In case of such a distribution shift between the datasets, our model may not work as expected.\",\"There are several limitations to our experiments: we work only with english data and with datasets concerning hate speech and toxicity.Frequently such data do not represent i.samples from the data that we might encounter in real life.In addition, experiments are all conducted in the simulation with these existing datasets.The annotations in the simulated experiments were already checked for quality by the original dataset creators.In real-world deployment, further steps would need to be taken to ensure that the entropy in annotations truly comes from disagreements and not other kinds of noise.While daal is designed to capture disagreement due to annotator positionalities, the datasets used may not have had a diverse enough pool of annotators to fully test this.In the portion of the mhs dataset used in our experiments, 67.9% of annotators were cisgender, straight, and white, while only 0.4% of examples targeted this same popula- tion.The wikipedia talk dataset does not provide demographic information about its annotators.A classifier for toxic text or hate speech trained on a pool of annotators whose backgrounds do not reflect anywhere near the full diversity of human identities is inherently limited.Applying such a classifier, whether it predicts a single label or a distribution, to text from and about marginalized populations not represented in the annotator pool carries inherent risks to the wellbeing of these populations.Such a classifier could systematically fail to flag content that annotators from privileged groups do not find harmful or incorrectly flag innocuous speech written by members of marginalized groups.\",\"In this article, we focus only on meeting minutes, speech, and press conference data.Many other text datasets such as transcripts from congressional and senate testimonies, beige books, green books, etc can be incorporated to understand pre-fomc drift better.We don\\u2019t use audio or video features in constructing the measure, which might contain additional information.It can be an interesting future study to compare measures generated from fomc text with an alternate measure that can be constructed from the news or social media data.In dataset construction, while splitting sentences, we use a simple rule-based approach.We leave it as an open problem for future researchers to find better methods for splitting sentences with opposite tones.In our trading strategy construction, we do not include transaction fees as it involves low-frequency trading.in addition, a more comprehensive zeroshot and few-shot generative llm benchmark with open-source models can be performed to provide a better comparison.\",\"This work puts forth a position: by the nature of a position paper, the work is deliberately intended to be evocative and opinionated, in some places not having unequivocal evidence for certain claims.Nonetheless, we believe centering power and change, and understanding evaluation as a political and sociological phenomenon, is likely to be useful under all conditions.Further, in understanding the qualities of evaluation relative to other social forces, we directly suggest that evaluation is more readily operationalized in more pluralistic ways than other key forces.While initial efforts indicate the potential for such holistic approaches that reflect many different desiderata as well as participatory approaches that permit contribution from different entities , it is still unclear how much adoption such approaches will get, and therefore how much power they will acquire.That is, the extent to which evaluation can realize this pluralistic vision still largely remains an unresolved aspiration than a readily realizable certainty.And, conversely, we do note that while current practices potentially put pluralism and resources at odds, they may be mutually compatible in other regimes , open-source software development ).Finally, we do not discuss other forces that we believe have not exhibited strong influence on nlp research thus far, in favor of allocating focus to evaluation and resources, which have had clear influence.To enumerate some of these other forces, we specifically note research norms, policy and regulation, and auditing\\u002fadvocacy.For , we note that while the nlp research community has many established norms , most of these do not directly\\u002fsignificantly influence what research topics different researchers work on.With that said, much as efforts like gdpr and privacy legislation has impacted scientific research on privacy , similar trends could occur in nlp research.3 akin to , for , we also have seen fairly little impact from auditing\\u002fadvocacy work on nlp research to our knowledge.But, much as work on auditing\\u002fadvocacy around face recognition influenced research in the computer vision community, we could see similar trends in nlp.\",\"The main limitation of the proposed framework is its dependency on a reasonable amount of real implicit hate instances to be used as the prompting input material.Obtaining implicit and subtle messages from social media is undoubtedly a challenging and time consuming task.More importantly, another limitation lies in the fact that the proposed framework does not rely on an automatic metric to determine if the generated messages are actually implicit.Therefore, a human-in-the-loop step for validating the obtained newly generated instances is still required.Additionally, there has been mounting pressure to obtain debiased plms, which might lead to the generation of less challenging examples.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"12_political_moral_hate\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"12_political_moral_hate\"],\"textfont\":{\"size\":12},\"x\":[9.332910537719727,9.283093452453613,9.43150520324707,8.993997573852539,8.984375,9.348570823669434,9.267433166503906,9.268567085266113,9.300333023071289,9.173699378967285,8.996798515319824,9.11129093170166,8.992171287536621,9.152442932128906,9.340027809143066,9.001591682434082,8.978045463562012,8.992897987365723,9.02924633026123,9.046924591064453,8.931620597839355,9.05980396270752,9.284300804138184,8.980762481689453,8.97110652923584,8.996585845947266,9.28821086883545,9.506633758544922,9.229934692382812,9.378137588500977,9.5620756149292,9.407928466796875,9.536916732788086,9.270305633544922,9.357154846191406,9.573067665100098,9.003302574157715,9.586814880371094,9.240416526794434,9.526531219482422,9.584248542785645,9.055190086364746,9.056492805480957,9.371119499206543,9.222376823425293],\"y\":[3.7374818325042725,3.755187511444092,3.3542494773864746,3.380384922027588,3.4571056365966797,3.697618246078491,3.7111802101135254,3.3758909702301025,3.735703229904175,3.7249643802642822,3.599374532699585,4.011032581329346,3.52970552444458,3.7053494453430176,3.735863447189331,3.692265748977661,3.435695171356201,3.4440701007843018,3.3880820274353027,3.6468639373779297,3.763538122177124,3.9247894287109375,3.747931957244873,3.414644956588745,3.505434274673462,3.5246169567108154,3.7216217517852783,3.5349791049957275,3.791555881500244,3.677692413330078,3.347536563873291,3.7019128799438477,3.496246099472046,3.7691948413848877,3.719027519226074,3.507577657699585,3.478745222091675,3.460455894470215,3.756859540939331,3.55802321434021,3.4266839027404785,3.373176097869873,3.313394546508789,3.643770933151245,3.597215175628662],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Until now hermes has only been evaluated on the formula prediction task, but we believe that the three-stage decoding pipeline and the sampling strategy for multi-level expansion in hermes can further be extended to other forms of code such as sql and linux commands.Another limitation is how to unify the experiences of hierarchical expanding and other formula writing orders, because we think that the human reasoning order in writing formulas may be a mixture of top-down, bottomup, left-to-right, etc.Hermes proposed new ideas on variable high-level expansion, but it is desirable to be further integrated with our experiences.\",\"To further inspire the follow-up work, we summarize our limitations as follows: 1) we only preliminarily reveal the overthinking phenomenon in the open-world scenario, and explore how to mitigate and utilize it during inference.We do not continue to conduct more in-depth research on the broader forms of overthinking in the open-world scenario and do not explore whether there are differences in its performance in different models.In addition, whether it can be solved or alleviated by other ways, such as training methods.we leave how to achieve the best accuracy-speed trade-offs to subsequent research.3) we have preliminarily verified that our method can be compatible with more detection algorithms and models, and look forward to exploring more methods and models.\",\".First, it only focuses on answering existential positive first-order logic queries but does not support the negation operation.We will later address this limitation by modeling the negation operation.Second, we uti- lize bert as the backbone model for inductive generalization due to computing resource limits.We plan to investigate the use of more powerful pre-trained language models with stronger generalizability in future research to improve inductive logical reasoning over kgs.\",\"There are two limitations: although mt r include three types of reasoning types , we only focus on relation reasoning task.For other tasks, it is also necessary to construct more datasets with the fusion of multiple reasoning types.Our primary focus remains monotonic reasoning, however, the combined reach of deduction and induction is only the tip of the iceberg of human reasoning.This also inspires us to focus on more nonmonotonic reasoning and more logical combinations.\",\"The proposed reasoning circuits framework intends to replace the need for thousands of annotated examples with a strong inductive bias of structured rationales.There is two issues with this approach at a conceptual level: 1.It may not always be possible to break down a multi-step reasoning problem cleanly into discrete reasoning steps, and another related issue it increasing complexity of the circuit with the complexity of the task.for the design of these reasoning circuits a researcher must develop a thorough understanding of this reasoning task, so that the final circuit design broadly covers all possible types of reasoning problems expected to be solved.An under- or illdesigned reasoning circuit may cause the system to either not support a certain portion of problems or produce non-sensical outputs.Essentially, there is trade off between a tighter control over reasoning by investing in a deep understanding of the problem leading to a comprehensive reasoning circuit design and lower annotations budget, versus, less control over logic and depending on a large number of annotations which allow the model to discover this logic on its own at much higher cost of large scale annotations budget.At the implementation and operations level one of the the key limitations our proposed system is the number of inference steps to solve the problem.The number of times model inference may be needed to solve a single example is equal the length of the longest task sequence chain in the reasoning circuit.One possible solution for this could be by training the model to solve the entire problem by generating all the steps of reasoning and the target string in a single inference step and could massively reduce inference time and costs.\",\"This paper presents the conceptual foundations of a novel architecture for narrative-based language understanding, along with an illustrative proof-ofconcept implementation.As such, it has been operationalised on a small scale only.Scaling up the approach to real-world applications is a highly non-trivial task that would not only require large investments but also significant innovative research efforts.Moreover, important aspects of the theoretical model have not been included in the proofof-concept implementation, in particular when it comes to modelling the confidence of an agent with respect to its beliefs and narratives.\",\"Common everyday things change over the years.While we try to choose ones that are in children\\u2019s vocabulary, over decades, devices evolve and humans change in which things they interact with more frequently, affecting which relationships would be more prominent in an average person\\u2019s mental model.So the parts mental models in such a dataset may not stay constant over time.other important future directions include to explore how more coherent mental models can help in complex reasoning tasks about everyday things, combine these parts mental models with mental models along other dimensions e., as well as using our dataset of commonsense queries about everyday things as a source of follow-up questions for existing qa tasks e.this paper only focuses on relationships between parts of everyday things.However, our approach parrot-con is easily extensible to other applications such as: \\u2022 spatial relations in other domains e.for geographical distances, we can similarly impose constraints on inverse relations like closer and further \\u2022 temporal relations e.\",\"The major limitation of the present study is that the effectiveness of the proposed method has been confirmed only for a single task.This is because most existing reasoning tasks are relatively simple that they can be solved by a single external tool at most.For example, most existing numerical reasoning tasks provide self-contained questions; that is, all the required knowledge is included in the questions.In such tasks, a calculator is all that is needed as an external tool.However, it would be rare for a single external tool to be sufficient in real-world applications such as medical text analysis.\",\".2, the current selection of fundamental reasoning skills for language models is limited by the availability of well-defined tasks and clear definitions of those tasks, as well as the availability of sufficient training data.As a result, some skills may overlap or may not be fundamental enough.For example, simple qa skill may overlap with ner skill to some extent.additionally, the selection and combination of fundamental reasoning skills can be further explored.For example, the inclusion of numerical reasoning ability to solve mathematical problems.Additionally, methods for skill-centric pre-training corpus construction can also be explored to improve the effectiveness of these skills.\",\"Limited number of operators considered following previous methods , we only consider binary operators.As we adopt a code-style output format, it is possible to introduce other non-binary operators supported by the python interpreter, e.however, obtaining labeled data with such operators may require laborious efforts.We believe it is an interesting research question on exploring how to teach models to solve practical questions e., math word problems, by writing code in a low-resource setting.Limited performance due to greedy decoding all the results we report in this work are produced via greedy decoding.A recent work reports that making large lms generate multiple answers and selecting the answer with the most votes can boost performance by a large margin.However, performing beam search for symbolic neural reasoners, e., deductreasoner, can be challenging in that searching space increases exponentially with the number of variables in the question.Designing effective beam search strategies for symbolic neural reasoners is a promising direction.\",\"We identify a few limitations of this work.First, our approach is applicable to language models pretrained with tables, which may not always be included in all language models.Second, our approach\\u2019s limited improvement in commonsense reasoning tasks suggests that its effectiveness may depend on the specific task and the level of structured reasoning required.\",\"Since rule abduction and inversion utilize the same groundings as the original rules, neuro-symbolic kgc models that are based on grounding the entire rule will not benefit from these augmentations.Abduction and inversion also require the model to be trained on a knowledge graph that contains the inverse relations r\\u22121 for each relation r.finally, since rnnlogic+ has a separate rule embedding for each rule, performing rule augmentation increases the number of parameters in the model and leads to longer training times and larger gpu memory consumption.\",\"In this paper, we provide an overview of the current state of knowledge on reasoning in large language models.Reasoning is a broad concept that encompasses various forms, making it impractical to summarize all related work in a single paper.Therefore, we focus on deductive reasoning, as it is the most commonly studied in the literature.Other forms of reasoning such as inductive reasoning and abductive reasoning may not be discussed in depth.Additionally, given the rapid evolution and significance of reasoning within large language models, it is crucial to note that new contributions may have emerged in the field concurrent with the writing of this paper., which emphasizes reasoning via language model prompting.Our coverage may not extend to papers released during or after 2023 such as evaluation on chatgpt.As such, we recommend readers to check the papers that cite this survey for a more comprehensive and updated understanding of this field.\",\"The outcome on multiple datasets verifies the powerful reasoning ability, which even works on models with only several billion parameters.However, our self-thinking procedure utilizes only one dataset, gsm8k, and the available training set size is only 7.the main reason is the scarcity of high-quality datasets with rich reasoning paths.And, collecting such data incurs huge computation costs and expensive human resources.Another limitation is that we have not conducted experiments on bigger language models, such as gpt-3 and palm, due to the expensive usage costs and the fact of no open-source codes.\",\"Our work is constrained into multi-choice question answering system and limited to common sense reasoning tasks, lacking more exploration in other reasoning tasks, e.arithmetic reasoning , conversational reasoning and symbolic reasoning.\",\".It also sheds light on developing better neuro-symbolic systems in general.Limitations despite the strong performance of pangu, we identify several limitations that call for further improvement.The first major limitation lies in efficiency.Because pangu requires an lm to iteratively score candidate plans, it is resource-consuming in terms of both time and computing.Compared with arcaneqa, which efficiently handles complex questions in kbqa, pangu is about twice as slow for both training and inference and consumes about twice as much gpu memory when using the same lm.Concretely, to predict a plan of l tokens, generation-based methods involve using an lm to do l forward passes.For pangu, the number of forward passes is proportional to the number of candidate plans, which can range widely.that being said, we would like to note that both arcaneqa and pangu are more efficient than most existing methods due to their efficient dynamic search design.For example, pangu is 8 times faster than rng-kbqa, according to the numbers reported in gu and su.Nonetheless, we list efficiency as a limitation because there is clear potential for further improvement.Second, though pangu has shown some promising results with codex, the true potential of enabling few-shot grounded language understanding with pangu has yet to be fully realized.We only experiment with a straightforward scoring function and have not experimented with different prompt designs systematically.third, though orthogonal to the general framework of our proposal, in our current instantiation, we assume gold plans for training.However, gold plans can be expensive to collect for some environments.Exploring fine-tuning lms with weak supervision can be an interesting direction.In addition to proposing candidate plans to the lm, the agent may also respond to the lm with rewards based on its decisions.Finally, one important merit of pangu, controllability, is under-explored in this paper, because it is not very necessary for kbqa.While for tasks like text-to-sql parsing, controllability could be a highly desirable property.Intruders may manipulate text-to-sql models to launch database attacks via sql injection.With pangu, we can easily get rid of malicious sql operations in candidate enumeration.However, for generationbased methods, such controls are hard to achieve during generation because the decoding process can be shortsighted\\u2014it is difficult to tell whether the current predicted token would lead to a malicious operation several steps later.\",\"One limitation of our method is that when there are rules of different lengths, the final result is decided by ensemble, not by building a model to generate a single rule with the best length.The second way is more natural and important because figuring out the length of the rule is also a key part of symbolic reasoning.However, it requires more parameterization and a more advanced way to optimize.\",\"We build a new benchmark for syllogistic reasoning.The limitations are mainly in the experiments part: due to the limited human resources, our test set is quite small, which may not support training large models directly.We evaluate all models by comparing their predictions with the groundtruth\",\"In this work, we experiment with gpt3, t5, and deberta.Other large pretrained lms, such as palm , is not covered in this work.We do not experiment with methods such as fine-tuning gpt3 due to the computation cost.The main purpose of this work is to uncover and analyze the fundamental limitations of lms on symbolic and arithmetic induction instead of improving their performance of reasoning tasks, so we do not directly compare the mitigation methods with the previous work such as scratchpad and in our experiments.\",\"The neuro-symbolic rule learning presented in the paper can handle most generic text-based games.Only in a few specific use cases, additional training of the amr parser would be required.Since amr is used for symbolic representation for text-based games, the vocabulary of the extracted triples is limited by the vocabulary of propbank semantic roles.For applications in a very specific kind of domain where the predicates and entities do not match with this pre-defined vocabulary , the amr semantic parsing engine needs to be retrained first on such specific data before using it for rule learning.However, even in the cases where the testing environment requires additional rules, nesta allows human-in-the-loop debugging to conveniently add them making it adaptable to generic environments.\",\"Mathworld is limited to cover math story problems using the four basic arithmetic operators.Furthermore, within the space of such problems, it does not cover \\u201csecond-order\\u201d msps.Neither does it cover negation nor inequalities.We only consider datasets with msps written in english in this work.However, mathworld should in principle be able to cover the same type of problems formulated in other languages as well.An obvious limitation of this work is the low performance on the task of solving msps.\",\"Due to the limited ability of the policy model, most theorem still failed to find the proof because of poorly suggested proof steps.Predicting the proof step from the proof state requires substantial reasoning ability.It\\u2019s observed in the experiment that the language model tends to produce the same proof step in training data, and is unsatisfactory in generalizing for new states.Another limitation resides in the proof-level value functions.Although the performance of the proof-level value functions shows promising improvement in the value function test set.The end-to-end pass rate diminishes the performance gap.This accounts for two major reasons: 1) weak policy model fails to produce correct action even if our value function correctly located the state to expand.2) our value function\\u2019s performance is still behind the performance threshold where the value really helps the search drastically.One future direction is to enhance the language model for better reasoning ability by using a larger language model or adding symbolic reasoning into the language model to produce more reasonable proof steps and better evaluate states\\u2019 value.\",\"Alert aims to encompass a wide range of reasoning skills, but some reasoning skills are missing, specifically in regards to symbolic reasoning ) and compositionality reasoning , cogs and cfq ).In terms of computing power, we have experimented with models that were accessible to us.We acknowledge that there are larger models that we were not able to train due to the limitations of our computational budget.During our analysis, we discovered that some datasets contain noise, where even human experts are unable to provide accurate answers for certain instances.While it is important to address this issue, it is a time-consuming process to carefully review and clean each instance in the dataset.\",\"We identify two main limitations of programfc.First, despite being complex in their surface form, the claims in the hover and feverous datasets mostly require only explicit multi-step reasoning, i., the decomposition can be derived from the claim\\u2019s syntactic structure or how the claim is framed.This lowers the difficulty of generating reasoning programs.However, for many real-world complex claims, the reasoning is often implicit.For example, for the claim \\u201caristotle couldn\\u2019t have used a laptop\\u201d, the reasoning program is: answer_1 = question; answer_2 = question; fact_1 = verify; label = predict generating reasoning programs for such implicit complex claims requires a deeper understanding of the claim and also access to world and commonsense knowledge.We conducted preliminary experiments on these types of claims, but we found that our codex-based generator struggled to produce a correct reasoning program.This highlights the gap in applying our programfc to fact-check real-world claims.second, programfc incurs a higher computational cost than baseline end-to-end fact-checking models.It requires calling large language models for program generation and further calling multiple sub-task models.This results in the actual computational time that is \\u223c4\\u20135\\u00d7 higher than for an endto-end flan-t5 model.\",\"In our study, we address the issue of linguistic forgetting via the injection of the strict non-linguistic skill of quantitative reasoning.Although quantitative reasoning with llms is an active research area, as discussed above, further fine-grained studies are required to extrapolate this behavior to tasks that leverage synergies between aspects of both linguistics and non-linguistics - such as math word problems or data-to-text generation.Further, investigations into the linguistic forgetting tendencies of different languages would lend an insight into the role of linguistic morphology in this behavior.The restrictions from our in-house gpu resources does not allow scaling this study to more recent models that exceed 100 billion parameters, although, due to the sharing of similar architectures, we forecast our findings to hold despite of model scaling.\",\".\\u2022 the current work is mainly applicable to log- ical entailment problems, where one needs to solve a classification problem of whether a goal can be proved, disproved, or neither proved nor disproved based on a theory., where one needs to apply logical reasoning to answer questions such as \\u201cwhat color is fiona?\\u201d.\\u2022 the current work assumes all the rules are given as input and the rule set is small enough to be included in the prompt.\\u2022 the calls made to the lm modules in lambada are dependent on the value from the previous call.That is, we need to wait for the results from one call before we decide what the next call must be.\\u2022 while we showed that lambada is more efficient than si in terms of the number of inference calls it makes to the lm, it still requires many more calls to the lm compared to approaches such as cot, hence increasing the required computation and cost.\",\", structured models have exhibited the capacity to outshine large language model prompting on mathqa.Additionally, the recent emergence of instructionfollowing models , exemplified by alpaca , has prompted our interest in equipping large language models with mathematical reasoning capacities while maintaining the integrity of their underlying language understanding capabilities.Limitations the methods we have employed for prompting and fine-tuning have yielded noticeable improvements, yet certain limitations persist within practical applications.To achieve optimal performance, we continue to rely on prompting using large language models, which prove to be costly for the research community.Furthermore, retrieval efficiency may present a challenge when dealing with extensive training sets, as identifying the top m exemplars for each example becomes increasingly time-consuming.Consequently, devising a more efficient algorithm to expedite the retrieval process represents a potential area for future exploration., we were unable to incorporate this approach due to budget constraints.Additionally, although training data has proven beneficial, the gains for smaller models are insufficient to surpass the performance of large language models.This observation may indicate the necessity for a fundamentally different model design or a superior pre-trained model or code-t5 ) as a more effective basis for fine-tuning.\",\"The cogen model introduced in this paper uses temporal relations as a process of abductive reasoning.Although, temporal relations have been shown to be very useful in abductive reasoning , the measure of the effectiveness of other types of relations about an observation have not been evaluated in this paper.In addition, because of the unavailability of a large number of human evaluators, we randomly selected 100 selected results as opposed to the entire result which would have been ideal.\",\"Our proposed thinksum has demonstrated strong performance on thirteen challenging big-bench tasks.However, it is important to acknowledge certain limitations of the system.Firstly, as the number of objects or facts that are reasoned over increases, the computation cost will also rise.However, increasing the number of objects will also make the task harder, and direct prompting may cease to work at all , while thinksum offers a generalizable methodology, as the atomic think operations do not increase in complexity as the number of objects grows.Secondly, when solving a new task, it is necessary to expend human effort to select specific operations in each step, as outlined in \\u00a72.This limitation is shared with prompt engineering of all kinds, including direct or chain-of-thought prompting: finding a prompt for a new task requires an often-cumbersome prompt engineering procedure.We have described thinksum as a general twostage paradigm, with an external inference step.This generality aims to facilitate the adaptation of thinksum to new tasks, with minimal modifications to the think and sum steps.Work on automating the prompt engineering procedure is a promising path towards overcoming this limitation.An alternative to prompt engineering that does not require such human effort is tuning of prompts or model parameters; however, this remains impractical for gpt-3-scale models, and attempts to tune models directly on symbolic reasoning chains have met with limited success.Last but not least, thinksum has mainly been evaluated with gpt-3 and instructgpt models.To further improve performance, it may be beneficial to apply thinksum to more recent instruction-tuned models such as flan-palm , text-davinci-003, chatgpt, and gpt-4, which seem more capable of robustly performing think steps.\",\"Our proposed method needs to construct counterfactual examples to estimate the natural direct effect of disconnected reasoning during the training phase, thus we need a little more gpu resources and computational time.However, the need of resource occupancy and time consumption of our approach does not increase during inference.Another limitation is that we use the learnable parameters to approximate the yk1,k2,k\\u2217.\",\"In this study, we provide a survey of reasoning with language model prompting.1 and will continue adding more related approaches with more detailed analysis.Despite our best efforts, there may be still some limitations that remain in this paper.due to the page limit, we may miss some important references and cannot afford all the technical details.We mainly review the cutting-edge methods within two years in \\u00a73, mainly from the acl, emnlp, naacl, neurips, iclr, arxiv, etc., and we will continue to pay attention to and supplement the latest works.Most of the reasoning benchmarks mentioned in \\u00a75 are gathered and categorized from the experimental part of mainstream works.The definition and boundary of each task may not be accurate enough.Besides, our work may miss some kind of reasoning tasks such as reasoning with generics , default inheritance reasoning , non-monotonic reasoning in nlp, and will try our best to fulfill this gap.we give detailed comparisons and discussions of language models and prompts in \\u00a74, and list some promising future directions in \\u00a76.All the conclusions are proposed and further speculated upon empirical analysis of existing works which may not be macroscopic enough.As the field evolves faster, we will update the latest opinions timely.\",\"Of pretrained lms on arithmetic reasoning and symbolic manipulation.We experiment with three simple symbolic manipulation tasks and show that improving the locating and induction capability of lms can be important for further improving their performance.\",\"This work is currently limited to the action chain as the abstract summary of the complete explanation for the given limited observation., considering the progressive textual descriptions as the complete explanation.We hope our work can advance the reasoning ai system research community.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"13_reasoning_pangu_symbolic\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"13_reasoning_pangu_symbolic\"],\"textfont\":{\"size\":12},\"x\":[10.874232292175293,11.119002342224121,10.981647491455078,10.808367729187012,10.809622764587402,10.727598190307617,11.235118865966797,10.809904098510742,10.98637866973877,10.827566146850586,11.08499526977539,11.307719230651855,10.88953685760498,11.149758338928223,10.733148574829102,11.25771427154541,11.155957221984863,11.10452938079834,10.955784797668457,11.314067840576172,10.891727447509766,11.092827796936035,11.022148132324219,10.823293685913086,10.96877384185791,10.94570541381836,10.718742370605469,10.889328956604004,10.842119216918945,11.125683784484863,10.754960060119629,10.944783210754395,10.884793281555176,10.970834732055664],\"y\":[-0.45661598443984985,-0.8257500529289246,-0.6255955100059509,-0.9439091682434082,-0.9427673816680908,-0.8424916863441467,-0.6473026871681213,-0.9544345736503601,-0.7530277371406555,-0.512045681476593,-0.587819516658783,-0.5775178074836731,-0.8590884804725647,-0.7916987538337708,-0.9284658432006836,-0.6803271174430847,-0.5987721085548401,-0.8145840764045715,-0.686672568321228,-0.5771040916442871,-0.6324397325515747,-0.8273049592971802,-0.8366799354553223,-0.9675044417381287,-0.7752751111984253,-0.6601759195327759,-0.4496540427207947,-0.8908569812774658,-0.9884002804756165,-0.7356981039047241,-0.5004253387451172,-0.7295451164245605,-0.9401647448539734,-0.7436398863792419],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"The study of language model in their alignment to linguistic theories are interdisciplinary and hence usually hard to find explicit connection between language model and theories.In this paper we claim that a generative model, ciwgan, can model both phonetic and phonology features.However, the two features are learned by two ciwgan instances from disjoint training data sets.Our finding couldn\\u2019t support or deny the following statements that are of researchers\\u2019 concern: 1.Generic gan model can learn phonology features like ciwgan.ciwgan can model phonetic and phonology features simultaneously from a single dataset.\",\"We discuss the limitations of our work.First of all, our model lyra is build upon pre-trained language models including bart and gpt-2.Although our method is much more data friendly than previous methods in that it does not require training on melody-lyric aligned data, our pipeline may not apply to low-resource languages which do not have ptlms.Second, our current adoption of melody constraints is still simple and based on a strong assumption of syllable stress and note duration.We encourage future investigation about other alignments such as the tone or pitch variations.Lastly, although we already have the music genre as an input feature, it remains an open question how to analyze or evaluate the generated lyrics with respect to a specific music genre.\",\"The major limitation is that we test our method only on a binaural speech dataset, in which there is a person moving slowly while speaking.Because this person moves slowly, the doppler effect is not so obvious.We will try to find or collect a sound dataset of a source moving at high speed, such as a running man, flying objects, or vehicles, and further, analyze the experimental phenomena at different speeds of the moving source.\",\"The speech recognition used was the wav2vec 2.some of the errors may have been influenced by the fine tuning of the final layers; this could lead to errors being corrected by the language model.0 produces character output which we transformed to phonemes using a grapheme-to-phoneme tool; this will lead to some loss in the variation.These limitations can be overcome to some extent by using a wav2vec 2.0 phoneme model which we plan for our next experiments.We have only worked on french to date, even though we believe that the method is applicable to other languages.Finally, the experiments were done only on timit.While this is a balanced dataset, use of other datasets will likely lead to better insights.\",\"The current system may require the user to have some music knowledge to compose the word boundary prompt from music.Hence, more efforts need to be made to fulfill this gap before such a system can operate fully automatically without the human user providing word boundary prompt themselves.We use the back-translation of mono-lingual data to augment the parallel training data, but the quality, especially the text style of back-translations has room to improve.Although we have tried using iterative bt to gradually refine the backward direction mt model to adapt its outputs to lyric style, we found some errors gradually accumulated in the back-translated data, which finally made our model perform unsatisfactorily for negative sentences, together with the decrease of controlling effectiveness.Further exploration is needed in this aspect.Similar to chat text, lyrics are usually composed in short sentences.Sometimes it would be challenging to guarantee the consistency of style and meaning for different sentences, if the current sentencelevel translation system are adopted.Hence, for building future lyric translation systems, it would be a better option to translate the lyrics directly at the paragraph level or document level.\",\"Of these self-supervised pre-trained speech models while fine-tuning them on downstream speech translation tasks.Acknolwedgements we are thankful to the organizers of the iwslt 2023 low resource and dialectal shared tasks.This work was generously supported by nsf grant iis-2125466 and by a meta sponsored research award.\",\"The cloning model used, yourtts, is trained on the vctk dataset which consists of high-quality speech signal.It is therefore unclear whether the same accuracy would be obtained with lower quality signal which may contain some background noise.The selection of a person ne replacement does not currently account for continuity: if the same person entity is referred to later, it may be substituted with a different entity to the previous occasion.In addition, the back-off strategy ignores aspects such as gender.To show the approach feasible, very little optimization was performed.Further training and parameter optimization is likely to lead to improved performance for both asr and ner models.The approach is currently only implemented for person nes but it could be extended very simply to other types of nes.However, the degree to which other entity types require obfuscation in speech is not clear to us as mentions of organizations may well not be identifying at all.\",\"Since each chinese character contains 1 syllable, our proposed model can control the number of syllables in the generation by the number of generated tokens.However, this method does not apply to languages with multisyllabic words.To rewrite lyrics with multisyllabic words while maintaining the same number of syllables, a special technique such as syllable-level subword tokenization may be needed.\",\"The limitations of our work include: 1) in our work, the structure of lyrics is the chorus and verse parts of songs, and it is learned in a data-driven manner, which highly relies on data quality.2) the settings of the lyric-to-beat model will limit the effect of our model.For this work, we make an assumption that all songs are with 4\\u002f4 time signatures for the lyric-to-beat model.If the time signature is not 4\\u002f4, we need to re-train the lyricto-beat model.3) better \\u201dbeat construction\\u201d can be investigated, such as using a language model to generate the beat sequence.We only explore the simple method and achieve satisfying results.4) the model trained from scratch may not achieve satisfying results.And a gpu with at least 20g memory may be needed to use the pre-trained language model to reproduce our work.\",\"In this study, we propose differentiable segmentation to learn how to segment speech from the underlying translation model, and verify its effectiveness on simultaneous speech translation.However, since it can be jointly trained with the underlying task , differentiable segmentation is not limited to the simulst task, but can be generalized to more streaming\\u002fonline tasks, such as streaming automatic speech recognition , simultaneous machine translation , real-time text-to-speech synthesis , online tagging and streaming parsing.\",\"While we show a clear benefit of data augmentation when the amount of available training data is limited, the performance gain seems to be lower when a larger quantity of manually transcribed speech data is available.Whether data augmentation is always beneficial is an open question.We did not measure the effect of sociolinguistic variables on the performance of the models.A risk might be that especially for the models for gronings, which were developed on the basis of speech data from only a few speakers, results might be negatively affected by differences in language background.We likewise did not measure the effect of nonlinguistic variation on the performance of the models.finally, we evaluated the effect of training data size and data augmentation on four different minority languages or language variants, each using a single test set.Of course, using a different test set might have affected the results.However, given that the pattern of results was similar across a range of language varieties we do not expect this difference to be large.\",\"The corpus and therefore also the asr baseline model only cover read speech.We have not tested the model on spontaneous speech, but we expect it to perform significantly worse on this type of data.Our data collection process for swiss german speech with standard german transcripts is designed to collect large amounts of data in a costefficient manner.We estimate costs to be 4 to 6 times lower compared to the transcription of existing recordings.However, there is a downside to our approach.Because it is based on a given standard german sentence, it can lead to swiss german speech that\\u2019s closer to standard german than the swiss german encountered in everyday conversations.The severity of the shift towards standard german depends on the individual speakers and their ability and effort to produce swiss german representations that are close to how they would speak in everyday conversations.While we made every effort to include as many different dialects as possible in the corpus, there are still strong dialects with a comparatively low german-speaking population that are insufficiently or not at all represented, e.some dialects from the canton of fribourg.This is due to the huge dialect diversity in switzerland.The gender ratio is not balanced for some dialect regions in the test set, especially not for vs, where the test set is female-only because we did not succeed to recruit any male speakers from this region during phase 1 of the data collection.However, preliminary experiments do not show a significant difference between genders in swiss german asr performance, so we do not expect this to lead to skewed results.Our asr baseline model and other models trained on the corpus may perform below average for children and people above seventy due to the lack of training data for these age groups.\",\"Dataset noise as the audios are obtained from the videos on youtube, the quality of the videos will have an impact on the quality of the final transcript.For example, inferior recording equipment may affect the quality of the sound, although we have done noise removal to keep the quality, the presence of background noise will cause some losses in the transcribing process.Relationship between transcripts and scenes in this work we get the transcript of shiba inu dogs, and we also find that the dataset covers a variety of activities and scenes.There may be an interesting relationship between the dog vocal units and the environment including the scene and activity.However, we did not quantitatively analyze the relationship.Considerably more work will need to be done to discover semantic information in dog barks.6 we cluster the syllables and assign phonetic symbols to them.1 we evaluate the result by mos.\",\"The adversarial learning still requests a proper selection of hyperparameters, otherwise the training procedure could be unstable.Besides, training speech diffusion probabilistic models typically require more computational resources, and degradation could be witnessed with decreased training data.Our proposed model lowers the requirements for high-quality speech synthesis, which may cause unemployment for people with related occupations, such as broadcasters and radio hosts.In addition, there is the potential for harm from non-consensual voice cloning or the generation of fake media, and the voices of the speakers in the recordings might be overused than they expect.\",\".Generally, s2st circumvents traditional cascaded systems which concatenate asr, mt and tts with high latency and high requirements of datasets.There are 3,000 around languages in the world who do not have their own writing systems or textual vocabularies.Through our duplex s2st models, we hope to be friendly to these languages so that more and more languages can be covered.\",\"There are majorly two limitations: firstly, we collect a chinese singing voice dataset and test our method only on this chinese dataset due to the difficulty of recruiting professional singers in different languages.secondly, our method adopts the diffusion model in pitch modeling and the postnet, which require multiple inference steps.\",\"Since two-pass direct s2st models require linguistic units as the target for the first-pass decoder, they cannot be used when the target language is unwritten.Compared to cascaded s2st systems, direct s2st systems require more data preparation steps, including training a hubert model, synthesizing target speech with a tts model, extracting discrete units with the hubert model, and training a unit-based vocoder, etc.Moreover, the target audio quality of direct speech-to-unit systems relies on the quality of discrete units generated by selfsupervised discrete models.It further depends on the availability of speech data to train hubert models for the target languages.Because s2st systems could generate speech that does not necessarily represent the source speech\\u2019s content, there is a potential risk of conveying wrong information.\",\"This study stems from a novel idea for chinese historical phonology studies.As few direct predecessors could offer hindsight, there are quite a few limitations to this study that may be addressed with further work.while the initial-final-tone decomposition is convenient in this context, it also limits the transferrability of the proposed tool to languages outside of the sinosphere.This calls for further exploration of more generalizeable approaches to phonological representation learning.finally, making full use of the dataset is crucial, and the stochastic train-test split used in this study may leave out important hints.Alternative sampling strategies, such as crossvalidation or bootstrapping, could enhance the robustness of the results.\",\".First, the univpm combines both restored clean audio and original input audio for downstream speech recognition, while without any trade-off to weight them.For example, under extremely noisy conditions the restored clean audio plays a more important role, while in less noisy scenarios the original audio may provide more valid information.Some weighting strategies to select the most effective audio information could benefit the downstream speech recognition.Second, the proposed clustering and viseme-phoneme mapping are actually unsupervised schemes, so that it could be promising to extend our univpm to the popular self-supervised learning framework, in order to make full use of the abundant unlabeled data.\",\".while we compared our models with conformer and transformer-based aed and ctc models, we did not include rnnt models due to their higher compute resource requirements.To accommodate deployment constraints, we employed a smaller model with approximately 60 million parameters, which limited its performance.building upon our previous success in adapting a non-streaming model for end-to-end speech-to-intent detection in customer support voicebots , we are motivated to investigate the feasibility of developing a single joint model for automatic speech recognition , end-of-speech detection, and spoken language understanding.Additionally, we are keen on exploring the development of multilingual asr models.\",\"One limitation of this work is that we have included results for only eleven languages.Training asr models, even on small datasets, requires significant computing and financial resources.Second, there are not that many freely available and well prepared asr datasets that are readily compatible with all four asr architectures.We sought to select a diverse set of languages and datasets with varying features in order to provide, we hope, a reasonable snapshot of how the state of the art performs in low-resource settings.\",\"As mentioned in our experimental setup, we provide results of av-s2st in lrs3-t with synthesized target speech, similar to the pioneer literature in s2st.One of our future directions is to develop a better benchmark dataset to improve translation performance.As mentioned in our results analysis, the bleu scores heavily depend on the asr quality, which may not accurately reflect the speech translation performance.Future directions could be improving asr quality or exploring other evaluation metrics without reliance on asr models.Av-transpeech lowers the requirements for audio-visual speech-to-speech translation, which may cause unemployment for people with related occupations such as interpreter and translator.In addition, there is the potential for harm from nonconsensual voice generation or fake media.The voices of the speakers in the recordings might be overused than they expect.\",\"Research on the speech-to-singing conversion is important for human voice study and useful for practical applications such as computer-based music productions or entertainment.However, current sts approaches require an input condition of a fine-grained target f0 contour, which is always unavailable.In addition, the f0 contour of a singing utterance often possesses rich speaker-related infor- mation, which still needs further disentanglement.Finetuning f0 contours in real applications brings significant extra work.One of our future directions is to simplify the input conditions, such as musical scores.Furthermore, the preliminary attempt at the zero-shot sts task may lead to a better perspective.Except for positive applications, sts systems may face ethical concerns.With the development of speech\\u002fsinging voice synthesis technology, the cost of faking an utterance of a specific individual is gradually declining.Researchers need further consideration of the regulation and recognition of speech\\u002fsinging voice synthesis.\",\"One limitation of our approach is that incorporating acoustic features from an ssl speech encoder, in our case wavlm, introduces extra latency overhead, as we use a standalone asr model for firstpass.Therefore, our approach may not be appropriate for certain applications that have exceptionally low latency constraints.Another limitation is that while multi-modal llms have the potential to improve asr performance, they can be more complex and harder to interpret than text-only llms.This makes it more challenging to understand the model\\u2019s decision making process or debug any potential errors.\",\"In this work, we have employed different strategies to identify the important utterances from early cohort.However, since a voice assistant system consists of many components, such as wakeword, automatic speech recognition, nlu, dialogue manager, where errors occurring in one step might result to the final overall incorrect response.We have not discussed or considered the interaction among these components in this study.Last but not least, weak signal learning using users\\u2019 feedbacks has shown to be beneficial in many studies, it is important to classify and identify the types of feedbacks that are relevant and those that are not relevant to nlu improvement.\",\"The current study is has some limitations.First, we used a single audio-transformer model, the pretrained wav2vec2.0-base, as a test bed to validate the fine-grained ans and couple them to their bn signatures.On the one hand, various audiotransformers have been proposed in the literature.On the other hand, the parameters of a pre-trained model are fine-tuned by downstream tasks and previous studies have shown that fine-tuning may lead dnns to increase their brain similarity.Thus, it would be interesting to explore whether there are consistent an-bn coupling patterns across different models, either pre-trained or fine-tuned.In addition, it is necessary to investigate these patterns across different languages.Second, existing studies have shown that audiotransformers are able to learn sound-generic, speech-specific and language-specific representations and those hierarchical representations are akin to the cortex.Thus, it would be interesting to explore whether the fine-grained ans carry such multi-level representations, and link them to brain responses.Third, the reproducibilty between the two sessions was high regarding to most of the results , but it was relatively low in some results.We speculate that this is the consequence of relatively smaller fmri training samples but much larger amount of vs-dbn model parameters in the session of forgot, in which the number of subjects is smaller but the fmri spatial resolution are higher.Higher spatial resolution results in much larger number of valid voxels compared to that in pieman and consequently more visible units in the vs-dbn model.Last but not least, the analyses presented in this study are intrinsically limited by the coarseness of spatial and temporal resolution of fmri data.Mapping from sound to an interpretable representation involves integrating neural activities on different spatial-scales down to sub-millimeters and on different timescales down to milliseconds.\",\"There are several limitations of this study.First, it is done on one language pair although we believe this should not qualitatively change the results.Second, only one set of standard model sizes was evaluated for ast student and nmt expert; we expect it be in line with reported findings for nmt.Finally, while alluding to the potential of using large pre-trained asr models instead of manual transcripts for il-based ast, our current work must be seen as a proof-of-concept experiment where we train asr models on a few hundred hours of audio, and discard the manual transcripts in il training, showing the feasibility of our idea.\",\"Our unsupervised speech recognition approach requires tools to phonemize text for the language of interest.Phonemizers are not available for all languages and this presents a bottleneck.0 model training with graphemic text units such as letters.We train bilingual unsupervised machine translation models with 2.1b english sentences and at least 46m sentences for the non-english language.For extremely low-resource languages, collecting millions of sentences for model training can be challenging.The feasibility of mbart-based online back-translation approach in this setup remains to be validated.\",\"Our theory currently assumes that input speech features are quantized into discrete units, as in , while preserving all the linguistic information in the speech.As a result, our theory does not account for the loss of linguistic information during the quantization process, as often occurred in realistic speech datasets.Further, more recent works have shown that continuous features, with the help of additional regularization losses, can achieve almost perfect asr-u.Such phenomena is beyond explanations based on our current theory and require generalizing our theory to continuous speech features.Further, our model assumes that sufficiently reliable phoneme boundaries are fed to the asr-u system, and kept fixed during training.It will be interesting to extend our framework to systems with trainable phoneme boundaries, such as the wav2vec-u systems, to better understand its effect on training stability.\",\"Many challenges remain in the follow-up of our work.Although our method achieves consistent improvements in most experiments, we notice that the benefit is limited in the noisy sets, especially under a high down-sampling ratio.This drives us to develop a more robust down-sampling method for preserving meaningful information even with high compression.\\u2022 our method compresses all the input acoustic features with the same ratio, where the ratio is determined according to the whole dataset.However, the speed of each audio is different, which results in obstacles to unified down-sampling.Ideally, each sample should be compressed with a self-adaptive ratio.\",\"We are evaluating s2st in an artificial setting given that we have to synthesize the text references.In fact, since there was no metric capable of evaluating the quality in speech, there was no motivation to build such benchmarks either.However, we expect that next benchmarks for the task will have speech references be- cause of the rise of end-to-end s2st systems and their quality increase.Blaser paves the way so that we can take advantage of such benchmarks when they appear.Our metric works at the sentence-level, by embedding the entire sentence into an intermediate space.moreover, we are aware that sometimes sentence embedding do not discriminate different numbers or words that belong to the same word family, which may disregard impactful errors such as the change of a number in the translation output.they were all paid a fair rate.We can not open-source the data form our experiments given that our sources are shared under no-derivative license.\",\"We list the limitations of our work as follows.Firstly, the model architecture we use to localize the stuttering speech is simple.secondly, we only test the english datasets.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"14_speech_asr_s2st\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"14_speech_asr_s2st\"],\"textfont\":{\"size\":12},\"x\":[14.14594554901123,14.336759567260742,14.431232452392578,14.512471199035645,14.361120223999023,14.355290412902832,14.443986892700195,14.307058334350586,14.301825523376465,14.369041442871094,14.338927268981934,14.472102165222168,14.533538818359375,14.367348670959473,14.328975677490234,14.339178085327148,14.404314994812012,14.145881652832031,14.477683067321777,14.373621940612793,13.94682502746582,14.446017265319824,14.458632469177246,14.427715301513672,14.56653881072998,14.365500450134277,14.107902526855469,14.450355529785156,14.507760047912598,14.465317726135254,14.4605712890625,14.471742630004883,14.375661849975586],\"y\":[2.35111927986145,2.5887436866760254,2.850756883621216,2.7799508571624756,2.591951370239258,2.6376168727874756,2.813932180404663,2.5836575031280518,2.6003549098968506,2.6248867511749268,2.574396848678589,2.6431884765625,2.924551248550415,2.9555540084838867,3.0338542461395264,2.76086163520813,2.9738986492156982,2.3828744888305664,3.074077844619751,2.791063070297241,2.4403584003448486,2.8972017765045166,2.786292552947998,2.8893167972564697,2.8356175422668457,2.940004348754883,2.5934154987335205,2.583972930908203,2.775266408920288,3.077293634414673,2.9262781143188477,2.7950642108917236,2.752418041229248],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\".There are some potential limitations as follows: 1) rmlm does not perform well on the sst-2 dataset, indicating it may not be applicable to phrase-level datasets with data scarcity.And in some extreme cases of short text, rmlm may often give incorrect predictions.We recommend doing more mlm pre-training using our wordlevel transformation if resources are available.2) the mitigation is mainly contributed by the transformation and the bert defender.However, there is a lack of exploration of different types of them in this paper.It is worth exploring different transformation schemes and a lightweight model ) as a defender to reduce the computation overhead.3) the adopted evaluation is for testing the performance of defense against word-level adversarial attacks.Rmlm may expose flaws in mitigating character-level or sentence-level attacks.The applicability of the proposed approach needs more investigation.\",\"A major limitation of this research work is only item classification, one specific type of nlu tasks, is used in our experiments.To better evaluate our proposed kcl-fnc method, an expanded testing task set will provide more convincing power.In addition, we only used cross-entropy loss when training models, in both representation and classifier learning stages.It will be interesting to see the compound effect when applying our proposed method together with some advanced loss types, such as ldam.\",\"Although the ability of in-context learning has been found for different architectures , we consider only transformer-based in-context learning in this paper because transformer is the current mainstream architecture of nlp.as for the dual form we point out between transformer attention and gradient descent, we consider a relaxed form of linear attention for qualitative analysis.as for empirical experiments, our analysis needs to record a large number of intermediate results for thousands of validation examples.Considering the storage space and computational cost of analysis, we only analyze gpt models with up to 2.in addition, for the clarity of the problem definition and the convenience of experiments, our analysis is based on only classification tasks.\",\"There are two main limitations of our work., other architectures might perform better than our model.Our claim that seq-gan-bert tries to maximize the information gained from unlabeled sentences is supported by superior performance over baselines defined in this work and other related models.Secondly, due to the lack of good quality labeled datasets, our test sets contained only 100 sentences.However, we believe that the consistency of our high-performing models across languages and multiple seeded experiments presents a positive sign for dc in low-resource settings.\",\"As we have shown, there is much room to improve the learning approach, which incur lower costs than increasing model\\u2019s parameters or elaborate data engineering.This paper is an exercise in guiding learning focus, and we argue that focusl is not perfect for the positioning method and the relevanceto-weight transformation method.For example, our positioning method may contain noise, and some words that are not important in given knowledge may be used as our learning focus.We will continue to explore better methods to guide the model\\u2019s learning focus.Meanwhile, our method only experiments on the basic cross-entropy loss, and still needs to be explored for other learning approaches such as contrastive learning.\",\"Due to our budget constraint, we only performed pretraining and downstream experiments with basesized transformer models.We also only applied the masked language modeling objective, but there are other effective pretraining objectives.Nonetheless, since we introduced minimal changes in architecture, we hope that subsequent work will benefit from our narrowing operations and conduct a wider range of pretraining and downstream experiments.While pretrained models can be applied to even more downstream tasks, we designed a reasonable task suite in this work, consisting of both glue sentence classification and the conll ner sequential classification tasks.\",\"Data size: sugar is relatively small compared to recently published datasets.This is due to the complexity of our problem setting and annotation pipeline.We prioritized quality over quantity and performed multiple steps of manual intervention to reduce errors, false negatives, and annotation artifacts.These problems have been reported in various nlp tasks not limited to conversational tasks.Nonetheless, our experiment has shown that pre-trained transformer models can be trained to outperform a tf-idf ranker by a clear margin, which is encouraging.In addition, we could automatically induce noisy but large-scale training instances from existing resources, for example, by harvesting event pairs that can be used as u and r from event knowledge bases such as atomic2020and generating situation statements using our generator.Representation of situation information: in sugar, situation information is represented in textual expressions.In real-world applications, such information could be collected via external apis and sensors and stored in non-textual forms.Our study is a proof-of-concept that shows the understanding of situational information is very important for response selection.Future research should explore ways to process situation information that is expressed in other forms of data.Even if the value is structured or images, we could transform them into textual forms as done in data-to-text research.Besides, we acknowledge that situational information is often under-specified in sugar because some information is considered to be common-sense or presupposed , and such information was not explicitly stated by human annotators during data collection.Therefore, response selection systems should be equipped with a mechanism to handle implicit knowledge to solve the task.\",\"Limitation our primary focus is on the ood robustness of text classification tasks.However, there are other nlp tasks that the community should not ignore.Glue-x currently does not include language generation tasks such as machine translation, summarization, and dialogue.Moreover, extending the current glue-x to more real-world datasets from different domains is of great importance.We aim to make glue-x a continuously maintained project.\",\".In this work, we propose new methods for finetuning language models.We acknowledge that similar to previous approaches, our experiments are limited to english datasets and specific supervised tasks.However, our method does not use language- or task-specific tricks and should apply to other languages and tasks.Moreover, its parameter and memory efficiency makes it an excellent private learner.However, it may underperform by a few points compared to full-finetuning larger datasets with higher intrinsic dimensionality due to using very few parameters.For example, slash struggles with generative tasks such as text summarization, as generative tasks are more complex and involve making predictions over the whole vocabulary.In contrast, classification tasks have relatively fewer output labels.In our initial experiments, slash reached a rouge-2 score of 12.93 on the xsum summarization task with pretrained bart, whereas full finetuning achieves a score of 21.The limitations of slash are due to the small number of parameters it updates.Since shift is applied to only certain biases, the number of parameters can not be increased beyond a limit.However, we show that slash is a more efficient and performant alternative to the methods that use a similar number of per-task parameters.Moreover, we showed that joint reparametrization improves parameter efficiency of other methods.As such, this principle can be extended to methods that are not restricted by a maximum limit on the number of parameters.For example, jr-warp\\u2019s parameters can be naturally increased by increasing the prompt length, which should improve the results further.\",\".First, the enhancement of the word-level module is not as strong as the remedy of the sentence-level module.Our word-level module solely achieves improvement compared with xbertscore but doesn\\u2019t improve as much as the sentence-level module.The main reason is that the xbertscore framework lacks sentence-level semantic knowledge.Besides, our word-level self-guided contrastive method doesn\\u2019t resort to external information and only consolidates the alignment already existing in the pre-trained language model.Second, refreeeval performs comparably with baseline models on language pairs involving german.We guess it is due to the evaluation of qe.mention that the evaluation results across all language pairs are unstable in \\u201cqe as a metric\\u201d track and can\\u2019t explain yet.and we\\u2019ll try to explore discrepancies among language pairs to optimize the results.In addition, our simple but effective data augmentation method - clause per- mutation doesn\\u2019t rely on rules or toolkits, which is an initial attempt at modeling fluency.It could benefit from further refinement such as languagespecific knowledge, syntactic and semantic parsing to recognize clauses.We\\u2019ll conduct an in-depth investigation into further work.\",\"One limitation of this work is the lack of diversity in the downstream tasks.In our experiments, five of the tasks are named entity recognition and one is a parts-of-speech task.As oov words make up a small portion of a sentence in the text for our downstream tasks, , analyzing the impact of higher quality oov estimates is not trivial.For example, in document classification, the predictions depend on each word in the document, and thus the evaluation of oov estimation will not just be based on the quality of the oov embeddings, but also on their effect on the result compared to known embeddings.This makes assessing oov estimation quality more challenging.As such, it is better to focus on tasks with word level output, so the quality of the oov estimates can be directly judged.However, this limits the types of downstream tasks being analyzed.A second limitation is the fact that all tasks use the english language.butions of subword pretraining and attention will vary with the language.However, previous oov works evaluate on english tasks, and as a result for comparison this paper does the same.\",\"Although all pre-training approaches require a sufficient amount of data, given how we defined keywords, longer sequences suit our approach better than short ones for studying the effects of keyword selection.Further, as shown in this study, our findings strongly imply that the strategy we suggested for adapting plms can effectively enhance their performance on text classification as the downstream task.To determine whether these findings can translate to other nlp applications, however, further experiments are required.\",\".With respect to the re-parameterization of parameters as presented in eq.In order to alleviate memory and computational burdens.Nonetheless, such a setting restricts us to only identifying local subspaces, rather than discovering global subspaces within the entire parameter space of a pre-trained language model.The existence of a task-specific global subspace is yet to be ascertained.If such a subspace does exist, the correlation between this global subspace and the identified local subspaces needs to be explored in future research.In terms of experimental settings, the evaluation tasks are limited to natural language understanding tasks, with a lack of natural language generation tasks.On model architecture, decoder-only and encoder-decoder models are not included.On model scale, we use basicsize models rather than large ones due to limited computational resources.\",\".2, however, we repeat and add to them explicitly here.Small resource scenario our study investigates mlp-based architectures for text classification tasks and finds competitive performance with vanilla transformers while having lower cost in terms of the green ai equation.However, the scope of our findings is naturally limited to the testing scenario, which is low-resource: our models are relatively small, not pretrained on large generalpurpose corpora, and trained on datasets with fewer than 1 million examples.We may not say with certainty that our results will also hold on larger scale.For the sake of hypothesis-driven research we consider it more valuable to run many controlled small-scale experiments rather than few large-scale experiments.Nonetheless, scaling up should certainly be part of future research directions, as this is essential for optimal task performance.Limitation to english pairwise sentence classification tasks since token mixing is the independent variable in our study, we put our main focus on english sentence-pair classification tasks with textual input only, which we presume to be most useful to assess differences between token mixing models.Of course, vanilla transformers are very flexible in the sense that, over the course of many studies, they have been shown to be very effective for a wide range of tasks, languages and data modalities.Whether or not the proposed hypermixer model possesses similar flexibility cannot be answered in this study.The hypermixer encoder arguably possesses similar inductive biases as transformers.We thus expect it to be straight-forward to apply to tasks that are also solved well by transformer encoders.For tasks such as language modeling, which involve a transformer decoder, significant modeling advancements are required to obtain a hypermixer equivalent.Limitation to mlp-based baselines similar to a trend in the computer vision community, our study investigates the suitability of mlp-based architectures for nlp.Due to their conceptual simplicity, these models promise to be easier to train, potentially leading to reduced green ai costs.To this end we compare our proposed hypermixer model to a range of other mlp-based models, and transformers.Apart from fnet and linear transformers, which are efficient transformer alternatives, we do not attempt an exhaustive comparison to non-mlp-based efficient nlp models.Hence, the scope of our claims does not extend to all efficient transformer models.However, these models are of course very relevant to this study, as they are targeted towards one of the factors of green ai cost.\",\"While the lait framework can significantly reduce the computation required for large-scale sentencelevel reasoning and classification tasks, we do foresee some limitations in its use.Caching pertoken representations for large numbers of text segments leads to a dramatic increase in memory requirements, which could be prohibitive for extremely low-compute end users.We also note that lait can further exacerbate segment-level bias in datasets.While we believe that careful data curation approaches ameliorate this issue, the risk of bias is not always known to downstream users and as such corrective datasets may not always be available.Finally, lait can increase the cost of training because the optimal degree of independence is not known until all lait-p models are evaluated, though in practical settings it is possible to perform a binary search of lait configurations because performance generally decreases monotonically as p increases; even a naive rule of setting p to a quarter of the model\\u2019s depth seems to provide some immediate gains while preserving 99% of the accuracy in all our evaluated tasks; and inference-time cost improvements will far outweigh training costs.\",\"Since the appearance of large pre-trained models such as gpt-3 , there has been a wave of using large models without fine-tuning to do in-context learning directly to complete various nlp tasks, or to freeze the parameters of large models and then only optimize task-oriented parameters.The proposed hierverb is a lightweight method especially suitable for the case of insufficient labeled training data, but it is difficult to directly extend to a large-scale language model because large language models are hard to fine-tune in many situations.\",\"Our study illuminates the deficiencies of the mrf approach and applies statistically-motivated approaches to craft more performant probabilistic models.However, it is admittedly not clear how these insights can immediately be applied to improve downstream nlp tasks.We focused on models over pairwise tokens in order to avoid sampling and work with exact distributions for the various approaches.However this limits the generality of our approach.We nonetheless believe that our empirical study is interesting on its own and suggests new paths for developing efficient and faithful mlms.\",\".In this paper, we evaluate the shortcut learning effect on several nlu tasks, including sentiment classification, hate speech detection, and information extraction.Our task selection is mainly based on the robustness and effectiveness of in-context learning on 2our implementation is grounded in lime.Therefore, we do not adopt tasks such as natural language inference, where in-context learning exhibits sub-optimal performance.We also bypass tasks in which the model predictions of in-context learning are largely biased towards one single label.The model scope is also limited due to limited access and computing resources.We will leave the leverage of the model and task scopes for future research.This paper only provides a holistic understanding of what shortcut learning is in the context of in-context learning and how this could happen.Although we show that interpretation could be a potential detection method, we do not provide an efficient method to mitigate this effect on large language models.We will leave it for future research.\",\".In particular, tending to minimize structural entropy, we design circa to construct coding trees for the label hierarchy.To further extract textual information, we propose hitin to update node embeddings of the coding tree iteratively.Experimental results demonstrate that hitin could enhance text representations with only structural information of the label hierarchy.Our model outperforms existing methods while greatly reducing memory increments.Limitations for text classification tasks, the text encoder is more important than other components.Due to the lack of label semantic information and simplified learning procedure, the robustness of text encoders directly affects the performance of our model.From table 2 and 3, we could observe that bert has already surpassed textrcnn by 4.besides, bert beats all the textrcnn-based methods on rcv1-v2 and nytimes.However, when applying bert as the text encoder, our model makes slight improvements to micro-f1, especially on wos.A probable reason is that bert was pre-trained on news corpus while wos consists of academic papers.\",\"Although our approach produces promising results on two datasets, there are certain limitations.firstly, we evaluate the damstf on two classification tasks.We do not conduct experiments on other nlp tasks, such as machine translation or named entity recognition.Nonetheless, as text classification is a fundamental task, other nlp applications can be specified as a case of classification.For example, named entity recognition can be formulated as a wordword relation classification task.Secondly, the meta-learning module carries out extra computation overhead.As the bi-level hyperparameters optimization involves a second-order derivate on the model\\u2019s parameters, their computation overhead is quadratic to the model\\u2019s parameters.In damstf, we use the approximation techniques in wind to compute the derivate, which is linear to the model\\u2019s parameters.\",\"This study is limited to classification tasks with an encoder architecture, short texts , datasets with at least several thousand examples, and a relatively low amount of mislabeled data.In theory, we could apply our method to longer texts, but our takeaways might not directly apply.\",\".Besides, restricted by our knowledge and efforts, the main experiments cannot cover all common tasks and all pre-trained models in nlp.Relatively, the token- and sequence-level classifications have demonstrated attractive experimental performance on most nlp tasks.Next, we also plan to extend w2cspace to more nlp tasks and find its more specific value.\",\"The method proposed in this paper has some limitations: this model learns not only the features of words, word formation, and context but also the relationships between labels.When there is too much noise, it can disturb the relationship between labels.The model needs more training time than other models to learn pretrained label relationships.\",\"Firstly, following the work of t0 , we mainly focus on nlp tasks that can be formulated as rank classification.This covers classification and multiple-choice tasks, but not other task categories such as generation or regression.We hope to extend our training and evaluation to encompass a wider range of task categories, and hope the research community will collaborate in creating resources for such study.Secondly, though we showed that fid-icl outperforms concat-icl, we still lack clear understanding on the source of such improvement.We hypothesized that fid enables the model to learn from in-context examples more effectively, yet our perturbation experiments show that fid-icl mod- els still learn little from input-label mapping.Much more work is needed to further understand of the working mechanism of icl models.Thirdly, given the complexity of our study, we limit the scope to encoder-decoder models.We made this decision due to the superior performance of encoder-decoder models in task-level generalization and their compatibility with fusion-in-decoder method.Also, our important baselines, t0 and t-few , are implemented with the t5 model family.\",\"Our approach is based on meta-learning and is designed for constrained situations where computing resources are limited, such as on-device settings.Therefore, using large and complex feature encoders like llm may pose scalability challenges.In addition, if the task involves a significant number of new classes, the model may not scale effectively.Lastly, our method is primarily suitable for text classification, such as news category or product review classification.It is not appropriate for text generation tasks.\",\"While our study provides valuable insights, it is important to keep in mind its limitations.Firstly, it was confined to text classification and did not include other nlp problems such as named entity recognition , question answering , etc.Expanding this research to a wider range of tasks would provide a better understanding of the methods\\u2019 performance in diverse data scenarios.Additionally, the inclusion of a task shift can be valuable, where the model is trained on a single task but ood data come from a totally different prediction problems.Secondly, we conducted our experiments using only roberta model.We chose a widely used language model for text classification, but there are several other architectures worth testing, especially large language models that now becoming extremely popular.A more comprehensive evaluation of the models and methods could provide more insights into whether the development of transformer-based methods contributes to better detection of ood data.Finally, due to restricted computational time, we did not perform a hyperparameter search for either model or methods.We just used recommend values from the original publications.\",\"The main limitation identified for our rl model is decreased performance as the vocabulary size increases.Our rl model also has a higher variance than some other topic models to which we compared.While our rl model performed well on all the data sets tested, this performance may not generalize to different data sets.The insights from the policy dropout sweep conducted may not apply to other topic models.May be overstated since the model in that paper used a deprecated sbert model that produces sentence embeddings of low quality2.For the comparison to nguyen and luu , we used slightly different preprocessing than the authors.While the model can work on any languages with associated embedding models, all data sets used in this paper were in english.Our model has additional hyperparameters compared to some other models.So, it may require more tuning and, therefore, more gpu computing.The initial model was developed on a system with 8gb of ram and a nvidia gtx 1060 with 3gb of vram for a total of approximately 100 gpu hours.Experiments using the new york times data set were run on a system with 256gb of ram and a nvidia rtx 3090 for a total of approximately 100 gpu hours.All other experiments were run on a system with 128gb of ram and a nvidia titan rtx for a total of approximately 600 gpu hours.\",\"Recent work has shown that models of a certain size exhibit learning properties that cannot be observed in smaller models.Due to practical limitations and environmental concerns, in this study we chose not to train models larger than t5-large.It is thus not possible to know how emergent properties in larger models may have affected the comparison between the different approaches compared here.We believe that our findings will nevertheless be useful to nlp practitioners who operate on a constrained compute budget and may thus opt for moderately-sized models anyway.We compare encoder-only and encoder-decoder models for multi-label classification.Decoder-only models are omitted since at present there are no decoder-only methods for multi-label classification in the literature.While we could have adapted the seq2seq approach in our experiments to operate in a decoder-only context, we deem this unsuitable for the datasets we work with, as they contain long documents which will quickly cause problems for standard decoder-only models like gpt-2.Domain-specific pre-trained language models exist for both the legal and biomedical domain, which outperform their generic counterparts when used for classification tasks.These models all have an encoder-only architecture, however, which renders them unsuitable for a comparison of encoderonly and encoder-decoder approaches to multilabel classification.Our experiments consider datasets from the legal and biomedical domains first and foremost because there are publicly available datasets with hierarchical labelling in these domains, unlike others.Moreover, we believe that working in critical application domains is a worthy purpose and covering two such domains with two different datasets in each domain gives us a good view on how the examined methods are expected to work in such domains.\",\"Limitation of requiring knowledge of models and training data, we extract simple patterns from the min-min optimization to make text unlearnable.Although our experiment explores patterns for text classification and question-answering tasks, the pipeline potentially works for any nlp task.to ensure the effectiveness of unlearnable modifications, we slightly tuned the training hyperparameters to achieve well-trained models, such as setting maximum gradient norms and early stopping according to validation sets.We open-source codes with configuration files, which contain hyperparameters regarding model architectures , batching , and training setups.\",\"Decompx is an explanation method for decomposing output tokens based on input tokens of a transformer model.Although the theory is applicable to other use cases, since our work is focused on english text classification tasks, extra care and evaluation experiments may be required to be used safely in other languages and settings.Due to limited resources, evaluation of large language models such as gpt-2 and t5 was not viable.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"15_classification_oov_slash\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"15_classification_oov_slash\"],\"textfont\":{\"size\":12},\"x\":[13.345585823059082,12.173717498779297,12.635650634765625,13.110300064086914,12.213339805603027,12.844834327697754,12.695423126220703,12.158259391784668,12.575974464416504,12.774670600891113,12.143172264099121,12.251838684082031,12.707100868225098,12.245795249938965,12.344976425170898,12.390482902526855,12.275773048400879,12.23831844329834,12.136202812194824,12.12755012512207,12.1000394821167,12.26046085357666,12.360578536987305,12.177974700927734,12.18265438079834,12.241220474243164,12.005987167358398,12.209199905395508,12.396036148071289,12.973235130310059,12.40987777709961],\"y\":[2.6495625972747803,2.2884528636932373,2.5595059394836426,2.70990252494812,2.5442728996276855,2.2081761360168457,2.1324410438537598,1.8931870460510254,2.1638200283050537,2.102423906326294,1.7617360353469849,1.8842308521270752,2.091747283935547,2.133113145828247,2.169203281402588,2.381390333175659,1.8786526918411255,2.1026852130889893,2.212951183319092,1.8187981843948364,2.2221057415008545,1.928613543510437,2.447552442550659,2.080716609954834,2.297978401184082,1.973524808883667,2.424001693725586,2.408696174621582,2.2625646591186523,2.2074484825134277,2.197981834411621],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"We explore early, very simple structured event representations.Furthermore, the wikihow articles may reflect the bias of their human authors.\",\"We see four main limitations regarding our work.First, we have evaluated our models on a dataset containing events of one type only.It remains to be seen how applicable our formulation and methods are to other historical datasets and event types.Second, given the nature of the historical question our dataset targets, it contains documents only from one language family.Extending our methodology to languages from other language families might pose further challenges in terms of multilinguality.Third, our method relies heavily on automatic translation tools, which are biased toward translating historical texts into modern language.This can negatively affect the performance of our models.Lastly, in real-life cases, machine readable historical texts are often extremely noisy, suffering from high level of ocr errors and other text extraction mistakes.Conversely, we have tested our methods on relatively clean datasets, with the unannotated dutch material as the only exception.\",\"In this work, we conduct research on event commonsense of open-domain dialogue systems for the first time.While achieving higher correlations with human judgments than existing baselines, accent has some limitations: first, the accent framework is based on a fixed set of event relations and the commonsense knowledge in atomic2020 which may fail to cover some potential event commonsense aspects.We believe augmenting the current framework with more commonsense resources is a worthwhile direction for the further improvement of accent.Second, the event-relation extractor in accent framework is a t5 model fine-tuned in a low resource setting.Although the current model can yield fairly strong performance, it is an important research direction to improve the joint event-relation extraction component because the extracted tuples serve as the symbolic representation for commonsense reasoning in accent framework.\",\"The findings of this study have to be seen in light of some limitations.Compared to cdgm , introducing multiple metrics to capture flexible cross-sentence event relationships would cause extra computation cost and slightly slow our training.The training parameters of different models can be seen in table 12.We deprive our model of multiple similarity metrics and denote it as the baseline model.We can see that our model increases 7% parameters compared to cdgm and 0.\",\"Although our experiments prove the superiority of our scprg model, it is only applicable to document-level eae tasks with known event triggers because both stcp and rlig calculate the attention product of the trigger and candidate spans.However, in real-life scenarios, event triggers are not always available.In view of this problem, we have a preliminary solution and plan to improve our model in the next work.The core idea of our method is to select and integrate context and role information based on candidate arguments and target events.Based on this idea, we briefly provide two solutions for the above limitation.First, we can make the model predict the best candidate trigger words.Second, we can replace trigger words with special event tokens.In the next work, we plan to extend our model to document-level eae tasks without trigger words and evaluate it through extensive experiments.\",\"Our method utilizes the amr annotations as additional training signals to alleviate the data scarcity problem in the event extraction task.In this problem setup, generally speaking, amr annotations are more expensive than event extraction annotations.Nonetheless, in reality, the amr dataset is much bigger than any existing event extraction dataset, and amr parsers usually have higher performance than event extraction models.Leveraging existing resources to improve event extraction without requiring additional cost is a feasible and practical direction.Our work has demonstrated the effectiveness of leveraging the feedback from amr to improve event argument extraction.However, it\\u2019s still under-explored what additional information and tasks can be leveraged as feedback to improve trigger detection.We did not have quantitative results for the alignment between amr and event graphs.The authors randomly sampled 50 event graphs from ace05-e and found 41 are aligned with their amr graphs based on human judgment.there is a large gap between the validation and testing datasets in terms of label distribution on ace05-e and ace05-e+.We observe that performance improvement on the validation set sometimes leads to performance decreasing on the test set.Both the validation and test dataset miss certain labels for event trigger types and argument role types.The annotations in the training set, validation set, and test set are scarce and highly unbalanced, which causes the low performance on trained models.We argue that a large-scale more balanced benchmark dataset in the event extraction domain can lead to more solid\",\"In this paper, we simplify intention identification into a sentence classification task, i., exploiting a specific procedural event in an event process to predict the intention of the whole event process.A more realistic way to model this task is to enter the entire event process rather than a single event.\",\"We state the limitations of this paper from the following three aspects: 1) regarding linguistics-aware data construction, we only perform seed-guided pattern enrichment for four reaction roles due to the lack of sufficient reliable patterns for other roles.2) as in the previous work, we adopt a fixed reaction scheme to extract structured chemical reaction information.However, there are always new informative roles in the text , such as experimental procedures , so how to predict both roles and arguments without being limited to a fixed scheme could be a meaningful research topic.3) reactie is capable of detecting chemical reactions within scientific literature by predicting if a given passage contains a product.However, accurate text segmentation of a paper remains an unresolved and crucial issue.Incomplete segmentation may result in the failure to fully extract reaction roles, while excessively long segmentation may negatively impact the model performance.Therefore, integrating a text segmentation module into the existing two-step pipeline may be the next stage in the chemical reaction extraction task.\",\"We have demonstrated that an accurate description can perform better for both supervised and weakly supervised event detection.However, the event types from most existing ontologies are not properly defined.For example, in ace annotation guideline , transfer-money is defined as \\u201cgiving, receiving, borrowing, or lending money when it is not in the context of purchasing something\\u201d.However, it is hard for the model to interpret it accurately, especially the constraints \\u201cnot in the context of purchasing something\\u201d.In addition, many event types from maven, e., achieve, award, and incident, are not associated with any definitions.A potential future research direction is to leverage mining-based approaches or state-of-the-art generators to automatically generate a comprehensive event type description based on various sources, such as annotation guidelines, example annotations, and external knowledge bases.\",\"This paper proposes a denoised structure-to-text augmentation framework for event extraction , which generates and selects additional training data iteratively through rl framework.However, we still gain the following limitations.\\u2022 the framework uses reinforcement learning to select effective samples, which is a process of iterative training and predicting the generation model, policy model, and event extraction models.The iterative training framework is complicated and time-consuming compared to the standalone event extraction model.\\u2022 even the argument loss decreases the number of unmatched arguments in a generated sentence, the generation model generates more fluent sentences while at the expense of the ability to ensure that all the event arguments are included completely.\",\"The main limitation of situationsupervision is that situation annotations can often be expensive to curate and difficult to design.Furthermore, we conducted experiments on only two datasets in this paper.\",\"In our proposed model, we introduce a hyperparameter n as the number of event proxy nodes.The value of n needs to be pre-set.Setting n to a value larger than the actual event number in a document would lead to computational redundancy as more proxy nodes would be mapped to the null event.However, setting n to a small value may miss some events in a document.We have experimented with automatically learning the value of n based on an input document in procnet.But we did not observe improved event extraction performance.As such, we simply set it to 16.in the chfinann dataset, 98% documents have less than 7 events annotated.This results in the learning of many redundant proxy nodes for such documents.It remains an open challenge on automatically learning a varying number of event proxy nodes based on an input document.Reducing the number of redundant proxy nodes can reduce training time further.we observed a relatively worse performance of procnet in dealing with long documents with more than 40 sentences as it does not explicitly model the interrelations of sentences.One possible direction is to explore the use of a heterogeneous graph which additionally models the entity-entity, entity-sentence, and sentence-sentence relations.\",\"Our analysis is primarily limited by the accuracy of underlying nlp models used in our character event extraction pipeline.For example, booknlp does not cluster nominal mentions of characters with the corresponding character names.This results in character event chains that do not account for all of the character\\u2019s actual events.Using allennlp to extract all action verbs in a sentence as the event triggers meant that not all of our events were on the same dimension: some events were intended or thought of, while others actually happened.Additionally, narrative events that are described in ways beyond just action verbs are not extracted.Our salient event identification algorithm might also filter out many events of analytic interest.Both characters whose gender are not specified in the story or who are gender-less are classified as \\u201cunknown\\u201d.There is no explicit way to extract non-binary characters as models tend to label uses of the pronoun \\\"them\\\" as plural.Thus, the current implementation is lim- ited to comparisons of female and male characters which perpetuates a gender binary.Our use of bootstrapping to calculate confidence intervals and determine statistical significance is valid under the assumption that the original fairtytaleqa sample is representative of all fairy tales.As the sample was collected only from popular open-source stories, this assumption may not hold.Lastly, bias exists beyond just gender groups and gender itself intersects with other social groups.We plan on expanding this component to include attributes such as race and ethnicity, age, and socioeconomic class.The cultural comparisons and overall analyses were too limited as the fairytaleqa dataset is very eurocentric with most fairy-tales coming from northern and western europe (table 4 in a.only some stories income from east asian, southern european, or indigenous north american cultures.Meanwhile, almost no fairytales are included from south america, the middle east, africa, south asia, or south east asia.Unfortunately, after considering the break down of event chains by gender and culture, the samples were too small to observe robust trends.\",\"In this work, our approach assumes event triggers and argument templates are given.This limits our approach\\u2019s applicability, as it requires an event detection system to produce event triggers and event types before llms can be prompted to generate event arguments.We only explore hierarchical events with only 2 levels from the ace05-e ontology and data, which has limited coverage of real-world complex event hierarchy.Similar to prior event argument extraction work, our approach relies on a human-curated hierarchical ontology.despite llms performing well on eae with few-shot data, compared to existing supervised approaches, their inference is relatively slow and costly11 since the llms we used are generally more than 100x larger in the number of parameters.Prior work has demonstrated a strong relationship between performance and in-context demonstrations; however, for ease of comparison to supervised baselines, we use the same set of examples from the training set for in-context learning.\",\"There are a few shortcomings that we discuss below: underrepresented fine-grained lms: although we had chosen a careful sampling method focused of event-centric informative dataset aiming to increase the likelihood of fine-grained lms\\u2019 occurrence , we think the low frequency of fine-grained lms in idrisi-ra is a major limitation as it contains solely 25.human errors: there are some human errors made during annotation due to the difficulty of the task.\\u2022 annotators sometimes fail in distinguishing between location and organization entities.\\u2022 different location types could be used interchangeably for the same locations which forms a difficulty for annotators.\\u2022 annotators highlight the locations that are mentioned as descriptions within the context of the tweet.We plan to overcome these errors as part of location mention disambiguation annotation that aim to remove ambiguity of geo\\u002fgeo entities (as a sequel to the geo\",\".Our model uses a pretrained language model module and graph neural network module to jointly represent event graphs.In addition, we make the event embedding space geometrically meaningful by imposing two constraints on event embeddings: event temporal order should be in accordance with event embedding norm, and event temporal relations should only exist between events whose embeddings are close enough.Experiments demonstrate that our method significantly outperforms baselines by generating accurate and globally consistent temporal event graphs.we also plan to make use of the generated temporal event graphs in downstream tasks, such as future event prediction and question answering.Limitations in the current design setting, our proposed model is only able to classify temporal relations between event pairs into one of three classes: before, after, and no relation.Our model should be more practically useful if we can extend it to predict more relation types in addition to temporal relations, such as parent-child and causecaused_by relations.We believe that our model is able to make such extension without too much modification., commonsense knowledge of event temporal relations.Our framework should be more powerful to deal with domain-specific articles if utilizing such knowledge in the framework.\",\"Although our modeling of event centrality is feasible and effective, there is still space for improvement.The performance of event centrality prediction could be higher by using more advanced encoding methods.Besides, it is meaningful to further explore the interactions among various types of event relations.Existing datasets only cover limited relation types at once, and many works focus on the identification of causal relations alone.In this paper, although we further consider the effect of coreference relations and perform joint classification, there are still some other relations that can be explored, such as temporal relations, subevent relations, etc.\",\"The limitations of our work are as follows: as we construct eventoa for a new task by manually annotating, the data size can be further extended.Our study is limited to english sources, and we hope work can pay attention to event ontologies in other languages.Building datasets for multilingual event ontology alignment would have a positive impact on applications in other languages beyond english.\",\"In this work we present a novel method based on representation augmentation to solve cross-lingual transfer learning for event detection.first, our current method only leverages sentencelevel context in input document to perform ed over different languages.This might not be optimal as document-level context has been shown to be helpful for ed that can be explored in future research to improve our cross-lingual models.Second, the evaluation for our model is limited to only three popular languages that are supported by existing pre-trained language models, unlabeled data, and text processing tools.As such, it is unclear whether the method can be adapted to many other languages with limited access to such resources.finally, our method requires joint training with a retrieval model that can impose additional computational costs.Reducing necessary computational costs for our model is an important direction to make it more accessible for different applications and domains.\",\"The design of the dynamic templates requires knowledge of the event ontology and is timeconsuming.The authors of the paper spent 30 hours designing the exclusive templates that cover all of the possible argument combinations for each argument role in ace ontology.With a more complicated ontology, a much larger amount of time is required.Another limitation of our approach is the offset retrieval method.If one sentence contains multiple mentions of the same entities, or even multiple text strings that have the same spellings but refer to different entities, the qga-ee model always retrieves the position where the mention appears for the first time in the sentence as the offset of the extracted target.It may be improved by asking the model to generate contextual text as a position reference.\",\"Traditional methods for the completion of atomic proposed to score all candidate tail events given the head event and the relation.The gcn for encoding graph embeddings of events induced two shortcomings: 1) it is difficult for a gcn to propagate information due to the sparse graph structure of atomic ; 2) it cannot sufficiently utilize semantic information of events.\",\"We would like to highlight a few limitations of our work.First, we would like to point out that geneva is designed to evaluate the generalizability of eae models.Although the dataset contains event type and event trigger annotations, it can only be viewed as a partially-annotated dataset if end-to-end event extraction is considered.Second, geneva is derived from an existing dataset framenet.Despite human validation efforts, there is no guarantee that all possible events in the sentence are exhaustively annotated.\",\"Our proposed degm for event skeleton generation still contains some limitations: \\u2022 it only considers the problem of event skeleton generation, a subtask of temporal complex event schema induction.It is promising to explore the whole task, which includes entities and entity-event relations.\\u2022 perspective from errors found that our model suffers from a tendency to generate correct duplicate substructures.\",\"Despite its promising results, our study has limitations.Our model primarily works with english text, limiting its applicability to other languages.Its focus on sentence-level extraction doesn\\u2019t consider document context, which could be investigated in future research.The employed training dataset is relatively small, potentially not encompassing all possible event types, thus affecting the model\\u2019s performance and generalizability.Additionally, our two-stage inference framework, while enhanced by a ranking module, is prone to error propagation.If a trigger isn\\u2019t identified in the first stage, its associated arguments cannot be extracted.\",\"In this work, we propose a novel method for eae that introduces learnable soft prompts to capture specific-example context and relevant documents for prompt customization and enrichment.first, similar to previous eae studies , our eae model assumes golden event triggers for event types that might not be available for real-world applications.second, to aggregate relevant document representations for soft prompt computation, our eae method leverage an event type mentioning graph that capture documents, event types, and their occurrence in training data.On the one hand, the graph does not involve argument roles that are directly related to eae and might provide richer information\\u002fcontext to obtain representation aggregation to augment soft prompts.On the other hand, our method only explores graph attention networks to perform representation aggregation while many other variants of graph neural networks have not been considered, e.third, despite the introduction of soft prompts with important benefits, our method still needs to rely on discrete prompts to explicitly specify event types and argument roles.Although our experiments demonstrate better stability of the proposed method with different discrete prompt variants, adapting our method to new languages will still require some prompt development effort to achieve optimal performance.\",\"As we mentioned, we only focus on event arguments with the assumption that event type is already provided.However, this is not always true for many applications in real life scenarios.Besides, considering the long input of document-level extraction, the computing memory consumption significantly increase to tens of times compared with its sentence-level counterpart.We only consider the 1\\u002f2-doc cases, although in reality more docs are possible.We believe finding a solution for decreasing the memory requirements would be of great impact for future research in this direction.\",\".First, as our method depends on event structure information which is obtained through automatic parser, if the parser is not good enough, then it will impact the performance.Second, since we focus on leveraging structural information, we restrict the experiments on text-based event explanation.\",\"The event schemas generated by our model are not directly comparable to those generated by previous work that utilized a close-domain ontology.As a result, we were unable to adopt the same metrics and evaluate our schemas on type-level event prediction tasks as in.Grounding the events generated by the llm into one of the types in the ontology could be added as a post-processing step to our model, but this would require some ontology-specific training data, which goes against our principles of designing an open-domain, portable framework.Our event schema does not explicitly represent entity coreference, entity relations, and entity attributes.The current schemas that we produce focus on events and their relations, with entity information captured implicitly through the event descriptions.For instance, the see medical professional event is described as \\u201cthe patient is seen by a doctor or other medical professional\\u201d and the proceeding obtain medical history event is described as \\u201cthe medical professional obtains a medical history from the patient\\u201d.The \\u201cmedical professional\\u201d and \\u201cpatient\\u201d are implied to be coreferential entities in this case, but not explicitly connected in the schema graph.Our approach is also quite distinct from prior work that consider a probabilistic model as an implicit schema where the schema graph, or event narrative chain can be sampled from.Probabilistic schema models have the advantage of being adaptive and can be conditioned on partially observed event sequences, but are hard to interpret.We make the conscious design decision to generate explicit, human-readable schema graphs instead of black-box schema models.Finally, our model relies on the usage of lms, which have been observed to sometimes show inconsistent behavior between different runs or when using different prompts with the same meaning.However, quantification of consistency has only been done for factual probing tasks while schema generation is a more open-ended task.For example, in our experiments on everyday scenarios, we observe that the model could generate distinct schemas for buying a mouse based on whether the purchase was done online or in person.\",\"Our work follows a pipeline approach, where the optimization of event trigger detection and argument identification are done separately.This causes the event trigger detection errors to be passed to the argument identification step.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"16_event_events_eae\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"16_event_events_eae\"],\"textfont\":{\"size\":12},\"x\":[12.862239837646484,12.838067054748535,12.852919578552246,12.803253173828125,12.846415519714355,12.861305236816406,12.787175178527832,11.876276969909668,12.845355987548828,12.83859920501709,12.782121658325195,12.859687805175781,12.871826171875,12.84227180480957,12.767256736755371,12.853734970092773,12.817021369934082,12.847338676452637,12.879952430725098,12.845574378967285,12.849142074584961,12.818137168884277,12.855901718139648,12.745742797851562,12.766524314880371,12.854581832885742,12.86844539642334,12.84534740447998,12.847970962524414,12.80103874206543],\"y\":[-0.9613609910011292,-0.8051849007606506,-0.9556552767753601,-0.7240808010101318,-0.983004629611969,-0.9374499917030334,-0.8714598417282104,1.1441187858581543,-0.9679899215698242,-0.9591647386550903,-0.9057884216308594,-0.9683087468147278,-0.9403589963912964,-0.9843263626098633,-0.8645706176757812,-0.9493076205253601,-0.9294042587280273,-0.964156985282898,-0.862252950668335,-0.996246337890625,-0.9630796313285828,-0.8948893547058105,-0.9793577790260315,-0.7808940410614014,-0.9136492013931274,-0.9671377539634705,-0.9510706663131714,-0.9800078272819519,-0.983052134513855,-0.8551410436630249],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Limitation since there is currently no standard evaluation metric for evaluating post-hoc explanations, we use aopc as the quantitative evaluation metric, which is widely used in the research field.even so, there is a risk of unfair comparisons because the aopc tends to give higher scores to erasure-based explanation methods such as loo.We don\\u2019t conduct human evaluation because we believe human evaluation needs a very large scale to guarantee objective and stable, of which we can afford the cost.Thus, we post visualizations of all explanations in our experiment to demonstrate the effectiveness of our approach.\",\"In this paper, we evaluate the quality of humanannotated natural language explanations towards the models\\u2019 prediction performance on multiple datasets.Although it is a natural step that our evaluation metric could be generalized to evaluate the helpfulness of model-generated explanations, we would like to caution that: our metric and evaluation experiment requires the models to generate explanations for the train split data, then use the data with generated explanations to fine-tune the second model with the infusion setting, which may not be suitable for those systems that are trained on train split data.In addition, we acknowledge that the human-annotated explanations are very expensive to collect, thus, a better mechanism ) is needed to improve human annotators\\u2019 performance.\",\"Subjective interpretation is an important and unavoidable component of both linguistic and neural language model analysis.The goal of datadriven research is to extend the sphere of concern beyond self-reflexive subjective judgments of the researcher to the shared subjectivities of a language community.Information about animacy reflected in an annotated dataset still reflects subjectivities, but shared ones.It is important to always be clear about where interpretation is happening, whose interpretations are taken into account, and how they affect what\",\"Explainability most current mwp solvers are only able to generate solutions.In our work, although we achieved better generalization ability, it is still hard to explain how the model solves mwps both correctly or incorrectly.These automated solvers would be much more helpful for tutoring students if they could explain their equation solutions by generating reasoning steps.\",\"Although we showed significant improvements in explanation generation using prompt-based few- shot learning, our work still has some limitations.First, we experimented only on the e-snli dataset: although e-snli is a reference for the task, it would be interesting to extend the proposed methodology to other datasets with natural language explanations for an extensive review).Second, we did not attempt to automatic prompt optimization: although this may bring further minor improvements, we decided to leave optimization to a next step, as it does not change the core contribution of our work.Third, we believe there is an intrinsic limitation in comparing our results with sota, as there is not a clear consensus on which metric is to be taken as the reference metric for benchmarking, along with the fact that measures sometimes disagree on scoring one system better than another.Finally, as for our our use of e-snli, we are assuming that for all sentence pairs in the dataset there is an implicit explanation of the semantic relation between the sentences.Under this assumption we always generate an explanation, even when the explanation is already explicit in one of the sentences.We think that a better capacity to detect those cases would bring relevant insight to our approach.\",\"Despite strong performance compared to few-shot our self-training methods still contain significant room for improvement compared to the fully supervised benchmarks.It would be interesting to try larger language models to see if it is possible to close this gap with more knowledge embedded into the pre-trained models.Our evaluation of free text rationales are limited by the automatic metrics, which are necessary but not sufficient to analyze quality of an explanation for decision making of the model.From example explanations , it is evident that we still lack understanding on multiple dimensions such as, when an explanation is factually wrong, is it due to the model believing in the wrong knowledge or is unable to retrieve the correct one.Works that probe a language model with various prompts could be useful for investigating in these directions.\",\".First, we use the tcav framework, which assumes that concepts are encoded in the linear space of semantic representations.However, recent works show that in some cases, linear discriminants are not enough to define the semantic representations of concepts in the embedding spaces.In this study, we used simple challenge sets to obtain a baseline for assessing the effectiveness of concept-based explanations in measuring false global sufficiency.our work is limited to pre-defined concepts and requires human input to define the concepts with examples.However, defining concepts in tcav is less restrictive than pre-defining features in other explainability methods, in that concepts are abstract ideas that can be defined without requiring in-depth knowledge of the model\\u2019s inner workings or the specific features it is using.This allows for a more flexible approach where users can test the model regarding their concept of interest.Our method can only be applied to concepts that are known to be important for the classifier and are prone to being over-represented in training sets.It\\u2019s important to check this condition independently before using our metrics.In cases where this condition does not hold true, the metrics we use in our work may be interpreted differently and may not be reliable indicators of global sufficiency.Also, we only considered two variations of emotion-related concepts.further, our metrics are limited to cases where different classifiers are being compared since the most important information is in the relative value of the metrics.Our metrics should not be used as absolute scores for testing a classifier.Testing a classifier for false causal relationships is most valuable for detecting the potential flaws of the models.If our metrics do not reveal a false relationship between the concept and the label, that should not be interpreted as an indicator of a flawless model.Ethical statement as with most ai technology, this approach can be used adversely to exploit the system\\u2019s vulnerabilities and produce toxic texts that would be undetectable by the studied classifier.Specifically, for methods that require access to the model\\u2019s inner layers, care should be taken so that only trusted parties could gain such access.The obtained knowledge should only be used for model transparency purposes, and the security concerns should be adequately addressed.Regarding environmental concerns, contemporary nlp systems based on pre-trained large language models, such as roberta, require significant computational resources to train and fine-tune.Larger training datasets, used for fine-tuning, usually result in better classification performance but also an even higher computational cost.To lower the cost of this study and its negative impact on the environment, we chose to use existing, publicly available classification models.\",\"Concepts in this work, we extract the concepts from semi-structured explanations whose format reassures consistency and non-ambiguity of the exploited concept.The selection of datasets and corresponding concepts is primarily conditioned by data availability, as the semi-structured explanations are available merely for a small set of datasets.We acknowledge that our selection of concepts is not representative for a vast variance of concepts that users might expect models to learn from context in interaction.Some important concepts\\u2019 features that we identify are following: a number of premises or reasoning inference steps needed to map the input to output, the granularity of the reasoning steps, a type of the premises; for instance, whether the familiarity with a given concept requires a memorization of an entity property , or a reasoning mechanics such as analogical reasoning.such taxonomy can motivate a more targeted collection of concepts from explanations, or annotation of new explanations demonstrating new concepts.Models we acknowledge the limitation in a variance of evaluated models given by their availability and our computational possibilities.We evaluate only two models of the gpt family due to the usage limits of openai api.Outside gpt models, we do not evaluate models over 20b parameters, given the infrastructure requirements of such settings.Nevertheless, we argue that the relevance of the models with constrained access, or resource requirements exceeding the limits of most organizations also remains a subject of open question.Datasets one should note that the sizes of our evaluation datasets, for which we are able to extract concepts from explanations , are too small to compare concept sensitivity between models.The sizes of our sensitivity evaluation datasets are the following: worldtree: 2,204 samples, openbookqa: 792, glue diagnostics: 282 samples, hotpotqa: 182 samples.it is understandable that the focus of development in in-context learning llms goes to measurable improvements on existing benchmarks, as ecologically-valid evaluations on end use-cases are timely and challenging to compare to related work.Nevertheless, in this highly-exposed and fastpaced direction, we identify the necessity for the emergence of fast proxy measures that can shed light on the decision-making of the llms as expected by their end users.The presented evaluation of models\\u2019 sensitivity to demonstrated reasoning concepts introduces a technical framework for quickly assessing models\\u2019 compliance with our expected functioning; however, a selection of a comprehensive set of concepts that we can agree our models should be able to learn remains a subject of open\",\"In this work, we introduce lasque, which models and learns the differential semantics of linguistic quantifiers present in natural language explanation to train a classifier guided by these explanations.We evaluate the efficacy of lasque over baselines on the clues benchmark.This work assumes that only a single quantifier is present in the explanations.However, in real-world settings, explanations may contain multiple quantifiers.for our experiments, we assume perfect extraction of quantifiers and limit our analysis to a limited set of quantifiers in this work.Furthermore, we assume that the effect of quantifiers in a sentence is the same irrespective of the domain of the sentence.For example, consider two sentences \\u2018pungent mushrooms are usually toxic\\u2019 and \\u2018people who smoke regularly usually suffer from cancer\\u2019.Here the effect of \\u2018usually\\u2019 is not exactly the same for two sentences that are from different domains.However, lasque is not sensitive to the task domain while modeling the semantics of the quantifier.\",\"The limitations of our work can be viewed from two perspectives.Firstly, we have not thoroughly investigated seq2seq architectures for explainable gec.Secondly, the current input of the explainable system is the gold correction during training, whereas, in practical applications, the input would be the output of a gec system.We have not yet explored methods to bridge this gap.\",\"We highlight three main limitations of our work.First, although we have explored gradient-based explanations that take the whole network into consideration and have been shown to be faithful in previous work , we do not explicitly explore how comet combines the sentence representations in the feed-forward that precedes the encoder model to produce the sentence-level score.Second, we have shown that combining attention with gradient information results in the best explanations for unite-based metrics.However, from a practical standpoint, running inference and extracting the explainability scores simultaneously may be more computationally expensive than other techniques: gradient-based metrics benefit from gpu infrastructure and require storing all gradient information.Third, we have not explored extracting explanations in low-resource settings.That is because high-quality mqm annotations for such language pairs are not yet available.Nevertheless, further research in those settings is needed to access the broader validity of our claims.\",\".To address this issue, we propose to jointly train with today and its explanation annotations, resulting in improved performances on multiple temporal reasoning benchmarks, namely tracie , matres , and today.We also demonstrate that today can be used to distill gpt3.5 and automatically generate and filter incidental supervision instances with high-quality explanations, which further improves performances.limitations this work initially builds on human annotations, which are relatively expensive compared to simple model generations.Due to such cost-related reasons, we do not include neutral contextual changes which are hard to annotate, and do not investigate the potential harms of annotated\\u002fgenerated language, e.we use t5 and gpt-3 architectures; however, there are more powerful architectures that could potentially improve our results.Lastly, this work only focuses on generalizing temporal reasoning, which is a challenging yet relatively narrow task for large language models.Through pilot experiments, we find that similar task formulation, annotation schemes, and model structures can be applied to other tasks, such as natural language inference and question answering.\",\".In this work, we limit our experiments to generating free-text explanations given a complete task sample., controllable explanation generation or synergetic generation of both task prediction and explanation.Besides, more work is needed to assess eib\\u2019s robustness and generalization when applying it to diverse nlp domains.These domains may differ in sample type, topic, or even with different preferred explanation attributes.The performance of eib is also tied to the quality of other systems or datasets, mainly the backbone language models and automatically constructed training corpus mixexpl.The predictions of our method are also restricted by the capacity of the generator of eib, where we use gpt2-small architecture as the decoding architecture.This phenomenon may be remedied if we design specific interactions with larger plm and other sources for explanation-related knowledge distillation.For example, designing more effective prompts to induce better explanation-related knowledge from plm to relieve the training pressure.While our paper focuses on the issues of explanation generation given zero-shot prompting outputs, we think eib is easy to extend to few-shot prompting base- lines since single-pass generation without updating also belongs to the features of conventional fewshot settings.quality estimation of the natural language explanation generation is largely dependent on human evaluation due to its open-ended characteristics.Current automatic evaluation metrics are not convincing and reliable when compared to human evaluation.However, reproducing the human evaluation results across different works may be difficult.This suggests that better automatic evaluation metrics are desperately needed for free-text explanation generation.\",\"Possible limitations include the limited plm architecture\\u002fsize and faithfulness evaluation metrics are not necessarily comprehensive.And we only focus on text classification tasks.As a result, we do not investigate other language classification and text generation tasks.If we can intrinsically know or derive the golden faithful explanations , the exploration of model robustness and explainability will be alternatively investigated for revealing the internal reasoning processes.And logic trap 3 which claims the model reasoning process is changed rather than the attribution method is unreliable.Different from this two works \\u2013 output probability perturbation or changing information flow, we view our results as complementary to their conclusion via sourcing the improvement of faithfulness.from a theoretical perspective, one possible reason is that the gradient of the model is more aligned with the normal direction to the close decision boundaries.Furthermore, we do not exhaustively experiment with all possible evaluation settings of interest even with the scale of our experiments.For example, saliency guided training methods could have been used as another baseline.\",\"In our experiments, the most significant limitation is the lack of computational resources.Experimental results in this paper and previous work have shown that a larger scale of models could lead to higher structural and semantic accuracy of explanation graphs in this task.Constrained by computational resources, bart-large is the largest model on which we can perform the complete process of experiments.We believe that graph generation would be better if sufficient resources were available to perform synthetic data based pre-training on a larger model.In addition, since the evaluation metrics for graph generation tasks are incomplete yet, we can only evaluate a few samples manually outside of the metrics of the dataset, which is more subjective.With more evaluation methods with standardized processes proposed, the results of the experiment will be evaluated more objectively.\",\"In this paper, we mainly focus on developing an amortized model to efficiently achieve a reliable estimation of sv.Though not experimented with in the paper, our method can be widely applied to other black-box post-hoc explanation methods including lime.Also, due to the limited budget, we only run experiments on bert-based models.However, as we do not make any assumption for the model as other blackbox explanation methods, our amortized model can be easily applied to other large language models.We only need to collect the model output and our model can be trained offline with just thousands of examples as we show in our method and experiments.Comparison and training with exact shapley values computing exact sv is computationally prohibitive for large language models on lengthy text inputs, as it necessitates the evaluation of llms on an exponential number of perturbation samples per instance.As a result, we resort to using svs-25, which serves as a reliable approximation, for training our amortized models.\",\"Our work focuses on building a two-stage framework for generating and learning from explanations.In our investigation, we are limited by the available computational resources, financial budgets, and datasets.Gpt-3 and pet are performant few-shot learners that work well for our use case.However, gpt-3 is not free to use and partly for financial considerations, we did not experiment with gpt3 in-context learning initially.The performance difference between gpt-3 babbage and davinci are aligned with the emergent abilities of largescale language models.Therefore, in the era of research with private large-scale language models, it would be useful for the research community to collectively build knowledge about how large-scale language models work.It would be useful to experiment with other models such as google\\u2019s palm and deepmind\\u2019s gopher.It is an important question for the research community to explore productive paths forward.Often, prompt engineering requires either significant manual work to come up with good templates or a big budget to run automatic prompt generation methods.we experimented with two natural language inference tasks, which tend to correlate with a certain form of explanations.One way to interpret the difference in our findings and chain-of-thought prompting is indeed that the reasoning in e-snli and e-hans are not the multi-step reasoning used in arithmetic reasoning.As tan argues, there are diverse types of explanations, which may lead to varying levels of effectiveness from a learning method.we picked examples randomly, but research has shown that calibration reorder- ing and example selection changes gpt-3\\u2019s behavior.We also used human explanations to fine-tune the gpt-3 model for explanation generation, but human explanations may not always be high-quality or the best guide for machine learning models.Additionally, we use roberta as our backbone model for the classifier used in both the non-gpt baselines and our flame framework.We manage to beat stronggpt-3 baselines that use explanations.While more powerful classifiers could also be used in place of roberta, we believe we have demonstrated the effectiveness of our method by using a simpler classifier.finally, it is worth noting that we use a particular setup of k = 16 for our experiments.While we believe that this is a reasonable few-shot learning setup, results could differ for different k.broader impacts we propose a framework to generate and learn from explanations and conduct in-depth analysis to understand the utility of explanations.Our work has the potential to help people understand the behavior or usage of large-scale language models and improve their trustworthiness.\",\"Our experimental setup excludes free-text rationales explaining the decisions of a model , because their output is not based on attribution scores or highlighted spans of the input text, so we argue that they are not trivially comparable.However, there are end-to-end rationalization frameworks that can accommodate arbitrary saliency methods , but require large language models that are expensive to train and perform inference with, so this is out of scope for this study.inferring high-quality explanations from large language models necessitates excessive amounts of compute and storage.Although gpt verbalizations are most promising, we urge the research community to look into more efficient ways to achieve similar results.emphasizing the concerns of rogers , we do not recommend the black-box model gpt-3.5 as a baseline for interpretability, because the model\\u2019s training data or internal parameters can not be accessed and the dangers of deprecation as well as the lack of reproducibility are serious con- cerns.However, we do think it has revealed great potential as a surface realization and contextualization tool for the task of saliency map verbalization.Is not solved by our verbalizations, as it is an inherent problem with feature attribution and rationalization.although multiple models and explanationgenerating methods are available, we specifically focus on one pair for both datasets , because the focus of our investigation is on the quality of the representation rather than the model.\",\"Our brain mri interpretations were evaluated by a single neurologist.Such annotations require deep expertise and are not easily carried out with high quality by trainees, which limited the amount of data we were able to collect.To ensure that the annotation would be as reliable as possible, we carefully thought of the dimensions in evaluating the generated interpretations and proposed a thorough annotation instruction guideline.further, the radiology reports we experiment with are from a single academic medical center, which makes the generalizability unclear.Beyond the brain mri interpretation experiments, our generation experiments are limited to a set of pre-trained models optimized for carrying out generation tasks in english.It is possible that multilingual models generating in languages other than english will show different properties.\",\"The experimental methodology employed in this study for both contrastive explanations and nmt is not directly extensible to languages other than english, due to the scarcity of resources such as models and annotations.The datasets employed in this study to evaluate contrastive explanations across various linguistic paradigms are restricted to sentences that possess a well-defined structure.As a result, it is possible that the conclusions drawn may not be generalizable to the broader distribution of sentences.Lastly, it should be noted that the method proposed in this study should not be used as a definitive explanation of model predictions in any other context.It is recommended to use the method as a debugging tool and should be employed in conjunction with other methods to gain a comprehensive understanding of model predictions.\",\"One limitation of our study is that interpretable feature spaces are at times only semi-interpretable.We infer from patterns of model behavior that buchanan features such as \\u2018human\\u2019, \\u2018child\\u2019, and \\u2018animal\\u2019 can be signals for animacy more broadly construed.The need to conjecture about what a feature means points to a weakness in our approach.Some interpretation will always be necessary, and with a more heavy-handed probing method like ours, it can\\u2019t be certain what effects are coming from the model and which are coming from the probe.One way to get around this need for subjective interpretation is to train a separate classifier for animacy more broadly understood, and then use the feature prediction model to examine what features are most relevant to the classifier.However, this method is not foolproof either.The classification distinction is wholly determined by the labeled data used to train the animacy probe, and the judgments are subjective.Even for a seemingly straightforward feature, the correct label is not always clear.Is a clock that sings the hour animate?\",\"We have demonstrated that cockatiel is capable of generating meaningful explanations that align with human concepts, and that they tend to explain rather faithfully the model.The concepts extracted of nmf are abstract and we interpret them using part 3 of the method.However, for the interpretation, we rely on our own understanding of the concept linked to the examples of words or clauses associated with the concept.This part therefore requires human supervision and will not be identical depending on who is looking.One way to add some objectivity to this concept labeling task would be to leverage topic modeling models to find a common theme to each concept.We recognize that this choice might not be optimal in every situation, as more complex concept may be advantageous in some cases, and more easily understandable ones, in others.We surmise that this choice might also depend on the amount of concepts and on the model\\u2019s expressivity.Finally, we have studied the meaningfulness and fidelity of our generated concepts, but ideally, the simulatability should also be tested.This property measures the explanation\\u2019s capacity to help humans predict the model\\u2019s behavior, and has recently caught the attention of the xai community.\",\"The observed effects in this work, in principle, can only be applied to the setting of our user study.Therefore this study serves only as a proof of existence, for a reasonably plausible and common setting in nlp research, that laypeople can be influenced by context outside of the attributed part of the input when comprehending a feature-attribution explanation.Action taken on design and implementation of explanation technology for nlp systems in another setting, or other systems of similar nature, should either investigate the generalization of effects to the setting in practice , or take conservative action in anticipation that the effects will generalize without compromising the possibility that they will not.\",\"Compared to other interpretability methods, marc is able to create explanations that more closely resemble human rationales.Nevertheless, the similarity to human rationales is always limited by the inner workings of the respective neural network: if a network\\u2019s reasoning does not mirror human reasoning, the resulting rationales will be incomprehensible to humans.Additionally, rationales created by marc are the result of a complete input optimization process.Therefore, the rationale creation usually requires hundreds of forward passes and gradient evaluations for the respective neural network, which makes the process of creating the rationale timeconsuming and therefore infeasible for many realtime applications.On modern hardware, creating a rationale for bertbase can take two to three minutes depending on the length of the input text, while resnet-101 and vit-b\\u002f16 are faster at about one minute.\",\"While our study provides valuable insights into the impact of finetuning on reasoning performance and the role of explanations during finetuning and prompting with respect to various reasoning skills, there are several limitations to our work.Firstly, we only consider a single llm, opt, as our base model.Our results may not generalize to other llms with different architectures or pretraining objectives.Secondly, we only use a limited set of reasoning datasets for finetuning due to the limited availability of open-source datasets with explanations.However, it is possible that our findings may not hold for models finetuned on larger closed datasets as usually seen in real-world scenarios.Thirdly, our experiments only cover a limited range of model sizes due to limitations in computational budget, therefore it is possible that our findings may not hold for much larger models.Finally, we only consider finetuning using fewshot prompting conditions in our experiments, and it is possible that our findings may not hold for models finetuned without in-context exemplars.Overall, while our study provides valuable insights into the impact of finetuning and explanations on reasoning performance, further research is needed to investigate these factors across a broader range of models, datasets, and finetuning strategies.\",\"Limited by the scale of annotated contrastive explanation corpus, our cpace model is only fine-tuned on approximate datasets selected with some designed principles.The performance of our method can be further improved with sufficient high-quality contrastive explanation annotated datasets over more nlp tasks.Moreover, in this paper, we mainly explore the effectiveness of the cpace model for multiple-choice commonsense questionanswering tasks, which is our goal, while previous retrieved-augmented methods cannot provide highly relevant knowledge or context for reasoning.Due to the fact that the contrastive explanation is designed to provide distinguishing information among given options [a1, a2,., an] or labels, there are no given candidates or labels in generative commonsense question-answering tasks, therefore, our cpace model cannot directly fit to other generative qa benchmark datasets.However, in our work, we provide some insights for future exploration, that is, generating question-specific distinguishing knowledge with a contrastive explanation generator can improve the performance and interpretation of current reasoning models.Meanwhile, although we validate the generalization of cpace on other qa tasks, including qasc and obqa, the effectiveness of our model in other nlp tasks requiring contrastive knowledge enhancement, such as open domain dialogue, needs to be further explored.\",\"Several limitations of our study include: 1.reliance on gpt-3, which is a closed-source product with an unknown training set ; and 3.focusing only on a single type of student model, opt.More broadly, learning from and with explanations carries some specific risks related to automation bias.While a model might rationalize its predictions using a seemingly coherent string of natural language steps, even if it eventually gets the prediction correct, there\\u2019s no guarantee that the eventually predicted output actually results from a process represented by the rationalization.A user might assign excessive confidence to that system based on the chain-of-thought.We observed many cases where the chain of thought seemed promising only to result in models ultimately making incorrect predictions in the final few tokens.Caution should be taken when displaying chain-of-thoughts to users.\",\"The proposed approach focuses more on logical reasoning on explanations for zero-shot classification.The semantic structures in explanations, such as inter-entity relations and event argument relations, are less touched.Within the range of logical reasoning, our focus are more on first-order logic, while leaving the\",\"We notice a few key limitations of our approach.Similar to what was shown by previous interpretability studies , incorporating explanations comes with some penalty on in-distribution accuracy when there is no spurious cue.This penalty decreases as model size increases, potentially because it is less challenging for larger models to generate good explanations.The second limitation is that our artificially constructed training set may not reflect the strength of the studied spurious cues in the real world.In our main experiments, we focus on the case where one spurious cue is perfectly correlated with the target label.For further exploration, we can study the alternative setting where there are multiple weak spurious cues instead of a single strong one.Finally, our work here is limited by the scope of the experiments.We only experiment with generative lms and binary classification tasks.Also, because of resource constraints, we only consider four datasets and eight types of spurious cues.Additional experiments using a wider variety of spurious cues and datasets would help to shed light on how our method generalizes to other scenarios.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"17_explanations_explanation_concepts\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"17_explanations_explanation_concepts\"],\"textfont\":{\"size\":12},\"x\":[11.28078556060791,11.247397422790527,11.340044021606445,11.227461814880371,11.24616813659668,11.215063095092773,11.2210054397583,11.253588676452637,11.24980640411377,11.246987342834473,11.2479248046875,11.252657890319824,11.25822639465332,11.205012321472168,11.33176040649414,11.25749397277832,11.261983871459961,11.255387306213379,11.270367622375488,11.29389476776123,11.29188346862793,11.264044761657715,11.283936500549316,11.225460052490234,11.167425155639648,11.231973648071289,11.187726020812988,11.277617454528809,11.260869026184082,11.253582954406738],\"y\":[-1.4395405054092407,-1.3880966901779175,-1.3978021144866943,-1.383232593536377,-1.376561164855957,-1.2567931413650513,-1.4333146810531616,-1.4075061082839966,-1.4032013416290283,-1.3989852666854858,-1.4039530754089355,-1.3970893621444702,-1.391151785850525,-1.26602041721344,-1.4601409435272217,-1.3665074110031128,-1.4079517126083374,-1.3804116249084473,-1.4018698930740356,-1.4259804487228394,-1.4353948831558228,-1.397943377494812,-1.434762716293335,-1.3820617198944092,-1.2857259511947632,-1.3252688646316528,-1.2485394477844238,-1.3770695924758911,-1.4113696813583374,-1.3822152614593506],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"In this paper, we simply prepend the retrieved prompt to the input embeddings before encoding.A well-designed method of combining prompts with the input embeddings, such as prefix tuning , may result in additional enhancements.2, prompt-based fine-tuning does not present obvious superiority over standard fine-tuning.\",\"The core of promptrank lies in calculating the probability of generating the candidate with a designed prompt by the decoder, which is used to rank the candidates.Our experiments have shown that the design of the prompt plays a crucial role in determining the performance of the method.While we have manually designed and selected some prompts to achieve state-of-the-art results, the process is time-consuming and may not guarantee an optimal result.To address this limitation, future research could focus on finding ways to automatically search for optimal prompts.\",\"All prompting methods are trying to extract knowledge from the large language models.Our paper compares their knowledge extraction abilities.Thus, the performance of roberta-large can serve as a reference point and provide insights for other llms.However, it is still necessary to assess each large language model independently to understand its capabilities comprehensively.We only tested a handful of simple manual prompt-and-verbaliser pairs which are included in tables 3 and 4.It is entirely possible that there is a lot of room for improvement in the design of manual prompt-and verbaliser pairs, thus providing us a even stronger baseline.We have opted to use ten trigger tokens in auto, in alignment with the experiment settings originally presented in the autoprompt paper.However, since the verbaliser domains generated under few-shot learning settings are noisy, reducing the number of trigger tokens may improve performance.\",\"Although we sidestep the challenge of selecting a specific prompt template for experimentation by opting for widely-used templates from previously published works, it is worth noting that numerous effective prompt templates are available, and the experimental results obtained using these templates would also provide valuable insights into testing our proposed method.Furthermore, while our method yields improvements, it is important to acknowledge that errors may exist in the rule-based automatic annotation of generic responses, which could potentially propagate to the learning of the diagonal parameter w.\",\"The limitations of this work are as follows: \\u2022 english.Our work builds autocomplete models for english language only.our work primarily focuses on deploying models on lower-end edge platforms where memory, as opposed to latency, is the major bottleneck.\\u2022 wikitext-103 dataset our work explores only wikitext-103 dataset for creating broad prompts.other datasets ) that explore the full range of low-frequency prompt patterns, which can arise in real-world situations.\\u2022 transformer-xl architecture our work studies only transformer-xl architecture to build word based and character based autocomplete models.\",\"As shown in table 1, our method is experimentally demonstrated to be effective for two llms.However, opt, a decoder-only model, is more suitable for the prompts generated by co-prompt.This seems to be because t0, the encoder-decoder model, requires a separate generator such as gpt2.The performance of prompts may vary to the generator involved in the vocabulary and training process.Also, there is a trade-off between search time and performance.While increasing the beam size and the number of document-query pairs enhances the probability of finding a more optimal prompt, it makes the search time proportionally longer.\",\".Our codeprompt focused on program and language generation tasks, so it is difficult to directly apply our method to program and language understanding tasks.We designed an input-dependent prompt template with fixed backbone words for a simple and efficient template.A more effective template can be crafted.We applied only codet5, the most state-of-the-art model, as the basis of the framework of our codeprompt.\",\"In this paper, we only evaluated our method on a limited number of nlu tasks and datasets.It is possible that our method may not generalize well to other tasks or domains that require different types of prompting knowledge or cloze-driven prompts.\",\".First, the prompt templates are manually designed, although we\\u2019ve introduced the rules and intuitions used in our implementation.Second, the proposed method may have low scalability to long text.Because we add the prompt at the end of the context, the prompt would be truncated if the context itself exceeds the maximum acceptable token length of the model.\",\"As we tentatively give a successful implementation of leveraging soft-prompt-based manner to benefit both single and multi-attribute ctg, such a paradigm deserves a closer and more detailed exploration.First, we explore multi-attribute ctg in the scenario of two-attribute composition, yet combining more attributes when generating a completion is more challenging and thrilling, and still in its fledgeless stage.Besides, while extensive experiments demonstrate that tailor consistently improves attribute-based ctg, applying our approach on a wider variety of plms will evaluate the effectiveness of tailor in a more generally way.\",\"Language our experiments are conducted on english, as all code-llms we know are pre-trained on english programming languages.Fundamentally, most popular programming languages are english-based, but international programming languages like scratch, or non-english-based programming languages like qalb also emerge.We look forward to the appearance of code-llms on these programming languages.Prompt engineering we manually design the prompts without prompt engineering techniques such as prompt search.The searched prompts may outperform the ones we used, but our experiments on interventions show that codex is fairly robust towards format perturbations.from the time we submitted the paper until now, several new llms have been released.We try to compare their performance with ours.We select three new llms: chatgpt, gpt-4 , and bard4, and feed the text prompts to them.Because we do not have access to some of their apis, we only experiment on a subset of 100 instances and report 4experiments are done with models updated to may 10, 2023.codex outperforms all these models in the automatic evaluation, but part of the reason is that these models provide more detailed outputs than the reference.Since codex is no longer available to the public, we provide codex generation results in our github repository.We also looked for alternatives and tried two open source code-llms codegen and starcoder with our code prompts.However, as shown in the case study, their performance is not comparable to codex, probably because they are more than ten times smaller in size.\",\"Although the proposed method provides interpretations for continuous prompts with both faithfulness and plausibility, it can still only be used as an approximation to find the most likely combination, since the process of combining discrete prompts to continuous prompts is irreversible.Moreover, the output layer of plms tends to degenerate and occupy an anisotropic cone in the vector space , which significantly increases the difficulty of finding the correct interpretations.We encourage future research to take the magnitude of token vectors and the tokens in their neighborhoods into consideration for a more robust interpretation.Due to space and time constraints, we only perform detailed experiments on p-tuning and the bidirectional language models like bert and roberta, which ignored numerous sota works such as prefix tuning , prompt tuning for continuous prompts and gpt , t5 for plms.We encourage future research to conduct experiments on more prompt methods and plms to investigate the generalizability of our method.Ethical statement we propose a novel view to interpret continuous prompts, which have been considered \\\"black boxes\\\", as combinations of human-understandable discrete tokens.Since the method itself is unbiased and faithful, and all experiments are conducted on publicly available datasets, we believe that our work does not create any potential ethical risk.Further, we discover shortcuts latent in continuous prompts, implying that systematic biases or discrimination may also exist in continuous prompts.These biases may originate from training datasets which are exploited by continuous prompts as a shortcut to the acquisition of true labels, or even originate from artificially implanted backdoors.We hope this work will provide the possibility to detect these potential biases in continuous prompts.Our created artifacts are intended to provide researchers or users with a tool for understanding decision-making and detecting possible unexpected shortcuts of continuous prompts, while at the same time offering the feasibility of cross-model transfer without extra training signals on target plms.They are compatible with the original access conditions.All use of existing artifacts is consistent with their intended use in this paper.\",\"In this paper we attempt to understand model responses using multiple prompts, and 2 different set- tings.we attempt at explaining our findings by analyzing the full text responses, but a more thorough analysis of the full text responses would shed more light into how these models behave.This will require extensive manual analysis of each statement and prompt response.Currently we do not explore every kind of full text response for each category type and prompt.More work needs to be done to systematically analyze the full text responses and connect them to the token responses and confidence scores.Besides, text-davinci-003 was the best performing llm when we started experimentation.Recently released chatgpt api and gpt-4 from openai, and other open source models were not analysed in this study, but one could extend our study to any class of llms to assess llm quality as well as the differences among them.\",\"With gpt-3 we only perform a subset of our evaluations of structured prompting on gpt-3, due to the cost of running the models in the api; this also means we do not run comprehensive prompt ablations to better tailor the setup for these models.Additionally, the results are difficult to interpret due to the black box nature of the gpt-3 models \\u2013 it may be due to pretraining data differences , the lack of prompt engineering for the models, or some other discrepancy.English-only experiments the experiments in this paper focus on english sequence tagging tasks, and it is unclear how well the proposed method generalizes to other languages.We find evidence of task-relevant data in pretraining corpora in nonenglish languages, which suggests there is signal for the approach to work in other languages.However, prior work shows that plms behave much worse when prompted outside of english but does not address the effect of pretraining data on this phenomenon.\",\"There are two limitations of the current mixpave model.First, although mixprompt can achieve comparable extraction performance with full finetuning, how to identify the optimal combination of the two prompts is challenging and remains unanswered.We conduct grid search in our experiments to empirically find the best prompts length.In future, we plan to investigate a systematic solution for identifying the optimal or a suboptimal combination.Second, our model learns attribute-specific prompts for a new attribute.We plan to explore a parametric network that could guide the learning of attribute-agnostic prompts.\",\"As with other prompting methods, preadd\\u2019s performance may vary depending on the exact wording of the prompt, and may require manual prompt design to achieve the best possible performance.Additionally, compared to more basic forms of prompting, preadd requires accessing the base language model\\u2019s output logits at each step of decoding, which can be inconvenient with certain apis such as the openai gpt3 api.5 tends to result in degenerate continuations.This is due to how the output logit distribution shift induced by preadd may significantly increase the logits of \\u201cnonsensical\\u201d tokens.The issue seems to appear predominantly in positive control applications of preadd , wherein the logit distributions \\u201cspike\\u201d more and have higher entropy.However, logit distribution truncation methods can be used in preadd to alleviate text quality decay by eliminating nonsensical tokens prior to applying the control.In this work, we evaluate toxicity using perspectiveapi as a convenient automatic metric, but we acknowledge that perspectiveapi is not a perfect measure of toxicity.For example, it may be biased against african-american english, and may fail to capture certain types of harmful outputs.Overoptimization against perspectiveapi could lead to unexpected side effects or biases in model outputs.Additionally, although controlled generation methods like preadd may reduce the toxicity of generated continuations in the presence of highly toxic prompts, they may still struggle to explicitly counter the original toxic language in the input.For our gender bias reduction task, we have focused only on occupations as provided in the winobias dataset.\",\"First, our work applies hand-crafted action labels as operation hints, which leads to some limitations to represent more complex operation steps.to eliminate the drawback where inaccurately generated operation prompts would mislead the next step, we can apply a verifier to evaluate the reliability of the generated operation prompts.When the reliability is low, we ditch the operation prompt to prevent it from guiding the model into an incorrect path.\",\"The proposed method is tested for a binary labeling scenario where each instance can belong to one of the labels but not both.The scenario of overlapping labeling space is not tested, nor is the scenario for multi-class labeling space.Since we aim to obtain high-quality prompts similar to the base prompt, if the base prompt is very restrictive, then the suggested prompt might be the same as the base prompt.The approach only applies to two moderately sized mlm models, and the extension to other larger models is not tested.\",\"Though our approach demonstrates better performances than the baseline models, how to design a good code-format prompt has not been fully inspected.Besides, we mainly conduct experiments on the black-box gpt-3 and codex models but they are not open-sourced and querying the gpt-3 model cost the economic budget.And the use of llms may bring environmental pollution.Another limitation of our approach is that the code-llms mainly trained on programming language datasets with english annotations.\",\"The proposed spc framework is model and task agnostic and can scale to different models and tasks.It is adaptable to domain\\u002ftask shifts and generalizes well with low data.However, the prompt length has to be tuned for each task, and there is no principled way of determining the optimal prompt length.This work does not explore the interpretation of soft prompts or prompt ensemble.ethical statement we hereby state that our study adheres to the acl code of\",\"It is important to note that the unified representation proposed in our study is just one option among many.Other linearization methods may potentially yield better results.has explored using code generation with jupyter notebooks and a hyper-text language model with structured prompting, respectively.Further research in these areas, such as converting all structured forms to markdown language or hyper-texts, may yield alternative unified representations.\",\"We highlight several limitations of our work: unnatural prompting format the choice to separate inputs and targets using a space character has proven effective to multitask finetune our decoder-only models.Nonetheless, poorly formatted prompts may result in undesirable behavior.For example, given the following prompt: \\u201ctranslate to english: je t\\u2019aime\\\", the model may continue the input with additional french content before starting to solve the task, i.translating the input from french to english.This can be mitigated by improving the prompts with a trailing full stop or a newline symbol.Encoder-decoder models, such as our mt0, do not suffer from this problem, as inputs and targets are fed into different parts of the model.Limited languages in xp3 the pretraining corpus of mt0 contains more than 101 languages , however, we finetune on only 46 languages.Likely, finetuning on the full 101 languages mt0 has seen during pretraining would lead to better performance.However, we decided to use only the languages of bloom in order to study language generalization.Similarly, one could likely attain better performance by enhancing xp3 with more datasets, such as via big-bench , or more prompts, such as via nl-augmenter.We have released an extended version of xp3 dubbed xp3x that covers 277 languages and is around ten times larger than xp3, but are yet to finetune models on it.Performance while our models show strong capabilities of performing tasks zero-shot, there remain numerous failure modes that are common in large language models.Similarly, in figure 15, mt0-13b is asked to provide an explanation, but answers with a question.We have made several modifications to the multitask finetuning recipe, such as loss weighting, mixing in long tasks, and various multilingual aspects, leading to the strong zero-shot performance of our models.However, there are many other changes to the multitask finetuning procedure that are worth exploring to get better models.Further, the pre-trained models we use, bloom and mt5, are suboptimal in many aspects such as compute allocation , pretraining datasets , pre-training objective and possibly model architecture.learning new languages during finetuning while we have investigated generalization to languages only seen during pretraining, we did not investigate generalization to languages only seen during finetuning.Our mt0 models are finetuned on several new languages not seen in pretraining.Out of those, we only evaluated on code , where mt0 performed at the random baseline.We point to follow-up work that has investigated the question of teaching bloomz new languages and work investigating adaptation of bloom.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"18_prompts_prompt_continuous\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"18_prompts_prompt_continuous\"],\"textfont\":{\"size\":12},\"x\":[9.970235824584961,10.040972709655762,10.03842830657959,10.012368202209473,9.94619083404541,10.05197525024414,10.032182693481445,10.02053451538086,10.013226509094238,9.802034378051758,10.034490585327148,10.040047645568848,10.03342056274414,9.824360847473145,9.988128662109375,9.959776878356934,10.014887809753418,10.087347030639648,10.062057495117188,10.006574630737305,10.04516315460205,9.976237297058105,10.000029563903809],\"y\":[0.5487226247787476,0.3243807256221771,0.2892020046710968,0.3527635633945465,0.2807055115699768,0.31792521476745605,0.5482576489448547,0.3035048246383667,0.4319530725479126,0.4322752058506012,0.5474297404289246,0.32915282249450684,0.4079941511154175,0.6124590635299683,0.38115110993385315,0.48052680492401123,0.4704897105693817,0.4618892967700958,0.5896487236022949,0.5534395575523376,0.5777254700660706,0.48538005352020264,0.4421353042125702],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Although our proposed knowledge transfer methods work well on wos in the few-shot setting, it is less effective on 5-datasets.Moreover, all methods fail in the full-shot setting.in addition, our approach requires a well-trained language model for task identification and a transformer-based model for parameter efficient tuning.Therefore, it is challenging to cooperate our approach with a language model with random initialization or non-transformer architecture.\",\"While our method is effective in zero-shot settings, we find that it has limited implications in supervised settings.This is because improving zero-shot translation presents a tug-of-war between language-agnostic and language-specific representations, each of which has a distinct effect on the model.Another major downside is reduced training speed relative to the baseline many-to-many model.We note that this is an artifact of the agreement loss which entails two forward-passes for each update.Finally, in the present work, we compute k-nns for every source word in a sentence.Although this has yielded strong results, we would like to explore a more explainable setting where k-nns can be applied to specific source words.\",\"All our experiments are done on the sequence labeling task, and they can be further evaluated on sentence classification tasks with classifier-based fine-tuning since the token used for classification represents the whole sentence.We provide a causal opinion on demonstration-based learning and a simple but not systematic method to alleviate the induced bias.Our demonstration-based learning builds upon previous works , where bert or roberta are used instead of large language models, such as instructgpt , palm , and opt.Furthermore, our conclusions are drawn from fewshot learning settings and cannot be directly applied to zero-shot inference.\",\"For those label names without semantic meanings, several keywords are still required for npprompt to work well.Furthermore, this study focuses exclusively on the zero-shot setting.However, there are potential avenues for exploration in the few-shot scenario, which is prevalent in practical applications.The applicability of npprompt to other tasks, such as ranking and relation extraction, remains uncertain and warrants further investigation.Designing a refinement method to jointly search for label words and templates can be a promising direction for future research.\",\"In this work, we proposed sstuning for zero-shot text classification tasks.During inference, we may need to design verbalizers even though we can use templates like \\\"this text is about [label name]\\\".For simplicity and fair comparison, we only refer to previous works for such designs, which may be sub-optimal.As shown in table 4, using the verbalizers \\\"terrible.\\\" for the sst-2 and imda tasks that we reported in the main results.If the labeled validation set is provided, the model may perform better by choosing verbalizers based on the validation set.Due to limited computation resources, we only tuned the model with 5.12 million samples, which is only a small portion of the available samples.We believe that tuning the model on a larger dataset help improve the performance.Even though the computational cost will also increase, it is worth it since no more training is needed at the inference phase.In addition, we did not do extensive hyperparameter searches except for the learning rate, which may further improve the performance.In our experiment, we only tested the method with discriminative models like roberta and albert.Its performance with generative models is not known.It is non-trivial to test on such models since generative models can do both natural language understanding tasks and natural language generation tasks.\",\"The current work has several limitations that warrant further investigation.Firstly, due to time constraints, we did not conduct experiments using the proposed framework on few-shot settings or a more challenging multi-label classification task.4 showed that the framework with the weight assignment resulted in only a marginal improvement in performance, suggesting that simcse may not be the most effective method for addressing prediction bias.5, we noticed that several irrelevant words are also generated as keywords with the language prompt, which may negatively impact the final representation.To address this issue, a better solution, such as keyword filtering, should be considered to improve the current framework.Lastly, we treated each word as a single atomic entity in the kg embedding space, regardless of its possible different senses or meanings.A more careful treatment of word meanings is necessary to handle the problem of polysemy.The race is on: second private team sets launch date for human spaceflight space..\",\"Rerankner conducts calibration after the regular training, which introduces extra computational overhead.This drives us to further improve the overall efficiency of our method.Recent works find that few-shot learning serves as an effective finetuning method of pretrained language models.It is reasonable to investigate our model under fewshot learning to reduce the overhead.Although we get competitive results with the state-of-the-art methods, there is still a gap between the oracle score and the best results.\",\"Although our primary effort in this work was to extract as much parallel corpora as possible, the improvement in the performance has been found to be only marginal.The labse and qe-based filtering experiments involve a hyper-parameter called \\\"threshold quality score.\\\" To achieve optimal results, we conduct experiments with different values of this hyper-parameter.The proposed few-shot transfer learning technique requires a small amount of data that needs to be annotated by multiple annotators.\",\"This paper acknowledges three main limitations: 1) the constraints of a zero-shot setting, 2) an uncertain generalization capacity due to limited data in the target task, and 3) the longer inference time required by a large language model.Given the absence of data for our task and the complexity of the target scenarios, collecting a large dataset for supervised or semi-supervised learning presents a significant challenge.As the first approach tackling this task, our framework performs the task in a zero-shot manner, but is applicable to fine-tuning if a substantial dataset becomes available.Consequently, we expect that future research could further train the proposed framework using supervised learning or fine-tuning, thereby enhancing the alignment of inferred implicit intents and recommended bots with training data.This would expand our method to various learning settings and validate its generalization capacity.Conversely, the gpt-j model used for recommending task-oriented bots is considerably large given academic resources, thereby slowing down inference speed.such a smaller language model could not only expedite the inference process to recommend task-oriented bots but also be conveniently fine-tuned using collected data.Despite these limitations, this work can be considered as the pioneering attempt to leverage commonsense knowledge to link task-oriented intents.The significant potential of this research direction is evidenced within this paper.\",\"In this paper, our main contribution is an effective and efficient framework for universal ie.We aim to introduce a new unified ie paradigm with extractive structures and triaffine attention mechanism, which can achieve better performance in a variety of tasks and scenarios with more efficient inferencespeed.However, it is non-trivial to decide whether a sophisticated and artificial prompt is required for complex datasets and large label sets.In addition, we only compare with limited baselines with specific datasets configurations when analyzing the performance of the uniex in supervised, few-shot and zero-shot settings.In experiments, we implement only a few comparative experiments between bert and roberta due to the limit of computational resources.\",\"This work focuses on pretraining large language models for zero-shot generalization.Although our proposed method is more efficient than baselines, it still requires significant computational resources, specifically gpu resources.our study is also limited by the computational budget, preventing us from training models as large as gpt-3 or t011b.However, our large++ model already rivals or outperforms previous state-of-the-art models.\",\"We conclude the limitations of our schema into two aspects.Firstly, our method benefits from the assumption that there exists similar semantics between the seen data and unseen samples.However, our work might not own obvious advantages in the case where the correlation among domains is weak, such as medical assistant and movie service.But notably, in such cases, most zero-shot learning methods will also fail to show well generalization.Secondly, we propose to train semanticindependent dst experts, which is ideal but we believe advanced components could move towards this goal, such as using advanced clustering algorithms and pretrained language models.\",\"Our method regen is a general framework for zero-shot text classification.In this work, we aim to first bring in simple and intuitive way to justify the power of unsupervised dense retrieval for zeroshot learning.besides, our experiment results are all based on bertbase sized models.Also, we point out that this work focuses on zero-shot text classification with task-specific verbalizers and unlabeled generic corpus, thus it can be nontrivial to adapt our framework to other tasks such as natural language inference as well as low-resource tasks where even the unlabeled generic corpus can be hard to collect.Extending regen to these settings will reduce the annotation burden under more challenging scenarios.\",\"In this work, we propose a structure-based pseudolabel generation method for zero-shot video sentence localization and propose a noise-resistant method to reduce the effect of pseudo-label noise.The limitations of our work are: although we generate free-form natural language queries, the distribution of generated queries may still differ from the distribution of queries in the dataset , which may degrade the performance during testing; our pseudo label refinement can correct the noisy event labels, but there is no mechanism to correct noisy queries.\",\"Our approach relies on the performance of the fewshot paraphrasing.This results in two limitations for our approach.One limitation is the difficulty in accessing gpt-3 and opt-175b models.These models currently need to be more widely available.Opt-175b has a free version but it is very slow.Another limitation is the need for annotated demonstrations for few-shot paraphrasing.While there are available models and tools, like quillbot, that can be used for this purpose, their quality is not comparable to gpt-3 and opt-175b.This can limit the power of these tools in our approach.Using human knowledge to paraphrase the demonstration can help these large models generate high-quality paraphrases but it is expensive.\",\"We only create a high-quality dataset for evaluation and a small-scale dataset for few-shot learning, since the lack of a large-scale chinese parallel ss corpus.The available research methods for chinese ss are limited to unsupervised learning and few-shot learning.then we can directly train more supervised models for chinese ss.Furthermore, we only analyze whether the current standard metrics are suitable for the evaluation of chinese ss, and leave the work of proposing a new metric for future study.Due to time constraints, we do not perform a human evaluation for the output of llms.\",\"While our approach is shown to be effective in improving the zero-shot adaption ability of these plms, the scope of this work has only been extended to english languages and has not been tested on other languages.In addition, another limitation of this work is the scope of the aspect.Aspect is defined across 3 main categories of intent, sentiment, and topic in the work.However, given the massive space of text label interpretations, our aspect range can be refined and expanded even further, lending to more analysis of the stability of implicit explicit training as the number of aspects grows.We do not investigate this scenario in this work.\",\"This paper introduces the problem of few-shot novel product title generation to efficiently and accurately generate informative and appealing titles for novel products with limited labeled data.However, the training of our proposed model relies on the paired image-attribute-title data, which may not be easily obtained simultaneously in the real world.Therefore, our model may not work well when high-quality image data or textual profile is missing.The limitations could be alleviated using techniques such as knowledge distillation or self-training.Besides, the writing styles of the generated titles are highly correlated with the training data.Hence, it requires specific and appropriate treatment by experienced practitioners, when deploying new products online.\",\"The current work is an initial attempt at studying the problem of zero-shot classification of semistructured documents.This work does not experiment with the variety of encoding strategies in the literature that combines textual, visual, and layout information.It is likely that richer document representations derived from these diverse encoders will further push the limits of zero-shot classification when combined with our proposed unsupervised contrastive pretraining procedure.Second, results in this paper are on a single dataset, i.While we mitigate this to a large extent by creating four nonoverlapping test splits , results on more datasets might yield more useful insights.In practice, the lack of datasets for this task is what makes this exploration difficult and might require creation of new resources\",\"In this paper, we focused on analyzing the properties of textual representations in the few-shot learning scenario.Its applicability to broader annotation scenarios could be presumed but is not supported by our empirical results.Our experimental setup is based on binary classification tasks using english datasets.While our approach is general and could be easily extended to multi-class scenarios, more work would be required to extend it to other more complex structured prediction settings such as sequence tagging.We see several ways in which this work could be extended.The most obvious extension consists of trying to generalize the notion of alignment to other tasks beyond sequence classification, such as sequence tagging.In this paper, we have used thas to understand the quality of a given textual representation.However, since thas is a function of a labeling and a representation, it could also be used to measure the quality of a labeling , given a fixed representation.For example, this might be used in the context of hierarchical labeling, to measure which level of label granularity is better aligned with some input representation.The goal of this paper was to provide an explanation for the success of pre-trained word embeddings for text classification in the few-shot learning scenario.We believe that with our proposed methodology we have successfully achieved this goal.However, it should be clear to the reader that we do not provide a method for picking the best representation, i.this is because our analysis requires access to labeled data and if labeled data is available the best way to select a model will be via cross-validation.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"19_zeroshot_fewshot_verbalizers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"19_zeroshot_fewshot_verbalizers\"],\"textfont\":{\"size\":12},\"x\":[12.920977592468262,12.947050094604492,12.920957565307617,12.965503692626953,12.953827857971191,12.913432121276855,12.838455200195312,12.886459350585938,12.893251419067383,12.876723289489746,12.9302396774292,12.934820175170898,12.959275245666504,12.958845138549805,12.469742774963379,12.89525318145752,12.919440269470215,13.0121488571167,13.005298614501953,12.897136688232422,12.904940605163574],\"y\":[4.720306396484375,4.856684684753418,4.7874627113342285,4.895744323730469,4.857238292694092,4.825674533843994,4.566287517547607,4.829352855682373,4.805818557739258,4.779520511627197,4.8339924812316895,4.821487903594971,4.874471187591553,4.876600742340088,0.9358227252960205,4.72144889831543,4.868340015411377,4.904472351074219,4.864570140838623,4.822456359863281,4.622387886047363],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\".We evaluate the proposed method on multiple datasets and results show that the proposed method yields new state-of-the-art performance.We analyze why our approach attains superior performance by conducting ablation studies and sentence representation visualization.We further apply our model as an aigc detector to distinguish chatgpt-generated texts from those generated by human experts and the experimental results demonstrate that our model outperforms human evaluators in the setting of paired answers.Limitations table 6 shows the results of our model and other methods on the agnews benchmark.Interestingly, we notice that our approach reports a slightly inferior performance when compared with mdf+imlm.We can see that methods using sentence representations based on token aggregation, e., fasttext9 or glove -based isoforest, ocsvm, and cvdd , as well as bert based mdf + imlm , perform especially well on agnews compared to their performance on other datasets.We conjecture that this is because agnews has a much larger variation of sequence length than other datasets.A larger length variation will lead to more acute fluctuations in perplexities, especially when adopting an autoregressive language model with unidirectional context such as gpt-2-small in this paper, making it more difficult to distinguish between id and ood examples than in other datasets.In contrast, sentence representation based methods benefit from directly estimating the ood score using the information from the whole sentence, thus producing superior performance.Fortunately, the limitation of auto-regressive modeling could be eliminated by leveraging transcormer as the base model of our approach, where bidirectional context is used for estimating tokens at each position.\",\"Since mpchat sources the data from reddit, it has the limitation that it may not be representative of the general population.First, all subreddits of mpchat are primarily written in english, and a significant percentage of reddit users are from english-speaking countries.The four countries with the highest desktop traffic on reddit are the us, uk, new zealand, and australia, accounting for 66% of the total user.Reported that reddit users are more likely to be male , young , college-educated , and politically liberal.Therefore, mpchat may reflect such somewhat narrow interests, and the demographic group represented by our model may be biased toward personal conversations suitable for it.\",\"Our study has several limitations that should be acknowledged.First, the study was conducted on a relatively small test dataset.second, while we employed a rigorous methodology for evaluating chatgpt\\u2019s performance, we have not measured other safety criteria, such as biases or privacy issues in using 14 15 2023-03-30-vicuna\\u002f this model.Third, our study focused only on the initial step of suicide risk assessment and did not explore the use of chatgpt in ongoing monitoring or intervention.Fourth, we are unsure if the umd dataset has been used in the training of chatgpt in any capacity since the specifics of the training data of chatgpt are not disclosed to the public.It is important to note that despite these limitations, our work represents an important first step in understanding the potential for chatgpt in suicide risk assessment.Future research should aim to address these limitations and explore the feasibility, safety and effectiveness of chatgpt in broader clinical settings.\",\"The primary limitations of the current study pertain to the selection of prompts and evaluation metrics.The experimental cost of requesting api responses from openai to assess chatgpt\\u2019s text generation abilities imposes significant constraints on our choice of datasets.Therefore, we have to limit our experimentation to only two related controllable text generation datasets.While we have evaluated chatgpt\\u2019s performance at both the document and sentence levels, we cannot extrapolate that chatgpt has similar performance for other text generation datasets.Additionally, the experimental cost prohibits us from conducting traversal experiments on the selection of hyperparameters.We relied on the default configuration recommended by openai, and we maintain consistency in all hyperparameters to ensure the fairness of the experiments.Secondly, although we have studied the impact of prompt engineering on chatgpt, the selection of prompts is mainly affected by human understanding, and the number of potential prompts is infinite.Hence, we cannot guarantee whether other prompts that we did not select will yield the same conclusions as our experiment.Furthermore, chatgpt is subject to continuous updates and iterations, which may lead to improved performance, making it difficult to predict if future versions of chatgpt will have similar results to our experiments.Finally, to select appropriate evaluation metrics, we have included both domain-related evaluation metrics and domain-independent evaluation indicators.However, we acknowledge that the automatic met- rics may sometimes not capture all aspects of the intended construct correctly.\",\"Of chatgpt in standard academic datasets.To our best knowledge, this is the first work that conducts an extensive evaluation of chatgpt in benchmark nlp datasets.We observe that even though chatgpt obtains impressive zero-shot performance across various tasks, it is still far from reaching human-level performance in many tasks.Moreover, potential biases and ethical concerns, as well as misinformation generation risks of chatgpt are discussed.In addition, a unique capability of chatgpt has been studied.we will make all our prompts and chatgpt-generated responses publicly available.\",\".It is not uncommon to spend a lot of efforts on coming up with the right prompt.Each use case may need a different prompt.\\u2022 overall, chatgpt provides a stable output especially if one asks for a specific output format.But there is still an element of volatility when one or few responses contain extraneous text, or the categories are outside of the predefined list.This is due to the conversational nature of the model, and such cases had to be processed as exceptions.\\u2022 chatgpt \\\"remembers\\\" the past conversations, but this memory is limited to the context window size which is only 4096 tokens.This makes it challenging to work with large datasets which have to be split into pieces to be processed independently.\\u2022 one has to remember that one must pay for the use of the chatgpt api - very long prompts used multiple times or too many examples for few-shot learning may be discouraged for cost savings purposes.\",\": chatgpt struggles to explain sequences that do not fit into the learned patterns.Further, it will not indicate when something is not funny or that it lacks a valid explanation.Instead, it comes up with a fictional but convincingsounding explanation, which is a known issue with chatgpt.we identified three main characteristics that generated jokes had in common, i.the presence of a single joke-characteristic, e., the question-answer template, is not sufficient for a sample to be wrongly classified as a joke.The fact that chatgpt was not misled by such surface characteristics shows that there is indeed a certain understanding of humorous elements of jokes.With more joke characteristics, a sample is more likely to be classified as a joke.Although chatgpt\\u2019s jokes are not newly generated, this does not necessarily take away from the system\\u2019s capabilities.Even we humans do not invent new jokes on the fly but mostly tell previously heard and memorized puns.However, whether an artificial agent is able to understand what it learned is an exceptionally tough question and partly rather philosophical than technical.In the present experiments, all prompts were posted in an empty, refreshed chat to avoid uncontrolled priming.But, clearly, context plays an important role in the perception of humor.Chatgpt is able to capture contextual information and adjust its responses accordingly to the preceding course of conversation.This is an intriguing capacity, which we would like to include in future investigations.\",\"Of this study are primarily due to budget and credit constraints.Consequently, our query rewriting observations are based on a sample size of 2000, leading to limited generalizability of findings.Another limitation of the limited resources was the limited context size of chatgpt and the relatively long nature of the questions in our dataset.Hence, we could not test prompting chatgpt with in-context examples for better query rewriting performance.finally, chatgpt architecture is not open source, preventing us from testing advanced prompting methods.the study is also subject to the risk of \\\"hallucinations\\\" in chatgpt\\u2019s responses, which may lead to imprecision in query rewriting.The study suggests further investigation into these issues to improve the accuracy and reliability of the results.We recommend further investigation into these limitations and any potential societal biases present in our dataset to enhance the reliability and performance of query rewriting.\",\"The limitations of this study are primarily due to budget and credit constraints.Consequently, our query rewriting observations are based on a sample size of 2000, leading to limited generalizability of findings.Another limitation of the limited resources was the limited context size of chatgpt and the relatively long nature of the questions in our dataset.Hence, we could not test prompting chatgpt with in-context examples for better query rewriting performance.Finally, chatgpt architecture is not open source, preventing us from testing advanced prompting methods.The study is also subject to the risk of \\\"hallucinations\\\" in chatgpt\\u2019s responses, which may lead to imprecision in query rewriting.The study suggests further investigation into these issues to improve the accuracy and reliability of the results.We recommend further investigation into these limitations and any potential societal biases present in our dataset to enhance the reliability and performance of query rewriting.\",\"We completed this evaluation based on the beta version of chatgpt, and the relevant results may change as openai continues to improve chatgpt.In addition, we use the technique datasets collected from third-party institutions and publicly available data on the internet.The large size of these datasets makes it difficult for us to verify their accuracy manually.Therefore, errors in these datasets may affect the conclusion of this paper.\",\"A limitation in this study is we only used opensource models.We were unable to evaluate chatgpt, for example, because the data use agreements under which these datasets are made available forbid sending the data to outside apis.Other models are frequently being released and we did not exhaustively test all publicly available language models.However, the focus of the paper is not to find the best llms but instead providing insights into using llms to improve transferability.\",\".First, our study is small in scope.By their nature, experts are difficult to recruit and consequently the domains we can cover are limited.The small sample also suggests that the quantitative measures may not be stable in a larger or more representative sample.Second, our observation process was somewhat artificial.We generated replies for our experts and did not to do any prompt tuning.This reflects the way the expert chose to ask the question, but does not capture the ceiling of performance that would be possible in a conversation.As the family medicine expert noted about our question comparing wikipedia to chatgpt, \\u201cfor more detail one could spend more time with wikipedia and to the organization themselves, but chat provides an immediate general summary and the opportunity to drill down further with ongoing questions and conversation.i have used chat gtp to do medical and biological research in a matter of minutes which would have taken me hours previously\\u201d.Is a useful step in that direction.Third, the responses across experts are not necessarily comparable.We allowed experts to choose their own questions and provide their own interpretations of the key measures like coherence or conciseness.Comparability of scales across contexts is a long-standing problem in survey research and we highlight some of the concerns around the accuracy question above.Nevertheless, we felt that asking a set of closedended questions would help to provide some aggregate judgment, adding some systematic data to the anecdotes shared in public forums.While we caution about drawing any binding conclusions from our preliminary work, we felt that given the fast-evolving nature of the technology, a quick assessment was merited.one important aspect that is out of scope in our analysis is differential accuracy by question asker.Latanya sweeney\\u2019s classic study of racial discrimination in online ads points to the possibility that how a question is asked or where it is asked from could result in inaccurate or harmful answers for marginalized communities.We have also focused exclusively on english language questions and answers, but differences in easily-available training data across languages can produce substantial differences in the information offered.For example, yang and roberts shows that embeddings trained on baidu baike\\u2014an online chinese encyclopedia\\u2014encode substantially different associations with sensitive historical events and people than chinese language wikipedia.There is much more to understand about the degree to which large language models can mimic expertise.\",\"At the time of writing this work, chatgpt is only available as a proprietary free research preview via a web interface.parts of our analysis are qualitative, as quantification is challenging due to limited accessability of the investigated model.Some details about the investigated model are not yet disclosed.This is true for the model design as well as for the data used to train chatgpt.Multiwoz is a freely available and widely used dataset, therefore no guarantee can be given that chatgpt has not been exposed to at least some meta details regarding this dataset.Given the nature of the free research preview, exact reproducibility is not guaranteed, as the model may change any time.However, it is expected that any future version of chatgpt retains its general abilities and behaviors.building a general purpose model such as chatgpt is extremely costly and an option only for few.However, once it exists, it may be utilized for a multitude of purposes.As a model, chatgpt does not need to be built for dst in order to be useful for dst.With capable enough general purpose models, fine-tuning towards specific tasks may be avoided.Fine-tuning is challenging for multiple reasons such as the need for adequate data, computational costs, risk of over-fitting and catastrophic forgetting, among others.Just like its sibling model, chatgpt will become available as model-as-a-service.The advantage of this is that a massive lm such as this is usable independent of the user\\u2019s hardware.But this advantage comes with the disadvantage that it will in all probability remain proprietary.In consequence, it will likely not be possible to ever run, adapt, train or modify chatgpt on local machines.Chatgpt as model-as-a-service is likely to remain a black box to customers and researchers, even if just in parts.In fact, a model update during our experimental evaluation prompted us to re-process a few of our test dialogues.This property impedes backward compatibility and the ability to trust in familiar behavior.A general purpose model may show too general behavior and converse about more than what is required or requested.This also poses vulnerabilities for adversarial attacks.To this end, models such as chatgpt have been trained with human feedback to better handle malicious intent and abusive behaviors.as such, its indefinite availability cannot be guaranteed.Further, recurring costs for access may be too high for certain downstream tasks.As a hosted service, latency might become a bottleneck or hindrance for its use as a component in complex applications.\",\"Rooted in inherent properties of general purpose models, preventing these to become holistic solutions to complex nlp problems without further research.We discussed opportunities provided by chatgpt and similar models to advance the development of specialized systems.\",\"Some limitations of the present study should be considered.First and foremost, the inherent statistical biases of chat-gpt and gpt-3.5 might skew the data distribution, which is difficult to control without knowing what data the models are trained on.It is therefore also certainly possible that the generative models have already been trained on the vaccinpraat dataset.Additionally, we found that the text examples in the prompts also affect the distribution.Moreover, these statistical biases might lead to repetitive sentence structures in the data.Second, the generated data contains false information about vaccines and covid.One should therefore act with caution when interpreting the synthetic data and should only consult fact-checked sources for information about covid vaccines.Moreover, while the messages are believable enough as a reader to be vaccine-hesitant, the messages are more \\\"neutral\\\" in nature than the goldstandard data.This is especially apparent in the chat-gpt dataset, which was to be expected because of the guardrails imposed on the model.This distribution shift could explain the degraded performance compared to the back-translated data in the in-platform.third, since we only focused on the vaccine hesitancy monitoring task, more research should be conducted with the presented method for more multi-label tasks.However, this method could only work effectively for datasets with a relatively small number of labels, as the descriptions need to fit in the prompt.However, the promising results from the conducted experiments and analyses should stimulate further exploration for other multi-label text classification tasks.\",\".such an evaluation will need to include larger numbers of human raters to improve the statistical power of the surveys.Recent automatic evaluations may also help improve development efforts, as a sufficiently powerful llm such as gpt-4 may be able to monitor the chatbot for regressions in its ability to speak fluently, sensibly or specifically.This assessment, known informally as the \\\"vicuna assessment\\\", cannot give an evaluation of the chatbot\\u2019s fit-for-purpose, but could be used to compare short conversations from several versions of the same chatbot.This could free up more human resources to evaluate the knowledgegroundedness and fit-for-purpose of future versions.In addition, given more computing budget and more time to engineer prompts, larger language models beyond llama 7b could be further studied or fine-tuned while experimenting with fine-tuning datasets and process.There are also many thresholds and parameters that could be further tested in the development of the knowledge-grounding system, wherein similarity measures inform the system\\u2019s decision to answer using a generative model versus responding with language directly from the knowledge base.\",\"Since the training datasets of chatgpt are unknown, some data used for evaluation may or may not exist during the training phase of chatgpt also, a new version called the gpt-4 model has been released that may ensure higher accuracy.Nonetheless, gpt-4 is very costly to use, around 60x more expensive than chatgpt.Meanwhile, even using the paid chatgpt plus2 subscription, it is available for just a limited use.Another limitation of this research is that the results mentioned in this paper for chatgpt may not be reproducible, as chatgpt may generate different responses for the same input prompt.Although the experimental results may change over time, this work will still give a concrete direction for future research using chatgpt like large language models in the biomedical domain.\",\", conducting the human evaluation by the authors does not lead to any unwanted biases or ethical concerns.Only the publicly available academic datasets are used that did not require any licensing.Thus, no personally identifiable information has been used while evaluating chatgpt responses.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"20_chatgpt_chatgpts_rewriting\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"20_chatgpt_chatgpts_rewriting\"],\"textfont\":{\"size\":12},\"x\":[9.573430061340332,9.273801803588867,9.315534591674805,9.57279109954834,9.535585403442383,9.359813690185547,9.399667739868164,9.358613967895508,9.37369155883789,9.380257606506348,9.377534866333008,9.254459381103516,9.317506790161133,9.515091896057129,9.531112670898438,9.23481273651123,9.320310592651367,9.314105033874512,9.3893404006958],\"y\":[1.1237637996673584,1.1643627882003784,1.1476413011550903,1.0672177076339722,1.259763479232788,1.0917614698410034,1.200753092765808,1.1004899740219116,1.0997679233551025,1.0995850563049316,0.8980354070663452,1.1794004440307617,1.102199912071228,1.056644320487976,1.2536202669143677,1.0561636686325073,1.114924669265747,1.156969666481018,1.1207258701324463],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"The schema information and grammar design are domain specific.We have tested our approach mainly on our own dataset, though we believe the similar approach can be applied to other tasks, as long as the meaning representation involves functions and arguments.Also, we have not explored all approaches to obtain the highest possible accuracy on this dataset, because our main goal is to show the effectiveness of the proposed approach, which we believe is clearly demonstrated by the current result.At this stage, the difference between in-distribution and out-of-distribution accuracies remain very large, and there is a large room for further improvements.We hope by releasing this dataset we can help promote research in related areas.\",\"In the case of complextable, where table images are generated using an auto html table creator that utilizes a web browser engine for rendering, applying tablevlm directly to recognize the structure of handwritten tables without fine-tuning poses a challenge.This is particularly evident when dealing with handwritten tables found in ancient documents.Moreover, the process of annotating the structural information of tables in handwritten documents is both time-consuming and laborious.As a result, there is ample room for further exploration and improvement in enhancing the accuracy of table structure recognition for handwritten tables.\",\"First, in this work we limit the experimentation to vertical relational web-tables only, following the format of benchmarks used in tableqa, i.While we believe that itr can easily be extended to horizontal entity web-tables, e., tables from wikipedia, we do not expect our algorithm to transparently work on other types of tables that we do not consider, e., matrix tables from scientific papers and\\u002for spreadsheets , where table items can be represented differently.second, itr selects the relevant table elements by using a question as query.Therefore, it can only be applied for tasks with table-text joint input such as tableqa we showcase in the paper, or also table entailment tasks, e.Unfortunately, itr cannot be used for tasks where table is the only input, e.Finally, while itr is beneficial for questions that do not rely on table completeness, its effectiveness is limited when, for example, all table cells are required to be predicted.Consider a question that requires cell counting, and the gold cells satisfying the query can be more than what we can feed a model with, e., \\u201chow many championship did player a get?\\u201d and player a has won 500 champions.However, this limitation does not arise from our approach and is rather inherited by existing tableqa models in the literature.Indeed, it can be a potential future direction of our work, which requires model innovation and table transformation that focuses on representing the information in a compact form.\",\"Our synthetic pre-training dataset was automatically generated from manual templates, which inspite of dataset creation scalability and low cost, may limit the diversity of the generated sql queries.Our model, multitabqa, requires improvement in numeracy understanding and numeric operations.Real numbers are especially challenging and the model may not be able to correctly generate all the digits of the number correctly rending the generated cell incorrect.Furthermore, large input tables pose a challenge as the input sequence may get truncated beyond the model\\u2019s maximum sequence length.This has practical limitation in the size and number of input tables which the model can accommodate before truncation which leads to incorrect answers.\",\"Our model is currently suitable for generating ordinary tables with attribute names and records, but it may struggle with more complex table formats that involve merged cells.To improve the flexibility of our model, we plan to investigate more versatile forms of table representation.Another limitation of our model is that our model training involves longer training time, compared with seq2seq baselines.This may be due to the inherent instability of target assignments.\",\"While our work displays many strengths, we highlight some important limitations in our analysis.Namely, we pretrain our stamp models on a range of sources containing structured knowledge, however our analysis is limited to text-to-sql tasks and does not demonstrate if such pretraining helps more generally in structured information tasks.For instance, stamp pretrains on tables with masked column recovery as a way to learn the structure of a table using the rows and natural language statement as context, and a context-to-output objective that always includes the table in the context \\u2014 since this matches the format of textto-sql tasks.second, we acknowledge that significant gpu resources are required for pretraining, even in continued pretraining approaches like ours which limit the breadth of ablations studies.Conversely, our work explores pretraining at smaller scales where certain phenomena like strong zeroshot performance is unlikely.Pretraining specifically on structured knowledge has an unknown value at larger scales with models having tens or hundreds of billions of parameters.\",\"As mentioned previously, the main limitation of our approach is that, very complex joins e.sequences of joins of different types and joining on columns which have different names in different tables is not straightforward in our approach.One extension to possibly handle this would be using a decoder to generate the complex sequence of joins and column relations.the select, where, order by and group by can still be done via our approach and we could also continue to use mtl.The second limitation of our approach is subquerying capability which currently we do not have a strategy to handle queries which would require them.However, this is notoriously hard even for existing sota semantic parsing algorithms e.Achieves only 50 exact match accuracy on the extra hard spider data subset and 61.\",\"Even though santa shows strong effectiveness on learning the representation of structured data, it heavily depends on the alignment signals between structured and unstructured data.Such alignment relations can be witnessed everywhere, but the quality of constructed pairs of structured and unstructured data directly determines the effectiveness of santa.Besides, we use the product bullet points and code descriptions as the unstructured data in our experiments, which is designed for specific tasks and limits the model\\u2019s generalization ability.On the other hand, santa mainly focuses on evaluating the structured data understanding ability through text data representation and matching.It is still unclear whether santa outperforms baseline models in all downstream tasks, such as code summarization and code generation.\",\"We raise a new challenge task in our medical dataset ccs.Comparing with existing datasets, ccs requires text-to-sql models to generalize to different databases with the similar structure in the same domain.To tackle this problem, we provide a baseline method named syntactic role prediction, which is an auxiliary task and can be combined with the main task in a multitasking way.Our experiments prove that srp can help improve the cross-schema generalization ability of models.However, the improvement is not that large.How to generalize models across different databases sharing the similar structure is still a challenge issue.\",\"Although ctbls enhances llms with tabular knowledge to generate grounded responses, certain limitations remain to be addressed.Firstly, the efficacy of ctbls is constrained by the total number of knowledge sources employed during the augmentation process.Token length restrictions in the openai api limit the knowledge augmentation to the top three cells of the table.Another limitation is the incapacity of ctbls to handle queries pertaining to the entire table.Figure 4 demonstrates one such instance in which the state tracker module accurately retrieves three rows of the table corresponding to oil and gas industries, yet the response generation module fails to utilize this information when transforming the retrieved state into a response.Generally, ctbls encounters difficulties with counting, comparing the values of cells, and other mathematical operations, an issue we aim to address in future research.\",\"Our proposed graph-guided sql generators are superior in generating complex sql queries.How- ever, the model has a large number of parameters and requires more computational resources, which is a common problem with current methods of generating complex sql queries.In addition, the proposed knowledge-enhanced re-ranking mechanism is proposed to leverage the knowledge in plm to choose the best sql query from the beam output.However, it does not take into account the database schema which can be the source of domain knowledge.\",\"Our proposed system was tested on two opendomain tableqa datasets, with one of them being relatively small compared to the other.Also, the current open-domain tableqa datasets are limited to look-up questions.They do not cover more complicated questions that involve multiple cells and complex table operations, such as sum\\u002fmax\\u002fmin\\u002fsubtract in some questions of wikisql and wikitablequestion.another limitation lies in the token length limit of modern transformer models.The best-achieving models typically accept up to 1024 tokens.This limitation becomes more obvious when tables grow longer and the information being sought go beyond the limit.We believe that, with better approaches addressing this limitation, our system can achieve better performance.The solution can be either applying sampling strategies to pick the rows and columns that are most relevant to answering the question, or increasing the capacity of future transformer models.\",\"As mentioned in table 3, one limitation of the current study is the small scale of the test sets with modern parsers.additionally, though we have shown using an auxiliary schema prediction model greatly improves the performance of a text-to-sql system, the addition of a model for the text-to-sql task is a limitation given the time and training resources required.\",\"Our approach has proven to be superior to previous methods on multiple public benchmark datasets.However, one major disadvantage of the table filling method is the increased training time and memory usage.The computational resources are required for the 2d table representation of word-pair relations for constructing and storing the table.In comparison, using a sequence representation as input could be generally more efficient.Our approach also faces the computational challenge.\",\"That may suffice for current text-to-sql benchmarks with small-scale schemas.However, for large-scale schemas, modifications to the plm encoding method are necessary.If the plm were to encode the question and schema separately, the emsl would still be required.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"21_tables_table_tableqa\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"21_tables_table_tableqa\"],\"textfont\":{\"size\":12},\"x\":[11.252795219421387,11.22783088684082,11.248559951782227,11.202678680419922,11.22109603881836,11.21629524230957,11.32421588897705,11.2724027633667,11.244917869567871,10.899080276489258,11.33370304107666,11.05650520324707,11.229732513427734,11.249458312988281,11.286130905151367,11.217694282531738],\"y\":[-0.07850528508424759,-0.09358378499746323,-0.11317764222621918,-0.08048240095376968,-0.09728600829839706,-0.07454057782888412,-0.09520148485898972,-0.06382512301206589,-0.06565506011247635,-0.22211700677871704,-0.07885909080505371,-0.1798430234193802,-0.048102546483278275,-0.08603601902723312,-0.03829484432935715,-0.09436731785535812],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\".First, our consistency study focuses on our used categorization model and was conducted on only one specific dataset.It might not perfectly generalize to other problems.Second, the proposed solutions are based solely on data augmentation without changing the current production settings and model.Other approaches such as changing the model\\u2019s objective function to take consistency into account might also benefit the solution.Lastly, in terms of user perspective, while our solution show significant improvement over the baseline, inconsistencies are still visible.\",\".\\u2022 because of the specificity of each jurisdiction, generalizing to other countries may not be possible on all labels with the exact same models.\\u2022 the manual annotation process is a weakness: while it results in gold-standard annotations, it is very time-consuming.we think it would be interesting to look at self-supervised methods, weak supervision, and annotation generation.The need for labeled data also prevents easy replication of the pipeline to new data sets, which would also require manually annotating.\\u2022 more precisely on the extracted categories, some categories lack precision and would require additional processing steps to achieve satisfactory results.For example, the category person sometimes refers to the claimant or their family, but sometimes refers to the name of the judge.\",\"Considering that the golden labels of all instances have been given in the datasets, we directly use these labels as manual labels without performing the manual labeling process.The practice implicitly assumes that all the manual labels are correct.However, with the increase of labeling scale, problems such as inconsistent labeling granularity across annotators and noise in manual labels gradually emerge.How to effectively improve the labeling quality and the robustness of the clustering model are worthy of attention.\",\"Selecting a representative set of examples to label becomes essential when working with limited labeled data.In this work, we use uniform sampling for our results, which might not be the best approach.while we evaluate our model on a multi-label dataset and show improvement over standard baselines, the effectiveness of our approach on such problems needs more investigation.\",\"Our paper has the following limitations: in realworld applications, the label hierarchy may be more than two levels.It is worth extending our method to such a setting and empirically verifying it.Our selection strategy simply takes top r% confident samples, which might result in class imbalance problem.Alleviating the imbalance problem may further improve our performance.\",\".2) we do not try more means to prevent knowledge from forgetting.We can probe into the intrinsic structure of unlabeled data in a more fine-grained way by improving the posterior estimation.3, we have verified that both exploration and utilization are indispensable, but at the same time, we only empirically choose the specific proportion of both, without theoretical analysis of the most appropriate proportion for each dataset.We look forward to making progress in the follow-up research on the above limitations.\",\".Our ugdre can reduce the false positive pseudo label by estimating the uncertainty of the model prediction.However, it is difficult to reduce the false negative pseudo labels by uncertainty estimation.Our framework also relies on human-annotated data to train the pre-denoising model, which causes the sensitivity of our framework to the quality of human-annotated data.Thus, the improvements of models that continue training on the docred dataset are not as well as on the re-docred dataset.Moreover, iterative training introduces additional computing overhead, which makes the training process time-consuming.\",\"It\\u2019s important to note that, in this paper, we focus on bias resulting from underlying distribution of training data.Bias that may result from pretraining of transformer models is not within the scope of this paper.Although we conduct a case study of finegrained hatespeech detection task, a collective effort from the research community is required to better quantify bias mitigation of our approach across multiple tasks and different types of bias.Another limitation of our work is that our proposed algorithm requires dynamic adjustment of clusters.For very large datasets, this may be computationally expensive.\",\".First, we update the cluster centroids at each step during training, which requires a large mini-batch to maintain clustering accuracy and consumes more gpu memory.Second, our method still may not identify false negatives accurately, as we use the training model for coarse-grained clustering rather than a well-trained model.We leave the improvement of memory consumption and further improving false negative discrimination for the future.\",\"Our implementation of proxy models applies those models after the whole data is generated.Due to this, in the resulting dataset, the number of instances can often be unbalanced between labels.Such a limitation might be addressable by training proxy models from intermediate datasets with a smaller number of instances, and using those models while generating the rest of the dataset.As the data become unbalanced during the generation, the generation pipeline can try to generate more instances with labels that are a minority in the intermediate dataset.However, when we piloted this approach, we identified potential problems.First, intermediately trained proxy models could perform worse than those trained after all data are generated, due to the lower diversity in intermediate data used to train proxy models.Second, if many data points generated with a specific label actually belong to another label , there can be cases where most instances of label b come from the prompt with label a.It can skew the linguistic patterns of instances within the dataset, as only a small number of texts for label b might have been from the prompt with label b.our implementation of efficient oosf was not effective in increasing model accuracy.It might be due to the negative impact of removing instances, such as filtering instances on the decision boundary.Applying oosf to the entire generated dataset and seeing the impact of their removal would be the first step.With a comprehensible understanding of oosf, we would be able to design better oosf strategies, such as filtering instances with various criteria.In this work, we only examined the text-davinci-002 model of gpt-3.we also examined only one prompt , while there may be other options.Combining human interventions with automatic annotation error detection can be another future direction.\",\"Our proposed approach is based on the premise that faithfulness errors observed in generation systems are due to noise in the dataset.While there is substantial evidence for this from prior work, and our method outperforms existing approaches on the datasets we used, it\\u2019s possible the the utility of our approach could drop in cases where we have clean, curated datasets.It\\u2019s possible that certain generation errors made by the model could be due to spurious patterns learned by the model that do not generalize well.In such cases, it\\u2019s unclear whether using our error attribution approach to remove training instances would alleviate the problem.However, as most large-scale datasets in natural language generation tend to be sourced from the internet, it\\u2019s inevitable that these datasets will likely contain at least a few erroneous examples that could lead to undesirable model generations.Therefore, we believe that our approach to using error attribution to clean datasets is still a valuable method to improve generation systems.\",\"The contribution of this paper is mainly theoretical.Like most of the pos identification algorithms, the optimization of a criterion among the space of all partitions requires the use of heuristics, and finding the optimum is never guaranteed.Additional work is required before a generalization model that is efficient in practice can be obtained.\",\"Sample separation based on model predictions can only eliminate part of the noise, and it costs extra time in training.Moreover, although our dynamic gce loss based on prediction entropy works well in distantly-supervised ner, eq.4 is determined mainly because it has superior experiment results and it lacks theoretical proof.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"22_oosf_instances_proxy\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"22_oosf_instances_proxy\"],\"textfont\":{\"size\":12},\"x\":[11.362147331237793,11.260026931762695,11.349010467529297,11.237588882446289,11.392753601074219,11.405047416687012,11.6300630569458,11.605416297912598,11.477330207824707,11.402349472045898,11.565646171569824,11.424688339233398,11.712180137634277,11.448018074035645],\"y\":[3.590418815612793,3.476921558380127,3.4953534603118896,3.5303194522857666,3.56766414642334,3.6374874114990234,3.605015277862549,3.6829304695129395,3.509000778198242,3.62233304977417,3.902014970779419,3.623914957046509,3.696322441101074,3.610746145248413],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Semantic underspecification has been extensively studied in semantics, pragmatics, psycholinguistics, communication sciences, and cognitive sciences.In this position paper, we review this literature only superficially, although we are aware that a generalized and exhaustive understanding of the phenomenon necessarily requires knowledge of this previous work.We encourage the scholars working on this topic to embrace its complexity and depth.The paper focuses on approaches, tasks, and models within multimodal nlp.As such, it almost completely neglects a\",\"Cross-cultural inference beyond codenames our work explores sociocultural pragmatic inference in a very limited setting, using a core vocabulary of just 100 words.Despite this limitation, we find significant diversity in our dataset; furthermore, our models successfully capture these diverse inferences.While a limitation of our work is its focus on a single setting, we expect domains outside of codenames to see similar variance.Understanding and highlighting miscommunication in dialog\\u2014due to culture-dependent misinterpretation\\u2014is one such extension.spurious correlations across sociocultural factors across all tasks but one , jointly modeling all sociocultural priors does not result in the highest performing model.Because our sociocultural factors already correlate with each other , we suspect that modeling all features may be redundant, adding spurious correlations and resulting in overfitting.Bigger models and task specific modeling currently, we evaluate small seq2seq models due to computational constraints; however, evaluation of 0-shot and few-shot performance on larger language models is necessary.Given the changing state of the codenames board\\u2014along with evidence that llms struggle with theory-ofmind-esque perspective taking \\u2014 our dataset can serve as a challenging benchmark for sociocultural understanding.However, successfully encoding game state into prompts for llms may require experimentation.Finally, our current task formulation and modeling setup are straightforward: we simply encode all information in-context and do not assume recursive reasoning like in rsa.human evaluations our evaluation is limited to automatic metrics and qualitative analysis.Evaluating cross cultural generation depends on the evaluator\\u2019s own culture.Each generation depends on the player\\u2019s sociocultural background; finding evaluators who match the player may be prohibitive.\",\".This paper takes an entirely different approach by zeroing in on a particular task, which has been augmented with a specific semantic evaluation , to highlight how difficult tasks, such as figurative language interpretation, benefit not only from model size but from specific embodied semantics.Figurative language is difficult for lms because its interpretation is often not conveyed directly by the conventional meaning of its words.Human nlu is embodied and grounded by physical interaction with the environment.Consequently, it could be expected that lms struggle when the interpretation of figurative language depends on a more embodied action.Yet, the opposite has been shown as more embodied concepts are more lexicalised and larger lms can interpret them better in figurative language.Hence, our study provides valuable insight that raises the question of whether this effect is limited to figurative language or translates to other nlu tasks for lms.\",\"The term culture has many meanings, and before attempting to incorporate commonsense with culture, one needs to establish a well defined definition and boundary along which test cases and examples would be constructed.By focusing exclusively on food and culinary customs, we have greatly restricted our domain of inquiry.However, culinary topics are universal, and span multiple domains of common sense reasoning.Incomplete representation of all cultures: there are limitations with using countries as a proxy for culture.As noted in \\u00a7 2, mappings between cultures and countries are many-to-many, not one-to-one.The majority of questions in our test set fork focus on culinary cultures and customs of only a few countries, and we do not expect the results to generalize to all the countries of the world.We choose to focus only on one topic and a small number of countries so that we may initiate research on this broad, challenging problem with a narrower, more well-defined task.We selected these cultures based on the cultural backgrounds of the authors and authors\\u2019 colleagues who were available to provide direct feedback on\\u002fvalidate the data.We hope this work paves the way for follow-up work investigating a broader set of cultures.Small annotator pool: the validation study in \\u00a7 2.1 is done on a small pool of annotators from a few countries represented in fork.While the study gave useful feedback about the dataset and question quality, a larger and more diverse set of annotators would reflect a broader range of perspectives within each country, and further reduce the potential for biases or inaccuracies in our data.\",\"The proposed dataset is annotated for verb metaphors in particular.However, other lexical units including adjectives and adverbs should also be studied in order to truly understand the role of metaphors in news.It is important to examine the diversity of the generated ideas when performing metaphor generation.In this study, we proposed a simple approach to cluster words using wordnet.However, the metric is far from perfect and can be improved.For the task of candidate generation, we performed word masking to generate metaphorical and literal substitutes as we were curious about the ability of llms to generate relevant metaphorical mappings while preserving the underlying semantic idea.Direct substitution of metaphorical candidates resulted in syntactically incoherent sentences in a few cases.It may be better to paraphrase the sentence after selecting the metaphorical mapping.Ethical concerns and broader impact we created the dataset from a publicly available news headlines dataset.This ensures that data is free from anonymity concerns, obscenities and any stereotyping or bias.As the task is cognitively intensive, we only assigned at most 150 headlines to each annotator.All annotators were duly acknowledged and appreciated by nvidia ai technology center for their contribution.The original dataset of news headlines is the under apache license 2.we are thus permitted to modify and redistribute it.Generating metaphors carries concerns due to the implicit potential to craft misleading text.The usage of metaphors has been shown to resonate emotionally with readers.This should not be a concern with our data as we only release generated candidates that preserve the underlying semantic meaning of the source headline.\",\"The scope of this work is limited to sentence-level detection due to the absence of any span-level annotated datasets for hyperbole detection.Also, we could only partially annotate the metaphor datasets due to resource constraints.Finally, we did not try sophisticated large language models in our work as our goal was to demonstrate the effectiveness of multitasking using a simple model, rather than to test the performance of more sophisticated models.\",\"Because decoding symbolism is a challenging new problem, our approach and experimental results have some limitations.First, our work builds on available resources, which may have a bias toward an english\\u002feuro-centric perspective.Second, the evaluative datasets that we curated have a limited coverage of possible symbols even within the english literary tradition.1, our study on situated symbolism is limited to symbolic pairs that can be found in static visual advertisements rather than longer form text or videos.Finally, while we have proposed one debiasing method based on re-ranking with pmi, which worked well for our experimental setting, there may be other methods and metrics more suited to different settings.We believe that despite these limitations, our proposed evaluative framework and methodology offers a good starting point for further exploration.\",\"This paper mainly focuses on modelling basic meaning to identify metaphors, typically learning basic meanings from literal annotations of the vua dataset.However, our analysis reveals that the literal annotations of the vua dataset are incomplete, which means that some words in vua have no literal instances annotated.Although we propose using contextual word embeddings as a backup in this paper, another promising solution for this issue might be using external resources such as dictionaries.Leveraging dictionaries is commonly used to assist manual metaphor detection, so it could also help our basicmip mechanism to generalise.\",\"While introducing a framework which deals with multiple languages and multiple figures of speech, this work is still only dealing with three figures of speech and seven languages.Many more phenomena and languages can still bring substantial challenges and insights if considered.Also, we deal with figurative language as labelled at the sentence level, but the word level is also not only interesting but important for broader natural language understanding and could yield different insights than those observed in the present work.We only mention in passing the influence that different cultural contexts have on figurative usages, and we make some observations on idioms, but this aspect would require a much bigger unpacking.We actually believe that of cross-lingual computational models can be an excellent diagnostic tool towards a finer-grained analysis of the interplay between culture and figurative language.We propose a successful method based on prompt learning and present experiments using a specific pre-trained model.Choosing different models and investigating even more than what we already do in this paper the influence of specific prompts would also be necessary to further generalise the efficacy of our approach.Finally, as with most language technology, the limitations of our approach, also in terms of accuracy , could lead to substantial inaccuracies which could be propagated in further processing.Considering that figures of speech are associated with emotional language, a word of warning is necessary regarding the direct deployment of our models.We do hope that writing about risks explicitly and also raising awareness of this possibility in the general public are ways to contain the effects of potential harmful consquences.We are open to any discussion and suggestions to minimise such risks.\",\"We have studied the task of socio-cultural norm discovery based ldc2022e20 dataset, which consists of everyday situational interactions in mandarin chinese.Although we believe that our approach can used in other cultural settings, the current state of the model might not be generalisable to other cultures, unless further tuning is possible.Our model\\u2019s ability in discovering such norms can help to improve conversational agents, however, real-world scenarios involving duplicitous or ambiguous terms might confuse our proposed approach.In addition, our model is limited to the textual modality, and we believe incorporating audio and visual features into the model can improve identifying socio-cultural norms.Nonetheless, the reliance of our model on large-scale pre-trained language models might result in some deployment challenges in situations with limited resources.Besides, all the reported results are by fixing a random seed running all experiments once.\",\".We plan to further examine the effect of prompt phrasing on the quality of the generated visual metaphors,and how that effect differs across different models.Limitations while the results of human-ai collaboration for visual metaphor generation are very promising, such a procedure might be time-consuming but at the same time necessary for maintaining quality.We want to acknowledge that both our llm and bestforming diffusion models are released through a paid api and are not open-sourced.While our best-performing system uses chain of thought prompting, there are several other prompting or task decomposition techniques that we did not perform an extensive comparison with.last but not least, there is still enough room for potential improvement in generating visual metaphors which can be achieved by designing better prompts or by improving the compositional generalization of diffusion models.\",\"Our scheduling strategy only re-arranges the training examples after each training epoch, limiting the flexibility of scheduling them compared with re-arranging the examples after each training step.Therefore, the order of the training examples will still be fixed within each training epoch.Besides, our method finds it challenging to transfer from the task of idiom usage recognition to that of metaphor detection.Therefore, more advanced methods for learning the broad nature of non-compositionality, including those of idioms and those of metaphors are needed.We leave this to a future study.\",\"First, despite our pursuit of attempting to understand figurative language use across cultures, we have barely scratched the surface in terms of diverse representation.Due to limited scope, budget, and resources, we collect data from 2-3 annotators per language, for seven languages.Further, culture can vary greatly within a language.Therefore, until we can represent all of the worlds\\u2019 people and their languages, there will always be room for improvement.We also acknowledge that the syntax captured in the dataset may not be the most diverse, as many examples follow the template \\u201c\\u003cx\\u003e is like \\u003cy\\u003e\\u201d.second, to analyse concept shift, we machine translate test sets into english.However, these translations can be erroneous to varying degrees, which may have resulted in an over-estimation of error attribution to concept shift.This could not be avoided however, due to limited resources of obtaining human translations.Third, english may not be the best language to transfer from in zero-shot evaluation of multilingual models.While we were constrained by training data availability, past works have shown that machine-translating train sets can help, an avenue we haven\\u2019t explored here.Even though we experiment with few-shot evaluation, there may exist an optimal combination of source languages which best transfer to our target languages.Fourth, the english authors recognized culturespecific terms that were not marked as cultural by annotators in the commonsense categorization across all languages.This may be because annotators, being mostly familiar with their own cultures, attributed culturally specific facts and terms as being common sense.Likewise, the englishspeaking participants may have viewed a separate set of facts as common sense which would not be agreed upon by people from a different culture.It is thus difficult to disentangle common sense and culture in many cases.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"23_figurative_metaphors_sociocultural\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"23_figurative_metaphors_sociocultural\"],\"textfont\":{\"size\":12},\"x\":[13.838948249816895,13.856439590454102,13.862044334411621,13.945123672485352,13.840290069580078,13.817581176757812,13.882002830505371,13.848099708557129,13.867269515991211,8.948078155517578,13.841913223266602,13.786362648010254,13.892631530761719,13.478984832763672],\"y\":[0.344791978597641,0.5656644701957703,0.344222754240036,0.579581081867218,0.2997490465641022,0.3303079903125763,0.3527841866016388,0.33489519357681274,0.3695196211338043,0.5851845741271973,0.3059886395931244,0.2974700927734375,0.4436620771884918,0.3964478373527527],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Our work has two limitations that may impact the generalization ability of our proposed framework.while this approach has demonstrated superior performance on the multimodal misinformation detection and sarcasm detection tasks, it may harm the generalization of our framework to more complex multimodal misinformation tasks, such as the detection of fake news that involves various modalities, including social networks, text, user responses, images and videos, as discussed in.Secondly, in our work, the incorporation of logic into the neural network relies on the use of product t-norm to differentiate logic operators.However, as shown in the ablation study , product t-norm may lead to vanishing gradients with the increase of logic atoms during the training stage, which may limit the ability of our proposed framework to handle more sophisticated scenarios.We plan to address these limitations in future research.\",\".We release any data , code, and models produced during this study publicly for further research by the community.We license this release under cc-by-sa 4.in the near future, we aim to annotate this data for tasks such as sarcasm detection - to develop a deeper understanding of how it is related to aggression and offensiveness.Additionally, the motivation for collecting the same data instances marked with aggression and offense labels is for a multitask learning-based model also to be able to identify when 1) the tone of a text is aggressive without being offensive vs., 2) the text is offensive, despite it not being overtly aggressive.We also aim to collect more data and annotate it using weak supervision.Finally, we also aim to expand on the theoretical underpinnings of sublime aggression and offense by attempting to identify these within other more tangential domains, viz.\",\"In this paper, we propose a novel sd method, entitled sd-aprr, which expresses negative situations of sarcasm by the potential results and human reactions of the associated events.We employ the comet to estimate the results and human reactions, and form event-augmented samples with them.We employ those augmented samples as the whole sarcastic texts from the direct access view.We suggest a masked graph-based encoder, enabling to generate discriminative sample embeddings.Experimental results demonstrate that our sd-aprr can achieve competitive performance compared with the existing baseline methods.We demonstrate two limitations: the datasets used in this work are mostly collected from social media.our exploration of sarcasm theories still has some space to improve.\",\".The design of the co-attention in our method can be improved.Currently the design of co-attention in our method is limited to four types, which affects its adaptability.In addition, due to the fact that there is only one publicly available dataset in multimodal sarcasm detection, we conduct our experiments based on it.This has limited the evaluation of the generalization of our method.\",\"We attempted to develop a novel framework for explainable complaint identification in a multitask setting.But the proposed approach is having some limitations as enumerated below: the proposed methodology has been validated on an english language complaint dataset; further training would be required to scale up to codemixed language datasets which are prevalent in multilingual countries.Users often post some images along with text while writing complaints.The current system is unable to handle such multi-modal forms of inputs.In some cases, users use an implicit sarcastic tone while writing complaints.In the current setup, sarcasm detection is not considered as a separate task.Thus the proposed system will not be capable of detecting complaints with implicit sarcasm.\",\"The present study comes with two major limitations.First, humor is highly subjective, and a valid and reliable evaluation is hard.Things can be perceived as funny for very different reasons - even for being particularly not funny, such as antijokes.Thus, when chatgpt generates an odd joke about ml, one could even argue that chatgpt has a sense of humor that is just different from ours.the present investigation focuses on one specific form of humor, namely standalone jokes.There are more manifestations to consider, which would require a much more complex experimental setup.Second, we cannot confidently trace back the outcome of the system or map it to specific input data.This is challenging for large data-driven models in general, but especially in this case, where we neither have access to the model itself nor to any training data or to the exemplary samples from rlhf.This prompt-based investigation creates a good intuition for the opportunities and limitations of chatgpt.\",\"Our work mainly suffers from two key limitations.1) ignore that the text embedded in the image could also reflect the sarcastic intention.As mentioned previously, we found that our model performs better on non-ocr samples than the ocr samples.This may be due to the fact that our model ignores the text embedded in the image.Nevertheless, such embedded text could also indicate the ironic intention, ).We believe recognizing the text of the image can boost the performance of existing multimodal sarcasm explanation models.2) ignore that different knowledge concepts may contribute differently to the sarcasm reasoning.As shown in figure 3 , the related concepts \\u201cdisgusting\\u201d and \\u201cpleasant\\u201d should contribute more than the concept \\u201cnight\\u201d in the sarcasm reasoning.Currently, our model equally treats all the knowledge concepts.\",\"This work contributes a debias benchmark mmsd2.0 for building reliable multi-modal sarcasm detection system.0 is built on the available mmsd benchmark.\",\"The new yorker cartoon caption contest represents a narrow slice of humor, deriving from a particular language, region, history, culture, style, and set of conventions.Hence, the results of this study do not represent or cover all types of humor.Our framing of the quality ranking task could be interpreted as seemingly prescriptive , but new yorker editorial selections should not be taken as ground truth for funniness; disagreement about what is funny is expected and valid.Our tasks operationalize the prediction of only average preferences , and these preferences may include a partiality or bias towards items that conform to the characteristics of prior contest winners or published new yorker cartoons.Finally, the explanations in our annotated corpus were largely written by a single author of this paper.\",\"To better understand the limitations of our proposed mtmsg, we also perform a qualitative error analysis of the incorrectly generated samples.We randomly select 100 incorrectly generated descriptions and find that our model might incorrectly generate those samples mainly due to the misunderstanding of the necessary intent information from the images and ocr tokens.The statistical results reveal that 37% of the incorrectly generated descriptions are caused because the main part of the sarcasm might lie in the images ), while the other 63% error cases are attributed to the failure of our model in capturing the intent information directly from the ocr tokens ).Specifically, in figure 5 , if we want to generate better descriptions, we need to capture the fine-grained visual attribute feature happy from the image; in figure 5 , we need to understand the intent information from the ocr tokens that ban guns can make us feel safe when we have lunch in the restaurant.besides, we will explore a language interpreter to further understand the key information contained in the ocr tokens.\",\": the datasets used in this work are mostly collected from social media.our exploration of sarcasm theories still has some space to improve.\",\"While this work represents the first effort towards a perspectivist language resource for irony detection, it has to be noticed that the resource is monolingual.Moreover, while we tried to maintain a fair balance in terms of demographic profile of the annotators, we limited the resource to five varieties of english tied to five countries, while leaving out other potential locations or even more nuanced distinctions among language varieties.About the self-identified gender dimension, we are aware of the wider spectrum of genders.However, this information is provided by the annotators only in a binary form.Another potential limitation is that, in the spirit of constructing a perspectivist corpus, we fully trusted the contributors.While the chosen crowdsourcing platform is known for a high quality standard obtained e.by vetting its contributors, and we added a layer of checks through attention test questions, random noise in the annotation may still be present and undetected.While this paper mainly presents a new language resource, we also included the results of several analyses and validation experiments.In this direction, a number of dimensions are still unexplored, along which the data could be analysed.For instance, the genre difference between the sources of the data and the distribution of different varieties of english were not yet explored.\",\"The primary limitation for this work is the difficulty of telling the difference between mistakes made by automatic systems and wrongly assigned importances from the attribution techniques.Additionally, our work currently only relies on a single pre-trained model that was fine-tuned on the currently only available data set for dutch irony detection.The patterns in computational modeling we described only apply to this particular system and data set and may very well differ when training a different model on data that was collected in a different way, where the data set may rely on different patterns and biases.Finally, despite the good agreement scores for binary irony classification, this remains a complex task where annotators can be uncertain about the label.In the end, the annotators for any irony or sarcasm detection task can only make assumptions about what the author of a text intended to convey.For our setup, both the annotators and the automated system predict whether a text is ironic without considering the corresponding context.In a realistic setting, most social media texts are reactions to previous comments or external events that can be essential in order to recognize the irony.This means that the model predictions can differ from the annotated label but still be a plausible interpretation.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"24_sarcasm_irony_humor\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"24_sarcasm_irony_humor\"],\"textfont\":{\"size\":12},\"x\":[8.113555908203125,8.157925605773926,8.110454559326172,8.112333297729492,8.113018035888672,8.128010749816895,8.103848457336426,8.108304977416992,8.1580228805542,8.11073112487793,8.108201026916504,8.0907621383667,8.104948997497559,8.116931915283203],\"y\":[3.579296588897705,3.5839412212371826,3.5759358406066895,3.5741798877716064,3.576519250869751,3.53039813041687,3.5796515941619873,3.5755908489227295,3.5065696239471436,3.574159622192383,3.5776219367980957,3.5970606803894043,3.5795557498931885,3.5700368881225586],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"In this work, we adopt a series of strategies for optimizing the generation models when corpus scaling up.Although we successfully train tome on largescale corpora, there is still a performance gap compared to mainstream dense retrieval methods under this scenario.This is also one of the limitations of current model-based retrieval methods, because this retrieval paradigm requires the model to memorize the entire corpus, unlike dense retrievers that have strong generalization capability for different documents in a large corpus.In addition, effective training on large-scale corpus also requires largescale computing resources and long training time, which will indirectly generate risks of energy consumption and emissions.\",\"In this paper, we propose a novel concept of dense retrieval, the matching representation.Based on this, we introduce a novel generalizable dense retrieval training method via training the balanced and extractable representation for matching.Despite the strong performance of our method in improving the generalization ability of dense retrieval models, more theoretical proof needs to be researched to gain the deeper understanding of generalization improvement.we believe that the deeper study of matching representation will promote the development of dense retrieval, because it not only alleviates the problem that query and passage cannot interact in depth during training, but also describes the essence of retrieval task.\",\".First, our in-domain evaluation experiments focus on passage retrieval for odqa.While the dense retriever is mostly successful in odqa, it can be also used in other types of retrieval tasks which may have different input and output format.For example, the kilt benchmark provides several knowledge-intensive tasks other than odqa.The performance of taser models trained on such retrieval tasks remain unknown.Second, compared with traditional sparse vector models like tf-idf and bm25, the cost of training is an inherent issue of dense retrievers.Although taser significantly reduce the number of model parameters, the training cost is still high.Third, in our experiments, we show that the learned routing does not outperform the deterministic routing.This may suggest a better architecture and\\u002for training algorithms for learned routing is needed to fully unleash the power of moe.2, there is still a gap between taser and bm25 in out-of-domain evaluation.\",\".2 and shown in table 3, the coherence of the rewrite generated by edircs is not as good as that generated by purely autoregressive rewriters.This may affect the performance of edircs when using dense retrievers.Possible solutions include using an additional token reordering model to improve the rewrite coherence or injecting the coherence signals or token positions information into the learning of edircs in an end-to-end way.Another concern is that the effect of our text editing-based model may be limited for a few long-tail cases where many expected rewrite tokens are not in the input session.How to better deal with the search dialogues whose search intents are too implicit or vague to be accurately expressed by inferring from the dialogue context alone is a valuable direction for further improvements of our model.\",\"Although recontriever narrows the gap between bm25 and unsupervised dense retrievers, it still lags behind bm25 when acting as a generalpurpose retriever.This issue may make recontriever not directly usable when facing a new domain, thus limiting its practicality.Also, as recontriever is initialized from the language model bertbase, there may exist social biases in recontriever and thus have the risk of offending people from under-represented groups.\",\"We point to several limitations of our work.First, our work considers a popular family of models referred to as \\u201cdense retrievers\\u201d, but other approaches for retrieval include sparse retrievers , generative retrievers , late-interaction models , inter alia.While our work draws interesting connections between dense and sparse retrieval, our main focus is on understanding and improving dense models.Second, all three dense models we analyze are bidirectional and were trained in a contrastive fashion.While most dense retrievers indeed satisfy these properties, there are works that suggested other approaches, both in terms of other architectures and other training frameworks.Last, while our work introduces new ways to interpret and analyze dense retrieval models, we believe our work is the tip of the iceberg, and there is still much work to be done in order to gain a full understanding of these models.\",\"We identify two sources of limitations in our work: the range of metrics we consider, and the range of models we explore in our experiments.In the interest of concision, we focused on cost and latency as well as system quality.These choices reflect a particular set of values when it comes to developing retrieval models.3, we briefly consider a wider range of metrics and highlight some of the values they encode.Even this list is not exhaustive, however.In general, we hope that our work leads to more discussion of the values that should be captured in the leaderboards in this space, and so we do not intend our choices to limit exploration here.For our post-hoc leaderboard , we surveyed the literature to find representative systems.We cannot claim that we have exhaustively listed all systems, and any omissions should count as limitations of our work.In particular, we note that we did not consider any re-ranking models, which would consume the top-k results from any of the retrievers we test and produce a re-arranged list.Such models would only add weight to our argument of diverse cost-quality tradeoffs, as re-ranking systems must determine which retriever to re-rank, how many passages to re-rank per query , and what hardware to use for re-ranking models, which are typically especially accelerator-intensive.For our experimental comparisons, we chose four models that we take to be representative of broad approaches in this area.However, different choices from within the space of all possibilities might have led to different conclusions.In addition, our experimental protocols may interact with our model choices in important ways.For example, the literature on splade suggests that it may be able to fit its index on machines with 8 or 16 gb of ram, but our experiments used 32 gb of ram.Our hope is merely that our results help encourage the development of leaderboards that offer numerous, fine-grained comparisons from many members of the scientific community, and that these leaderboards come to reflect different values for scoring and ranking such systems as well.\",\"Although our tart-full model shows the effectiveness of instruction-tuning for retrieval, on some datasets tart-dual shows large performance degradation from its non-instruction-following counterpart.We hypothesize that a smaller model size and limited interactions between query and document embeddings are the main factors.We conduct primarily experiments training larger dual-encoder models such as sgpt on berri but still observe some notable performance drop on some datasets, which indicate only scaling up encoders may not significantly improve instructionfollowing retrieval systems.retrieval tasks are excluded in prior work on instruction-following of llms.This work is the first to explore instruction tuning in the area of retrieval, and we annotate more than 100 instructions for approximately 40 tasks, and we demonstrate the effectiveness of the dataset scale in retrieval.Yet, recent work show that scaling up the number of the training datasets improves llms\\u2019 ability to adapt to new task via instructions, and the current dataset scale might not be optimal.We open-source our instruction data and call for community efforts to collect more retrieval tasks and human-written instructions as in instruction-following for lms , to investigate whether further increasing the number of the datasets lead to improvements.\",\"Limitation the main limitation of this work is the technical novelty of hybrid retriever.Hyrbid-drboost is built on top of drboost, and the interpolation of bm25 with drboost.However, we would like to point out that our study can serve as an important finding for real-life applications.Previous retrievers are built on top of indexing-heavy dense retrievers, such as dpr.This limits their applications where memory is a hard constraints, for example, on-devices.Our study suggests that a light hybrid retriever can save memory but maintain sufficient performance.\",\"While our approach effectively mitigates query latency through a cascade ranking paradigm, it necessitates additional computational resources during training due to the need for attention score calculation and alignment in the optimization process.Additionally, our model incorporates passage-level relevance scores into the ranker, generating a cooperative matching representation during document ranking, which could marginally augment the inference time.In our future endeavors, we aim to explore more efficient methodologies that can further improve ranking efficiency.Furthermore, it is worth noting that our approach has been tested using specific backbone models.To fully evaluate the effectiveness of our method, it is essential to conduct experiments with a diverse range of backbone models, which remains an avenue for further exploration.\",\"In this study, we primarily focus on the examination and experimentation of the dsi-qg model, with plans to expand our research to include more recent models that utilize differentiable search indexing, such as the nci model.While our approach has demonstrated effective improvements in dsi retrieval outcomes, and both tct-colbert and our proposed dsi-qg-multi performed well in our empirical analysis concerning relevance ordering, we cannot dismiss the possibility that these favorable results may be attributed to the extraction of insights from a specific bert reranker model that shares similarities or correlations with the one used to define the desired ranking.Despite showing improvement over dsi-qg, our model remains slightly less effective than stateof-the-art dense retrieval methods such as tctcolbert v2.Our approach offers advantages over dense retrieval models such as reduced storage and maintenance overhead, as dsi models do not require additional index structures for online use.Though index structures are utilized during the training phase of dsi-qg-multi, the generated index structures are temporary in nature.Furthermore, due to the limitation of model memory, current research on dsi only experiments on a subset of the entire ms marco dataset or small dataset such as nq.Therefore, an important future direction is to develop more efficient architectures to deal with the issue of memory bottleneck, for example, by using the current popular large language models or constructing aggregation structures for storing all information in hierarchical pieces.\",\".However, it is unclear how it may be adapted for the single-representation dense retrieval prf model.In addition, in this work, we did not test the effect of the hard negative sampling and the number of negative samples for cwprf.Finally, while we have focused on passage retrieval, longer document retrieval can be addressed through splitting documents into passages during indexing, retrieval and prf, and applying a max-passage aggregation to obtain a document ranking.while prf approaches typically increase query response time, they can also be used as teacher approaches to realise more effective and efficient student models ).This means that improved prf approaches, such as cwprf, can also have downstream benefits to other retrieval approaches.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"25_dense_retrieval_retrievers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"25_dense_retrieval_retrievers\"],\"textfont\":{\"size\":12},\"x\":[12.006119728088379,11.63624095916748,11.659534454345703,11.696953773498535,11.608386039733887,11.627903938293457,11.660623550415039,11.639924049377441,11.66489028930664,11.602282524108887,11.552093505859375,11.608053207397461,11.663582801818848],\"y\":[2.652681350708008,2.7298786640167236,2.703474283218384,2.702542781829834,2.7428863048553467,2.730558395385742,2.703566074371338,2.715564489364624,2.763549566268921,2.702545166015625,2.684589385986328,2.674659252166748,2.7088747024536133],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\".Our investigation in the zero-shot experiment is not exhaustive, we focused on the interplay between the three main tasks that also provide datasets of similar size: argument identification, evidence detection, and argument quality.However, there are other tasks, such as stance classification or argument structure identification.Other tasks might be better source tasks for estimating argument quality.our experiments are based on the most popular datasets in argument mining and argument quality and may not generalize to other more specialized text domains, such as law or politics.Using only english datasets limits the generalizability of the results to other languages and cultures.The ability to identify and evaluate the quality of arguments may be different in other languages and cultures, and the annotators may not be able to accurately capture these differences.This may lead to a lack of robustness and reliability of the results.\",\"The limitations of the paper are twofold.First, we need to train the annotators to be familiar with another annotation paradigm: creating counterfactual samples for the labeled factual samples.It is an additional cost for active learning although our user study has shown that annotating counterfactual samples has similar costs to labeling factual samples.Second, we require the annotators to manually find and edit the causal features, which is not effective enough.It can be improved by developing tools like generative models to automatically edit features for annotator judgment.\",\"The limitations of our work can be summarized in three points.therefore, selectively applying each method depending on which traits are to score might further improve the model.Second, although the use of pre-engineered features, such as our topic-coherence feature, has the advantage of interpretability , it requires additional engineering steps, as in other aes studies using hand-crafted features.Finally, despite the large improvements observed on the specific datasets asap and asasp++, the model has not experimented on other datasets.Feedback prize dataset8 is well-designed for scoring english-written argumentative writings with multiple trait labels, but the prompts are not defined; thus, it does not fit for cross-prompt aes.Essay-br dataset contains essays on multiple prompts with labeled multiple trait scores.\",\"The dataset we collected annotates questionable assumptions based on the current facts at the time of annotation.for instance, the question what episode does aidan appear in just like that contains an assumption that he appeared in some episode of the show.As of january 2023, this is invalid because the season that he has been confirmed to appear in has not aired yet, meaning that no episode of the show is public.But this assumption will become valid when the show airs.Furthermore, annotation errors may arise due to the intrinsically challenging nature of verifying that something did not happen or does not exist.While the annotators tried to provide convincing evidence and used their best judgment to decide on the unverifiability of the assumptions, they could not check every relevant document available online.In theory, this would be necessary for perfect verification of nonexistence, unless there is an explicit statement of nonexistence.However, such statements are inherently rare due to reporting bias.The annotators often used pragmatic inference, taking omission as supporting evidence that the event did not happen or the entity does not exist.This is nonetheless heuristic and may be a source of annotation errors.\",\"There are two limitations of this work.The used corpus of argumentative microtexts contains only fully argumentative texts of moderate complexity.Real-world argument texts do not always consist only of argumentative statements.However, the method could potentially be used on other argumentation annotation corpora as well; one of the main reasons for choosing the corpus was to have a parallel full version in a second language.Another reason is the ability to match the edu and adu segmentations directly.Although the amount of training data is artificially doubled, it may not be enough to train models on the proportionally increased noise.\",\"The data set presented is still quite small for machine-learning models, as is the number of annotators.Since the annotation required a lot of human effort, we chose fewer, but experienced, student assistants as annotators to ensure a high quality of the annotations.The agreement for effectiveness and argumentative function is low.To address this weakness we used the following strategies: a) an examination of the confusion matrices reveals that the annotation scheme is not exclusive, that is, a story can take on multiple argumentative functions.We therefore include different, aggregated versions of our dataset that include this annotation layer as a multi-label layer.B) we address the subjectivity of the two annotation layers in a regression analysis.The interactions between each annotator and certain annotated properties show annotator-specific differences, which should also not be ignored in the modeling.A crowd-sourcing study could build on the initial findings and collect more annotations for effectiveness to investigate perspectivism in this context.Finally, we lacked sufficient space to analyze the existing annotations of the sub-corpora of our resource and discuss them with our new annotations.\",\"We discuss some limitations of this work for future research efforts.The range of the domains could be more comprehensive to cover social media and law.The experiments can potentially cover more models.8, there are more comparable retrievers and qa readers.finally, due to the complexity of the raw ir data, it is costly to collect our datasets.This is manifested by not only the monetary costs, but also the human efforts to create guidelines, to coach annotators, and to manually audit and validate annotations.\",\", our results indicate that, in general, revision-based data can be employed effectively for the given tasks, contributing towards solutions for each of the considered challenges.Specifically, our suggested sampling strategy revealed that training on claim versions with a higher revision distance between them improves the performance when detecting claims in need of improvement.Moreover, we found that the impact of the available types of contextual information is not only task-dependent but also depends on the quality issue that a claim suffers from.We argue that the developed approaches can help assist automated argument analysis and guide writers in improving their argumentative texts.With our work, we seek to encourage further research on improving writing support not only in debate communities but in educational settings as well.\",\"We find several limitations in this work.nevertheless, we believe that there is a gap in the literature for the task presented in this work, hence our introduction of the environmental claim detection task, the dataset, and models.Second, we collect data from sustainability reports, earning calls, and annual reports.However, this does not cover the universe of text where environmental claims are made, e.also, environmental claims can be made about environmental improvements on a wide range of topics such as carbon emissions, water pollution, and recycling, among others.We discussed creating different datasets, where each dataset is dedicated to one specific is- 9task force on climate-related financial disclosures sue.third, sometimes it is necessary to have access to more context to determine whether a sentence is an environmental claim.We discussed whether it would be beneficial to annotate whole paragraphs instead.However, the trade-off would be exploding annotation work and costs, hence our decision to introduce environmental claims as a sentence-level classification task.Nevertheless, given a unlimited budget, we would have pursued annotating whole paragraphs instead., sustainability reports, are mostly published by european and us-listed companies, which is reflected in our dataset.We crawled these reports from the sec10, hence our dataset contains mostly claims made by big firms and firms from developed countries.It is conceivable that smaller firms and firms from nondeveloped countries make different environmental claims, and models trained on our dataset might not be suitable to detect these claims.Moreover, our work is subject to all concerns raised in the\",\"This resource paper describes the dataset in detail, providing strong baselines and first initial crossdomain experiments.It does not aim to provide an extensive set of experiments on cross-domain argumentative zoning yet.The agreement study was performed on complete documents and hence has only limited data for several labels.Due to the limited funding of the project, we could double-annotate the entire dataset.Finally, we only test one model class.A potential next step is to test a bigger variety of models and embeddings.Because az labels are interdependent within a document, especially document-level models or crfbased models are promising methods to try.We have also tested only one method to deal with the strong class imbalance in the dataset.We have not yet tested further such methods or data augmentation methods.\",\"The collection and verification of this work has required help from over 250 annotators.This makes the dataset difficult to replicate, as is the case with many dataset papers.We have selected annotators carefully, considering relevant experience and using techniques to determine annotator quality to minimise the subjective variance.We have tried to cover the arguments involved in debating by talking to experts and people from debate circuits across the world, with different experiences and expertise.However, due to the nature of this activity, it is possible that there are arguments and experiences have not been covered in the dataset.These could be experiences of marginalized communities, underrepresented debate circuits, etc.Moreover, some debate motions used are relevant to the time period in which the motion was the most prominent.Our dataset does not account for the changes that might have taken place pertinent to that issue after the generation of arguments.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"26_argumentative_environmental_firms\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"26_argumentative_environmental_firms\"],\"textfont\":{\"size\":12},\"x\":[9.616691589355469,9.770288467407227,9.607961654663086,9.806766510009766,9.593354225158691,9.802565574645996,9.552978515625,9.614975929260254,9.658817291259766,9.597123146057129,9.605162620544434,9.656970977783203],\"y\":[2.924877405166626,2.9552907943725586,2.8747947216033936,2.8799209594726562,2.942869186401367,2.8320255279541016,2.878858804702759,2.7878174781799316,2.7938015460968018,2.9724996089935303,2.995065212249756,2.8943474292755127],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Of group dro which have been overlooked before, and can be viewed as a cornerstone for future study in the worst-group generalization.Limitations although our unsupervised frameworkq-diversity shows great superiority, when it comes to limitations, we acknowledge that our empirical validations on real-world datasets just follow current benchmarks that shed light on the group shifts caused by spurious correlations.Although we conduct experiments on the scenarios with noisy labels and various ood datasets, practically, apart from superficial clues, a series of contributing factors that lead to group shifts are worth further exploration.\",\".The core of our work is built on conducting extensive human evaluations, to understand how well lay humans can solve tasks with rationales.In order to replicate these findings to other tasks, one would require the same scale of human evaluations, which are expensive and tedious.These tasks are also difficult to explain to lay crowdworkers, because of which several rounds of turking are required to reach good annotator agreements.Given these shortcomings of human evaluation, a reliable metric that estimates human utility is necessary.Generating generalization questions is not completely automated.Even though we prompt gpt-3 with varied demonstrations to generate generalization questions of each type, we still have to manually filter them to obtain a cleaner set of questions.Furthermore, in order to obtain gold answers of these questions, we generate answers by prompting gpt-3 again, which also requires further validation.A completely automated method of generating these questions would lead lm updates to be independent of human involvement.Even though gen-u has a better correlation with human utility, the correlation is still low.To train models to produce free-text rationales with more human-utility through quark , it is first necessary to have an accurate metric that can serve as a reward function\\u002fscoring metric for human utility.In this work, we found that human generalization is good indicator of human-utility.However, given that quark requires frequent reward scoring, it is infeasible to use human annotations for the same.Our proposed automatic metric gen-u that simulates human generalization has a good correlation with human utility , but overall, it still has a low correlation with human utility of rationales.Developing a score with better correlation with human utility will decrease the effect of this limitation and lead to training that further increases human utility of generated rationales.\",\"Our work is subject to certain limitations, one of which pertains to financial constraints that hindered the ability to conduct large-scale experimentation with the data annotation methods proposed.As a result, the findings of this study may not be fully representative of larger datasets or populations.Additionally, the utilization of gpt-3 as a model presents challenges in terms of interpretability, as it operates as a \\\"black box\\\" system.To further investigate this subject, it would be bene- ficial to conduct larger-scale experiments and to compare the performances of gpt-3, chatgpt9, and gpt-4 and the open-sourced llms like llama.\",\".Our study is targeted specifically at gpt-3 and it would be interesting to study feature bias patterns on other large language models such as opt and bloom ; and it is possible that our intervention methods may have different effects on these language models trained with different data sources and scales.apart from model coverage, our analysis is focused on only four common binary classification tasks.Our main metric, h-accuracy, compares the predictions between a learned function f and a feature function h.for simplicity, we only study binary functions to illustrate the main ideas, but the framework applies equally well if f and h are multi-class classifiers.For example, in the case of the threeway nli task, we might set h1 to predict on the basis of entailment \\u002f contradiction \\u002f neutral, and h2 to predict on the basis of the genres \\u2013 e.our current experiments are limited to a set of hand-crafted features.One potential way to systematically scale our approach is to develop novel techniques for automatic feature discovery, for example, to cluster the data and treat each cluster as having a distinct feature.while our empirical findings shed light on the feature bias patterns of gpt-3, we do not yet have a\",\"One limitation of our approach is that we aggregated ic and gc measurements over clusters during the automatic subgroup discovery process, but we did not fully consider the relationships between clusters.A more comprehensive strategy for utilizing beneficial relationships and a more precise approach to potential conflicts between clusters could lead to further improvements in overall performance.Additionally, our mnli experiments were conducted on large dataset that had multiple clusters with errors.We chose to focus on the top10 clusters with the most errors due to limitations in resources for running a user study.While tdg on top-k clusters has demonstrated effectiveness in improving performance, there is still the potential for further improvements by working on a larger number of clusters.At the same time, we emphasize that tdg should be used as the last step to improve performance in low-performing groups.If these groups are numerous, it means the model is likely under-trained, and other techniques should be applied first.\",\".Firstly, we considered only one family of response distributions.We chose normal distributions because their behavior is well-understood and they are easy to work with.However, the structural similarities between normal distributions and the best performing metrics\\u2014namely, absolute error\\u2014 suggests that, more generally, the best test metrics for nhst may vary depending on the underlying response distributions.Therefore, we recommend that use of our framework should potentially vary depending on the dataset being considered, and might have other distributions commonly found in model and gold standard items and responses, such as exponential or multinomial distributions.Similarly, we only considered p-value estimators that are based on bootstrap sampling.Implementation of our framework in future use would benefit from matching the estimator to the test metric.For instance, permutation tests are the most common way to estimate p-values for spearman correlation, and analytical tests such as student\\u2019s or macnemar\\u2019s, which are commonly used even when the underlying assumptions on which they are based are not likely to hold.As such, the sampling method could change based on which metric is best for the task\\u002fdata.\",\"Comparison with gpt-3: there are growing concerns in the research community about the lack of open availability of gpt-3.There are several versions of the model and the details of the training data used for each version are largely unavailable.Direct comparison with gpt-3 is, therefore, becoming increasingly challenging.In this work, we compare against the \\u2018text-davinci-001\\u2019 version of the gpt-3 model and note that newer models might do better.However, extracting the best performance from gpt-3 is beside the point of our work.We believe that as a community, we must investigate alternative approaches that do not only rely on scale.Undesirable generations: language models, large and small, have been shown to be prone to generating toxic text.I2d2 relies on gpt-2 xl could also potentially generate toxic statements.While the trained critic model is able to filter out most toxic generations, we estimate the proportion of undesirable generations using the delphi model.3% of the generations may not be morally acceptable, either because the statements are not accurate, not verifiable, too restrictive, or they are potentially toxic.Self-imitation iterations: in this work, we only try two iterations of self-imitation due to resource constraints.But, based on the performance improvements we observed after two iterations, we hypothesize that the improvements could diminish with each future iteration.Runtime efficiency a batch of 32 generations from i2d2 takes 3mins on a single rtx a6000 gpu.Neurologic decoding is the most computationally expensive component.As constrained decoding methods become more efficient, the runtime of i2d2 will also improve.\",\"One limitation of this study is a potential lack of reproducibility for some experiments due to the employment of the proprietary gpt 3.we acknowledge a potential bias in our dataset, as only a quarter of the full corpus was manually reviewed based on existing annotations, i.0 were excluded from our analysis so far.Therefore, the matched dataset of controls will likely overrepresent the prevalence of eccnps, while not adequately representing the full spectrum of hard negative cases.However, we considered a manual review of the full corpus as unnecessary due to the very low number of eccnps expected in the remaining documents.Further, we note that also other kinds of coordination ellipses besides eccnps may be relevant for downstream information extraction tasks.Although preliminary annotations for these exist in ggponc 2.0, we excluded them from our analysis, as annotations were not comprehensive, and their resolution is often more ambiguous than elliptical compounds.\",\"Our study here focused on the most capable gpt3.5 model, text-davinci-002, at the time the experiments were conducted.However, significant further paradigm shifts could change the distribution of errors in such a way that certain of our factors become less critical.In addition, the latest iterations of gpt have a much greater input window size, which help them digest much larger swaths of text in one go and potentially make our pipelined approaches less needed in certain settings.Furthermore, the text-davinci-002 model is fine-tuned with data produced by human demonstrations.The precise data used is not publicly available, so it is difficult to use our results to make claims about what data or fine-tuning regimen leads to what failure modes in these models.Recent work has noted that language models may be susceptible to learning biases from training data , and this phenomenon has also been observed for gpt-3.we did not stress test the models studied for biases and furthermore only experimented on english-language data.When properly used, the summarization models described in this paper can be time-saving.However, as noted above, summary outputs may be factually inconsistent with the input documents or not fully representative of the input, and in such a case could contribute to misinformation.This issue is present among all current abstractive models and is an area of active research.\",\"Our main limitation comes from dataset size.This was limited because we used human evaluation to label model responses as truthful or untruthful.That is, we have manually confirmed gpt-judge labels on davinci responses, and extrapolated the system to ada, babbage, and curie.Frankly, the limitations caused by the small size of the dataset were quite evident because the truthfulness detector was often biased towards producing one label.We attempted to solve this problem using lower regularization parameters, but this often produced models with lower performances.An ideal solution to this problem would be training the truthfulness detector on a large set of training instances, which is also our future direction.\",\".In \\u00a73, we conducted a principled comparison between bert, elmo, and gpt-1 under comparable experimental conditions.This comparison notably excludes more recent models that benefit from more parameters, larger training data, or different loss functions, such as roberta, electra, and t5.In \\u00a73, we have conducted a principled comparison by varying only the pre-training objective function and the length of model training, whilst keeping all the other variables constant.In practice, however, the exact choice of these different control variables can interact and affect the findings in a material way.It is conceivable \\u2014 and rather likely \\u2014 that our findings on the performance gap between bert, elmo, and gpt-1 may change under different experimental settings.our efficient learning scenario in \\u00a73 constitutes a simulated one, where we artificially limit the number of updates to 200,000 steps.our experiments are thus far conducted only in english.The increasing prevalence of closed-source \\u002f proprietary plms.Despite our recommendations and calls for change, we acknowledge the fact that recent plm trends have shifted more towards proprietary and closed-source models \\u2014 a development we attribute to the rapidly increasing commercialization potential of this technology.Under this trend, very little is known about how each plm is developed, as the vast majority of the technical details are kept proprietary.While these trends may mean that our recommendations are more unlikely to be adopted by proprietary plms, we argue that our position paper and recommendations are still important for two reasons.First, open-sourced community models, such as bloom , opt , and alpaca , are gaining traction, and have rapidly narrowed the gap with proprietary models.This progress reflects the community\\u2019s strong desire to have open-sourced models that can rival proprietary ones in terms of model quality.The rise of these open-sourced models thus gives rise to the question: how can these community-driven models help the community pay off our scientific debt? To that end, our recommendations provide concrete and actionable steps in this direction.For instance, our recommendations call for standardizing the pretraining dataset, which has not yet been done thus far, even though there are plausible, open-sourced datasets that can be used for doing so.Furthermore, we also encourage the community to release the full evaluation results of their models, alongside the relevant hyper-parameter information, etc., such that we can collectively build a more comprehensive scaling law through crowd-sourcing.Second, prior work that conducts extensive ablation studies and rigorous experiments remains the exception, rather than the rule.Our position paper includes a call for change that will make it easier to pay off this scientific debt going forward, which is ever-more important in light of impressive progress from both proprietary and open-sourced plms.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"27_proprietary_opensourced_recommendations\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"27_proprietary_opensourced_recommendations\"],\"textfont\":{\"size\":12},\"x\":[10.814390182495117,10.754087448120117,10.798171997070312,11.285440444946289,10.7875394821167,10.96948528289795,11.029439926147461,10.872095108032227,9.805534362792969,11.01862621307373,11.033255577087402,10.833460807800293],\"y\":[3.462264060974121,2.5179519653320312,2.55387020111084,2.5431454181671143,3.4232730865478516,2.6169888973236084,2.6207339763641357,2.6115448474884033,1.5325560569763184,2.6199264526367188,2.630885124206543,2.6484673023223877],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Compared to the empirical risk minimization scheme, the introduced self-regularization scheme incurs certain overhead because each mini-batch of data will go through two models.For bertbase scale pre-trained language models, the additional memory overhead is about 27% and the additional training time overhead is about 30%.Nevertheless, once pruned, the sparsified model can enjoy considerable efficiency gains in terms of storage and inference time.Therefore, this is a trade-off that future practitioners might need to consider.\",\".Accuracy-oriented methods can still hardly reduce the inference efficiency.Yet, our proposed same effectively reduce the speedup ratio by 89%, which is comparable to 93% on base-size models.Impact of modification rate: in our main results, we set the allowable modification rate \\u03f5 as 10% of the input words.We further investigate whether same can reduce the inference efficiency under lower modification rate.The experiment results across glue benchmark on deebert-base and pabee-bert-base under are summarized in table 7.even constrained with a very low modification rate, e., 3%, both variants of same can still significantly reduce the model\\u2019s efficiency.In addition, with increasing modification rate, same leads to higher reduction in efficiency.Ablation study: to understand the inner mechanism of same, we conduct ablation studies on each component.As shown in table 8, solely using heuristic loss can already lead to significant effi- ciency drop.In addition, using loss combination, and adding layer-wise importance weights can both further increase the high computation ratio.Finally, same utilizes all the sub-components, which leads to the lowest inference efficiency.Semantic similarity: while we constrain the modification rate in our experiments to keep the semantic meaning consistent, the semantic similarity between benign and adversarial examples is not explicitly constrained.Therefore, we further investigate the sentence semantic similarity between original and adversarial examples on sst2 dataset.Specifically, we first obtain the sentence representations of adversarial and original sample with a state-of-the-art st5-large embedding model , and then compute their pairwise cosine similarity.With deebertbase and pabee-bert-base as the victim model, the same-word has an average cosine similarity of 0.89, and same-char has an average cosine similarity 0.the results suggest that both variants of same can well preserve the inputs\\u2019 semantic meaning, at the same time, reduce the efficiency of dynamic transformers.Visualization: to illustrate the impact of efficiency-based v.correctness-based adversarial perturbations, we present a case study of adversarial samples produced from sst-2 dataset in table 9.for better explainability, we show examples with one-word only modification.as shown in table 9, our efficiency-based method will perturb the word but to bujt, thereby altering the explicit turning relationship between two sentences.While humans can make the correct prediction even without the word but, it can be challenging for dynamic transformers to infer the turning relationship in the early stage.Therefore, they fail to satisfy the exiting conditions, resulting in reduced inference efficiency.In contrast, correctnessbased approaches will keep the transition word and adversarially modify the word deeper, e.with the transition word but, the model will emphasize more on the latter sentence, and easily get a high model confidence.\",\"Pram effectively handles the zrcl-ner task but has certain limitations.Firstly, since pram relies on the cross-lingual inference ability of mplm, its transfer ability may be restricted if the target language is not among the pre-trained languages of mplm.Secondly, the high memory requirements of pram may occurs when the task is the multisource transfer, where we need to set a large batch size to ensure the stable update of prototypes on different source languages.\",\".Firstly, we evaluate umls-kgi-bert on a very narrow range of tasks limited to token classification - a broader range of information extraction and reasoning tasks would be necessary for a more complete picture of the utility of our pretraining methods.In addition, we only train models for mid-to-high-resource languages; to properly validate the applicability of this approach, in particular the lessening of the need to rely on large training corpora, it will be necessary to train and evaluate such models in more low-resource settings.Table 6: macro-f1 scores for the ablation experiments.Dataset base model kg tasks cas-pos cas-sg quaero-medline essai-pos drbert-4gb - 90.50 ncbi-disease biored-ner jnlpba04 pubmedbert - 93.\",\"We discuss the limitations of pluglm as follows: despite the strong performance achieved by our approach with dpm, it results in a reduced inference efficiency at the same time due to the mips search.For example, pluglm is about two times slower than pure transformer-based models in glue.This would be more crucial when the external memory is much larger.in this paper, we choose wikipedia for dpm construction and pluglm pre-training.While wikipedia is the most commonly used data source for language model pre-training , there are also many other types of knowledge not covered in wikipedia, and how to integrate different types of knowledge into our framework remains under-explored.Although this paper proposes a general architecture that is applicable to plms of all kinds and sizes including bidirectional , unidirectional and encoder-decoder-based plm , we only experiment with bidirectional models in moderate size.In particular, we believe this architectural design would be greatly beneficial for llm for the following reasons: the parameters of llm could not be easily updated once the pre-training is done due to the unaffordable training cost.The additional latency cost by mips retrieval is negligible compared with that of the whole llm.\",\"We have not tested all possible recent methods on lemon.We have used expensive gpu resources to speed up the training process on lemon, with 8 nvidia a100 sheets, but consistent results can also be obtained with 8 v100 sheets.other languages, such as japanese and korean, could benefit from the same technique, but have not been studied in this work.\",\"Due to limited availability of compute resources, we were unable to scale up the model architecture to the large sizes becoming increasingly mainstream today.Similarly, the upstream corpus we used is 16gb in size, and while it is large enough such that it was used to pretrain bert , much larger pretraining datasets are in use today such as the colossal common crawl corpus.The relative performance achieved by using self-pretraining vs pretraining on upstream corpus can likely vary with the size of the model and upstream corpus, and more compute-heavy large scale experiments are needed to characterize it.\",\".Therefore, plugd has a higher storage requirement than conventional encodingcoupling methods.We encourage in the experiments, we adopt t5 as our ptm backbone.Actually, the proposed framework can also be applied to more pre-trained models with various model architectures.Besides, recent trends show that larger models tend to build more expressive text representations.It is worth exploring plugd with larger ptms with billions of parameters to learn informative document plugins.In this paper, we adopt an external retriever to retrieve relevant documents for each input query.Recent progress in retrievalaugmented language models shows that training the ptms with an end-to-end textual knowledge retriever can promote downstream performance.We believe document plugins can also serve as the ex- ternal knowledge base and enhancing plugd with end-to-end retrieval is a promising direction.\",\"In this work, we demonstrate the effectiveness of the proposed mp2 with the backbone ptm of cpt-large on a set of chinese nlp tasks.Due to the expensive pre-training cost, we did not explore mp2 on other ptms with varying sizes, pretraining objectives and architectures.Besides, it is also unknown how does the number of pre-training tasks affect the performance of mp2.For resourcerich languages such as english and chinese, it would be promising for mp2 to be well-performed since one can easily collect sufficient public upstream tasks.Nevertheless, for low-resource languages or domains, the effect of mp2 is still underexplored.\",\", the unk entry for ratsqlb is empty.Random initialization means that model results after each training may vary slightly, so we only focus on the more salient features.more importantly, the single-word performance of ratsqlo without emsl is close to that of ratsql and ratsqlo.1, the representation ability on multiword of glove is worse than that of bert.The results support this view where the performance of ratsqlo and ratsql on multi-word is worse than that on single-word.When replacing the glove with bert, due to the improvement of its multi-word representation ability, the performance of ratsqlb with and without emsl are close in single and multiple words.From the right side of table 8, it can also be found that the bert brings around 5% absolute improvement on multi-word, while that on single-word is only 2%.\",\"The proposed framework has a limitation in terms of the large gpu resources required, as it necessitates double the memory compared to training a crs alone.Due to this limitation, we have to forego the use of pre-trained language models such as bert, which could have been beneficial in enhancing language quality, but their extreme memory requirements make it infeasible.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"28_modification_mp2_multiword\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"28_modification_mp2_multiword\"],\"textfont\":{\"size\":12},\"x\":[13.157628059387207,12.979100227355957,13.186514854431152,13.24641227722168,13.322733879089355,13.270655632019043,13.175507545471191,13.208261489868164,13.13655948638916,13.259763717651367,13.164571762084961,13.191608428955078],\"y\":[2.971980333328247,2.6652121543884277,2.65029239654541,2.769920587539673,2.8000214099884033,2.7044589519500732,2.7897696495056152,2.578014373779297,2.686814546585083,2.6946475505828857,2.927328109741211,2.748950958251953],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"A limitation of our approach is that it relies heavily on the quality and relevance of the prompts used.The prompts were engineered based on observations made in the training data and this approach may not work if the prompts are not representative of the corpus.Finally, our approach may not be suitable for all types of teacher-student dialogues and may require modifications for different contexts or domains.One possible concern with the techniques mentioned in this paper is the limited reproducibility of openai\\u2019s language models, such as gpt-3.the weights of these models are proprietary and not publicly accessible, which makes it challenging to replicate the findings of earlier research or expand on them.\",\"A fraction of 157 essays was too long to fit into our transformer model so that arguments later in the text have not been identified at all.As gold standard information about the writing prompt for a specific essay was not released with the dataset, we had to rely on automatically assigned prompt information with an estimated average accuracy of 0.in a realistic class-room setting, however, the information about the writing prompt would be readily available, thus we probably underestimated the effect of adding prompt information.We tested our models on the persuade dataset of english high-school writings only, thus we cannot be sure whether results transfer to other educational contexts and languages.since our model was trained on a limited amount of data, it may have the potential risk of discouraging students to write innovative, but effective arguments.As discussed in automatic essay scoring approaches, computers may be able to analyze writing for the presence or absence of certain words or structures , but they cannot really understand or appreciate a writer\\u2019s message in the same sense that human readers can.\",\".\\u2022 the ai feedback in our experiments comes from gpt-3, which requires a fee to use.\\u2022 we do not explore the effect of different task description prompts on the quality of generated sample pairs, which may influence the performance of claif.\\u2022 in clhaif, we only use the ai feedback for positive sample pairs.How to utilize ai feedback for negative sample pairs remains to be studied.\",\"In this work, we have only analyzed the common errors of two models in the instructional dialogue task.One open question is whether other gpt-based models or models with other architectures also have the same issue in this task.Our work and dataset are also limited to the english language.\",\"Limitation in relying solely on automatic evaluation metrics prompt engineering to adapt language and tone in tutoring systems our experiments reveal an intriguing finding where manipulating the prompt influences the tone and language of the generated response, presenting an opportunity for tutoring systems to potentially adapt to the students\\u2019 learning styles and\\u002for teaching goals.Further research should delve into teaching instruction methods, potentially exploring the pedagogy of constructivist learning or engaging students in ill-structured exercises for productive failure using llms of this nature.Gpt-3\\u2019s robust handling of errors and noncanonical form of language during the data preparation phase, a manual inspection of the data revealed the presence of grammatical and spelling errors in some utterances.Additionally, since the dataset originated from chatroom text-based conversations, there were instances where mathematical symbols were used instead of natural language, such as this example utterance output teacher: but e.pleased with their visit = good idea.It is worth noting that we did not employ any nlp processing toolkit to correct these errors or non-canonical forms in the dialogue utterances.However, despite this, the gpt-3 model could still generate appropriate responses effectively.Provides valuable opportunities for tutors to adapt to students\\u2019 native languages.Code-switching strategies as such have been found to enhance teaching, including the explanation of concepts , and leveraging ai tutoring systems can facilitate this process.Llms possess multilingual capabilities that enable them to address language barriers, accommodate low-resource languages, and exhibit promising performance even on unseen languages.To enhance accessibility, the development and adoption of open-source multilingual models, such as bloom , should be encouraged, thereby facilitating the utilization of llms in educational applications across diverse linguistic contexts.\",\"In generating pedagogically sound responses consistently.We then fine-tuned gpt-2 and dialogpt on the tscc dataset and evaluated their performance using bertscore and dialogrpt metrics.We also proposed an approach using rl to optimize directly for pedagogical values.We hypothesized that several dataset characteristics pose challenges to achieving superior performance with fine-tuning.To this end, we recommend the extension of the dataset to include longer prompts with extended context.Finally, we also draw attention to the need for more domain-specific metrics in enabling the generation of accurate, context-aware, and factually correct teacher responses.\",\"Our proposed model has also some limitations.First, the requirement of large memory power of gpu due to the use of gpt-2medium in the training of pal.Further, weight optimization for each of the possible combinations of different rewards may lead to model training and validation time to months.Hence, some heuristic is adopted to choose some sets of combinations of reward weights.In case of continuous, short and direct responses during interaction like \\u2018yes\\u2019, \\u2018i don\\u2019t know\\u2019, \\u2018no\\u2019, \\u20182\\u2019, \\u2018yeah\\u2019, the system first tries to counsel client by inquiring about their issue but after three or four turns it starts deviating and may generate repetitive or inconsistent responses.This can be due to the fact that the datasets which are used to train the pal mostly consists of interactive dialogues with long utterances, hence model gets confused when treated with short and direct responses.Lastly, it is also observed that sometimes, model asks too many questions to the user.hence, the model should be forced to generate only relevant inquiries by discriminating the irrelevant inquiries.This opens up the door for future studies to build a counseling dialogue system.\",\"Of manual classroom observation and provide scalable, automated feedback on instructional practice.While our results reveal that chatgpt has room for improvement in generating insightful and novel feedback for teaching, our proposed tasks and evaluation process provide a foundation for future research to address the challenges of teacher coaching using nlp.Our work underscores the challenge and importance of generating helpful feedback for teacher coaching.Moving forward, we propose several directions for further research, such as improved prompting methods and reinforcement learning with feedback from coaches.Ultimately, we envision a future where generative ai can play a crucial role in supporting effective teacher education and professional development, leading to improved outcomes for students.\",\", future research could explore ways to combine the strengths of humans and language models to produce even more accurate and informative explanations.Therefore, chatgpt has the potential to assist teachers and other professionals in the creation of high-quality assessment items through a well-designed prompt, which can help ensure that items have a single correct answer, independent options, non-overlapping options, and plausible options.Furthermore, chatgpt ability to classify items based on ecd principles is promising, but further research is needed.For example, the evidences could be provided to language models and ask them to classify each item in one of them.Also, they could be asked to create the options and the respective explanations based on some kind of guidelines such as the one cited.\",\".A range of factors may affect instructional outcomes, only a subset of which could be measured with this data.Making strong claims about the link between discourse moves and instructional outcomes requires experimental validation.For example, the quality of the math task that the students are working may affect the discourse as well as learning outcomes.We can isolate the effect of discourse moves by randomly assigning teachers to learning opportunities that help them improve their use of these moves, and examining downstream impacts of these new talk moves on student outcomes.Has taken a similar approach successfully in an informal teaching context, but such a study is yet to be done in a k-12 context.Although the ncte transcript dataset is the largest available dataset of u.classroom transcripts, it only captures a tiny fraction of u.classrooms and hence there are limitations to its representativeness.The data represents mostly white female teachers working in mid-size to large districts, so it would be valuable to collect new data from other types of districts and a more diverse teacher population.The fact that the data was collected a decade ago may pose limitations to its ongoing relevance; during the period under study , many schools were transitioning toward common core-aligned instruction in mathematics but yet lacked high-quality curriculum materials for doing so.That said, research in education reform has long attested to the fact that teaching practices have remained relatively constant over time and that there are strong socio-cultural pressures that maintain the instructional status quo.In general, it is important to carefully validate measures built on the ncte data on a new domain to ensure that it is representative of the target population.education research has attested the limitations of standardized assessments in capturing student learning and reasoning.Student questionnaires in the ncte data can provide an alternative perspective on students\\u2019 experiences and mathematics outcomes but these responses have a lot of missing values, and hence it may not provide robust estimates.Furthermore, understanding equity in instruction is a high priority for our research team and for the field more generally.However, studying equity within this data is challenging, since student speakers are not linked to administrative files containing student background and achievement variables.That said, such speaker-level demographic data is rarely available in instructional contexts, for important ethical reasons, and thus this limitation may encourage researchers to develop measures of instructional equity that leverage classroom-level, instead of speaker-level demographic information.\",\"This paper presents a dataset of expert-curated socratic conversations where instructors assist novice programmers in fixing buggy solutions to simple computational problems.The dataset serves as a benchmark for evaluating the socratic debugging capabilities of lms.5, its precision, and recall remain below human expert levels , highlighting the need for further research.We find that gpt-family language models may generate repetitive and irrelevant socratic utterances that could mislead learners.The utterances may also appear too early in the conversation, causing confusion, and can be overly direct, potentially diminishing learning outcomes.Study limitations include: the automatic metrics are limited in capturing the correctness, helpfulness, and relevance of a socratic utterance, and the benchmark dataset may not represent all common novice misconceptions.Moreover, the manual evaluation is limited to 5 dialogues and could be expanded, but this process is highly time-consuming.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"29_instructional_teaching_students\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"29_instructional_teaching_students\"],\"textfont\":{\"size\":12},\"x\":[9.548015594482422,9.895105361938477,9.52209758758545,9.598806381225586,9.638100624084473,9.509757995605469,9.517975807189941,9.500367164611816,9.551833152770996,9.686979293823242,9.582605361938477,9.595603942871094],\"y\":[0.7267842888832092,0.807807445526123,0.8538795113563538,0.7860575318336487,0.7572054862976074,0.7536084651947021,0.8582301139831543,0.7455077767372131,0.8008758425712585,0.7965689897537231,0.7402281165122986,0.7842502593994141],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Limitation, our primary contributions include: \\u2022 counterfact+, a dynamic specificity benchmark, which adapts to the model edit under test, and is more sensitive than the existing benchmark \\u2022 neighborhood kl divergence , a specificity metric based on the full probability distribution as a complement to the currently used metrics which focus only on the tokens directly implicated in the model edit.Limitations the main limitation of the approach we took for improving model editing benchmarks is that it is ultimately based on manual inspection of test cases to understand the failure modes of model editing methods.This approach is not scalable and has a significant cost in terms of time and effort.As far as the specific benchmark we propose is concerned, more research is needed to assess its effectiveness for more complex scenarios such as dialogue and multi-turn conversations.We also have not investigated the application of our benchmark to scenarios in which multiple model edits are performed simultaneously.Furthermore, we do not evaluate other types of model edits, such as parameter pruning, and transfer learning.this could include corpus-level analysis and dynamic approaches like red-teaming or dynamic benchmarking to uncover subtle adverse effects.\",\"This work focused on experiments on english datasets and did not explore other languages.While we expect that our assumptions hold across languages and the proposed methods and metrics can be applied without any further modifications to other languages this has not been explicitly verified.Additionally, we ensured to experiment with counterfactual editors that are representative of the main counterfactual editing methodologies, however we did not exhaustively cover all publicly available editors as our main goal was to demonstrate that our proposed method is widely applicable rather than to exhaustively compare editors.\",\"The present study is limited to exploring biases in mlms for the gender dimension only.we also used machine translation on english counterfactuals to obtain cda data in each language in our dataset.Translations are prone to errors and issues like translaionese , especially for the lower resource languages, and therefore can lead to the unreliability of the quality of generated counterfactuals were generated.This can help us scale our counterfactual generation process to a much higher number of samples while also avoiding any losses in quality that may arise due to machine translation.Our multilingual disco metric is currently limited to 6 indian languages and we hope our work will inspire further extension to cover different language families for improving the focus on multilingual biases evaluation.\",\"Of existing methods and providing a more comprehensive view of a model\\u2019s predictions.Limitations our work shows that crest is a suitable framework for generating high-quality counterfactuals and producing plausible rationales, and we hope that crest motivates new research to develop more robust and interpretable models.We note, however, two main limitations in our framework.First, our counterfactuals are the result of a large language model , and as such, they may carry all the limitations within these models.Therefore, caution should be exercised when making statements about the quality of counterfactuals beyond the metrics reported in this paper, especially if these statements might have societal impacts.Second, crest relies on a rationalizer to produce highlights-based explanations, and therefore it is limited in its ability to answer interpretability questions that go beyond the tokens of the factual or counterfactual input.\",\"In its current formulation, rev might reward a rationale for an incorrect prediction as long as the rationale supports the prediction with relevant additional information.Additionally, our metric does not consider the factuality of rationales.another limitation is that the utility of rev depends on the quality of crowd-sourced rationales used to train the evaluator.Building a good automatic metric rev requires high-quality rationales that provide sufficient new information to explain the corresponding labels.The architecture of evaluation models also has an impact on rev evaluation.\",\"While we have argued that our approach to collecting counterfactual data via disco is agnostic to the particular task and language, we emphasize that the experiments we report are limited to english and the task of nli.Given that english is a high-resource language, there could be additional challenges in re-creating our pipeline for other languages.as with the related studies we cite ), given the high costs associated with largescale prompting, we are unable to ablate all parts of our data generation pipeline.Similar to virtually all experiments involving llm prompting, such differences could affect the results and quality of the resulting augmentation datasets.Similarly, given the high costs of human annotation, we have limited our human evaluation to around 500 random instances , which follows other related studies.\",\"Limitations to our work are as follows: we only study the three magnitude effects for the number word and digit denotations of the numbers 1 to 9.The effects for the number 0, numbers greater than 10, decimal numbers, negative numbers, etc.Are beyond the scope of this study.The mapping of llm behaviors to human behaviors and effects might vary for each effect.Thus, we might require a different linking hypothesis for each such effect.We only use the models built for english tasks and do not evaluate multi-lingual models.We report and analyze aggregated scores across different dimensions.There can be some information loss in this aggregation.Our choice of models is limited by certain resource constraints.The behavioral analysis of this study is one-way: we look for human performance characteristics and behaviors in llms.Future research can utilize llms to discover new numerical effects and look for the corresponding performance characteristics in humans.This could spur new research in cognitive science.The results show similar outputs to low dimensional human output and show that we do not need explicit neural circuitry for number understanding.We do not suggest models actually are humanlike in how they process numbers.\",\"One major shortcoming of feag method is the dependency on creation of counterfactual inputs.If there is an error in counterfactual generation, we might get a wrong feature effect estimate.Thus, for simplicity, our evaluation considered tokens as features.The parallel development of counterfactual input generation methods would hopefully ease this issue and allow feag to be used reliably for spurious correlations on more complex features too.\",\"Several main limitations exist in our study in its current form.First, our reported results only simulated experimental participants by manipulating the temperature hyperparameter.We compared this approach with natural language prompting for experiment 1, but that prompting did not increase \\\"participant\\\" diversity, so it was abandoned.Moreover, approaches for simulating psycholinguistic experimental \\\"participants\\\" could go far beyond what was tried here; our prompting method was relatively limited, and more detailed prompting could be included in future experimental simulations.Second, making a direct comparison with actual psycholinguistic experiments might not be the only method to investigate llms\\u2019 discourse capacity.A comprehensive list of discourse probing tasks might play a similar role despite a different way.Third, this study is strictly behavioral: limited by both computational resources and obscure mechanisms of in-context learning, we do not dive into models\\u2019 internal representations in our analyses.\",\"We are releasing scone as a diagnostic tool for conducting controlled scientific experiments.This is our primary intended use, and we advise against uncritical use of scone for real-world applications, as we have not audited the dataset for such purposes.As a diagnostic tool, scone\\u2019s primary limitation is its focus on english.Cross-linguistically, we find many strategies for expressing negation.The english-language strategy of using mostly adverbial modifiers for sentential negation is not the only one by any means, and we would expect to see quite different results for languages in which negation is expressed, for example, with verbal suffixes.This highlights the value of potential future efforts extending scone to other languages.By the same token, we acknowledge that many linguistic phenomena interact with negation even internal to english.Scone restricts to negation in the context of lexical entailment, and mostly uses \\u201cnot\\u201d as the negative morpheme.This excludes a wide range of negation morphemes and negation strategies that ultimately need to be brought into the picture.Finally, we note that there may be undesirable biases in scone that could interact with biases in the models.Scone is in part derived from snli, which is known to contain gaps, social biases, and artifacts , and scone may inherit some of these.\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"30_counterfactuals_scone_counterfactual\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"30_counterfactuals_scone_counterfactual\"],\"textfont\":{\"size\":12},\"x\":[11.210502624511719,11.236587524414062,11.170992851257324,11.193377494812012,10.897392272949219,11.15462589263916,11.086121559143066,11.182721138000488,11.076476097106934,11.256063461303711,11.146486282348633],\"y\":[2.01076078414917,1.9563953876495361,1.935896635055542,1.9087300300598145,2.5598957538604736,1.8559261560440063,1.824581503868103,2.187316417694092,1.7874271869659424,1.808978796005249,1.983590841293335],\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"shapes\":[{\"line\":{\"color\":\"#CFD8DC\",\"width\":2},\"type\":\"line\",\"x0\":11.730491960048676,\"x1\":11.730491960048676,\"y0\":-1.679162085056305,\"y1\":5.978470945358277},{\"line\":{\"color\":\"#9E9E9E\",\"width\":2},\"type\":\"line\",\"x0\":6.268957924842835,\"x1\":17.192025995254518,\"y0\":2.149654430150986,\"y1\":2.149654430150986}],\"annotations\":[{\"showarrow\":false,\"text\":\"D1\",\"x\":6.268957924842835,\"y\":2.149654430150986,\"yshift\":10},{\"showarrow\":false,\"text\":\"D2\",\"x\":11.730491960048676,\"xshift\":10,\"y\":5.978470945358277}],\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eDocuments and Topics\\u003c\\u002fb\\u003e\",\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":750,\"xaxis\":{\"visible\":false},\"yaxis\":{\"visible\":false}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4e2b4db4-b718-4003-9aff-8e520637e89d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "# topic_model = BERTopic()\n",
        "# topics, _ = topic_model.fit_transform(docs)\n",
        "topic_distr, _ = topic_model.approximate_distribution(df_filtered_doc['Text'], min_similarity=0)\n"
      ],
      "metadata": {
        "id": "Lpg0n8bc9KdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To visualize the probabilities of topic assignment\n",
        "topic_model.visualize_distribution(probs[0])\n",
        "\n",
        "# To visualize the topic distributions in a document\n",
        "topic_model.visualize_distribution(topic_distr[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "outputId": "c724596a-b2b8-475e-e1e3-8715c5213351",
        "id": "WuUoWoO49KdY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"e4f21c6f-2ba8-4e6b-a5a3-4faa8fdc177d\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e4f21c6f-2ba8-4e6b-a5a3-4faa8fdc177d\")) {                    Plotly.newPlot(                        \"e4f21c6f-2ba8-4e6b-a5a3-4faa8fdc177d\",                        [{\"marker\":{\"color\":\"#C8D2D7\",\"line\":{\"color\":\"#6E8484\",\"width\":1}},\"orientation\":\"h\",\"x\":[0.05340434607183693,0.04459030435499159,0.03611083838215529,0.03514960553353012,0.03354355501772536,0.0363115956783243,0.03693215357877028,0.03386506045744227,0.03801470400373076,0.03318006415777547,0.035835591900201434,0.03417068858629075,0.03350234230561889,0.02906242716713014,0.03943025325025328,0.03302309897021493,0.03820493146639285,0.03338293785175566,0.03716899311593049,0.0338727313869504,0.024883405726346227,0.024460034457882765,0.032380026133353855,0.03153235211572407,0.021746925775212898,0.016658999150442996,0.02043416360843616,0.021093779553480265,0.03250966736686714,0.018075932264341916,0.027468490610890687],\"y\":[\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e: translation_languages_mu...\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e: entity_entities_knowledg...\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e: visual_image_multimodal_...\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e: dialogue_conversations_d...\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e: clinical_medical_patient...\",\"\\u003cb\\u003eTopic 5\\u003c\\u002fb\\u003e: hyperparameters_metaretr...\",\"\\u003cb\\u003eTopic 6\\u003c\\u002fb\\u003e: adversarial_attacks_atta...\",\"\\u003cb\\u003eTopic 7\\u003c\\u002fb\\u003e: summarization_summaries_...\",\"\\u003cb\\u003eTopic 8\\u003c\\u002fb\\u003e: sentiment_emotion_emotio...\",\"\\u003cb\\u003eTopic 9\\u003c\\u002fb\\u003e: gender_bias_debiasing_ha...\",\"\\u003cb\\u003eTopic 10\\u003c\\u002fb\\u003e: qa_question_questions_a...\",\"\\u003cb\\u003eTopic 11\\u003c\\u002fb\\u003e: transformer_nat_transfo...\",\"\\u003cb\\u003eTopic 12\\u003c\\u002fb\\u003e: political_moral_hate_so...\",\"\\u003cb\\u003eTopic 13\\u003c\\u002fb\\u003e: reasoning_pangu_symboli...\",\"\\u003cb\\u003eTopic 14\\u003c\\u002fb\\u003e: speech_asr_s2st_voice_l...\",\"\\u003cb\\u003eTopic 15\\u003c\\u002fb\\u003e: classification_oov_slas...\",\"\\u003cb\\u003eTopic 16\\u003c\\u002fb\\u003e: event_events_eae_argume...\",\"\\u003cb\\u003eTopic 17\\u003c\\u002fb\\u003e: explanations_explanatio...\",\"\\u003cb\\u003eTopic 18\\u003c\\u002fb\\u003e: prompts_prompt_continuo...\",\"\\u003cb\\u003eTopic 19\\u003c\\u002fb\\u003e: zeroshot_fewshot_verbal...\",\"\\u003cb\\u003eTopic 20\\u003c\\u002fb\\u003e: chatgpt_chatgpts_rewrit...\",\"\\u003cb\\u003eTopic 21\\u003c\\u002fb\\u003e: tables_table_tableqa_ce...\",\"\\u003cb\\u003eTopic 22\\u003c\\u002fb\\u003e: oosf_instances_proxy_la...\",\"\\u003cb\\u003eTopic 23\\u003c\\u002fb\\u003e: figurative_metaphors_so...\",\"\\u003cb\\u003eTopic 24\\u003c\\u002fb\\u003e: sarcasm_irony_humor_ocr...\",\"\\u003cb\\u003eTopic 25\\u003c\\u002fb\\u003e: dense_retrieval_retriev...\",\"\\u003cb\\u003eTopic 26\\u003c\\u002fb\\u003e: argumentative_environme...\",\"\\u003cb\\u003eTopic 27\\u003c\\u002fb\\u003e: proprietary_opensourced...\",\"\\u003cb\\u003eTopic 28\\u003c\\u002fb\\u003e: modification_mp2_multiw...\",\"\\u003cb\\u003eTopic 29\\u003c\\u002fb\\u003e: instructional_teaching_...\",\"\\u003cb\\u003eTopic 30\\u003c\\u002fb\\u003e: counterfactuals_scone_c...\"],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eTopic Probability Distribution\\u003c\\u002fb\\u003e\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"xaxis\":{\"title\":{\"text\":\"Probability\"}},\"width\":800,\"height\":600},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e4f21c6f-2ba8-4e6b-a5a3-4faa8fdc177d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hierarchical_topics = topic_model.hierarchical_topics(df_filtered_doc['Text'])\n",
        "\n",
        "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "573f08e8-0f3c-4a70-c114-57ed2915887c",
        "id": "FgH_0uHq9KdZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:00<00:00, 162.79it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"bcdad4aa-8687-4951-aaf4-296cfa961d9b\" class=\"plotly-graph-div\" style=\"height:665px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bcdad4aa-8687-4951-aaf4-296cfa961d9b\")) {                    Plotly.newPlot(                        \"bcdad4aa-8687-4951-aaf4-296cfa961d9b\",                        [{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"hyperparameters_metaretriever_pts_radius_optimization\",\"\",\"\",\"transformer_nat_transformers_decoding_encoder\"],\"x\":[0.0,0.7255787319866616,0.7255787319866616,0.0],\"xaxis\":\"x\",\"y\":[-35.0,-35.0,-45.0,-45.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"classification_oov_slash_hypermixer_mlpbased\",\"\",\"\",\"transformer_nat_transformers_parameters_decoding\"],\"x\":[0.0,0.7446097655702633,0.7446097655702633,0.7255787319866616],\"xaxis\":\"x\",\"y\":[-25.0,-25.0,-40.0,-40.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"transformer_parameters_transformers_nat_cost\",\"\",\"\",\"reasoning_pangu_symbolic_rule_thinksum\"],\"x\":[0.7446097655702633,0.7914313961840043,0.7914313961840043,0.0],\"xaxis\":\"x\",\"y\":[-32.5,-32.5,-55.0,-55.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"zeroshot_fewshot_verbalizers_ss_bots\",\"\",\"\",\"reasoning_transformer_parameters_transformers_nat\"],\"x\":[0.0,0.8017204244959464,0.8017204244959464,0.7914313961840043],\"xaxis\":\"x\",\"y\":[-15.0,-15.0,-43.75,-43.75],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"reasoning_transformer_parameters_transformers_cost\",\"\",\"\",\"prompts_prompt_continuous_preadd_mt0\"],\"x\":[0.8017204244959464,0.8095775129523718,0.8095775129523718,0.0],\"xaxis\":\"x\",\"y\":[-29.375,-29.375,-65.0,-65.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"tables_table_tableqa_cells_ctbls\",\"\",\"\",\"reasoning_transformer_prompt_parameters_transformers\"],\"x\":[0.0,0.8614370380894582,0.8614370380894582,0.8095775129523718],\"xaxis\":\"x\",\"y\":[-5.0,-5.0,-47.1875,-47.1875],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"entity_entities_knowledge_graph_kg\",\"\",\"\",\"visual_image_multimodal_scene_images\"],\"x\":[0.0,0.6706588158177551,0.6706588158177551,0.0],\"xaxis\":\"x\",\"y\":[-125.0,-125.0,-135.0,-135.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"dialogue_conversations_dialog_dialogues_multiparty\",\"\",\"\",\"entity_visual_image_entities_multimodal\"],\"x\":[0.0,0.6928733007615606,0.6928733007615606,0.6706588158177551],\"xaxis\":\"x\",\"y\":[-115.0,-115.0,-130.0,-130.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"sentiment_emotion_emotional_wer_reviews\",\"\",\"\",\"dialogue_entity_visual_knowledge_graph\"],\"x\":[0.0,0.7096204726257432,0.7096204726257432,0.6928733007615606],\"xaxis\":\"x\",\"y\":[-105.0,-105.0,-122.5,-122.5],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"dialogue_entity_visual_graph_knowledge\",\"\",\"\",\"qa_question_questions_answer_qe\"],\"x\":[0.7096204726257432,0.7256704614267191,0.7256704614267191,0.0],\"xaxis\":\"x\",\"y\":[-113.75,-113.75,-145.0,-145.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"adversarial_attacks_attack_backdoor_al\",\"\",\"\",\"dialogue_entity_visual_knowledge_multimodal\"],\"x\":[0.0,0.7385433705428008,0.7385433705428008,0.7256704614267191],\"xaxis\":\"x\",\"y\":[-95.0,-95.0,-129.375,-129.375],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"explanations_explanation_concepts_reasoning_cpace\",\"\",\"\",\"dialogue_entity_adversarial_knowledge_attacks\"],\"x\":[0.0,0.7635629156140754,0.7635629156140754,0.7385433705428008],\"xaxis\":\"x\",\"y\":[-85.0,-85.0,-112.1875,-112.1875],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"clinical_medical_patients_notes_biomedical\",\"\",\"\",\"summarization_summaries_summary_factual_msmo\"],\"x\":[0.0,0.6961650652575639,0.6961650652575639,0.0],\"xaxis\":\"x\",\"y\":[-155.0,-155.0,-165.0,-165.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"translation_languages_multilingual_translations_crosslingual\",\"\",\"\",\"gender_bias_debiasing_harms_social\"],\"x\":[0.0,0.6581083274486548,0.6581083274486548,0.0],\"xaxis\":\"x\",\"y\":[-175.0,-175.0,-185.0,-185.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"translation_languages_gender_bias_debiasing\",\"\",\"\",\"political_moral_hate_social_media\"],\"x\":[0.6581083274486548,0.6921685551212133,0.6921685551212133,0.0],\"xaxis\":\"x\",\"y\":[-180.0,-180.0,-195.0,-195.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"summarization_clinical_medical_summaries_patients\",\"\",\"\",\"translation_languages_bias_gender_social\"],\"x\":[0.6961650652575639,0.7333898402218662,0.7333898402218662,0.6921685551212133],\"xaxis\":\"x\",\"y\":[-160.0,-160.0,-187.5,-187.5],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"dialogue_entity_adversarial_knowledge_attacks\",\"\",\"\",\"languages_summarization_bias_translation_gender\"],\"x\":[0.7635629156140754,0.778580047877181,0.778580047877181,0.7333898402218662],\"xaxis\":\"x\",\"y\":[-98.59375,-98.59375,-173.75,-173.75],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"dialogue_languages_evaluation_language_summarization\",\"\",\"\",\"event_events_eae_argument_trigger\"],\"x\":[0.778580047877181,0.8022546573931936,0.8022546573931936,0.0],\"xaxis\":\"x\",\"y\":[-136.171875,-136.171875,-205.0,-205.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"speech_asr_s2st_voice_lyrics\",\"\",\"\",\"dialogue_languages_dataset_language_evaluation\"],\"x\":[0.0,0.8411370750068082,0.8411370750068082,0.8022546573931936],\"xaxis\":\"x\",\"y\":[-75.0,-75.0,-170.5859375,-170.5859375],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"dialogue_languages_dataset_language_datasets\",\"\",\"\",\"figurative_metaphors_sociocultural_metaphor_metaphorical\"],\"x\":[0.8411370750068082,0.8702550326444013,0.8702550326444013,0.0],\"xaxis\":\"x\",\"y\":[-122.79296875,-122.79296875,-215.0,-215.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"transformer_reasoning_parameters_prompt_tasks\",\"\",\"\",\"dialogue_languages_dataset_language_evaluation\"],\"x\":[0.8614370380894582,0.8977144464239208,0.8977144464239208,0.8702550326444013],\"xaxis\":\"x\",\"y\":[-26.09375,-26.09375,-168.896484375,-168.896484375],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"chatgpt_chatgpts_rewriting_responses_query\",\"\",\"\",\"instructional_teaching_students_moves_ncte\"],\"x\":[0.0,0.835989313844496,0.835989313844496,0.0],\"xaxis\":\"x\",\"y\":[-235.0,-235.0,-245.0,-245.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"sarcasm_irony_humor_ocr_yorker\",\"\",\"\",\"chatgpt_responses_chatgpts_instructional_teaching\"],\"x\":[0.0,0.8840210630899525,0.8840210630899525,0.835989313844496],\"xaxis\":\"x\",\"y\":[-225.0,-225.0,-240.0,-240.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"proprietary_opensourced_recommendations_utility_eccnps\",\"\",\"\",\"oosf_instances_proxy_label_false\"],\"x\":[0.0,0.8486238184475704,0.8486238184475704,0.0],\"xaxis\":\"x\",\"y\":[-265.0,-265.0,-275.0,-275.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"counterfactuals_scone_counterfactual_rev_negation\",\"\",\"\",\"proprietary_opensourced_oosf_utility_recommendations\"],\"x\":[0.0,0.870393915381184,0.870393915381184,0.8486238184475704],\"xaxis\":\"x\",\"y\":[-255.0,-255.0,-270.0,-270.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"proprietary_opensourced_counterfactuals_scone_utility\",\"\",\"\",\"argumentative_environmental_firms_argument_episode\"],\"x\":[0.870393915381184,0.887108132427792,0.887108132427792,0.0],\"xaxis\":\"x\",\"y\":[-262.5,-262.5,-285.0,-285.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"chatgpt_sarcasm_responses_chatgpts_instructional\",\"\",\"\",\"argumentative_counterfactual_proprietary_opensourced_counterfactuals\"],\"x\":[0.8840210630899525,0.9149499516735708,0.9149499516735708,0.887108132427792],\"xaxis\":\"x\",\"y\":[-232.5,-232.5,-273.75,-273.75],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"dense_retrieval_retrievers_bm25_recontriever\",\"\",\"\",\"modification_mp2_multiword_pram_pluglm\"],\"x\":[0.0,0.8710438876492081,0.8710438876492081,0.0],\"xaxis\":\"x\",\"y\":[-295.0,-295.0,-305.0,-305.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"chatgpt_sarcasm_responses_proprietary_argumentative\",\"\",\"\",\"dense_retrieval_retrievers_bm25_modification\"],\"x\":[0.9149499516735708,0.9316453510054099,0.9316453510054099,0.8710438876492081],\"xaxis\":\"x\",\"y\":[-253.125,-253.125,-300.0,-300.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"language_work_languages_dataset_different\",\"\",\"\",\"chatgpt_dense_sarcasm_retrieval_responses\"],\"x\":[0.8977144464239208,0.9717978340662432,0.9717978340662432,0.9316453510054099],\"xaxis\":\"x\",\"y\":[-97.4951171875,-97.4951171875,-276.5625,-276.5625],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"transformer_parameters_transformers_nat_cost\",\"reasoning_transformer_parameters_transformers_cost\",\"dialogue_entity_visual_graph_knowledge\",\"translation_languages_gender_bias_debiasing\",\"summarization_clinical_medical_summaries_patients\",\"dialogue_entity_adversarial_knowledge_attacks\",\"dialogue_languages_evaluation_language_summarization\",\"dialogue_languages_dataset_language_datasets\",\"transformer_reasoning_parameters_prompt_tasks\",\"proprietary_opensourced_counterfactuals_scone_utility\",\"chatgpt_sarcasm_responses_chatgpts_instructional\",\"chatgpt_sarcasm_responses_proprietary_argumentative\",\"language_work_languages_dataset_different\"],\"marker\":{\"color\":\"black\"},\"mode\":\"markers\",\"showlegend\":false,\"x\":[0.7446097655702633,0.8017204244959464,0.7096204726257432,0.6581083274486548,0.6961650652575639,0.7635629156140754,0.778580047877181,0.8411370750068082,0.8614370380894582,0.870393915381184,0.8840210630899525,0.9149499516735708,0.8977144464239208],\"y\":[-32.5,-29.375,-113.75,-180.0,-160.0,-98.59375,-136.171875,-122.79296875,-26.09375,-262.5,-232.5,-253.125,-97.4951171875],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"transformer_nat_transformers_parameters_decoding\",\"reasoning_transformer_parameters_transformers_nat\",\"reasoning_transformer_prompt_parameters_transformers\",\"entity_visual_image_entities_multimodal\",\"dialogue_entity_visual_knowledge_graph\",\"dialogue_entity_visual_knowledge_multimodal\",\"dialogue_entity_adversarial_knowledge_attacks\",\"translation_languages_bias_gender_social\",\"languages_summarization_bias_translation_gender\",\"dialogue_languages_dataset_language_evaluation\",\"dialogue_languages_dataset_language_evaluation\",\"chatgpt_responses_chatgpts_instructional_teaching\",\"proprietary_opensourced_oosf_utility_recommendations\",\"argumentative_counterfactual_proprietary_opensourced_counterfactuals\",\"dense_retrieval_retrievers_bm25_modification\",\"chatgpt_dense_sarcasm_retrieval_responses\"],\"marker\":{\"color\":\"black\"},\"mode\":\"markers\",\"showlegend\":false,\"x\":[0.7255787319866616,0.7914313961840043,0.8095775129523718,0.6706588158177551,0.6928733007615606,0.7256704614267191,0.7385433705428008,0.6921685551212133,0.7333898402218662,0.8022546573931936,0.8702550326444013,0.835989313844496,0.8486238184475704,0.887108132427792,0.8710438876492081,0.9316453510054099],\"y\":[-40.0,-43.75,-47.1875,-130.0,-122.5,-129.375,-112.1875,-187.5,-173.75,-170.5859375,-168.896484375,-240.0,-270.0,-273.75,-300.0,-276.5625],\"type\":\"scatter\"}],                        {\"autosize\":false,\"height\":665,\"hovermode\":\"closest\",\"showlegend\":false,\"width\":1000,\"xaxis\":{\"mirror\":\"allticks\",\"rangemode\":\"tozero\",\"showgrid\":false,\"showline\":true,\"showticklabels\":true,\"ticks\":\"outside\",\"type\":\"linear\",\"zeroline\":false},\"yaxis\":{\"mirror\":\"allticks\",\"rangemode\":\"tozero\",\"showgrid\":false,\"showline\":true,\"showticklabels\":true,\"tickmode\":\"array\",\"ticks\":\"outside\",\"ticktext\":[\"21_tables_table_tableqa\",\"19_zeroshot_fewshot_verbali...\",\"15_classification_oov_slash\",\"5_hyperparameters_metaretri...\",\"11_transformer_nat_transfor...\",\"13_reasoning_pangu_symbolic\",\"18_prompts_prompt_continuous\",\"14_speech_asr_s2st\",\"17_explanations_explanation...\",\"6_adversarial_attacks_attack\",\"8_sentiment_emotion_emotional\",\"3_dialogue_conversations_di...\",\"1_entity_entities_knowledge\",\"2_visual_image_multimodal\",\"10_qa_question_questions\",\"4_clinical_medical_patients\",\"7_summarization_summaries_s...\",\"0_translation_languages_mul...\",\"9_gender_bias_debiasing\",\"12_political_moral_hate\",\"16_event_events_eae\",\"23_figurative_metaphors_soc...\",\"24_sarcasm_irony_humor\",\"20_chatgpt_chatgpts_rewriting\",\"29_instructional_teaching_s...\",\"30_counterfactuals_scone_co...\",\"27_proprietary_opensourced_...\",\"22_oosf_instances_proxy\",\"26_argumentative_environmen...\",\"25_dense_retrieval_retrievers\",\"28_modification_mp2_multiword\"],\"tickvals\":[-5.0,-15.0,-25.0,-35.0,-45.0,-55.0,-65.0,-75.0,-85.0,-95.0,-105.0,-115.0,-125.0,-135.0,-145.0,-155.0,-165.0,-175.0,-185.0,-195.0,-205.0,-215.0,-225.0,-235.0,-245.0,-255.0,-265.0,-275.0,-285.0,-295.0,-305.0],\"type\":\"linear\",\"zeroline\":false,\"range\":[-310.0,0.0]},\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eHierarchical Clustering\\u003c\\u002fb\\u003e\",\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"plot_bgcolor\":\"#ECEFF1\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('bcdad4aa-8687-4951-aaf4-296cfa961d9b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text based topic tree\n",
        "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
        "print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e3eaec-23e2-4c0e-84ef-c01380b457a9",
        "id": "VEdJUohb9KdZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "├─language_work_languages_dataset_different\n",
            "│    ├─transformer_reasoning_parameters_prompt_tasks\n",
            "│    │    ├─■──tables_table_tableqa_cells_ctbls ── Topic: 21\n",
            "│    │    └─reasoning_transformer_prompt_parameters_transformers\n",
            "│    │         ├─reasoning_transformer_parameters_transformers_cost\n",
            "│    │         │    ├─■──zeroshot_fewshot_verbalizers_ss_bots ── Topic: 19\n",
            "│    │         │    └─reasoning_transformer_parameters_transformers_nat\n",
            "│    │         │         ├─transformer_parameters_transformers_nat_cost\n",
            "│    │         │         │    ├─■──classification_oov_slash_hypermixer_mlpbased ── Topic: 15\n",
            "│    │         │         │    └─transformer_nat_transformers_parameters_decoding\n",
            "│    │         │         │         ├─■──hyperparameters_metaretriever_pts_radius_optimization ── Topic: 5\n",
            "│    │         │         │         └─■──transformer_nat_transformers_decoding_encoder ── Topic: 11\n",
            "│    │         │         └─■──reasoning_pangu_symbolic_rule_thinksum ── Topic: 13\n",
            "│    │         └─■──prompts_prompt_continuous_preadd_mt0 ── Topic: 18\n",
            "│    └─dialogue_languages_dataset_language_evaluation\n",
            "│         ├─dialogue_languages_dataset_language_datasets\n",
            "│         │    ├─■──speech_asr_s2st_voice_lyrics ── Topic: 14\n",
            "│         │    └─dialogue_languages_dataset_language_evaluation\n",
            "│         │         ├─dialogue_languages_evaluation_language_summarization\n",
            "│         │         │    ├─dialogue_entity_adversarial_knowledge_attacks\n",
            "│         │         │    │    ├─■──explanations_explanation_concepts_reasoning_cpace ── Topic: 17\n",
            "│         │         │    │    └─dialogue_entity_adversarial_knowledge_attacks\n",
            "│         │         │    │         ├─■──adversarial_attacks_attack_backdoor_al ── Topic: 6\n",
            "│         │         │    │         └─dialogue_entity_visual_knowledge_multimodal\n",
            "│         │         │    │              ├─dialogue_entity_visual_graph_knowledge\n",
            "│         │         │    │              │    ├─■──sentiment_emotion_emotional_wer_reviews ── Topic: 8\n",
            "│         │         │    │              │    └─dialogue_entity_visual_knowledge_graph\n",
            "│         │         │    │              │         ├─■──dialogue_conversations_dialog_dialogues_multiparty ── Topic: 3\n",
            "│         │         │    │              │         └─entity_visual_image_entities_multimodal\n",
            "│         │         │    │              │              ├─■──entity_entities_knowledge_graph_kg ── Topic: 1\n",
            "│         │         │    │              │              └─■──visual_image_multimodal_scene_images ── Topic: 2\n",
            "│         │         │    │              └─■──qa_question_questions_answer_qe ── Topic: 10\n",
            "│         │         │    └─languages_summarization_bias_translation_gender\n",
            "│         │         │         ├─summarization_clinical_medical_summaries_patients\n",
            "│         │         │         │    ├─■──clinical_medical_patients_notes_biomedical ── Topic: 4\n",
            "│         │         │         │    └─■──summarization_summaries_summary_factual_msmo ── Topic: 7\n",
            "│         │         │         └─translation_languages_bias_gender_social\n",
            "│         │         │              ├─translation_languages_gender_bias_debiasing\n",
            "│         │         │              │    ├─■──translation_languages_multilingual_translations_crosslingual ── Topic: 0\n",
            "│         │         │              │    └─■──gender_bias_debiasing_harms_social ── Topic: 9\n",
            "│         │         │              └─■──political_moral_hate_social_media ── Topic: 12\n",
            "│         │         └─■──event_events_eae_argument_trigger ── Topic: 16\n",
            "│         └─■──figurative_metaphors_sociocultural_metaphor_metaphorical ── Topic: 23\n",
            "└─chatgpt_dense_sarcasm_retrieval_responses\n",
            "     ├─chatgpt_sarcasm_responses_proprietary_argumentative\n",
            "     │    ├─chatgpt_sarcasm_responses_chatgpts_instructional\n",
            "     │    │    ├─■──sarcasm_irony_humor_ocr_yorker ── Topic: 24\n",
            "     │    │    └─chatgpt_responses_chatgpts_instructional_teaching\n",
            "     │    │         ├─■──chatgpt_chatgpts_rewriting_responses_query ── Topic: 20\n",
            "     │    │         └─■──instructional_teaching_students_moves_ncte ── Topic: 29\n",
            "     │    └─argumentative_counterfactual_proprietary_opensourced_counterfactuals\n",
            "     │         ├─proprietary_opensourced_counterfactuals_scone_utility\n",
            "     │         │    ├─■──counterfactuals_scone_counterfactual_rev_negation ── Topic: 30\n",
            "     │         │    └─proprietary_opensourced_oosf_utility_recommendations\n",
            "     │         │         ├─■──proprietary_opensourced_recommendations_utility_eccnps ── Topic: 27\n",
            "     │         │         └─■──oosf_instances_proxy_label_false ── Topic: 22\n",
            "     │         └─■──argumentative_environmental_firms_argument_episode ── Topic: 26\n",
            "     └─dense_retrieval_retrievers_bm25_modification\n",
            "          ├─■──dense_retrieval_retrievers_bm25_recontriever ── Topic: 25\n",
            "          └─■──modification_mp2_multiword_pram_pluglm ── Topic: 28\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the visualization with the original embeddings\n",
        "topic_model.visualize_hierarchical_documents(df_filtered_doc['Text'], hierarchical_topics, embeddings=embeddings)\n",
        "\n",
        "# Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
        "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
        "topic_model.visualize_hierarchical_documents(df_filtered_doc['Text'], hierarchical_topics, reduced_embeddings=reduced_embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        },
        "outputId": "322a7736-556d-4e73-c00e-6637e39456f6",
        "id": "P4QmiPIr9KdZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"75c17da4-f863-4bed-bd58-8ba3c821cc29\" class=\"plotly-graph-div\" style=\"height:750px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"75c17da4-f863-4bed-bd58-8ba3c821cc29\")) {                    Plotly.newPlot(                        \"75c17da4-f863-4bed-bd58-8ba3c821cc29\",                        [{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"#CFD8DC\",\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"other\",\"showlegend\":false,\"x\":[12.260786056518555,10.01115894317627,11.924004554748535,10.16279411315918,12.00646686553955,10.273680686950684,6.795833110809326,10.738099098205566,12.415397644042969,12.344215393066406,8.005266189575195,11.672365188598633,9.678071022033691,11.613967895507812,10.691057205200195,10.729043006896973,12.570730209350586,12.728907585144043,11.094691276550293,12.467524528503418,10.7769136428833,11.365942001342773,11.85193157196045,10.574968338012695,13.300118446350098,8.84424877166748,9.322251319885254,11.329916000366211,9.31579303741455,10.838811874389648,11.459781646728516,11.970154762268066,12.745402336120605,10.581709861755371,9.499332427978516,10.223129272460938,9.936136245727539,11.424376487731934,7.634429931640625,12.565068244934082,12.609332084655762,11.734045028686523,12.76197338104248,8.449498176574707,10.840675354003906,11.946686744689941,8.837322235107422,9.319323539733887,12.509225845336914,10.419879913330078,6.7955498695373535,9.324790954589844,12.908655166625977,11.152015686035156,11.986949920654297,12.218483924865723,11.286534309387207,11.993667602539062,9.96347427368164,11.670063018798828,9.893813133239746,10.930827140808105,9.840456008911133,9.265555381774902,8.651935577392578,9.824747085571289,9.816277503967285,12.087455749511719,11.567899703979492,6.796971321105957,12.668866157531738,12.218778610229492,10.99422836303711,11.2903470993042,13.343911170959473,12.024370193481445,12.494938850402832,12.23508071899414,10.537565231323242,9.071687698364258,9.39167308807373,13.654808044433594,11.47091293334961,11.624275207519531,12.246249198913574,12.476778984069824,11.365124702453613,12.479470252990723,12.073331832885742,10.643670082092285,10.698538780212402,11.801766395568848,11.808372497558594,9.954280853271484,12.55838394165039,8.872209548950195,11.538613319396973,8.866655349731445,10.459000587463379,11.395777702331543,9.855978012084961,11.591615676879883,10.30197811126709,11.654211044311523,11.18188190460205,10.506263732910156,13.050704002380371,11.941532135009766,12.512845993041992,11.652451515197754,13.646256446838379,11.867741584777832,12.657735824584961,12.094124794006348,10.286686897277832,12.581634521484375,11.336400985717773,9.937158584594727,11.021777153015137,9.948188781738281,12.35839557647705,12.886040687561035,9.261913299560547,9.89763355255127,9.462775230407715,9.968618392944336,12.362512588500977,10.358255386352539,8.327774047851562,11.927535057067871,10.00265884399414,12.349808692932129,10.623787879943848,10.678670883178711,11.847443580627441,9.692587852478027,9.262121200561523,10.958932876586914,12.889772415161133,10.948387145996094,11.634035110473633,12.080227851867676,10.378564834594727,10.096247673034668,11.42621898651123,8.7933931350708,11.231094360351562,8.999340057373047,11.527088165283203,12.19135570526123,7.8714375495910645,11.001914978027344,12.61523151397705,9.75306224822998,11.896245002746582,12.523083686828613,9.378335952758789,10.35689926147461,11.08590316772461,13.147154808044434,9.818265914916992,11.11213493347168,11.444499969482422,11.590463638305664,10.737903594970703,9.770731925964355,10.78557014465332,11.933521270751953,10.511313438415527,9.82026481628418,8.356404304504395,10.73415470123291,10.45881462097168,10.396477699279785,12.194050788879395,12.752180099487305,11.78748607635498,10.255226135253906,12.165645599365234,12.292634963989258,12.844697952270508,10.628177642822266,8.569395065307617,11.763611793518066,12.15890884399414,12.740022659301758,12.746966361999512,9.62110424041748,11.45886516571045,10.926743507385254,12.772931098937988,12.837591171264648,12.200713157653809,9.013678550720215,10.967081069946289,9.76983642578125,12.884422302246094,11.670434951782227,13.589189529418945,12.576118469238281,11.409375190734863,9.541470527648926,8.66622257232666,11.438347816467285,11.34601879119873,10.590363502502441,11.35364818572998,11.930695533752441,6.797292709350586,10.794591903686523,10.309884071350098,10.247868537902832,9.914916038513184,13.303763389587402,12.540189743041992,11.354313850402832,8.383038520812988,12.193739891052246,9.227989196777344,9.241358757019043,9.536521911621094,10.984012603759766,12.313669204711914,9.644474029541016,9.546388626098633,9.345878601074219,11.460395812988281,10.699294090270996,9.629450798034668,8.48170280456543,11.0349760055542,10.570517539978027,10.367918014526367,11.483748435974121,10.360127449035645,10.180638313293457,12.789478302001953,10.665085792541504,11.708563804626465,12.97873592376709,11.337641716003418,10.709122657775879,13.167486190795898,12.409531593322754,8.32143497467041,9.063648223876953,10.371134757995605,11.557292938232422,10.586905479431152,9.22173023223877,11.61303424835205,8.834911346435547,13.149587631225586,11.683677673339844,9.396690368652344,11.555591583251953,11.163105964660645,10.546306610107422,13.552511215209961,12.85168170928955,11.200281143188477,9.939804077148438,10.729233741760254,12.406366348266602,11.353873252868652,11.169713973999023,9.460506439208984,10.654193878173828,11.77177906036377,11.467199325561523,12.088428497314453,13.498458862304688,11.279095649719238,12.567978858947754,10.763456344604492,10.033524513244629,12.673567771911621,10.10020637512207,8.970285415649414,9.142569541931152,11.128690719604492,12.259600639343262,8.503023147583008,11.229401588439941,9.610413551330566,10.481382369995117,12.30782699584961,11.052600860595703,13.069833755493164,9.056635856628418,9.835722923278809,11.540289878845215,10.57509994506836,10.630483627319336,12.53976058959961,12.878406524658203,12.195188522338867,12.416590690612793,8.807579040527344,12.123668670654297,10.993809700012207,11.357285499572754,11.609417915344238,8.505378723144531,9.010087966918945,9.596749305725098,12.824377059936523,10.293586730957031,11.473138809204102,12.474746704101562,10.027557373046875,12.210655212402344,10.91020393371582,13.3267240524292,13.175515174865723,11.291525840759277,11.628263473510742,11.242234230041504,9.939929008483887,12.68603515625,9.105117797851562,12.55679702758789,10.582772254943848,11.557234764099121,12.54797649383545,11.068909645080566,12.6818208694458,11.577909469604492,11.18771743774414,13.261770248413086,11.224922180175781,11.866551399230957,9.587552070617676,10.551297187805176,11.903278350830078,12.07540225982666,10.768095970153809,12.016518592834473,8.86601448059082,12.697779655456543,12.205658912658691,10.315733909606934,11.997912406921387,12.761860847473145,11.485032081604004,12.877477645874023,10.77319622039795,9.71633529663086,12.789429664611816,10.579119682312012,12.169694900512695,11.369839668273926,12.047872543334961,11.594593048095703,11.944458961486816,11.05200481414795,9.081618309020996,12.835399627685547,12.314804077148438,9.093341827392578,11.577603340148926,9.215981483459473,11.374435424804688,12.266894340515137,11.160942077636719,8.430356979370117,12.54307746887207,11.239245414733887,10.539368629455566,11.465906143188477,7.657243728637695,12.263589859008789,9.044021606445312,11.270294189453125,9.69029426574707,12.90316104888916,11.74852466583252,8.697430610656738,10.244534492492676,13.194223403930664,8.26854419708252,12.263144493103027,12.15650463104248,10.97741985321045,11.557283401489258,12.207938194274902,9.880388259887695,13.715070724487305,8.669084548950195,11.517677307128906,12.063190460205078,10.891190528869629,8.157964706420898,13.416830062866211,12.172284126281738,9.77486515045166,12.416096687316895,8.317665100097656,12.529857635498047,10.84004020690918,8.873795509338379,12.759370803833008,10.572245597839355,11.383156776428223,11.408194541931152,12.709611892700195,13.176079750061035,7.645970344543457,10.45201587677002,11.888107299804688,8.155491828918457,10.934144973754883,12.047857284545898,11.55852222442627,10.887688636779785,10.773085594177246,10.705704689025879,9.601393699645996,10.455825805664062,9.596738815307617,10.703749656677246,11.308526992797852,9.27015209197998,10.01099681854248,12.847784996032715,12.372241020202637,10.8928804397583,11.546853065490723,9.608847618103027,10.049355506896973,10.234009742736816,12.77387809753418,7.639250755310059,9.369508743286133,11.111124992370605,10.788805961608887,9.64129638671875,10.971961975097656,13.150918960571289,10.595973014831543,9.54521369934082,12.856410026550293,10.756369590759277,11.570320129394531,10.633820533752441,10.61214542388916,11.196186065673828,11.974361419677734,11.977964401245117,9.437406539916992,11.594237327575684,11.464212417602539,10.141727447509766,11.141068458557129,9.565552711486816,12.504912376403809,10.439518928527832,8.916755676269531,12.542708396911621,8.100579261779785,13.614336013793945,9.942835807800293,12.88668441772461,12.289319038391113,10.191901206970215,11.653940200805664,12.273872375488281,9.7015380859375,11.526108741760254,11.894497871398926,9.942673683166504,12.299449920654297,11.311140060424805,12.881195068359375,8.470643043518066,12.338445663452148,11.502923011779785,10.742378234863281,12.947842597961426,11.659990310668945,12.496929168701172,8.366921424865723,9.1043119430542,12.329166412353516,11.74715518951416,11.585480690002441,10.612318992614746,11.377769470214844,11.649738311767578,11.907842636108398,13.692667007446289,11.34955883026123,12.52751636505127,8.80586051940918,12.406229019165039,12.7645902633667,12.445311546325684,11.75776195526123,11.090094566345215,11.42638111114502,10.89482593536377,10.761418342590332,11.150059700012207,10.034558296203613,10.338700294494629,11.178735733032227,12.835885047912598,11.53531551361084,12.381514549255371,11.54314136505127,11.90941047668457,9.358724594116211,10.740930557250977,11.939253807067871,11.149989128112793,11.659534454345703,10.353327751159668,11.331498146057129,10.64175796508789,12.81151008605957,8.662858009338379,10.857192993164062,10.787764549255371,8.953280448913574,12.585387229919434,7.643540382385254,10.447787284851074,10.80845832824707,12.5751371383667,9.113858222961426,9.70373821258545,12.737845420837402,11.416006088256836,12.489952087402344,8.133481979370117,10.278565406799316,11.977149963378906,9.470913887023926,10.66490364074707,10.00718879699707,9.176525115966797,11.815064430236816,8.752781867980957,11.483132362365723,11.89669418334961,12.553421020507812,13.118978500366211,9.782742500305176,10.74406623840332,12.168991088867188,11.756896018981934,12.940293312072754,11.56460189819336,8.64371109008789,11.702585220336914,11.278419494628906,10.558026313781738,11.820279121398926,10.459695816040039,11.502100944519043,11.530735969543457,10.50390911102295,12.36990737915039,12.139140129089355,11.131754875183105,10.48403263092041,13.041622161865234,11.032854080200195,11.734776496887207,10.479324340820312,9.945302963256836,11.825505256652832,11.676788330078125,9.411858558654785,12.318131446838379,11.462925910949707,12.87320613861084,13.661906242370605,12.474398612976074,10.583564758300781,11.804007530212402,11.14046573638916,11.841476440429688,11.781903266906738,9.183506965637207,11.8534574508667,11.49117660522461,11.63294506072998,11.224637985229492,12.764777183532715,12.330574035644531,11.128299713134766,12.93774127960205,10.32089614868164,12.478330612182617,11.827890396118164,11.804841041564941,11.44624137878418,11.820039749145508,10.238183975219727,7.784416675567627,12.978934288024902,12.451269149780273,9.322071075439453,11.821974754333496,10.541068077087402,11.961499214172363,8.238553047180176,12.170052528381348,12.736010551452637,12.764181137084961,11.65296745300293,12.675060272216797,11.974373817443848,10.117522239685059,9.6543607711792,12.757356643676758,12.056167602539062,9.412654876708984,11.711962699890137,9.174324989318848,10.823249816894531,9.90158462524414,10.913339614868164,8.113393783569336,9.993379592895508,12.194329261779785,13.33237361907959,11.182921409606934,12.732467651367188,13.046422004699707,9.75806713104248,8.685029029846191,11.561057090759277,10.053182601928711,11.998501777648926,13.20248031616211,12.104700088500977,10.906420707702637,11.117636680603027,10.446064949035645,9.715156555175781,12.530933380126953,12.754565238952637,11.461576461791992,11.491990089416504,9.396021842956543,8.587562561035156,12.263270378112793,11.42301082611084,13.491022109985352,10.364217758178711,11.991475105285645,12.44273853302002,11.70559310913086,10.35807991027832,9.996321678161621,9.326005935668945,12.807846069335938,13.570623397827148,11.12037467956543,10.716169357299805,10.50700569152832,10.22071361541748,12.695380210876465,11.297102928161621,9.085335731506348,12.861201286315918,11.170536994934082,10.041293144226074,10.805912017822266,11.3994722366333,9.783839225769043,11.411025047302246,10.608845710754395,9.42233657836914,11.715644836425781,11.849699020385742,12.349161148071289,8.975476264953613,10.749741554260254,9.420133590698242,10.120325088500977,12.375459671020508,10.686391830444336,11.969694137573242,11.85922622680664,12.443221092224121,9.37954330444336,13.088716506958008,10.281312942504883,12.127240180969238,11.985560417175293,12.442952156066895,12.119172096252441,9.390953063964844,10.271842956542969,10.183282852172852,12.758428573608398,9.88254451751709,11.704732894897461,11.48423957824707,9.189234733581543,11.172453880310059,11.48064136505127,10.315546035766602,12.061239242553711,11.559906005859375,11.86010456085205,11.664885520935059,12.843629837036133,9.624467849731445,12.458562850952148,12.246566772460938,11.838500022888184,10.999716758728027,9.12990951538086,11.840607643127441,11.96013069152832,11.915284156799316,12.08629035949707,10.903203964233398,8.849540710449219,11.167780876159668,10.531004905700684,12.467206954956055,10.145415306091309,9.549249649047852,6.795407295227051,12.180326461791992,12.817878723144531,10.757889747619629,11.568016052246094,11.672552108764648,7.97341775894165,10.022518157958984,9.927380561828613,10.70942211151123,11.941184043884277,12.794625282287598,12.879822731018066,12.178878784179688,11.878340721130371,10.52446460723877,10.071585655212402,10.56197738647461,11.002744674682617,12.245721817016602,10.67922592163086,12.140796661376953,12.277287483215332,11.456153869628906,12.081787109375,11.62030029296875,10.880305290222168,10.69717025756836,9.24069881439209,12.846972465515137,10.142430305480957,12.146927833557129,9.958215713500977,9.50244140625,10.51760196685791,10.551128387451172,11.390852928161621,11.688502311706543,11.491974830627441,12.741220474243164,12.295635223388672,12.218851089477539,8.685344696044922,13.047622680664062,11.958925247192383,9.383042335510254,11.218199729919434,11.51757526397705,9.624102592468262,13.13804817199707,12.326669692993164,12.624595642089844,12.161945343017578,9.43790054321289,11.324953079223633,8.503750801086426,11.257233619689941,10.58021354675293,9.62783145904541,9.703179359436035,11.924786567687988,8.906972885131836,9.254066467285156,12.899577140808105,11.277064323425293,11.152223587036133,12.76430606842041,10.148778915405273,12.378602027893066,11.560495376586914,12.818056106567383,12.91047191619873,8.343107223510742,10.998586654663086,11.666386604309082,10.141661643981934,11.702606201171875,10.432024955749512,10.108580589294434,9.184564590454102,13.428140640258789,11.512848854064941,12.044175148010254,11.576578140258789,12.28610610961914,11.385955810546875,12.952836036682129,12.021284103393555,11.287923812866211,12.099640846252441,12.082901000976562,12.028682708740234,12.383368492126465,8.773681640625,12.879551887512207,11.319714546203613,8.952622413635254,11.267468452453613,12.477086067199707,12.503398895263672,9.42236614227295,11.20785140991211,11.332497596740723,8.951685905456543,10.856745719909668,10.309080123901367,11.73598861694336,8.723488807678223,12.038217544555664,9.878068923950195,12.995759963989258,10.974395751953125,9.64431381225586,10.158196449279785,13.631218910217285,8.542502403259277,11.165329933166504,10.789525032043457,13.625770568847656,11.960083961486816,12.425254821777344,11.940552711486816,13.470690727233887,13.038926124572754,7.834033012390137,9.87307071685791,11.216033935546875,12.414644241333008,12.093451499938965,11.877202987670898,12.003032684326172,10.026086807250977,12.491192817687988,13.477025032043457,12.77425479888916,10.46001148223877,8.880654335021973,9.948962211608887,10.48256778717041,12.090274810791016,11.583839416503906,11.40567684173584,6.7874345779418945,9.088058471679688,11.758285522460938,11.31846809387207,11.73054313659668,12.907270431518555,12.770581245422363,11.114065170288086,11.227639198303223,10.648099899291992,12.066269874572754,13.3859224319458,12.3936767578125,11.536118507385254,12.09136962890625,7.971031665802002,10.897802352905273,9.173691749572754,11.609256744384766,12.269068717956543,9.517958641052246,11.856656074523926,11.710532188415527,12.882142066955566,12.293147087097168,9.226627349853516,11.812376022338867,9.573324203491211,11.98694896697998,11.025315284729004,12.620606422424316,12.66535758972168,11.626893997192383,11.87524127960205,9.740761756896973,11.66956615447998,9.677865982055664,13.708874702453613,13.700621604919434,10.8056058883667,12.719548225402832,11.30461597442627,9.15710163116455,11.5343599319458,9.219347953796387,11.886124610900879,11.679952621459961,12.069474220275879,12.361296653747559,10.542925834655762,10.790267944335938,12.395968437194824,10.26074504852295,12.145330429077148,11.74125862121582,12.40096664428711,10.930389404296875,8.466429710388184,11.920580863952637,12.118764877319336,13.445467948913574,10.998306274414062,11.986994743347168,12.903426170349121,11.159873962402344,12.18824291229248,9.372452735900879,12.712847709655762,13.078712463378906,10.631550788879395,9.169002532958984,8.790238380432129,11.811575889587402,12.28490924835205,9.769627571105957,12.567846298217773,10.349342346191406,11.191412925720215,11.537104606628418,10.127388000488281,10.513239860534668,8.915346145629883,11.823579788208008,11.909771919250488,11.520251274108887,13.776591300964355,12.594861030578613,6.803589344024658,10.791962623596191,11.956903457641602,7.651430606842041,12.864334106445312,11.632024765014648,10.91291332244873,11.907623291015625,10.763083457946777,11.546504974365234,13.262833595275879,12.147619247436523,11.872856140136719,12.41122055053711,12.41154670715332,13.054023742675781,10.615890502929688,9.164365768432617],\"y\":[1.976219654083252,1.265775442123413,1.0926563739776611,-0.5182633996009827,2.4570963382720947,0.48775091767311096,-0.910475492477417,-0.5370233654975891,1.2164555788040161,1.3136889934539795,-0.12520691752433777,1.813036561012268,1.1669845581054688,1.774245023727417,-0.3897693157196045,1.3097110986709595,3.413337469100952,1.671027660369873,0.8366751074790955,3.2766458988189697,0.3775566816329956,0.44715550541877747,2.3352620601654053,3.230710506439209,2.2592904567718506,0.6792904138565063,0.25117701292037964,0.6122933626174927,1.182349681854248,0.9189866781234741,3.0709445476531982,0.9211485385894775,3.500269889831543,3.7989087104797363,1.823943853378296,1.6083993911743164,-0.23820368945598602,2.4324207305908203,0.19927099347114563,3.7869391441345215,2.054600715637207,0.9205240607261658,3.078073263168335,0.10503587871789932,0.8371968865394592,0.3198699951171875,-0.16870951652526855,2.7976632118225098,2.6678123474121094,-0.570141077041626,-0.9106079339981079,3.7905776500701904,2.7144033908843994,0.9463844299316406,1.3737200498580933,0.33007562160491943,-0.9850686192512512,1.1608271598815918,1.0532095432281494,2.060023546218872,1.5419471263885498,1.308414101600647,-1.1935114860534668,-0.17752167582511902,-0.4857664108276367,3.832371234893799,1.3047598600387573,3.552154541015625,1.942522644996643,-0.909134566783905,2.3067994117736816,0.9871791005134583,2.9792444705963135,2.830186128616333,0.7007690072059631,3.5236191749572754,3.808678388595581,0.5856729745864868,0.5755625367164612,-0.488295316696167,0.6351603269577026,0.5080202221870422,1.0401160717010498,2.3617639541625977,1.8510946035385132,0.9317828416824341,3.2759201526641846,2.4336321353912354,-1.299113392829895,3.8617777824401855,3.202488422393799,1.037284016609192,-0.5317288637161255,-0.5715892314910889,1.2100167274475098,1.357236623764038,3.533149480819702,3.0465500354766846,0.1627165526151657,0.7375492453575134,0.9511770009994507,0.07118048518896103,-0.3919447958469391,1.378462314605713,-0.3244394361972809,0.7082329988479614,2.143453359603882,1.1407673358917236,0.9031526446342468,2.8212311267852783,1.1145505905151367,1.4429399967193604,2.296320676803589,-0.001279126270674169,-0.10712704062461853,3.0526630878448486,0.7859264612197876,-0.6041418313980103,1.40556800365448,-0.5446928143501282,1.2216320037841797,3.4056971073150635,3.86862850189209,1.1494468450546265,0.1541697382926941,-0.2646180987358093,2.934096097946167,1.085005283355713,0.1298372447490692,1.4595853090286255,0.5229455232620239,1.7139097452163696,3.179950475692749,0.8122704029083252,2.313836097717285,0.9151896834373474,0.0694771260023117,2.251661777496338,2.7393033504486084,1.8044404983520508,2.4212090969085693,1.369358777999878,1.4411309957504272,1.778269648551941,1.9663785696029663,-0.2085247039794922,1.8884222507476807,0.01605393923819065,0.9991214275360107,2.1229145526885986,-0.17486093938350677,1.2444441318511963,1.5270284414291382,1.034622311592102,0.5874497890472412,1.0864499807357788,0.7937063574790955,0.2653231918811798,0.6648417711257935,2.1787331104278564,-1.4033302068710327,0.993794858455658,3.9591336250305176,2.2620389461517334,2.2779126167297363,1.026957631111145,1.6402356624603271,1.106472134590149,3.609027624130249,4.001310348510742,2.4320061206817627,1.3439748287200928,1.444767951965332,0.8551923036575317,0.15037234127521515,2.425750494003296,2.3774797916412354,2.089541435241699,0.14875546097755432,1.2632057666778564,1.3453816175460815,3.9026010036468506,0.47671282291412354,1.526449203491211,1.530004620552063,0.44732722640037537,1.5337772369384766,-0.5403087735176086,-1.0967426300048828,0.833954393863678,0.7645677924156189,2.70750093460083,1.2675151824951172,0.1613716185092926,3.286445379257202,0.9860133528709412,3.4668221473693848,1.4706974029541016,-0.06515169888734818,3.346012830734253,3.19698166847229,1.0387732982635498,-0.47503626346588135,1.1350661516189575,-0.12550556659698486,0.48462581634521484,3.611987352371216,0.44945693016052246,-0.9105373620986938,0.9302946925163269,2.0090830326080322,0.1268618106842041,-0.5888694524765015,1.5191290378570557,2.7231905460357666,3.63657283782959,0.20960384607315063,0.04279949516057968,2.0899455547332764,2.395681142807007,-0.3550689220428467,1.1873329877853394,1.6772342920303345,-0.551196813583374,1.0045689344406128,3.5376405715942383,-0.2728564739227295,1.6083232164382935,3.451906681060791,0.4097013771533966,0.28619876503944397,2.9995458126068115,1.430558204650879,2.9486751556396484,1.4180095195770264,3.394707679748535,-1.0172475576400757,1.6186503171920776,2.2325010299682617,3.3960022926330566,2.3339428901672363,2.508234739303589,1.917661428451538,2.007206678390503,0.7356185913085938,1.3998340368270874,1.426846981048584,-0.10650590807199478,3.196619987487793,1.5687898397445679,3.3810524940490723,-0.20458725094795227,1.947638988494873,-0.17957328259944916,0.6638663411140442,1.4990065097808838,1.0839821100234985,-0.13390646874904633,1.798355221748352,1.460438847541809,2.6365201473236084,2.2331058979034424,0.5038972496986389,1.672692894935608,3.618595838546753,2.4914488792419434,-0.7637554407119751,2.602285861968994,1.6152315139770508,2.3165502548217773,4.036905288696289,1.7626341581344604,-0.30745241045951843,1.3061516284942627,3.0544943809509277,2.2432503700256348,0.5260573625564575,0.26869526505470276,0.03409036993980408,-0.5223769545555115,0.39339277148246765,2.0766706466674805,-0.38802358508110046,3.801676034927368,2.3770365715026855,3.5236270427703857,3.0124449729919434,3.137105703353882,3.03592848777771,1.0055789947509766,2.3000667095184326,3.485448122024536,2.8649325370788574,3.2783849239349365,2.067582130432129,1.7887386083602905,1.117841124534607,1.9235390424728394,-0.5475263595581055,2.5567400455474854,-0.42588913440704346,1.6459540128707886,3.0523805618286133,3.548125743865967,0.9675278663635254,1.0511583089828491,1.8744837045669556,-0.2662609815597534,2.020230531692505,2.395968198776245,1.5245238542556763,2.208904504776001,-0.23743867874145508,2.1941235065460205,3.066039800643921,0.8780515789985657,-0.3361736238002777,0.12655974924564362,0.5816671252250671,2.5916388034820557,1.8386311531066895,0.9971460103988647,-0.09463711082935333,1.4201574325561523,3.414994478225708,-0.5812013745307922,1.0859824419021606,3.207399845123291,1.0073139667510986,1.059942603111267,0.9441660642623901,2.421081066131592,-0.9699038863182068,-0.7918293476104736,3.2947211265563965,3.453516721725464,2.6633572578430176,1.12492835521698,0.9515612125396729,1.8676871061325073,2.2264864444732666,0.4588735103607178,-0.7277568578720093,-1.0359772443771362,2.857710599899292,2.6983368396759033,1.5439820289611816,0.8810650110244751,3.6186933517456055,3.788325071334839,2.0987229347229004,3.1248533725738525,-0.6990334391593933,3.2795536518096924,0.4755188822746277,2.3451409339904785,1.9918140172958374,2.239830255508423,2.980741500854492,3.4494681358337402,2.7023935317993164,0.8716931343078613,1.143310546875,0.12861472368240356,1.2377668619155884,-0.3922814130783081,3.3432180881500244,1.7890307903289795,0.9444081783294678,2.8529300689697266,0.18657220900058746,2.6263394355773926,0.48836591839790344,0.770980954170227,0.9084759950637817,2.9118480682373047,0.899668276309967,0.3940073549747467,0.8528211116790771,1.6868844032287598,0.24871914088726044,2.4808976650238037,2.190305709838867,1.4015684127807617,-0.11070117354393005,1.4967073202133179,0.9283192753791809,0.4603424668312073,1.1601427793502808,1.9638222455978394,-1.3094513416290283,1.8755643367767334,-0.034235879778862,2.5533201694488525,2.5379700660705566,3.77651309967041,2.733030319213867,0.3953371047973633,2.7038984298706055,1.0902845859527588,3.95662522315979,3.4667394161224365,1.4680428504943848,-0.2102063000202179,1.9835692644119263,0.25430288910865784,2.2354414463043213,0.19437259435653687,1.457194447517395,1.478986144065857,0.024254370480775833,0.9681276082992554,-1.2682996988296509,1.4637945890426636,0.9524343013763428,3.2517035007476807,1.0663663148880005,1.047045350074768,2.5798704624176025,1.9295272827148438,-0.4900413751602173,1.2294294834136963,3.744098663330078,1.1958093643188477,3.4060301780700684,1.4086087942123413,3.195308208465576,-0.21053600311279297,1.9594486951828003,1.3294121026992798,0.7901973128318787,-1.0279371738433838,0.18727345764636993,2.7890584468841553,1.1563249826431274,-0.1735643744468689,2.2202253341674805,0.9077820777893066,2.668337821960449,2.603579521179199,0.24776341021060944,1.158795714378357,1.6285470724105835,1.96036958694458,3.842803716659546,-0.05351594090461731,1.0603022575378418,3.5160486698150635,3.203173875808716,-0.22633570432662964,1.7337943315505981,1.1724951267242432,0.17315110564231873,0.7105343341827393,2.4431610107421875,3.8104262351989746,0.10712434351444244,0.713238537311554,3.1322901248931885,3.2645468711853027,0.534716784954071,0.5656581521034241,3.5241639614105225,2.5362460613250732,-0.7327529191970825,-0.8334300518035889,2.0500335693359375,0.9878270626068115,2.0549156665802,3.280917167663574,0.543793261051178,0.40602004528045654,1.2652292251586914,1.6847025156021118,0.6613606810569763,0.48743921518325806,1.5848220586776733,0.15714702010154724,3.097687244415283,3.4039037227630615,3.8253872394561768,-0.3216516077518463,0.36484676599502563,3.5613582134246826,0.849107563495636,2.804208278656006,3.8222780227661133,1.8004229068756104,1.4634736776351929,3.1937408447265625,0.8675617575645447,3.476331949234009,0.3727441728115082,3.1206748485565186,1.5812299251556396,-0.6251404285430908,3.3036246299743652,3.7486493587493896,1.7302109003067017,2.3743913173675537,0.9696566462516785,0.4081820845603943,0.6557876467704773,2.6788806915283203,3.448852300643921,1.163936972618103,3.5292248725891113,2.8257150650024414,3.515986680984497,1.9399545192718506,0.4077995717525482,0.766963541507721,-0.4135100543498993,0.42341694235801697,2.8088972568511963,-0.6076659560203552,0.1522863209247589,3.2291979789733887,1.588698387145996,1.8325613737106323,0.9248793125152588,1.3020033836364746,3.420180559158325,1.643860101699829,1.1540601253509521,0.21492479741573334,0.1739095002412796,1.5416576862335205,3.289651393890381,-0.5035665035247803,-0.46198034286499023,3.0262765884399414,3.655320405960083,1.8784151077270508,0.3909582197666168,0.8705004453659058,1.1364428997039795,1.208755612373352,0.26025456190109253,0.44635772705078125,0.12183551490306854,-0.15554407238960266,-0.25469839572906494,-0.45932525396347046,0.3801633417606354,2.1852316856384277,3.0103812217712402,0.16040968894958496,0.17387855052947998,2.1033482551574707,3.5950064659118652,1.6760215759277344,3.2061989307403564,0.3962271809577942,2.2076709270477295,-0.8994259834289551,2.514460802078247,0.979424774646759,3.5055930614471436,0.26043587923049927,0.08888838440179825,0.9722239375114441,1.9090890884399414,1.4592763185501099,0.7288630604743958,0.5196076035499573,2.52851939201355,0.3626737594604492,0.9660953879356384,1.4570801258087158,1.5067415237426758,1.0723236799240112,1.9673024415969849,0.5451234579086304,2.6414546966552734,0.7439343333244324,3.4168541431427,1.108274221420288,0.9759180545806885,3.0499231815338135,0.3342808485031128,1.1762615442276,1.2191663980484009,1.9902498722076416,2.7178072929382324,2.1931447982788086,1.4139740467071533,3.4444408416748047,-0.2844769358634949,2.468306541442871,2.982534408569336,1.1627116203308105,2.345379114151001,-0.26387879252433777,2.15932035446167,-0.09391316771507263,2.315988540649414,3.9019370079040527,1.4993696212768555,0.8209245204925537,0.1551581770181656,2.0490360260009766,1.3861424922943115,1.0227948427200317,-0.12519274652004242,3.113816976547241,1.2089561223983765,-0.26088747382164,1.8862637281417847,3.144066333770752,-1.0393073558807373,2.0746145248413086,1.443308711051941,1.180871844291687,-0.6809756755828857,1.017892599105835,1.8907074928283691,-1.2943123579025269,0.1569855809211731,0.6024736166000366,0.8391635417938232,2.21470046043396,-1.1607123613357544,1.8066606521606445,2.297105312347412,2.370729446411133,1.7525073289871216,0.9918195605278015,0.6040194034576416,2.8629562854766846,3.4195144176483154,1.8813725709915161,3.7637126445770264,0.20570334792137146,2.110116958618164,1.3198965787887573,1.0308681726455688,1.5510575771331787,1.0269742012023926,3.243104934692383,0.5696408748626709,-1.0560033321380615,3.80950927734375,-0.9885093569755554,2.1285574436187744,0.27239152789115906,-0.9925388693809509,2.664095401763916,0.5445898771286011,3.4431591033935547,2.9532716274261475,1.4311859607696533,2.9324100017547607,3.054269313812256,0.8955436944961548,0.025119181722402573,3.7716782093048096,0.6967297196388245,2.6491522789001465,2.385422945022583,3.098644495010376,3.803661823272705,1.4175082445144653,2.0697543621063232,3.0386083126068115,3.8905580043792725,1.805167555809021,3.370250940322876,0.6452329754829407,1.198987364768982,4.435891628265381,3.9789042472839355,0.2224683314561844,0.784505307674408,3.900918483734131,-0.08793401718139648,-0.20969590544700623,0.9383320212364197,2.9824304580688477,0.4146740138530731,3.451068878173828,0.58146733045578,2.629016876220703,2.5896897315979004,0.8220140933990479,2.19604229927063,2.3683815002441406,1.0513523817062378,-0.5379165410995483,2.0229125022888184,2.577913522720337,1.3024059534072876,2.953939914703369,1.1526473760604858,-1.245646595954895,0.6586165428161621,1.9431781768798828,0.795759916305542,0.4541209042072296,1.3323132991790771,0.9036451578140259,1.0493559837341309,2.936338424682617,1.658136010169983,2.7779719829559326,0.217825248837471,-0.08116167783737183,0.8917245864868164,1.92644202709198,1.5383085012435913,3.03080153465271,-0.5870935916900635,1.1749986410140991,1.7433806657791138,3.2300631999969482,1.3536816835403442,1.4494167566299438,2.132977247238159,1.142106533050537,1.0964027643203735,-1.2843916416168213,2.0309720039367676,0.38181358575820923,0.4180848002433777,1.397927165031433,1.0205312967300415,-1.0413587093353271,-0.3866738975048065,-0.9114582538604736,2.2314321994781494,-0.9689415693283081,0.8953730463981628,-0.7223781943321228,3.3937878608703613,-0.1200912594795227,1.4780992269515991,-1.0540677309036255,1.243822693824768,1.1921714544296265,-0.9881787300109863,2.7142724990844727,1.035949468612671,1.7600966691970825,3.096761703491211,1.0765949487686157,-0.3055479824542999,0.8458605408668518,1.8884222507476807,0.7532628178596497,3.692111015319824,2.047429084777832,1.5398664474487305,2.6263246536254883,3.3768866062164307,1.3354458808898926,2.8829421997070312,3.073280096054077,2.618222951889038,2.2130420207977295,1.131805658340454,1.4583265781402588,-0.6291894912719727,3.3120579719543457,3.235522985458374,2.4912800788879395,0.8031163215637207,2.785531520843506,1.2172937393188477,2.5934207439422607,0.44533485174179077,3.7607927322387695,0.8943837881088257,2.2763075828552246,0.15780554711818695,2.040842056274414,0.27494242787361145,3.4471356868743896,2.545928955078125,3.3168694972991943,3.057359218597412,2.0106849670410156,3.7272539138793945,-0.2229020744562149,-0.3912656009197235,2.5650482177734375,0.5276594758033752,1.9719535112380981,3.524745225906372,1.5316689014434814,0.3282936215400696,2.1106743812561035,2.4451394081115723,1.7900829315185547,2.8499183654785156,2.917431354522705,-0.6730669736862183,2.2880237102508545,2.3319878578186035,0.7816553711891174,3.079967975616455,0.3783358633518219,3.771744728088379,3.4605367183685303,3.4098503589630127,2.232668399810791,2.7577970027923584,0.8026202917098999,0.8486886024475098,1.336401104927063,1.9623465538024902,2.0950241088867188,2.3696439266204834,2.694972276687622,0.7788678407669067,1.5454350709915161,0.642559289932251,1.9104548692703247,2.817577838897705,1.2944279909133911,2.2089552879333496,2.9942898750305176,0.6504705548286438,2.6843504905700684,1.2824846506118774,-0.555378794670105,1.9917773008346558,2.4853670597076416,2.2720730304718018,0.05783434212207794,1.741772174835205,3.9805495738983154,1.6283323764801025,3.9042468070983887,-0.08662469685077667,1.0114973783493042,-0.2651676535606384,2.276035785675049,-0.6100764274597168,2.0067296028137207,1.3208794593811035,1.0322285890579224,1.9896197319030762,2.3109211921691895,2.2940375804901123,0.7046573758125305,2.1666882038116455,0.8269322514533997,1.038419246673584,3.1033222675323486,2.011228084564209,0.4995725154876709,3.0119717121124268,1.7737351655960083,1.1829763650894165,3.768042802810669,2.453611135482788,-1.2905018329620361,0.4045698344707489,-0.7276555299758911,1.108733057975769,0.7043386101722717,1.7317860126495361,3.1328790187835693,1.5019123554229736,0.00757896201685071,1.7896612882614136,4.704597473144531,-1.2845878601074219,1.471731185913086,3.7090561389923096,-0.9188922643661499,0.25417712330818176,0.40908655524253845,-0.6070359349250793,3.279542922973633,2.3822507858276367,-1.0290439128875732,-1.0308316946029663,1.6611809730529785,0.5246748328208923,1.5900543928146362,1.5552963018417358,1.9574549198150635,0.3487606346607208,1.906638503074646,-0.15992236137390137,-0.1188686341047287,1.874509334564209,2.240257501602173,3.598365068435669,2.4335744380950928,3.23817777633667,1.554663896560669,2.292228937149048,1.6409223079681396,1.495506763458252,3.2188000679016113,3.7398290634155273,2.8911819458007812,3.769780397415161,2.451735019683838,2.5036814212799072,3.150447130203247,1.2291960716247559,0.9691900610923767,3.689431667327881,-0.07314413040876389,0.73487389087677,0.839801013469696,1.4524890184402466,2.1931369304656982,1.3259851932525635,3.0612916946411133,0.2924751043319702,0.6916547417640686,2.3885679244995117,0.7535249590873718,1.8109625577926636,3.2876040935516357,2.6223843097686768,2.415797472000122,2.450838565826416,1.9013489484786987,2.8400003910064697,3.637033224105835,1.2191064357757568,0.49888309836387634,0.527450680732727,3.3894202709198,2.8021371364593506,1.5861856937408447,-0.5647964477539062,1.773905634880066,2.8972055912017822,0.4014616310596466,3.552330493927002,3.5676872730255127,0.44267264008522034,2.05513334274292,2.6361255645751953,2.9887442588806152,0.289135605096817,3.268169641494751,3.1244053840637207,0.8573437333106995,-0.19672121107578278,-0.7705172300338745,1.6629849672317505,3.544947862625122,-0.6638178825378418,3.600919008255005,-0.5163041353225708,-0.025210803374648094,1.4350130558013916,1.8289581537246704,0.6889894008636475,1.6060850620269775,-0.9072462916374207,2.4202475547790527,1.693398356437683,0.15449632704257965,3.578507661819458,2.314469575881958,4.283806324005127,2.3266613483428955,0.48093342781066895,3.2087161540985107,-0.08493365347385406,1.7356622219085693,1.580642580986023,1.29006028175354,0.9935421347618103,2.0631296634674072,2.585738182067871,1.8831286430358887],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"1_entity_entities_knowledge\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"1_entity_entities_knowledge\"],\"x\":[11.134943962097168,11.025127410888672,11.245323181152344,11.604127883911133,11.156891822814941,11.352185249328613,11.527443885803223,11.282083511352539,11.126087188720703,11.73695182800293,11.573534965515137,13.375391006469727,11.404452323913574,11.218412399291992,10.824934959411621,11.162917137145996,11.561422348022461,11.366466522216797,10.656908988952637,11.300393104553223,11.272549629211426,11.278218269348145,11.19459056854248,11.2599458694458,11.612763404846191,10.9222412109375,11.204724311828613,11.096510887145996,11.001474380493164,11.493638038635254,11.407891273498535,11.244793891906738,11.168529510498047,11.017486572265625,11.451024055480957,11.223573684692383,11.187163352966309,11.271790504455566,11.26801872253418,11.24870491027832,11.188407897949219,11.215272903442383,11.429038047790527,11.241147994995117,11.28900146484375,11.684981346130371,11.643854141235352,11.422016143798828,11.644282341003418,11.211750030517578,11.794293403625488,10.666879653930664,11.676538467407227,11.204644203186035,11.4844388961792,11.210556983947754,10.822954177856445,11.602618217468262,11.172928810119629,11.261439323425293,11.18604850769043,11.24404525756836,11.392868995666504,11.273892402648926,11.069701194763184,11.81900691986084,11.298412322998047,11.697004318237305,11.151240348815918,11.614058494567871,11.220532417297363,11.269240379333496,11.204486846923828,11.25660228729248,11.144911766052246,11.228436470031738,11.389638900756836,11.027185440063477,11.341411590576172,11.200804710388184,10.692567825317383,11.169511795043945,11.02439022064209,10.92267894744873,11.365792274475098,11.304564476013184,11.123757362365723,11.317601203918457,11.2233304977417,11.252074241638184,10.953468322753906,11.1937837600708,11.191131591796875,11.507041931152344,11.293615341186523],\"y\":[-0.2791852056980133,-0.56585294008255,-0.33805036544799805,-0.5237462520599365,0.26701799035072327,-0.765414834022522,-0.7155061364173889,-0.6682496070861816,-0.6063860654830933,-0.3886547386646271,-0.6360691785812378,-0.5970664024353027,-0.6077590584754944,-0.3316837251186371,0.027478214353322983,-1.0895836353302002,-0.5561929941177368,0.06399431079626083,0.6634865999221802,-0.7742803692817688,0.14976142346858978,-1.053679347038269,-0.8100492358207703,-0.6574156284332275,-0.6182628273963928,2.9431962966918945,-0.8820046186447144,-0.8308306336402893,-0.7561229467391968,0.06208992749452591,-1.2723103761672974,0.021186748519539833,-0.9865592122077942,0.42442458868026733,-0.5611394047737122,-0.19294996559619904,-0.6945754289627075,-0.3245929181575775,0.14573052525520325,-0.8709970712661743,-0.352821946144104,-0.20292386412620544,-0.3928586542606354,-0.45419180393218994,-0.6536911725997925,-0.43990081548690796,0.47350263595581055,0.13158506155014038,-0.7095873355865479,-1.041664481163025,-0.30340245366096497,0.33025580644607544,-0.5825144648551941,-0.9634666442871094,0.1646973192691803,-0.9077123999595642,0.321090430021286,-0.4256440997123718,0.016457444056868553,-0.3007483184337616,-0.9926905035972595,-0.40955081582069397,-0.7163980603218079,-0.7613950967788696,-0.5563022494316101,-0.30304890871047974,-0.6749861836433411,-0.5403029322624207,-0.3122972548007965,-0.610802948474884,-0.7764879465103149,-0.3963066339492798,-0.9999945163726807,-0.2833714783191681,-0.40003347396850586,-0.9637208580970764,-0.45351165533065796,-0.5818671584129333,-0.7205086350440979,-0.9992696046829224,0.6563940644264221,-1.185823917388916,-1.0215363502502441,-0.13112245500087738,-0.6537771224975586,-0.7294257283210754,-0.7630423307418823,-0.7293157577514648,-0.9494206309318542,-0.3285471796989441,-0.1588168889284134,-0.36618950963020325,-0.9938457608222961,0.16926676034927368,-0.4374297559261322],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"2_visual_image_multimodal\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"2_visual_image_multimodal\"],\"x\":[13.65318489074707,13.939993858337402,13.685359001159668,13.784351348876953,13.504968643188477,13.94735336303711,13.143325805664062,13.834990501403809,13.44897747039795,13.599161148071289,13.199520111083984,13.714217185974121,13.943765640258789,13.477546691894531,13.838189125061035,13.14396858215332,13.451980590820312,13.139006614685059,13.305334091186523,13.572847366333008,13.578728675842285,13.640463829040527,13.756519317626953,13.478137016296387,13.640913963317871,13.371881484985352,13.30630874633789,13.778650283813477,13.48436164855957,13.48245620727539,13.242944717407227,13.188318252563477,13.671472549438477,13.161858558654785,13.182262420654297,13.67568302154541,13.578418731689453,13.574835777282715,13.530089378356934,12.509833335876465,13.550978660583496,13.269086837768555,13.929557800292969,13.697040557861328,13.825508117675781,13.642824172973633,13.51134204864502,13.453692436218262,13.557821273803711,13.553349494934082,13.648940086364746,13.369372367858887,13.12069034576416,13.566463470458984,13.692540168762207,13.704095840454102,13.338266372680664,13.631470680236816,13.964845657348633,13.344286918640137,13.934170722961426,13.56275463104248,13.731254577636719,13.44790267944336,13.115975379943848,13.600167274475098,8.303739547729492,13.527002334594727,13.690975189208984,13.423891067504883,13.938860893249512,13.628800392150879,13.415291786193848,13.883581161499023,13.465723037719727,13.598390579223633,13.958017349243164,13.456777572631836,13.795967102050781,9.905937194824219,13.677034378051758,13.943849563598633,13.411235809326172,13.518133163452148,13.331656455993652,13.444724082946777],\"y\":[-0.4724136292934418,0.05542946234345436,-0.2140544056892395,-0.305911123752594,-0.019443433731794357,0.04754577577114105,-0.5232704877853394,-0.11347304284572601,-0.42826223373413086,-0.3631307780742645,-0.34572187066078186,-0.06021670997142792,0.02899486944079399,-0.5741272568702698,-0.17813809216022491,-0.3307811915874481,-0.5624246597290039,-0.33823102712631226,-0.49408844113349915,-0.2875242233276367,-0.5673142671585083,-0.5054513216018677,-0.19457708299160004,0.04647556692361832,0.15025267004966736,-0.4029132127761841,-0.09265140444040298,-0.06226486340165138,-0.49906080961227417,-0.42240944504737854,-0.37416812777519226,-0.4376997649669647,-0.5433786511421204,-0.48809999227523804,-0.3587982952594757,-0.1461024433374405,-0.4376213848590851,-0.5315554738044739,-0.5797322392463684,2.760768413543701,-0.5617931485176086,-0.37373608350753784,1.0235728025436401,-0.2975458800792694,-0.12453179061412811,-0.39394140243530273,0.08051753044128418,-0.5586355328559875,-0.5786116719245911,-0.5049486756324768,0.1441514790058136,-0.7706948518753052,-0.21628828346729279,-0.3426320552825928,-0.16151897609233856,-0.39145922660827637,-0.5925371646881104,-0.2956721782684326,0.027539370581507683,-0.30378881096839905,0.042292892932891846,-0.12347960472106934,-0.06413992494344711,-0.6200217604637146,-0.4954264461994171,-0.34516313672065735,3.190534830093384,-0.2043280303478241,-0.5706292986869812,-0.7155386209487915,0.12354090809822083,-0.219037726521492,-0.293334424495697,0.07163935899734497,-0.5650919675827026,-0.29002729058265686,0.07026058435440063,-0.5817172527313232,-0.10595829039812088,-1.2771755456924438,-0.2213153839111328,0.0876251682639122,-0.7389064431190491,-0.14196763932704926,-0.6306360363960266,-0.22324824333190918],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"3_dialogue_conversations_dialog\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"3_dialogue_conversations_dialog\"],\"x\":[8.079171180725098,8.06274700164795,7.97455358505249,7.848491191864014,8.032004356384277,7.964422702789307,7.964263916015625,8.134442329406738,8.114529609680176,7.841091632843018,7.8066725730896,8.066001892089844,7.995242118835449,8.329507827758789,8.135189056396484,7.784605979919434,7.775929927825928,8.180281639099121,8.422222137451172,7.919281005859375,7.885015487670898,7.895800590515137,7.771629810333252,8.177929878234863,7.973200798034668,8.28538703918457,7.766735553741455,7.8133392333984375,7.980544567108154,7.834801197052002,7.792785167694092,8.400049209594727,8.152592658996582,7.950541019439697,7.951925754547119,8.21005630493164,7.920266628265381,7.854425430297852,8.000518798828125,7.90179443359375,8.401914596557617,8.250751495361328,7.81003999710083,8.432668685913086,7.853431224822998,8.134767532348633,8.218738555908203,7.8007283210754395,8.225289344787598,7.825068473815918,8.072797775268555,8.098258972167969,8.405604362487793,7.813677787780762,7.821072101593018,7.985950946807861,7.810969352722168,8.3153657913208,8.099970817565918,7.857753276824951,8.26423168182373,7.91890811920166,7.952602386474609,7.912245273590088,7.974617958068848,7.784420490264893,8.138467788696289,8.036944389343262,8.017165184020996],\"y\":[0.011064082384109497,0.014209670014679432,-0.09134206920862198,-0.2708755433559418,-0.20944665372371674,-0.15542514622211456,-0.016267064958810806,0.33848637342453003,1.3138884241925552e-05,-0.3145374655723572,-0.37487006187438965,0.08202043920755386,-0.06734184175729752,-0.26890313625335693,-0.2482864111661911,-0.37177231907844543,-0.342668354511261,0.05140113830566406,-0.009848969988524914,-0.26512715220451355,-0.21851959824562073,-0.20532433688640594,-0.34504765272140503,-0.12112276256084442,-0.11948814243078232,-0.36536410450935364,-0.43950170278549194,-0.3522815406322479,-0.16667716205120087,-0.33151018619537354,-0.4151010513305664,-0.21129287779331207,0.18422266840934753,-0.13335680961608887,0.17345526814460754,0.07078302651643753,-0.06395117193460464,-0.33869442343711853,-0.20178943872451782,-0.20231139659881592,-0.02338528260588646,0.17116601765155792,-0.3580891191959381,-0.31286415457725525,-0.32009008526802063,-0.1331133246421814,0.18067573010921478,-0.2697615623474121,0.08013986051082611,-0.31005680561065674,0.028209263458848,0.10702250152826309,0.2667584717273712,-0.3471834659576416,-0.3380453586578369,-0.07149399816989899,-0.4023083448410034,-0.10830654948949814,-0.17893050611019135,-0.2754707932472229,0.22065694630146027,-0.1637326180934906,-0.12212429195642471,-0.17535138130187988,-0.2760526239871979,-0.01606772653758526,0.11322040110826492,0.021302759647369385,-0.137142151594162],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"4_clinical_medical_patients\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"4_clinical_medical_patients\"],\"x\":[7.761767864227295,7.890107154846191,8.475704193115234,8.049864768981934,7.910464286804199,7.735909938812256,8.021018028259277,7.9187726974487305,7.908551216125488,7.699703216552734,7.959645748138428,7.826615333557129,8.282672882080078,7.934024333953857,8.200965881347656,7.851012229919434,7.659298419952393,7.964986801147461,7.375672340393066,7.989088535308838,8.16100788116455,7.880589008331299,7.8358564376831055,7.8488359451293945,7.915194511413574,8.008590698242188,7.819448471069336,7.807568073272705,7.392760753631592,7.372259140014648,7.680557727813721,8.323735237121582,7.405981540679932,7.390983581542969,8.47547721862793,7.814692974090576,8.292101860046387,8.115535736083984,7.816876411437988,7.764798164367676,7.4307541847229,8.173230171203613,7.816125869750977,7.800354957580566,7.687350749969482,7.416838645935059,8.04630184173584,7.372389793395996,8.061315536499023,7.425755023956299,7.877260684967041,8.388016700744629,7.779290199279785,8.227629661560059,8.057212829589844,8.02259349822998,7.782587051391602,8.23399543762207,7.90402889251709,7.8760199546813965,7.4668121337890625,8.308961868286133,7.824159622192383,7.712260723114014,7.914191246032715,7.685945987701416,7.940428733825684,7.877172470092773],\"y\":[1.5433651208877563,1.4605567455291748,1.4761929512023926,1.4625173807144165,1.4584766626358032,1.4409211874008179,1.4749916791915894,1.4284682273864746,1.4748163223266602,1.4806615114212036,1.4631659984588623,1.411305546760559,1.4848947525024414,1.0450985431671143,1.4943875074386597,1.3517125844955444,1.4878875017166138,1.4422603845596313,1.5346200466156006,1.5406434535980225,1.499199628829956,1.4141327142715454,1.4414336681365967,1.3886102437973022,1.4608150720596313,1.4566928148269653,1.5145379304885864,1.6541327238082886,1.5264090299606323,1.531654953956604,1.4845945835113525,1.4692519903182983,1.5288569927215576,1.5096898078918457,1.3595173358917236,1.587659478187561,1.4848111867904663,1.480909824371338,1.49443781375885,1.6546357870101929,1.5368572473526,1.2316579818725586,1.3251349925994873,1.5530869960784912,1.5514334440231323,1.5379899740219116,1.494699239730835,1.525313377380371,1.4642879962921143,1.526352882385254,1.288293480873108,1.483452320098877,1.4212583303451538,1.5053179264068604,1.3147481679916382,1.3840261697769165,1.5364909172058105,1.516196370124817,1.4429274797439575,1.4955360889434814,1.5462852716445923,1.454986810684204,1.4761298894882202,1.4729735851287842,1.450022578239441,1.443420648574829,1.4701296091079712,1.4678795337677002],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"5_hyperparameters_metaretriever_pts\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"5_hyperparameters_metaretriever_pts\"],\"x\":[12.020341873168945,12.169936180114746,12.101693153381348,12.094861030578613,11.659080505371094,12.379535675048828,12.050992965698242,12.262616157531738,12.431180000305176,11.988661766052246,12.082118034362793,12.085043907165527,12.180397033691406,12.176328659057617,11.84795093536377,12.084325790405273,12.128952980041504,12.28459644317627,12.194994926452637,12.008187294006348,11.911856651306152,12.048596382141113,12.229466438293457,12.182258605957031,12.1466703414917,12.20826530456543,11.903266906738281,11.976790428161621,11.813048362731934,12.021183967590332,12.020978927612305,12.224409103393555,11.788299560546875,12.209701538085938,11.93454647064209,11.924176216125488,12.019671440124512,12.825284004211426,12.15970516204834,12.297412872314453,12.158596992492676,12.110843658447266,12.26720905303955,12.509387016296387,12.426708221435547,12.31076431274414,12.200416564941406,12.108731269836426,12.027480125427246,12.294480323791504,12.179040908813477,11.868775367736816,12.016388893127441,12.533409118652344,11.871016502380371,12.278585433959961,12.497551918029785,11.953156471252441,12.103267669677734,12.292784690856934,12.235295295715332,11.931829452514648,12.1331148147583],\"y\":[3.4334287643432617,3.9167301654815674,3.949054002761841,3.438246250152588,3.445382595062256,3.36970853805542,3.7780611515045166,2.977111577987671,3.824798583984375,4.014473915100098,3.966614246368408,3.947669744491577,3.546800136566162,3.5740740299224854,3.722076654434204,3.7931604385375977,3.4850759506225586,3.1680963039398193,4.030865669250488,3.494990587234497,3.631016969680786,3.9821364879608154,3.874598264694214,3.628248453140259,3.362018585205078,3.6282668113708496,3.4427807331085205,3.539586305618286,3.9404172897338867,3.503599166870117,3.3990092277526855,3.441103219985962,2.9686286449432373,3.454322576522827,3.507044553756714,3.588791847229004,3.930246591567993,3.598909854888916,3.4998040199279785,3.598375082015991,3.547398805618286,3.815009355545044,3.1761443614959717,3.7004573345184326,2.9148776531219482,3.357351303100586,3.646010398864746,4.046837329864502,3.844369649887085,3.3957107067108154,3.5331528186798096,3.4589388370513916,3.6157045364379883,3.513286590576172,3.4719655513763428,3.176704168319702,3.3323616981506348,3.9445858001708984,3.7024219036102295,3.222160577774048,3.219435214996338,3.6485061645507812,3.5754635334014893],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"6_adversarial_attacks_attack\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"6_adversarial_attacks_attack\"],\"x\":[10.677026748657227,10.417326927185059,10.19378662109375,10.34382438659668,10.359889030456543,10.168207168579102,10.379969596862793,10.382912635803223,10.3951997756958,10.006451606750488,10.593812942504883,10.523648262023926,10.564718246459961,10.390353202819824,10.409890174865723,10.375975608825684,10.270461082458496,10.380226135253906,10.342330932617188,10.389986038208008,10.570111274719238,10.563359260559082,10.393143653869629,10.445023536682129,10.406803131103516,10.406514167785645,9.976093292236328,10.553617477416992,10.514777183532715,10.404207229614258,10.352548599243164,10.402541160583496,10.373939514160156,10.66649055480957,10.147748947143555,10.177634239196777,10.380538940429688,10.481019020080566,10.569191932678223,10.38936710357666,10.453319549560547,10.417245864868164,10.462665557861328,10.532641410827637,10.139423370361328,10.995478630065918,10.441753387451172,10.408876419067383,10.595641136169434,10.508939743041992,10.599928855895996,10.383362770080566,10.315134048461914,10.520706176757812,10.196410179138184,10.421682357788086,10.112748146057129,10.42435073852539,9.957427978515625,10.400481224060059],\"y\":[4.530323028564453,4.6652750968933105,4.259049415588379,4.17231559753418,4.669703960418701,4.087711334228516,4.67042350769043,4.63741397857666,4.61146879196167,4.00691556930542,4.465812683105469,4.730312824249268,3.997835874557495,4.689051151275635,4.215987682342529,4.713289260864258,3.9740796089172363,4.662024021148682,4.6290106773376465,4.683803558349609,4.155202865600586,4.592298984527588,4.655975341796875,4.570188999176025,4.67529296875,4.149698734283447,4.111392498016357,4.625186920166016,4.129754066467285,4.608168125152588,4.609062194824219,4.684344291687012,4.598238468170166,4.515726089477539,4.097564697265625,4.126251697540283,4.632607936859131,3.846534013748169,4.030094146728516,4.677065372467041,4.595035552978516,4.631924152374268,4.152017593383789,4.6212286949157715,4.092752933502197,4.058029651641846,4.524569034576416,4.68303918838501,4.761435031890869,4.133310794830322,4.037563800811768,4.604251384735107,4.569831848144531,4.141808032989502,4.4533562660217285,4.653535842895508,4.054136753082275,4.630998134613037,3.962613344192505,4.415659427642822],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"7_summarization_summaries_summary\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"7_summarization_summaries_summary\"],\"x\":[9.407848358154297,9.24934196472168,9.988895416259766,9.616469383239746,9.254940032958984,9.619887351989746,9.763313293457031,9.41704273223877,9.903557777404785,9.640640258789062,9.904218673706055,9.833464622497559,9.501309394836426,10.281272888183594,9.5150146484375,9.301063537597656,9.468867301940918,9.583669662475586,9.652227401733398,9.2976655960083,9.610597610473633,9.950066566467285,9.628329277038574,9.354300498962402,9.685493469238281,9.437472343444824,9.71446418762207,9.970356941223145,9.591911315917969,9.4934720993042,11.245697975158691,9.630011558532715,9.857994079589844,9.644820213317871,9.684257507324219,9.61834716796875,9.774521827697754,9.085481643676758,10.036049842834473,9.738116264343262,9.506299018859863,9.30492877960205,9.699036598205566,9.771791458129883,9.192770957946777,9.514463424682617,9.63692855834961,9.843480110168457,9.38808536529541,9.520946502685547,9.217379570007324,9.838109970092773,9.648056030273438,9.77692699432373,9.458232879638672,9.670414924621582,9.680063247680664,9.057123184204102,9.632370948791504],\"y\":[1.5877009630203247,1.2727618217468262,1.4802215099334717,1.6104549169540405,1.4569437503814697,1.539625644683838,1.472213864326477,2.0593249797821045,1.3333063125610352,1.6195672750473022,0.646668553352356,1.6028765439987183,1.468073844909668,1.4336971044540405,1.469533920288086,1.5507453680038452,1.6590402126312256,1.5533472299575806,1.5326228141784668,1.4909952878952026,1.5933552980422974,0.6876698732376099,1.5868359804153442,1.573804259300232,1.5753282308578491,1.2330971956253052,1.6163599491119385,0.7645369172096252,1.5960463285446167,1.578489899635315,0.5172238349914551,1.5027610063552856,1.38727867603302,1.5584850311279297,1.571655035018921,1.5743860006332397,1.5758932828903198,1.8300682306289673,1.7321003675460815,1.5929148197174072,1.4410037994384766,1.594342589378357,1.557225227355957,1.3510727882385254,1.2829564809799194,1.1802170276641846,1.5979278087615967,1.6627514362335205,1.6593539714813232,1.5607953071594238,1.3650426864624023,0.5767338275909424,1.5249618291854858,1.5589072704315186,1.6015998125076294,1.5737286806106567,1.52560555934906,1.5944088697433472,1.4602868556976318],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"8_sentiment_emotion_emotional\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"8_sentiment_emotion_emotional\"],\"x\":[9.02020263671875,8.764460563659668,9.149687767028809,8.96601676940918,8.83845329284668,7.854395389556885,8.691113471984863,8.074891090393066,7.991252422332764,8.15771198272705,8.11121940612793,7.889158725738525,8.559086799621582,7.732449054718018,8.129582405090332,7.739405155181885,8.906782150268555,8.779287338256836,8.384539604187012,8.096807479858398,7.73988151550293,7.74143123626709,10.807848930358887,8.938881874084473,8.25653076171875,8.156562805175781,8.910407066345215,8.929011344909668,8.912620544433594,8.349308967590332,9.150344848632812,9.063459396362305,8.188298225402832,8.082099914550781,8.124395370483398,8.095884323120117,7.761401653289795,7.85822057723999,7.735099792480469,9.143465995788574,7.9051079750061035,8.62359619140625,8.771903991699219,8.10483455657959,8.881553649902344,7.842296600341797,7.751319408416748,8.932684898376465,8.793137550354004,8.686760902404785,8.676131248474121,8.958822250366211,7.80643892288208,8.740694046020508,7.7468767166137695,8.771867752075195,8.442422866821289],\"y\":[2.3929059505462646,2.3886959552764893,2.2481915950775146,2.366041660308838,2.3146793842315674,2.3162174224853516,2.369158983230591,2.5022075176239014,2.4162755012512207,2.523205280303955,2.5030629634857178,2.322309732437134,2.426605224609375,2.189079761505127,2.5072808265686035,2.1927618980407715,2.2802486419677734,2.404411792755127,2.5210015773773193,2.5109353065490723,2.188095808029175,2.2342336177825928,1.3085989952087402,2.270885467529297,2.520770311355591,2.4987809658050537,2.2820372581481934,2.2575674057006836,2.305651903152466,2.480987310409546,2.6763017177581787,2.3891241550445557,2.503016233444214,2.507106065750122,2.5494508743286133,2.4986634254455566,2.2242064476013184,2.3021676540374756,2.150664806365967,2.744899034500122,2.3470451831817627,2.496939182281494,2.4149718284606934,2.483858346939087,2.3855793476104736,2.2803268432617188,2.255380868911743,2.259631872177124,2.3189663887023926,2.3722052574157715,2.399129867553711,2.2720420360565186,2.2237091064453125,2.365219831466675,2.2338740825653076,2.3938751220703125,2.3587722778320312],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"10_qa_question_questions\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"10_qa_question_questions\"],\"x\":[9.40961742401123,9.401433944702148,9.173538208007812,9.561808586120605,9.133511543273926,9.540070533752441,9.700275421142578,9.41574478149414,9.289504051208496,9.243343353271484,9.41695785522461,9.496110916137695,9.767770767211914,9.445943832397461,9.416340827941895,9.574084281921387,9.292332649230957,9.226899147033691,9.462366104125977,9.370545387268066,9.44404411315918,9.387925148010254,9.353792190551758,9.420747756958008,9.548852920532227,9.489824295043945,9.414810180664062,9.33342456817627,9.402825355529785,9.672316551208496,9.393604278564453,9.239797592163086,9.441904067993164,9.516083717346191,9.340642929077148,9.363749504089355,9.59388256072998,9.435190200805664,9.716001510620117,9.322286605834961,9.686580657958984,9.433467864990234,9.423392295837402,9.48121166229248,9.288214683532715,9.267616271972656,9.553618431091309,9.478880882263184,9.371923446655273,9.760424613952637,9.519524574279785,9.401512145996094,9.502867698669434,9.440361976623535],\"y\":[-0.5178394317626953,-0.650516152381897,-0.479851096868515,-0.4310290515422821,-0.5378267168998718,-0.6259913444519043,-0.7380387187004089,-0.21319614350795746,-0.7479232549667358,-0.610841691493988,-0.6754353642463684,-0.5430178642272949,-0.6768491268157959,-0.45748335123062134,-0.3581988215446472,-0.8913076519966125,-0.698038637638092,-0.7323350310325623,-0.4265297055244446,-0.6116015911102295,-0.9226518869400024,-0.7730879783630371,-0.7615750432014465,-0.7331415414810181,-0.8823621869087219,-0.8506748676300049,-0.7885114550590515,-0.723310649394989,-0.7920389175415039,-0.6305829882621765,-0.698114275932312,-0.7590868473052979,-0.9453780651092529,-0.47471097111701965,-0.7153941988945007,-0.7514808177947998,-0.32000821828842163,-0.5421023368835449,-0.5268405079841614,-0.7256784439086914,-0.6647657752037048,-0.5601599812507629,-0.8130180239677429,-0.5797785520553589,-0.76743084192276,-0.740344226360321,-0.6408556699752808,-0.4104425609111786,-0.3353818655014038,-0.6775698661804199,-0.7086683511734009,-0.9370354413986206,-0.7324972152709961,-0.6511043310165405],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"11_transformer_nat_transformers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"11_transformer_nat_transformers\"],\"x\":[13.446449279785156,13.293713569641113,13.339015007019043,13.234209060668945,13.390213012695312,13.275945663452148,13.059819221496582,13.199735641479492,13.168375968933105,13.352516174316406,13.391827583312988,13.276883125305176,13.3883638381958,13.391206741333008,13.22536563873291,13.355441093444824,13.415063858032227,13.445826530456543,13.263446807861328,13.253372192382812,13.080596923828125,13.292510986328125,13.387595176696777,13.407983779907227,13.065677642822266,13.157443046569824,13.285446166992188,13.371879577636719,13.443427085876465,13.312077522277832,13.325447082519531,13.201516151428223,13.412653923034668,13.387580871582031,13.376065254211426,13.23760986328125,13.391357421875,13.28543758392334,13.155158996582031,13.134252548217773,13.264272689819336,13.326519012451172,13.441741943359375,13.180994987487793,13.229814529418945,13.385966300964355,13.26877498626709,13.252023696899414,13.296429634094238],\"y\":[2.849513053894043,2.6924357414245605,2.519382953643799,2.685671091079712,3.4131829738616943,3.3496320247650146,2.9062132835388184,2.5787570476531982,2.8339319229125977,2.6660850048065186,3.363393783569336,3.0692508220672607,3.403278112411499,2.94502854347229,2.862663984298706,2.849548578262329,3.311357021331787,2.857738494873047,2.829335927963257,2.7103164196014404,2.6873369216918945,3.1780202388763428,2.415661573410034,2.984997510910034,2.9960906505584717,3.0859627723693848,3.064985752105713,3.3739285469055176,2.99613094329834,3.3052146434783936,2.759016990661621,2.554168224334717,2.7810420989990234,3.370600700378418,2.8825161457061768,2.5941309928894043,3.402747631072998,3.1956849098205566,2.574057102203369,2.4850573539733887,3.1060783863067627,2.757913827896118,2.8156826496124268,2.6702122688293457,3.1264901161193848,3.3880455493927,2.8700947761535645,2.6230366230010986,2.932117223739624],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"12_political_moral_hate\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"12_political_moral_hate\"],\"x\":[8.319539070129395,8.872060775756836,8.969176292419434,8.707355499267578,8.74543285369873,8.927281379699707,8.319100379943848,8.32375717163086,8.666680335998535,8.335197448730469,8.70714282989502,9.042021751403809,8.712772369384766,8.291357040405273,8.325663566589355,8.38085651397705,8.75160026550293,8.50749397277832,8.352300643920898,8.444624900817871,8.73957347869873,8.366410255432129,8.543909072875977,8.721636772155762,8.295104026794434,8.693644523620605,8.322171211242676,8.898469924926758,8.4146146774292,8.773049354553223,8.323225975036621,9.074934959411621,8.57706356048584,8.7324857711792,8.81857967376709,9.022188186645508,8.334091186523438,8.382010459899902,8.761948585510254,8.723730087280273,9.060052871704102,8.774007797241211,8.736745834350586,8.403592109680176,8.618061065673828],\"y\":[3.193037986755371,3.0153839588165283,3.2098007202148438,3.411353826522827,3.419900894165039,2.993455171585083,3.2046985626220703,3.197061538696289,3.5652174949645996,3.1496386528015137,3.4359893798828125,3.163926362991333,3.4493963718414307,3.234320640563965,3.2002310752868652,3.272998094558716,3.3818790912628174,3.3397650718688965,3.0758256912231445,3.1623947620391846,3.384615182876587,3.2408924102783203,3.358213186264038,3.416884660720825,3.167921781539917,3.4248197078704834,3.2836413383483887,3.2514102458953857,3.342782497406006,3.3918700218200684,3.194164752960205,3.047151803970337,3.55344820022583,3.2842931747436523,3.3557095527648926,3.1627235412597656,3.2219274044036865,3.0630640983581543,3.3806440830230713,3.417919874191284,3.0919387340545654,3.379578113555908,3.3994202613830566,3.3199284076690674,3.2775278091430664],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"13_reasoning_pangu_symbolic\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"13_reasoning_pangu_symbolic\"],\"x\":[9.99891185760498,9.666812896728516,9.87675666809082,9.759414672851562,9.77737045288086,9.712926864624023,9.860082626342773,10.150372505187988,9.757026672363281,9.756099700927734,9.88352108001709,9.667094230651855,9.834576606750488,9.662117958068848,9.90071964263916,9.823662757873535,9.885151863098145,9.897980690002441,9.599701881408691,9.828813552856445,10.166223526000977,9.664942741394043,9.68136215209961,9.909587860107422,9.613624572753906,9.683416366577148,9.677172660827637,9.895627975463867,9.831110954284668,9.791001319885254,9.888591766357422,10.030932426452637,10.003292083740234,9.82230281829834],\"y\":[-1.2806227207183838,-1.4134907722473145,-1.3984967470169067,-0.8591680526733398,-1.1667323112487793,-0.9986860752105713,-1.226072072982788,-1.2499401569366455,-1.3567789793014526,-1.3018178939819336,-1.279323697090149,-1.3926376104354858,-0.8900179862976074,-1.3667707443237305,-1.3356115818023682,-1.4169893264770508,-1.3518718481063843,-1.17157781124115,-1.1077362298965454,-1.3785203695297241,-1.242591381072998,-1.3728218078613281,-1.36669921875,-1.1680376529693604,-1.3303998708724976,-0.8987433314323425,-1.368369460105896,-1.1938246488571167,-1.3907575607299805,-1.302925705909729,-1.1836531162261963,-1.1196755170822144,-1.154442310333252,-1.243509292602539],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"14_speech_asr_s2st\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"14_speech_asr_s2st\"],\"x\":[13.670157432556152,13.798901557922363,13.369604110717773,13.828919410705566,13.80864143371582,13.781868934631348,13.87149715423584,13.169575691223145,13.689194679260254,13.725122451782227,13.867948532104492,13.486695289611816,13.795697212219238,13.88539981842041,13.794876098632812,13.703969955444336,13.475425720214844,13.485220909118652,13.642592430114746,13.45029067993164,13.17920970916748,13.825347900390625,13.710762977600098,13.833584785461426,13.823281288146973,13.777862548828125,13.709671974182129,13.589035034179688,13.624615669250488,13.818294525146484,13.520774841308594,13.499052047729492,13.662908554077148],\"y\":[1.930212140083313,1.9589751958847046,1.9856090545654297,2.118469476699829,1.8433870077133179,2.083873748779297,1.9659463167190552,1.6238290071487427,1.9088102579116821,1.984571933746338,1.8491621017456055,1.8348524570465088,1.9095227718353271,2.1351234912872314,2.0434975624084473,1.9872238636016846,1.9833941459655762,1.972872018814087,1.8849225044250488,1.9644986391067505,1.6182628870010376,2.087661027908325,1.9592905044555664,1.9400484561920166,1.8442121744155884,2.052821636199951,2.1948325634002686,1.9715791940689087,1.9485830068588257,1.9467763900756836,2.033796548843384,2.05224347114563,1.9568393230438232],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"15_classification_oov_slash\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"15_classification_oov_slash\"],\"x\":[11.668291091918945,11.893047332763672,11.399813652038574,11.399984359741211,11.689655303955078,11.275130271911621,11.221017837524414,12.198081016540527,12.027531623840332,11.27303695678711,11.897500038146973,11.873612403869629,11.806347846984863,11.46186637878418,11.398143768310547,11.580697059631348,11.322710990905762,11.420403480529785,11.564152717590332,11.334028244018555,11.482306480407715,11.22659969329834,11.433297157287598,11.450419425964355,11.489837646484375,11.502764701843262,11.421748161315918,11.442119598388672,12.117900848388672,11.278861999511719,11.551697731018066],\"y\":[1.6213551759719849,1.7356750965118408,1.8189188241958618,2.011199474334717,1.6284370422363281,1.6911828517913818,1.434216856956482,1.9124113321304321,1.708951473236084,1.345273733139038,1.9241002798080444,1.706599235534668,1.5955106019973755,1.6075619459152222,1.8218833208084106,1.9240765571594238,1.5612646341323853,1.4347044229507446,1.8333743810653687,1.4203897714614868,1.508184552192688,1.6106964349746704,1.7237390279769897,1.6263391971588135,1.7565888166427612,1.8806627988815308,1.4971145391464233,1.4235330820083618,2.043459415435791,1.6557258367538452,1.682104468345642],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"16_event_events_eae\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"16_event_events_eae\"],\"x\":[11.366277694702148,11.415215492248535,11.413336753845215,11.3681058883667,11.386617660522461,11.464384078979492,11.333633422851562,11.434808731079102,11.411873817443848,11.339577674865723,11.400299072265625,11.394913673400879,11.41909122467041,11.410325050354004,11.436091423034668,11.440723419189453,11.421466827392578,11.424187660217285,10.752309799194336,11.30122184753418,11.429636001586914,11.37482738494873,11.405726432800293,11.432455062866211,11.494464874267578,11.39643669128418,11.412282943725586,11.394112586975098,11.40078353881836,11.381903648376465],\"y\":[-1.7047370672225952,-1.7189042568206787,-1.7727406024932861,-1.7914379835128784,-1.7604166269302368,-1.5030242204666138,-1.8196848630905151,-1.7163331508636475,-1.6494393348693848,-1.6843397617340088,-1.7724716663360596,-1.7692655324935913,-1.7378288507461548,-1.7441824674606323,-1.464735746383667,-1.7443996667861938,-1.7241114377975464,-1.6847602128982544,0.36447063088417053,-1.677884578704834,-1.6699968576431274,-1.7729989290237427,-1.7663508653640747,-1.7174915075302124,-1.603288173675537,-1.7217741012573242,-1.7533800601959229,-1.768631100654602,-1.7753701210021973,-1.6422590017318726],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"17_explanations_explanation_concepts\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"17_explanations_explanation_concepts\"],\"x\":[9.92600154876709,9.90449047088623,9.936745643615723,9.92866325378418,9.815776824951172,9.909724235534668,9.910490989685059,9.891134262084961,9.903509140014648,9.932821273803711,9.916892051696777,9.828373908996582,9.930082321166992,9.88005256652832,9.91939926147461,10.002853393554688,9.763079643249512,9.934072494506836,9.936637878417969,9.912662506103516,9.736724853515625,9.912676811218262,9.76375961303711,9.90211296081543,9.953424453735352,9.921619415283203,9.907418251037598,9.980151176452637,9.924909591674805,9.899526596069336],\"y\":[-2.003943920135498,-2.018934726715088,-2.0210397243499756,-1.9936052560806274,-1.9008338451385498,-1.989972710609436,-2.0100209712982178,-1.9968105554580688,-2.0147523880004883,-2.0181238651275635,-2.0108845233917236,-1.9405567646026611,-1.9686578512191772,-1.9617516994476318,-2.0073821544647217,-1.730658769607544,-1.9098670482635498,-1.9983524084091187,-2.0032131671905518,-2.021005392074585,-1.78288996219635,-1.9908539056777954,-1.9123069047927856,-1.9989206790924072,-1.8872182369232178,-2.0079305171966553,-1.9907784461975098,-1.9225029945373535,-2.0194551944732666,-1.9666630029678345],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"18_prompts_prompt_continuous\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"18_prompts_prompt_continuous\"],\"x\":[9.238378524780273,9.30147647857666,9.232583999633789,9.182598114013672,9.238131523132324,9.218341827392578,9.234349250793457,9.302995681762695,9.232303619384766,9.124175071716309,9.416687965393066,9.293940544128418,9.349409103393555,9.29613208770752,9.22963809967041,9.208431243896484,9.365690231323242,9.037209510803223,9.345200538635254,9.246965408325195,9.236005783081055,9.268945693969727,9.25452709197998],\"y\":[0.11233166605234146,0.2616926431655884,0.13499541580677032,0.13999022543430328,0.13154223561286926,0.03455710411071777,0.1343674212694168,0.09137420356273651,0.0880853608250618,-0.03193724900484085,0.282012939453125,0.19786401093006134,0.27887430787086487,0.23954655230045319,0.1252308189868927,0.2048257291316986,0.21515387296676636,0.2964838743209839,0.28831443190574646,0.10827220976352692,0.010295077227056026,0.17448420822620392,0.15992532670497894],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"19_zeroshot_fewshot_verbalizers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"19_zeroshot_fewshot_verbalizers\"],\"x\":[11.956093788146973,12.490596771240234,12.429107666015625,12.604439735412598,12.537409782409668,12.447066307067871,12.514530181884766,12.494388580322266,12.444210052490234,12.318142890930176,12.335335731506348,12.499151229858398,12.503713607788086,12.477373123168945,12.43051815032959,12.483855247497559,12.453995704650879,12.551338195800781,12.408796310424805,12.414511680603027,12.439727783203125],\"y\":[-0.01690675877034664,-0.3107600808143616,-0.2527309060096741,-0.2602718472480774,-0.33801159262657166,-0.32423314452171326,-0.19620446860790253,-0.286507248878479,-0.2868269979953766,2.4987032413482666,-0.19626036286354065,-0.2882573902606964,2.8162295818328857,-0.27515092492103577,-0.28718024492263794,-0.29625701904296875,-0.3112342059612274,-0.32359012961387634,-0.29335451126098633,-0.24639558792114258,0.026239966973662376],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"20_chatgpt_chatgpts_rewriting\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"20_chatgpt_chatgpts_rewriting\"],\"x\":[8.408228874206543,8.558862686157227,8.468222618103027,8.394969940185547,8.488740921020508,8.462080955505371,8.458389282226562,8.648653030395508,8.45824146270752,8.496983528137207,8.497143745422363,8.60595989227295,8.652069091796875,8.46937370300293,8.726569175720215,8.704874992370605,8.474954605102539,8.499213218688965,8.526307106018066],\"y\":[0.8488311767578125,0.8990485668182373,0.930065929889679,0.9668167233467102,0.8830121159553528,0.8673952221870422,0.9546473622322083,0.9203405976295471,0.9518395662307739,0.8990188241004944,0.8826806545257568,1.001754879951477,0.8333197236061096,0.94631427526474,0.7836666703224182,0.7688788771629333,0.916922390460968,0.7554416656494141,0.88944411277771],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"21_tables_table_tableqa\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"21_tables_table_tableqa\"],\"x\":[10.386235237121582,10.229713439941406,10.389017105102539,10.301673889160156,10.361222267150879,10.3976469039917,10.311823844909668,10.32546329498291,10.373322486877441,10.308487892150879,10.342009544372559,10.355380058288574,10.305009841918945,10.051471710205078,10.35975456237793,10.3198823928833],\"y\":[-0.7522066235542297,-0.7360655665397644,-0.7566789388656616,-0.7287562489509583,-0.7565518617630005,-0.6721810698509216,-0.7266584038734436,-0.7462540864944458,-0.6999889016151428,-0.7211228609085083,-0.7179433107376099,-0.7139708399772644,-0.7397772073745728,-0.6260511875152588,-0.7241697311401367,-0.7212250828742981],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"22_oosf_instances_proxy\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"22_oosf_instances_proxy\"],\"x\":[11.03643798828125,11.07364559173584,11.248010635375977,11.170310974121094,10.949195861816406,11.285311698913574,10.957159996032715,11.096034049987793,11.2238130569458,11.023275375366211,11.042583465576172,11.16788101196289,11.188385009765625,11.112465858459473],\"y\":[3.106004476547241,3.0234382152557373,3.3541853427886963,3.115736246109009,3.0922281742095947,3.283050775527954,3.0349786281585693,3.2528135776519775,3.086516857147217,3.087618112564087,3.101942300796509,3.0745949745178223,3.133884906768799,3.1343841552734375],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"23_figurative_metaphors_sociocultural\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"23_figurative_metaphors_sociocultural\"],\"x\":[12.890239715576172,12.9169282913208,12.898111343383789,12.95676326751709,12.863883018493652,12.8355073928833,12.85640811920166,13.136683464050293,12.889006614685059,7.990469932556152,12.911820411682129,12.735673904418945,12.899605751037598,12.521623611450195],\"y\":[0.026861172169446945,-0.11374158412218094,-0.0861361101269722,-0.08715442568063736,0.14325851202011108,-0.09389468282461166,-0.06494750827550888,-0.21590127050876617,-0.029119817540049553,0.18973292410373688,-0.05029120296239853,0.2098502218723297,-0.02140832133591175,-0.014837851747870445],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"24_sarcasm_irony_humor\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"24_sarcasm_irony_humor\"],\"x\":[7.443446159362793,7.453991889953613,7.434319496154785,7.457515239715576,7.445913314819336,7.432080268859863,7.492039680480957,7.444361686706543,7.45388650894165,7.470397472381592,7.456826210021973,7.457319259643555,7.452033042907715,7.453394889831543],\"y\":[3.3650591373443604,3.3583810329437256,3.3732411861419678,3.3514108657836914,3.3623621463775635,3.374535083770752,3.3646886348724365,3.3602709770202637,3.329127788543701,3.332568883895874,3.3524117469787598,3.345237970352173,3.361050605773926,3.3561806678771973],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"25_dense_retrieval_retrievers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"25_dense_retrieval_retrievers\"],\"x\":[11.089601516723633,11.02492904663086,10.883186340332031,10.981522560119629,10.981151580810547,10.95821762084961,10.9121675491333,10.982845306396484,11.157200813293457,10.896283149719238,10.92767333984375,10.96117877960205,10.979663848876953],\"y\":[2.4378020763397217,2.470427989959717,2.49906587600708,2.4681739807128906,2.4417030811309814,2.4632935523986816,2.5861077308654785,2.4596455097198486,2.333122968673706,2.6070048809051514,2.5345096588134766,2.460688352584839,2.480128526687622],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"26_argumentative_environmental_firms\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"26_argumentative_environmental_firms\"],\"x\":[9.245034217834473,9.240274429321289,9.255699157714844,9.306416511535645,9.330011367797852,9.5535249710083,9.301241874694824,9.279705047607422,9.539828300476074,9.543222427368164,9.282510757446289,9.352497100830078],\"y\":[2.6040866374969482,2.5882256031036377,2.6099462509155273,2.364428997039795,2.5036041736602783,2.5556437969207764,2.5966074466705322,2.450493335723877,2.5414693355560303,2.4918816089630127,2.483466386795044,2.526350498199463],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"27_proprietary_opensourced_recommendatio\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"27_proprietary_opensourced_recommendatio\"],\"x\":[8.700318336486816,8.662346839904785,8.710755348205566,8.685635566711426,10.063787460327148,8.825141906738281,9.746978759765625,8.632473945617676,8.7011137008667,9.762179374694824,10.958747863769531,9.222679138183594],\"y\":[1.2634673118591309,1.199538230895996,1.2940387725830078,1.2151010036468506,2.198474407196045,1.475010633468628,3.3699069023132324,1.1698120832443237,1.2515336275100708,3.3798534870147705,1.8569083213806152,1.78851318359375],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"28_modification_mp2_multiword\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"28_modification_mp2_multiword\"],\"x\":[12.416332244873047,12.561809539794922,12.500757217407227,12.375421524047852,12.55825138092041,12.481864929199219,12.49730396270752,12.1989107131958,12.471221923828125,12.578041076660156,12.469315528869629,12.464476585388184],\"y\":[1.9628962278366089,2.3608059883117676,2.222209930419922,2.1565310955047607,2.1810107231140137,2.1097028255462646,2.3813178539276123,2.188523292541504,2.159067153930664,2.3556671142578125,1.6200826168060303,2.1543469429016113],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"29_instructional_teaching_students\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"29_instructional_teaching_students\"],\"x\":[8.679919242858887,8.863896369934082,8.659948348999023,8.705171585083008,8.791703224182129,8.700974464416504,8.720808029174805,9.014729499816895,8.698103904724121,8.63776969909668,8.665627479553223,8.739877700805664],\"y\":[0.4001754820346832,0.5290360450744629,0.39178118109703064,0.5974481105804443,0.46730977296829224,0.44593337178230286,0.3917016386985779,0.42155221104621887,0.3866553008556366,0.5923276543617249,0.5228016972541809,0.4678838849067688],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"30_counterfactuals_scone_counterfactual\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"30_counterfactuals_scone_counterfactual\"],\"x\":[10.26634407043457,10.128600120544434,10.363204956054688,10.149825096130371,10.254651069641113,10.238754272460938,10.266803741455078,10.18940258026123,10.27680492401123,10.135074615478516,10.226946830749512],\"y\":[2.246767520904541,2.1721770763397217,2.3008975982666016,2.423623561859131,2.2409117221832275,2.265772581100464,2.3193318843841553,2.154175281524658,2.2092831134796143,2.1171963214874268,2.24501371383667],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"31_translation_languages_gender_bias_deb\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"31_translation_languages\"],\"x\":[13.87353229522705,13.208016395568848,14.106237411499023,13.092193603515625,13.336145401000977,13.386882781982422,12.319914817810059,13.021512985229492,13.98583698272705,12.727828025817871,14.158334732055664,12.96531867980957,13.214096069335938,13.292146682739258,13.683382034301758,13.187692642211914,13.279541969299316,12.989252090454102,12.971858978271484,12.976456642150879,13.035022735595703,12.499113082885742,13.1813325881958,12.979851722717285,13.296390533447266,13.039684295654297,13.842495918273926,13.132991790771484,13.9254732131958,12.167425155639648,14.098097801208496,13.920403480529785,12.558815002441406,12.849774360656738,12.970587730407715,14.02845287322998,12.911493301391602,13.765254974365234,13.905959129333496,13.326302528381348,13.942405700683594,12.925593376159668,13.41197395324707,13.25645637512207,12.991842269897461,13.592504501342773,13.934208869934082,13.955780029296875,13.298561096191406,13.263577461242676,12.345218658447266,13.947273254394531,13.009997367858887,13.643670082092285,13.180303573608398,13.74915599822998,12.808438301086426,13.863404273986816,12.991873741149902,13.162498474121094,12.859356880187988,13.075448036193848,13.156818389892578,12.886246681213379,13.997769355773926,13.03771686553955,12.784741401672363,12.894862174987793,12.97797966003418,13.60038948059082,14.051366806030273,12.930017471313477,12.712410926818848,12.782764434814453,12.85952377319336,12.852415084838867,13.387727737426758,13.939677238464355,12.471564292907715,13.912345886230469,12.641358375549316,13.367488861083984,13.8226318359375,13.64598560333252,13.757844924926758,12.249322891235352,13.732461929321289,13.127846717834473,12.968668937683105,13.813699722290039,13.807978630065918,13.008394241333008,13.787175178527832,13.993060111999512,13.531773567199707,13.984949111938477,13.01711654663086,14.002456665039062,13.822026252746582,13.829243659973145,13.329290390014648,12.908177375793457,14.121039390563965,13.951969146728516,12.813560485839844,13.843708038330078,13.839166641235352,14.121064186096191,12.558928489685059,13.817070960998535,13.30069637298584,13.859855651855469,13.09512710571289,12.266813278198242,12.803173065185547,13.087562561035156,13.733831405639648,13.884516716003418,12.79514217376709,13.156198501586914,12.635796546936035,13.271025657653809,12.98554801940918,13.871160507202148,13.031496047973633,14.019685745239258,13.770309448242188,13.913249969482422,13.995428085327148,12.395220756530762,13.254109382629395,12.519502639770508,13.016617774963379,13.239713668823242,13.700984954833984,13.32162857055664,13.05946159362793,12.602531433105469,12.626888275146484,13.298456192016602,13.010909080505371,12.473347663879395,13.30741024017334,12.52789306640625,13.24752140045166,14.047928810119629,12.949474334716797,13.951871871948242,14.054276466369629,12.840779304504395,12.930652618408203,13.249177932739258,14.00323486328125,12.957128524780273,13.71989631652832,12.736191749572754,13.225967407226562,9.189112663269043,9.039446830749512,9.115840911865234,9.096116065979004,9.138891220092773,8.879927635192871,8.893594741821289,8.761467933654785,8.84201431274414,8.92170524597168,8.984542846679688,8.870153427124023,9.148591041564941,9.1036376953125,8.759328842163086,9.173047065734863,9.000730514526367,9.311477661132812,9.226938247680664,8.916337966918945,9.266477584838867,8.85345458984375,8.894457817077637,9.150871276855469,8.838128089904785,8.816519737243652,9.046035766601562,8.901216506958008,9.159720420837402,8.813364028930664,9.301010131835938,8.885666847229004,9.035273551940918,9.151689529418945,9.173745155334473,9.153688430786133,9.068913459777832,9.077773094177246,8.93253231048584,9.23687744140625,8.845597267150879,8.906045913696289,9.104510307312012,8.794829368591309,8.974413871765137,8.559188842773438,9.103609085083008,8.886739730834961,8.847504615783691,8.852912902832031,8.7335786819458,8.971410751342773,8.835782051086426,8.914285659790039,12.199424743652344],\"y\":[1.3817375898361206,1.0612719058990479,1.1385481357574463,1.08525550365448,1.1626895666122437,1.2480696439743042,0.4848286211490631,1.2720167636871338,1.1438474655151367,1.3171401023864746,1.4076839685440063,1.264595627784729,1.0884864330291748,1.1517572402954102,1.355188012123108,1.7229746580123901,0.972616970539093,1.1486741304397583,0.8810750246047974,1.257293701171875,0.9572843313217163,0.8085752129554749,0.9309483766555786,0.7958930730819702,1.0975350141525269,0.6360377073287964,0.9854050874710083,1.798175573348999,1.3100166320800781,0.67192143201828,1.2066819667816162,1.2865713834762573,0.9056100845336914,0.6249911189079285,1.7463622093200684,1.2420947551727295,0.7832163572311401,1.1565204858779907,1.3271960020065308,1.3933436870574951,1.0287328958511353,0.7349646091461182,1.1217514276504517,1.414216160774231,1.152062177658081,1.4249526262283325,1.1767480373382568,1.2981083393096924,1.1263619661331177,1.0864992141723633,0.5083565711975098,1.0866432189941406,1.8492233753204346,1.125476598739624,0.944848895072937,1.1385208368301392,0.678071916103363,1.3165032863616943,0.6076777577400208,1.3482673168182373,1.6210135221481323,0.9811966419219971,1.2457525730133057,1.171410322189331,1.1819112300872803,0.8604665398597717,0.942077100276947,0.516913115978241,1.7408256530761719,1.4816997051239014,1.3886042833328247,1.7745593786239624,0.794830322265625,0.5049740076065063,0.8629714250564575,0.9947225451469421,1.3124425411224365,1.181308388710022,0.65378338098526,0.9567456841468811,0.8112806677818298,1.0578453540802002,1.270293116569519,1.1971663236618042,1.2050573825836182,0.49335476756095886,1.5063146352767944,1.3702335357666016,1.2522984743118286,1.2417391538619995,1.4349349737167358,1.29513418674469,1.3764736652374268,1.1924550533294678,1.3044599294662476,1.0554274320602417,0.9203598499298096,1.1263551712036133,1.4461302757263184,1.4113985300064087,1.1233012676239014,1.292132019996643,1.383273959159851,1.2921757698059082,1.4860016107559204,1.4392153024673462,1.4001719951629639,0.9837578535079956,0.6659143567085266,0.9185031056404114,1.1586668491363525,1.4281121492385864,1.1629738807678223,0.510953962802887,0.5897344946861267,0.8766728639602661,1.4713213443756104,1.1049691438674927,0.5188847184181213,1.4383823871612549,1.3952010869979858,1.3247922658920288,0.9874094128608704,1.13014554977417,0.8806256651878357,1.1321403980255127,1.1713860034942627,1.29535710811615,1.1741254329681396,0.570444643497467,1.2960554361343384,0.7852280735969543,0.9467426538467407,1.7510077953338623,1.3233586549758911,1.1152814626693726,1.0483297109603882,0.647891640663147,0.5596342086791992,1.3211227655410767,0.9677881002426147,0.6502654552459717,1.0543253421783447,0.6772043704986572,1.212782382965088,1.1973955631256104,1.1335924863815308,1.4092952013015747,1.3452917337417603,0.6293814778327942,1.8506160974502563,1.066854476928711,1.2767295837402344,0.7192075848579407,1.712160587310791,0.8005959391593933,1.1127575635910034,3.968130350112915,4.086702346801758,3.7755074501037598,3.991488218307495,3.975116014480591,4.029293060302734,4.046248435974121,4.013284683227539,4.176393032073975,4.156956672668457,4.055082321166992,3.9321210384368896,3.937089204788208,3.9232289791107178,3.8601222038269043,3.866779327392578,3.9942166805267334,3.850517511367798,3.827030897140503,4.08809232711792,3.6295218467712402,4.113508701324463,4.187417507171631,3.6063523292541504,3.9586288928985596,4.24702787399292,3.9166204929351807,4.1035475730896,3.9102394580841064,4.182351112365723,3.869297981262207,3.981309175491333,3.9789490699768066,3.9490838050842285,3.944530487060547,3.942408323287964,4.038426399230957,4.008914947509766,4.139644622802734,3.7226343154907227,4.209354877471924,4.045825958251953,3.9882144927978516,4.28642463684082,4.0150346755981445,3.615358352661133,4.036133289337158,4.028831958770752,3.9747371673583984,3.9180045127868652,3.7974116802215576,4.060174942016602,4.20427942276001,4.09943962097168,1.8563777208328247],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"#CFD8DC\",\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"other\",\"showlegend\":false,\"x\":[12.260786056518555,10.01115894317627,11.924004554748535,10.16279411315918,12.00646686553955,10.273680686950684,6.795833110809326,10.738099098205566,12.415397644042969,12.344215393066406,8.005266189575195,11.672365188598633,9.678071022033691,11.613967895507812,10.691057205200195,10.729043006896973,12.570730209350586,12.728907585144043,11.094691276550293,12.467524528503418,10.7769136428833,11.365942001342773,11.85193157196045,10.574968338012695,13.300118446350098,8.84424877166748,9.322251319885254,11.329916000366211,9.31579303741455,10.838811874389648,11.459781646728516,11.970154762268066,12.745402336120605,10.581709861755371,9.499332427978516,10.223129272460938,9.936136245727539,11.424376487731934,7.634429931640625,12.565068244934082,12.609332084655762,11.734045028686523,12.76197338104248,8.449498176574707,10.840675354003906,11.946686744689941,8.837322235107422,9.319323539733887,12.509225845336914,10.419879913330078,6.7955498695373535,9.324790954589844,12.908655166625977,11.152015686035156,11.986949920654297,12.218483924865723,11.286534309387207,11.993667602539062,9.96347427368164,11.670063018798828,9.893813133239746,10.930827140808105,9.840456008911133,9.265555381774902,8.651935577392578,9.824747085571289,9.816277503967285,12.087455749511719,11.567899703979492,6.796971321105957,12.668866157531738,12.218778610229492,10.99422836303711,11.2903470993042,13.343911170959473,12.024370193481445,12.494938850402832,12.23508071899414,10.537565231323242,9.071687698364258,9.39167308807373,13.654808044433594,11.47091293334961,11.624275207519531,12.246249198913574,12.476778984069824,11.365124702453613,12.479470252990723,12.073331832885742,10.643670082092285,10.698538780212402,11.801766395568848,11.808372497558594,9.954280853271484,12.55838394165039,8.872209548950195,11.538613319396973,8.866655349731445,10.459000587463379,11.395777702331543,9.855978012084961,11.591615676879883,10.30197811126709,11.654211044311523,11.18188190460205,10.506263732910156,13.050704002380371,11.941532135009766,12.512845993041992,11.652451515197754,13.646256446838379,11.867741584777832,12.657735824584961,12.094124794006348,10.286686897277832,12.581634521484375,11.336400985717773,9.937158584594727,11.021777153015137,9.948188781738281,12.35839557647705,12.886040687561035,9.261913299560547,9.89763355255127,9.462775230407715,9.968618392944336,12.362512588500977,10.358255386352539,8.327774047851562,11.927535057067871,10.00265884399414,12.349808692932129,10.623787879943848,10.678670883178711,11.847443580627441,9.692587852478027,9.262121200561523,10.958932876586914,12.889772415161133,10.948387145996094,11.634035110473633,12.080227851867676,10.378564834594727,10.096247673034668,11.42621898651123,8.7933931350708,11.231094360351562,8.999340057373047,11.527088165283203,12.19135570526123,7.8714375495910645,11.001914978027344,12.61523151397705,9.75306224822998,11.896245002746582,12.523083686828613,9.378335952758789,10.35689926147461,11.08590316772461,13.147154808044434,9.818265914916992,11.11213493347168,11.444499969482422,11.590463638305664,10.737903594970703,9.770731925964355,10.78557014465332,11.933521270751953,10.511313438415527,9.82026481628418,8.356404304504395,10.73415470123291,10.45881462097168,10.396477699279785,12.194050788879395,12.752180099487305,11.78748607635498,10.255226135253906,12.165645599365234,12.292634963989258,12.844697952270508,10.628177642822266,8.569395065307617,11.763611793518066,12.15890884399414,12.740022659301758,12.746966361999512,9.62110424041748,11.45886516571045,10.926743507385254,12.772931098937988,12.837591171264648,12.200713157653809,9.013678550720215,10.967081069946289,9.76983642578125,12.884422302246094,11.670434951782227,13.589189529418945,12.576118469238281,11.409375190734863,9.541470527648926,8.66622257232666,11.438347816467285,11.34601879119873,10.590363502502441,11.35364818572998,11.930695533752441,6.797292709350586,10.794591903686523,10.309884071350098,10.247868537902832,9.914916038513184,13.303763389587402,12.540189743041992,11.354313850402832,8.383038520812988,12.193739891052246,9.227989196777344,9.241358757019043,9.536521911621094,10.984012603759766,12.313669204711914,9.644474029541016,9.546388626098633,9.345878601074219,11.460395812988281,10.699294090270996,9.629450798034668,8.48170280456543,11.0349760055542,10.570517539978027,10.367918014526367,11.483748435974121,10.360127449035645,10.180638313293457,12.789478302001953,10.665085792541504,11.708563804626465,12.97873592376709,11.337641716003418,10.709122657775879,13.167486190795898,12.409531593322754,8.32143497467041,9.063648223876953,10.371134757995605,11.557292938232422,10.586905479431152,9.22173023223877,11.61303424835205,8.834911346435547,13.149587631225586,11.683677673339844,9.396690368652344,11.555591583251953,11.163105964660645,10.546306610107422,13.552511215209961,12.85168170928955,11.200281143188477,9.939804077148438,10.729233741760254,12.406366348266602,11.353873252868652,11.169713973999023,9.460506439208984,10.654193878173828,11.77177906036377,11.467199325561523,12.088428497314453,13.498458862304688,11.279095649719238,12.567978858947754,10.763456344604492,10.033524513244629,12.673567771911621,10.10020637512207,8.970285415649414,9.142569541931152,11.128690719604492,12.259600639343262,8.503023147583008,11.229401588439941,9.610413551330566,10.481382369995117,12.30782699584961,11.052600860595703,13.069833755493164,9.056635856628418,9.835722923278809,11.540289878845215,10.57509994506836,10.630483627319336,12.53976058959961,12.878406524658203,12.195188522338867,12.416590690612793,8.807579040527344,12.123668670654297,10.993809700012207,11.357285499572754,11.609417915344238,8.505378723144531,9.010087966918945,9.596749305725098,12.824377059936523,10.293586730957031,11.473138809204102,12.474746704101562,10.027557373046875,12.210655212402344,10.91020393371582,13.3267240524292,13.175515174865723,11.291525840759277,11.628263473510742,11.242234230041504,9.939929008483887,12.68603515625,9.105117797851562,12.55679702758789,10.582772254943848,11.557234764099121,12.54797649383545,11.068909645080566,12.6818208694458,11.577909469604492,11.18771743774414,13.261770248413086,11.224922180175781,11.866551399230957,9.587552070617676,10.551297187805176,11.903278350830078,12.07540225982666,10.768095970153809,12.016518592834473,8.86601448059082,12.697779655456543,12.205658912658691,10.315733909606934,11.997912406921387,12.761860847473145,11.485032081604004,12.877477645874023,10.77319622039795,9.71633529663086,12.789429664611816,10.579119682312012,12.169694900512695,11.369839668273926,12.047872543334961,11.594593048095703,11.944458961486816,11.05200481414795,9.081618309020996,12.835399627685547,12.314804077148438,9.093341827392578,11.577603340148926,9.215981483459473,11.374435424804688,12.266894340515137,11.160942077636719,8.430356979370117,12.54307746887207,11.239245414733887,10.539368629455566,11.465906143188477,7.657243728637695,12.263589859008789,9.044021606445312,11.270294189453125,9.69029426574707,12.90316104888916,11.74852466583252,8.697430610656738,10.244534492492676,13.194223403930664,8.26854419708252,12.263144493103027,12.15650463104248,10.97741985321045,11.557283401489258,12.207938194274902,9.880388259887695,13.715070724487305,8.669084548950195,11.517677307128906,12.063190460205078,10.891190528869629,8.157964706420898,13.416830062866211,12.172284126281738,9.77486515045166,12.416096687316895,8.317665100097656,12.529857635498047,10.84004020690918,8.873795509338379,12.759370803833008,10.572245597839355,11.383156776428223,11.408194541931152,12.709611892700195,13.176079750061035,7.645970344543457,10.45201587677002,11.888107299804688,8.155491828918457,10.934144973754883,12.047857284545898,11.55852222442627,10.887688636779785,10.773085594177246,10.705704689025879,9.601393699645996,10.455825805664062,9.596738815307617,10.703749656677246,11.308526992797852,9.27015209197998,10.01099681854248,12.847784996032715,12.372241020202637,10.8928804397583,11.546853065490723,9.608847618103027,10.049355506896973,10.234009742736816,12.77387809753418,7.639250755310059,9.369508743286133,11.111124992370605,10.788805961608887,9.64129638671875,10.971961975097656,13.150918960571289,10.595973014831543,9.54521369934082,12.856410026550293,10.756369590759277,11.570320129394531,10.633820533752441,10.61214542388916,11.196186065673828,11.974361419677734,11.977964401245117,9.437406539916992,11.594237327575684,11.464212417602539,10.141727447509766,11.141068458557129,9.565552711486816,12.504912376403809,10.439518928527832,8.916755676269531,12.542708396911621,8.100579261779785,13.614336013793945,9.942835807800293,12.88668441772461,12.289319038391113,10.191901206970215,11.653940200805664,12.273872375488281,9.7015380859375,11.526108741760254,11.894497871398926,9.942673683166504,12.299449920654297,11.311140060424805,12.881195068359375,8.470643043518066,12.338445663452148,11.502923011779785,10.742378234863281,12.947842597961426,11.659990310668945,12.496929168701172,8.366921424865723,9.1043119430542,12.329166412353516,11.74715518951416,11.585480690002441,10.612318992614746,11.377769470214844,11.649738311767578,11.907842636108398,13.692667007446289,11.34955883026123,12.52751636505127,8.80586051940918,12.406229019165039,12.7645902633667,12.445311546325684,11.75776195526123,11.090094566345215,11.42638111114502,10.89482593536377,10.761418342590332,11.150059700012207,10.034558296203613,10.338700294494629,11.178735733032227,12.835885047912598,11.53531551361084,12.381514549255371,11.54314136505127,11.90941047668457,9.358724594116211,10.740930557250977,11.939253807067871,11.149989128112793,11.659534454345703,10.353327751159668,11.331498146057129,10.64175796508789,12.81151008605957,8.662858009338379,10.857192993164062,10.787764549255371,8.953280448913574,12.585387229919434,7.643540382385254,10.447787284851074,10.80845832824707,12.5751371383667,9.113858222961426,9.70373821258545,12.737845420837402,11.416006088256836,12.489952087402344,8.133481979370117,10.278565406799316,11.977149963378906,9.470913887023926,10.66490364074707,10.00718879699707,9.176525115966797,11.815064430236816,8.752781867980957,11.483132362365723,11.89669418334961,12.553421020507812,13.118978500366211,9.782742500305176,10.74406623840332,12.168991088867188,11.756896018981934,12.940293312072754,11.56460189819336,8.64371109008789,11.702585220336914,11.278419494628906,10.558026313781738,11.820279121398926,10.459695816040039,11.502100944519043,11.530735969543457,10.50390911102295,12.36990737915039,12.139140129089355,11.131754875183105,10.48403263092041,13.041622161865234,11.032854080200195,11.734776496887207,10.479324340820312,9.945302963256836,11.825505256652832,11.676788330078125,9.411858558654785,12.318131446838379,11.462925910949707,12.87320613861084,13.661906242370605,12.474398612976074,10.583564758300781,11.804007530212402,11.14046573638916,11.841476440429688,11.781903266906738,9.183506965637207,11.8534574508667,11.49117660522461,11.63294506072998,11.224637985229492,12.764777183532715,12.330574035644531,11.128299713134766,12.93774127960205,10.32089614868164,12.478330612182617,11.827890396118164,11.804841041564941,11.44624137878418,11.820039749145508,10.238183975219727,7.784416675567627,12.978934288024902,12.451269149780273,9.322071075439453,11.821974754333496,10.541068077087402,11.961499214172363,8.238553047180176,12.170052528381348,12.736010551452637,12.764181137084961,11.65296745300293,12.675060272216797,11.974373817443848,10.117522239685059,9.6543607711792,12.757356643676758,12.056167602539062,9.412654876708984,11.711962699890137,9.174324989318848,10.823249816894531,9.90158462524414,10.913339614868164,8.113393783569336,9.993379592895508,12.194329261779785,13.33237361907959,11.182921409606934,12.732467651367188,13.046422004699707,9.75806713104248,8.685029029846191,11.561057090759277,10.053182601928711,11.998501777648926,13.20248031616211,12.104700088500977,10.906420707702637,11.117636680603027,10.446064949035645,9.715156555175781,12.530933380126953,12.754565238952637,11.461576461791992,11.491990089416504,9.396021842956543,8.587562561035156,12.263270378112793,11.42301082611084,13.491022109985352,10.364217758178711,11.991475105285645,12.44273853302002,11.70559310913086,10.35807991027832,9.996321678161621,9.326005935668945,12.807846069335938,13.570623397827148,11.12037467956543,10.716169357299805,10.50700569152832,10.22071361541748,12.695380210876465,11.297102928161621,9.085335731506348,12.861201286315918,11.170536994934082,10.041293144226074,10.805912017822266,11.3994722366333,9.783839225769043,11.411025047302246,10.608845710754395,9.42233657836914,11.715644836425781,11.849699020385742,12.349161148071289,8.975476264953613,10.749741554260254,9.420133590698242,10.120325088500977,12.375459671020508,10.686391830444336,11.969694137573242,11.85922622680664,12.443221092224121,9.37954330444336,13.088716506958008,10.281312942504883,12.127240180969238,11.985560417175293,12.442952156066895,12.119172096252441,9.390953063964844,10.271842956542969,10.183282852172852,12.758428573608398,9.88254451751709,11.704732894897461,11.48423957824707,9.189234733581543,11.172453880310059,11.48064136505127,10.315546035766602,12.061239242553711,11.559906005859375,11.86010456085205,11.664885520935059,12.843629837036133,9.624467849731445,12.458562850952148,12.246566772460938,11.838500022888184,10.999716758728027,9.12990951538086,11.840607643127441,11.96013069152832,11.915284156799316,12.08629035949707,10.903203964233398,8.849540710449219,11.167780876159668,10.531004905700684,12.467206954956055,10.145415306091309,9.549249649047852,6.795407295227051,12.180326461791992,12.817878723144531,10.757889747619629,11.568016052246094,11.672552108764648,7.97341775894165,10.022518157958984,9.927380561828613,10.70942211151123,11.941184043884277,12.794625282287598,12.879822731018066,12.178878784179688,11.878340721130371,10.52446460723877,10.071585655212402,10.56197738647461,11.002744674682617,12.245721817016602,10.67922592163086,12.140796661376953,12.277287483215332,11.456153869628906,12.081787109375,11.62030029296875,10.880305290222168,10.69717025756836,9.24069881439209,12.846972465515137,10.142430305480957,12.146927833557129,9.958215713500977,9.50244140625,10.51760196685791,10.551128387451172,11.390852928161621,11.688502311706543,11.491974830627441,12.741220474243164,12.295635223388672,12.218851089477539,8.685344696044922,13.047622680664062,11.958925247192383,9.383042335510254,11.218199729919434,11.51757526397705,9.624102592468262,13.13804817199707,12.326669692993164,12.624595642089844,12.161945343017578,9.43790054321289,11.324953079223633,8.503750801086426,11.257233619689941,10.58021354675293,9.62783145904541,9.703179359436035,11.924786567687988,8.906972885131836,9.254066467285156,12.899577140808105,11.277064323425293,11.152223587036133,12.76430606842041,10.148778915405273,12.378602027893066,11.560495376586914,12.818056106567383,12.91047191619873,8.343107223510742,10.998586654663086,11.666386604309082,10.141661643981934,11.702606201171875,10.432024955749512,10.108580589294434,9.184564590454102,13.428140640258789,11.512848854064941,12.044175148010254,11.576578140258789,12.28610610961914,11.385955810546875,12.952836036682129,12.021284103393555,11.287923812866211,12.099640846252441,12.082901000976562,12.028682708740234,12.383368492126465,8.773681640625,12.879551887512207,11.319714546203613,8.952622413635254,11.267468452453613,12.477086067199707,12.503398895263672,9.42236614227295,11.20785140991211,11.332497596740723,8.951685905456543,10.856745719909668,10.309080123901367,11.73598861694336,8.723488807678223,12.038217544555664,9.878068923950195,12.995759963989258,10.974395751953125,9.64431381225586,10.158196449279785,13.631218910217285,8.542502403259277,11.165329933166504,10.789525032043457,13.625770568847656,11.960083961486816,12.425254821777344,11.940552711486816,13.470690727233887,13.038926124572754,7.834033012390137,9.87307071685791,11.216033935546875,12.414644241333008,12.093451499938965,11.877202987670898,12.003032684326172,10.026086807250977,12.491192817687988,13.477025032043457,12.77425479888916,10.46001148223877,8.880654335021973,9.948962211608887,10.48256778717041,12.090274810791016,11.583839416503906,11.40567684173584,6.7874345779418945,9.088058471679688,11.758285522460938,11.31846809387207,11.73054313659668,12.907270431518555,12.770581245422363,11.114065170288086,11.227639198303223,10.648099899291992,12.066269874572754,13.3859224319458,12.3936767578125,11.536118507385254,12.09136962890625,7.971031665802002,10.897802352905273,9.173691749572754,11.609256744384766,12.269068717956543,9.517958641052246,11.856656074523926,11.710532188415527,12.882142066955566,12.293147087097168,9.226627349853516,11.812376022338867,9.573324203491211,11.98694896697998,11.025315284729004,12.620606422424316,12.66535758972168,11.626893997192383,11.87524127960205,9.740761756896973,11.66956615447998,9.677865982055664,13.708874702453613,13.700621604919434,10.8056058883667,12.719548225402832,11.30461597442627,9.15710163116455,11.5343599319458,9.219347953796387,11.886124610900879,11.679952621459961,12.069474220275879,12.361296653747559,10.542925834655762,10.790267944335938,12.395968437194824,10.26074504852295,12.145330429077148,11.74125862121582,12.40096664428711,10.930389404296875,8.466429710388184,11.920580863952637,12.118764877319336,13.445467948913574,10.998306274414062,11.986994743347168,12.903426170349121,11.159873962402344,12.18824291229248,9.372452735900879,12.712847709655762,13.078712463378906,10.631550788879395,9.169002532958984,8.790238380432129,11.811575889587402,12.28490924835205,9.769627571105957,12.567846298217773,10.349342346191406,11.191412925720215,11.537104606628418,10.127388000488281,10.513239860534668,8.915346145629883,11.823579788208008,11.909771919250488,11.520251274108887,13.776591300964355,12.594861030578613,6.803589344024658,10.791962623596191,11.956903457641602,7.651430606842041,12.864334106445312,11.632024765014648,10.91291332244873,11.907623291015625,10.763083457946777,11.546504974365234,13.262833595275879,12.147619247436523,11.872856140136719,12.41122055053711,12.41154670715332,13.054023742675781,10.615890502929688,9.164365768432617],\"y\":[1.976219654083252,1.265775442123413,1.0926563739776611,-0.5182633996009827,2.4570963382720947,0.48775091767311096,-0.910475492477417,-0.5370233654975891,1.2164555788040161,1.3136889934539795,-0.12520691752433777,1.813036561012268,1.1669845581054688,1.774245023727417,-0.3897693157196045,1.3097110986709595,3.413337469100952,1.671027660369873,0.8366751074790955,3.2766458988189697,0.3775566816329956,0.44715550541877747,2.3352620601654053,3.230710506439209,2.2592904567718506,0.6792904138565063,0.25117701292037964,0.6122933626174927,1.182349681854248,0.9189866781234741,3.0709445476531982,0.9211485385894775,3.500269889831543,3.7989087104797363,1.823943853378296,1.6083993911743164,-0.23820368945598602,2.4324207305908203,0.19927099347114563,3.7869391441345215,2.054600715637207,0.9205240607261658,3.078073263168335,0.10503587871789932,0.8371968865394592,0.3198699951171875,-0.16870951652526855,2.7976632118225098,2.6678123474121094,-0.570141077041626,-0.9106079339981079,3.7905776500701904,2.7144033908843994,0.9463844299316406,1.3737200498580933,0.33007562160491943,-0.9850686192512512,1.1608271598815918,1.0532095432281494,2.060023546218872,1.5419471263885498,1.308414101600647,-1.1935114860534668,-0.17752167582511902,-0.4857664108276367,3.832371234893799,1.3047598600387573,3.552154541015625,1.942522644996643,-0.909134566783905,2.3067994117736816,0.9871791005134583,2.9792444705963135,2.830186128616333,0.7007690072059631,3.5236191749572754,3.808678388595581,0.5856729745864868,0.5755625367164612,-0.488295316696167,0.6351603269577026,0.5080202221870422,1.0401160717010498,2.3617639541625977,1.8510946035385132,0.9317828416824341,3.2759201526641846,2.4336321353912354,-1.299113392829895,3.8617777824401855,3.202488422393799,1.037284016609192,-0.5317288637161255,-0.5715892314910889,1.2100167274475098,1.357236623764038,3.533149480819702,3.0465500354766846,0.1627165526151657,0.7375492453575134,0.9511770009994507,0.07118048518896103,-0.3919447958469391,1.378462314605713,-0.3244394361972809,0.7082329988479614,2.143453359603882,1.1407673358917236,0.9031526446342468,2.8212311267852783,1.1145505905151367,1.4429399967193604,2.296320676803589,-0.001279126270674169,-0.10712704062461853,3.0526630878448486,0.7859264612197876,-0.6041418313980103,1.40556800365448,-0.5446928143501282,1.2216320037841797,3.4056971073150635,3.86862850189209,1.1494468450546265,0.1541697382926941,-0.2646180987358093,2.934096097946167,1.085005283355713,0.1298372447490692,1.4595853090286255,0.5229455232620239,1.7139097452163696,3.179950475692749,0.8122704029083252,2.313836097717285,0.9151896834373474,0.0694771260023117,2.251661777496338,2.7393033504486084,1.8044404983520508,2.4212090969085693,1.369358777999878,1.4411309957504272,1.778269648551941,1.9663785696029663,-0.2085247039794922,1.8884222507476807,0.01605393923819065,0.9991214275360107,2.1229145526885986,-0.17486093938350677,1.2444441318511963,1.5270284414291382,1.034622311592102,0.5874497890472412,1.0864499807357788,0.7937063574790955,0.2653231918811798,0.6648417711257935,2.1787331104278564,-1.4033302068710327,0.993794858455658,3.9591336250305176,2.2620389461517334,2.2779126167297363,1.026957631111145,1.6402356624603271,1.106472134590149,3.609027624130249,4.001310348510742,2.4320061206817627,1.3439748287200928,1.444767951965332,0.8551923036575317,0.15037234127521515,2.425750494003296,2.3774797916412354,2.089541435241699,0.14875546097755432,1.2632057666778564,1.3453816175460815,3.9026010036468506,0.47671282291412354,1.526449203491211,1.530004620552063,0.44732722640037537,1.5337772369384766,-0.5403087735176086,-1.0967426300048828,0.833954393863678,0.7645677924156189,2.70750093460083,1.2675151824951172,0.1613716185092926,3.286445379257202,0.9860133528709412,3.4668221473693848,1.4706974029541016,-0.06515169888734818,3.346012830734253,3.19698166847229,1.0387732982635498,-0.47503626346588135,1.1350661516189575,-0.12550556659698486,0.48462581634521484,3.611987352371216,0.44945693016052246,-0.9105373620986938,0.9302946925163269,2.0090830326080322,0.1268618106842041,-0.5888694524765015,1.5191290378570557,2.7231905460357666,3.63657283782959,0.20960384607315063,0.04279949516057968,2.0899455547332764,2.395681142807007,-0.3550689220428467,1.1873329877853394,1.6772342920303345,-0.551196813583374,1.0045689344406128,3.5376405715942383,-0.2728564739227295,1.6083232164382935,3.451906681060791,0.4097013771533966,0.28619876503944397,2.9995458126068115,1.430558204650879,2.9486751556396484,1.4180095195770264,3.394707679748535,-1.0172475576400757,1.6186503171920776,2.2325010299682617,3.3960022926330566,2.3339428901672363,2.508234739303589,1.917661428451538,2.007206678390503,0.7356185913085938,1.3998340368270874,1.426846981048584,-0.10650590807199478,3.196619987487793,1.5687898397445679,3.3810524940490723,-0.20458725094795227,1.947638988494873,-0.17957328259944916,0.6638663411140442,1.4990065097808838,1.0839821100234985,-0.13390646874904633,1.798355221748352,1.460438847541809,2.6365201473236084,2.2331058979034424,0.5038972496986389,1.672692894935608,3.618595838546753,2.4914488792419434,-0.7637554407119751,2.602285861968994,1.6152315139770508,2.3165502548217773,4.036905288696289,1.7626341581344604,-0.30745241045951843,1.3061516284942627,3.0544943809509277,2.2432503700256348,0.5260573625564575,0.26869526505470276,0.03409036993980408,-0.5223769545555115,0.39339277148246765,2.0766706466674805,-0.38802358508110046,3.801676034927368,2.3770365715026855,3.5236270427703857,3.0124449729919434,3.137105703353882,3.03592848777771,1.0055789947509766,2.3000667095184326,3.485448122024536,2.8649325370788574,3.2783849239349365,2.067582130432129,1.7887386083602905,1.117841124534607,1.9235390424728394,-0.5475263595581055,2.5567400455474854,-0.42588913440704346,1.6459540128707886,3.0523805618286133,3.548125743865967,0.9675278663635254,1.0511583089828491,1.8744837045669556,-0.2662609815597534,2.020230531692505,2.395968198776245,1.5245238542556763,2.208904504776001,-0.23743867874145508,2.1941235065460205,3.066039800643921,0.8780515789985657,-0.3361736238002777,0.12655974924564362,0.5816671252250671,2.5916388034820557,1.8386311531066895,0.9971460103988647,-0.09463711082935333,1.4201574325561523,3.414994478225708,-0.5812013745307922,1.0859824419021606,3.207399845123291,1.0073139667510986,1.059942603111267,0.9441660642623901,2.421081066131592,-0.9699038863182068,-0.7918293476104736,3.2947211265563965,3.453516721725464,2.6633572578430176,1.12492835521698,0.9515612125396729,1.8676871061325073,2.2264864444732666,0.4588735103607178,-0.7277568578720093,-1.0359772443771362,2.857710599899292,2.6983368396759033,1.5439820289611816,0.8810650110244751,3.6186933517456055,3.788325071334839,2.0987229347229004,3.1248533725738525,-0.6990334391593933,3.2795536518096924,0.4755188822746277,2.3451409339904785,1.9918140172958374,2.239830255508423,2.980741500854492,3.4494681358337402,2.7023935317993164,0.8716931343078613,1.143310546875,0.12861472368240356,1.2377668619155884,-0.3922814130783081,3.3432180881500244,1.7890307903289795,0.9444081783294678,2.8529300689697266,0.18657220900058746,2.6263394355773926,0.48836591839790344,0.770980954170227,0.9084759950637817,2.9118480682373047,0.899668276309967,0.3940073549747467,0.8528211116790771,1.6868844032287598,0.24871914088726044,2.4808976650238037,2.190305709838867,1.4015684127807617,-0.11070117354393005,1.4967073202133179,0.9283192753791809,0.4603424668312073,1.1601427793502808,1.9638222455978394,-1.3094513416290283,1.8755643367767334,-0.034235879778862,2.5533201694488525,2.5379700660705566,3.77651309967041,2.733030319213867,0.3953371047973633,2.7038984298706055,1.0902845859527588,3.95662522315979,3.4667394161224365,1.4680428504943848,-0.2102063000202179,1.9835692644119263,0.25430288910865784,2.2354414463043213,0.19437259435653687,1.457194447517395,1.478986144065857,0.024254370480775833,0.9681276082992554,-1.2682996988296509,1.4637945890426636,0.9524343013763428,3.2517035007476807,1.0663663148880005,1.047045350074768,2.5798704624176025,1.9295272827148438,-0.4900413751602173,1.2294294834136963,3.744098663330078,1.1958093643188477,3.4060301780700684,1.4086087942123413,3.195308208465576,-0.21053600311279297,1.9594486951828003,1.3294121026992798,0.7901973128318787,-1.0279371738433838,0.18727345764636993,2.7890584468841553,1.1563249826431274,-0.1735643744468689,2.2202253341674805,0.9077820777893066,2.668337821960449,2.603579521179199,0.24776341021060944,1.158795714378357,1.6285470724105835,1.96036958694458,3.842803716659546,-0.05351594090461731,1.0603022575378418,3.5160486698150635,3.203173875808716,-0.22633570432662964,1.7337943315505981,1.1724951267242432,0.17315110564231873,0.7105343341827393,2.4431610107421875,3.8104262351989746,0.10712434351444244,0.713238537311554,3.1322901248931885,3.2645468711853027,0.534716784954071,0.5656581521034241,3.5241639614105225,2.5362460613250732,-0.7327529191970825,-0.8334300518035889,2.0500335693359375,0.9878270626068115,2.0549156665802,3.280917167663574,0.543793261051178,0.40602004528045654,1.2652292251586914,1.6847025156021118,0.6613606810569763,0.48743921518325806,1.5848220586776733,0.15714702010154724,3.097687244415283,3.4039037227630615,3.8253872394561768,-0.3216516077518463,0.36484676599502563,3.5613582134246826,0.849107563495636,2.804208278656006,3.8222780227661133,1.8004229068756104,1.4634736776351929,3.1937408447265625,0.8675617575645447,3.476331949234009,0.3727441728115082,3.1206748485565186,1.5812299251556396,-0.6251404285430908,3.3036246299743652,3.7486493587493896,1.7302109003067017,2.3743913173675537,0.9696566462516785,0.4081820845603943,0.6557876467704773,2.6788806915283203,3.448852300643921,1.163936972618103,3.5292248725891113,2.8257150650024414,3.515986680984497,1.9399545192718506,0.4077995717525482,0.766963541507721,-0.4135100543498993,0.42341694235801697,2.8088972568511963,-0.6076659560203552,0.1522863209247589,3.2291979789733887,1.588698387145996,1.8325613737106323,0.9248793125152588,1.3020033836364746,3.420180559158325,1.643860101699829,1.1540601253509521,0.21492479741573334,0.1739095002412796,1.5416576862335205,3.289651393890381,-0.5035665035247803,-0.46198034286499023,3.0262765884399414,3.655320405960083,1.8784151077270508,0.3909582197666168,0.8705004453659058,1.1364428997039795,1.208755612373352,0.26025456190109253,0.44635772705078125,0.12183551490306854,-0.15554407238960266,-0.25469839572906494,-0.45932525396347046,0.3801633417606354,2.1852316856384277,3.0103812217712402,0.16040968894958496,0.17387855052947998,2.1033482551574707,3.5950064659118652,1.6760215759277344,3.2061989307403564,0.3962271809577942,2.2076709270477295,-0.8994259834289551,2.514460802078247,0.979424774646759,3.5055930614471436,0.26043587923049927,0.08888838440179825,0.9722239375114441,1.9090890884399414,1.4592763185501099,0.7288630604743958,0.5196076035499573,2.52851939201355,0.3626737594604492,0.9660953879356384,1.4570801258087158,1.5067415237426758,1.0723236799240112,1.9673024415969849,0.5451234579086304,2.6414546966552734,0.7439343333244324,3.4168541431427,1.108274221420288,0.9759180545806885,3.0499231815338135,0.3342808485031128,1.1762615442276,1.2191663980484009,1.9902498722076416,2.7178072929382324,2.1931447982788086,1.4139740467071533,3.4444408416748047,-0.2844769358634949,2.468306541442871,2.982534408569336,1.1627116203308105,2.345379114151001,-0.26387879252433777,2.15932035446167,-0.09391316771507263,2.315988540649414,3.9019370079040527,1.4993696212768555,0.8209245204925537,0.1551581770181656,2.0490360260009766,1.3861424922943115,1.0227948427200317,-0.12519274652004242,3.113816976547241,1.2089561223983765,-0.26088747382164,1.8862637281417847,3.144066333770752,-1.0393073558807373,2.0746145248413086,1.443308711051941,1.180871844291687,-0.6809756755828857,1.017892599105835,1.8907074928283691,-1.2943123579025269,0.1569855809211731,0.6024736166000366,0.8391635417938232,2.21470046043396,-1.1607123613357544,1.8066606521606445,2.297105312347412,2.370729446411133,1.7525073289871216,0.9918195605278015,0.6040194034576416,2.8629562854766846,3.4195144176483154,1.8813725709915161,3.7637126445770264,0.20570334792137146,2.110116958618164,1.3198965787887573,1.0308681726455688,1.5510575771331787,1.0269742012023926,3.243104934692383,0.5696408748626709,-1.0560033321380615,3.80950927734375,-0.9885093569755554,2.1285574436187744,0.27239152789115906,-0.9925388693809509,2.664095401763916,0.5445898771286011,3.4431591033935547,2.9532716274261475,1.4311859607696533,2.9324100017547607,3.054269313812256,0.8955436944961548,0.025119181722402573,3.7716782093048096,0.6967297196388245,2.6491522789001465,2.385422945022583,3.098644495010376,3.803661823272705,1.4175082445144653,2.0697543621063232,3.0386083126068115,3.8905580043792725,1.805167555809021,3.370250940322876,0.6452329754829407,1.198987364768982,4.435891628265381,3.9789042472839355,0.2224683314561844,0.784505307674408,3.900918483734131,-0.08793401718139648,-0.20969590544700623,0.9383320212364197,2.9824304580688477,0.4146740138530731,3.451068878173828,0.58146733045578,2.629016876220703,2.5896897315979004,0.8220140933990479,2.19604229927063,2.3683815002441406,1.0513523817062378,-0.5379165410995483,2.0229125022888184,2.577913522720337,1.3024059534072876,2.953939914703369,1.1526473760604858,-1.245646595954895,0.6586165428161621,1.9431781768798828,0.795759916305542,0.4541209042072296,1.3323132991790771,0.9036451578140259,1.0493559837341309,2.936338424682617,1.658136010169983,2.7779719829559326,0.217825248837471,-0.08116167783737183,0.8917245864868164,1.92644202709198,1.5383085012435913,3.03080153465271,-0.5870935916900635,1.1749986410140991,1.7433806657791138,3.2300631999969482,1.3536816835403442,1.4494167566299438,2.132977247238159,1.142106533050537,1.0964027643203735,-1.2843916416168213,2.0309720039367676,0.38181358575820923,0.4180848002433777,1.397927165031433,1.0205312967300415,-1.0413587093353271,-0.3866738975048065,-0.9114582538604736,2.2314321994781494,-0.9689415693283081,0.8953730463981628,-0.7223781943321228,3.3937878608703613,-0.1200912594795227,1.4780992269515991,-1.0540677309036255,1.243822693824768,1.1921714544296265,-0.9881787300109863,2.7142724990844727,1.035949468612671,1.7600966691970825,3.096761703491211,1.0765949487686157,-0.3055479824542999,0.8458605408668518,1.8884222507476807,0.7532628178596497,3.692111015319824,2.047429084777832,1.5398664474487305,2.6263246536254883,3.3768866062164307,1.3354458808898926,2.8829421997070312,3.073280096054077,2.618222951889038,2.2130420207977295,1.131805658340454,1.4583265781402588,-0.6291894912719727,3.3120579719543457,3.235522985458374,2.4912800788879395,0.8031163215637207,2.785531520843506,1.2172937393188477,2.5934207439422607,0.44533485174179077,3.7607927322387695,0.8943837881088257,2.2763075828552246,0.15780554711818695,2.040842056274414,0.27494242787361145,3.4471356868743896,2.545928955078125,3.3168694972991943,3.057359218597412,2.0106849670410156,3.7272539138793945,-0.2229020744562149,-0.3912656009197235,2.5650482177734375,0.5276594758033752,1.9719535112380981,3.524745225906372,1.5316689014434814,0.3282936215400696,2.1106743812561035,2.4451394081115723,1.7900829315185547,2.8499183654785156,2.917431354522705,-0.6730669736862183,2.2880237102508545,2.3319878578186035,0.7816553711891174,3.079967975616455,0.3783358633518219,3.771744728088379,3.4605367183685303,3.4098503589630127,2.232668399810791,2.7577970027923584,0.8026202917098999,0.8486886024475098,1.336401104927063,1.9623465538024902,2.0950241088867188,2.3696439266204834,2.694972276687622,0.7788678407669067,1.5454350709915161,0.642559289932251,1.9104548692703247,2.817577838897705,1.2944279909133911,2.2089552879333496,2.9942898750305176,0.6504705548286438,2.6843504905700684,1.2824846506118774,-0.555378794670105,1.9917773008346558,2.4853670597076416,2.2720730304718018,0.05783434212207794,1.741772174835205,3.9805495738983154,1.6283323764801025,3.9042468070983887,-0.08662469685077667,1.0114973783493042,-0.2651676535606384,2.276035785675049,-0.6100764274597168,2.0067296028137207,1.3208794593811035,1.0322285890579224,1.9896197319030762,2.3109211921691895,2.2940375804901123,0.7046573758125305,2.1666882038116455,0.8269322514533997,1.038419246673584,3.1033222675323486,2.011228084564209,0.4995725154876709,3.0119717121124268,1.7737351655960083,1.1829763650894165,3.768042802810669,2.453611135482788,-1.2905018329620361,0.4045698344707489,-0.7276555299758911,1.108733057975769,0.7043386101722717,1.7317860126495361,3.1328790187835693,1.5019123554229736,0.00757896201685071,1.7896612882614136,4.704597473144531,-1.2845878601074219,1.471731185913086,3.7090561389923096,-0.9188922643661499,0.25417712330818176,0.40908655524253845,-0.6070359349250793,3.279542922973633,2.3822507858276367,-1.0290439128875732,-1.0308316946029663,1.6611809730529785,0.5246748328208923,1.5900543928146362,1.5552963018417358,1.9574549198150635,0.3487606346607208,1.906638503074646,-0.15992236137390137,-0.1188686341047287,1.874509334564209,2.240257501602173,3.598365068435669,2.4335744380950928,3.23817777633667,1.554663896560669,2.292228937149048,1.6409223079681396,1.495506763458252,3.2188000679016113,3.7398290634155273,2.8911819458007812,3.769780397415161,2.451735019683838,2.5036814212799072,3.150447130203247,1.2291960716247559,0.9691900610923767,3.689431667327881,-0.07314413040876389,0.73487389087677,0.839801013469696,1.4524890184402466,2.1931369304656982,1.3259851932525635,3.0612916946411133,0.2924751043319702,0.6916547417640686,2.3885679244995117,0.7535249590873718,1.8109625577926636,3.2876040935516357,2.6223843097686768,2.415797472000122,2.450838565826416,1.9013489484786987,2.8400003910064697,3.637033224105835,1.2191064357757568,0.49888309836387634,0.527450680732727,3.3894202709198,2.8021371364593506,1.5861856937408447,-0.5647964477539062,1.773905634880066,2.8972055912017822,0.4014616310596466,3.552330493927002,3.5676872730255127,0.44267264008522034,2.05513334274292,2.6361255645751953,2.9887442588806152,0.289135605096817,3.268169641494751,3.1244053840637207,0.8573437333106995,-0.19672121107578278,-0.7705172300338745,1.6629849672317505,3.544947862625122,-0.6638178825378418,3.600919008255005,-0.5163041353225708,-0.025210803374648094,1.4350130558013916,1.8289581537246704,0.6889894008636475,1.6060850620269775,-0.9072462916374207,2.4202475547790527,1.693398356437683,0.15449632704257965,3.578507661819458,2.314469575881958,4.283806324005127,2.3266613483428955,0.48093342781066895,3.2087161540985107,-0.08493365347385406,1.7356622219085693,1.580642580986023,1.29006028175354,0.9935421347618103,2.0631296634674072,2.585738182067871,1.8831286430358887],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"4_clinical_medical_patients\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"4_clinical_medical_patients\"],\"x\":[7.761767864227295,7.890107154846191,8.475704193115234,8.049864768981934,7.910464286804199,7.735909938812256,8.021018028259277,7.9187726974487305,7.908551216125488,7.699703216552734,7.959645748138428,7.826615333557129,8.282672882080078,7.934024333953857,8.200965881347656,7.851012229919434,7.659298419952393,7.964986801147461,7.375672340393066,7.989088535308838,8.16100788116455,7.880589008331299,7.8358564376831055,7.8488359451293945,7.915194511413574,8.008590698242188,7.819448471069336,7.807568073272705,7.392760753631592,7.372259140014648,7.680557727813721,8.323735237121582,7.405981540679932,7.390983581542969,8.47547721862793,7.814692974090576,8.292101860046387,8.115535736083984,7.816876411437988,7.764798164367676,7.4307541847229,8.173230171203613,7.816125869750977,7.800354957580566,7.687350749969482,7.416838645935059,8.04630184173584,7.372389793395996,8.061315536499023,7.425755023956299,7.877260684967041,8.388016700744629,7.779290199279785,8.227629661560059,8.057212829589844,8.02259349822998,7.782587051391602,8.23399543762207,7.90402889251709,7.8760199546813965,7.4668121337890625,8.308961868286133,7.824159622192383,7.712260723114014,7.914191246032715,7.685945987701416,7.940428733825684,7.877172470092773],\"y\":[1.5433651208877563,1.4605567455291748,1.4761929512023926,1.4625173807144165,1.4584766626358032,1.4409211874008179,1.4749916791915894,1.4284682273864746,1.4748163223266602,1.4806615114212036,1.4631659984588623,1.411305546760559,1.4848947525024414,1.0450985431671143,1.4943875074386597,1.3517125844955444,1.4878875017166138,1.4422603845596313,1.5346200466156006,1.5406434535980225,1.499199628829956,1.4141327142715454,1.4414336681365967,1.3886102437973022,1.4608150720596313,1.4566928148269653,1.5145379304885864,1.6541327238082886,1.5264090299606323,1.531654953956604,1.4845945835113525,1.4692519903182983,1.5288569927215576,1.5096898078918457,1.3595173358917236,1.587659478187561,1.4848111867904663,1.480909824371338,1.49443781375885,1.6546357870101929,1.5368572473526,1.2316579818725586,1.3251349925994873,1.5530869960784912,1.5514334440231323,1.5379899740219116,1.494699239730835,1.525313377380371,1.4642879962921143,1.526352882385254,1.288293480873108,1.483452320098877,1.4212583303451538,1.5053179264068604,1.3147481679916382,1.3840261697769165,1.5364909172058105,1.516196370124817,1.4429274797439575,1.4955360889434814,1.5462852716445923,1.454986810684204,1.4761298894882202,1.4729735851287842,1.450022578239441,1.443420648574829,1.4701296091079712,1.4678795337677002],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"5_hyperparameters_metaretriever_pts\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"5_hyperparameters_metaretriever_pts\"],\"x\":[12.020341873168945,12.169936180114746,12.101693153381348,12.094861030578613,11.659080505371094,12.379535675048828,12.050992965698242,12.262616157531738,12.431180000305176,11.988661766052246,12.082118034362793,12.085043907165527,12.180397033691406,12.176328659057617,11.84795093536377,12.084325790405273,12.128952980041504,12.28459644317627,12.194994926452637,12.008187294006348,11.911856651306152,12.048596382141113,12.229466438293457,12.182258605957031,12.1466703414917,12.20826530456543,11.903266906738281,11.976790428161621,11.813048362731934,12.021183967590332,12.020978927612305,12.224409103393555,11.788299560546875,12.209701538085938,11.93454647064209,11.924176216125488,12.019671440124512,12.825284004211426,12.15970516204834,12.297412872314453,12.158596992492676,12.110843658447266,12.26720905303955,12.509387016296387,12.426708221435547,12.31076431274414,12.200416564941406,12.108731269836426,12.027480125427246,12.294480323791504,12.179040908813477,11.868775367736816,12.016388893127441,12.533409118652344,11.871016502380371,12.278585433959961,12.497551918029785,11.953156471252441,12.103267669677734,12.292784690856934,12.235295295715332,11.931829452514648,12.1331148147583],\"y\":[3.4334287643432617,3.9167301654815674,3.949054002761841,3.438246250152588,3.445382595062256,3.36970853805542,3.7780611515045166,2.977111577987671,3.824798583984375,4.014473915100098,3.966614246368408,3.947669744491577,3.546800136566162,3.5740740299224854,3.722076654434204,3.7931604385375977,3.4850759506225586,3.1680963039398193,4.030865669250488,3.494990587234497,3.631016969680786,3.9821364879608154,3.874598264694214,3.628248453140259,3.362018585205078,3.6282668113708496,3.4427807331085205,3.539586305618286,3.9404172897338867,3.503599166870117,3.3990092277526855,3.441103219985962,2.9686286449432373,3.454322576522827,3.507044553756714,3.588791847229004,3.930246591567993,3.598909854888916,3.4998040199279785,3.598375082015991,3.547398805618286,3.815009355545044,3.1761443614959717,3.7004573345184326,2.9148776531219482,3.357351303100586,3.646010398864746,4.046837329864502,3.844369649887085,3.3957107067108154,3.5331528186798096,3.4589388370513916,3.6157045364379883,3.513286590576172,3.4719655513763428,3.176704168319702,3.3323616981506348,3.9445858001708984,3.7024219036102295,3.222160577774048,3.219435214996338,3.6485061645507812,3.5754635334014893],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"6_adversarial_attacks_attack\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"6_adversarial_attacks_attack\"],\"x\":[10.677026748657227,10.417326927185059,10.19378662109375,10.34382438659668,10.359889030456543,10.168207168579102,10.379969596862793,10.382912635803223,10.3951997756958,10.006451606750488,10.593812942504883,10.523648262023926,10.564718246459961,10.390353202819824,10.409890174865723,10.375975608825684,10.270461082458496,10.380226135253906,10.342330932617188,10.389986038208008,10.570111274719238,10.563359260559082,10.393143653869629,10.445023536682129,10.406803131103516,10.406514167785645,9.976093292236328,10.553617477416992,10.514777183532715,10.404207229614258,10.352548599243164,10.402541160583496,10.373939514160156,10.66649055480957,10.147748947143555,10.177634239196777,10.380538940429688,10.481019020080566,10.569191932678223,10.38936710357666,10.453319549560547,10.417245864868164,10.462665557861328,10.532641410827637,10.139423370361328,10.995478630065918,10.441753387451172,10.408876419067383,10.595641136169434,10.508939743041992,10.599928855895996,10.383362770080566,10.315134048461914,10.520706176757812,10.196410179138184,10.421682357788086,10.112748146057129,10.42435073852539,9.957427978515625,10.400481224060059],\"y\":[4.530323028564453,4.6652750968933105,4.259049415588379,4.17231559753418,4.669703960418701,4.087711334228516,4.67042350769043,4.63741397857666,4.61146879196167,4.00691556930542,4.465812683105469,4.730312824249268,3.997835874557495,4.689051151275635,4.215987682342529,4.713289260864258,3.9740796089172363,4.662024021148682,4.6290106773376465,4.683803558349609,4.155202865600586,4.592298984527588,4.655975341796875,4.570188999176025,4.67529296875,4.149698734283447,4.111392498016357,4.625186920166016,4.129754066467285,4.608168125152588,4.609062194824219,4.684344291687012,4.598238468170166,4.515726089477539,4.097564697265625,4.126251697540283,4.632607936859131,3.846534013748169,4.030094146728516,4.677065372467041,4.595035552978516,4.631924152374268,4.152017593383789,4.6212286949157715,4.092752933502197,4.058029651641846,4.524569034576416,4.68303918838501,4.761435031890869,4.133310794830322,4.037563800811768,4.604251384735107,4.569831848144531,4.141808032989502,4.4533562660217285,4.653535842895508,4.054136753082275,4.630998134613037,3.962613344192505,4.415659427642822],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"7_summarization_summaries_summary\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"7_summarization_summaries_summary\"],\"x\":[9.407848358154297,9.24934196472168,9.988895416259766,9.616469383239746,9.254940032958984,9.619887351989746,9.763313293457031,9.41704273223877,9.903557777404785,9.640640258789062,9.904218673706055,9.833464622497559,9.501309394836426,10.281272888183594,9.5150146484375,9.301063537597656,9.468867301940918,9.583669662475586,9.652227401733398,9.2976655960083,9.610597610473633,9.950066566467285,9.628329277038574,9.354300498962402,9.685493469238281,9.437472343444824,9.71446418762207,9.970356941223145,9.591911315917969,9.4934720993042,11.245697975158691,9.630011558532715,9.857994079589844,9.644820213317871,9.684257507324219,9.61834716796875,9.774521827697754,9.085481643676758,10.036049842834473,9.738116264343262,9.506299018859863,9.30492877960205,9.699036598205566,9.771791458129883,9.192770957946777,9.514463424682617,9.63692855834961,9.843480110168457,9.38808536529541,9.520946502685547,9.217379570007324,9.838109970092773,9.648056030273438,9.77692699432373,9.458232879638672,9.670414924621582,9.680063247680664,9.057123184204102,9.632370948791504],\"y\":[1.5877009630203247,1.2727618217468262,1.4802215099334717,1.6104549169540405,1.4569437503814697,1.539625644683838,1.472213864326477,2.0593249797821045,1.3333063125610352,1.6195672750473022,0.646668553352356,1.6028765439987183,1.468073844909668,1.4336971044540405,1.469533920288086,1.5507453680038452,1.6590402126312256,1.5533472299575806,1.5326228141784668,1.4909952878952026,1.5933552980422974,0.6876698732376099,1.5868359804153442,1.573804259300232,1.5753282308578491,1.2330971956253052,1.6163599491119385,0.7645369172096252,1.5960463285446167,1.578489899635315,0.5172238349914551,1.5027610063552856,1.38727867603302,1.5584850311279297,1.571655035018921,1.5743860006332397,1.5758932828903198,1.8300682306289673,1.7321003675460815,1.5929148197174072,1.4410037994384766,1.594342589378357,1.557225227355957,1.3510727882385254,1.2829564809799194,1.1802170276641846,1.5979278087615967,1.6627514362335205,1.6593539714813232,1.5607953071594238,1.3650426864624023,0.5767338275909424,1.5249618291854858,1.5589072704315186,1.6015998125076294,1.5737286806106567,1.52560555934906,1.5944088697433472,1.4602868556976318],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"8_sentiment_emotion_emotional\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"8_sentiment_emotion_emotional\"],\"x\":[9.02020263671875,8.764460563659668,9.149687767028809,8.96601676940918,8.83845329284668,7.854395389556885,8.691113471984863,8.074891090393066,7.991252422332764,8.15771198272705,8.11121940612793,7.889158725738525,8.559086799621582,7.732449054718018,8.129582405090332,7.739405155181885,8.906782150268555,8.779287338256836,8.384539604187012,8.096807479858398,7.73988151550293,7.74143123626709,10.807848930358887,8.938881874084473,8.25653076171875,8.156562805175781,8.910407066345215,8.929011344909668,8.912620544433594,8.349308967590332,9.150344848632812,9.063459396362305,8.188298225402832,8.082099914550781,8.124395370483398,8.095884323120117,7.761401653289795,7.85822057723999,7.735099792480469,9.143465995788574,7.9051079750061035,8.62359619140625,8.771903991699219,8.10483455657959,8.881553649902344,7.842296600341797,7.751319408416748,8.932684898376465,8.793137550354004,8.686760902404785,8.676131248474121,8.958822250366211,7.80643892288208,8.740694046020508,7.7468767166137695,8.771867752075195,8.442422866821289],\"y\":[2.3929059505462646,2.3886959552764893,2.2481915950775146,2.366041660308838,2.3146793842315674,2.3162174224853516,2.369158983230591,2.5022075176239014,2.4162755012512207,2.523205280303955,2.5030629634857178,2.322309732437134,2.426605224609375,2.189079761505127,2.5072808265686035,2.1927618980407715,2.2802486419677734,2.404411792755127,2.5210015773773193,2.5109353065490723,2.188095808029175,2.2342336177825928,1.3085989952087402,2.270885467529297,2.520770311355591,2.4987809658050537,2.2820372581481934,2.2575674057006836,2.305651903152466,2.480987310409546,2.6763017177581787,2.3891241550445557,2.503016233444214,2.507106065750122,2.5494508743286133,2.4986634254455566,2.2242064476013184,2.3021676540374756,2.150664806365967,2.744899034500122,2.3470451831817627,2.496939182281494,2.4149718284606934,2.483858346939087,2.3855793476104736,2.2803268432617188,2.255380868911743,2.259631872177124,2.3189663887023926,2.3722052574157715,2.399129867553711,2.2720420360565186,2.2237091064453125,2.365219831466675,2.2338740825653076,2.3938751220703125,2.3587722778320312],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"10_qa_question_questions\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"10_qa_question_questions\"],\"x\":[9.40961742401123,9.401433944702148,9.173538208007812,9.561808586120605,9.133511543273926,9.540070533752441,9.700275421142578,9.41574478149414,9.289504051208496,9.243343353271484,9.41695785522461,9.496110916137695,9.767770767211914,9.445943832397461,9.416340827941895,9.574084281921387,9.292332649230957,9.226899147033691,9.462366104125977,9.370545387268066,9.44404411315918,9.387925148010254,9.353792190551758,9.420747756958008,9.548852920532227,9.489824295043945,9.414810180664062,9.33342456817627,9.402825355529785,9.672316551208496,9.393604278564453,9.239797592163086,9.441904067993164,9.516083717346191,9.340642929077148,9.363749504089355,9.59388256072998,9.435190200805664,9.716001510620117,9.322286605834961,9.686580657958984,9.433467864990234,9.423392295837402,9.48121166229248,9.288214683532715,9.267616271972656,9.553618431091309,9.478880882263184,9.371923446655273,9.760424613952637,9.519524574279785,9.401512145996094,9.502867698669434,9.440361976623535],\"y\":[-0.5178394317626953,-0.650516152381897,-0.479851096868515,-0.4310290515422821,-0.5378267168998718,-0.6259913444519043,-0.7380387187004089,-0.21319614350795746,-0.7479232549667358,-0.610841691493988,-0.6754353642463684,-0.5430178642272949,-0.6768491268157959,-0.45748335123062134,-0.3581988215446472,-0.8913076519966125,-0.698038637638092,-0.7323350310325623,-0.4265297055244446,-0.6116015911102295,-0.9226518869400024,-0.7730879783630371,-0.7615750432014465,-0.7331415414810181,-0.8823621869087219,-0.8506748676300049,-0.7885114550590515,-0.723310649394989,-0.7920389175415039,-0.6305829882621765,-0.698114275932312,-0.7590868473052979,-0.9453780651092529,-0.47471097111701965,-0.7153941988945007,-0.7514808177947998,-0.32000821828842163,-0.5421023368835449,-0.5268405079841614,-0.7256784439086914,-0.6647657752037048,-0.5601599812507629,-0.8130180239677429,-0.5797785520553589,-0.76743084192276,-0.740344226360321,-0.6408556699752808,-0.4104425609111786,-0.3353818655014038,-0.6775698661804199,-0.7086683511734009,-0.9370354413986206,-0.7324972152709961,-0.6511043310165405],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"11_transformer_nat_transformers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"11_transformer_nat_transformers\"],\"x\":[13.446449279785156,13.293713569641113,13.339015007019043,13.234209060668945,13.390213012695312,13.275945663452148,13.059819221496582,13.199735641479492,13.168375968933105,13.352516174316406,13.391827583312988,13.276883125305176,13.3883638381958,13.391206741333008,13.22536563873291,13.355441093444824,13.415063858032227,13.445826530456543,13.263446807861328,13.253372192382812,13.080596923828125,13.292510986328125,13.387595176696777,13.407983779907227,13.065677642822266,13.157443046569824,13.285446166992188,13.371879577636719,13.443427085876465,13.312077522277832,13.325447082519531,13.201516151428223,13.412653923034668,13.387580871582031,13.376065254211426,13.23760986328125,13.391357421875,13.28543758392334,13.155158996582031,13.134252548217773,13.264272689819336,13.326519012451172,13.441741943359375,13.180994987487793,13.229814529418945,13.385966300964355,13.26877498626709,13.252023696899414,13.296429634094238],\"y\":[2.849513053894043,2.6924357414245605,2.519382953643799,2.685671091079712,3.4131829738616943,3.3496320247650146,2.9062132835388184,2.5787570476531982,2.8339319229125977,2.6660850048065186,3.363393783569336,3.0692508220672607,3.403278112411499,2.94502854347229,2.862663984298706,2.849548578262329,3.311357021331787,2.857738494873047,2.829335927963257,2.7103164196014404,2.6873369216918945,3.1780202388763428,2.415661573410034,2.984997510910034,2.9960906505584717,3.0859627723693848,3.064985752105713,3.3739285469055176,2.99613094329834,3.3052146434783936,2.759016990661621,2.554168224334717,2.7810420989990234,3.370600700378418,2.8825161457061768,2.5941309928894043,3.402747631072998,3.1956849098205566,2.574057102203369,2.4850573539733887,3.1060783863067627,2.757913827896118,2.8156826496124268,2.6702122688293457,3.1264901161193848,3.3880455493927,2.8700947761535645,2.6230366230010986,2.932117223739624],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"13_reasoning_pangu_symbolic\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"13_reasoning_pangu_symbolic\"],\"x\":[9.99891185760498,9.666812896728516,9.87675666809082,9.759414672851562,9.77737045288086,9.712926864624023,9.860082626342773,10.150372505187988,9.757026672363281,9.756099700927734,9.88352108001709,9.667094230651855,9.834576606750488,9.662117958068848,9.90071964263916,9.823662757873535,9.885151863098145,9.897980690002441,9.599701881408691,9.828813552856445,10.166223526000977,9.664942741394043,9.68136215209961,9.909587860107422,9.613624572753906,9.683416366577148,9.677172660827637,9.895627975463867,9.831110954284668,9.791001319885254,9.888591766357422,10.030932426452637,10.003292083740234,9.82230281829834],\"y\":[-1.2806227207183838,-1.4134907722473145,-1.3984967470169067,-0.8591680526733398,-1.1667323112487793,-0.9986860752105713,-1.226072072982788,-1.2499401569366455,-1.3567789793014526,-1.3018178939819336,-1.279323697090149,-1.3926376104354858,-0.8900179862976074,-1.3667707443237305,-1.3356115818023682,-1.4169893264770508,-1.3518718481063843,-1.17157781124115,-1.1077362298965454,-1.3785203695297241,-1.242591381072998,-1.3728218078613281,-1.36669921875,-1.1680376529693604,-1.3303998708724976,-0.8987433314323425,-1.368369460105896,-1.1938246488571167,-1.3907575607299805,-1.302925705909729,-1.1836531162261963,-1.1196755170822144,-1.154442310333252,-1.243509292602539],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"14_speech_asr_s2st\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"14_speech_asr_s2st\"],\"x\":[13.670157432556152,13.798901557922363,13.369604110717773,13.828919410705566,13.80864143371582,13.781868934631348,13.87149715423584,13.169575691223145,13.689194679260254,13.725122451782227,13.867948532104492,13.486695289611816,13.795697212219238,13.88539981842041,13.794876098632812,13.703969955444336,13.475425720214844,13.485220909118652,13.642592430114746,13.45029067993164,13.17920970916748,13.825347900390625,13.710762977600098,13.833584785461426,13.823281288146973,13.777862548828125,13.709671974182129,13.589035034179688,13.624615669250488,13.818294525146484,13.520774841308594,13.499052047729492,13.662908554077148],\"y\":[1.930212140083313,1.9589751958847046,1.9856090545654297,2.118469476699829,1.8433870077133179,2.083873748779297,1.9659463167190552,1.6238290071487427,1.9088102579116821,1.984571933746338,1.8491621017456055,1.8348524570465088,1.9095227718353271,2.1351234912872314,2.0434975624084473,1.9872238636016846,1.9833941459655762,1.972872018814087,1.8849225044250488,1.9644986391067505,1.6182628870010376,2.087661027908325,1.9592905044555664,1.9400484561920166,1.8442121744155884,2.052821636199951,2.1948325634002686,1.9715791940689087,1.9485830068588257,1.9467763900756836,2.033796548843384,2.05224347114563,1.9568393230438232],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"15_classification_oov_slash\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"15_classification_oov_slash\"],\"x\":[11.668291091918945,11.893047332763672,11.399813652038574,11.399984359741211,11.689655303955078,11.275130271911621,11.221017837524414,12.198081016540527,12.027531623840332,11.27303695678711,11.897500038146973,11.873612403869629,11.806347846984863,11.46186637878418,11.398143768310547,11.580697059631348,11.322710990905762,11.420403480529785,11.564152717590332,11.334028244018555,11.482306480407715,11.22659969329834,11.433297157287598,11.450419425964355,11.489837646484375,11.502764701843262,11.421748161315918,11.442119598388672,12.117900848388672,11.278861999511719,11.551697731018066],\"y\":[1.6213551759719849,1.7356750965118408,1.8189188241958618,2.011199474334717,1.6284370422363281,1.6911828517913818,1.434216856956482,1.9124113321304321,1.708951473236084,1.345273733139038,1.9241002798080444,1.706599235534668,1.5955106019973755,1.6075619459152222,1.8218833208084106,1.9240765571594238,1.5612646341323853,1.4347044229507446,1.8333743810653687,1.4203897714614868,1.508184552192688,1.6106964349746704,1.7237390279769897,1.6263391971588135,1.7565888166427612,1.8806627988815308,1.4971145391464233,1.4235330820083618,2.043459415435791,1.6557258367538452,1.682104468345642],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"16_event_events_eae\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"16_event_events_eae\"],\"x\":[11.366277694702148,11.415215492248535,11.413336753845215,11.3681058883667,11.386617660522461,11.464384078979492,11.333633422851562,11.434808731079102,11.411873817443848,11.339577674865723,11.400299072265625,11.394913673400879,11.41909122467041,11.410325050354004,11.436091423034668,11.440723419189453,11.421466827392578,11.424187660217285,10.752309799194336,11.30122184753418,11.429636001586914,11.37482738494873,11.405726432800293,11.432455062866211,11.494464874267578,11.39643669128418,11.412282943725586,11.394112586975098,11.40078353881836,11.381903648376465],\"y\":[-1.7047370672225952,-1.7189042568206787,-1.7727406024932861,-1.7914379835128784,-1.7604166269302368,-1.5030242204666138,-1.8196848630905151,-1.7163331508636475,-1.6494393348693848,-1.6843397617340088,-1.7724716663360596,-1.7692655324935913,-1.7378288507461548,-1.7441824674606323,-1.464735746383667,-1.7443996667861938,-1.7241114377975464,-1.6847602128982544,0.36447063088417053,-1.677884578704834,-1.6699968576431274,-1.7729989290237427,-1.7663508653640747,-1.7174915075302124,-1.603288173675537,-1.7217741012573242,-1.7533800601959229,-1.768631100654602,-1.7753701210021973,-1.6422590017318726],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"17_explanations_explanation_concepts\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"17_explanations_explanation_concepts\"],\"x\":[9.92600154876709,9.90449047088623,9.936745643615723,9.92866325378418,9.815776824951172,9.909724235534668,9.910490989685059,9.891134262084961,9.903509140014648,9.932821273803711,9.916892051696777,9.828373908996582,9.930082321166992,9.88005256652832,9.91939926147461,10.002853393554688,9.763079643249512,9.934072494506836,9.936637878417969,9.912662506103516,9.736724853515625,9.912676811218262,9.76375961303711,9.90211296081543,9.953424453735352,9.921619415283203,9.907418251037598,9.980151176452637,9.924909591674805,9.899526596069336],\"y\":[-2.003943920135498,-2.018934726715088,-2.0210397243499756,-1.9936052560806274,-1.9008338451385498,-1.989972710609436,-2.0100209712982178,-1.9968105554580688,-2.0147523880004883,-2.0181238651275635,-2.0108845233917236,-1.9405567646026611,-1.9686578512191772,-1.9617516994476318,-2.0073821544647217,-1.730658769607544,-1.9098670482635498,-1.9983524084091187,-2.0032131671905518,-2.021005392074585,-1.78288996219635,-1.9908539056777954,-1.9123069047927856,-1.9989206790924072,-1.8872182369232178,-2.0079305171966553,-1.9907784461975098,-1.9225029945373535,-2.0194551944732666,-1.9666630029678345],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"18_prompts_prompt_continuous\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"18_prompts_prompt_continuous\"],\"x\":[9.238378524780273,9.30147647857666,9.232583999633789,9.182598114013672,9.238131523132324,9.218341827392578,9.234349250793457,9.302995681762695,9.232303619384766,9.124175071716309,9.416687965393066,9.293940544128418,9.349409103393555,9.29613208770752,9.22963809967041,9.208431243896484,9.365690231323242,9.037209510803223,9.345200538635254,9.246965408325195,9.236005783081055,9.268945693969727,9.25452709197998],\"y\":[0.11233166605234146,0.2616926431655884,0.13499541580677032,0.13999022543430328,0.13154223561286926,0.03455710411071777,0.1343674212694168,0.09137420356273651,0.0880853608250618,-0.03193724900484085,0.282012939453125,0.19786401093006134,0.27887430787086487,0.23954655230045319,0.1252308189868927,0.2048257291316986,0.21515387296676636,0.2964838743209839,0.28831443190574646,0.10827220976352692,0.010295077227056026,0.17448420822620392,0.15992532670497894],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"19_zeroshot_fewshot_verbalizers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"19_zeroshot_fewshot_verbalizers\"],\"x\":[11.956093788146973,12.490596771240234,12.429107666015625,12.604439735412598,12.537409782409668,12.447066307067871,12.514530181884766,12.494388580322266,12.444210052490234,12.318142890930176,12.335335731506348,12.499151229858398,12.503713607788086,12.477373123168945,12.43051815032959,12.483855247497559,12.453995704650879,12.551338195800781,12.408796310424805,12.414511680603027,12.439727783203125],\"y\":[-0.01690675877034664,-0.3107600808143616,-0.2527309060096741,-0.2602718472480774,-0.33801159262657166,-0.32423314452171326,-0.19620446860790253,-0.286507248878479,-0.2868269979953766,2.4987032413482666,-0.19626036286354065,-0.2882573902606964,2.8162295818328857,-0.27515092492103577,-0.28718024492263794,-0.29625701904296875,-0.3112342059612274,-0.32359012961387634,-0.29335451126098633,-0.24639558792114258,0.026239966973662376],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"20_chatgpt_chatgpts_rewriting\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"20_chatgpt_chatgpts_rewriting\"],\"x\":[8.408228874206543,8.558862686157227,8.468222618103027,8.394969940185547,8.488740921020508,8.462080955505371,8.458389282226562,8.648653030395508,8.45824146270752,8.496983528137207,8.497143745422363,8.60595989227295,8.652069091796875,8.46937370300293,8.726569175720215,8.704874992370605,8.474954605102539,8.499213218688965,8.526307106018066],\"y\":[0.8488311767578125,0.8990485668182373,0.930065929889679,0.9668167233467102,0.8830121159553528,0.8673952221870422,0.9546473622322083,0.9203405976295471,0.9518395662307739,0.8990188241004944,0.8826806545257568,1.001754879951477,0.8333197236061096,0.94631427526474,0.7836666703224182,0.7688788771629333,0.916922390460968,0.7554416656494141,0.88944411277771],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"21_tables_table_tableqa\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"21_tables_table_tableqa\"],\"x\":[10.386235237121582,10.229713439941406,10.389017105102539,10.301673889160156,10.361222267150879,10.3976469039917,10.311823844909668,10.32546329498291,10.373322486877441,10.308487892150879,10.342009544372559,10.355380058288574,10.305009841918945,10.051471710205078,10.35975456237793,10.3198823928833],\"y\":[-0.7522066235542297,-0.7360655665397644,-0.7566789388656616,-0.7287562489509583,-0.7565518617630005,-0.6721810698509216,-0.7266584038734436,-0.7462540864944458,-0.6999889016151428,-0.7211228609085083,-0.7179433107376099,-0.7139708399772644,-0.7397772073745728,-0.6260511875152588,-0.7241697311401367,-0.7212250828742981],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"22_oosf_instances_proxy\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"22_oosf_instances_proxy\"],\"x\":[11.03643798828125,11.07364559173584,11.248010635375977,11.170310974121094,10.949195861816406,11.285311698913574,10.957159996032715,11.096034049987793,11.2238130569458,11.023275375366211,11.042583465576172,11.16788101196289,11.188385009765625,11.112465858459473],\"y\":[3.106004476547241,3.0234382152557373,3.3541853427886963,3.115736246109009,3.0922281742095947,3.283050775527954,3.0349786281585693,3.2528135776519775,3.086516857147217,3.087618112564087,3.101942300796509,3.0745949745178223,3.133884906768799,3.1343841552734375],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"23_figurative_metaphors_sociocultural\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"23_figurative_metaphors_sociocultural\"],\"x\":[12.890239715576172,12.9169282913208,12.898111343383789,12.95676326751709,12.863883018493652,12.8355073928833,12.85640811920166,13.136683464050293,12.889006614685059,7.990469932556152,12.911820411682129,12.735673904418945,12.899605751037598,12.521623611450195],\"y\":[0.026861172169446945,-0.11374158412218094,-0.0861361101269722,-0.08715442568063736,0.14325851202011108,-0.09389468282461166,-0.06494750827550888,-0.21590127050876617,-0.029119817540049553,0.18973292410373688,-0.05029120296239853,0.2098502218723297,-0.02140832133591175,-0.014837851747870445],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"24_sarcasm_irony_humor\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"24_sarcasm_irony_humor\"],\"x\":[7.443446159362793,7.453991889953613,7.434319496154785,7.457515239715576,7.445913314819336,7.432080268859863,7.492039680480957,7.444361686706543,7.45388650894165,7.470397472381592,7.456826210021973,7.457319259643555,7.452033042907715,7.453394889831543],\"y\":[3.3650591373443604,3.3583810329437256,3.3732411861419678,3.3514108657836914,3.3623621463775635,3.374535083770752,3.3646886348724365,3.3602709770202637,3.329127788543701,3.332568883895874,3.3524117469787598,3.345237970352173,3.361050605773926,3.3561806678771973],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"25_dense_retrieval_retrievers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"25_dense_retrieval_retrievers\"],\"x\":[11.089601516723633,11.02492904663086,10.883186340332031,10.981522560119629,10.981151580810547,10.95821762084961,10.9121675491333,10.982845306396484,11.157200813293457,10.896283149719238,10.92767333984375,10.96117877960205,10.979663848876953],\"y\":[2.4378020763397217,2.470427989959717,2.49906587600708,2.4681739807128906,2.4417030811309814,2.4632935523986816,2.5861077308654785,2.4596455097198486,2.333122968673706,2.6070048809051514,2.5345096588134766,2.460688352584839,2.480128526687622],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"26_argumentative_environmental_firms\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"26_argumentative_environmental_firms\"],\"x\":[9.245034217834473,9.240274429321289,9.255699157714844,9.306416511535645,9.330011367797852,9.5535249710083,9.301241874694824,9.279705047607422,9.539828300476074,9.543222427368164,9.282510757446289,9.352497100830078],\"y\":[2.6040866374969482,2.5882256031036377,2.6099462509155273,2.364428997039795,2.5036041736602783,2.5556437969207764,2.5966074466705322,2.450493335723877,2.5414693355560303,2.4918816089630127,2.483466386795044,2.526350498199463],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"27_proprietary_opensourced_recommendatio\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"27_proprietary_opensourced_recommendatio\"],\"x\":[8.700318336486816,8.662346839904785,8.710755348205566,8.685635566711426,10.063787460327148,8.825141906738281,9.746978759765625,8.632473945617676,8.7011137008667,9.762179374694824,10.958747863769531,9.222679138183594],\"y\":[1.2634673118591309,1.199538230895996,1.2940387725830078,1.2151010036468506,2.198474407196045,1.475010633468628,3.3699069023132324,1.1698120832443237,1.2515336275100708,3.3798534870147705,1.8569083213806152,1.78851318359375],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"28_modification_mp2_multiword\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"28_modification_mp2_multiword\"],\"x\":[12.416332244873047,12.561809539794922,12.500757217407227,12.375421524047852,12.55825138092041,12.481864929199219,12.49730396270752,12.1989107131958,12.471221923828125,12.578041076660156,12.469315528869629,12.464476585388184],\"y\":[1.9628962278366089,2.3608059883117676,2.222209930419922,2.1565310955047607,2.1810107231140137,2.1097028255462646,2.3813178539276123,2.188523292541504,2.159067153930664,2.3556671142578125,1.6200826168060303,2.1543469429016113],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"29_instructional_teaching_students\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"29_instructional_teaching_students\"],\"x\":[8.679919242858887,8.863896369934082,8.659948348999023,8.705171585083008,8.791703224182129,8.700974464416504,8.720808029174805,9.014729499816895,8.698103904724121,8.63776969909668,8.665627479553223,8.739877700805664],\"y\":[0.4001754820346832,0.5290360450744629,0.39178118109703064,0.5974481105804443,0.46730977296829224,0.44593337178230286,0.3917016386985779,0.42155221104621887,0.3866553008556366,0.5923276543617249,0.5228016972541809,0.4678838849067688],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"30_counterfactuals_scone_counterfactual\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"30_counterfactuals_scone_counterfactual\"],\"x\":[10.26634407043457,10.128600120544434,10.363204956054688,10.149825096130371,10.254651069641113,10.238754272460938,10.266803741455078,10.18940258026123,10.27680492401123,10.135074615478516,10.226946830749512],\"y\":[2.246767520904541,2.1721770763397217,2.3008975982666016,2.423623561859131,2.2409117221832275,2.265772581100464,2.3193318843841553,2.154175281524658,2.2092831134796143,2.1171963214874268,2.24501371383667],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"33_translation_languages_bias_gender_soc\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"33_translation_languages\"],\"x\":[13.87353229522705,13.208016395568848,14.106237411499023,13.092193603515625,13.336145401000977,13.386882781982422,12.319914817810059,13.021512985229492,13.98583698272705,12.727828025817871,14.158334732055664,12.96531867980957,13.214096069335938,13.292146682739258,13.683382034301758,13.187692642211914,13.279541969299316,12.989252090454102,12.971858978271484,12.976456642150879,13.035022735595703,12.499113082885742,13.1813325881958,12.979851722717285,13.296390533447266,13.039684295654297,13.842495918273926,13.132991790771484,13.9254732131958,12.167425155639648,14.098097801208496,13.920403480529785,12.558815002441406,12.849774360656738,12.970587730407715,14.02845287322998,12.911493301391602,13.765254974365234,13.905959129333496,13.326302528381348,13.942405700683594,12.925593376159668,13.41197395324707,13.25645637512207,12.991842269897461,13.592504501342773,13.934208869934082,13.955780029296875,13.298561096191406,13.263577461242676,12.345218658447266,13.947273254394531,13.009997367858887,13.643670082092285,13.180303573608398,13.74915599822998,12.808438301086426,13.863404273986816,12.991873741149902,13.162498474121094,12.859356880187988,13.075448036193848,13.156818389892578,12.886246681213379,13.997769355773926,13.03771686553955,12.784741401672363,12.894862174987793,12.97797966003418,13.60038948059082,14.051366806030273,12.930017471313477,12.712410926818848,12.782764434814453,12.85952377319336,12.852415084838867,13.387727737426758,13.939677238464355,12.471564292907715,13.912345886230469,12.641358375549316,13.367488861083984,13.8226318359375,13.64598560333252,13.757844924926758,12.249322891235352,13.732461929321289,13.127846717834473,12.968668937683105,13.813699722290039,13.807978630065918,13.008394241333008,13.787175178527832,13.993060111999512,13.531773567199707,13.984949111938477,13.01711654663086,14.002456665039062,13.822026252746582,13.829243659973145,13.329290390014648,12.908177375793457,14.121039390563965,13.951969146728516,12.813560485839844,13.843708038330078,13.839166641235352,14.121064186096191,12.558928489685059,13.817070960998535,13.30069637298584,13.859855651855469,13.09512710571289,12.266813278198242,12.803173065185547,13.087562561035156,13.733831405639648,13.884516716003418,12.79514217376709,13.156198501586914,12.635796546936035,13.271025657653809,12.98554801940918,13.871160507202148,13.031496047973633,14.019685745239258,13.770309448242188,13.913249969482422,13.995428085327148,12.395220756530762,13.254109382629395,12.519502639770508,13.016617774963379,13.239713668823242,13.700984954833984,13.32162857055664,13.05946159362793,12.602531433105469,12.626888275146484,13.298456192016602,13.010909080505371,12.473347663879395,13.30741024017334,12.52789306640625,13.24752140045166,14.047928810119629,12.949474334716797,13.951871871948242,14.054276466369629,12.840779304504395,12.930652618408203,13.249177932739258,14.00323486328125,12.957128524780273,13.71989631652832,12.736191749572754,13.225967407226562,9.189112663269043,9.039446830749512,9.115840911865234,9.096116065979004,9.138891220092773,8.879927635192871,8.893594741821289,8.761467933654785,8.84201431274414,8.92170524597168,8.984542846679688,8.870153427124023,9.148591041564941,9.1036376953125,8.759328842163086,9.173047065734863,9.000730514526367,9.311477661132812,9.226938247680664,8.916337966918945,9.266477584838867,8.85345458984375,8.894457817077637,9.150871276855469,8.838128089904785,8.816519737243652,9.046035766601562,8.901216506958008,9.159720420837402,8.813364028930664,9.301010131835938,8.885666847229004,9.035273551940918,9.151689529418945,9.173745155334473,9.153688430786133,9.068913459777832,9.077773094177246,8.93253231048584,9.23687744140625,8.845597267150879,8.906045913696289,9.104510307312012,8.794829368591309,8.974413871765137,8.559188842773438,9.103609085083008,8.886739730834961,8.847504615783691,8.852912902832031,8.7335786819458,8.971410751342773,8.835782051086426,8.914285659790039,8.319539070129395,8.872060775756836,8.969176292419434,8.707355499267578,8.74543285369873,8.927281379699707,8.319100379943848,8.32375717163086,8.666680335998535,8.335197448730469,8.70714282989502,9.042021751403809,8.712772369384766,8.291357040405273,8.325663566589355,8.38085651397705,8.75160026550293,8.50749397277832,8.352300643920898,8.444624900817871,8.73957347869873,8.366410255432129,8.543909072875977,8.721636772155762,8.295104026794434,8.693644523620605,8.322171211242676,8.898469924926758,8.4146146774292,8.773049354553223,8.323225975036621,9.074934959411621,8.57706356048584,8.7324857711792,8.81857967376709,9.022188186645508,8.334091186523438,8.382010459899902,8.761948585510254,8.723730087280273,9.060052871704102,8.774007797241211,8.736745834350586,8.403592109680176,11.581463813781738],\"y\":[1.3817375898361206,1.0612719058990479,1.1385481357574463,1.08525550365448,1.1626895666122437,1.2480696439743042,0.4848286211490631,1.2720167636871338,1.1438474655151367,1.3171401023864746,1.4076839685440063,1.264595627784729,1.0884864330291748,1.1517572402954102,1.355188012123108,1.7229746580123901,0.972616970539093,1.1486741304397583,0.8810750246047974,1.257293701171875,0.9572843313217163,0.8085752129554749,0.9309483766555786,0.7958930730819702,1.0975350141525269,0.6360377073287964,0.9854050874710083,1.798175573348999,1.3100166320800781,0.67192143201828,1.2066819667816162,1.2865713834762573,0.9056100845336914,0.6249911189079285,1.7463622093200684,1.2420947551727295,0.7832163572311401,1.1565204858779907,1.3271960020065308,1.3933436870574951,1.0287328958511353,0.7349646091461182,1.1217514276504517,1.414216160774231,1.152062177658081,1.4249526262283325,1.1767480373382568,1.2981083393096924,1.1263619661331177,1.0864992141723633,0.5083565711975098,1.0866432189941406,1.8492233753204346,1.125476598739624,0.944848895072937,1.1385208368301392,0.678071916103363,1.3165032863616943,0.6076777577400208,1.3482673168182373,1.6210135221481323,0.9811966419219971,1.2457525730133057,1.171410322189331,1.1819112300872803,0.8604665398597717,0.942077100276947,0.516913115978241,1.7408256530761719,1.4816997051239014,1.3886042833328247,1.7745593786239624,0.794830322265625,0.5049740076065063,0.8629714250564575,0.9947225451469421,1.3124425411224365,1.181308388710022,0.65378338098526,0.9567456841468811,0.8112806677818298,1.0578453540802002,1.270293116569519,1.1971663236618042,1.2050573825836182,0.49335476756095886,1.5063146352767944,1.3702335357666016,1.2522984743118286,1.2417391538619995,1.4349349737167358,1.29513418674469,1.3764736652374268,1.1924550533294678,1.3044599294662476,1.0554274320602417,0.9203598499298096,1.1263551712036133,1.4461302757263184,1.4113985300064087,1.1233012676239014,1.292132019996643,1.383273959159851,1.2921757698059082,1.4860016107559204,1.4392153024673462,1.4001719951629639,0.9837578535079956,0.6659143567085266,0.9185031056404114,1.1586668491363525,1.4281121492385864,1.1629738807678223,0.510953962802887,0.5897344946861267,0.8766728639602661,1.4713213443756104,1.1049691438674927,0.5188847184181213,1.4383823871612549,1.3952010869979858,1.3247922658920288,0.9874094128608704,1.13014554977417,0.8806256651878357,1.1321403980255127,1.1713860034942627,1.29535710811615,1.1741254329681396,0.570444643497467,1.2960554361343384,0.7852280735969543,0.9467426538467407,1.7510077953338623,1.3233586549758911,1.1152814626693726,1.0483297109603882,0.647891640663147,0.5596342086791992,1.3211227655410767,0.9677881002426147,0.6502654552459717,1.0543253421783447,0.6772043704986572,1.212782382965088,1.1973955631256104,1.1335924863815308,1.4092952013015747,1.3452917337417603,0.6293814778327942,1.8506160974502563,1.066854476928711,1.2767295837402344,0.7192075848579407,1.712160587310791,0.8005959391593933,1.1127575635910034,3.968130350112915,4.086702346801758,3.7755074501037598,3.991488218307495,3.975116014480591,4.029293060302734,4.046248435974121,4.013284683227539,4.176393032073975,4.156956672668457,4.055082321166992,3.9321210384368896,3.937089204788208,3.9232289791107178,3.8601222038269043,3.866779327392578,3.9942166805267334,3.850517511367798,3.827030897140503,4.08809232711792,3.6295218467712402,4.113508701324463,4.187417507171631,3.6063523292541504,3.9586288928985596,4.24702787399292,3.9166204929351807,4.1035475730896,3.9102394580841064,4.182351112365723,3.869297981262207,3.981309175491333,3.9789490699768066,3.9490838050842285,3.944530487060547,3.942408323287964,4.038426399230957,4.008914947509766,4.139644622802734,3.7226343154907227,4.209354877471924,4.045825958251953,3.9882144927978516,4.28642463684082,4.0150346755981445,3.615358352661133,4.036133289337158,4.028831958770752,3.9747371673583984,3.9180045127868652,3.7974116802215576,4.060174942016602,4.20427942276001,4.09943962097168,3.193037986755371,3.0153839588165283,3.2098007202148438,3.411353826522827,3.419900894165039,2.993455171585083,3.2046985626220703,3.197061538696289,3.5652174949645996,3.1496386528015137,3.4359893798828125,3.163926362991333,3.4493963718414307,3.234320640563965,3.2002310752868652,3.272998094558716,3.3818790912628174,3.3397650718688965,3.0758256912231445,3.1623947620391846,3.384615182876587,3.2408924102783203,3.358213186264038,3.416884660720825,3.167921781539917,3.4248197078704834,3.2836413383483887,3.2514102458953857,3.342782497406006,3.3918700218200684,3.194164752960205,3.047151803970337,3.55344820022583,3.2842931747436523,3.3557095527648926,3.1627235412597656,3.2219274044036865,3.0630640983581543,3.3806440830230713,3.417919874191284,3.0919387340545654,3.379578113555908,3.3994202613830566,3.3199284076690674,2.1015961170196533],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"34_dialogue_entity_visual_knowledge_grap\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"34_dialogue_entity\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"x\":[11.134943962097168,11.025127410888672,11.245323181152344,11.604127883911133,11.156891822814941,11.352185249328613,11.527443885803223,11.282083511352539,11.126087188720703,11.73695182800293,11.573534965515137,13.375391006469727,11.404452323913574,11.218412399291992,10.824934959411621,11.162917137145996,11.561422348022461,11.366466522216797,10.656908988952637,11.300393104553223,11.272549629211426,11.278218269348145,11.19459056854248,11.2599458694458,11.612763404846191,10.9222412109375,11.204724311828613,11.096510887145996,11.001474380493164,11.493638038635254,11.407891273498535,11.244793891906738,11.168529510498047,11.017486572265625,11.451024055480957,11.223573684692383,11.187163352966309,11.271790504455566,11.26801872253418,11.24870491027832,11.188407897949219,11.215272903442383,11.429038047790527,11.241147994995117,11.28900146484375,11.684981346130371,11.643854141235352,11.422016143798828,11.644282341003418,11.211750030517578,11.794293403625488,10.666879653930664,11.676538467407227,11.204644203186035,11.4844388961792,11.210556983947754,10.822954177856445,11.602618217468262,11.172928810119629,11.261439323425293,11.18604850769043,11.24404525756836,11.392868995666504,11.273892402648926,11.069701194763184,11.81900691986084,11.298412322998047,11.697004318237305,11.151240348815918,11.614058494567871,11.220532417297363,11.269240379333496,11.204486846923828,11.25660228729248,11.144911766052246,11.228436470031738,11.389638900756836,11.027185440063477,11.341411590576172,11.200804710388184,10.692567825317383,11.169511795043945,11.02439022064209,10.92267894744873,11.365792274475098,11.304564476013184,11.123757362365723,11.317601203918457,11.2233304977417,11.132582664489746,null,11.1937837600708,11.191131591796875,11.507041931152344,13.65318489074707,13.939993858337402,13.685359001159668,13.784351348876953,13.504968643188477,13.94735336303711,13.143325805664062,13.834990501403809,13.44897747039795,13.599161148071289,13.199520111083984,13.714217185974121,13.943765640258789,13.477546691894531,13.838189125061035,13.14396858215332,13.451980590820312,13.139006614685059,13.305334091186523,13.572847366333008,13.578728675842285,13.640463829040527,13.756519317626953,13.478137016296387,13.640913963317871,13.371881484985352,13.30630874633789,13.778650283813477,13.48436164855957,13.48245620727539,13.242944717407227,13.188318252563477,13.671472549438477,13.161858558654785,13.182262420654297,13.67568302154541,13.578418731689453,13.574835777282715,13.530089378356934,12.509833335876465,13.550978660583496,13.269086837768555,13.929557800292969,13.697040557861328,13.825508117675781,13.642824172973633,13.51134204864502,13.453692436218262,13.557821273803711,13.553349494934082,13.648940086364746,13.369372367858887,13.12069034576416,13.566463470458984,13.692540168762207,13.704095840454102,13.338266372680664,13.631470680236816,13.964845657348633,13.344286918640137,13.934170722961426,13.56275463104248,13.731254577636719,13.44790267944336,13.115975379943848,13.600167274475098,8.303739547729492,13.527002334594727,13.690975189208984,13.423891067504883,13.938860893249512,13.628800392150879,13.415291786193848,13.883581161499023,13.465723037719727,13.598390579223633,13.958017349243164,13.456777572631836,13.795967102050781,9.905937194824219,13.677034378051758,13.943849563598633,13.411235809326172,13.518133163452148,13.331656455993652,8.079171180725098,8.06274700164795,7.97455358505249,7.848491191864014,8.032004356384277,7.964422702789307,7.964263916015625,8.134442329406738,8.114529609680176,7.841091632843018,7.8066725730896,8.066001892089844,7.995242118835449,8.329507827758789,8.135189056396484,7.784605979919434,7.775929927825928,8.180281639099121,8.422222137451172,7.919281005859375,7.885015487670898,7.895800590515137,7.771629810333252,8.177929878234863,7.973200798034668,8.28538703918457,7.766735553741455,7.8133392333984375,7.980544567108154,7.834801197052002,7.792785167694092,8.400049209594727,8.152592658996582,7.950541019439697,7.951925754547119,8.21005630493164,7.920266628265381,7.854425430297852,8.000518798828125,7.90179443359375,8.401914596557617,8.250751495361328,7.81003999710083,8.432668685913086,7.853431224822998,8.134767532348633,8.218738555908203,7.8007283210754395,8.225289344787598,7.825068473815918,8.072797775268555,8.098258972167969,8.405604362487793,7.813677787780762,7.821072101593018,7.985950946807861,7.810969352722168,8.3153657913208,8.099970817565918,7.857753276824951,8.26423168182373,7.91890811920166,7.952602386474609,7.912245273590088,7.974617958068848,7.784420490264893,8.138467788696289,8.036944389343262],\"y\":[-0.2791852056980133,-0.56585294008255,-0.33805036544799805,-0.5237462520599365,0.26701799035072327,-0.765414834022522,-0.7155061364173889,-0.6682496070861816,-0.6063860654830933,-0.3886547386646271,-0.6360691785812378,-0.5970664024353027,-0.6077590584754944,-0.3316837251186371,0.027478214353322983,-1.0895836353302002,-0.5561929941177368,0.06399431079626083,0.6634865999221802,-0.7742803692817688,0.14976142346858978,-1.053679347038269,-0.8100492358207703,-0.6574156284332275,-0.6182628273963928,2.9431962966918945,-0.8820046186447144,-0.8308306336402893,-0.7561229467391968,0.06208992749452591,-1.2723103761672974,0.021186748519539833,-0.9865592122077942,0.42442458868026733,-0.5611394047737122,-0.19294996559619904,-0.6945754289627075,-0.3245929181575775,0.14573052525520325,-0.8709970712661743,-0.352821946144104,-0.20292386412620544,-0.3928586542606354,-0.45419180393218994,-0.6536911725997925,-0.43990081548690796,0.47350263595581055,0.13158506155014038,-0.7095873355865479,-1.041664481163025,-0.30340245366096497,0.33025580644607544,-0.5825144648551941,-0.9634666442871094,0.1646973192691803,-0.9077123999595642,0.321090430021286,-0.4256440997123718,0.016457444056868553,-0.3007483184337616,-0.9926905035972595,-0.40955081582069397,-0.7163980603218079,-0.7613950967788696,-0.5563022494316101,-0.30304890871047974,-0.6749861836433411,-0.5403029322624207,-0.3122972548007965,-0.610802948474884,-0.7764879465103149,-0.3963066339492798,-0.9999945163726807,-0.2833714783191681,-0.40003347396850586,-0.9637208580970764,-0.45351165533065796,-0.5818671584129333,-0.7205086350440979,-0.9992696046829224,0.6563940644264221,-1.185823917388916,-1.0215363502502441,-0.13112245500087738,-0.6537771224975586,-0.7294257283210754,-0.7630423307418823,-0.7293157577514648,-0.9494206309318542,-0.28155016899108887,null,-0.36618950963020325,-0.9938457608222961,0.16926676034927368,-0.4724136292934418,0.05542946234345436,-0.2140544056892395,-0.305911123752594,-0.019443433731794357,0.04754577577114105,-0.5232704877853394,-0.11347304284572601,-0.42826223373413086,-0.3631307780742645,-0.34572187066078186,-0.06021670997142792,0.02899486944079399,-0.5741272568702698,-0.17813809216022491,-0.3307811915874481,-0.5624246597290039,-0.33823102712631226,-0.49408844113349915,-0.2875242233276367,-0.5673142671585083,-0.5054513216018677,-0.19457708299160004,0.04647556692361832,0.15025267004966736,-0.4029132127761841,-0.09265140444040298,-0.06226486340165138,-0.49906080961227417,-0.42240944504737854,-0.37416812777519226,-0.4376997649669647,-0.5433786511421204,-0.48809999227523804,-0.3587982952594757,-0.1461024433374405,-0.4376213848590851,-0.5315554738044739,-0.5797322392463684,2.760768413543701,-0.5617931485176086,-0.37373608350753784,1.0235728025436401,-0.2975458800792694,-0.12453179061412811,-0.39394140243530273,0.08051753044128418,-0.5586355328559875,-0.5786116719245911,-0.5049486756324768,0.1441514790058136,-0.7706948518753052,-0.21628828346729279,-0.3426320552825928,-0.16151897609233856,-0.39145922660827637,-0.5925371646881104,-0.2956721782684326,0.027539370581507683,-0.30378881096839905,0.042292892932891846,-0.12347960472106934,-0.06413992494344711,-0.6200217604637146,-0.4954264461994171,-0.34516313672065735,3.190534830093384,-0.2043280303478241,-0.5706292986869812,-0.7155386209487915,0.12354090809822083,-0.219037726521492,-0.293334424495697,0.07163935899734497,-0.5650919675827026,-0.29002729058265686,0.07026058435440063,-0.5817172527313232,-0.10595829039812088,-1.2771755456924438,-0.2213153839111328,0.0876251682639122,-0.7389064431190491,-0.14196763932704926,-0.6306360363960266,0.011064082384109497,0.014209670014679432,-0.09134206920862198,-0.2708755433559418,-0.20944665372371674,-0.15542514622211456,-0.016267064958810806,0.33848637342453003,1.3138884241925552e-05,-0.3145374655723572,-0.37487006187438965,0.08202043920755386,-0.06734184175729752,-0.26890313625335693,-0.2482864111661911,-0.37177231907844543,-0.342668354511261,0.05140113830566406,-0.009848969988524914,-0.26512715220451355,-0.21851959824562073,-0.20532433688640594,-0.34504765272140503,-0.12112276256084442,-0.11948814243078232,-0.36536410450935364,-0.43950170278549194,-0.3522815406322479,-0.16667716205120087,-0.33151018619537354,-0.4151010513305664,-0.21129287779331207,0.18422266840934753,-0.13335680961608887,0.17345526814460754,0.07078302651643753,-0.06395117193460464,-0.33869442343711853,-0.20178943872451782,-0.20231139659881592,-0.02338528260588646,0.17116601765155792,-0.3580891191959381,-0.31286415457725525,-0.32009008526802063,-0.1331133246421814,0.18067573010921478,-0.2697615623474121,0.08013986051082611,-0.31005680561065674,0.028209263458848,0.10702250152826309,0.2667584717273712,-0.3471834659576416,-0.3380453586578369,-0.07149399816989899,-0.4023083448410034,-0.10830654948949814,-0.17893050611019135,-0.2754707932472229,0.22065694630146027,-0.1637326180934906,-0.12212429195642471,-0.17535138130187988,-0.2760526239871979,-0.01606772653758526,0.11322040110826492,0.021302759647369385],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"#CFD8DC\",\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"other\",\"showlegend\":false,\"x\":[12.260786056518555,10.01115894317627,11.924004554748535,10.16279411315918,12.00646686553955,10.273680686950684,6.795833110809326,10.738099098205566,12.415397644042969,12.344215393066406,8.005266189575195,11.672365188598633,9.678071022033691,11.613967895507812,10.691057205200195,10.729043006896973,12.570730209350586,12.728907585144043,11.094691276550293,12.467524528503418,10.7769136428833,11.365942001342773,11.85193157196045,10.574968338012695,13.300118446350098,8.84424877166748,9.322251319885254,11.329916000366211,9.31579303741455,10.838811874389648,11.459781646728516,11.970154762268066,12.745402336120605,10.581709861755371,9.499332427978516,10.223129272460938,9.936136245727539,11.424376487731934,7.634429931640625,12.565068244934082,12.609332084655762,11.734045028686523,12.76197338104248,8.449498176574707,10.840675354003906,11.946686744689941,8.837322235107422,9.319323539733887,12.509225845336914,10.419879913330078,6.7955498695373535,9.324790954589844,12.908655166625977,11.152015686035156,11.986949920654297,12.218483924865723,11.286534309387207,11.993667602539062,9.96347427368164,11.670063018798828,9.893813133239746,10.930827140808105,9.840456008911133,9.265555381774902,8.651935577392578,9.824747085571289,9.816277503967285,12.087455749511719,11.567899703979492,6.796971321105957,12.668866157531738,12.218778610229492,10.99422836303711,11.2903470993042,13.343911170959473,12.024370193481445,12.494938850402832,12.23508071899414,10.537565231323242,9.071687698364258,9.39167308807373,13.654808044433594,11.47091293334961,11.624275207519531,12.246249198913574,12.476778984069824,11.365124702453613,12.479470252990723,12.073331832885742,10.643670082092285,10.698538780212402,11.801766395568848,11.808372497558594,9.954280853271484,12.55838394165039,8.872209548950195,11.538613319396973,8.866655349731445,10.459000587463379,11.395777702331543,9.855978012084961,11.591615676879883,10.30197811126709,11.654211044311523,11.18188190460205,10.506263732910156,13.050704002380371,11.941532135009766,12.512845993041992,11.652451515197754,13.646256446838379,11.867741584777832,12.657735824584961,12.094124794006348,10.286686897277832,12.581634521484375,11.336400985717773,9.937158584594727,11.021777153015137,9.948188781738281,12.35839557647705,12.886040687561035,9.261913299560547,9.89763355255127,9.462775230407715,9.968618392944336,12.362512588500977,10.358255386352539,8.327774047851562,11.927535057067871,10.00265884399414,12.349808692932129,10.623787879943848,10.678670883178711,11.847443580627441,9.692587852478027,9.262121200561523,10.958932876586914,12.889772415161133,10.948387145996094,11.634035110473633,12.080227851867676,10.378564834594727,10.096247673034668,11.42621898651123,8.7933931350708,11.231094360351562,8.999340057373047,11.527088165283203,12.19135570526123,7.8714375495910645,11.001914978027344,12.61523151397705,9.75306224822998,11.896245002746582,12.523083686828613,9.378335952758789,10.35689926147461,11.08590316772461,13.147154808044434,9.818265914916992,11.11213493347168,11.444499969482422,11.590463638305664,10.737903594970703,9.770731925964355,10.78557014465332,11.933521270751953,10.511313438415527,9.82026481628418,8.356404304504395,10.73415470123291,10.45881462097168,10.396477699279785,12.194050788879395,12.752180099487305,11.78748607635498,10.255226135253906,12.165645599365234,12.292634963989258,12.844697952270508,10.628177642822266,8.569395065307617,11.763611793518066,12.15890884399414,12.740022659301758,12.746966361999512,9.62110424041748,11.45886516571045,10.926743507385254,12.772931098937988,12.837591171264648,12.200713157653809,9.013678550720215,10.967081069946289,9.76983642578125,12.884422302246094,11.670434951782227,13.589189529418945,12.576118469238281,11.409375190734863,9.541470527648926,8.66622257232666,11.438347816467285,11.34601879119873,10.590363502502441,11.35364818572998,11.930695533752441,6.797292709350586,10.794591903686523,10.309884071350098,10.247868537902832,9.914916038513184,13.303763389587402,12.540189743041992,11.354313850402832,8.383038520812988,12.193739891052246,9.227989196777344,9.241358757019043,9.536521911621094,10.984012603759766,12.313669204711914,9.644474029541016,9.546388626098633,9.345878601074219,11.460395812988281,10.699294090270996,9.629450798034668,8.48170280456543,11.0349760055542,10.570517539978027,10.367918014526367,11.483748435974121,10.360127449035645,10.180638313293457,12.789478302001953,10.665085792541504,11.708563804626465,12.97873592376709,11.337641716003418,10.709122657775879,13.167486190795898,12.409531593322754,8.32143497467041,9.063648223876953,10.371134757995605,11.557292938232422,10.586905479431152,9.22173023223877,11.61303424835205,8.834911346435547,13.149587631225586,11.683677673339844,9.396690368652344,11.555591583251953,11.163105964660645,10.546306610107422,13.552511215209961,12.85168170928955,11.200281143188477,9.939804077148438,10.729233741760254,12.406366348266602,11.353873252868652,11.169713973999023,9.460506439208984,10.654193878173828,11.77177906036377,11.467199325561523,12.088428497314453,13.498458862304688,11.279095649719238,12.567978858947754,10.763456344604492,10.033524513244629,12.673567771911621,10.10020637512207,8.970285415649414,9.142569541931152,11.128690719604492,12.259600639343262,8.503023147583008,11.229401588439941,9.610413551330566,10.481382369995117,12.30782699584961,11.052600860595703,13.069833755493164,9.056635856628418,9.835722923278809,11.540289878845215,10.57509994506836,10.630483627319336,12.53976058959961,12.878406524658203,12.195188522338867,12.416590690612793,8.807579040527344,12.123668670654297,10.993809700012207,11.357285499572754,11.609417915344238,8.505378723144531,9.010087966918945,9.596749305725098,12.824377059936523,10.293586730957031,11.473138809204102,12.474746704101562,10.027557373046875,12.210655212402344,10.91020393371582,13.3267240524292,13.175515174865723,11.291525840759277,11.628263473510742,11.242234230041504,9.939929008483887,12.68603515625,9.105117797851562,12.55679702758789,10.582772254943848,11.557234764099121,12.54797649383545,11.068909645080566,12.6818208694458,11.577909469604492,11.18771743774414,13.261770248413086,11.224922180175781,11.866551399230957,9.587552070617676,10.551297187805176,11.903278350830078,12.07540225982666,10.768095970153809,12.016518592834473,8.86601448059082,12.697779655456543,12.205658912658691,10.315733909606934,11.997912406921387,12.761860847473145,11.485032081604004,12.877477645874023,10.77319622039795,9.71633529663086,12.789429664611816,10.579119682312012,12.169694900512695,11.369839668273926,12.047872543334961,11.594593048095703,11.944458961486816,11.05200481414795,9.081618309020996,12.835399627685547,12.314804077148438,9.093341827392578,11.577603340148926,9.215981483459473,11.374435424804688,12.266894340515137,11.160942077636719,8.430356979370117,12.54307746887207,11.239245414733887,10.539368629455566,11.465906143188477,7.657243728637695,12.263589859008789,9.044021606445312,11.270294189453125,9.69029426574707,12.90316104888916,11.74852466583252,8.697430610656738,10.244534492492676,13.194223403930664,8.26854419708252,12.263144493103027,12.15650463104248,10.97741985321045,11.557283401489258,12.207938194274902,9.880388259887695,13.715070724487305,8.669084548950195,11.517677307128906,12.063190460205078,10.891190528869629,8.157964706420898,13.416830062866211,12.172284126281738,9.77486515045166,12.416096687316895,8.317665100097656,12.529857635498047,10.84004020690918,8.873795509338379,12.759370803833008,10.572245597839355,11.383156776428223,11.408194541931152,12.709611892700195,13.176079750061035,7.645970344543457,10.45201587677002,11.888107299804688,8.155491828918457,10.934144973754883,12.047857284545898,11.55852222442627,10.887688636779785,10.773085594177246,10.705704689025879,9.601393699645996,10.455825805664062,9.596738815307617,10.703749656677246,11.308526992797852,9.27015209197998,10.01099681854248,12.847784996032715,12.372241020202637,10.8928804397583,11.546853065490723,9.608847618103027,10.049355506896973,10.234009742736816,12.77387809753418,7.639250755310059,9.369508743286133,11.111124992370605,10.788805961608887,9.64129638671875,10.971961975097656,13.150918960571289,10.595973014831543,9.54521369934082,12.856410026550293,10.756369590759277,11.570320129394531,10.633820533752441,10.61214542388916,11.196186065673828,11.974361419677734,11.977964401245117,9.437406539916992,11.594237327575684,11.464212417602539,10.141727447509766,11.141068458557129,9.565552711486816,12.504912376403809,10.439518928527832,8.916755676269531,12.542708396911621,8.100579261779785,13.614336013793945,9.942835807800293,12.88668441772461,12.289319038391113,10.191901206970215,11.653940200805664,12.273872375488281,9.7015380859375,11.526108741760254,11.894497871398926,9.942673683166504,12.299449920654297,11.311140060424805,12.881195068359375,8.470643043518066,12.338445663452148,11.502923011779785,10.742378234863281,12.947842597961426,11.659990310668945,12.496929168701172,8.366921424865723,9.1043119430542,12.329166412353516,11.74715518951416,11.585480690002441,10.612318992614746,11.377769470214844,11.649738311767578,11.907842636108398,13.692667007446289,11.34955883026123,12.52751636505127,8.80586051940918,12.406229019165039,12.7645902633667,12.445311546325684,11.75776195526123,11.090094566345215,11.42638111114502,10.89482593536377,10.761418342590332,11.150059700012207,10.034558296203613,10.338700294494629,11.178735733032227,12.835885047912598,11.53531551361084,12.381514549255371,11.54314136505127,11.90941047668457,9.358724594116211,10.740930557250977,11.939253807067871,11.149989128112793,11.659534454345703,10.353327751159668,11.331498146057129,10.64175796508789,12.81151008605957,8.662858009338379,10.857192993164062,10.787764549255371,8.953280448913574,12.585387229919434,7.643540382385254,10.447787284851074,10.80845832824707,12.5751371383667,9.113858222961426,9.70373821258545,12.737845420837402,11.416006088256836,12.489952087402344,8.133481979370117,10.278565406799316,11.977149963378906,9.470913887023926,10.66490364074707,10.00718879699707,9.176525115966797,11.815064430236816,8.752781867980957,11.483132362365723,11.89669418334961,12.553421020507812,13.118978500366211,9.782742500305176,10.74406623840332,12.168991088867188,11.756896018981934,12.940293312072754,11.56460189819336,8.64371109008789,11.702585220336914,11.278419494628906,10.558026313781738,11.820279121398926,10.459695816040039,11.502100944519043,11.530735969543457,10.50390911102295,12.36990737915039,12.139140129089355,11.131754875183105,10.48403263092041,13.041622161865234,11.032854080200195,11.734776496887207,10.479324340820312,9.945302963256836,11.825505256652832,11.676788330078125,9.411858558654785,12.318131446838379,11.462925910949707,12.87320613861084,13.661906242370605,12.474398612976074,10.583564758300781,11.804007530212402,11.14046573638916,11.841476440429688,11.781903266906738,9.183506965637207,11.8534574508667,11.49117660522461,11.63294506072998,11.224637985229492,12.764777183532715,12.330574035644531,11.128299713134766,12.93774127960205,10.32089614868164,12.478330612182617,11.827890396118164,11.804841041564941,11.44624137878418,11.820039749145508,10.238183975219727,7.784416675567627,12.978934288024902,12.451269149780273,9.322071075439453,11.821974754333496,10.541068077087402,11.961499214172363,8.238553047180176,12.170052528381348,12.736010551452637,12.764181137084961,11.65296745300293,12.675060272216797,11.974373817443848,10.117522239685059,9.6543607711792,12.757356643676758,12.056167602539062,9.412654876708984,11.711962699890137,9.174324989318848,10.823249816894531,9.90158462524414,10.913339614868164,8.113393783569336,9.993379592895508,12.194329261779785,13.33237361907959,11.182921409606934,12.732467651367188,13.046422004699707,9.75806713104248,8.685029029846191,11.561057090759277,10.053182601928711,11.998501777648926,13.20248031616211,12.104700088500977,10.906420707702637,11.117636680603027,10.446064949035645,9.715156555175781,12.530933380126953,12.754565238952637,11.461576461791992,11.491990089416504,9.396021842956543,8.587562561035156,12.263270378112793,11.42301082611084,13.491022109985352,10.364217758178711,11.991475105285645,12.44273853302002,11.70559310913086,10.35807991027832,9.996321678161621,9.326005935668945,12.807846069335938,13.570623397827148,11.12037467956543,10.716169357299805,10.50700569152832,10.22071361541748,12.695380210876465,11.297102928161621,9.085335731506348,12.861201286315918,11.170536994934082,10.041293144226074,10.805912017822266,11.3994722366333,9.783839225769043,11.411025047302246,10.608845710754395,9.42233657836914,11.715644836425781,11.849699020385742,12.349161148071289,8.975476264953613,10.749741554260254,9.420133590698242,10.120325088500977,12.375459671020508,10.686391830444336,11.969694137573242,11.85922622680664,12.443221092224121,9.37954330444336,13.088716506958008,10.281312942504883,12.127240180969238,11.985560417175293,12.442952156066895,12.119172096252441,9.390953063964844,10.271842956542969,10.183282852172852,12.758428573608398,9.88254451751709,11.704732894897461,11.48423957824707,9.189234733581543,11.172453880310059,11.48064136505127,10.315546035766602,12.061239242553711,11.559906005859375,11.86010456085205,11.664885520935059,12.843629837036133,9.624467849731445,12.458562850952148,12.246566772460938,11.838500022888184,10.999716758728027,9.12990951538086,11.840607643127441,11.96013069152832,11.915284156799316,12.08629035949707,10.903203964233398,8.849540710449219,11.167780876159668,10.531004905700684,12.467206954956055,10.145415306091309,9.549249649047852,6.795407295227051,12.180326461791992,12.817878723144531,10.757889747619629,11.568016052246094,11.672552108764648,7.97341775894165,10.022518157958984,9.927380561828613,10.70942211151123,11.941184043884277,12.794625282287598,12.879822731018066,12.178878784179688,11.878340721130371,10.52446460723877,10.071585655212402,10.56197738647461,11.002744674682617,12.245721817016602,10.67922592163086,12.140796661376953,12.277287483215332,11.456153869628906,12.081787109375,11.62030029296875,10.880305290222168,10.69717025756836,9.24069881439209,12.846972465515137,10.142430305480957,12.146927833557129,9.958215713500977,9.50244140625,10.51760196685791,10.551128387451172,11.390852928161621,11.688502311706543,11.491974830627441,12.741220474243164,12.295635223388672,12.218851089477539,8.685344696044922,13.047622680664062,11.958925247192383,9.383042335510254,11.218199729919434,11.51757526397705,9.624102592468262,13.13804817199707,12.326669692993164,12.624595642089844,12.161945343017578,9.43790054321289,11.324953079223633,8.503750801086426,11.257233619689941,10.58021354675293,9.62783145904541,9.703179359436035,11.924786567687988,8.906972885131836,9.254066467285156,12.899577140808105,11.277064323425293,11.152223587036133,12.76430606842041,10.148778915405273,12.378602027893066,11.560495376586914,12.818056106567383,12.91047191619873,8.343107223510742,10.998586654663086,11.666386604309082,10.141661643981934,11.702606201171875,10.432024955749512,10.108580589294434,9.184564590454102,13.428140640258789,11.512848854064941,12.044175148010254,11.576578140258789,12.28610610961914,11.385955810546875,12.952836036682129,12.021284103393555,11.287923812866211,12.099640846252441,12.082901000976562,12.028682708740234,12.383368492126465,8.773681640625,12.879551887512207,11.319714546203613,8.952622413635254,11.267468452453613,12.477086067199707,12.503398895263672,9.42236614227295,11.20785140991211,11.332497596740723,8.951685905456543,10.856745719909668,10.309080123901367,11.73598861694336,8.723488807678223,12.038217544555664,9.878068923950195,12.995759963989258,10.974395751953125,9.64431381225586,10.158196449279785,13.631218910217285,8.542502403259277,11.165329933166504,10.789525032043457,13.625770568847656,11.960083961486816,12.425254821777344,11.940552711486816,13.470690727233887,13.038926124572754,7.834033012390137,9.87307071685791,11.216033935546875,12.414644241333008,12.093451499938965,11.877202987670898,12.003032684326172,10.026086807250977,12.491192817687988,13.477025032043457,12.77425479888916,10.46001148223877,8.880654335021973,9.948962211608887,10.48256778717041,12.090274810791016,11.583839416503906,11.40567684173584,6.7874345779418945,9.088058471679688,11.758285522460938,11.31846809387207,11.73054313659668,12.907270431518555,12.770581245422363,11.114065170288086,11.227639198303223,10.648099899291992,12.066269874572754,13.3859224319458,12.3936767578125,11.536118507385254,12.09136962890625,7.971031665802002,10.897802352905273,9.173691749572754,11.609256744384766,12.269068717956543,9.517958641052246,11.856656074523926,11.710532188415527,12.882142066955566,12.293147087097168,9.226627349853516,11.812376022338867,9.573324203491211,11.98694896697998,11.025315284729004,12.620606422424316,12.66535758972168,11.626893997192383,11.87524127960205,9.740761756896973,11.66956615447998,9.677865982055664,13.708874702453613,13.700621604919434,10.8056058883667,12.719548225402832,11.30461597442627,9.15710163116455,11.5343599319458,9.219347953796387,11.886124610900879,11.679952621459961,12.069474220275879,12.361296653747559,10.542925834655762,10.790267944335938,12.395968437194824,10.26074504852295,12.145330429077148,11.74125862121582,12.40096664428711,10.930389404296875,8.466429710388184,11.920580863952637,12.118764877319336,13.445467948913574,10.998306274414062,11.986994743347168,12.903426170349121,11.159873962402344,12.18824291229248,9.372452735900879,12.712847709655762,13.078712463378906,10.631550788879395,9.169002532958984,8.790238380432129,11.811575889587402,12.28490924835205,9.769627571105957,12.567846298217773,10.349342346191406,11.191412925720215,11.537104606628418,10.127388000488281,10.513239860534668,8.915346145629883,11.823579788208008,11.909771919250488,11.520251274108887,13.776591300964355,12.594861030578613,6.803589344024658,10.791962623596191,11.956903457641602,7.651430606842041,12.864334106445312,11.632024765014648,10.91291332244873,11.907623291015625,10.763083457946777,11.546504974365234,13.262833595275879,12.147619247436523,11.872856140136719,12.41122055053711,12.41154670715332,13.054023742675781,10.615890502929688,9.164365768432617],\"y\":[1.976219654083252,1.265775442123413,1.0926563739776611,-0.5182633996009827,2.4570963382720947,0.48775091767311096,-0.910475492477417,-0.5370233654975891,1.2164555788040161,1.3136889934539795,-0.12520691752433777,1.813036561012268,1.1669845581054688,1.774245023727417,-0.3897693157196045,1.3097110986709595,3.413337469100952,1.671027660369873,0.8366751074790955,3.2766458988189697,0.3775566816329956,0.44715550541877747,2.3352620601654053,3.230710506439209,2.2592904567718506,0.6792904138565063,0.25117701292037964,0.6122933626174927,1.182349681854248,0.9189866781234741,3.0709445476531982,0.9211485385894775,3.500269889831543,3.7989087104797363,1.823943853378296,1.6083993911743164,-0.23820368945598602,2.4324207305908203,0.19927099347114563,3.7869391441345215,2.054600715637207,0.9205240607261658,3.078073263168335,0.10503587871789932,0.8371968865394592,0.3198699951171875,-0.16870951652526855,2.7976632118225098,2.6678123474121094,-0.570141077041626,-0.9106079339981079,3.7905776500701904,2.7144033908843994,0.9463844299316406,1.3737200498580933,0.33007562160491943,-0.9850686192512512,1.1608271598815918,1.0532095432281494,2.060023546218872,1.5419471263885498,1.308414101600647,-1.1935114860534668,-0.17752167582511902,-0.4857664108276367,3.832371234893799,1.3047598600387573,3.552154541015625,1.942522644996643,-0.909134566783905,2.3067994117736816,0.9871791005134583,2.9792444705963135,2.830186128616333,0.7007690072059631,3.5236191749572754,3.808678388595581,0.5856729745864868,0.5755625367164612,-0.488295316696167,0.6351603269577026,0.5080202221870422,1.0401160717010498,2.3617639541625977,1.8510946035385132,0.9317828416824341,3.2759201526641846,2.4336321353912354,-1.299113392829895,3.8617777824401855,3.202488422393799,1.037284016609192,-0.5317288637161255,-0.5715892314910889,1.2100167274475098,1.357236623764038,3.533149480819702,3.0465500354766846,0.1627165526151657,0.7375492453575134,0.9511770009994507,0.07118048518896103,-0.3919447958469391,1.378462314605713,-0.3244394361972809,0.7082329988479614,2.143453359603882,1.1407673358917236,0.9031526446342468,2.8212311267852783,1.1145505905151367,1.4429399967193604,2.296320676803589,-0.001279126270674169,-0.10712704062461853,3.0526630878448486,0.7859264612197876,-0.6041418313980103,1.40556800365448,-0.5446928143501282,1.2216320037841797,3.4056971073150635,3.86862850189209,1.1494468450546265,0.1541697382926941,-0.2646180987358093,2.934096097946167,1.085005283355713,0.1298372447490692,1.4595853090286255,0.5229455232620239,1.7139097452163696,3.179950475692749,0.8122704029083252,2.313836097717285,0.9151896834373474,0.0694771260023117,2.251661777496338,2.7393033504486084,1.8044404983520508,2.4212090969085693,1.369358777999878,1.4411309957504272,1.778269648551941,1.9663785696029663,-0.2085247039794922,1.8884222507476807,0.01605393923819065,0.9991214275360107,2.1229145526885986,-0.17486093938350677,1.2444441318511963,1.5270284414291382,1.034622311592102,0.5874497890472412,1.0864499807357788,0.7937063574790955,0.2653231918811798,0.6648417711257935,2.1787331104278564,-1.4033302068710327,0.993794858455658,3.9591336250305176,2.2620389461517334,2.2779126167297363,1.026957631111145,1.6402356624603271,1.106472134590149,3.609027624130249,4.001310348510742,2.4320061206817627,1.3439748287200928,1.444767951965332,0.8551923036575317,0.15037234127521515,2.425750494003296,2.3774797916412354,2.089541435241699,0.14875546097755432,1.2632057666778564,1.3453816175460815,3.9026010036468506,0.47671282291412354,1.526449203491211,1.530004620552063,0.44732722640037537,1.5337772369384766,-0.5403087735176086,-1.0967426300048828,0.833954393863678,0.7645677924156189,2.70750093460083,1.2675151824951172,0.1613716185092926,3.286445379257202,0.9860133528709412,3.4668221473693848,1.4706974029541016,-0.06515169888734818,3.346012830734253,3.19698166847229,1.0387732982635498,-0.47503626346588135,1.1350661516189575,-0.12550556659698486,0.48462581634521484,3.611987352371216,0.44945693016052246,-0.9105373620986938,0.9302946925163269,2.0090830326080322,0.1268618106842041,-0.5888694524765015,1.5191290378570557,2.7231905460357666,3.63657283782959,0.20960384607315063,0.04279949516057968,2.0899455547332764,2.395681142807007,-0.3550689220428467,1.1873329877853394,1.6772342920303345,-0.551196813583374,1.0045689344406128,3.5376405715942383,-0.2728564739227295,1.6083232164382935,3.451906681060791,0.4097013771533966,0.28619876503944397,2.9995458126068115,1.430558204650879,2.9486751556396484,1.4180095195770264,3.394707679748535,-1.0172475576400757,1.6186503171920776,2.2325010299682617,3.3960022926330566,2.3339428901672363,2.508234739303589,1.917661428451538,2.007206678390503,0.7356185913085938,1.3998340368270874,1.426846981048584,-0.10650590807199478,3.196619987487793,1.5687898397445679,3.3810524940490723,-0.20458725094795227,1.947638988494873,-0.17957328259944916,0.6638663411140442,1.4990065097808838,1.0839821100234985,-0.13390646874904633,1.798355221748352,1.460438847541809,2.6365201473236084,2.2331058979034424,0.5038972496986389,1.672692894935608,3.618595838546753,2.4914488792419434,-0.7637554407119751,2.602285861968994,1.6152315139770508,2.3165502548217773,4.036905288696289,1.7626341581344604,-0.30745241045951843,1.3061516284942627,3.0544943809509277,2.2432503700256348,0.5260573625564575,0.26869526505470276,0.03409036993980408,-0.5223769545555115,0.39339277148246765,2.0766706466674805,-0.38802358508110046,3.801676034927368,2.3770365715026855,3.5236270427703857,3.0124449729919434,3.137105703353882,3.03592848777771,1.0055789947509766,2.3000667095184326,3.485448122024536,2.8649325370788574,3.2783849239349365,2.067582130432129,1.7887386083602905,1.117841124534607,1.9235390424728394,-0.5475263595581055,2.5567400455474854,-0.42588913440704346,1.6459540128707886,3.0523805618286133,3.548125743865967,0.9675278663635254,1.0511583089828491,1.8744837045669556,-0.2662609815597534,2.020230531692505,2.395968198776245,1.5245238542556763,2.208904504776001,-0.23743867874145508,2.1941235065460205,3.066039800643921,0.8780515789985657,-0.3361736238002777,0.12655974924564362,0.5816671252250671,2.5916388034820557,1.8386311531066895,0.9971460103988647,-0.09463711082935333,1.4201574325561523,3.414994478225708,-0.5812013745307922,1.0859824419021606,3.207399845123291,1.0073139667510986,1.059942603111267,0.9441660642623901,2.421081066131592,-0.9699038863182068,-0.7918293476104736,3.2947211265563965,3.453516721725464,2.6633572578430176,1.12492835521698,0.9515612125396729,1.8676871061325073,2.2264864444732666,0.4588735103607178,-0.7277568578720093,-1.0359772443771362,2.857710599899292,2.6983368396759033,1.5439820289611816,0.8810650110244751,3.6186933517456055,3.788325071334839,2.0987229347229004,3.1248533725738525,-0.6990334391593933,3.2795536518096924,0.4755188822746277,2.3451409339904785,1.9918140172958374,2.239830255508423,2.980741500854492,3.4494681358337402,2.7023935317993164,0.8716931343078613,1.143310546875,0.12861472368240356,1.2377668619155884,-0.3922814130783081,3.3432180881500244,1.7890307903289795,0.9444081783294678,2.8529300689697266,0.18657220900058746,2.6263394355773926,0.48836591839790344,0.770980954170227,0.9084759950637817,2.9118480682373047,0.899668276309967,0.3940073549747467,0.8528211116790771,1.6868844032287598,0.24871914088726044,2.4808976650238037,2.190305709838867,1.4015684127807617,-0.11070117354393005,1.4967073202133179,0.9283192753791809,0.4603424668312073,1.1601427793502808,1.9638222455978394,-1.3094513416290283,1.8755643367767334,-0.034235879778862,2.5533201694488525,2.5379700660705566,3.77651309967041,2.733030319213867,0.3953371047973633,2.7038984298706055,1.0902845859527588,3.95662522315979,3.4667394161224365,1.4680428504943848,-0.2102063000202179,1.9835692644119263,0.25430288910865784,2.2354414463043213,0.19437259435653687,1.457194447517395,1.478986144065857,0.024254370480775833,0.9681276082992554,-1.2682996988296509,1.4637945890426636,0.9524343013763428,3.2517035007476807,1.0663663148880005,1.047045350074768,2.5798704624176025,1.9295272827148438,-0.4900413751602173,1.2294294834136963,3.744098663330078,1.1958093643188477,3.4060301780700684,1.4086087942123413,3.195308208465576,-0.21053600311279297,1.9594486951828003,1.3294121026992798,0.7901973128318787,-1.0279371738433838,0.18727345764636993,2.7890584468841553,1.1563249826431274,-0.1735643744468689,2.2202253341674805,0.9077820777893066,2.668337821960449,2.603579521179199,0.24776341021060944,1.158795714378357,1.6285470724105835,1.96036958694458,3.842803716659546,-0.05351594090461731,1.0603022575378418,3.5160486698150635,3.203173875808716,-0.22633570432662964,1.7337943315505981,1.1724951267242432,0.17315110564231873,0.7105343341827393,2.4431610107421875,3.8104262351989746,0.10712434351444244,0.713238537311554,3.1322901248931885,3.2645468711853027,0.534716784954071,0.5656581521034241,3.5241639614105225,2.5362460613250732,-0.7327529191970825,-0.8334300518035889,2.0500335693359375,0.9878270626068115,2.0549156665802,3.280917167663574,0.543793261051178,0.40602004528045654,1.2652292251586914,1.6847025156021118,0.6613606810569763,0.48743921518325806,1.5848220586776733,0.15714702010154724,3.097687244415283,3.4039037227630615,3.8253872394561768,-0.3216516077518463,0.36484676599502563,3.5613582134246826,0.849107563495636,2.804208278656006,3.8222780227661133,1.8004229068756104,1.4634736776351929,3.1937408447265625,0.8675617575645447,3.476331949234009,0.3727441728115082,3.1206748485565186,1.5812299251556396,-0.6251404285430908,3.3036246299743652,3.7486493587493896,1.7302109003067017,2.3743913173675537,0.9696566462516785,0.4081820845603943,0.6557876467704773,2.6788806915283203,3.448852300643921,1.163936972618103,3.5292248725891113,2.8257150650024414,3.515986680984497,1.9399545192718506,0.4077995717525482,0.766963541507721,-0.4135100543498993,0.42341694235801697,2.8088972568511963,-0.6076659560203552,0.1522863209247589,3.2291979789733887,1.588698387145996,1.8325613737106323,0.9248793125152588,1.3020033836364746,3.420180559158325,1.643860101699829,1.1540601253509521,0.21492479741573334,0.1739095002412796,1.5416576862335205,3.289651393890381,-0.5035665035247803,-0.46198034286499023,3.0262765884399414,3.655320405960083,1.8784151077270508,0.3909582197666168,0.8705004453659058,1.1364428997039795,1.208755612373352,0.26025456190109253,0.44635772705078125,0.12183551490306854,-0.15554407238960266,-0.25469839572906494,-0.45932525396347046,0.3801633417606354,2.1852316856384277,3.0103812217712402,0.16040968894958496,0.17387855052947998,2.1033482551574707,3.5950064659118652,1.6760215759277344,3.2061989307403564,0.3962271809577942,2.2076709270477295,-0.8994259834289551,2.514460802078247,0.979424774646759,3.5055930614471436,0.26043587923049927,0.08888838440179825,0.9722239375114441,1.9090890884399414,1.4592763185501099,0.7288630604743958,0.5196076035499573,2.52851939201355,0.3626737594604492,0.9660953879356384,1.4570801258087158,1.5067415237426758,1.0723236799240112,1.9673024415969849,0.5451234579086304,2.6414546966552734,0.7439343333244324,3.4168541431427,1.108274221420288,0.9759180545806885,3.0499231815338135,0.3342808485031128,1.1762615442276,1.2191663980484009,1.9902498722076416,2.7178072929382324,2.1931447982788086,1.4139740467071533,3.4444408416748047,-0.2844769358634949,2.468306541442871,2.982534408569336,1.1627116203308105,2.345379114151001,-0.26387879252433777,2.15932035446167,-0.09391316771507263,2.315988540649414,3.9019370079040527,1.4993696212768555,0.8209245204925537,0.1551581770181656,2.0490360260009766,1.3861424922943115,1.0227948427200317,-0.12519274652004242,3.113816976547241,1.2089561223983765,-0.26088747382164,1.8862637281417847,3.144066333770752,-1.0393073558807373,2.0746145248413086,1.443308711051941,1.180871844291687,-0.6809756755828857,1.017892599105835,1.8907074928283691,-1.2943123579025269,0.1569855809211731,0.6024736166000366,0.8391635417938232,2.21470046043396,-1.1607123613357544,1.8066606521606445,2.297105312347412,2.370729446411133,1.7525073289871216,0.9918195605278015,0.6040194034576416,2.8629562854766846,3.4195144176483154,1.8813725709915161,3.7637126445770264,0.20570334792137146,2.110116958618164,1.3198965787887573,1.0308681726455688,1.5510575771331787,1.0269742012023926,3.243104934692383,0.5696408748626709,-1.0560033321380615,3.80950927734375,-0.9885093569755554,2.1285574436187744,0.27239152789115906,-0.9925388693809509,2.664095401763916,0.5445898771286011,3.4431591033935547,2.9532716274261475,1.4311859607696533,2.9324100017547607,3.054269313812256,0.8955436944961548,0.025119181722402573,3.7716782093048096,0.6967297196388245,2.6491522789001465,2.385422945022583,3.098644495010376,3.803661823272705,1.4175082445144653,2.0697543621063232,3.0386083126068115,3.8905580043792725,1.805167555809021,3.370250940322876,0.6452329754829407,1.198987364768982,4.435891628265381,3.9789042472839355,0.2224683314561844,0.784505307674408,3.900918483734131,-0.08793401718139648,-0.20969590544700623,0.9383320212364197,2.9824304580688477,0.4146740138530731,3.451068878173828,0.58146733045578,2.629016876220703,2.5896897315979004,0.8220140933990479,2.19604229927063,2.3683815002441406,1.0513523817062378,-0.5379165410995483,2.0229125022888184,2.577913522720337,1.3024059534072876,2.953939914703369,1.1526473760604858,-1.245646595954895,0.6586165428161621,1.9431781768798828,0.795759916305542,0.4541209042072296,1.3323132991790771,0.9036451578140259,1.0493559837341309,2.936338424682617,1.658136010169983,2.7779719829559326,0.217825248837471,-0.08116167783737183,0.8917245864868164,1.92644202709198,1.5383085012435913,3.03080153465271,-0.5870935916900635,1.1749986410140991,1.7433806657791138,3.2300631999969482,1.3536816835403442,1.4494167566299438,2.132977247238159,1.142106533050537,1.0964027643203735,-1.2843916416168213,2.0309720039367676,0.38181358575820923,0.4180848002433777,1.397927165031433,1.0205312967300415,-1.0413587093353271,-0.3866738975048065,-0.9114582538604736,2.2314321994781494,-0.9689415693283081,0.8953730463981628,-0.7223781943321228,3.3937878608703613,-0.1200912594795227,1.4780992269515991,-1.0540677309036255,1.243822693824768,1.1921714544296265,-0.9881787300109863,2.7142724990844727,1.035949468612671,1.7600966691970825,3.096761703491211,1.0765949487686157,-0.3055479824542999,0.8458605408668518,1.8884222507476807,0.7532628178596497,3.692111015319824,2.047429084777832,1.5398664474487305,2.6263246536254883,3.3768866062164307,1.3354458808898926,2.8829421997070312,3.073280096054077,2.618222951889038,2.2130420207977295,1.131805658340454,1.4583265781402588,-0.6291894912719727,3.3120579719543457,3.235522985458374,2.4912800788879395,0.8031163215637207,2.785531520843506,1.2172937393188477,2.5934207439422607,0.44533485174179077,3.7607927322387695,0.8943837881088257,2.2763075828552246,0.15780554711818695,2.040842056274414,0.27494242787361145,3.4471356868743896,2.545928955078125,3.3168694972991943,3.057359218597412,2.0106849670410156,3.7272539138793945,-0.2229020744562149,-0.3912656009197235,2.5650482177734375,0.5276594758033752,1.9719535112380981,3.524745225906372,1.5316689014434814,0.3282936215400696,2.1106743812561035,2.4451394081115723,1.7900829315185547,2.8499183654785156,2.917431354522705,-0.6730669736862183,2.2880237102508545,2.3319878578186035,0.7816553711891174,3.079967975616455,0.3783358633518219,3.771744728088379,3.4605367183685303,3.4098503589630127,2.232668399810791,2.7577970027923584,0.8026202917098999,0.8486886024475098,1.336401104927063,1.9623465538024902,2.0950241088867188,2.3696439266204834,2.694972276687622,0.7788678407669067,1.5454350709915161,0.642559289932251,1.9104548692703247,2.817577838897705,1.2944279909133911,2.2089552879333496,2.9942898750305176,0.6504705548286438,2.6843504905700684,1.2824846506118774,-0.555378794670105,1.9917773008346558,2.4853670597076416,2.2720730304718018,0.05783434212207794,1.741772174835205,3.9805495738983154,1.6283323764801025,3.9042468070983887,-0.08662469685077667,1.0114973783493042,-0.2651676535606384,2.276035785675049,-0.6100764274597168,2.0067296028137207,1.3208794593811035,1.0322285890579224,1.9896197319030762,2.3109211921691895,2.2940375804901123,0.7046573758125305,2.1666882038116455,0.8269322514533997,1.038419246673584,3.1033222675323486,2.011228084564209,0.4995725154876709,3.0119717121124268,1.7737351655960083,1.1829763650894165,3.768042802810669,2.453611135482788,-1.2905018329620361,0.4045698344707489,-0.7276555299758911,1.108733057975769,0.7043386101722717,1.7317860126495361,3.1328790187835693,1.5019123554229736,0.00757896201685071,1.7896612882614136,4.704597473144531,-1.2845878601074219,1.471731185913086,3.7090561389923096,-0.9188922643661499,0.25417712330818176,0.40908655524253845,-0.6070359349250793,3.279542922973633,2.3822507858276367,-1.0290439128875732,-1.0308316946029663,1.6611809730529785,0.5246748328208923,1.5900543928146362,1.5552963018417358,1.9574549198150635,0.3487606346607208,1.906638503074646,-0.15992236137390137,-0.1188686341047287,1.874509334564209,2.240257501602173,3.598365068435669,2.4335744380950928,3.23817777633667,1.554663896560669,2.292228937149048,1.6409223079681396,1.495506763458252,3.2188000679016113,3.7398290634155273,2.8911819458007812,3.769780397415161,2.451735019683838,2.5036814212799072,3.150447130203247,1.2291960716247559,0.9691900610923767,3.689431667327881,-0.07314413040876389,0.73487389087677,0.839801013469696,1.4524890184402466,2.1931369304656982,1.3259851932525635,3.0612916946411133,0.2924751043319702,0.6916547417640686,2.3885679244995117,0.7535249590873718,1.8109625577926636,3.2876040935516357,2.6223843097686768,2.415797472000122,2.450838565826416,1.9013489484786987,2.8400003910064697,3.637033224105835,1.2191064357757568,0.49888309836387634,0.527450680732727,3.3894202709198,2.8021371364593506,1.5861856937408447,-0.5647964477539062,1.773905634880066,2.8972055912017822,0.4014616310596466,3.552330493927002,3.5676872730255127,0.44267264008522034,2.05513334274292,2.6361255645751953,2.9887442588806152,0.289135605096817,3.268169641494751,3.1244053840637207,0.8573437333106995,-0.19672121107578278,-0.7705172300338745,1.6629849672317505,3.544947862625122,-0.6638178825378418,3.600919008255005,-0.5163041353225708,-0.025210803374648094,1.4350130558013916,1.8289581537246704,0.6889894008636475,1.6060850620269775,-0.9072462916374207,2.4202475547790527,1.693398356437683,0.15449632704257965,3.578507661819458,2.314469575881958,4.283806324005127,2.3266613483428955,0.48093342781066895,3.2087161540985107,-0.08493365347385406,1.7356622219085693,1.580642580986023,1.29006028175354,0.9935421347618103,2.0631296634674072,2.585738182067871,1.8831286430358887],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"6_adversarial_attacks_attack\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"6_adversarial_attacks_attack\"],\"x\":[10.677026748657227,10.417326927185059,10.19378662109375,10.34382438659668,10.359889030456543,10.168207168579102,10.379969596862793,10.382912635803223,10.3951997756958,10.006451606750488,10.593812942504883,10.523648262023926,10.564718246459961,10.390353202819824,10.409890174865723,10.375975608825684,10.270461082458496,10.380226135253906,10.342330932617188,10.389986038208008,10.570111274719238,10.563359260559082,10.393143653869629,10.445023536682129,10.406803131103516,10.406514167785645,9.976093292236328,10.553617477416992,10.514777183532715,10.404207229614258,10.352548599243164,10.402541160583496,10.373939514160156,10.66649055480957,10.147748947143555,10.177634239196777,10.380538940429688,10.481019020080566,10.569191932678223,10.38936710357666,10.453319549560547,10.417245864868164,10.462665557861328,10.532641410827637,10.139423370361328,10.995478630065918,10.441753387451172,10.408876419067383,10.595641136169434,10.508939743041992,10.599928855895996,10.383362770080566,10.315134048461914,10.520706176757812,10.196410179138184,10.421682357788086,10.112748146057129,10.42435073852539,9.957427978515625,10.400481224060059],\"y\":[4.530323028564453,4.6652750968933105,4.259049415588379,4.17231559753418,4.669703960418701,4.087711334228516,4.67042350769043,4.63741397857666,4.61146879196167,4.00691556930542,4.465812683105469,4.730312824249268,3.997835874557495,4.689051151275635,4.215987682342529,4.713289260864258,3.9740796089172363,4.662024021148682,4.6290106773376465,4.683803558349609,4.155202865600586,4.592298984527588,4.655975341796875,4.570188999176025,4.67529296875,4.149698734283447,4.111392498016357,4.625186920166016,4.129754066467285,4.608168125152588,4.609062194824219,4.684344291687012,4.598238468170166,4.515726089477539,4.097564697265625,4.126251697540283,4.632607936859131,3.846534013748169,4.030094146728516,4.677065372467041,4.595035552978516,4.631924152374268,4.152017593383789,4.6212286949157715,4.092752933502197,4.058029651641846,4.524569034576416,4.68303918838501,4.761435031890869,4.133310794830322,4.037563800811768,4.604251384735107,4.569831848144531,4.141808032989502,4.4533562660217285,4.653535842895508,4.054136753082275,4.630998134613037,3.962613344192505,4.415659427642822],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"10_qa_question_questions\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"10_qa_question_questions\"],\"x\":[9.40961742401123,9.401433944702148,9.173538208007812,9.561808586120605,9.133511543273926,9.540070533752441,9.700275421142578,9.41574478149414,9.289504051208496,9.243343353271484,9.41695785522461,9.496110916137695,9.767770767211914,9.445943832397461,9.416340827941895,9.574084281921387,9.292332649230957,9.226899147033691,9.462366104125977,9.370545387268066,9.44404411315918,9.387925148010254,9.353792190551758,9.420747756958008,9.548852920532227,9.489824295043945,9.414810180664062,9.33342456817627,9.402825355529785,9.672316551208496,9.393604278564453,9.239797592163086,9.441904067993164,9.516083717346191,9.340642929077148,9.363749504089355,9.59388256072998,9.435190200805664,9.716001510620117,9.322286605834961,9.686580657958984,9.433467864990234,9.423392295837402,9.48121166229248,9.288214683532715,9.267616271972656,9.553618431091309,9.478880882263184,9.371923446655273,9.760424613952637,9.519524574279785,9.401512145996094,9.502867698669434,9.440361976623535],\"y\":[-0.5178394317626953,-0.650516152381897,-0.479851096868515,-0.4310290515422821,-0.5378267168998718,-0.6259913444519043,-0.7380387187004089,-0.21319614350795746,-0.7479232549667358,-0.610841691493988,-0.6754353642463684,-0.5430178642272949,-0.6768491268157959,-0.45748335123062134,-0.3581988215446472,-0.8913076519966125,-0.698038637638092,-0.7323350310325623,-0.4265297055244446,-0.6116015911102295,-0.9226518869400024,-0.7730879783630371,-0.7615750432014465,-0.7331415414810181,-0.8823621869087219,-0.8506748676300049,-0.7885114550590515,-0.723310649394989,-0.7920389175415039,-0.6305829882621765,-0.698114275932312,-0.7590868473052979,-0.9453780651092529,-0.47471097111701965,-0.7153941988945007,-0.7514808177947998,-0.32000821828842163,-0.5421023368835449,-0.5268405079841614,-0.7256784439086914,-0.6647657752037048,-0.5601599812507629,-0.8130180239677429,-0.5797785520553589,-0.76743084192276,-0.740344226360321,-0.6408556699752808,-0.4104425609111786,-0.3353818655014038,-0.6775698661804199,-0.7086683511734009,-0.9370354413986206,-0.7324972152709961,-0.6511043310165405],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"13_reasoning_pangu_symbolic\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"13_reasoning_pangu_symbolic\"],\"x\":[9.99891185760498,9.666812896728516,9.87675666809082,9.759414672851562,9.77737045288086,9.712926864624023,9.860082626342773,10.150372505187988,9.757026672363281,9.756099700927734,9.88352108001709,9.667094230651855,9.834576606750488,9.662117958068848,9.90071964263916,9.823662757873535,9.885151863098145,9.897980690002441,9.599701881408691,9.828813552856445,10.166223526000977,9.664942741394043,9.68136215209961,9.909587860107422,9.613624572753906,9.683416366577148,9.677172660827637,9.895627975463867,9.831110954284668,9.791001319885254,9.888591766357422,10.030932426452637,10.003292083740234,9.82230281829834],\"y\":[-1.2806227207183838,-1.4134907722473145,-1.3984967470169067,-0.8591680526733398,-1.1667323112487793,-0.9986860752105713,-1.226072072982788,-1.2499401569366455,-1.3567789793014526,-1.3018178939819336,-1.279323697090149,-1.3926376104354858,-0.8900179862976074,-1.3667707443237305,-1.3356115818023682,-1.4169893264770508,-1.3518718481063843,-1.17157781124115,-1.1077362298965454,-1.3785203695297241,-1.242591381072998,-1.3728218078613281,-1.36669921875,-1.1680376529693604,-1.3303998708724976,-0.8987433314323425,-1.368369460105896,-1.1938246488571167,-1.3907575607299805,-1.302925705909729,-1.1836531162261963,-1.1196755170822144,-1.154442310333252,-1.243509292602539],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"14_speech_asr_s2st\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"14_speech_asr_s2st\"],\"x\":[13.670157432556152,13.798901557922363,13.369604110717773,13.828919410705566,13.80864143371582,13.781868934631348,13.87149715423584,13.169575691223145,13.689194679260254,13.725122451782227,13.867948532104492,13.486695289611816,13.795697212219238,13.88539981842041,13.794876098632812,13.703969955444336,13.475425720214844,13.485220909118652,13.642592430114746,13.45029067993164,13.17920970916748,13.825347900390625,13.710762977600098,13.833584785461426,13.823281288146973,13.777862548828125,13.709671974182129,13.589035034179688,13.624615669250488,13.818294525146484,13.520774841308594,13.499052047729492,13.662908554077148],\"y\":[1.930212140083313,1.9589751958847046,1.9856090545654297,2.118469476699829,1.8433870077133179,2.083873748779297,1.9659463167190552,1.6238290071487427,1.9088102579116821,1.984571933746338,1.8491621017456055,1.8348524570465088,1.9095227718353271,2.1351234912872314,2.0434975624084473,1.9872238636016846,1.9833941459655762,1.972872018814087,1.8849225044250488,1.9644986391067505,1.6182628870010376,2.087661027908325,1.9592905044555664,1.9400484561920166,1.8442121744155884,2.052821636199951,2.1948325634002686,1.9715791940689087,1.9485830068588257,1.9467763900756836,2.033796548843384,2.05224347114563,1.9568393230438232],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"15_classification_oov_slash\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"15_classification_oov_slash\"],\"x\":[11.668291091918945,11.893047332763672,11.399813652038574,11.399984359741211,11.689655303955078,11.275130271911621,11.221017837524414,12.198081016540527,12.027531623840332,11.27303695678711,11.897500038146973,11.873612403869629,11.806347846984863,11.46186637878418,11.398143768310547,11.580697059631348,11.322710990905762,11.420403480529785,11.564152717590332,11.334028244018555,11.482306480407715,11.22659969329834,11.433297157287598,11.450419425964355,11.489837646484375,11.502764701843262,11.421748161315918,11.442119598388672,12.117900848388672,11.278861999511719,11.551697731018066],\"y\":[1.6213551759719849,1.7356750965118408,1.8189188241958618,2.011199474334717,1.6284370422363281,1.6911828517913818,1.434216856956482,1.9124113321304321,1.708951473236084,1.345273733139038,1.9241002798080444,1.706599235534668,1.5955106019973755,1.6075619459152222,1.8218833208084106,1.9240765571594238,1.5612646341323853,1.4347044229507446,1.8333743810653687,1.4203897714614868,1.508184552192688,1.6106964349746704,1.7237390279769897,1.6263391971588135,1.7565888166427612,1.8806627988815308,1.4971145391464233,1.4235330820083618,2.043459415435791,1.6557258367538452,1.682104468345642],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"16_event_events_eae\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"16_event_events_eae\"],\"x\":[11.366277694702148,11.415215492248535,11.413336753845215,11.3681058883667,11.386617660522461,11.464384078979492,11.333633422851562,11.434808731079102,11.411873817443848,11.339577674865723,11.400299072265625,11.394913673400879,11.41909122467041,11.410325050354004,11.436091423034668,11.440723419189453,11.421466827392578,11.424187660217285,10.752309799194336,11.30122184753418,11.429636001586914,11.37482738494873,11.405726432800293,11.432455062866211,11.494464874267578,11.39643669128418,11.412282943725586,11.394112586975098,11.40078353881836,11.381903648376465],\"y\":[-1.7047370672225952,-1.7189042568206787,-1.7727406024932861,-1.7914379835128784,-1.7604166269302368,-1.5030242204666138,-1.8196848630905151,-1.7163331508636475,-1.6494393348693848,-1.6843397617340088,-1.7724716663360596,-1.7692655324935913,-1.7378288507461548,-1.7441824674606323,-1.464735746383667,-1.7443996667861938,-1.7241114377975464,-1.6847602128982544,0.36447063088417053,-1.677884578704834,-1.6699968576431274,-1.7729989290237427,-1.7663508653640747,-1.7174915075302124,-1.603288173675537,-1.7217741012573242,-1.7533800601959229,-1.768631100654602,-1.7753701210021973,-1.6422590017318726],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"17_explanations_explanation_concepts\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"17_explanations_explanation_concepts\"],\"x\":[9.92600154876709,9.90449047088623,9.936745643615723,9.92866325378418,9.815776824951172,9.909724235534668,9.910490989685059,9.891134262084961,9.903509140014648,9.932821273803711,9.916892051696777,9.828373908996582,9.930082321166992,9.88005256652832,9.91939926147461,10.002853393554688,9.763079643249512,9.934072494506836,9.936637878417969,9.912662506103516,9.736724853515625,9.912676811218262,9.76375961303711,9.90211296081543,9.953424453735352,9.921619415283203,9.907418251037598,9.980151176452637,9.924909591674805,9.899526596069336],\"y\":[-2.003943920135498,-2.018934726715088,-2.0210397243499756,-1.9936052560806274,-1.9008338451385498,-1.989972710609436,-2.0100209712982178,-1.9968105554580688,-2.0147523880004883,-2.0181238651275635,-2.0108845233917236,-1.9405567646026611,-1.9686578512191772,-1.9617516994476318,-2.0073821544647217,-1.730658769607544,-1.9098670482635498,-1.9983524084091187,-2.0032131671905518,-2.021005392074585,-1.78288996219635,-1.9908539056777954,-1.9123069047927856,-1.9989206790924072,-1.8872182369232178,-2.0079305171966553,-1.9907784461975098,-1.9225029945373535,-2.0194551944732666,-1.9666630029678345],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"18_prompts_prompt_continuous\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"18_prompts_prompt_continuous\"],\"x\":[9.238378524780273,9.30147647857666,9.232583999633789,9.182598114013672,9.238131523132324,9.218341827392578,9.234349250793457,9.302995681762695,9.232303619384766,9.124175071716309,9.416687965393066,9.293940544128418,9.349409103393555,9.29613208770752,9.22963809967041,9.208431243896484,9.365690231323242,9.037209510803223,9.345200538635254,9.246965408325195,9.236005783081055,9.268945693969727,9.25452709197998],\"y\":[0.11233166605234146,0.2616926431655884,0.13499541580677032,0.13999022543430328,0.13154223561286926,0.03455710411071777,0.1343674212694168,0.09137420356273651,0.0880853608250618,-0.03193724900484085,0.282012939453125,0.19786401093006134,0.27887430787086487,0.23954655230045319,0.1252308189868927,0.2048257291316986,0.21515387296676636,0.2964838743209839,0.28831443190574646,0.10827220976352692,0.010295077227056026,0.17448420822620392,0.15992532670497894],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"19_zeroshot_fewshot_verbalizers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"19_zeroshot_fewshot_verbalizers\"],\"x\":[11.956093788146973,12.490596771240234,12.429107666015625,12.604439735412598,12.537409782409668,12.447066307067871,12.514530181884766,12.494388580322266,12.444210052490234,12.318142890930176,12.335335731506348,12.499151229858398,12.503713607788086,12.477373123168945,12.43051815032959,12.483855247497559,12.453995704650879,12.551338195800781,12.408796310424805,12.414511680603027,12.439727783203125],\"y\":[-0.01690675877034664,-0.3107600808143616,-0.2527309060096741,-0.2602718472480774,-0.33801159262657166,-0.32423314452171326,-0.19620446860790253,-0.286507248878479,-0.2868269979953766,2.4987032413482666,-0.19626036286354065,-0.2882573902606964,2.8162295818328857,-0.27515092492103577,-0.28718024492263794,-0.29625701904296875,-0.3112342059612274,-0.32359012961387634,-0.29335451126098633,-0.24639558792114258,0.026239966973662376],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"20_chatgpt_chatgpts_rewriting\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"20_chatgpt_chatgpts_rewriting\"],\"x\":[8.408228874206543,8.558862686157227,8.468222618103027,8.394969940185547,8.488740921020508,8.462080955505371,8.458389282226562,8.648653030395508,8.45824146270752,8.496983528137207,8.497143745422363,8.60595989227295,8.652069091796875,8.46937370300293,8.726569175720215,8.704874992370605,8.474954605102539,8.499213218688965,8.526307106018066],\"y\":[0.8488311767578125,0.8990485668182373,0.930065929889679,0.9668167233467102,0.8830121159553528,0.8673952221870422,0.9546473622322083,0.9203405976295471,0.9518395662307739,0.8990188241004944,0.8826806545257568,1.001754879951477,0.8333197236061096,0.94631427526474,0.7836666703224182,0.7688788771629333,0.916922390460968,0.7554416656494141,0.88944411277771],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"21_tables_table_tableqa\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"21_tables_table_tableqa\"],\"x\":[10.386235237121582,10.229713439941406,10.389017105102539,10.301673889160156,10.361222267150879,10.3976469039917,10.311823844909668,10.32546329498291,10.373322486877441,10.308487892150879,10.342009544372559,10.355380058288574,10.305009841918945,10.051471710205078,10.35975456237793,10.3198823928833],\"y\":[-0.7522066235542297,-0.7360655665397644,-0.7566789388656616,-0.7287562489509583,-0.7565518617630005,-0.6721810698509216,-0.7266584038734436,-0.7462540864944458,-0.6999889016151428,-0.7211228609085083,-0.7179433107376099,-0.7139708399772644,-0.7397772073745728,-0.6260511875152588,-0.7241697311401367,-0.7212250828742981],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"22_oosf_instances_proxy\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"22_oosf_instances_proxy\"],\"x\":[11.03643798828125,11.07364559173584,11.248010635375977,11.170310974121094,10.949195861816406,11.285311698913574,10.957159996032715,11.096034049987793,11.2238130569458,11.023275375366211,11.042583465576172,11.16788101196289,11.188385009765625,11.112465858459473],\"y\":[3.106004476547241,3.0234382152557373,3.3541853427886963,3.115736246109009,3.0922281742095947,3.283050775527954,3.0349786281585693,3.2528135776519775,3.086516857147217,3.087618112564087,3.101942300796509,3.0745949745178223,3.133884906768799,3.1343841552734375],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"23_figurative_metaphors_sociocultural\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"23_figurative_metaphors_sociocultural\"],\"x\":[12.890239715576172,12.9169282913208,12.898111343383789,12.95676326751709,12.863883018493652,12.8355073928833,12.85640811920166,13.136683464050293,12.889006614685059,7.990469932556152,12.911820411682129,12.735673904418945,12.899605751037598,12.521623611450195],\"y\":[0.026861172169446945,-0.11374158412218094,-0.0861361101269722,-0.08715442568063736,0.14325851202011108,-0.09389468282461166,-0.06494750827550888,-0.21590127050876617,-0.029119817540049553,0.18973292410373688,-0.05029120296239853,0.2098502218723297,-0.02140832133591175,-0.014837851747870445],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"24_sarcasm_irony_humor\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"24_sarcasm_irony_humor\"],\"x\":[7.443446159362793,7.453991889953613,7.434319496154785,7.457515239715576,7.445913314819336,7.432080268859863,7.492039680480957,7.444361686706543,7.45388650894165,7.470397472381592,7.456826210021973,7.457319259643555,7.452033042907715,7.453394889831543],\"y\":[3.3650591373443604,3.3583810329437256,3.3732411861419678,3.3514108657836914,3.3623621463775635,3.374535083770752,3.3646886348724365,3.3602709770202637,3.329127788543701,3.332568883895874,3.3524117469787598,3.345237970352173,3.361050605773926,3.3561806678771973],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"25_dense_retrieval_retrievers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"25_dense_retrieval_retrievers\"],\"x\":[11.089601516723633,11.02492904663086,10.883186340332031,10.981522560119629,10.981151580810547,10.95821762084961,10.9121675491333,10.982845306396484,11.157200813293457,10.896283149719238,10.92767333984375,10.96117877960205,10.979663848876953],\"y\":[2.4378020763397217,2.470427989959717,2.49906587600708,2.4681739807128906,2.4417030811309814,2.4632935523986816,2.5861077308654785,2.4596455097198486,2.333122968673706,2.6070048809051514,2.5345096588134766,2.460688352584839,2.480128526687622],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"26_argumentative_environmental_firms\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"26_argumentative_environmental_firms\"],\"x\":[9.245034217834473,9.240274429321289,9.255699157714844,9.306416511535645,9.330011367797852,9.5535249710083,9.301241874694824,9.279705047607422,9.539828300476074,9.543222427368164,9.282510757446289,9.352497100830078],\"y\":[2.6040866374969482,2.5882256031036377,2.6099462509155273,2.364428997039795,2.5036041736602783,2.5556437969207764,2.5966074466705322,2.450493335723877,2.5414693355560303,2.4918816089630127,2.483466386795044,2.526350498199463],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"27_proprietary_opensourced_recommendatio\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"27_proprietary_opensourced_recommendatio\"],\"x\":[8.700318336486816,8.662346839904785,8.710755348205566,8.685635566711426,10.063787460327148,8.825141906738281,9.746978759765625,8.632473945617676,8.7011137008667,9.762179374694824,10.958747863769531,9.222679138183594],\"y\":[1.2634673118591309,1.199538230895996,1.2940387725830078,1.2151010036468506,2.198474407196045,1.475010633468628,3.3699069023132324,1.1698120832443237,1.2515336275100708,3.3798534870147705,1.8569083213806152,1.78851318359375],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"28_modification_mp2_multiword\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"28_modification_mp2_multiword\"],\"x\":[12.416332244873047,12.561809539794922,12.500757217407227,12.375421524047852,12.55825138092041,12.481864929199219,12.49730396270752,12.1989107131958,12.471221923828125,12.578041076660156,12.469315528869629,12.464476585388184],\"y\":[1.9628962278366089,2.3608059883117676,2.222209930419922,2.1565310955047607,2.1810107231140137,2.1097028255462646,2.3813178539276123,2.188523292541504,2.159067153930664,2.3556671142578125,1.6200826168060303,2.1543469429016113],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"29_instructional_teaching_students\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"29_instructional_teaching_students\"],\"x\":[8.679919242858887,8.863896369934082,8.659948348999023,8.705171585083008,8.791703224182129,8.700974464416504,8.720808029174805,9.014729499816895,8.698103904724121,8.63776969909668,8.665627479553223,8.739877700805664],\"y\":[0.4001754820346832,0.5290360450744629,0.39178118109703064,0.5974481105804443,0.46730977296829224,0.44593337178230286,0.3917016386985779,0.42155221104621887,0.3866553008556366,0.5923276543617249,0.5228016972541809,0.4678838849067688],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"30_counterfactuals_scone_counterfactual\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"30_counterfactuals_scone_counterfactual\"],\"x\":[10.26634407043457,10.128600120544434,10.363204956054688,10.149825096130371,10.254651069641113,10.238754272460938,10.266803741455078,10.18940258026123,10.27680492401123,10.135074615478516,10.226946830749512],\"y\":[2.246767520904541,2.1721770763397217,2.3008975982666016,2.423623561859131,2.2409117221832275,2.265772581100464,2.3193318843841553,2.154175281524658,2.2092831134796143,2.1171963214874268,2.24501371383667],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"33_translation_languages_bias_gender_soc\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"33_translation_languages\"],\"x\":[13.87353229522705,13.208016395568848,14.106237411499023,13.092193603515625,13.336145401000977,13.386882781982422,12.319914817810059,13.021512985229492,13.98583698272705,12.727828025817871,14.158334732055664,12.96531867980957,13.214096069335938,13.292146682739258,13.683382034301758,13.187692642211914,13.279541969299316,12.989252090454102,12.971858978271484,12.976456642150879,13.035022735595703,12.499113082885742,13.1813325881958,12.979851722717285,13.296390533447266,13.039684295654297,13.842495918273926,13.132991790771484,13.9254732131958,12.167425155639648,14.098097801208496,13.920403480529785,12.558815002441406,12.849774360656738,12.970587730407715,14.02845287322998,12.911493301391602,13.765254974365234,13.905959129333496,13.326302528381348,13.942405700683594,12.925593376159668,13.41197395324707,13.25645637512207,12.991842269897461,13.592504501342773,13.934208869934082,13.955780029296875,13.298561096191406,13.263577461242676,12.345218658447266,13.947273254394531,13.009997367858887,13.643670082092285,13.180303573608398,13.74915599822998,12.808438301086426,13.863404273986816,12.991873741149902,13.162498474121094,12.859356880187988,13.075448036193848,13.156818389892578,12.886246681213379,13.997769355773926,13.03771686553955,12.784741401672363,12.894862174987793,12.97797966003418,13.60038948059082,14.051366806030273,12.930017471313477,12.712410926818848,12.782764434814453,12.85952377319336,12.852415084838867,13.387727737426758,13.939677238464355,12.471564292907715,13.912345886230469,12.641358375549316,13.367488861083984,13.8226318359375,13.64598560333252,13.757844924926758,12.249322891235352,13.732461929321289,13.127846717834473,12.968668937683105,13.813699722290039,13.807978630065918,13.008394241333008,13.787175178527832,13.993060111999512,13.531773567199707,13.984949111938477,13.01711654663086,14.002456665039062,13.822026252746582,13.829243659973145,13.329290390014648,12.908177375793457,14.121039390563965,13.951969146728516,12.813560485839844,13.843708038330078,13.839166641235352,14.121064186096191,12.558928489685059,13.817070960998535,13.30069637298584,13.859855651855469,13.09512710571289,12.266813278198242,12.803173065185547,13.087562561035156,13.733831405639648,13.884516716003418,12.79514217376709,13.156198501586914,12.635796546936035,13.271025657653809,12.98554801940918,13.871160507202148,13.031496047973633,14.019685745239258,13.770309448242188,13.913249969482422,13.995428085327148,12.395220756530762,13.254109382629395,12.519502639770508,13.016617774963379,13.239713668823242,13.700984954833984,13.32162857055664,13.05946159362793,12.602531433105469,12.626888275146484,13.298456192016602,13.010909080505371,12.473347663879395,13.30741024017334,12.52789306640625,13.24752140045166,14.047928810119629,12.949474334716797,13.951871871948242,14.054276466369629,12.840779304504395,12.930652618408203,13.249177932739258,14.00323486328125,12.957128524780273,13.71989631652832,12.736191749572754,13.225967407226562,9.189112663269043,9.039446830749512,9.115840911865234,9.096116065979004,9.138891220092773,8.879927635192871,8.893594741821289,8.761467933654785,8.84201431274414,8.92170524597168,8.984542846679688,8.870153427124023,9.148591041564941,9.1036376953125,8.759328842163086,9.173047065734863,9.000730514526367,9.311477661132812,9.226938247680664,8.916337966918945,9.266477584838867,8.85345458984375,8.894457817077637,9.150871276855469,8.838128089904785,8.816519737243652,9.046035766601562,8.901216506958008,9.159720420837402,8.813364028930664,9.301010131835938,8.885666847229004,9.035273551940918,9.151689529418945,9.173745155334473,9.153688430786133,9.068913459777832,9.077773094177246,8.93253231048584,9.23687744140625,8.845597267150879,8.906045913696289,9.104510307312012,8.794829368591309,8.974413871765137,8.559188842773438,9.103609085083008,8.886739730834961,8.847504615783691,8.852912902832031,8.7335786819458,8.971410751342773,8.835782051086426,8.914285659790039,8.319539070129395,8.872060775756836,8.969176292419434,8.707355499267578,8.74543285369873,8.927281379699707,8.319100379943848,8.32375717163086,8.666680335998535,8.335197448730469,8.70714282989502,9.042021751403809,8.712772369384766,8.291357040405273,8.325663566589355,8.38085651397705,8.75160026550293,8.50749397277832,8.352300643920898,8.444624900817871,8.73957347869873,8.366410255432129,8.543909072875977,8.721636772155762,8.295104026794434,8.693644523620605,8.322171211242676,8.898469924926758,8.4146146774292,8.773049354553223,8.323225975036621,9.074934959411621,8.57706356048584,8.7324857711792,8.81857967376709,9.022188186645508,8.334091186523438,8.382010459899902,8.761948585510254,8.723730087280273,9.060052871704102,8.774007797241211,8.736745834350586,8.403592109680176,11.581463813781738],\"y\":[1.3817375898361206,1.0612719058990479,1.1385481357574463,1.08525550365448,1.1626895666122437,1.2480696439743042,0.4848286211490631,1.2720167636871338,1.1438474655151367,1.3171401023864746,1.4076839685440063,1.264595627784729,1.0884864330291748,1.1517572402954102,1.355188012123108,1.7229746580123901,0.972616970539093,1.1486741304397583,0.8810750246047974,1.257293701171875,0.9572843313217163,0.8085752129554749,0.9309483766555786,0.7958930730819702,1.0975350141525269,0.6360377073287964,0.9854050874710083,1.798175573348999,1.3100166320800781,0.67192143201828,1.2066819667816162,1.2865713834762573,0.9056100845336914,0.6249911189079285,1.7463622093200684,1.2420947551727295,0.7832163572311401,1.1565204858779907,1.3271960020065308,1.3933436870574951,1.0287328958511353,0.7349646091461182,1.1217514276504517,1.414216160774231,1.152062177658081,1.4249526262283325,1.1767480373382568,1.2981083393096924,1.1263619661331177,1.0864992141723633,0.5083565711975098,1.0866432189941406,1.8492233753204346,1.125476598739624,0.944848895072937,1.1385208368301392,0.678071916103363,1.3165032863616943,0.6076777577400208,1.3482673168182373,1.6210135221481323,0.9811966419219971,1.2457525730133057,1.171410322189331,1.1819112300872803,0.8604665398597717,0.942077100276947,0.516913115978241,1.7408256530761719,1.4816997051239014,1.3886042833328247,1.7745593786239624,0.794830322265625,0.5049740076065063,0.8629714250564575,0.9947225451469421,1.3124425411224365,1.181308388710022,0.65378338098526,0.9567456841468811,0.8112806677818298,1.0578453540802002,1.270293116569519,1.1971663236618042,1.2050573825836182,0.49335476756095886,1.5063146352767944,1.3702335357666016,1.2522984743118286,1.2417391538619995,1.4349349737167358,1.29513418674469,1.3764736652374268,1.1924550533294678,1.3044599294662476,1.0554274320602417,0.9203598499298096,1.1263551712036133,1.4461302757263184,1.4113985300064087,1.1233012676239014,1.292132019996643,1.383273959159851,1.2921757698059082,1.4860016107559204,1.4392153024673462,1.4001719951629639,0.9837578535079956,0.6659143567085266,0.9185031056404114,1.1586668491363525,1.4281121492385864,1.1629738807678223,0.510953962802887,0.5897344946861267,0.8766728639602661,1.4713213443756104,1.1049691438674927,0.5188847184181213,1.4383823871612549,1.3952010869979858,1.3247922658920288,0.9874094128608704,1.13014554977417,0.8806256651878357,1.1321403980255127,1.1713860034942627,1.29535710811615,1.1741254329681396,0.570444643497467,1.2960554361343384,0.7852280735969543,0.9467426538467407,1.7510077953338623,1.3233586549758911,1.1152814626693726,1.0483297109603882,0.647891640663147,0.5596342086791992,1.3211227655410767,0.9677881002426147,0.6502654552459717,1.0543253421783447,0.6772043704986572,1.212782382965088,1.1973955631256104,1.1335924863815308,1.4092952013015747,1.3452917337417603,0.6293814778327942,1.8506160974502563,1.066854476928711,1.2767295837402344,0.7192075848579407,1.712160587310791,0.8005959391593933,1.1127575635910034,3.968130350112915,4.086702346801758,3.7755074501037598,3.991488218307495,3.975116014480591,4.029293060302734,4.046248435974121,4.013284683227539,4.176393032073975,4.156956672668457,4.055082321166992,3.9321210384368896,3.937089204788208,3.9232289791107178,3.8601222038269043,3.866779327392578,3.9942166805267334,3.850517511367798,3.827030897140503,4.08809232711792,3.6295218467712402,4.113508701324463,4.187417507171631,3.6063523292541504,3.9586288928985596,4.24702787399292,3.9166204929351807,4.1035475730896,3.9102394580841064,4.182351112365723,3.869297981262207,3.981309175491333,3.9789490699768066,3.9490838050842285,3.944530487060547,3.942408323287964,4.038426399230957,4.008914947509766,4.139644622802734,3.7226343154907227,4.209354877471924,4.045825958251953,3.9882144927978516,4.28642463684082,4.0150346755981445,3.615358352661133,4.036133289337158,4.028831958770752,3.9747371673583984,3.9180045127868652,3.7974116802215576,4.060174942016602,4.20427942276001,4.09943962097168,3.193037986755371,3.0153839588165283,3.2098007202148438,3.411353826522827,3.419900894165039,2.993455171585083,3.2046985626220703,3.197061538696289,3.5652174949645996,3.1496386528015137,3.4359893798828125,3.163926362991333,3.4493963718414307,3.234320640563965,3.2002310752868652,3.272998094558716,3.3818790912628174,3.3397650718688965,3.0758256912231445,3.1623947620391846,3.384615182876587,3.2408924102783203,3.358213186264038,3.416884660720825,3.167921781539917,3.4248197078704834,3.2836413383483887,3.2514102458953857,3.342782497406006,3.3918700218200684,3.194164752960205,3.047151803970337,3.55344820022583,3.2842931747436523,3.3557095527648926,3.1627235412597656,3.2219274044036865,3.0630640983581543,3.3806440830230713,3.417919874191284,3.0919387340545654,3.379578113555908,3.3994202613830566,3.3199284076690674,2.1015961170196533],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"35_summarization_clinical_medical_summar\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"35_summarization_clinical\"],\"x\":[7.761767864227295,7.890107154846191,8.475704193115234,8.049864768981934,7.910464286804199,7.735909938812256,8.021018028259277,7.9187726974487305,7.908551216125488,7.699703216552734,7.959645748138428,7.826615333557129,8.282672882080078,7.934024333953857,8.200965881347656,7.851012229919434,7.659298419952393,7.964986801147461,7.375672340393066,7.989088535308838,8.16100788116455,7.880589008331299,7.8358564376831055,7.8488359451293945,7.915194511413574,8.008590698242188,7.819448471069336,7.807568073272705,7.392760753631592,7.372259140014648,7.680557727813721,8.323735237121582,7.405981540679932,7.390983581542969,8.47547721862793,7.814692974090576,8.292101860046387,8.115535736083984,7.816876411437988,7.764798164367676,7.4307541847229,8.173230171203613,7.816125869750977,7.800354957580566,7.687350749969482,7.416838645935059,8.04630184173584,7.372389793395996,8.061315536499023,7.425755023956299,7.877260684967041,8.388016700744629,7.779290199279785,8.227629661560059,8.057212829589844,8.02259349822998,7.782587051391602,8.23399543762207,7.90402889251709,7.8760199546813965,7.4668121337890625,8.308961868286133,7.824159622192383,7.712260723114014,7.914191246032715,7.685945987701416,7.940428733825684,9.407848358154297,9.24934196472168,9.988895416259766,9.616469383239746,9.254940032958984,9.619887351989746,9.763313293457031,9.41704273223877,9.903557777404785,9.640640258789062,9.904218673706055,9.833464622497559,9.501309394836426,10.281272888183594,9.5150146484375,9.301063537597656,9.468867301940918,9.583669662475586,9.652227401733398,9.2976655960083,9.610597610473633,9.950066566467285,9.628329277038574,9.354300498962402,9.685493469238281,9.437472343444824,9.71446418762207,9.970356941223145,9.591911315917969,9.4934720993042,11.245697975158691,9.630011558532715,9.857994079589844,9.644820213317871,9.684257507324219,9.61834716796875,9.774521827697754,9.085481643676758,10.036049842834473,9.738116264343262,9.506299018859863,9.30492877960205,9.699036598205566,9.771791458129883,9.192770957946777,9.514463424682617,9.63692855834961,9.843480110168457,9.38808536529541,9.520946502685547,9.217379570007324,9.838109970092773,9.648056030273438,9.77692699432373,9.458232879638672,9.670414924621582,9.680063247680664,9.057123184204102,8.691584587097168],\"y\":[1.5433651208877563,1.4605567455291748,1.4761929512023926,1.4625173807144165,1.4584766626358032,1.4409211874008179,1.4749916791915894,1.4284682273864746,1.4748163223266602,1.4806615114212036,1.4631659984588623,1.411305546760559,1.4848947525024414,1.0450985431671143,1.4943875074386597,1.3517125844955444,1.4878875017166138,1.4422603845596313,1.5346200466156006,1.5406434535980225,1.499199628829956,1.4141327142715454,1.4414336681365967,1.3886102437973022,1.4608150720596313,1.4566928148269653,1.5145379304885864,1.6541327238082886,1.5264090299606323,1.531654953956604,1.4845945835113525,1.4692519903182983,1.5288569927215576,1.5096898078918457,1.3595173358917236,1.587659478187561,1.4848111867904663,1.480909824371338,1.49443781375885,1.6546357870101929,1.5368572473526,1.2316579818725586,1.3251349925994873,1.5530869960784912,1.5514334440231323,1.5379899740219116,1.494699239730835,1.525313377380371,1.4642879962921143,1.526352882385254,1.288293480873108,1.483452320098877,1.4212583303451538,1.5053179264068604,1.3147481679916382,1.3840261697769165,1.5364909172058105,1.516196370124817,1.4429274797439575,1.4955360889434814,1.5462852716445923,1.454986810684204,1.4761298894882202,1.4729735851287842,1.450022578239441,1.443420648574829,1.4701296091079712,1.5877009630203247,1.2727618217468262,1.4802215099334717,1.6104549169540405,1.4569437503814697,1.539625644683838,1.472213864326477,2.0593249797821045,1.3333063125610352,1.6195672750473022,0.646668553352356,1.6028765439987183,1.468073844909668,1.4336971044540405,1.469533920288086,1.5507453680038452,1.6590402126312256,1.5533472299575806,1.5326228141784668,1.4909952878952026,1.5933552980422974,0.6876698732376099,1.5868359804153442,1.573804259300232,1.5753282308578491,1.2330971956253052,1.6163599491119385,0.7645369172096252,1.5960463285446167,1.578489899635315,0.5172238349914551,1.5027610063552856,1.38727867603302,1.5584850311279297,1.571655035018921,1.5743860006332397,1.5758932828903198,1.8300682306289673,1.7321003675460815,1.5929148197174072,1.4410037994384766,1.594342589378357,1.557225227355957,1.3510727882385254,1.2829564809799194,1.1802170276641846,1.5979278087615967,1.6627514362335205,1.6593539714813232,1.5607953071594238,1.3650426864624023,0.5767338275909424,1.5249618291854858,1.5589072704315186,1.6015998125076294,1.5737286806106567,1.52560555934906,1.5944088697433472,1.4643566608428955],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"36_dialogue_entity_visual_graph_knowledg\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"36_dialogue_entity\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"x\":[11.134943962097168,11.025127410888672,11.245323181152344,11.604127883911133,11.156891822814941,11.352185249328613,11.527443885803223,11.282083511352539,11.126087188720703,11.73695182800293,11.573534965515137,13.375391006469727,11.404452323913574,11.218412399291992,10.824934959411621,11.162917137145996,11.561422348022461,11.366466522216797,10.656908988952637,11.300393104553223,11.272549629211426,11.278218269348145,11.19459056854248,11.2599458694458,11.612763404846191,10.9222412109375,11.204724311828613,11.096510887145996,11.001474380493164,11.493638038635254,11.407891273498535,11.244793891906738,11.168529510498047,11.017486572265625,11.451024055480957,11.223573684692383,11.187163352966309,11.271790504455566,11.26801872253418,11.24870491027832,11.188407897949219,11.215272903442383,11.429038047790527,11.241147994995117,11.28900146484375,11.684981346130371,11.643854141235352,11.422016143798828,11.644282341003418,11.211750030517578,11.794293403625488,10.666879653930664,11.676538467407227,11.204644203186035,11.4844388961792,11.210556983947754,10.822954177856445,11.602618217468262,11.172928810119629,11.261439323425293,11.18604850769043,11.24404525756836,11.392868995666504,11.273892402648926,11.069701194763184,11.81900691986084,11.298412322998047,11.697004318237305,11.151240348815918,11.614058494567871,11.220532417297363,11.269240379333496,11.204486846923828,11.25660228729248,11.144911766052246,11.228436470031738,11.389638900756836,11.027185440063477,11.341411590576172,11.200804710388184,10.692567825317383,11.169511795043945,11.02439022064209,10.92267894744873,11.365792274475098,11.304564476013184,11.123757362365723,11.317601203918457,11.2233304977417,11.252074241638184,10.953468322753906,11.1937837600708,11.191131591796875,11.507041931152344,13.65318489074707,13.939993858337402,13.685359001159668,13.784351348876953,13.504968643188477,13.94735336303711,13.143325805664062,13.834990501403809,13.44897747039795,13.599161148071289,13.199520111083984,13.714217185974121,13.943765640258789,13.477546691894531,13.838189125061035,13.14396858215332,13.451980590820312,13.139006614685059,13.305334091186523,13.572847366333008,13.578728675842285,13.640463829040527,13.756519317626953,13.478137016296387,13.640913963317871,13.371881484985352,13.30630874633789,13.778650283813477,13.48436164855957,13.48245620727539,13.242944717407227,13.188318252563477,13.671472549438477,13.161858558654785,13.182262420654297,13.67568302154541,13.578418731689453,13.574835777282715,13.530089378356934,12.509833335876465,13.550978660583496,13.269086837768555,13.929557800292969,13.697040557861328,13.825508117675781,13.642824172973633,13.51134204864502,13.453692436218262,13.557821273803711,13.553349494934082,13.648940086364746,10.626568794250488,null,13.566463470458984,13.692540168762207,13.704095840454102,13.338266372680664,13.631470680236816,13.964845657348633,13.344286918640137,13.934170722961426,13.56275463104248,13.731254577636719,13.44790267944336,13.115975379943848,13.600167274475098,8.303739547729492,13.527002334594727,13.690975189208984,13.423891067504883,13.938860893249512,13.628800392150879,13.415291786193848,13.883581161499023,13.465723037719727,13.598390579223633,13.958017349243164,13.456777572631836,13.795967102050781,9.905937194824219,13.677034378051758,13.943849563598633,13.411235809326172,13.518133163452148,13.331656455993652,8.079171180725098,8.06274700164795,7.97455358505249,7.848491191864014,8.032004356384277,7.964422702789307,7.964263916015625,8.134442329406738,8.114529609680176,7.841091632843018,7.8066725730896,8.066001892089844,7.995242118835449,8.329507827758789,8.135189056396484,7.784605979919434,7.775929927825928,8.180281639099121,8.422222137451172,7.919281005859375,7.885015487670898,7.895800590515137,7.771629810333252,8.177929878234863,7.973200798034668,8.28538703918457,7.766735553741455,7.8133392333984375,7.980544567108154,7.834801197052002,7.792785167694092,8.400049209594727,8.152592658996582,7.950541019439697,7.951925754547119,8.21005630493164,7.920266628265381,7.854425430297852,8.000518798828125,7.90179443359375,8.401914596557617,8.250751495361328,7.81003999710083,8.432668685913086,7.853431224822998,8.134767532348633,8.218738555908203,7.8007283210754395,8.225289344787598,7.825068473815918,8.072797775268555,8.098258972167969,8.405604362487793,7.813677787780762,7.821072101593018,7.985950946807861,7.810969352722168,8.3153657913208,8.099970817565918,7.857753276824951,8.26423168182373,7.91890811920166,7.952602386474609,7.912245273590088,7.974617958068848,7.784420490264893,8.138467788696289,8.036944389343262,9.02020263671875,8.764460563659668,9.149687767028809,8.96601676940918,8.83845329284668,7.854395389556885,8.691113471984863,8.074891090393066,7.991252422332764,8.15771198272705,8.11121940612793,7.889158725738525,8.559086799621582,7.732449054718018,8.129582405090332,7.739405155181885,8.906782150268555,8.779287338256836,8.384539604187012,8.096807479858398,7.73988151550293,7.74143123626709,10.807848930358887,8.938881874084473,8.25653076171875,8.156562805175781,8.910407066345215,8.929011344909668,8.912620544433594,8.349308967590332,9.150344848632812,9.063459396362305,8.188298225402832,8.082099914550781,8.124395370483398,8.095884323120117,7.761401653289795,7.85822057723999,7.735099792480469,9.143465995788574,7.9051079750061035,8.62359619140625,8.771903991699219,8.10483455657959,8.881553649902344,7.842296600341797,7.751319408416748,8.932684898376465,8.793137550354004,8.686760902404785,8.676131248474121,8.958822250366211,7.80643892288208,8.740694046020508,7.7468767166137695,8.771867752075195],\"y\":[-0.2791852056980133,-0.56585294008255,-0.33805036544799805,-0.5237462520599365,0.26701799035072327,-0.765414834022522,-0.7155061364173889,-0.6682496070861816,-0.6063860654830933,-0.3886547386646271,-0.6360691785812378,-0.5970664024353027,-0.6077590584754944,-0.3316837251186371,0.027478214353322983,-1.0895836353302002,-0.5561929941177368,0.06399431079626083,0.6634865999221802,-0.7742803692817688,0.14976142346858978,-1.053679347038269,-0.8100492358207703,-0.6574156284332275,-0.6182628273963928,2.9431962966918945,-0.8820046186447144,-0.8308306336402893,-0.7561229467391968,0.06208992749452591,-1.2723103761672974,0.021186748519539833,-0.9865592122077942,0.42442458868026733,-0.5611394047737122,-0.19294996559619904,-0.6945754289627075,-0.3245929181575775,0.14573052525520325,-0.8709970712661743,-0.352821946144104,-0.20292386412620544,-0.3928586542606354,-0.45419180393218994,-0.6536911725997925,-0.43990081548690796,0.47350263595581055,0.13158506155014038,-0.7095873355865479,-1.041664481163025,-0.30340245366096497,0.33025580644607544,-0.5825144648551941,-0.9634666442871094,0.1646973192691803,-0.9077123999595642,0.321090430021286,-0.4256440997123718,0.016457444056868553,-0.3007483184337616,-0.9926905035972595,-0.40955081582069397,-0.7163980603218079,-0.7613950967788696,-0.5563022494316101,-0.30304890871047974,-0.6749861836433411,-0.5403029322624207,-0.3122972548007965,-0.610802948474884,-0.7764879465103149,-0.3963066339492798,-0.9999945163726807,-0.2833714783191681,-0.40003347396850586,-0.9637208580970764,-0.45351165533065796,-0.5818671584129333,-0.7205086350440979,-0.9992696046829224,0.6563940644264221,-1.185823917388916,-1.0215363502502441,-0.13112245500087738,-0.6537771224975586,-0.7294257283210754,-0.7630423307418823,-0.7293157577514648,-0.9494206309318542,-0.3285471796989441,-0.1588168889284134,-0.36618950963020325,-0.9938457608222961,0.16926676034927368,-0.4724136292934418,0.05542946234345436,-0.2140544056892395,-0.305911123752594,-0.019443433731794357,0.04754577577114105,-0.5232704877853394,-0.11347304284572601,-0.42826223373413086,-0.3631307780742645,-0.34572187066078186,-0.06021670997142792,0.02899486944079399,-0.5741272568702698,-0.17813809216022491,-0.3307811915874481,-0.5624246597290039,-0.33823102712631226,-0.49408844113349915,-0.2875242233276367,-0.5673142671585083,-0.5054513216018677,-0.19457708299160004,0.04647556692361832,0.15025267004966736,-0.4029132127761841,-0.09265140444040298,-0.06226486340165138,-0.49906080961227417,-0.42240944504737854,-0.37416812777519226,-0.4376997649669647,-0.5433786511421204,-0.48809999227523804,-0.3587982952594757,-0.1461024433374405,-0.4376213848590851,-0.5315554738044739,-0.5797322392463684,2.760768413543701,-0.5617931485176086,-0.37373608350753784,1.0235728025436401,-0.2975458800792694,-0.12453179061412811,-0.39394140243530273,0.08051753044128418,-0.5586355328559875,-0.5786116719245911,-0.5049486756324768,0.1441514790058136,0.20823633670806885,null,-0.3426320552825928,-0.16151897609233856,-0.39145922660827637,-0.5925371646881104,-0.2956721782684326,0.027539370581507683,-0.30378881096839905,0.042292892932891846,-0.12347960472106934,-0.06413992494344711,-0.6200217604637146,-0.4954264461994171,-0.34516313672065735,3.190534830093384,-0.2043280303478241,-0.5706292986869812,-0.7155386209487915,0.12354090809822083,-0.219037726521492,-0.293334424495697,0.07163935899734497,-0.5650919675827026,-0.29002729058265686,0.07026058435440063,-0.5817172527313232,-0.10595829039812088,-1.2771755456924438,-0.2213153839111328,0.0876251682639122,-0.7389064431190491,-0.14196763932704926,-0.6306360363960266,0.011064082384109497,0.014209670014679432,-0.09134206920862198,-0.2708755433559418,-0.20944665372371674,-0.15542514622211456,-0.016267064958810806,0.33848637342453003,1.3138884241925552e-05,-0.3145374655723572,-0.37487006187438965,0.08202043920755386,-0.06734184175729752,-0.26890313625335693,-0.2482864111661911,-0.37177231907844543,-0.342668354511261,0.05140113830566406,-0.009848969988524914,-0.26512715220451355,-0.21851959824562073,-0.20532433688640594,-0.34504765272140503,-0.12112276256084442,-0.11948814243078232,-0.36536410450935364,-0.43950170278549194,-0.3522815406322479,-0.16667716205120087,-0.33151018619537354,-0.4151010513305664,-0.21129287779331207,0.18422266840934753,-0.13335680961608887,0.17345526814460754,0.07078302651643753,-0.06395117193460464,-0.33869442343711853,-0.20178943872451782,-0.20231139659881592,-0.02338528260588646,0.17116601765155792,-0.3580891191959381,-0.31286415457725525,-0.32009008526802063,-0.1331133246421814,0.18067573010921478,-0.2697615623474121,0.08013986051082611,-0.31005680561065674,0.028209263458848,0.10702250152826309,0.2667584717273712,-0.3471834659576416,-0.3380453586578369,-0.07149399816989899,-0.4023083448410034,-0.10830654948949814,-0.17893050611019135,-0.2754707932472229,0.22065694630146027,-0.1637326180934906,-0.12212429195642471,-0.17535138130187988,-0.2760526239871979,-0.01606772653758526,0.11322040110826492,0.021302759647369385,2.3929059505462646,2.3886959552764893,2.2481915950775146,2.366041660308838,2.3146793842315674,2.3162174224853516,2.369158983230591,2.5022075176239014,2.4162755012512207,2.523205280303955,2.5030629634857178,2.322309732437134,2.426605224609375,2.189079761505127,2.5072808265686035,2.1927618980407715,2.2802486419677734,2.404411792755127,2.5210015773773193,2.5109353065490723,2.188095808029175,2.2342336177825928,1.3085989952087402,2.270885467529297,2.520770311355591,2.4987809658050537,2.2820372581481934,2.2575674057006836,2.305651903152466,2.480987310409546,2.6763017177581787,2.3891241550445557,2.503016233444214,2.507106065750122,2.5494508743286133,2.4986634254455566,2.2242064476013184,2.3021676540374756,2.150664806365967,2.744899034500122,2.3470451831817627,2.496939182281494,2.4149718284606934,2.483858346939087,2.3855793476104736,2.2803268432617188,2.255380868911743,2.259631872177124,2.3189663887023926,2.3722052574157715,2.399129867553711,2.2720420360565186,2.2237091064453125,2.365219831466675,2.2338740825653076,2.3938751220703125],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"37_transformer_nat_transformers_paramete\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"37_transformer_nat\"],\"x\":[12.020341873168945,12.169936180114746,12.101693153381348,12.094861030578613,11.659080505371094,12.379535675048828,12.050992965698242,12.262616157531738,12.431180000305176,11.988661766052246,12.082118034362793,12.085043907165527,12.180397033691406,12.176328659057617,11.84795093536377,12.084325790405273,12.128952980041504,12.28459644317627,12.194994926452637,12.008187294006348,11.911856651306152,12.048596382141113,12.229466438293457,12.182258605957031,12.1466703414917,12.20826530456543,11.903266906738281,11.976790428161621,11.813048362731934,12.021183967590332,12.020978927612305,12.224409103393555,11.788299560546875,12.209701538085938,11.93454647064209,11.924176216125488,12.019671440124512,12.825284004211426,12.15970516204834,12.297412872314453,12.158596992492676,12.110843658447266,12.26720905303955,12.509387016296387,12.426708221435547,12.31076431274414,12.200416564941406,12.108731269836426,12.027480125427246,12.294480323791504,12.179040908813477,11.868775367736816,12.016388893127441,12.533409118652344,11.871016502380371,12.278585433959961,12.497551918029785,11.953156471252441,12.103267669677734,12.292784690856934,12.235295295715332,11.931829452514648,13.446449279785156,13.293713569641113,13.339015007019043,13.234209060668945,13.390213012695312,13.275945663452148,13.059819221496582,13.199735641479492,13.168375968933105,13.352516174316406,13.391827583312988,13.276883125305176,13.3883638381958,13.391206741333008,13.22536563873291,13.355441093444824,13.415063858032227,13.445826530456543,13.263446807861328,13.253372192382812,13.080596923828125,13.292510986328125,13.387595176696777,13.407983779907227,13.065677642822266,13.157443046569824,13.285446166992188,13.371879577636719,13.443427085876465,13.312077522277832,13.325447082519531,13.201516151428223,13.412653923034668,13.387580871582031,13.376065254211426,13.23760986328125,13.391357421875,13.28543758392334,13.155158996582031,13.134252548217773,13.264272689819336,13.326519012451172,13.441741943359375,13.180994987487793,13.229814529418945,13.385966300964355,13.26877498626709,13.252023696899414,12.640745162963867],\"y\":[3.4334287643432617,3.9167301654815674,3.949054002761841,3.438246250152588,3.445382595062256,3.36970853805542,3.7780611515045166,2.977111577987671,3.824798583984375,4.014473915100098,3.966614246368408,3.947669744491577,3.546800136566162,3.5740740299224854,3.722076654434204,3.7931604385375977,3.4850759506225586,3.1680963039398193,4.030865669250488,3.494990587234497,3.631016969680786,3.9821364879608154,3.874598264694214,3.628248453140259,3.362018585205078,3.6282668113708496,3.4427807331085205,3.539586305618286,3.9404172897338867,3.503599166870117,3.3990092277526855,3.441103219985962,2.9686286449432373,3.454322576522827,3.507044553756714,3.588791847229004,3.930246591567993,3.598909854888916,3.4998040199279785,3.598375082015991,3.547398805618286,3.815009355545044,3.1761443614959717,3.7004573345184326,2.9148776531219482,3.357351303100586,3.646010398864746,4.046837329864502,3.844369649887085,3.3957107067108154,3.5331528186798096,3.4589388370513916,3.6157045364379883,3.513286590576172,3.4719655513763428,3.176704168319702,3.3323616981506348,3.9445858001708984,3.7024219036102295,3.222160577774048,3.219435214996338,3.6485061645507812,2.849513053894043,2.6924357414245605,2.519382953643799,2.685671091079712,3.4131829738616943,3.3496320247650146,2.9062132835388184,2.5787570476531982,2.8339319229125977,2.6660850048065186,3.363393783569336,3.0692508220672607,3.403278112411499,2.94502854347229,2.862663984298706,2.849548578262329,3.311357021331787,2.857738494873047,2.829335927963257,2.7103164196014404,2.6873369216918945,3.1780202388763428,2.415661573410034,2.984997510910034,2.9960906505584717,3.0859627723693848,3.064985752105713,3.3739285469055176,2.99613094329834,3.3052146434783936,2.759016990661621,2.554168224334717,2.7810420989990234,3.370600700378418,2.8825161457061768,2.5941309928894043,3.402747631072998,3.1956849098205566,2.574057102203369,2.4850573539733887,3.1060783863067627,2.757913827896118,2.8156826496124268,2.6702122688293457,3.1264901161193848,3.3880455493927,2.8700947761535645,2.6230366230010986,3.2947304248809814],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"#CFD8DC\",\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"other\",\"showlegend\":false,\"x\":[12.260786056518555,10.01115894317627,11.924004554748535,10.16279411315918,12.00646686553955,10.273680686950684,6.795833110809326,10.738099098205566,12.415397644042969,12.344215393066406,8.005266189575195,11.672365188598633,9.678071022033691,11.613967895507812,10.691057205200195,10.729043006896973,12.570730209350586,12.728907585144043,11.094691276550293,12.467524528503418,10.7769136428833,11.365942001342773,11.85193157196045,10.574968338012695,13.300118446350098,8.84424877166748,9.322251319885254,11.329916000366211,9.31579303741455,10.838811874389648,11.459781646728516,11.970154762268066,12.745402336120605,10.581709861755371,9.499332427978516,10.223129272460938,9.936136245727539,11.424376487731934,7.634429931640625,12.565068244934082,12.609332084655762,11.734045028686523,12.76197338104248,8.449498176574707,10.840675354003906,11.946686744689941,8.837322235107422,9.319323539733887,12.509225845336914,10.419879913330078,6.7955498695373535,9.324790954589844,12.908655166625977,11.152015686035156,11.986949920654297,12.218483924865723,11.286534309387207,11.993667602539062,9.96347427368164,11.670063018798828,9.893813133239746,10.930827140808105,9.840456008911133,9.265555381774902,8.651935577392578,9.824747085571289,9.816277503967285,12.087455749511719,11.567899703979492,6.796971321105957,12.668866157531738,12.218778610229492,10.99422836303711,11.2903470993042,13.343911170959473,12.024370193481445,12.494938850402832,12.23508071899414,10.537565231323242,9.071687698364258,9.39167308807373,13.654808044433594,11.47091293334961,11.624275207519531,12.246249198913574,12.476778984069824,11.365124702453613,12.479470252990723,12.073331832885742,10.643670082092285,10.698538780212402,11.801766395568848,11.808372497558594,9.954280853271484,12.55838394165039,8.872209548950195,11.538613319396973,8.866655349731445,10.459000587463379,11.395777702331543,9.855978012084961,11.591615676879883,10.30197811126709,11.654211044311523,11.18188190460205,10.506263732910156,13.050704002380371,11.941532135009766,12.512845993041992,11.652451515197754,13.646256446838379,11.867741584777832,12.657735824584961,12.094124794006348,10.286686897277832,12.581634521484375,11.336400985717773,9.937158584594727,11.021777153015137,9.948188781738281,12.35839557647705,12.886040687561035,9.261913299560547,9.89763355255127,9.462775230407715,9.968618392944336,12.362512588500977,10.358255386352539,8.327774047851562,11.927535057067871,10.00265884399414,12.349808692932129,10.623787879943848,10.678670883178711,11.847443580627441,9.692587852478027,9.262121200561523,10.958932876586914,12.889772415161133,10.948387145996094,11.634035110473633,12.080227851867676,10.378564834594727,10.096247673034668,11.42621898651123,8.7933931350708,11.231094360351562,8.999340057373047,11.527088165283203,12.19135570526123,7.8714375495910645,11.001914978027344,12.61523151397705,9.75306224822998,11.896245002746582,12.523083686828613,9.378335952758789,10.35689926147461,11.08590316772461,13.147154808044434,9.818265914916992,11.11213493347168,11.444499969482422,11.590463638305664,10.737903594970703,9.770731925964355,10.78557014465332,11.933521270751953,10.511313438415527,9.82026481628418,8.356404304504395,10.73415470123291,10.45881462097168,10.396477699279785,12.194050788879395,12.752180099487305,11.78748607635498,10.255226135253906,12.165645599365234,12.292634963989258,12.844697952270508,10.628177642822266,8.569395065307617,11.763611793518066,12.15890884399414,12.740022659301758,12.746966361999512,9.62110424041748,11.45886516571045,10.926743507385254,12.772931098937988,12.837591171264648,12.200713157653809,9.013678550720215,10.967081069946289,9.76983642578125,12.884422302246094,11.670434951782227,13.589189529418945,12.576118469238281,11.409375190734863,9.541470527648926,8.66622257232666,11.438347816467285,11.34601879119873,10.590363502502441,11.35364818572998,11.930695533752441,6.797292709350586,10.794591903686523,10.309884071350098,10.247868537902832,9.914916038513184,13.303763389587402,12.540189743041992,11.354313850402832,8.383038520812988,12.193739891052246,9.227989196777344,9.241358757019043,9.536521911621094,10.984012603759766,12.313669204711914,9.644474029541016,9.546388626098633,9.345878601074219,11.460395812988281,10.699294090270996,9.629450798034668,8.48170280456543,11.0349760055542,10.570517539978027,10.367918014526367,11.483748435974121,10.360127449035645,10.180638313293457,12.789478302001953,10.665085792541504,11.708563804626465,12.97873592376709,11.337641716003418,10.709122657775879,13.167486190795898,12.409531593322754,8.32143497467041,9.063648223876953,10.371134757995605,11.557292938232422,10.586905479431152,9.22173023223877,11.61303424835205,8.834911346435547,13.149587631225586,11.683677673339844,9.396690368652344,11.555591583251953,11.163105964660645,10.546306610107422,13.552511215209961,12.85168170928955,11.200281143188477,9.939804077148438,10.729233741760254,12.406366348266602,11.353873252868652,11.169713973999023,9.460506439208984,10.654193878173828,11.77177906036377,11.467199325561523,12.088428497314453,13.498458862304688,11.279095649719238,12.567978858947754,10.763456344604492,10.033524513244629,12.673567771911621,10.10020637512207,8.970285415649414,9.142569541931152,11.128690719604492,12.259600639343262,8.503023147583008,11.229401588439941,9.610413551330566,10.481382369995117,12.30782699584961,11.052600860595703,13.069833755493164,9.056635856628418,9.835722923278809,11.540289878845215,10.57509994506836,10.630483627319336,12.53976058959961,12.878406524658203,12.195188522338867,12.416590690612793,8.807579040527344,12.123668670654297,10.993809700012207,11.357285499572754,11.609417915344238,8.505378723144531,9.010087966918945,9.596749305725098,12.824377059936523,10.293586730957031,11.473138809204102,12.474746704101562,10.027557373046875,12.210655212402344,10.91020393371582,13.3267240524292,13.175515174865723,11.291525840759277,11.628263473510742,11.242234230041504,9.939929008483887,12.68603515625,9.105117797851562,12.55679702758789,10.582772254943848,11.557234764099121,12.54797649383545,11.068909645080566,12.6818208694458,11.577909469604492,11.18771743774414,13.261770248413086,11.224922180175781,11.866551399230957,9.587552070617676,10.551297187805176,11.903278350830078,12.07540225982666,10.768095970153809,12.016518592834473,8.86601448059082,12.697779655456543,12.205658912658691,10.315733909606934,11.997912406921387,12.761860847473145,11.485032081604004,12.877477645874023,10.77319622039795,9.71633529663086,12.789429664611816,10.579119682312012,12.169694900512695,11.369839668273926,12.047872543334961,11.594593048095703,11.944458961486816,11.05200481414795,9.081618309020996,12.835399627685547,12.314804077148438,9.093341827392578,11.577603340148926,9.215981483459473,11.374435424804688,12.266894340515137,11.160942077636719,8.430356979370117,12.54307746887207,11.239245414733887,10.539368629455566,11.465906143188477,7.657243728637695,12.263589859008789,9.044021606445312,11.270294189453125,9.69029426574707,12.90316104888916,11.74852466583252,8.697430610656738,10.244534492492676,13.194223403930664,8.26854419708252,12.263144493103027,12.15650463104248,10.97741985321045,11.557283401489258,12.207938194274902,9.880388259887695,13.715070724487305,8.669084548950195,11.517677307128906,12.063190460205078,10.891190528869629,8.157964706420898,13.416830062866211,12.172284126281738,9.77486515045166,12.416096687316895,8.317665100097656,12.529857635498047,10.84004020690918,8.873795509338379,12.759370803833008,10.572245597839355,11.383156776428223,11.408194541931152,12.709611892700195,13.176079750061035,7.645970344543457,10.45201587677002,11.888107299804688,8.155491828918457,10.934144973754883,12.047857284545898,11.55852222442627,10.887688636779785,10.773085594177246,10.705704689025879,9.601393699645996,10.455825805664062,9.596738815307617,10.703749656677246,11.308526992797852,9.27015209197998,10.01099681854248,12.847784996032715,12.372241020202637,10.8928804397583,11.546853065490723,9.608847618103027,10.049355506896973,10.234009742736816,12.77387809753418,7.639250755310059,9.369508743286133,11.111124992370605,10.788805961608887,9.64129638671875,10.971961975097656,13.150918960571289,10.595973014831543,9.54521369934082,12.856410026550293,10.756369590759277,11.570320129394531,10.633820533752441,10.61214542388916,11.196186065673828,11.974361419677734,11.977964401245117,9.437406539916992,11.594237327575684,11.464212417602539,10.141727447509766,11.141068458557129,9.565552711486816,12.504912376403809,10.439518928527832,8.916755676269531,12.542708396911621,8.100579261779785,13.614336013793945,9.942835807800293,12.88668441772461,12.289319038391113,10.191901206970215,11.653940200805664,12.273872375488281,9.7015380859375,11.526108741760254,11.894497871398926,9.942673683166504,12.299449920654297,11.311140060424805,12.881195068359375,8.470643043518066,12.338445663452148,11.502923011779785,10.742378234863281,12.947842597961426,11.659990310668945,12.496929168701172,8.366921424865723,9.1043119430542,12.329166412353516,11.74715518951416,11.585480690002441,10.612318992614746,11.377769470214844,11.649738311767578,11.907842636108398,13.692667007446289,11.34955883026123,12.52751636505127,8.80586051940918,12.406229019165039,12.7645902633667,12.445311546325684,11.75776195526123,11.090094566345215,11.42638111114502,10.89482593536377,10.761418342590332,11.150059700012207,10.034558296203613,10.338700294494629,11.178735733032227,12.835885047912598,11.53531551361084,12.381514549255371,11.54314136505127,11.90941047668457,9.358724594116211,10.740930557250977,11.939253807067871,11.149989128112793,11.659534454345703,10.353327751159668,11.331498146057129,10.64175796508789,12.81151008605957,8.662858009338379,10.857192993164062,10.787764549255371,8.953280448913574,12.585387229919434,7.643540382385254,10.447787284851074,10.80845832824707,12.5751371383667,9.113858222961426,9.70373821258545,12.737845420837402,11.416006088256836,12.489952087402344,8.133481979370117,10.278565406799316,11.977149963378906,9.470913887023926,10.66490364074707,10.00718879699707,9.176525115966797,11.815064430236816,8.752781867980957,11.483132362365723,11.89669418334961,12.553421020507812,13.118978500366211,9.782742500305176,10.74406623840332,12.168991088867188,11.756896018981934,12.940293312072754,11.56460189819336,8.64371109008789,11.702585220336914,11.278419494628906,10.558026313781738,11.820279121398926,10.459695816040039,11.502100944519043,11.530735969543457,10.50390911102295,12.36990737915039,12.139140129089355,11.131754875183105,10.48403263092041,13.041622161865234,11.032854080200195,11.734776496887207,10.479324340820312,9.945302963256836,11.825505256652832,11.676788330078125,9.411858558654785,12.318131446838379,11.462925910949707,12.87320613861084,13.661906242370605,12.474398612976074,10.583564758300781,11.804007530212402,11.14046573638916,11.841476440429688,11.781903266906738,9.183506965637207,11.8534574508667,11.49117660522461,11.63294506072998,11.224637985229492,12.764777183532715,12.330574035644531,11.128299713134766,12.93774127960205,10.32089614868164,12.478330612182617,11.827890396118164,11.804841041564941,11.44624137878418,11.820039749145508,10.238183975219727,7.784416675567627,12.978934288024902,12.451269149780273,9.322071075439453,11.821974754333496,10.541068077087402,11.961499214172363,8.238553047180176,12.170052528381348,12.736010551452637,12.764181137084961,11.65296745300293,12.675060272216797,11.974373817443848,10.117522239685059,9.6543607711792,12.757356643676758,12.056167602539062,9.412654876708984,11.711962699890137,9.174324989318848,10.823249816894531,9.90158462524414,10.913339614868164,8.113393783569336,9.993379592895508,12.194329261779785,13.33237361907959,11.182921409606934,12.732467651367188,13.046422004699707,9.75806713104248,8.685029029846191,11.561057090759277,10.053182601928711,11.998501777648926,13.20248031616211,12.104700088500977,10.906420707702637,11.117636680603027,10.446064949035645,9.715156555175781,12.530933380126953,12.754565238952637,11.461576461791992,11.491990089416504,9.396021842956543,8.587562561035156,12.263270378112793,11.42301082611084,13.491022109985352,10.364217758178711,11.991475105285645,12.44273853302002,11.70559310913086,10.35807991027832,9.996321678161621,9.326005935668945,12.807846069335938,13.570623397827148,11.12037467956543,10.716169357299805,10.50700569152832,10.22071361541748,12.695380210876465,11.297102928161621,9.085335731506348,12.861201286315918,11.170536994934082,10.041293144226074,10.805912017822266,11.3994722366333,9.783839225769043,11.411025047302246,10.608845710754395,9.42233657836914,11.715644836425781,11.849699020385742,12.349161148071289,8.975476264953613,10.749741554260254,9.420133590698242,10.120325088500977,12.375459671020508,10.686391830444336,11.969694137573242,11.85922622680664,12.443221092224121,9.37954330444336,13.088716506958008,10.281312942504883,12.127240180969238,11.985560417175293,12.442952156066895,12.119172096252441,9.390953063964844,10.271842956542969,10.183282852172852,12.758428573608398,9.88254451751709,11.704732894897461,11.48423957824707,9.189234733581543,11.172453880310059,11.48064136505127,10.315546035766602,12.061239242553711,11.559906005859375,11.86010456085205,11.664885520935059,12.843629837036133,9.624467849731445,12.458562850952148,12.246566772460938,11.838500022888184,10.999716758728027,9.12990951538086,11.840607643127441,11.96013069152832,11.915284156799316,12.08629035949707,10.903203964233398,8.849540710449219,11.167780876159668,10.531004905700684,12.467206954956055,10.145415306091309,9.549249649047852,6.795407295227051,12.180326461791992,12.817878723144531,10.757889747619629,11.568016052246094,11.672552108764648,7.97341775894165,10.022518157958984,9.927380561828613,10.70942211151123,11.941184043884277,12.794625282287598,12.879822731018066,12.178878784179688,11.878340721130371,10.52446460723877,10.071585655212402,10.56197738647461,11.002744674682617,12.245721817016602,10.67922592163086,12.140796661376953,12.277287483215332,11.456153869628906,12.081787109375,11.62030029296875,10.880305290222168,10.69717025756836,9.24069881439209,12.846972465515137,10.142430305480957,12.146927833557129,9.958215713500977,9.50244140625,10.51760196685791,10.551128387451172,11.390852928161621,11.688502311706543,11.491974830627441,12.741220474243164,12.295635223388672,12.218851089477539,8.685344696044922,13.047622680664062,11.958925247192383,9.383042335510254,11.218199729919434,11.51757526397705,9.624102592468262,13.13804817199707,12.326669692993164,12.624595642089844,12.161945343017578,9.43790054321289,11.324953079223633,8.503750801086426,11.257233619689941,10.58021354675293,9.62783145904541,9.703179359436035,11.924786567687988,8.906972885131836,9.254066467285156,12.899577140808105,11.277064323425293,11.152223587036133,12.76430606842041,10.148778915405273,12.378602027893066,11.560495376586914,12.818056106567383,12.91047191619873,8.343107223510742,10.998586654663086,11.666386604309082,10.141661643981934,11.702606201171875,10.432024955749512,10.108580589294434,9.184564590454102,13.428140640258789,11.512848854064941,12.044175148010254,11.576578140258789,12.28610610961914,11.385955810546875,12.952836036682129,12.021284103393555,11.287923812866211,12.099640846252441,12.082901000976562,12.028682708740234,12.383368492126465,8.773681640625,12.879551887512207,11.319714546203613,8.952622413635254,11.267468452453613,12.477086067199707,12.503398895263672,9.42236614227295,11.20785140991211,11.332497596740723,8.951685905456543,10.856745719909668,10.309080123901367,11.73598861694336,8.723488807678223,12.038217544555664,9.878068923950195,12.995759963989258,10.974395751953125,9.64431381225586,10.158196449279785,13.631218910217285,8.542502403259277,11.165329933166504,10.789525032043457,13.625770568847656,11.960083961486816,12.425254821777344,11.940552711486816,13.470690727233887,13.038926124572754,7.834033012390137,9.87307071685791,11.216033935546875,12.414644241333008,12.093451499938965,11.877202987670898,12.003032684326172,10.026086807250977,12.491192817687988,13.477025032043457,12.77425479888916,10.46001148223877,8.880654335021973,9.948962211608887,10.48256778717041,12.090274810791016,11.583839416503906,11.40567684173584,6.7874345779418945,9.088058471679688,11.758285522460938,11.31846809387207,11.73054313659668,12.907270431518555,12.770581245422363,11.114065170288086,11.227639198303223,10.648099899291992,12.066269874572754,13.3859224319458,12.3936767578125,11.536118507385254,12.09136962890625,7.971031665802002,10.897802352905273,9.173691749572754,11.609256744384766,12.269068717956543,9.517958641052246,11.856656074523926,11.710532188415527,12.882142066955566,12.293147087097168,9.226627349853516,11.812376022338867,9.573324203491211,11.98694896697998,11.025315284729004,12.620606422424316,12.66535758972168,11.626893997192383,11.87524127960205,9.740761756896973,11.66956615447998,9.677865982055664,13.708874702453613,13.700621604919434,10.8056058883667,12.719548225402832,11.30461597442627,9.15710163116455,11.5343599319458,9.219347953796387,11.886124610900879,11.679952621459961,12.069474220275879,12.361296653747559,10.542925834655762,10.790267944335938,12.395968437194824,10.26074504852295,12.145330429077148,11.74125862121582,12.40096664428711,10.930389404296875,8.466429710388184,11.920580863952637,12.118764877319336,13.445467948913574,10.998306274414062,11.986994743347168,12.903426170349121,11.159873962402344,12.18824291229248,9.372452735900879,12.712847709655762,13.078712463378906,10.631550788879395,9.169002532958984,8.790238380432129,11.811575889587402,12.28490924835205,9.769627571105957,12.567846298217773,10.349342346191406,11.191412925720215,11.537104606628418,10.127388000488281,10.513239860534668,8.915346145629883,11.823579788208008,11.909771919250488,11.520251274108887,13.776591300964355,12.594861030578613,6.803589344024658,10.791962623596191,11.956903457641602,7.651430606842041,12.864334106445312,11.632024765014648,10.91291332244873,11.907623291015625,10.763083457946777,11.546504974365234,13.262833595275879,12.147619247436523,11.872856140136719,12.41122055053711,12.41154670715332,13.054023742675781,10.615890502929688,9.164365768432617],\"y\":[1.976219654083252,1.265775442123413,1.0926563739776611,-0.5182633996009827,2.4570963382720947,0.48775091767311096,-0.910475492477417,-0.5370233654975891,1.2164555788040161,1.3136889934539795,-0.12520691752433777,1.813036561012268,1.1669845581054688,1.774245023727417,-0.3897693157196045,1.3097110986709595,3.413337469100952,1.671027660369873,0.8366751074790955,3.2766458988189697,0.3775566816329956,0.44715550541877747,2.3352620601654053,3.230710506439209,2.2592904567718506,0.6792904138565063,0.25117701292037964,0.6122933626174927,1.182349681854248,0.9189866781234741,3.0709445476531982,0.9211485385894775,3.500269889831543,3.7989087104797363,1.823943853378296,1.6083993911743164,-0.23820368945598602,2.4324207305908203,0.19927099347114563,3.7869391441345215,2.054600715637207,0.9205240607261658,3.078073263168335,0.10503587871789932,0.8371968865394592,0.3198699951171875,-0.16870951652526855,2.7976632118225098,2.6678123474121094,-0.570141077041626,-0.9106079339981079,3.7905776500701904,2.7144033908843994,0.9463844299316406,1.3737200498580933,0.33007562160491943,-0.9850686192512512,1.1608271598815918,1.0532095432281494,2.060023546218872,1.5419471263885498,1.308414101600647,-1.1935114860534668,-0.17752167582511902,-0.4857664108276367,3.832371234893799,1.3047598600387573,3.552154541015625,1.942522644996643,-0.909134566783905,2.3067994117736816,0.9871791005134583,2.9792444705963135,2.830186128616333,0.7007690072059631,3.5236191749572754,3.808678388595581,0.5856729745864868,0.5755625367164612,-0.488295316696167,0.6351603269577026,0.5080202221870422,1.0401160717010498,2.3617639541625977,1.8510946035385132,0.9317828416824341,3.2759201526641846,2.4336321353912354,-1.299113392829895,3.8617777824401855,3.202488422393799,1.037284016609192,-0.5317288637161255,-0.5715892314910889,1.2100167274475098,1.357236623764038,3.533149480819702,3.0465500354766846,0.1627165526151657,0.7375492453575134,0.9511770009994507,0.07118048518896103,-0.3919447958469391,1.378462314605713,-0.3244394361972809,0.7082329988479614,2.143453359603882,1.1407673358917236,0.9031526446342468,2.8212311267852783,1.1145505905151367,1.4429399967193604,2.296320676803589,-0.001279126270674169,-0.10712704062461853,3.0526630878448486,0.7859264612197876,-0.6041418313980103,1.40556800365448,-0.5446928143501282,1.2216320037841797,3.4056971073150635,3.86862850189209,1.1494468450546265,0.1541697382926941,-0.2646180987358093,2.934096097946167,1.085005283355713,0.1298372447490692,1.4595853090286255,0.5229455232620239,1.7139097452163696,3.179950475692749,0.8122704029083252,2.313836097717285,0.9151896834373474,0.0694771260023117,2.251661777496338,2.7393033504486084,1.8044404983520508,2.4212090969085693,1.369358777999878,1.4411309957504272,1.778269648551941,1.9663785696029663,-0.2085247039794922,1.8884222507476807,0.01605393923819065,0.9991214275360107,2.1229145526885986,-0.17486093938350677,1.2444441318511963,1.5270284414291382,1.034622311592102,0.5874497890472412,1.0864499807357788,0.7937063574790955,0.2653231918811798,0.6648417711257935,2.1787331104278564,-1.4033302068710327,0.993794858455658,3.9591336250305176,2.2620389461517334,2.2779126167297363,1.026957631111145,1.6402356624603271,1.106472134590149,3.609027624130249,4.001310348510742,2.4320061206817627,1.3439748287200928,1.444767951965332,0.8551923036575317,0.15037234127521515,2.425750494003296,2.3774797916412354,2.089541435241699,0.14875546097755432,1.2632057666778564,1.3453816175460815,3.9026010036468506,0.47671282291412354,1.526449203491211,1.530004620552063,0.44732722640037537,1.5337772369384766,-0.5403087735176086,-1.0967426300048828,0.833954393863678,0.7645677924156189,2.70750093460083,1.2675151824951172,0.1613716185092926,3.286445379257202,0.9860133528709412,3.4668221473693848,1.4706974029541016,-0.06515169888734818,3.346012830734253,3.19698166847229,1.0387732982635498,-0.47503626346588135,1.1350661516189575,-0.12550556659698486,0.48462581634521484,3.611987352371216,0.44945693016052246,-0.9105373620986938,0.9302946925163269,2.0090830326080322,0.1268618106842041,-0.5888694524765015,1.5191290378570557,2.7231905460357666,3.63657283782959,0.20960384607315063,0.04279949516057968,2.0899455547332764,2.395681142807007,-0.3550689220428467,1.1873329877853394,1.6772342920303345,-0.551196813583374,1.0045689344406128,3.5376405715942383,-0.2728564739227295,1.6083232164382935,3.451906681060791,0.4097013771533966,0.28619876503944397,2.9995458126068115,1.430558204650879,2.9486751556396484,1.4180095195770264,3.394707679748535,-1.0172475576400757,1.6186503171920776,2.2325010299682617,3.3960022926330566,2.3339428901672363,2.508234739303589,1.917661428451538,2.007206678390503,0.7356185913085938,1.3998340368270874,1.426846981048584,-0.10650590807199478,3.196619987487793,1.5687898397445679,3.3810524940490723,-0.20458725094795227,1.947638988494873,-0.17957328259944916,0.6638663411140442,1.4990065097808838,1.0839821100234985,-0.13390646874904633,1.798355221748352,1.460438847541809,2.6365201473236084,2.2331058979034424,0.5038972496986389,1.672692894935608,3.618595838546753,2.4914488792419434,-0.7637554407119751,2.602285861968994,1.6152315139770508,2.3165502548217773,4.036905288696289,1.7626341581344604,-0.30745241045951843,1.3061516284942627,3.0544943809509277,2.2432503700256348,0.5260573625564575,0.26869526505470276,0.03409036993980408,-0.5223769545555115,0.39339277148246765,2.0766706466674805,-0.38802358508110046,3.801676034927368,2.3770365715026855,3.5236270427703857,3.0124449729919434,3.137105703353882,3.03592848777771,1.0055789947509766,2.3000667095184326,3.485448122024536,2.8649325370788574,3.2783849239349365,2.067582130432129,1.7887386083602905,1.117841124534607,1.9235390424728394,-0.5475263595581055,2.5567400455474854,-0.42588913440704346,1.6459540128707886,3.0523805618286133,3.548125743865967,0.9675278663635254,1.0511583089828491,1.8744837045669556,-0.2662609815597534,2.020230531692505,2.395968198776245,1.5245238542556763,2.208904504776001,-0.23743867874145508,2.1941235065460205,3.066039800643921,0.8780515789985657,-0.3361736238002777,0.12655974924564362,0.5816671252250671,2.5916388034820557,1.8386311531066895,0.9971460103988647,-0.09463711082935333,1.4201574325561523,3.414994478225708,-0.5812013745307922,1.0859824419021606,3.207399845123291,1.0073139667510986,1.059942603111267,0.9441660642623901,2.421081066131592,-0.9699038863182068,-0.7918293476104736,3.2947211265563965,3.453516721725464,2.6633572578430176,1.12492835521698,0.9515612125396729,1.8676871061325073,2.2264864444732666,0.4588735103607178,-0.7277568578720093,-1.0359772443771362,2.857710599899292,2.6983368396759033,1.5439820289611816,0.8810650110244751,3.6186933517456055,3.788325071334839,2.0987229347229004,3.1248533725738525,-0.6990334391593933,3.2795536518096924,0.4755188822746277,2.3451409339904785,1.9918140172958374,2.239830255508423,2.980741500854492,3.4494681358337402,2.7023935317993164,0.8716931343078613,1.143310546875,0.12861472368240356,1.2377668619155884,-0.3922814130783081,3.3432180881500244,1.7890307903289795,0.9444081783294678,2.8529300689697266,0.18657220900058746,2.6263394355773926,0.48836591839790344,0.770980954170227,0.9084759950637817,2.9118480682373047,0.899668276309967,0.3940073549747467,0.8528211116790771,1.6868844032287598,0.24871914088726044,2.4808976650238037,2.190305709838867,1.4015684127807617,-0.11070117354393005,1.4967073202133179,0.9283192753791809,0.4603424668312073,1.1601427793502808,1.9638222455978394,-1.3094513416290283,1.8755643367767334,-0.034235879778862,2.5533201694488525,2.5379700660705566,3.77651309967041,2.733030319213867,0.3953371047973633,2.7038984298706055,1.0902845859527588,3.95662522315979,3.4667394161224365,1.4680428504943848,-0.2102063000202179,1.9835692644119263,0.25430288910865784,2.2354414463043213,0.19437259435653687,1.457194447517395,1.478986144065857,0.024254370480775833,0.9681276082992554,-1.2682996988296509,1.4637945890426636,0.9524343013763428,3.2517035007476807,1.0663663148880005,1.047045350074768,2.5798704624176025,1.9295272827148438,-0.4900413751602173,1.2294294834136963,3.744098663330078,1.1958093643188477,3.4060301780700684,1.4086087942123413,3.195308208465576,-0.21053600311279297,1.9594486951828003,1.3294121026992798,0.7901973128318787,-1.0279371738433838,0.18727345764636993,2.7890584468841553,1.1563249826431274,-0.1735643744468689,2.2202253341674805,0.9077820777893066,2.668337821960449,2.603579521179199,0.24776341021060944,1.158795714378357,1.6285470724105835,1.96036958694458,3.842803716659546,-0.05351594090461731,1.0603022575378418,3.5160486698150635,3.203173875808716,-0.22633570432662964,1.7337943315505981,1.1724951267242432,0.17315110564231873,0.7105343341827393,2.4431610107421875,3.8104262351989746,0.10712434351444244,0.713238537311554,3.1322901248931885,3.2645468711853027,0.534716784954071,0.5656581521034241,3.5241639614105225,2.5362460613250732,-0.7327529191970825,-0.8334300518035889,2.0500335693359375,0.9878270626068115,2.0549156665802,3.280917167663574,0.543793261051178,0.40602004528045654,1.2652292251586914,1.6847025156021118,0.6613606810569763,0.48743921518325806,1.5848220586776733,0.15714702010154724,3.097687244415283,3.4039037227630615,3.8253872394561768,-0.3216516077518463,0.36484676599502563,3.5613582134246826,0.849107563495636,2.804208278656006,3.8222780227661133,1.8004229068756104,1.4634736776351929,3.1937408447265625,0.8675617575645447,3.476331949234009,0.3727441728115082,3.1206748485565186,1.5812299251556396,-0.6251404285430908,3.3036246299743652,3.7486493587493896,1.7302109003067017,2.3743913173675537,0.9696566462516785,0.4081820845603943,0.6557876467704773,2.6788806915283203,3.448852300643921,1.163936972618103,3.5292248725891113,2.8257150650024414,3.515986680984497,1.9399545192718506,0.4077995717525482,0.766963541507721,-0.4135100543498993,0.42341694235801697,2.8088972568511963,-0.6076659560203552,0.1522863209247589,3.2291979789733887,1.588698387145996,1.8325613737106323,0.9248793125152588,1.3020033836364746,3.420180559158325,1.643860101699829,1.1540601253509521,0.21492479741573334,0.1739095002412796,1.5416576862335205,3.289651393890381,-0.5035665035247803,-0.46198034286499023,3.0262765884399414,3.655320405960083,1.8784151077270508,0.3909582197666168,0.8705004453659058,1.1364428997039795,1.208755612373352,0.26025456190109253,0.44635772705078125,0.12183551490306854,-0.15554407238960266,-0.25469839572906494,-0.45932525396347046,0.3801633417606354,2.1852316856384277,3.0103812217712402,0.16040968894958496,0.17387855052947998,2.1033482551574707,3.5950064659118652,1.6760215759277344,3.2061989307403564,0.3962271809577942,2.2076709270477295,-0.8994259834289551,2.514460802078247,0.979424774646759,3.5055930614471436,0.26043587923049927,0.08888838440179825,0.9722239375114441,1.9090890884399414,1.4592763185501099,0.7288630604743958,0.5196076035499573,2.52851939201355,0.3626737594604492,0.9660953879356384,1.4570801258087158,1.5067415237426758,1.0723236799240112,1.9673024415969849,0.5451234579086304,2.6414546966552734,0.7439343333244324,3.4168541431427,1.108274221420288,0.9759180545806885,3.0499231815338135,0.3342808485031128,1.1762615442276,1.2191663980484009,1.9902498722076416,2.7178072929382324,2.1931447982788086,1.4139740467071533,3.4444408416748047,-0.2844769358634949,2.468306541442871,2.982534408569336,1.1627116203308105,2.345379114151001,-0.26387879252433777,2.15932035446167,-0.09391316771507263,2.315988540649414,3.9019370079040527,1.4993696212768555,0.8209245204925537,0.1551581770181656,2.0490360260009766,1.3861424922943115,1.0227948427200317,-0.12519274652004242,3.113816976547241,1.2089561223983765,-0.26088747382164,1.8862637281417847,3.144066333770752,-1.0393073558807373,2.0746145248413086,1.443308711051941,1.180871844291687,-0.6809756755828857,1.017892599105835,1.8907074928283691,-1.2943123579025269,0.1569855809211731,0.6024736166000366,0.8391635417938232,2.21470046043396,-1.1607123613357544,1.8066606521606445,2.297105312347412,2.370729446411133,1.7525073289871216,0.9918195605278015,0.6040194034576416,2.8629562854766846,3.4195144176483154,1.8813725709915161,3.7637126445770264,0.20570334792137146,2.110116958618164,1.3198965787887573,1.0308681726455688,1.5510575771331787,1.0269742012023926,3.243104934692383,0.5696408748626709,-1.0560033321380615,3.80950927734375,-0.9885093569755554,2.1285574436187744,0.27239152789115906,-0.9925388693809509,2.664095401763916,0.5445898771286011,3.4431591033935547,2.9532716274261475,1.4311859607696533,2.9324100017547607,3.054269313812256,0.8955436944961548,0.025119181722402573,3.7716782093048096,0.6967297196388245,2.6491522789001465,2.385422945022583,3.098644495010376,3.803661823272705,1.4175082445144653,2.0697543621063232,3.0386083126068115,3.8905580043792725,1.805167555809021,3.370250940322876,0.6452329754829407,1.198987364768982,4.435891628265381,3.9789042472839355,0.2224683314561844,0.784505307674408,3.900918483734131,-0.08793401718139648,-0.20969590544700623,0.9383320212364197,2.9824304580688477,0.4146740138530731,3.451068878173828,0.58146733045578,2.629016876220703,2.5896897315979004,0.8220140933990479,2.19604229927063,2.3683815002441406,1.0513523817062378,-0.5379165410995483,2.0229125022888184,2.577913522720337,1.3024059534072876,2.953939914703369,1.1526473760604858,-1.245646595954895,0.6586165428161621,1.9431781768798828,0.795759916305542,0.4541209042072296,1.3323132991790771,0.9036451578140259,1.0493559837341309,2.936338424682617,1.658136010169983,2.7779719829559326,0.217825248837471,-0.08116167783737183,0.8917245864868164,1.92644202709198,1.5383085012435913,3.03080153465271,-0.5870935916900635,1.1749986410140991,1.7433806657791138,3.2300631999969482,1.3536816835403442,1.4494167566299438,2.132977247238159,1.142106533050537,1.0964027643203735,-1.2843916416168213,2.0309720039367676,0.38181358575820923,0.4180848002433777,1.397927165031433,1.0205312967300415,-1.0413587093353271,-0.3866738975048065,-0.9114582538604736,2.2314321994781494,-0.9689415693283081,0.8953730463981628,-0.7223781943321228,3.3937878608703613,-0.1200912594795227,1.4780992269515991,-1.0540677309036255,1.243822693824768,1.1921714544296265,-0.9881787300109863,2.7142724990844727,1.035949468612671,1.7600966691970825,3.096761703491211,1.0765949487686157,-0.3055479824542999,0.8458605408668518,1.8884222507476807,0.7532628178596497,3.692111015319824,2.047429084777832,1.5398664474487305,2.6263246536254883,3.3768866062164307,1.3354458808898926,2.8829421997070312,3.073280096054077,2.618222951889038,2.2130420207977295,1.131805658340454,1.4583265781402588,-0.6291894912719727,3.3120579719543457,3.235522985458374,2.4912800788879395,0.8031163215637207,2.785531520843506,1.2172937393188477,2.5934207439422607,0.44533485174179077,3.7607927322387695,0.8943837881088257,2.2763075828552246,0.15780554711818695,2.040842056274414,0.27494242787361145,3.4471356868743896,2.545928955078125,3.3168694972991943,3.057359218597412,2.0106849670410156,3.7272539138793945,-0.2229020744562149,-0.3912656009197235,2.5650482177734375,0.5276594758033752,1.9719535112380981,3.524745225906372,1.5316689014434814,0.3282936215400696,2.1106743812561035,2.4451394081115723,1.7900829315185547,2.8499183654785156,2.917431354522705,-0.6730669736862183,2.2880237102508545,2.3319878578186035,0.7816553711891174,3.079967975616455,0.3783358633518219,3.771744728088379,3.4605367183685303,3.4098503589630127,2.232668399810791,2.7577970027923584,0.8026202917098999,0.8486886024475098,1.336401104927063,1.9623465538024902,2.0950241088867188,2.3696439266204834,2.694972276687622,0.7788678407669067,1.5454350709915161,0.642559289932251,1.9104548692703247,2.817577838897705,1.2944279909133911,2.2089552879333496,2.9942898750305176,0.6504705548286438,2.6843504905700684,1.2824846506118774,-0.555378794670105,1.9917773008346558,2.4853670597076416,2.2720730304718018,0.05783434212207794,1.741772174835205,3.9805495738983154,1.6283323764801025,3.9042468070983887,-0.08662469685077667,1.0114973783493042,-0.2651676535606384,2.276035785675049,-0.6100764274597168,2.0067296028137207,1.3208794593811035,1.0322285890579224,1.9896197319030762,2.3109211921691895,2.2940375804901123,0.7046573758125305,2.1666882038116455,0.8269322514533997,1.038419246673584,3.1033222675323486,2.011228084564209,0.4995725154876709,3.0119717121124268,1.7737351655960083,1.1829763650894165,3.768042802810669,2.453611135482788,-1.2905018329620361,0.4045698344707489,-0.7276555299758911,1.108733057975769,0.7043386101722717,1.7317860126495361,3.1328790187835693,1.5019123554229736,0.00757896201685071,1.7896612882614136,4.704597473144531,-1.2845878601074219,1.471731185913086,3.7090561389923096,-0.9188922643661499,0.25417712330818176,0.40908655524253845,-0.6070359349250793,3.279542922973633,2.3822507858276367,-1.0290439128875732,-1.0308316946029663,1.6611809730529785,0.5246748328208923,1.5900543928146362,1.5552963018417358,1.9574549198150635,0.3487606346607208,1.906638503074646,-0.15992236137390137,-0.1188686341047287,1.874509334564209,2.240257501602173,3.598365068435669,2.4335744380950928,3.23817777633667,1.554663896560669,2.292228937149048,1.6409223079681396,1.495506763458252,3.2188000679016113,3.7398290634155273,2.8911819458007812,3.769780397415161,2.451735019683838,2.5036814212799072,3.150447130203247,1.2291960716247559,0.9691900610923767,3.689431667327881,-0.07314413040876389,0.73487389087677,0.839801013469696,1.4524890184402466,2.1931369304656982,1.3259851932525635,3.0612916946411133,0.2924751043319702,0.6916547417640686,2.3885679244995117,0.7535249590873718,1.8109625577926636,3.2876040935516357,2.6223843097686768,2.415797472000122,2.450838565826416,1.9013489484786987,2.8400003910064697,3.637033224105835,1.2191064357757568,0.49888309836387634,0.527450680732727,3.3894202709198,2.8021371364593506,1.5861856937408447,-0.5647964477539062,1.773905634880066,2.8972055912017822,0.4014616310596466,3.552330493927002,3.5676872730255127,0.44267264008522034,2.05513334274292,2.6361255645751953,2.9887442588806152,0.289135605096817,3.268169641494751,3.1244053840637207,0.8573437333106995,-0.19672121107578278,-0.7705172300338745,1.6629849672317505,3.544947862625122,-0.6638178825378418,3.600919008255005,-0.5163041353225708,-0.025210803374648094,1.4350130558013916,1.8289581537246704,0.6889894008636475,1.6060850620269775,-0.9072462916374207,2.4202475547790527,1.693398356437683,0.15449632704257965,3.578507661819458,2.314469575881958,4.283806324005127,2.3266613483428955,0.48093342781066895,3.2087161540985107,-0.08493365347385406,1.7356622219085693,1.580642580986023,1.29006028175354,0.9935421347618103,2.0631296634674072,2.585738182067871,1.8831286430358887],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"13_reasoning_pangu_symbolic\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"13_reasoning_pangu_symbolic\"],\"x\":[9.99891185760498,9.666812896728516,9.87675666809082,9.759414672851562,9.77737045288086,9.712926864624023,9.860082626342773,10.150372505187988,9.757026672363281,9.756099700927734,9.88352108001709,9.667094230651855,9.834576606750488,9.662117958068848,9.90071964263916,9.823662757873535,9.885151863098145,9.897980690002441,9.599701881408691,9.828813552856445,10.166223526000977,9.664942741394043,9.68136215209961,9.909587860107422,9.613624572753906,9.683416366577148,9.677172660827637,9.895627975463867,9.831110954284668,9.791001319885254,9.888591766357422,10.030932426452637,10.003292083740234,9.82230281829834],\"y\":[-1.2806227207183838,-1.4134907722473145,-1.3984967470169067,-0.8591680526733398,-1.1667323112487793,-0.9986860752105713,-1.226072072982788,-1.2499401569366455,-1.3567789793014526,-1.3018178939819336,-1.279323697090149,-1.3926376104354858,-0.8900179862976074,-1.3667707443237305,-1.3356115818023682,-1.4169893264770508,-1.3518718481063843,-1.17157781124115,-1.1077362298965454,-1.3785203695297241,-1.242591381072998,-1.3728218078613281,-1.36669921875,-1.1680376529693604,-1.3303998708724976,-0.8987433314323425,-1.368369460105896,-1.1938246488571167,-1.3907575607299805,-1.302925705909729,-1.1836531162261963,-1.1196755170822144,-1.154442310333252,-1.243509292602539],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"14_speech_asr_s2st\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"14_speech_asr_s2st\"],\"x\":[13.670157432556152,13.798901557922363,13.369604110717773,13.828919410705566,13.80864143371582,13.781868934631348,13.87149715423584,13.169575691223145,13.689194679260254,13.725122451782227,13.867948532104492,13.486695289611816,13.795697212219238,13.88539981842041,13.794876098632812,13.703969955444336,13.475425720214844,13.485220909118652,13.642592430114746,13.45029067993164,13.17920970916748,13.825347900390625,13.710762977600098,13.833584785461426,13.823281288146973,13.777862548828125,13.709671974182129,13.589035034179688,13.624615669250488,13.818294525146484,13.520774841308594,13.499052047729492,13.662908554077148],\"y\":[1.930212140083313,1.9589751958847046,1.9856090545654297,2.118469476699829,1.8433870077133179,2.083873748779297,1.9659463167190552,1.6238290071487427,1.9088102579116821,1.984571933746338,1.8491621017456055,1.8348524570465088,1.9095227718353271,2.1351234912872314,2.0434975624084473,1.9872238636016846,1.9833941459655762,1.972872018814087,1.8849225044250488,1.9644986391067505,1.6182628870010376,2.087661027908325,1.9592905044555664,1.9400484561920166,1.8442121744155884,2.052821636199951,2.1948325634002686,1.9715791940689087,1.9485830068588257,1.9467763900756836,2.033796548843384,2.05224347114563,1.9568393230438232],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"15_classification_oov_slash\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"15_classification_oov_slash\"],\"x\":[11.668291091918945,11.893047332763672,11.399813652038574,11.399984359741211,11.689655303955078,11.275130271911621,11.221017837524414,12.198081016540527,12.027531623840332,11.27303695678711,11.897500038146973,11.873612403869629,11.806347846984863,11.46186637878418,11.398143768310547,11.580697059631348,11.322710990905762,11.420403480529785,11.564152717590332,11.334028244018555,11.482306480407715,11.22659969329834,11.433297157287598,11.450419425964355,11.489837646484375,11.502764701843262,11.421748161315918,11.442119598388672,12.117900848388672,11.278861999511719,11.551697731018066],\"y\":[1.6213551759719849,1.7356750965118408,1.8189188241958618,2.011199474334717,1.6284370422363281,1.6911828517913818,1.434216856956482,1.9124113321304321,1.708951473236084,1.345273733139038,1.9241002798080444,1.706599235534668,1.5955106019973755,1.6075619459152222,1.8218833208084106,1.9240765571594238,1.5612646341323853,1.4347044229507446,1.8333743810653687,1.4203897714614868,1.508184552192688,1.6106964349746704,1.7237390279769897,1.6263391971588135,1.7565888166427612,1.8806627988815308,1.4971145391464233,1.4235330820083618,2.043459415435791,1.6557258367538452,1.682104468345642],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"16_event_events_eae\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"16_event_events_eae\"],\"x\":[11.366277694702148,11.415215492248535,11.413336753845215,11.3681058883667,11.386617660522461,11.464384078979492,11.333633422851562,11.434808731079102,11.411873817443848,11.339577674865723,11.400299072265625,11.394913673400879,11.41909122467041,11.410325050354004,11.436091423034668,11.440723419189453,11.421466827392578,11.424187660217285,10.752309799194336,11.30122184753418,11.429636001586914,11.37482738494873,11.405726432800293,11.432455062866211,11.494464874267578,11.39643669128418,11.412282943725586,11.394112586975098,11.40078353881836,11.381903648376465],\"y\":[-1.7047370672225952,-1.7189042568206787,-1.7727406024932861,-1.7914379835128784,-1.7604166269302368,-1.5030242204666138,-1.8196848630905151,-1.7163331508636475,-1.6494393348693848,-1.6843397617340088,-1.7724716663360596,-1.7692655324935913,-1.7378288507461548,-1.7441824674606323,-1.464735746383667,-1.7443996667861938,-1.7241114377975464,-1.6847602128982544,0.36447063088417053,-1.677884578704834,-1.6699968576431274,-1.7729989290237427,-1.7663508653640747,-1.7174915075302124,-1.603288173675537,-1.7217741012573242,-1.7533800601959229,-1.768631100654602,-1.7753701210021973,-1.6422590017318726],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"17_explanations_explanation_concepts\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"17_explanations_explanation_concepts\"],\"x\":[9.92600154876709,9.90449047088623,9.936745643615723,9.92866325378418,9.815776824951172,9.909724235534668,9.910490989685059,9.891134262084961,9.903509140014648,9.932821273803711,9.916892051696777,9.828373908996582,9.930082321166992,9.88005256652832,9.91939926147461,10.002853393554688,9.763079643249512,9.934072494506836,9.936637878417969,9.912662506103516,9.736724853515625,9.912676811218262,9.76375961303711,9.90211296081543,9.953424453735352,9.921619415283203,9.907418251037598,9.980151176452637,9.924909591674805,9.899526596069336],\"y\":[-2.003943920135498,-2.018934726715088,-2.0210397243499756,-1.9936052560806274,-1.9008338451385498,-1.989972710609436,-2.0100209712982178,-1.9968105554580688,-2.0147523880004883,-2.0181238651275635,-2.0108845233917236,-1.9405567646026611,-1.9686578512191772,-1.9617516994476318,-2.0073821544647217,-1.730658769607544,-1.9098670482635498,-1.9983524084091187,-2.0032131671905518,-2.021005392074585,-1.78288996219635,-1.9908539056777954,-1.9123069047927856,-1.9989206790924072,-1.8872182369232178,-2.0079305171966553,-1.9907784461975098,-1.9225029945373535,-2.0194551944732666,-1.9666630029678345],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"18_prompts_prompt_continuous\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"18_prompts_prompt_continuous\"],\"x\":[9.238378524780273,9.30147647857666,9.232583999633789,9.182598114013672,9.238131523132324,9.218341827392578,9.234349250793457,9.302995681762695,9.232303619384766,9.124175071716309,9.416687965393066,9.293940544128418,9.349409103393555,9.29613208770752,9.22963809967041,9.208431243896484,9.365690231323242,9.037209510803223,9.345200538635254,9.246965408325195,9.236005783081055,9.268945693969727,9.25452709197998],\"y\":[0.11233166605234146,0.2616926431655884,0.13499541580677032,0.13999022543430328,0.13154223561286926,0.03455710411071777,0.1343674212694168,0.09137420356273651,0.0880853608250618,-0.03193724900484085,0.282012939453125,0.19786401093006134,0.27887430787086487,0.23954655230045319,0.1252308189868927,0.2048257291316986,0.21515387296676636,0.2964838743209839,0.28831443190574646,0.10827220976352692,0.010295077227056026,0.17448420822620392,0.15992532670497894],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"19_zeroshot_fewshot_verbalizers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"19_zeroshot_fewshot_verbalizers\"],\"x\":[11.956093788146973,12.490596771240234,12.429107666015625,12.604439735412598,12.537409782409668,12.447066307067871,12.514530181884766,12.494388580322266,12.444210052490234,12.318142890930176,12.335335731506348,12.499151229858398,12.503713607788086,12.477373123168945,12.43051815032959,12.483855247497559,12.453995704650879,12.551338195800781,12.408796310424805,12.414511680603027,12.439727783203125],\"y\":[-0.01690675877034664,-0.3107600808143616,-0.2527309060096741,-0.2602718472480774,-0.33801159262657166,-0.32423314452171326,-0.19620446860790253,-0.286507248878479,-0.2868269979953766,2.4987032413482666,-0.19626036286354065,-0.2882573902606964,2.8162295818328857,-0.27515092492103577,-0.28718024492263794,-0.29625701904296875,-0.3112342059612274,-0.32359012961387634,-0.29335451126098633,-0.24639558792114258,0.026239966973662376],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"20_chatgpt_chatgpts_rewriting\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"20_chatgpt_chatgpts_rewriting\"],\"x\":[8.408228874206543,8.558862686157227,8.468222618103027,8.394969940185547,8.488740921020508,8.462080955505371,8.458389282226562,8.648653030395508,8.45824146270752,8.496983528137207,8.497143745422363,8.60595989227295,8.652069091796875,8.46937370300293,8.726569175720215,8.704874992370605,8.474954605102539,8.499213218688965,8.526307106018066],\"y\":[0.8488311767578125,0.8990485668182373,0.930065929889679,0.9668167233467102,0.8830121159553528,0.8673952221870422,0.9546473622322083,0.9203405976295471,0.9518395662307739,0.8990188241004944,0.8826806545257568,1.001754879951477,0.8333197236061096,0.94631427526474,0.7836666703224182,0.7688788771629333,0.916922390460968,0.7554416656494141,0.88944411277771],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"21_tables_table_tableqa\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"21_tables_table_tableqa\"],\"x\":[10.386235237121582,10.229713439941406,10.389017105102539,10.301673889160156,10.361222267150879,10.3976469039917,10.311823844909668,10.32546329498291,10.373322486877441,10.308487892150879,10.342009544372559,10.355380058288574,10.305009841918945,10.051471710205078,10.35975456237793,10.3198823928833],\"y\":[-0.7522066235542297,-0.7360655665397644,-0.7566789388656616,-0.7287562489509583,-0.7565518617630005,-0.6721810698509216,-0.7266584038734436,-0.7462540864944458,-0.6999889016151428,-0.7211228609085083,-0.7179433107376099,-0.7139708399772644,-0.7397772073745728,-0.6260511875152588,-0.7241697311401367,-0.7212250828742981],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"22_oosf_instances_proxy\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"22_oosf_instances_proxy\"],\"x\":[11.03643798828125,11.07364559173584,11.248010635375977,11.170310974121094,10.949195861816406,11.285311698913574,10.957159996032715,11.096034049987793,11.2238130569458,11.023275375366211,11.042583465576172,11.16788101196289,11.188385009765625,11.112465858459473],\"y\":[3.106004476547241,3.0234382152557373,3.3541853427886963,3.115736246109009,3.0922281742095947,3.283050775527954,3.0349786281585693,3.2528135776519775,3.086516857147217,3.087618112564087,3.101942300796509,3.0745949745178223,3.133884906768799,3.1343841552734375],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"23_figurative_metaphors_sociocultural\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"23_figurative_metaphors_sociocultural\"],\"x\":[12.890239715576172,12.9169282913208,12.898111343383789,12.95676326751709,12.863883018493652,12.8355073928833,12.85640811920166,13.136683464050293,12.889006614685059,7.990469932556152,12.911820411682129,12.735673904418945,12.899605751037598,12.521623611450195],\"y\":[0.026861172169446945,-0.11374158412218094,-0.0861361101269722,-0.08715442568063736,0.14325851202011108,-0.09389468282461166,-0.06494750827550888,-0.21590127050876617,-0.029119817540049553,0.18973292410373688,-0.05029120296239853,0.2098502218723297,-0.02140832133591175,-0.014837851747870445],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"24_sarcasm_irony_humor\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"24_sarcasm_irony_humor\"],\"x\":[7.443446159362793,7.453991889953613,7.434319496154785,7.457515239715576,7.445913314819336,7.432080268859863,7.492039680480957,7.444361686706543,7.45388650894165,7.470397472381592,7.456826210021973,7.457319259643555,7.452033042907715,7.453394889831543],\"y\":[3.3650591373443604,3.3583810329437256,3.3732411861419678,3.3514108657836914,3.3623621463775635,3.374535083770752,3.3646886348724365,3.3602709770202637,3.329127788543701,3.332568883895874,3.3524117469787598,3.345237970352173,3.361050605773926,3.3561806678771973],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"25_dense_retrieval_retrievers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"25_dense_retrieval_retrievers\"],\"x\":[11.089601516723633,11.02492904663086,10.883186340332031,10.981522560119629,10.981151580810547,10.95821762084961,10.9121675491333,10.982845306396484,11.157200813293457,10.896283149719238,10.92767333984375,10.96117877960205,10.979663848876953],\"y\":[2.4378020763397217,2.470427989959717,2.49906587600708,2.4681739807128906,2.4417030811309814,2.4632935523986816,2.5861077308654785,2.4596455097198486,2.333122968673706,2.6070048809051514,2.5345096588134766,2.460688352584839,2.480128526687622],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"26_argumentative_environmental_firms\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"26_argumentative_environmental_firms\"],\"x\":[9.245034217834473,9.240274429321289,9.255699157714844,9.306416511535645,9.330011367797852,9.5535249710083,9.301241874694824,9.279705047607422,9.539828300476074,9.543222427368164,9.282510757446289,9.352497100830078],\"y\":[2.6040866374969482,2.5882256031036377,2.6099462509155273,2.364428997039795,2.5036041736602783,2.5556437969207764,2.5966074466705322,2.450493335723877,2.5414693355560303,2.4918816089630127,2.483466386795044,2.526350498199463],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"27_proprietary_opensourced_recommendatio\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"27_proprietary_opensourced_recommendatio\"],\"x\":[8.700318336486816,8.662346839904785,8.710755348205566,8.685635566711426,10.063787460327148,8.825141906738281,9.746978759765625,8.632473945617676,8.7011137008667,9.762179374694824,10.958747863769531,9.222679138183594],\"y\":[1.2634673118591309,1.199538230895996,1.2940387725830078,1.2151010036468506,2.198474407196045,1.475010633468628,3.3699069023132324,1.1698120832443237,1.2515336275100708,3.3798534870147705,1.8569083213806152,1.78851318359375],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"28_modification_mp2_multiword\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"28_modification_mp2_multiword\"],\"x\":[12.416332244873047,12.561809539794922,12.500757217407227,12.375421524047852,12.55825138092041,12.481864929199219,12.49730396270752,12.1989107131958,12.471221923828125,12.578041076660156,12.469315528869629,12.464476585388184],\"y\":[1.9628962278366089,2.3608059883117676,2.222209930419922,2.1565310955047607,2.1810107231140137,2.1097028255462646,2.3813178539276123,2.188523292541504,2.159067153930664,2.3556671142578125,1.6200826168060303,2.1543469429016113],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"29_instructional_teaching_students\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"29_instructional_teaching_students\"],\"x\":[8.679919242858887,8.863896369934082,8.659948348999023,8.705171585083008,8.791703224182129,8.700974464416504,8.720808029174805,9.014729499816895,8.698103904724121,8.63776969909668,8.665627479553223,8.739877700805664],\"y\":[0.4001754820346832,0.5290360450744629,0.39178118109703064,0.5974481105804443,0.46730977296829224,0.44593337178230286,0.3917016386985779,0.42155221104621887,0.3866553008556366,0.5923276543617249,0.5228016972541809,0.4678838849067688],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"30_counterfactuals_scone_counterfactual\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"30_counterfactuals_scone_counterfactual\"],\"x\":[10.26634407043457,10.128600120544434,10.363204956054688,10.149825096130371,10.254651069641113,10.238754272460938,10.266803741455078,10.18940258026123,10.27680492401123,10.135074615478516,10.226946830749512],\"y\":[2.246767520904541,2.1721770763397217,2.3008975982666016,2.423623561859131,2.2409117221832275,2.265772581100464,2.3193318843841553,2.154175281524658,2.2092831134796143,2.1171963214874268,2.24501371383667],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"37_transformer_nat_transformers_paramete\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"37_transformer_nat\"],\"x\":[12.020341873168945,12.169936180114746,12.101693153381348,12.094861030578613,11.659080505371094,12.379535675048828,12.050992965698242,12.262616157531738,12.431180000305176,11.988661766052246,12.082118034362793,12.085043907165527,12.180397033691406,12.176328659057617,11.84795093536377,12.084325790405273,12.128952980041504,12.28459644317627,12.194994926452637,12.008187294006348,11.911856651306152,12.048596382141113,12.229466438293457,12.182258605957031,12.1466703414917,12.20826530456543,11.903266906738281,11.976790428161621,11.813048362731934,12.021183967590332,12.020978927612305,12.224409103393555,11.788299560546875,12.209701538085938,11.93454647064209,11.924176216125488,12.019671440124512,12.825284004211426,12.15970516204834,12.297412872314453,12.158596992492676,12.110843658447266,12.26720905303955,12.509387016296387,12.426708221435547,12.31076431274414,12.200416564941406,12.108731269836426,12.027480125427246,12.294480323791504,12.179040908813477,11.868775367736816,12.016388893127441,12.533409118652344,11.871016502380371,12.278585433959961,12.497551918029785,11.953156471252441,12.103267669677734,12.292784690856934,12.235295295715332,11.931829452514648,13.446449279785156,13.293713569641113,13.339015007019043,13.234209060668945,13.390213012695312,13.275945663452148,13.059819221496582,13.199735641479492,13.168375968933105,13.352516174316406,13.391827583312988,13.276883125305176,13.3883638381958,13.391206741333008,13.22536563873291,13.355441093444824,13.415063858032227,13.445826530456543,13.263446807861328,13.253372192382812,13.080596923828125,13.292510986328125,13.387595176696777,13.407983779907227,13.065677642822266,13.157443046569824,13.285446166992188,13.371879577636719,13.443427085876465,13.312077522277832,13.325447082519531,13.201516151428223,13.412653923034668,13.387580871582031,13.376065254211426,13.23760986328125,13.391357421875,13.28543758392334,13.155158996582031,13.134252548217773,13.264272689819336,13.326519012451172,13.441741943359375,13.180994987487793,13.229814529418945,13.385966300964355,13.26877498626709,13.252023696899414,12.640745162963867],\"y\":[3.4334287643432617,3.9167301654815674,3.949054002761841,3.438246250152588,3.445382595062256,3.36970853805542,3.7780611515045166,2.977111577987671,3.824798583984375,4.014473915100098,3.966614246368408,3.947669744491577,3.546800136566162,3.5740740299224854,3.722076654434204,3.7931604385375977,3.4850759506225586,3.1680963039398193,4.030865669250488,3.494990587234497,3.631016969680786,3.9821364879608154,3.874598264694214,3.628248453140259,3.362018585205078,3.6282668113708496,3.4427807331085205,3.539586305618286,3.9404172897338867,3.503599166870117,3.3990092277526855,3.441103219985962,2.9686286449432373,3.454322576522827,3.507044553756714,3.588791847229004,3.930246591567993,3.598909854888916,3.4998040199279785,3.598375082015991,3.547398805618286,3.815009355545044,3.1761443614959717,3.7004573345184326,2.9148776531219482,3.357351303100586,3.646010398864746,4.046837329864502,3.844369649887085,3.3957107067108154,3.5331528186798096,3.4589388370513916,3.6157045364379883,3.513286590576172,3.4719655513763428,3.176704168319702,3.3323616981506348,3.9445858001708984,3.7024219036102295,3.222160577774048,3.219435214996338,3.6485061645507812,2.849513053894043,2.6924357414245605,2.519382953643799,2.685671091079712,3.4131829738616943,3.3496320247650146,2.9062132835388184,2.5787570476531982,2.8339319229125977,2.6660850048065186,3.363393783569336,3.0692508220672607,3.403278112411499,2.94502854347229,2.862663984298706,2.849548578262329,3.311357021331787,2.857738494873047,2.829335927963257,2.7103164196014404,2.6873369216918945,3.1780202388763428,2.415661573410034,2.984997510910034,2.9960906505584717,3.0859627723693848,3.064985752105713,3.3739285469055176,2.99613094329834,3.3052146434783936,2.759016990661621,2.554168224334717,2.7810420989990234,3.370600700378418,2.8825161457061768,2.5941309928894043,3.402747631072998,3.1956849098205566,2.574057102203369,2.4850573539733887,3.1060783863067627,2.757913827896118,2.8156826496124268,2.6702122688293457,3.1264901161193848,3.3880455493927,2.8700947761535645,2.6230366230010986,3.2947304248809814],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"39_languages_summarization_bias_translat\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"39_languages_summarization\"],\"x\":[13.87353229522705,13.208016395568848,14.106237411499023,13.092193603515625,13.336145401000977,13.386882781982422,12.319914817810059,13.021512985229492,13.98583698272705,12.727828025817871,14.158334732055664,12.96531867980957,13.214096069335938,13.292146682739258,13.683382034301758,13.187692642211914,13.279541969299316,12.989252090454102,12.971858978271484,12.976456642150879,13.035022735595703,12.499113082885742,13.1813325881958,12.979851722717285,13.296390533447266,13.039684295654297,13.842495918273926,13.132991790771484,13.9254732131958,12.167425155639648,14.098097801208496,13.920403480529785,12.558815002441406,12.849774360656738,12.970587730407715,14.02845287322998,12.911493301391602,13.765254974365234,13.905959129333496,13.326302528381348,13.942405700683594,12.925593376159668,13.41197395324707,13.25645637512207,12.991842269897461,13.592504501342773,13.934208869934082,13.955780029296875,13.298561096191406,13.263577461242676,12.345218658447266,13.947273254394531,13.009997367858887,13.643670082092285,13.180303573608398,13.74915599822998,12.808438301086426,13.863404273986816,12.991873741149902,13.162498474121094,12.859356880187988,13.075448036193848,13.156818389892578,12.886246681213379,13.997769355773926,13.03771686553955,12.784741401672363,12.894862174987793,12.97797966003418,13.60038948059082,14.051366806030273,12.930017471313477,12.712410926818848,12.782764434814453,12.85952377319336,12.852415084838867,13.387727737426758,13.939677238464355,12.471564292907715,13.912345886230469,12.641358375549316,13.367488861083984,13.8226318359375,13.64598560333252,13.757844924926758,12.249322891235352,13.732461929321289,13.127846717834473,12.968668937683105,13.813699722290039,13.807978630065918,13.008394241333008,13.787175178527832,13.993060111999512,13.531773567199707,13.984949111938477,13.01711654663086,14.002456665039062,13.822026252746582,13.829243659973145,13.329290390014648,12.908177375793457,14.121039390563965,13.951969146728516,12.813560485839844,13.843708038330078,13.839166641235352,14.121064186096191,12.558928489685059,13.817070960998535,13.30069637298584,13.859855651855469,13.09512710571289,12.266813278198242,12.803173065185547,13.087562561035156,13.733831405639648,13.884516716003418,12.79514217376709,13.156198501586914,12.635796546936035,13.271025657653809,12.98554801940918,13.871160507202148,13.031496047973633,14.019685745239258,13.770309448242188,13.913249969482422,13.995428085327148,12.395220756530762,13.254109382629395,12.519502639770508,13.016617774963379,13.239713668823242,13.700984954833984,13.32162857055664,13.05946159362793,12.602531433105469,12.626888275146484,13.298456192016602,13.010909080505371,12.473347663879395,13.30741024017334,12.52789306640625,13.24752140045166,14.047928810119629,12.949474334716797,13.951871871948242,14.054276466369629,12.840779304504395,12.930652618408203,13.249177932739258,14.00323486328125,12.957128524780273,13.71989631652832,12.736191749572754,13.225967407226562,7.761767864227295,7.890107154846191,8.475704193115234,8.049864768981934,7.910464286804199,7.735909938812256,8.021018028259277,7.9187726974487305,7.908551216125488,7.699703216552734,7.959645748138428,7.826615333557129,8.282672882080078,7.934024333953857,8.200965881347656,7.851012229919434,7.659298419952393,7.964986801147461,7.375672340393066,7.989088535308838,8.16100788116455,7.880589008331299,7.8358564376831055,7.8488359451293945,7.915194511413574,8.008590698242188,7.819448471069336,7.807568073272705,7.392760753631592,7.372259140014648,7.680557727813721,8.323735237121582,7.405981540679932,7.390983581542969,8.47547721862793,7.814692974090576,8.292101860046387,8.115535736083984,7.816876411437988,7.764798164367676,7.4307541847229,8.173230171203613,7.816125869750977,7.800354957580566,7.687350749969482,7.416838645935059,8.04630184173584,7.372389793395996,8.061315536499023,7.425755023956299,7.877260684967041,8.388016700744629,7.779290199279785,8.227629661560059,8.057212829589844,8.02259349822998,7.782587051391602,8.23399543762207,7.90402889251709,7.8760199546813965,7.4668121337890625,8.308961868286133,7.824159622192383,7.712260723114014,7.914191246032715,7.685945987701416,7.940428733825684,9.407848358154297,9.24934196472168,9.988895416259766,9.616469383239746,9.254940032958984,9.619887351989746,9.763313293457031,9.41704273223877,9.903557777404785,9.640640258789062,9.904218673706055,9.833464622497559,9.501309394836426,10.281272888183594,9.5150146484375,9.301063537597656,9.468867301940918,9.583669662475586,9.652227401733398,9.2976655960083,9.610597610473633,9.950066566467285,9.628329277038574,9.354300498962402,9.685493469238281,9.437472343444824,9.71446418762207,9.970356941223145,9.591911315917969,9.4934720993042,11.245697975158691,9.630011558532715,9.857994079589844,9.644820213317871,9.684257507324219,9.61834716796875,9.774521827697754,9.085481643676758,10.036049842834473,9.738116264343262,9.506299018859863,9.30492877960205,9.699036598205566,9.771791458129883,9.192770957946777,9.514463424682617,9.63692855834961,9.843480110168457,9.38808536529541,9.520946502685547,9.217379570007324,9.838109970092773,9.648056030273438,9.77692699432373,9.458232879638672,9.670414924621582,9.680063247680664,9.057123184204102,9.189112663269043,9.039446830749512,9.115840911865234,9.096116065979004,9.138891220092773,8.879927635192871,8.893594741821289,8.761467933654785,8.84201431274414,8.92170524597168,8.984542846679688,8.870153427124023,9.148591041564941,9.1036376953125,8.759328842163086,9.173047065734863,9.000730514526367,9.311477661132812,9.226938247680664,8.916337966918945,9.266477584838867,8.85345458984375,8.894457817077637,9.150871276855469,8.838128089904785,8.816519737243652,9.046035766601562,8.901216506958008,9.159720420837402,8.813364028930664,9.301010131835938,8.885666847229004,9.035273551940918,9.151689529418945,9.173745155334473,9.153688430786133,9.068913459777832,9.077773094177246,8.93253231048584,9.23687744140625,8.845597267150879,8.906045913696289,9.104510307312012,8.794829368591309,8.974413871765137,8.559188842773438,9.103609085083008,8.886739730834961,8.847504615783691,8.852912902832031,8.7335786819458,8.971410751342773,8.835782051086426,8.914285659790039,8.319539070129395,8.872060775756836,8.969176292419434,8.707355499267578,8.74543285369873,8.927281379699707,8.319100379943848,8.32375717163086,8.666680335998535,8.335197448730469,8.70714282989502,9.042021751403809,8.712772369384766,8.291357040405273,8.325663566589355,8.38085651397705,8.75160026550293,8.50749397277832,8.352300643920898,8.444624900817871,8.73957347869873,8.366410255432129,8.543909072875977,8.721636772155762,8.295104026794434,8.693644523620605,8.322171211242676,8.898469924926758,8.4146146774292,8.773049354553223,8.323225975036621,9.074934959411621,8.57706356048584,8.7324857711792,8.81857967376709,9.022188186645508,8.334091186523438,8.382010459899902,8.761948585510254,8.723730087280273,9.060052871704102,8.774007797241211,8.736745834350586,8.403592109680176,10.630845069885254],\"y\":[1.3817375898361206,1.0612719058990479,1.1385481357574463,1.08525550365448,1.1626895666122437,1.2480696439743042,0.4848286211490631,1.2720167636871338,1.1438474655151367,1.3171401023864746,1.4076839685440063,1.264595627784729,1.0884864330291748,1.1517572402954102,1.355188012123108,1.7229746580123901,0.972616970539093,1.1486741304397583,0.8810750246047974,1.257293701171875,0.9572843313217163,0.8085752129554749,0.9309483766555786,0.7958930730819702,1.0975350141525269,0.6360377073287964,0.9854050874710083,1.798175573348999,1.3100166320800781,0.67192143201828,1.2066819667816162,1.2865713834762573,0.9056100845336914,0.6249911189079285,1.7463622093200684,1.2420947551727295,0.7832163572311401,1.1565204858779907,1.3271960020065308,1.3933436870574951,1.0287328958511353,0.7349646091461182,1.1217514276504517,1.414216160774231,1.152062177658081,1.4249526262283325,1.1767480373382568,1.2981083393096924,1.1263619661331177,1.0864992141723633,0.5083565711975098,1.0866432189941406,1.8492233753204346,1.125476598739624,0.944848895072937,1.1385208368301392,0.678071916103363,1.3165032863616943,0.6076777577400208,1.3482673168182373,1.6210135221481323,0.9811966419219971,1.2457525730133057,1.171410322189331,1.1819112300872803,0.8604665398597717,0.942077100276947,0.516913115978241,1.7408256530761719,1.4816997051239014,1.3886042833328247,1.7745593786239624,0.794830322265625,0.5049740076065063,0.8629714250564575,0.9947225451469421,1.3124425411224365,1.181308388710022,0.65378338098526,0.9567456841468811,0.8112806677818298,1.0578453540802002,1.270293116569519,1.1971663236618042,1.2050573825836182,0.49335476756095886,1.5063146352767944,1.3702335357666016,1.2522984743118286,1.2417391538619995,1.4349349737167358,1.29513418674469,1.3764736652374268,1.1924550533294678,1.3044599294662476,1.0554274320602417,0.9203598499298096,1.1263551712036133,1.4461302757263184,1.4113985300064087,1.1233012676239014,1.292132019996643,1.383273959159851,1.2921757698059082,1.4860016107559204,1.4392153024673462,1.4001719951629639,0.9837578535079956,0.6659143567085266,0.9185031056404114,1.1586668491363525,1.4281121492385864,1.1629738807678223,0.510953962802887,0.5897344946861267,0.8766728639602661,1.4713213443756104,1.1049691438674927,0.5188847184181213,1.4383823871612549,1.3952010869979858,1.3247922658920288,0.9874094128608704,1.13014554977417,0.8806256651878357,1.1321403980255127,1.1713860034942627,1.29535710811615,1.1741254329681396,0.570444643497467,1.2960554361343384,0.7852280735969543,0.9467426538467407,1.7510077953338623,1.3233586549758911,1.1152814626693726,1.0483297109603882,0.647891640663147,0.5596342086791992,1.3211227655410767,0.9677881002426147,0.6502654552459717,1.0543253421783447,0.6772043704986572,1.212782382965088,1.1973955631256104,1.1335924863815308,1.4092952013015747,1.3452917337417603,0.6293814778327942,1.8506160974502563,1.066854476928711,1.2767295837402344,0.7192075848579407,1.712160587310791,0.8005959391593933,1.1127575635910034,1.5433651208877563,1.4605567455291748,1.4761929512023926,1.4625173807144165,1.4584766626358032,1.4409211874008179,1.4749916791915894,1.4284682273864746,1.4748163223266602,1.4806615114212036,1.4631659984588623,1.411305546760559,1.4848947525024414,1.0450985431671143,1.4943875074386597,1.3517125844955444,1.4878875017166138,1.4422603845596313,1.5346200466156006,1.5406434535980225,1.499199628829956,1.4141327142715454,1.4414336681365967,1.3886102437973022,1.4608150720596313,1.4566928148269653,1.5145379304885864,1.6541327238082886,1.5264090299606323,1.531654953956604,1.4845945835113525,1.4692519903182983,1.5288569927215576,1.5096898078918457,1.3595173358917236,1.587659478187561,1.4848111867904663,1.480909824371338,1.49443781375885,1.6546357870101929,1.5368572473526,1.2316579818725586,1.3251349925994873,1.5530869960784912,1.5514334440231323,1.5379899740219116,1.494699239730835,1.525313377380371,1.4642879962921143,1.526352882385254,1.288293480873108,1.483452320098877,1.4212583303451538,1.5053179264068604,1.3147481679916382,1.3840261697769165,1.5364909172058105,1.516196370124817,1.4429274797439575,1.4955360889434814,1.5462852716445923,1.454986810684204,1.4761298894882202,1.4729735851287842,1.450022578239441,1.443420648574829,1.4701296091079712,1.5877009630203247,1.2727618217468262,1.4802215099334717,1.6104549169540405,1.4569437503814697,1.539625644683838,1.472213864326477,2.0593249797821045,1.3333063125610352,1.6195672750473022,0.646668553352356,1.6028765439987183,1.468073844909668,1.4336971044540405,1.469533920288086,1.5507453680038452,1.6590402126312256,1.5533472299575806,1.5326228141784668,1.4909952878952026,1.5933552980422974,0.6876698732376099,1.5868359804153442,1.573804259300232,1.5753282308578491,1.2330971956253052,1.6163599491119385,0.7645369172096252,1.5960463285446167,1.578489899635315,0.5172238349914551,1.5027610063552856,1.38727867603302,1.5584850311279297,1.571655035018921,1.5743860006332397,1.5758932828903198,1.8300682306289673,1.7321003675460815,1.5929148197174072,1.4410037994384766,1.594342589378357,1.557225227355957,1.3510727882385254,1.2829564809799194,1.1802170276641846,1.5979278087615967,1.6627514362335205,1.6593539714813232,1.5607953071594238,1.3650426864624023,0.5767338275909424,1.5249618291854858,1.5589072704315186,1.6015998125076294,1.5737286806106567,1.52560555934906,1.5944088697433472,3.968130350112915,4.086702346801758,3.7755074501037598,3.991488218307495,3.975116014480591,4.029293060302734,4.046248435974121,4.013284683227539,4.176393032073975,4.156956672668457,4.055082321166992,3.9321210384368896,3.937089204788208,3.9232289791107178,3.8601222038269043,3.866779327392578,3.9942166805267334,3.850517511367798,3.827030897140503,4.08809232711792,3.6295218467712402,4.113508701324463,4.187417507171631,3.6063523292541504,3.9586288928985596,4.24702787399292,3.9166204929351807,4.1035475730896,3.9102394580841064,4.182351112365723,3.869297981262207,3.981309175491333,3.9789490699768066,3.9490838050842285,3.944530487060547,3.942408323287964,4.038426399230957,4.008914947509766,4.139644622802734,3.7226343154907227,4.209354877471924,4.045825958251953,3.9882144927978516,4.28642463684082,4.0150346755981445,3.615358352661133,4.036133289337158,4.028831958770752,3.9747371673583984,3.9180045127868652,3.7974116802215576,4.060174942016602,4.20427942276001,4.09943962097168,3.193037986755371,3.0153839588165283,3.2098007202148438,3.411353826522827,3.419900894165039,2.993455171585083,3.2046985626220703,3.197061538696289,3.5652174949645996,3.1496386528015137,3.4359893798828125,3.163926362991333,3.4493963718414307,3.234320640563965,3.2002310752868652,3.272998094558716,3.3818790912628174,3.3397650718688965,3.0758256912231445,3.1623947620391846,3.384615182876587,3.2408924102783203,3.358213186264038,3.416884660720825,3.167921781539917,3.4248197078704834,3.2836413383483887,3.2514102458953857,3.342782497406006,3.3918700218200684,3.194164752960205,3.047151803970337,3.55344820022583,3.2842931747436523,3.3557095527648926,3.1627235412597656,3.2219274044036865,3.0630640983581543,3.3806440830230713,3.417919874191284,3.0919387340545654,3.379578113555908,3.3994202613830566,3.3199284076690674,1.8919777870178223],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"40_dialogue_entity_adversarial_knowledge\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"40_dialogue_entity\"],\"x\":[11.134943962097168,11.025127410888672,11.245323181152344,11.604127883911133,11.156891822814941,11.352185249328613,11.527443885803223,11.282083511352539,11.126087188720703,11.73695182800293,11.573534965515137,13.375391006469727,11.404452323913574,11.218412399291992,10.824934959411621,11.162917137145996,11.561422348022461,11.366466522216797,10.656908988952637,11.300393104553223,11.272549629211426,11.278218269348145,11.19459056854248,11.2599458694458,11.612763404846191,10.9222412109375,11.204724311828613,11.096510887145996,11.001474380493164,11.493638038635254,11.407891273498535,11.244793891906738,11.168529510498047,11.017486572265625,11.451024055480957,11.223573684692383,11.187163352966309,11.271790504455566,11.26801872253418,11.24870491027832,11.188407897949219,11.215272903442383,11.429038047790527,11.241147994995117,11.28900146484375,11.684981346130371,11.643854141235352,11.422016143798828,11.644282341003418,11.211750030517578,11.794293403625488,10.666879653930664,11.676538467407227,11.204644203186035,11.4844388961792,11.210556983947754,10.822954177856445,11.602618217468262,11.172928810119629,11.261439323425293,11.18604850769043,11.24404525756836,11.392868995666504,11.273892402648926,11.069701194763184,11.81900691986084,11.298412322998047,11.697004318237305,11.151240348815918,11.614058494567871,11.220532417297363,11.269240379333496,11.204486846923828,11.25660228729248,11.144911766052246,11.228436470031738,11.389638900756836,11.027185440063477,11.341411590576172,11.200804710388184,10.692567825317383,11.169511795043945,11.02439022064209,10.92267894744873,11.365792274475098,11.304564476013184,11.123757362365723,11.317601203918457,11.2233304977417,11.252074241638184,10.953468322753906,11.1937837600708,11.191131591796875,11.507041931152344,13.65318489074707,13.939993858337402,13.685359001159668,13.784351348876953,13.504968643188477,13.94735336303711,13.143325805664062,13.834990501403809,13.44897747039795,13.599161148071289,13.199520111083984,13.714217185974121,13.943765640258789,13.477546691894531,13.838189125061035,13.14396858215332,13.451980590820312,13.139006614685059,13.305334091186523,13.572847366333008,13.578728675842285,13.640463829040527,13.756519317626953,13.478137016296387,13.640913963317871,13.371881484985352,13.30630874633789,13.778650283813477,13.48436164855957,13.48245620727539,13.242944717407227,13.188318252563477,13.671472549438477,13.161858558654785,13.182262420654297,13.67568302154541,13.578418731689453,13.574835777282715,13.530089378356934,12.509833335876465,13.550978660583496,13.269086837768555,13.929557800292969,13.697040557861328,13.825508117675781,13.642824172973633,13.51134204864502,13.453692436218262,13.557821273803711,13.553349494934082,13.648940086364746,13.369372367858887,13.12069034576416,13.566463470458984,13.692540168762207,13.704095840454102,13.338266372680664,13.631470680236816,13.964845657348633,13.344286918640137,13.934170722961426,13.56275463104248,13.731254577636719,13.44790267944336,13.115975379943848,13.600167274475098,8.303739547729492,13.527002334594727,13.690975189208984,13.423891067504883,13.938860893249512,13.628800392150879,13.415291786193848,13.883581161499023,13.465723037719727,13.598390579223633,13.958017349243164,13.456777572631836,13.795967102050781,9.905937194824219,13.677034378051758,13.943849563598633,13.411235809326172,13.518133163452148,13.331656455993652,8.079171180725098,8.06274700164795,7.97455358505249,7.848491191864014,8.032004356384277,7.964422702789307,7.964263916015625,8.134442329406738,8.114529609680176,7.841091632843018,7.8066725730896,8.066001892089844,7.995242118835449,8.329507827758789,8.135189056396484,7.784605979919434,7.775929927825928,8.180281639099121,8.422222137451172,7.919281005859375,7.885015487670898,7.895800590515137,7.771629810333252,8.177929878234863,7.973200798034668,8.28538703918457,7.766735553741455,7.8133392333984375,7.980544567108154,7.834801197052002,7.792785167694092,8.400049209594727,8.152592658996582,7.950541019439697,7.951925754547119,8.21005630493164,7.920266628265381,7.854425430297852,8.000518798828125,7.90179443359375,8.401914596557617,8.250751495361328,7.81003999710083,8.432668685913086,7.853431224822998,8.134767532348633,8.218738555908203,7.8007283210754395,8.225289344787598,7.825068473815918,8.072797775268555,8.098258972167969,8.405604362487793,7.813677787780762,7.821072101593018,7.985950946807861,7.810969352722168,8.3153657913208,8.099970817565918,7.857753276824951,8.26423168182373,7.91890811920166,7.952602386474609,7.912245273590088,7.974617958068848,7.784420490264893,8.138467788696289,8.036944389343262,10.677026748657227,10.417326927185059,10.19378662109375,10.34382438659668,10.359889030456543,10.168207168579102,10.379969596862793,10.382912635803223,10.3951997756958,10.006451606750488,10.593812942504883,10.523648262023926,10.564718246459961,10.390353202819824,10.409890174865723,10.375975608825684,10.270461082458496,10.380226135253906,10.342330932617188,10.389986038208008,10.570111274719238,10.563359260559082,10.393143653869629,10.445023536682129,10.406803131103516,10.406514167785645,9.976093292236328,10.553617477416992,10.514777183532715,10.404207229614258,10.352548599243164,10.402541160583496,10.373939514160156,10.66649055480957,10.147748947143555,10.177634239196777,10.380538940429688,10.481019020080566,10.569191932678223,10.38936710357666,10.453319549560547,10.417245864868164,10.462665557861328,10.532641410827637,10.139423370361328,10.995478630065918,10.441753387451172,10.408876419067383,10.595641136169434,10.508939743041992,10.599928855895996,10.383362770080566,10.315134048461914,10.520706176757812,10.196410179138184,10.421682357788086,10.112748146057129,10.42435073852539,9.957427978515625,9.02020263671875,8.764460563659668,9.149687767028809,8.96601676940918,8.83845329284668,7.854395389556885,8.691113471984863,8.074891090393066,7.991252422332764,8.15771198272705,8.11121940612793,7.889158725738525,8.559086799621582,7.732449054718018,8.129582405090332,7.739405155181885,8.906782150268555,8.779287338256836,8.384539604187012,8.096807479858398,7.73988151550293,7.74143123626709,10.807848930358887,8.938881874084473,8.25653076171875,8.156562805175781,8.910407066345215,8.929011344909668,8.912620544433594,8.349308967590332,9.150344848632812,9.063459396362305,8.188298225402832,8.082099914550781,8.124395370483398,8.095884323120117,7.761401653289795,7.85822057723999,7.735099792480469,9.143465995788574,7.9051079750061035,8.62359619140625,8.771903991699219,8.10483455657959,8.881553649902344,7.842296600341797,7.751319408416748,8.932684898376465,8.793137550354004,8.686760902404785,8.676131248474121,8.958822250366211,7.80643892288208,8.740694046020508,7.7468767166137695,8.771867752075195,9.40961742401123,9.401433944702148,9.173538208007812,9.561808586120605,9.133511543273926,9.540070533752441,9.700275421142578,9.41574478149414,9.289504051208496,9.243343353271484,9.41695785522461,9.496110916137695,9.767770767211914,9.445943832397461,9.416340827941895,9.574084281921387,9.292332649230957,9.226899147033691,9.462366104125977,9.370545387268066,9.44404411315918,9.387925148010254,9.353792190551758,9.420747756958008,9.548852920532227,9.489824295043945,9.414810180664062,9.33342456817627,9.402825355529785,9.672316551208496,9.393604278564453,9.239797592163086,9.441904067993164,9.516083717346191,9.340642929077148,9.363749504089355,9.59388256072998,9.435190200805664,9.716001510620117,9.322286605834961,9.686580657958984,9.433467864990234,9.423392295837402,9.48121166229248,9.288214683532715,9.267616271972656,9.553618431091309,9.478880882263184,9.371923446655273,9.760424613952637,9.519524574279785,9.401512145996094,9.502867698669434,10.448945045471191],\"y\":[-0.2791852056980133,-0.56585294008255,-0.33805036544799805,-0.5237462520599365,0.26701799035072327,-0.765414834022522,-0.7155061364173889,-0.6682496070861816,-0.6063860654830933,-0.3886547386646271,-0.6360691785812378,-0.5970664024353027,-0.6077590584754944,-0.3316837251186371,0.027478214353322983,-1.0895836353302002,-0.5561929941177368,0.06399431079626083,0.6634865999221802,-0.7742803692817688,0.14976142346858978,-1.053679347038269,-0.8100492358207703,-0.6574156284332275,-0.6182628273963928,2.9431962966918945,-0.8820046186447144,-0.8308306336402893,-0.7561229467391968,0.06208992749452591,-1.2723103761672974,0.021186748519539833,-0.9865592122077942,0.42442458868026733,-0.5611394047737122,-0.19294996559619904,-0.6945754289627075,-0.3245929181575775,0.14573052525520325,-0.8709970712661743,-0.352821946144104,-0.20292386412620544,-0.3928586542606354,-0.45419180393218994,-0.6536911725997925,-0.43990081548690796,0.47350263595581055,0.13158506155014038,-0.7095873355865479,-1.041664481163025,-0.30340245366096497,0.33025580644607544,-0.5825144648551941,-0.9634666442871094,0.1646973192691803,-0.9077123999595642,0.321090430021286,-0.4256440997123718,0.016457444056868553,-0.3007483184337616,-0.9926905035972595,-0.40955081582069397,-0.7163980603218079,-0.7613950967788696,-0.5563022494316101,-0.30304890871047974,-0.6749861836433411,-0.5403029322624207,-0.3122972548007965,-0.610802948474884,-0.7764879465103149,-0.3963066339492798,-0.9999945163726807,-0.2833714783191681,-0.40003347396850586,-0.9637208580970764,-0.45351165533065796,-0.5818671584129333,-0.7205086350440979,-0.9992696046829224,0.6563940644264221,-1.185823917388916,-1.0215363502502441,-0.13112245500087738,-0.6537771224975586,-0.7294257283210754,-0.7630423307418823,-0.7293157577514648,-0.9494206309318542,-0.3285471796989441,-0.1588168889284134,-0.36618950963020325,-0.9938457608222961,0.16926676034927368,-0.4724136292934418,0.05542946234345436,-0.2140544056892395,-0.305911123752594,-0.019443433731794357,0.04754577577114105,-0.5232704877853394,-0.11347304284572601,-0.42826223373413086,-0.3631307780742645,-0.34572187066078186,-0.06021670997142792,0.02899486944079399,-0.5741272568702698,-0.17813809216022491,-0.3307811915874481,-0.5624246597290039,-0.33823102712631226,-0.49408844113349915,-0.2875242233276367,-0.5673142671585083,-0.5054513216018677,-0.19457708299160004,0.04647556692361832,0.15025267004966736,-0.4029132127761841,-0.09265140444040298,-0.06226486340165138,-0.49906080961227417,-0.42240944504737854,-0.37416812777519226,-0.4376997649669647,-0.5433786511421204,-0.48809999227523804,-0.3587982952594757,-0.1461024433374405,-0.4376213848590851,-0.5315554738044739,-0.5797322392463684,2.760768413543701,-0.5617931485176086,-0.37373608350753784,1.0235728025436401,-0.2975458800792694,-0.12453179061412811,-0.39394140243530273,0.08051753044128418,-0.5586355328559875,-0.5786116719245911,-0.5049486756324768,0.1441514790058136,-0.7706948518753052,-0.21628828346729279,-0.3426320552825928,-0.16151897609233856,-0.39145922660827637,-0.5925371646881104,-0.2956721782684326,0.027539370581507683,-0.30378881096839905,0.042292892932891846,-0.12347960472106934,-0.06413992494344711,-0.6200217604637146,-0.4954264461994171,-0.34516313672065735,3.190534830093384,-0.2043280303478241,-0.5706292986869812,-0.7155386209487915,0.12354090809822083,-0.219037726521492,-0.293334424495697,0.07163935899734497,-0.5650919675827026,-0.29002729058265686,0.07026058435440063,-0.5817172527313232,-0.10595829039812088,-1.2771755456924438,-0.2213153839111328,0.0876251682639122,-0.7389064431190491,-0.14196763932704926,-0.6306360363960266,0.011064082384109497,0.014209670014679432,-0.09134206920862198,-0.2708755433559418,-0.20944665372371674,-0.15542514622211456,-0.016267064958810806,0.33848637342453003,1.3138884241925552e-05,-0.3145374655723572,-0.37487006187438965,0.08202043920755386,-0.06734184175729752,-0.26890313625335693,-0.2482864111661911,-0.37177231907844543,-0.342668354511261,0.05140113830566406,-0.009848969988524914,-0.26512715220451355,-0.21851959824562073,-0.20532433688640594,-0.34504765272140503,-0.12112276256084442,-0.11948814243078232,-0.36536410450935364,-0.43950170278549194,-0.3522815406322479,-0.16667716205120087,-0.33151018619537354,-0.4151010513305664,-0.21129287779331207,0.18422266840934753,-0.13335680961608887,0.17345526814460754,0.07078302651643753,-0.06395117193460464,-0.33869442343711853,-0.20178943872451782,-0.20231139659881592,-0.02338528260588646,0.17116601765155792,-0.3580891191959381,-0.31286415457725525,-0.32009008526802063,-0.1331133246421814,0.18067573010921478,-0.2697615623474121,0.08013986051082611,-0.31005680561065674,0.028209263458848,0.10702250152826309,0.2667584717273712,-0.3471834659576416,-0.3380453586578369,-0.07149399816989899,-0.4023083448410034,-0.10830654948949814,-0.17893050611019135,-0.2754707932472229,0.22065694630146027,-0.1637326180934906,-0.12212429195642471,-0.17535138130187988,-0.2760526239871979,-0.01606772653758526,0.11322040110826492,0.021302759647369385,4.530323028564453,4.6652750968933105,4.259049415588379,4.17231559753418,4.669703960418701,4.087711334228516,4.67042350769043,4.63741397857666,4.61146879196167,4.00691556930542,4.465812683105469,4.730312824249268,3.997835874557495,4.689051151275635,4.215987682342529,4.713289260864258,3.9740796089172363,4.662024021148682,4.6290106773376465,4.683803558349609,4.155202865600586,4.592298984527588,4.655975341796875,4.570188999176025,4.67529296875,4.149698734283447,4.111392498016357,4.625186920166016,4.129754066467285,4.608168125152588,4.609062194824219,4.684344291687012,4.598238468170166,4.515726089477539,4.097564697265625,4.126251697540283,4.632607936859131,3.846534013748169,4.030094146728516,4.677065372467041,4.595035552978516,4.631924152374268,4.152017593383789,4.6212286949157715,4.092752933502197,4.058029651641846,4.524569034576416,4.68303918838501,4.761435031890869,4.133310794830322,4.037563800811768,4.604251384735107,4.569831848144531,4.141808032989502,4.4533562660217285,4.653535842895508,4.054136753082275,4.630998134613037,3.962613344192505,2.3929059505462646,2.3886959552764893,2.2481915950775146,2.366041660308838,2.3146793842315674,2.3162174224853516,2.369158983230591,2.5022075176239014,2.4162755012512207,2.523205280303955,2.5030629634857178,2.322309732437134,2.426605224609375,2.189079761505127,2.5072808265686035,2.1927618980407715,2.2802486419677734,2.404411792755127,2.5210015773773193,2.5109353065490723,2.188095808029175,2.2342336177825928,1.3085989952087402,2.270885467529297,2.520770311355591,2.4987809658050537,2.2820372581481934,2.2575674057006836,2.305651903152466,2.480987310409546,2.6763017177581787,2.3891241550445557,2.503016233444214,2.507106065750122,2.5494508743286133,2.4986634254455566,2.2242064476013184,2.3021676540374756,2.150664806365967,2.744899034500122,2.3470451831817627,2.496939182281494,2.4149718284606934,2.483858346939087,2.3855793476104736,2.2803268432617188,2.255380868911743,2.259631872177124,2.3189663887023926,2.3722052574157715,2.399129867553711,2.2720420360565186,2.2237091064453125,2.365219831466675,2.2338740825653076,2.3938751220703125,-0.5178394317626953,-0.650516152381897,-0.479851096868515,-0.4310290515422821,-0.5378267168998718,-0.6259913444519043,-0.7380387187004089,-0.21319614350795746,-0.7479232549667358,-0.610841691493988,-0.6754353642463684,-0.5430178642272949,-0.6768491268157959,-0.45748335123062134,-0.3581988215446472,-0.8913076519966125,-0.698038637638092,-0.7323350310325623,-0.4265297055244446,-0.6116015911102295,-0.9226518869400024,-0.7730879783630371,-0.7615750432014465,-0.7331415414810181,-0.8823621869087219,-0.8506748676300049,-0.7885114550590515,-0.723310649394989,-0.7920389175415039,-0.6305829882621765,-0.698114275932312,-0.7590868473052979,-0.9453780651092529,-0.47471097111701965,-0.7153941988945007,-0.7514808177947998,-0.32000821828842163,-0.5421023368835449,-0.5268405079841614,-0.7256784439086914,-0.6647657752037048,-0.5601599812507629,-0.8130180239677429,-0.5797785520553589,-0.76743084192276,-0.740344226360321,-0.6408556699752808,-0.4104425609111786,-0.3353818655014038,-0.6775698661804199,-0.7086683511734009,-0.9370354413986206,-0.7324972152709961,0.6956299543380737],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"#CFD8DC\",\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"other\",\"showlegend\":false,\"x\":[12.260786056518555,10.01115894317627,11.924004554748535,10.16279411315918,12.00646686553955,10.273680686950684,6.795833110809326,10.738099098205566,12.415397644042969,12.344215393066406,8.005266189575195,11.672365188598633,9.678071022033691,11.613967895507812,10.691057205200195,10.729043006896973,12.570730209350586,12.728907585144043,11.094691276550293,12.467524528503418,10.7769136428833,11.365942001342773,11.85193157196045,10.574968338012695,13.300118446350098,8.84424877166748,9.322251319885254,11.329916000366211,9.31579303741455,10.838811874389648,11.459781646728516,11.970154762268066,12.745402336120605,10.581709861755371,9.499332427978516,10.223129272460938,9.936136245727539,11.424376487731934,7.634429931640625,12.565068244934082,12.609332084655762,11.734045028686523,12.76197338104248,8.449498176574707,10.840675354003906,11.946686744689941,8.837322235107422,9.319323539733887,12.509225845336914,10.419879913330078,6.7955498695373535,9.324790954589844,12.908655166625977,11.152015686035156,11.986949920654297,12.218483924865723,11.286534309387207,11.993667602539062,9.96347427368164,11.670063018798828,9.893813133239746,10.930827140808105,9.840456008911133,9.265555381774902,8.651935577392578,9.824747085571289,9.816277503967285,12.087455749511719,11.567899703979492,6.796971321105957,12.668866157531738,12.218778610229492,10.99422836303711,11.2903470993042,13.343911170959473,12.024370193481445,12.494938850402832,12.23508071899414,10.537565231323242,9.071687698364258,9.39167308807373,13.654808044433594,11.47091293334961,11.624275207519531,12.246249198913574,12.476778984069824,11.365124702453613,12.479470252990723,12.073331832885742,10.643670082092285,10.698538780212402,11.801766395568848,11.808372497558594,9.954280853271484,12.55838394165039,8.872209548950195,11.538613319396973,8.866655349731445,10.459000587463379,11.395777702331543,9.855978012084961,11.591615676879883,10.30197811126709,11.654211044311523,11.18188190460205,10.506263732910156,13.050704002380371,11.941532135009766,12.512845993041992,11.652451515197754,13.646256446838379,11.867741584777832,12.657735824584961,12.094124794006348,10.286686897277832,12.581634521484375,11.336400985717773,9.937158584594727,11.021777153015137,9.948188781738281,12.35839557647705,12.886040687561035,9.261913299560547,9.89763355255127,9.462775230407715,9.968618392944336,12.362512588500977,10.358255386352539,8.327774047851562,11.927535057067871,10.00265884399414,12.349808692932129,10.623787879943848,10.678670883178711,11.847443580627441,9.692587852478027,9.262121200561523,10.958932876586914,12.889772415161133,10.948387145996094,11.634035110473633,12.080227851867676,10.378564834594727,10.096247673034668,11.42621898651123,8.7933931350708,11.231094360351562,8.999340057373047,11.527088165283203,12.19135570526123,7.8714375495910645,11.001914978027344,12.61523151397705,9.75306224822998,11.896245002746582,12.523083686828613,9.378335952758789,10.35689926147461,11.08590316772461,13.147154808044434,9.818265914916992,11.11213493347168,11.444499969482422,11.590463638305664,10.737903594970703,9.770731925964355,10.78557014465332,11.933521270751953,10.511313438415527,9.82026481628418,8.356404304504395,10.73415470123291,10.45881462097168,10.396477699279785,12.194050788879395,12.752180099487305,11.78748607635498,10.255226135253906,12.165645599365234,12.292634963989258,12.844697952270508,10.628177642822266,8.569395065307617,11.763611793518066,12.15890884399414,12.740022659301758,12.746966361999512,9.62110424041748,11.45886516571045,10.926743507385254,12.772931098937988,12.837591171264648,12.200713157653809,9.013678550720215,10.967081069946289,9.76983642578125,12.884422302246094,11.670434951782227,13.589189529418945,12.576118469238281,11.409375190734863,9.541470527648926,8.66622257232666,11.438347816467285,11.34601879119873,10.590363502502441,11.35364818572998,11.930695533752441,6.797292709350586,10.794591903686523,10.309884071350098,10.247868537902832,9.914916038513184,13.303763389587402,12.540189743041992,11.354313850402832,8.383038520812988,12.193739891052246,9.227989196777344,9.241358757019043,9.536521911621094,10.984012603759766,12.313669204711914,9.644474029541016,9.546388626098633,9.345878601074219,11.460395812988281,10.699294090270996,9.629450798034668,8.48170280456543,11.0349760055542,10.570517539978027,10.367918014526367,11.483748435974121,10.360127449035645,10.180638313293457,12.789478302001953,10.665085792541504,11.708563804626465,12.97873592376709,11.337641716003418,10.709122657775879,13.167486190795898,12.409531593322754,8.32143497467041,9.063648223876953,10.371134757995605,11.557292938232422,10.586905479431152,9.22173023223877,11.61303424835205,8.834911346435547,13.149587631225586,11.683677673339844,9.396690368652344,11.555591583251953,11.163105964660645,10.546306610107422,13.552511215209961,12.85168170928955,11.200281143188477,9.939804077148438,10.729233741760254,12.406366348266602,11.353873252868652,11.169713973999023,9.460506439208984,10.654193878173828,11.77177906036377,11.467199325561523,12.088428497314453,13.498458862304688,11.279095649719238,12.567978858947754,10.763456344604492,10.033524513244629,12.673567771911621,10.10020637512207,8.970285415649414,9.142569541931152,11.128690719604492,12.259600639343262,8.503023147583008,11.229401588439941,9.610413551330566,10.481382369995117,12.30782699584961,11.052600860595703,13.069833755493164,9.056635856628418,9.835722923278809,11.540289878845215,10.57509994506836,10.630483627319336,12.53976058959961,12.878406524658203,12.195188522338867,12.416590690612793,8.807579040527344,12.123668670654297,10.993809700012207,11.357285499572754,11.609417915344238,8.505378723144531,9.010087966918945,9.596749305725098,12.824377059936523,10.293586730957031,11.473138809204102,12.474746704101562,10.027557373046875,12.210655212402344,10.91020393371582,13.3267240524292,13.175515174865723,11.291525840759277,11.628263473510742,11.242234230041504,9.939929008483887,12.68603515625,9.105117797851562,12.55679702758789,10.582772254943848,11.557234764099121,12.54797649383545,11.068909645080566,12.6818208694458,11.577909469604492,11.18771743774414,13.261770248413086,11.224922180175781,11.866551399230957,9.587552070617676,10.551297187805176,11.903278350830078,12.07540225982666,10.768095970153809,12.016518592834473,8.86601448059082,12.697779655456543,12.205658912658691,10.315733909606934,11.997912406921387,12.761860847473145,11.485032081604004,12.877477645874023,10.77319622039795,9.71633529663086,12.789429664611816,10.579119682312012,12.169694900512695,11.369839668273926,12.047872543334961,11.594593048095703,11.944458961486816,11.05200481414795,9.081618309020996,12.835399627685547,12.314804077148438,9.093341827392578,11.577603340148926,9.215981483459473,11.374435424804688,12.266894340515137,11.160942077636719,8.430356979370117,12.54307746887207,11.239245414733887,10.539368629455566,11.465906143188477,7.657243728637695,12.263589859008789,9.044021606445312,11.270294189453125,9.69029426574707,12.90316104888916,11.74852466583252,8.697430610656738,10.244534492492676,13.194223403930664,8.26854419708252,12.263144493103027,12.15650463104248,10.97741985321045,11.557283401489258,12.207938194274902,9.880388259887695,13.715070724487305,8.669084548950195,11.517677307128906,12.063190460205078,10.891190528869629,8.157964706420898,13.416830062866211,12.172284126281738,9.77486515045166,12.416096687316895,8.317665100097656,12.529857635498047,10.84004020690918,8.873795509338379,12.759370803833008,10.572245597839355,11.383156776428223,11.408194541931152,12.709611892700195,13.176079750061035,7.645970344543457,10.45201587677002,11.888107299804688,8.155491828918457,10.934144973754883,12.047857284545898,11.55852222442627,10.887688636779785,10.773085594177246,10.705704689025879,9.601393699645996,10.455825805664062,9.596738815307617,10.703749656677246,11.308526992797852,9.27015209197998,10.01099681854248,12.847784996032715,12.372241020202637,10.8928804397583,11.546853065490723,9.608847618103027,10.049355506896973,10.234009742736816,12.77387809753418,7.639250755310059,9.369508743286133,11.111124992370605,10.788805961608887,9.64129638671875,10.971961975097656,13.150918960571289,10.595973014831543,9.54521369934082,12.856410026550293,10.756369590759277,11.570320129394531,10.633820533752441,10.61214542388916,11.196186065673828,11.974361419677734,11.977964401245117,9.437406539916992,11.594237327575684,11.464212417602539,10.141727447509766,11.141068458557129,9.565552711486816,12.504912376403809,10.439518928527832,8.916755676269531,12.542708396911621,8.100579261779785,13.614336013793945,9.942835807800293,12.88668441772461,12.289319038391113,10.191901206970215,11.653940200805664,12.273872375488281,9.7015380859375,11.526108741760254,11.894497871398926,9.942673683166504,12.299449920654297,11.311140060424805,12.881195068359375,8.470643043518066,12.338445663452148,11.502923011779785,10.742378234863281,12.947842597961426,11.659990310668945,12.496929168701172,8.366921424865723,9.1043119430542,12.329166412353516,11.74715518951416,11.585480690002441,10.612318992614746,11.377769470214844,11.649738311767578,11.907842636108398,13.692667007446289,11.34955883026123,12.52751636505127,8.80586051940918,12.406229019165039,12.7645902633667,12.445311546325684,11.75776195526123,11.090094566345215,11.42638111114502,10.89482593536377,10.761418342590332,11.150059700012207,10.034558296203613,10.338700294494629,11.178735733032227,12.835885047912598,11.53531551361084,12.381514549255371,11.54314136505127,11.90941047668457,9.358724594116211,10.740930557250977,11.939253807067871,11.149989128112793,11.659534454345703,10.353327751159668,11.331498146057129,10.64175796508789,12.81151008605957,8.662858009338379,10.857192993164062,10.787764549255371,8.953280448913574,12.585387229919434,7.643540382385254,10.447787284851074,10.80845832824707,12.5751371383667,9.113858222961426,9.70373821258545,12.737845420837402,11.416006088256836,12.489952087402344,8.133481979370117,10.278565406799316,11.977149963378906,9.470913887023926,10.66490364074707,10.00718879699707,9.176525115966797,11.815064430236816,8.752781867980957,11.483132362365723,11.89669418334961,12.553421020507812,13.118978500366211,9.782742500305176,10.74406623840332,12.168991088867188,11.756896018981934,12.940293312072754,11.56460189819336,8.64371109008789,11.702585220336914,11.278419494628906,10.558026313781738,11.820279121398926,10.459695816040039,11.502100944519043,11.530735969543457,10.50390911102295,12.36990737915039,12.139140129089355,11.131754875183105,10.48403263092041,13.041622161865234,11.032854080200195,11.734776496887207,10.479324340820312,9.945302963256836,11.825505256652832,11.676788330078125,9.411858558654785,12.318131446838379,11.462925910949707,12.87320613861084,13.661906242370605,12.474398612976074,10.583564758300781,11.804007530212402,11.14046573638916,11.841476440429688,11.781903266906738,9.183506965637207,11.8534574508667,11.49117660522461,11.63294506072998,11.224637985229492,12.764777183532715,12.330574035644531,11.128299713134766,12.93774127960205,10.32089614868164,12.478330612182617,11.827890396118164,11.804841041564941,11.44624137878418,11.820039749145508,10.238183975219727,7.784416675567627,12.978934288024902,12.451269149780273,9.322071075439453,11.821974754333496,10.541068077087402,11.961499214172363,8.238553047180176,12.170052528381348,12.736010551452637,12.764181137084961,11.65296745300293,12.675060272216797,11.974373817443848,10.117522239685059,9.6543607711792,12.757356643676758,12.056167602539062,9.412654876708984,11.711962699890137,9.174324989318848,10.823249816894531,9.90158462524414,10.913339614868164,8.113393783569336,9.993379592895508,12.194329261779785,13.33237361907959,11.182921409606934,12.732467651367188,13.046422004699707,9.75806713104248,8.685029029846191,11.561057090759277,10.053182601928711,11.998501777648926,13.20248031616211,12.104700088500977,10.906420707702637,11.117636680603027,10.446064949035645,9.715156555175781,12.530933380126953,12.754565238952637,11.461576461791992,11.491990089416504,9.396021842956543,8.587562561035156,12.263270378112793,11.42301082611084,13.491022109985352,10.364217758178711,11.991475105285645,12.44273853302002,11.70559310913086,10.35807991027832,9.996321678161621,9.326005935668945,12.807846069335938,13.570623397827148,11.12037467956543,10.716169357299805,10.50700569152832,10.22071361541748,12.695380210876465,11.297102928161621,9.085335731506348,12.861201286315918,11.170536994934082,10.041293144226074,10.805912017822266,11.3994722366333,9.783839225769043,11.411025047302246,10.608845710754395,9.42233657836914,11.715644836425781,11.849699020385742,12.349161148071289,8.975476264953613,10.749741554260254,9.420133590698242,10.120325088500977,12.375459671020508,10.686391830444336,11.969694137573242,11.85922622680664,12.443221092224121,9.37954330444336,13.088716506958008,10.281312942504883,12.127240180969238,11.985560417175293,12.442952156066895,12.119172096252441,9.390953063964844,10.271842956542969,10.183282852172852,12.758428573608398,9.88254451751709,11.704732894897461,11.48423957824707,9.189234733581543,11.172453880310059,11.48064136505127,10.315546035766602,12.061239242553711,11.559906005859375,11.86010456085205,11.664885520935059,12.843629837036133,9.624467849731445,12.458562850952148,12.246566772460938,11.838500022888184,10.999716758728027,9.12990951538086,11.840607643127441,11.96013069152832,11.915284156799316,12.08629035949707,10.903203964233398,8.849540710449219,11.167780876159668,10.531004905700684,12.467206954956055,10.145415306091309,9.549249649047852,6.795407295227051,12.180326461791992,12.817878723144531,10.757889747619629,11.568016052246094,11.672552108764648,7.97341775894165,10.022518157958984,9.927380561828613,10.70942211151123,11.941184043884277,12.794625282287598,12.879822731018066,12.178878784179688,11.878340721130371,10.52446460723877,10.071585655212402,10.56197738647461,11.002744674682617,12.245721817016602,10.67922592163086,12.140796661376953,12.277287483215332,11.456153869628906,12.081787109375,11.62030029296875,10.880305290222168,10.69717025756836,9.24069881439209,12.846972465515137,10.142430305480957,12.146927833557129,9.958215713500977,9.50244140625,10.51760196685791,10.551128387451172,11.390852928161621,11.688502311706543,11.491974830627441,12.741220474243164,12.295635223388672,12.218851089477539,8.685344696044922,13.047622680664062,11.958925247192383,9.383042335510254,11.218199729919434,11.51757526397705,9.624102592468262,13.13804817199707,12.326669692993164,12.624595642089844,12.161945343017578,9.43790054321289,11.324953079223633,8.503750801086426,11.257233619689941,10.58021354675293,9.62783145904541,9.703179359436035,11.924786567687988,8.906972885131836,9.254066467285156,12.899577140808105,11.277064323425293,11.152223587036133,12.76430606842041,10.148778915405273,12.378602027893066,11.560495376586914,12.818056106567383,12.91047191619873,8.343107223510742,10.998586654663086,11.666386604309082,10.141661643981934,11.702606201171875,10.432024955749512,10.108580589294434,9.184564590454102,13.428140640258789,11.512848854064941,12.044175148010254,11.576578140258789,12.28610610961914,11.385955810546875,12.952836036682129,12.021284103393555,11.287923812866211,12.099640846252441,12.082901000976562,12.028682708740234,12.383368492126465,8.773681640625,12.879551887512207,11.319714546203613,8.952622413635254,11.267468452453613,12.477086067199707,12.503398895263672,9.42236614227295,11.20785140991211,11.332497596740723,8.951685905456543,10.856745719909668,10.309080123901367,11.73598861694336,8.723488807678223,12.038217544555664,9.878068923950195,12.995759963989258,10.974395751953125,9.64431381225586,10.158196449279785,13.631218910217285,8.542502403259277,11.165329933166504,10.789525032043457,13.625770568847656,11.960083961486816,12.425254821777344,11.940552711486816,13.470690727233887,13.038926124572754,7.834033012390137,9.87307071685791,11.216033935546875,12.414644241333008,12.093451499938965,11.877202987670898,12.003032684326172,10.026086807250977,12.491192817687988,13.477025032043457,12.77425479888916,10.46001148223877,8.880654335021973,9.948962211608887,10.48256778717041,12.090274810791016,11.583839416503906,11.40567684173584,6.7874345779418945,9.088058471679688,11.758285522460938,11.31846809387207,11.73054313659668,12.907270431518555,12.770581245422363,11.114065170288086,11.227639198303223,10.648099899291992,12.066269874572754,13.3859224319458,12.3936767578125,11.536118507385254,12.09136962890625,7.971031665802002,10.897802352905273,9.173691749572754,11.609256744384766,12.269068717956543,9.517958641052246,11.856656074523926,11.710532188415527,12.882142066955566,12.293147087097168,9.226627349853516,11.812376022338867,9.573324203491211,11.98694896697998,11.025315284729004,12.620606422424316,12.66535758972168,11.626893997192383,11.87524127960205,9.740761756896973,11.66956615447998,9.677865982055664,13.708874702453613,13.700621604919434,10.8056058883667,12.719548225402832,11.30461597442627,9.15710163116455,11.5343599319458,9.219347953796387,11.886124610900879,11.679952621459961,12.069474220275879,12.361296653747559,10.542925834655762,10.790267944335938,12.395968437194824,10.26074504852295,12.145330429077148,11.74125862121582,12.40096664428711,10.930389404296875,8.466429710388184,11.920580863952637,12.118764877319336,13.445467948913574,10.998306274414062,11.986994743347168,12.903426170349121,11.159873962402344,12.18824291229248,9.372452735900879,12.712847709655762,13.078712463378906,10.631550788879395,9.169002532958984,8.790238380432129,11.811575889587402,12.28490924835205,9.769627571105957,12.567846298217773,10.349342346191406,11.191412925720215,11.537104606628418,10.127388000488281,10.513239860534668,8.915346145629883,11.823579788208008,11.909771919250488,11.520251274108887,13.776591300964355,12.594861030578613,6.803589344024658,10.791962623596191,11.956903457641602,7.651430606842041,12.864334106445312,11.632024765014648,10.91291332244873,11.907623291015625,10.763083457946777,11.546504974365234,13.262833595275879,12.147619247436523,11.872856140136719,12.41122055053711,12.41154670715332,13.054023742675781,10.615890502929688,9.164365768432617],\"y\":[1.976219654083252,1.265775442123413,1.0926563739776611,-0.5182633996009827,2.4570963382720947,0.48775091767311096,-0.910475492477417,-0.5370233654975891,1.2164555788040161,1.3136889934539795,-0.12520691752433777,1.813036561012268,1.1669845581054688,1.774245023727417,-0.3897693157196045,1.3097110986709595,3.413337469100952,1.671027660369873,0.8366751074790955,3.2766458988189697,0.3775566816329956,0.44715550541877747,2.3352620601654053,3.230710506439209,2.2592904567718506,0.6792904138565063,0.25117701292037964,0.6122933626174927,1.182349681854248,0.9189866781234741,3.0709445476531982,0.9211485385894775,3.500269889831543,3.7989087104797363,1.823943853378296,1.6083993911743164,-0.23820368945598602,2.4324207305908203,0.19927099347114563,3.7869391441345215,2.054600715637207,0.9205240607261658,3.078073263168335,0.10503587871789932,0.8371968865394592,0.3198699951171875,-0.16870951652526855,2.7976632118225098,2.6678123474121094,-0.570141077041626,-0.9106079339981079,3.7905776500701904,2.7144033908843994,0.9463844299316406,1.3737200498580933,0.33007562160491943,-0.9850686192512512,1.1608271598815918,1.0532095432281494,2.060023546218872,1.5419471263885498,1.308414101600647,-1.1935114860534668,-0.17752167582511902,-0.4857664108276367,3.832371234893799,1.3047598600387573,3.552154541015625,1.942522644996643,-0.909134566783905,2.3067994117736816,0.9871791005134583,2.9792444705963135,2.830186128616333,0.7007690072059631,3.5236191749572754,3.808678388595581,0.5856729745864868,0.5755625367164612,-0.488295316696167,0.6351603269577026,0.5080202221870422,1.0401160717010498,2.3617639541625977,1.8510946035385132,0.9317828416824341,3.2759201526641846,2.4336321353912354,-1.299113392829895,3.8617777824401855,3.202488422393799,1.037284016609192,-0.5317288637161255,-0.5715892314910889,1.2100167274475098,1.357236623764038,3.533149480819702,3.0465500354766846,0.1627165526151657,0.7375492453575134,0.9511770009994507,0.07118048518896103,-0.3919447958469391,1.378462314605713,-0.3244394361972809,0.7082329988479614,2.143453359603882,1.1407673358917236,0.9031526446342468,2.8212311267852783,1.1145505905151367,1.4429399967193604,2.296320676803589,-0.001279126270674169,-0.10712704062461853,3.0526630878448486,0.7859264612197876,-0.6041418313980103,1.40556800365448,-0.5446928143501282,1.2216320037841797,3.4056971073150635,3.86862850189209,1.1494468450546265,0.1541697382926941,-0.2646180987358093,2.934096097946167,1.085005283355713,0.1298372447490692,1.4595853090286255,0.5229455232620239,1.7139097452163696,3.179950475692749,0.8122704029083252,2.313836097717285,0.9151896834373474,0.0694771260023117,2.251661777496338,2.7393033504486084,1.8044404983520508,2.4212090969085693,1.369358777999878,1.4411309957504272,1.778269648551941,1.9663785696029663,-0.2085247039794922,1.8884222507476807,0.01605393923819065,0.9991214275360107,2.1229145526885986,-0.17486093938350677,1.2444441318511963,1.5270284414291382,1.034622311592102,0.5874497890472412,1.0864499807357788,0.7937063574790955,0.2653231918811798,0.6648417711257935,2.1787331104278564,-1.4033302068710327,0.993794858455658,3.9591336250305176,2.2620389461517334,2.2779126167297363,1.026957631111145,1.6402356624603271,1.106472134590149,3.609027624130249,4.001310348510742,2.4320061206817627,1.3439748287200928,1.444767951965332,0.8551923036575317,0.15037234127521515,2.425750494003296,2.3774797916412354,2.089541435241699,0.14875546097755432,1.2632057666778564,1.3453816175460815,3.9026010036468506,0.47671282291412354,1.526449203491211,1.530004620552063,0.44732722640037537,1.5337772369384766,-0.5403087735176086,-1.0967426300048828,0.833954393863678,0.7645677924156189,2.70750093460083,1.2675151824951172,0.1613716185092926,3.286445379257202,0.9860133528709412,3.4668221473693848,1.4706974029541016,-0.06515169888734818,3.346012830734253,3.19698166847229,1.0387732982635498,-0.47503626346588135,1.1350661516189575,-0.12550556659698486,0.48462581634521484,3.611987352371216,0.44945693016052246,-0.9105373620986938,0.9302946925163269,2.0090830326080322,0.1268618106842041,-0.5888694524765015,1.5191290378570557,2.7231905460357666,3.63657283782959,0.20960384607315063,0.04279949516057968,2.0899455547332764,2.395681142807007,-0.3550689220428467,1.1873329877853394,1.6772342920303345,-0.551196813583374,1.0045689344406128,3.5376405715942383,-0.2728564739227295,1.6083232164382935,3.451906681060791,0.4097013771533966,0.28619876503944397,2.9995458126068115,1.430558204650879,2.9486751556396484,1.4180095195770264,3.394707679748535,-1.0172475576400757,1.6186503171920776,2.2325010299682617,3.3960022926330566,2.3339428901672363,2.508234739303589,1.917661428451538,2.007206678390503,0.7356185913085938,1.3998340368270874,1.426846981048584,-0.10650590807199478,3.196619987487793,1.5687898397445679,3.3810524940490723,-0.20458725094795227,1.947638988494873,-0.17957328259944916,0.6638663411140442,1.4990065097808838,1.0839821100234985,-0.13390646874904633,1.798355221748352,1.460438847541809,2.6365201473236084,2.2331058979034424,0.5038972496986389,1.672692894935608,3.618595838546753,2.4914488792419434,-0.7637554407119751,2.602285861968994,1.6152315139770508,2.3165502548217773,4.036905288696289,1.7626341581344604,-0.30745241045951843,1.3061516284942627,3.0544943809509277,2.2432503700256348,0.5260573625564575,0.26869526505470276,0.03409036993980408,-0.5223769545555115,0.39339277148246765,2.0766706466674805,-0.38802358508110046,3.801676034927368,2.3770365715026855,3.5236270427703857,3.0124449729919434,3.137105703353882,3.03592848777771,1.0055789947509766,2.3000667095184326,3.485448122024536,2.8649325370788574,3.2783849239349365,2.067582130432129,1.7887386083602905,1.117841124534607,1.9235390424728394,-0.5475263595581055,2.5567400455474854,-0.42588913440704346,1.6459540128707886,3.0523805618286133,3.548125743865967,0.9675278663635254,1.0511583089828491,1.8744837045669556,-0.2662609815597534,2.020230531692505,2.395968198776245,1.5245238542556763,2.208904504776001,-0.23743867874145508,2.1941235065460205,3.066039800643921,0.8780515789985657,-0.3361736238002777,0.12655974924564362,0.5816671252250671,2.5916388034820557,1.8386311531066895,0.9971460103988647,-0.09463711082935333,1.4201574325561523,3.414994478225708,-0.5812013745307922,1.0859824419021606,3.207399845123291,1.0073139667510986,1.059942603111267,0.9441660642623901,2.421081066131592,-0.9699038863182068,-0.7918293476104736,3.2947211265563965,3.453516721725464,2.6633572578430176,1.12492835521698,0.9515612125396729,1.8676871061325073,2.2264864444732666,0.4588735103607178,-0.7277568578720093,-1.0359772443771362,2.857710599899292,2.6983368396759033,1.5439820289611816,0.8810650110244751,3.6186933517456055,3.788325071334839,2.0987229347229004,3.1248533725738525,-0.6990334391593933,3.2795536518096924,0.4755188822746277,2.3451409339904785,1.9918140172958374,2.239830255508423,2.980741500854492,3.4494681358337402,2.7023935317993164,0.8716931343078613,1.143310546875,0.12861472368240356,1.2377668619155884,-0.3922814130783081,3.3432180881500244,1.7890307903289795,0.9444081783294678,2.8529300689697266,0.18657220900058746,2.6263394355773926,0.48836591839790344,0.770980954170227,0.9084759950637817,2.9118480682373047,0.899668276309967,0.3940073549747467,0.8528211116790771,1.6868844032287598,0.24871914088726044,2.4808976650238037,2.190305709838867,1.4015684127807617,-0.11070117354393005,1.4967073202133179,0.9283192753791809,0.4603424668312073,1.1601427793502808,1.9638222455978394,-1.3094513416290283,1.8755643367767334,-0.034235879778862,2.5533201694488525,2.5379700660705566,3.77651309967041,2.733030319213867,0.3953371047973633,2.7038984298706055,1.0902845859527588,3.95662522315979,3.4667394161224365,1.4680428504943848,-0.2102063000202179,1.9835692644119263,0.25430288910865784,2.2354414463043213,0.19437259435653687,1.457194447517395,1.478986144065857,0.024254370480775833,0.9681276082992554,-1.2682996988296509,1.4637945890426636,0.9524343013763428,3.2517035007476807,1.0663663148880005,1.047045350074768,2.5798704624176025,1.9295272827148438,-0.4900413751602173,1.2294294834136963,3.744098663330078,1.1958093643188477,3.4060301780700684,1.4086087942123413,3.195308208465576,-0.21053600311279297,1.9594486951828003,1.3294121026992798,0.7901973128318787,-1.0279371738433838,0.18727345764636993,2.7890584468841553,1.1563249826431274,-0.1735643744468689,2.2202253341674805,0.9077820777893066,2.668337821960449,2.603579521179199,0.24776341021060944,1.158795714378357,1.6285470724105835,1.96036958694458,3.842803716659546,-0.05351594090461731,1.0603022575378418,3.5160486698150635,3.203173875808716,-0.22633570432662964,1.7337943315505981,1.1724951267242432,0.17315110564231873,0.7105343341827393,2.4431610107421875,3.8104262351989746,0.10712434351444244,0.713238537311554,3.1322901248931885,3.2645468711853027,0.534716784954071,0.5656581521034241,3.5241639614105225,2.5362460613250732,-0.7327529191970825,-0.8334300518035889,2.0500335693359375,0.9878270626068115,2.0549156665802,3.280917167663574,0.543793261051178,0.40602004528045654,1.2652292251586914,1.6847025156021118,0.6613606810569763,0.48743921518325806,1.5848220586776733,0.15714702010154724,3.097687244415283,3.4039037227630615,3.8253872394561768,-0.3216516077518463,0.36484676599502563,3.5613582134246826,0.849107563495636,2.804208278656006,3.8222780227661133,1.8004229068756104,1.4634736776351929,3.1937408447265625,0.8675617575645447,3.476331949234009,0.3727441728115082,3.1206748485565186,1.5812299251556396,-0.6251404285430908,3.3036246299743652,3.7486493587493896,1.7302109003067017,2.3743913173675537,0.9696566462516785,0.4081820845603943,0.6557876467704773,2.6788806915283203,3.448852300643921,1.163936972618103,3.5292248725891113,2.8257150650024414,3.515986680984497,1.9399545192718506,0.4077995717525482,0.766963541507721,-0.4135100543498993,0.42341694235801697,2.8088972568511963,-0.6076659560203552,0.1522863209247589,3.2291979789733887,1.588698387145996,1.8325613737106323,0.9248793125152588,1.3020033836364746,3.420180559158325,1.643860101699829,1.1540601253509521,0.21492479741573334,0.1739095002412796,1.5416576862335205,3.289651393890381,-0.5035665035247803,-0.46198034286499023,3.0262765884399414,3.655320405960083,1.8784151077270508,0.3909582197666168,0.8705004453659058,1.1364428997039795,1.208755612373352,0.26025456190109253,0.44635772705078125,0.12183551490306854,-0.15554407238960266,-0.25469839572906494,-0.45932525396347046,0.3801633417606354,2.1852316856384277,3.0103812217712402,0.16040968894958496,0.17387855052947998,2.1033482551574707,3.5950064659118652,1.6760215759277344,3.2061989307403564,0.3962271809577942,2.2076709270477295,-0.8994259834289551,2.514460802078247,0.979424774646759,3.5055930614471436,0.26043587923049927,0.08888838440179825,0.9722239375114441,1.9090890884399414,1.4592763185501099,0.7288630604743958,0.5196076035499573,2.52851939201355,0.3626737594604492,0.9660953879356384,1.4570801258087158,1.5067415237426758,1.0723236799240112,1.9673024415969849,0.5451234579086304,2.6414546966552734,0.7439343333244324,3.4168541431427,1.108274221420288,0.9759180545806885,3.0499231815338135,0.3342808485031128,1.1762615442276,1.2191663980484009,1.9902498722076416,2.7178072929382324,2.1931447982788086,1.4139740467071533,3.4444408416748047,-0.2844769358634949,2.468306541442871,2.982534408569336,1.1627116203308105,2.345379114151001,-0.26387879252433777,2.15932035446167,-0.09391316771507263,2.315988540649414,3.9019370079040527,1.4993696212768555,0.8209245204925537,0.1551581770181656,2.0490360260009766,1.3861424922943115,1.0227948427200317,-0.12519274652004242,3.113816976547241,1.2089561223983765,-0.26088747382164,1.8862637281417847,3.144066333770752,-1.0393073558807373,2.0746145248413086,1.443308711051941,1.180871844291687,-0.6809756755828857,1.017892599105835,1.8907074928283691,-1.2943123579025269,0.1569855809211731,0.6024736166000366,0.8391635417938232,2.21470046043396,-1.1607123613357544,1.8066606521606445,2.297105312347412,2.370729446411133,1.7525073289871216,0.9918195605278015,0.6040194034576416,2.8629562854766846,3.4195144176483154,1.8813725709915161,3.7637126445770264,0.20570334792137146,2.110116958618164,1.3198965787887573,1.0308681726455688,1.5510575771331787,1.0269742012023926,3.243104934692383,0.5696408748626709,-1.0560033321380615,3.80950927734375,-0.9885093569755554,2.1285574436187744,0.27239152789115906,-0.9925388693809509,2.664095401763916,0.5445898771286011,3.4431591033935547,2.9532716274261475,1.4311859607696533,2.9324100017547607,3.054269313812256,0.8955436944961548,0.025119181722402573,3.7716782093048096,0.6967297196388245,2.6491522789001465,2.385422945022583,3.098644495010376,3.803661823272705,1.4175082445144653,2.0697543621063232,3.0386083126068115,3.8905580043792725,1.805167555809021,3.370250940322876,0.6452329754829407,1.198987364768982,4.435891628265381,3.9789042472839355,0.2224683314561844,0.784505307674408,3.900918483734131,-0.08793401718139648,-0.20969590544700623,0.9383320212364197,2.9824304580688477,0.4146740138530731,3.451068878173828,0.58146733045578,2.629016876220703,2.5896897315979004,0.8220140933990479,2.19604229927063,2.3683815002441406,1.0513523817062378,-0.5379165410995483,2.0229125022888184,2.577913522720337,1.3024059534072876,2.953939914703369,1.1526473760604858,-1.245646595954895,0.6586165428161621,1.9431781768798828,0.795759916305542,0.4541209042072296,1.3323132991790771,0.9036451578140259,1.0493559837341309,2.936338424682617,1.658136010169983,2.7779719829559326,0.217825248837471,-0.08116167783737183,0.8917245864868164,1.92644202709198,1.5383085012435913,3.03080153465271,-0.5870935916900635,1.1749986410140991,1.7433806657791138,3.2300631999969482,1.3536816835403442,1.4494167566299438,2.132977247238159,1.142106533050537,1.0964027643203735,-1.2843916416168213,2.0309720039367676,0.38181358575820923,0.4180848002433777,1.397927165031433,1.0205312967300415,-1.0413587093353271,-0.3866738975048065,-0.9114582538604736,2.2314321994781494,-0.9689415693283081,0.8953730463981628,-0.7223781943321228,3.3937878608703613,-0.1200912594795227,1.4780992269515991,-1.0540677309036255,1.243822693824768,1.1921714544296265,-0.9881787300109863,2.7142724990844727,1.035949468612671,1.7600966691970825,3.096761703491211,1.0765949487686157,-0.3055479824542999,0.8458605408668518,1.8884222507476807,0.7532628178596497,3.692111015319824,2.047429084777832,1.5398664474487305,2.6263246536254883,3.3768866062164307,1.3354458808898926,2.8829421997070312,3.073280096054077,2.618222951889038,2.2130420207977295,1.131805658340454,1.4583265781402588,-0.6291894912719727,3.3120579719543457,3.235522985458374,2.4912800788879395,0.8031163215637207,2.785531520843506,1.2172937393188477,2.5934207439422607,0.44533485174179077,3.7607927322387695,0.8943837881088257,2.2763075828552246,0.15780554711818695,2.040842056274414,0.27494242787361145,3.4471356868743896,2.545928955078125,3.3168694972991943,3.057359218597412,2.0106849670410156,3.7272539138793945,-0.2229020744562149,-0.3912656009197235,2.5650482177734375,0.5276594758033752,1.9719535112380981,3.524745225906372,1.5316689014434814,0.3282936215400696,2.1106743812561035,2.4451394081115723,1.7900829315185547,2.8499183654785156,2.917431354522705,-0.6730669736862183,2.2880237102508545,2.3319878578186035,0.7816553711891174,3.079967975616455,0.3783358633518219,3.771744728088379,3.4605367183685303,3.4098503589630127,2.232668399810791,2.7577970027923584,0.8026202917098999,0.8486886024475098,1.336401104927063,1.9623465538024902,2.0950241088867188,2.3696439266204834,2.694972276687622,0.7788678407669067,1.5454350709915161,0.642559289932251,1.9104548692703247,2.817577838897705,1.2944279909133911,2.2089552879333496,2.9942898750305176,0.6504705548286438,2.6843504905700684,1.2824846506118774,-0.555378794670105,1.9917773008346558,2.4853670597076416,2.2720730304718018,0.05783434212207794,1.741772174835205,3.9805495738983154,1.6283323764801025,3.9042468070983887,-0.08662469685077667,1.0114973783493042,-0.2651676535606384,2.276035785675049,-0.6100764274597168,2.0067296028137207,1.3208794593811035,1.0322285890579224,1.9896197319030762,2.3109211921691895,2.2940375804901123,0.7046573758125305,2.1666882038116455,0.8269322514533997,1.038419246673584,3.1033222675323486,2.011228084564209,0.4995725154876709,3.0119717121124268,1.7737351655960083,1.1829763650894165,3.768042802810669,2.453611135482788,-1.2905018329620361,0.4045698344707489,-0.7276555299758911,1.108733057975769,0.7043386101722717,1.7317860126495361,3.1328790187835693,1.5019123554229736,0.00757896201685071,1.7896612882614136,4.704597473144531,-1.2845878601074219,1.471731185913086,3.7090561389923096,-0.9188922643661499,0.25417712330818176,0.40908655524253845,-0.6070359349250793,3.279542922973633,2.3822507858276367,-1.0290439128875732,-1.0308316946029663,1.6611809730529785,0.5246748328208923,1.5900543928146362,1.5552963018417358,1.9574549198150635,0.3487606346607208,1.906638503074646,-0.15992236137390137,-0.1188686341047287,1.874509334564209,2.240257501602173,3.598365068435669,2.4335744380950928,3.23817777633667,1.554663896560669,2.292228937149048,1.6409223079681396,1.495506763458252,3.2188000679016113,3.7398290634155273,2.8911819458007812,3.769780397415161,2.451735019683838,2.5036814212799072,3.150447130203247,1.2291960716247559,0.9691900610923767,3.689431667327881,-0.07314413040876389,0.73487389087677,0.839801013469696,1.4524890184402466,2.1931369304656982,1.3259851932525635,3.0612916946411133,0.2924751043319702,0.6916547417640686,2.3885679244995117,0.7535249590873718,1.8109625577926636,3.2876040935516357,2.6223843097686768,2.415797472000122,2.450838565826416,1.9013489484786987,2.8400003910064697,3.637033224105835,1.2191064357757568,0.49888309836387634,0.527450680732727,3.3894202709198,2.8021371364593506,1.5861856937408447,-0.5647964477539062,1.773905634880066,2.8972055912017822,0.4014616310596466,3.552330493927002,3.5676872730255127,0.44267264008522034,2.05513334274292,2.6361255645751953,2.9887442588806152,0.289135605096817,3.268169641494751,3.1244053840637207,0.8573437333106995,-0.19672121107578278,-0.7705172300338745,1.6629849672317505,3.544947862625122,-0.6638178825378418,3.600919008255005,-0.5163041353225708,-0.025210803374648094,1.4350130558013916,1.8289581537246704,0.6889894008636475,1.6060850620269775,-0.9072462916374207,2.4202475547790527,1.693398356437683,0.15449632704257965,3.578507661819458,2.314469575881958,4.283806324005127,2.3266613483428955,0.48093342781066895,3.2087161540985107,-0.08493365347385406,1.7356622219085693,1.580642580986023,1.29006028175354,0.9935421347618103,2.0631296634674072,2.585738182067871,1.8831286430358887],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"13_reasoning_pangu_symbolic\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"13_reasoning_pangu_symbolic\"],\"x\":[9.99891185760498,9.666812896728516,9.87675666809082,9.759414672851562,9.77737045288086,9.712926864624023,9.860082626342773,10.150372505187988,9.757026672363281,9.756099700927734,9.88352108001709,9.667094230651855,9.834576606750488,9.662117958068848,9.90071964263916,9.823662757873535,9.885151863098145,9.897980690002441,9.599701881408691,9.828813552856445,10.166223526000977,9.664942741394043,9.68136215209961,9.909587860107422,9.613624572753906,9.683416366577148,9.677172660827637,9.895627975463867,9.831110954284668,9.791001319885254,9.888591766357422,10.030932426452637,10.003292083740234,9.82230281829834],\"y\":[-1.2806227207183838,-1.4134907722473145,-1.3984967470169067,-0.8591680526733398,-1.1667323112487793,-0.9986860752105713,-1.226072072982788,-1.2499401569366455,-1.3567789793014526,-1.3018178939819336,-1.279323697090149,-1.3926376104354858,-0.8900179862976074,-1.3667707443237305,-1.3356115818023682,-1.4169893264770508,-1.3518718481063843,-1.17157781124115,-1.1077362298965454,-1.3785203695297241,-1.242591381072998,-1.3728218078613281,-1.36669921875,-1.1680376529693604,-1.3303998708724976,-0.8987433314323425,-1.368369460105896,-1.1938246488571167,-1.3907575607299805,-1.302925705909729,-1.1836531162261963,-1.1196755170822144,-1.154442310333252,-1.243509292602539],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"14_speech_asr_s2st\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"14_speech_asr_s2st\"],\"x\":[13.670157432556152,13.798901557922363,13.369604110717773,13.828919410705566,13.80864143371582,13.781868934631348,13.87149715423584,13.169575691223145,13.689194679260254,13.725122451782227,13.867948532104492,13.486695289611816,13.795697212219238,13.88539981842041,13.794876098632812,13.703969955444336,13.475425720214844,13.485220909118652,13.642592430114746,13.45029067993164,13.17920970916748,13.825347900390625,13.710762977600098,13.833584785461426,13.823281288146973,13.777862548828125,13.709671974182129,13.589035034179688,13.624615669250488,13.818294525146484,13.520774841308594,13.499052047729492,13.662908554077148],\"y\":[1.930212140083313,1.9589751958847046,1.9856090545654297,2.118469476699829,1.8433870077133179,2.083873748779297,1.9659463167190552,1.6238290071487427,1.9088102579116821,1.984571933746338,1.8491621017456055,1.8348524570465088,1.9095227718353271,2.1351234912872314,2.0434975624084473,1.9872238636016846,1.9833941459655762,1.972872018814087,1.8849225044250488,1.9644986391067505,1.6182628870010376,2.087661027908325,1.9592905044555664,1.9400484561920166,1.8442121744155884,2.052821636199951,2.1948325634002686,1.9715791940689087,1.9485830068588257,1.9467763900756836,2.033796548843384,2.05224347114563,1.9568393230438232],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"16_event_events_eae\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"16_event_events_eae\"],\"x\":[11.366277694702148,11.415215492248535,11.413336753845215,11.3681058883667,11.386617660522461,11.464384078979492,11.333633422851562,11.434808731079102,11.411873817443848,11.339577674865723,11.400299072265625,11.394913673400879,11.41909122467041,11.410325050354004,11.436091423034668,11.440723419189453,11.421466827392578,11.424187660217285,10.752309799194336,11.30122184753418,11.429636001586914,11.37482738494873,11.405726432800293,11.432455062866211,11.494464874267578,11.39643669128418,11.412282943725586,11.394112586975098,11.40078353881836,11.381903648376465],\"y\":[-1.7047370672225952,-1.7189042568206787,-1.7727406024932861,-1.7914379835128784,-1.7604166269302368,-1.5030242204666138,-1.8196848630905151,-1.7163331508636475,-1.6494393348693848,-1.6843397617340088,-1.7724716663360596,-1.7692655324935913,-1.7378288507461548,-1.7441824674606323,-1.464735746383667,-1.7443996667861938,-1.7241114377975464,-1.6847602128982544,0.36447063088417053,-1.677884578704834,-1.6699968576431274,-1.7729989290237427,-1.7663508653640747,-1.7174915075302124,-1.603288173675537,-1.7217741012573242,-1.7533800601959229,-1.768631100654602,-1.7753701210021973,-1.6422590017318726],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"18_prompts_prompt_continuous\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"18_prompts_prompt_continuous\"],\"x\":[9.238378524780273,9.30147647857666,9.232583999633789,9.182598114013672,9.238131523132324,9.218341827392578,9.234349250793457,9.302995681762695,9.232303619384766,9.124175071716309,9.416687965393066,9.293940544128418,9.349409103393555,9.29613208770752,9.22963809967041,9.208431243896484,9.365690231323242,9.037209510803223,9.345200538635254,9.246965408325195,9.236005783081055,9.268945693969727,9.25452709197998],\"y\":[0.11233166605234146,0.2616926431655884,0.13499541580677032,0.13999022543430328,0.13154223561286926,0.03455710411071777,0.1343674212694168,0.09137420356273651,0.0880853608250618,-0.03193724900484085,0.282012939453125,0.19786401093006134,0.27887430787086487,0.23954655230045319,0.1252308189868927,0.2048257291316986,0.21515387296676636,0.2964838743209839,0.28831443190574646,0.10827220976352692,0.010295077227056026,0.17448420822620392,0.15992532670497894],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"19_zeroshot_fewshot_verbalizers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"19_zeroshot_fewshot_verbalizers\"],\"x\":[11.956093788146973,12.490596771240234,12.429107666015625,12.604439735412598,12.537409782409668,12.447066307067871,12.514530181884766,12.494388580322266,12.444210052490234,12.318142890930176,12.335335731506348,12.499151229858398,12.503713607788086,12.477373123168945,12.43051815032959,12.483855247497559,12.453995704650879,12.551338195800781,12.408796310424805,12.414511680603027,12.439727783203125],\"y\":[-0.01690675877034664,-0.3107600808143616,-0.2527309060096741,-0.2602718472480774,-0.33801159262657166,-0.32423314452171326,-0.19620446860790253,-0.286507248878479,-0.2868269979953766,2.4987032413482666,-0.19626036286354065,-0.2882573902606964,2.8162295818328857,-0.27515092492103577,-0.28718024492263794,-0.29625701904296875,-0.3112342059612274,-0.32359012961387634,-0.29335451126098633,-0.24639558792114258,0.026239966973662376],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"20_chatgpt_chatgpts_rewriting\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"20_chatgpt_chatgpts_rewriting\"],\"x\":[8.408228874206543,8.558862686157227,8.468222618103027,8.394969940185547,8.488740921020508,8.462080955505371,8.458389282226562,8.648653030395508,8.45824146270752,8.496983528137207,8.497143745422363,8.60595989227295,8.652069091796875,8.46937370300293,8.726569175720215,8.704874992370605,8.474954605102539,8.499213218688965,8.526307106018066],\"y\":[0.8488311767578125,0.8990485668182373,0.930065929889679,0.9668167233467102,0.8830121159553528,0.8673952221870422,0.9546473622322083,0.9203405976295471,0.9518395662307739,0.8990188241004944,0.8826806545257568,1.001754879951477,0.8333197236061096,0.94631427526474,0.7836666703224182,0.7688788771629333,0.916922390460968,0.7554416656494141,0.88944411277771],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"21_tables_table_tableqa\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"21_tables_table_tableqa\"],\"x\":[10.386235237121582,10.229713439941406,10.389017105102539,10.301673889160156,10.361222267150879,10.3976469039917,10.311823844909668,10.32546329498291,10.373322486877441,10.308487892150879,10.342009544372559,10.355380058288574,10.305009841918945,10.051471710205078,10.35975456237793,10.3198823928833],\"y\":[-0.7522066235542297,-0.7360655665397644,-0.7566789388656616,-0.7287562489509583,-0.7565518617630005,-0.6721810698509216,-0.7266584038734436,-0.7462540864944458,-0.6999889016151428,-0.7211228609085083,-0.7179433107376099,-0.7139708399772644,-0.7397772073745728,-0.6260511875152588,-0.7241697311401367,-0.7212250828742981],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"22_oosf_instances_proxy\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"22_oosf_instances_proxy\"],\"x\":[11.03643798828125,11.07364559173584,11.248010635375977,11.170310974121094,10.949195861816406,11.285311698913574,10.957159996032715,11.096034049987793,11.2238130569458,11.023275375366211,11.042583465576172,11.16788101196289,11.188385009765625,11.112465858459473],\"y\":[3.106004476547241,3.0234382152557373,3.3541853427886963,3.115736246109009,3.0922281742095947,3.283050775527954,3.0349786281585693,3.2528135776519775,3.086516857147217,3.087618112564087,3.101942300796509,3.0745949745178223,3.133884906768799,3.1343841552734375],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"23_figurative_metaphors_sociocultural\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"23_figurative_metaphors_sociocultural\"],\"x\":[12.890239715576172,12.9169282913208,12.898111343383789,12.95676326751709,12.863883018493652,12.8355073928833,12.85640811920166,13.136683464050293,12.889006614685059,7.990469932556152,12.911820411682129,12.735673904418945,12.899605751037598,12.521623611450195],\"y\":[0.026861172169446945,-0.11374158412218094,-0.0861361101269722,-0.08715442568063736,0.14325851202011108,-0.09389468282461166,-0.06494750827550888,-0.21590127050876617,-0.029119817540049553,0.18973292410373688,-0.05029120296239853,0.2098502218723297,-0.02140832133591175,-0.014837851747870445],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"24_sarcasm_irony_humor\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"24_sarcasm_irony_humor\"],\"x\":[7.443446159362793,7.453991889953613,7.434319496154785,7.457515239715576,7.445913314819336,7.432080268859863,7.492039680480957,7.444361686706543,7.45388650894165,7.470397472381592,7.456826210021973,7.457319259643555,7.452033042907715,7.453394889831543],\"y\":[3.3650591373443604,3.3583810329437256,3.3732411861419678,3.3514108657836914,3.3623621463775635,3.374535083770752,3.3646886348724365,3.3602709770202637,3.329127788543701,3.332568883895874,3.3524117469787598,3.345237970352173,3.361050605773926,3.3561806678771973],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"25_dense_retrieval_retrievers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"25_dense_retrieval_retrievers\"],\"x\":[11.089601516723633,11.02492904663086,10.883186340332031,10.981522560119629,10.981151580810547,10.95821762084961,10.9121675491333,10.982845306396484,11.157200813293457,10.896283149719238,10.92767333984375,10.96117877960205,10.979663848876953],\"y\":[2.4378020763397217,2.470427989959717,2.49906587600708,2.4681739807128906,2.4417030811309814,2.4632935523986816,2.5861077308654785,2.4596455097198486,2.333122968673706,2.6070048809051514,2.5345096588134766,2.460688352584839,2.480128526687622],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"26_argumentative_environmental_firms\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"26_argumentative_environmental_firms\"],\"x\":[9.245034217834473,9.240274429321289,9.255699157714844,9.306416511535645,9.330011367797852,9.5535249710083,9.301241874694824,9.279705047607422,9.539828300476074,9.543222427368164,9.282510757446289,9.352497100830078],\"y\":[2.6040866374969482,2.5882256031036377,2.6099462509155273,2.364428997039795,2.5036041736602783,2.5556437969207764,2.5966074466705322,2.450493335723877,2.5414693355560303,2.4918816089630127,2.483466386795044,2.526350498199463],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"27_proprietary_opensourced_recommendatio\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"27_proprietary_opensourced_recommendatio\"],\"x\":[8.700318336486816,8.662346839904785,8.710755348205566,8.685635566711426,10.063787460327148,8.825141906738281,9.746978759765625,8.632473945617676,8.7011137008667,9.762179374694824,10.958747863769531,9.222679138183594],\"y\":[1.2634673118591309,1.199538230895996,1.2940387725830078,1.2151010036468506,2.198474407196045,1.475010633468628,3.3699069023132324,1.1698120832443237,1.2515336275100708,3.3798534870147705,1.8569083213806152,1.78851318359375],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"28_modification_mp2_multiword\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"28_modification_mp2_multiword\"],\"x\":[12.416332244873047,12.561809539794922,12.500757217407227,12.375421524047852,12.55825138092041,12.481864929199219,12.49730396270752,12.1989107131958,12.471221923828125,12.578041076660156,12.469315528869629,12.464476585388184],\"y\":[1.9628962278366089,2.3608059883117676,2.222209930419922,2.1565310955047607,2.1810107231140137,2.1097028255462646,2.3813178539276123,2.188523292541504,2.159067153930664,2.3556671142578125,1.6200826168060303,2.1543469429016113],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"29_instructional_teaching_students\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"29_instructional_teaching_students\"],\"x\":[8.679919242858887,8.863896369934082,8.659948348999023,8.705171585083008,8.791703224182129,8.700974464416504,8.720808029174805,9.014729499816895,8.698103904724121,8.63776969909668,8.665627479553223,8.739877700805664],\"y\":[0.4001754820346832,0.5290360450744629,0.39178118109703064,0.5974481105804443,0.46730977296829224,0.44593337178230286,0.3917016386985779,0.42155221104621887,0.3866553008556366,0.5923276543617249,0.5228016972541809,0.4678838849067688],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"30_counterfactuals_scone_counterfactual\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"30_counterfactuals_scone_counterfactual\"],\"x\":[10.26634407043457,10.128600120544434,10.363204956054688,10.149825096130371,10.254651069641113,10.238754272460938,10.266803741455078,10.18940258026123,10.27680492401123,10.135074615478516,10.226946830749512],\"y\":[2.246767520904541,2.1721770763397217,2.3008975982666016,2.423623561859131,2.2409117221832275,2.265772581100464,2.3193318843841553,2.154175281524658,2.2092831134796143,2.1171963214874268,2.24501371383667],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"41_transformer_parameters_transformers_n\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"41_transformer_parameters\"],\"x\":[12.020341873168945,12.169936180114746,12.101693153381348,12.094861030578613,11.659080505371094,12.379535675048828,12.050992965698242,12.262616157531738,12.431180000305176,11.988661766052246,12.082118034362793,12.085043907165527,12.180397033691406,12.176328659057617,11.84795093536377,12.084325790405273,12.128952980041504,12.28459644317627,12.194994926452637,12.008187294006348,11.911856651306152,12.048596382141113,12.229466438293457,12.182258605957031,12.1466703414917,12.20826530456543,11.903266906738281,11.976790428161621,11.813048362731934,12.021183967590332,12.020978927612305,12.224409103393555,11.788299560546875,12.209701538085938,11.93454647064209,11.924176216125488,12.019671440124512,12.825284004211426,12.15970516204834,12.297412872314453,12.158596992492676,12.110843658447266,12.26720905303955,12.509387016296387,12.426708221435547,12.31076431274414,12.200416564941406,12.108731269836426,12.027480125427246,12.294480323791504,12.179040908813477,11.868775367736816,12.016388893127441,12.533409118652344,11.871016502380371,12.278585433959961,12.497551918029785,11.953156471252441,12.103267669677734,12.292784690856934,12.235295295715332,11.931829452514648,13.446449279785156,13.293713569641113,13.339015007019043,13.234209060668945,13.390213012695312,13.275945663452148,13.059819221496582,13.199735641479492,13.168375968933105,13.352516174316406,13.391827583312988,13.276883125305176,13.3883638381958,13.391206741333008,13.22536563873291,13.355441093444824,13.415063858032227,13.445826530456543,13.263446807861328,13.253372192382812,13.080596923828125,13.292510986328125,13.387595176696777,13.407983779907227,13.065677642822266,13.157443046569824,13.285446166992188,13.371879577636719,13.443427085876465,13.312077522277832,13.325447082519531,13.201516151428223,13.412653923034668,13.387580871582031,13.376065254211426,13.23760986328125,13.391357421875,13.28543758392334,13.155158996582031,13.134252548217773,13.264272689819336,13.326519012451172,13.441741943359375,13.180994987487793,13.229814529418945,13.385966300964355,13.26877498626709,13.252023696899414,11.668291091918945,11.893047332763672,11.399813652038574,11.399984359741211,11.689655303955078,11.275130271911621,11.221017837524414,12.198081016540527,12.027531623840332,11.27303695678711,11.897500038146973,11.873612403869629,11.806347846984863,11.46186637878418,11.398143768310547,11.580697059631348,11.322710990905762,11.420403480529785,11.564152717590332,11.334028244018555,11.482306480407715,11.22659969329834,11.433297157287598,11.450419425964355,11.489837646484375,11.502764701843262,11.421748161315918,11.442119598388672,12.117900848388672,11.278861999511719,12.407376289367676],\"y\":[3.4334287643432617,3.9167301654815674,3.949054002761841,3.438246250152588,3.445382595062256,3.36970853805542,3.7780611515045166,2.977111577987671,3.824798583984375,4.014473915100098,3.966614246368408,3.947669744491577,3.546800136566162,3.5740740299224854,3.722076654434204,3.7931604385375977,3.4850759506225586,3.1680963039398193,4.030865669250488,3.494990587234497,3.631016969680786,3.9821364879608154,3.874598264694214,3.628248453140259,3.362018585205078,3.6282668113708496,3.4427807331085205,3.539586305618286,3.9404172897338867,3.503599166870117,3.3990092277526855,3.441103219985962,2.9686286449432373,3.454322576522827,3.507044553756714,3.588791847229004,3.930246591567993,3.598909854888916,3.4998040199279785,3.598375082015991,3.547398805618286,3.815009355545044,3.1761443614959717,3.7004573345184326,2.9148776531219482,3.357351303100586,3.646010398864746,4.046837329864502,3.844369649887085,3.3957107067108154,3.5331528186798096,3.4589388370513916,3.6157045364379883,3.513286590576172,3.4719655513763428,3.176704168319702,3.3323616981506348,3.9445858001708984,3.7024219036102295,3.222160577774048,3.219435214996338,3.6485061645507812,2.849513053894043,2.6924357414245605,2.519382953643799,2.685671091079712,3.4131829738616943,3.3496320247650146,2.9062132835388184,2.5787570476531982,2.8339319229125977,2.6660850048065186,3.363393783569336,3.0692508220672607,3.403278112411499,2.94502854347229,2.862663984298706,2.849548578262329,3.311357021331787,2.857738494873047,2.829335927963257,2.7103164196014404,2.6873369216918945,3.1780202388763428,2.415661573410034,2.984997510910034,2.9960906505584717,3.0859627723693848,3.064985752105713,3.3739285469055176,2.99613094329834,3.3052146434783936,2.759016990661621,2.554168224334717,2.7810420989990234,3.370600700378418,2.8825161457061768,2.5941309928894043,3.402747631072998,3.1956849098205566,2.574057102203369,2.4850573539733887,3.1060783863067627,2.757913827896118,2.8156826496124268,2.6702122688293457,3.1264901161193848,3.3880455493927,2.8700947761535645,2.6230366230010986,1.6213551759719849,1.7356750965118408,1.8189188241958618,2.011199474334717,1.6284370422363281,1.6911828517913818,1.434216856956482,1.9124113321304321,1.708951473236084,1.345273733139038,1.9241002798080444,1.706599235534668,1.5955106019973755,1.6075619459152222,1.8218833208084106,1.9240765571594238,1.5612646341323853,1.4347044229507446,1.8333743810653687,1.4203897714614868,1.508184552192688,1.6106964349746704,1.7237390279769897,1.6263391971588135,1.7565888166427612,1.8806627988815308,1.4971145391464233,1.4235330820083618,2.043459415435791,1.6557258367538452,2.9491677284240723],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"43_dialogue_languages_evaluation_languag\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"43_dialogue_languages\"],\"x\":[13.87353229522705,13.208016395568848,14.106237411499023,13.092193603515625,13.336145401000977,13.386882781982422,12.319914817810059,13.021512985229492,13.98583698272705,12.727828025817871,14.158334732055664,12.96531867980957,13.214096069335938,13.292146682739258,13.683382034301758,13.187692642211914,13.279541969299316,12.989252090454102,12.971858978271484,12.976456642150879,13.035022735595703,12.499113082885742,13.1813325881958,12.979851722717285,13.296390533447266,13.039684295654297,13.842495918273926,13.132991790771484,13.9254732131958,12.167425155639648,14.098097801208496,13.920403480529785,12.558815002441406,12.849774360656738,12.970587730407715,14.02845287322998,12.911493301391602,13.765254974365234,13.905959129333496,13.326302528381348,13.942405700683594,12.925593376159668,13.41197395324707,13.25645637512207,12.991842269897461,13.592504501342773,13.934208869934082,13.955780029296875,13.298561096191406,13.263577461242676,12.345218658447266,13.947273254394531,13.009997367858887,13.643670082092285,13.180303573608398,13.74915599822998,12.808438301086426,13.863404273986816,12.991873741149902,13.162498474121094,12.859356880187988,13.075448036193848,13.156818389892578,12.886246681213379,13.997769355773926,13.03771686553955,12.784741401672363,12.894862174987793,12.97797966003418,13.60038948059082,14.051366806030273,12.930017471313477,12.712410926818848,12.782764434814453,12.85952377319336,12.852415084838867,13.387727737426758,13.939677238464355,12.471564292907715,13.912345886230469,12.641358375549316,13.367488861083984,13.8226318359375,13.64598560333252,13.757844924926758,12.249322891235352,13.732461929321289,13.127846717834473,12.968668937683105,13.813699722290039,13.807978630065918,13.008394241333008,13.787175178527832,13.993060111999512,13.531773567199707,13.984949111938477,13.01711654663086,14.002456665039062,13.822026252746582,13.829243659973145,13.329290390014648,12.908177375793457,14.121039390563965,13.951969146728516,12.813560485839844,13.843708038330078,13.839166641235352,14.121064186096191,12.558928489685059,13.817070960998535,13.30069637298584,13.859855651855469,13.09512710571289,12.266813278198242,12.803173065185547,13.087562561035156,13.733831405639648,13.884516716003418,12.79514217376709,13.156198501586914,12.635796546936035,13.271025657653809,12.98554801940918,13.871160507202148,13.031496047973633,14.019685745239258,13.770309448242188,13.913249969482422,13.995428085327148,12.395220756530762,13.254109382629395,12.519502639770508,13.016617774963379,13.239713668823242,13.700984954833984,13.32162857055664,13.05946159362793,12.602531433105469,12.626888275146484,13.298456192016602,13.010909080505371,12.473347663879395,13.30741024017334,12.52789306640625,13.24752140045166,14.047928810119629,12.949474334716797,13.951871871948242,14.054276466369629,12.840779304504395,12.930652618408203,13.249177932739258,14.00323486328125,12.957128524780273,13.71989631652832,12.736191749572754,13.225967407226562,11.134943962097168,11.025127410888672,11.245323181152344,11.604127883911133,11.156891822814941,11.352185249328613,11.527443885803223,11.282083511352539,11.126087188720703,11.73695182800293,11.573534965515137,13.375391006469727,11.404452323913574,11.218412399291992,10.824934959411621,11.162917137145996,11.561422348022461,11.366466522216797,10.656908988952637,11.300393104553223,11.272549629211426,11.278218269348145,11.19459056854248,11.2599458694458,11.612763404846191,10.9222412109375,11.204724311828613,11.096510887145996,11.001474380493164,11.493638038635254,11.407891273498535,11.244793891906738,11.168529510498047,11.017486572265625,11.451024055480957,11.223573684692383,11.187163352966309,11.271790504455566,11.26801872253418,11.24870491027832,11.188407897949219,11.215272903442383,11.429038047790527,11.241147994995117,11.28900146484375,11.684981346130371,11.643854141235352,11.422016143798828,11.644282341003418,11.211750030517578,11.794293403625488,10.666879653930664,11.676538467407227,11.204644203186035,11.4844388961792,11.210556983947754,10.822954177856445,11.602618217468262,11.172928810119629,11.261439323425293,11.18604850769043,11.24404525756836,11.392868995666504,11.273892402648926,11.069701194763184,11.81900691986084,11.298412322998047,11.697004318237305,11.151240348815918,11.614058494567871,11.220532417297363,11.269240379333496,11.204486846923828,11.25660228729248,11.144911766052246,11.228436470031738,11.389638900756836,11.027185440063477,11.341411590576172,11.200804710388184,10.692567825317383,11.169511795043945,11.02439022064209,10.92267894744873,11.365792274475098,11.304564476013184,11.123757362365723,11.317601203918457,11.2233304977417,11.252074241638184,10.953468322753906,11.1937837600708,11.191131591796875,11.507041931152344,13.65318489074707,13.939993858337402,13.685359001159668,13.784351348876953,13.504968643188477,13.94735336303711,13.143325805664062,13.834990501403809,13.44897747039795,13.599161148071289,13.199520111083984,13.714217185974121,13.943765640258789,13.477546691894531,13.838189125061035,13.14396858215332,13.451980590820312,13.139006614685059,13.305334091186523,13.572847366333008,13.578728675842285,13.640463829040527,13.756519317626953,13.478137016296387,13.640913963317871,13.371881484985352,13.30630874633789,13.778650283813477,13.48436164855957,13.48245620727539,13.242944717407227,13.188318252563477,13.671472549438477,13.161858558654785,13.182262420654297,13.67568302154541,13.578418731689453,13.574835777282715,13.530089378356934,12.509833335876465,13.550978660583496,13.269086837768555,13.929557800292969,13.697040557861328,13.825508117675781,13.642824172973633,13.51134204864502,13.453692436218262,13.557821273803711,13.553349494934082,13.648940086364746,13.369372367858887,13.12069034576416,13.566463470458984,13.692540168762207,13.704095840454102,13.338266372680664,13.631470680236816,13.964845657348633,13.344286918640137,13.934170722961426,13.56275463104248,13.731254577636719,13.44790267944336,13.115975379943848,13.600167274475098,8.303739547729492,13.527002334594727,13.690975189208984,13.423891067504883,13.938860893249512,13.628800392150879,13.415291786193848,13.883581161499023,13.465723037719727,13.598390579223633,13.958017349243164,13.456777572631836,13.795967102050781,9.905937194824219,13.677034378051758,13.943849563598633,13.411235809326172,13.518133163452148,13.331656455993652,8.079171180725098,8.06274700164795,7.97455358505249,7.848491191864014,8.032004356384277,7.964422702789307,7.964263916015625,8.134442329406738,8.114529609680176,7.841091632843018,7.8066725730896,8.066001892089844,7.995242118835449,8.329507827758789,8.135189056396484,7.784605979919434,7.775929927825928,8.180281639099121,8.422222137451172,7.919281005859375,7.885015487670898,7.895800590515137,7.771629810333252,8.177929878234863,7.973200798034668,8.28538703918457,7.766735553741455,7.8133392333984375,7.980544567108154,7.834801197052002,7.792785167694092,8.400049209594727,8.152592658996582,7.950541019439697,7.951925754547119,8.21005630493164,7.920266628265381,7.854425430297852,8.000518798828125,7.90179443359375,8.401914596557617,8.250751495361328,7.81003999710083,8.432668685913086,7.853431224822998,8.134767532348633,8.218738555908203,7.8007283210754395,8.225289344787598,7.825068473815918,8.072797775268555,8.098258972167969,8.405604362487793,7.813677787780762,7.821072101593018,7.985950946807861,7.810969352722168,8.3153657913208,8.099970817565918,7.857753276824951,8.26423168182373,7.91890811920166,7.952602386474609,7.912245273590088,7.974617958068848,7.784420490264893,8.138467788696289,8.036944389343262,7.761767864227295,7.890107154846191,8.475704193115234,8.049864768981934,7.910464286804199,7.735909938812256,8.021018028259277,7.9187726974487305,7.908551216125488,7.699703216552734,7.959645748138428,7.826615333557129,8.282672882080078,7.934024333953857,8.200965881347656,7.851012229919434,7.659298419952393,7.964986801147461,7.375672340393066,7.989088535308838,8.16100788116455,7.880589008331299,7.8358564376831055,7.8488359451293945,7.915194511413574,8.008590698242188,7.819448471069336,7.807568073272705,7.392760753631592,7.372259140014648,7.680557727813721,8.323735237121582,7.405981540679932,7.390983581542969,8.47547721862793,7.814692974090576,8.292101860046387,8.115535736083984,7.816876411437988,7.764798164367676,7.4307541847229,8.173230171203613,7.816125869750977,7.800354957580566,7.687350749969482,7.416838645935059,8.04630184173584,7.372389793395996,8.061315536499023,7.425755023956299,7.877260684967041,8.388016700744629,7.779290199279785,8.227629661560059,8.057212829589844,8.02259349822998,7.782587051391602,8.23399543762207,7.90402889251709,7.8760199546813965,7.4668121337890625,8.308961868286133,7.824159622192383,7.712260723114014,7.914191246032715,7.685945987701416,7.940428733825684,10.677026748657227,10.417326927185059,10.19378662109375,10.34382438659668,10.359889030456543,10.168207168579102,10.379969596862793,10.382912635803223,10.3951997756958,10.006451606750488,10.593812942504883,10.523648262023926,10.564718246459961,10.390353202819824,10.409890174865723,10.375975608825684,10.270461082458496,10.380226135253906,10.342330932617188,10.389986038208008,10.570111274719238,10.563359260559082,10.393143653869629,10.445023536682129,10.406803131103516,10.406514167785645,9.976093292236328,10.553617477416992,10.514777183532715,10.404207229614258,10.352548599243164,10.402541160583496,10.373939514160156,10.66649055480957,10.147748947143555,10.177634239196777,10.380538940429688,10.481019020080566,10.569191932678223,10.38936710357666,10.453319549560547,10.417245864868164,10.462665557861328,10.532641410827637,10.139423370361328,10.995478630065918,10.441753387451172,10.408876419067383,10.595641136169434,10.508939743041992,10.599928855895996,10.383362770080566,10.315134048461914,10.520706176757812,10.196410179138184,10.421682357788086,10.112748146057129,10.42435073852539,9.957427978515625,9.407848358154297,9.24934196472168,9.988895416259766,9.616469383239746,9.254940032958984,9.619887351989746,9.763313293457031,9.41704273223877,9.903557777404785,9.640640258789062,9.904218673706055,9.833464622497559,9.501309394836426,10.281272888183594,9.5150146484375,9.301063537597656,9.468867301940918,9.583669662475586,9.652227401733398,9.2976655960083,9.610597610473633,9.950066566467285,9.628329277038574,9.354300498962402,9.685493469238281,9.437472343444824,9.71446418762207,9.970356941223145,9.591911315917969,9.4934720993042,11.245697975158691,9.630011558532715,9.857994079589844,9.644820213317871,9.684257507324219,9.61834716796875,9.774521827697754,9.085481643676758,10.036049842834473,9.738116264343262,9.506299018859863,9.30492877960205,9.699036598205566,9.771791458129883,9.192770957946777,9.514463424682617,9.63692855834961,9.843480110168457,9.38808536529541,9.520946502685547,9.217379570007324,9.838109970092773,9.648056030273438,9.77692699432373,9.458232879638672,9.670414924621582,9.680063247680664,9.057123184204102,9.02020263671875,8.764460563659668,9.149687767028809,8.96601676940918,8.83845329284668,7.854395389556885,8.691113471984863,8.074891090393066,7.991252422332764,8.15771198272705,8.11121940612793,7.889158725738525,8.559086799621582,7.732449054718018,8.129582405090332,7.739405155181885,8.906782150268555,8.779287338256836,8.384539604187012,8.096807479858398,7.73988151550293,7.74143123626709,10.807848930358887,8.938881874084473,8.25653076171875,8.156562805175781,8.910407066345215,8.929011344909668,8.912620544433594,8.349308967590332,9.150344848632812,9.063459396362305,8.188298225402832,8.082099914550781,8.124395370483398,8.095884323120117,7.761401653289795,7.85822057723999,7.735099792480469,9.143465995788574,7.9051079750061035,8.62359619140625,8.771903991699219,8.10483455657959,8.881553649902344,7.842296600341797,7.751319408416748,8.932684898376465,8.793137550354004,8.686760902404785,8.676131248474121,8.958822250366211,7.80643892288208,8.740694046020508,7.7468767166137695,8.771867752075195,9.189112663269043,9.039446830749512,9.115840911865234,9.096116065979004,9.138891220092773,8.879927635192871,8.893594741821289,8.761467933654785,8.84201431274414,8.92170524597168,8.984542846679688,8.870153427124023,9.148591041564941,9.1036376953125,8.759328842163086,9.173047065734863,9.000730514526367,9.311477661132812,9.226938247680664,8.916337966918945,9.266477584838867,8.85345458984375,8.894457817077637,9.150871276855469,8.838128089904785,8.816519737243652,9.046035766601562,8.901216506958008,9.159720420837402,8.813364028930664,9.301010131835938,8.885666847229004,9.035273551940918,9.151689529418945,9.173745155334473,9.153688430786133,9.068913459777832,9.077773094177246,8.93253231048584,9.23687744140625,8.845597267150879,8.906045913696289,9.104510307312012,8.794829368591309,8.974413871765137,8.559188842773438,9.103609085083008,8.886739730834961,8.847504615783691,8.852912902832031,8.7335786819458,8.971410751342773,8.835782051086426,8.914285659790039,9.40961742401123,9.401433944702148,9.173538208007812,9.561808586120605,9.133511543273926,9.540070533752441,9.700275421142578,9.41574478149414,9.289504051208496,9.243343353271484,9.41695785522461,9.496110916137695,9.767770767211914,9.445943832397461,9.416340827941895,9.574084281921387,9.292332649230957,9.226899147033691,9.462366104125977,9.370545387268066,9.44404411315918,9.387925148010254,9.353792190551758,9.420747756958008,9.548852920532227,9.489824295043945,9.414810180664062,9.33342456817627,9.402825355529785,9.672316551208496,9.393604278564453,9.239797592163086,9.441904067993164,9.516083717346191,9.340642929077148,9.363749504089355,9.59388256072998,9.435190200805664,9.716001510620117,9.322286605834961,9.686580657958984,9.433467864990234,9.423392295837402,9.48121166229248,9.288214683532715,9.267616271972656,9.553618431091309,9.478880882263184,9.371923446655273,9.760424613952637,9.519524574279785,9.401512145996094,9.502867698669434,8.319539070129395,8.872060775756836,8.969176292419434,8.707355499267578,8.74543285369873,8.927281379699707,8.319100379943848,8.32375717163086,8.666680335998535,8.335197448730469,8.70714282989502,9.042021751403809,8.712772369384766,8.291357040405273,8.325663566589355,8.38085651397705,8.75160026550293,8.50749397277832,8.352300643920898,8.444624900817871,8.73957347869873,8.366410255432129,8.543909072875977,8.721636772155762,8.295104026794434,8.693644523620605,8.322171211242676,8.898469924926758,8.4146146774292,8.773049354553223,8.323225975036621,9.074934959411621,8.57706356048584,8.7324857711792,8.81857967376709,9.022188186645508,8.334091186523438,8.382010459899902,8.761948585510254,8.723730087280273,9.060052871704102,8.774007797241211,8.736745834350586,8.403592109680176,9.92600154876709,9.90449047088623,9.936745643615723,9.92866325378418,9.815776824951172,9.909724235534668,9.910490989685059,9.891134262084961,9.903509140014648,9.932821273803711,9.916892051696777,9.828373908996582,9.930082321166992,9.88005256652832,9.91939926147461,10.002853393554688,9.763079643249512,9.934072494506836,9.936637878417969,9.912662506103516,9.736724853515625,9.912676811218262,9.76375961303711,9.90211296081543,9.953424453735352,9.921619415283203,9.907418251037598,9.980151176452637,9.924909591674805,10.513494491577148],\"y\":[1.3817375898361206,1.0612719058990479,1.1385481357574463,1.08525550365448,1.1626895666122437,1.2480696439743042,0.4848286211490631,1.2720167636871338,1.1438474655151367,1.3171401023864746,1.4076839685440063,1.264595627784729,1.0884864330291748,1.1517572402954102,1.355188012123108,1.7229746580123901,0.972616970539093,1.1486741304397583,0.8810750246047974,1.257293701171875,0.9572843313217163,0.8085752129554749,0.9309483766555786,0.7958930730819702,1.0975350141525269,0.6360377073287964,0.9854050874710083,1.798175573348999,1.3100166320800781,0.67192143201828,1.2066819667816162,1.2865713834762573,0.9056100845336914,0.6249911189079285,1.7463622093200684,1.2420947551727295,0.7832163572311401,1.1565204858779907,1.3271960020065308,1.3933436870574951,1.0287328958511353,0.7349646091461182,1.1217514276504517,1.414216160774231,1.152062177658081,1.4249526262283325,1.1767480373382568,1.2981083393096924,1.1263619661331177,1.0864992141723633,0.5083565711975098,1.0866432189941406,1.8492233753204346,1.125476598739624,0.944848895072937,1.1385208368301392,0.678071916103363,1.3165032863616943,0.6076777577400208,1.3482673168182373,1.6210135221481323,0.9811966419219971,1.2457525730133057,1.171410322189331,1.1819112300872803,0.8604665398597717,0.942077100276947,0.516913115978241,1.7408256530761719,1.4816997051239014,1.3886042833328247,1.7745593786239624,0.794830322265625,0.5049740076065063,0.8629714250564575,0.9947225451469421,1.3124425411224365,1.181308388710022,0.65378338098526,0.9567456841468811,0.8112806677818298,1.0578453540802002,1.270293116569519,1.1971663236618042,1.2050573825836182,0.49335476756095886,1.5063146352767944,1.3702335357666016,1.2522984743118286,1.2417391538619995,1.4349349737167358,1.29513418674469,1.3764736652374268,1.1924550533294678,1.3044599294662476,1.0554274320602417,0.9203598499298096,1.1263551712036133,1.4461302757263184,1.4113985300064087,1.1233012676239014,1.292132019996643,1.383273959159851,1.2921757698059082,1.4860016107559204,1.4392153024673462,1.4001719951629639,0.9837578535079956,0.6659143567085266,0.9185031056404114,1.1586668491363525,1.4281121492385864,1.1629738807678223,0.510953962802887,0.5897344946861267,0.8766728639602661,1.4713213443756104,1.1049691438674927,0.5188847184181213,1.4383823871612549,1.3952010869979858,1.3247922658920288,0.9874094128608704,1.13014554977417,0.8806256651878357,1.1321403980255127,1.1713860034942627,1.29535710811615,1.1741254329681396,0.570444643497467,1.2960554361343384,0.7852280735969543,0.9467426538467407,1.7510077953338623,1.3233586549758911,1.1152814626693726,1.0483297109603882,0.647891640663147,0.5596342086791992,1.3211227655410767,0.9677881002426147,0.6502654552459717,1.0543253421783447,0.6772043704986572,1.212782382965088,1.1973955631256104,1.1335924863815308,1.4092952013015747,1.3452917337417603,0.6293814778327942,1.8506160974502563,1.066854476928711,1.2767295837402344,0.7192075848579407,1.712160587310791,0.8005959391593933,1.1127575635910034,-0.2791852056980133,-0.56585294008255,-0.33805036544799805,-0.5237462520599365,0.26701799035072327,-0.765414834022522,-0.7155061364173889,-0.6682496070861816,-0.6063860654830933,-0.3886547386646271,-0.6360691785812378,-0.5970664024353027,-0.6077590584754944,-0.3316837251186371,0.027478214353322983,-1.0895836353302002,-0.5561929941177368,0.06399431079626083,0.6634865999221802,-0.7742803692817688,0.14976142346858978,-1.053679347038269,-0.8100492358207703,-0.6574156284332275,-0.6182628273963928,2.9431962966918945,-0.8820046186447144,-0.8308306336402893,-0.7561229467391968,0.06208992749452591,-1.2723103761672974,0.021186748519539833,-0.9865592122077942,0.42442458868026733,-0.5611394047737122,-0.19294996559619904,-0.6945754289627075,-0.3245929181575775,0.14573052525520325,-0.8709970712661743,-0.352821946144104,-0.20292386412620544,-0.3928586542606354,-0.45419180393218994,-0.6536911725997925,-0.43990081548690796,0.47350263595581055,0.13158506155014038,-0.7095873355865479,-1.041664481163025,-0.30340245366096497,0.33025580644607544,-0.5825144648551941,-0.9634666442871094,0.1646973192691803,-0.9077123999595642,0.321090430021286,-0.4256440997123718,0.016457444056868553,-0.3007483184337616,-0.9926905035972595,-0.40955081582069397,-0.7163980603218079,-0.7613950967788696,-0.5563022494316101,-0.30304890871047974,-0.6749861836433411,-0.5403029322624207,-0.3122972548007965,-0.610802948474884,-0.7764879465103149,-0.3963066339492798,-0.9999945163726807,-0.2833714783191681,-0.40003347396850586,-0.9637208580970764,-0.45351165533065796,-0.5818671584129333,-0.7205086350440979,-0.9992696046829224,0.6563940644264221,-1.185823917388916,-1.0215363502502441,-0.13112245500087738,-0.6537771224975586,-0.7294257283210754,-0.7630423307418823,-0.7293157577514648,-0.9494206309318542,-0.3285471796989441,-0.1588168889284134,-0.36618950963020325,-0.9938457608222961,0.16926676034927368,-0.4724136292934418,0.05542946234345436,-0.2140544056892395,-0.305911123752594,-0.019443433731794357,0.04754577577114105,-0.5232704877853394,-0.11347304284572601,-0.42826223373413086,-0.3631307780742645,-0.34572187066078186,-0.06021670997142792,0.02899486944079399,-0.5741272568702698,-0.17813809216022491,-0.3307811915874481,-0.5624246597290039,-0.33823102712631226,-0.49408844113349915,-0.2875242233276367,-0.5673142671585083,-0.5054513216018677,-0.19457708299160004,0.04647556692361832,0.15025267004966736,-0.4029132127761841,-0.09265140444040298,-0.06226486340165138,-0.49906080961227417,-0.42240944504737854,-0.37416812777519226,-0.4376997649669647,-0.5433786511421204,-0.48809999227523804,-0.3587982952594757,-0.1461024433374405,-0.4376213848590851,-0.5315554738044739,-0.5797322392463684,2.760768413543701,-0.5617931485176086,-0.37373608350753784,1.0235728025436401,-0.2975458800792694,-0.12453179061412811,-0.39394140243530273,0.08051753044128418,-0.5586355328559875,-0.5786116719245911,-0.5049486756324768,0.1441514790058136,-0.7706948518753052,-0.21628828346729279,-0.3426320552825928,-0.16151897609233856,-0.39145922660827637,-0.5925371646881104,-0.2956721782684326,0.027539370581507683,-0.30378881096839905,0.042292892932891846,-0.12347960472106934,-0.06413992494344711,-0.6200217604637146,-0.4954264461994171,-0.34516313672065735,3.190534830093384,-0.2043280303478241,-0.5706292986869812,-0.7155386209487915,0.12354090809822083,-0.219037726521492,-0.293334424495697,0.07163935899734497,-0.5650919675827026,-0.29002729058265686,0.07026058435440063,-0.5817172527313232,-0.10595829039812088,-1.2771755456924438,-0.2213153839111328,0.0876251682639122,-0.7389064431190491,-0.14196763932704926,-0.6306360363960266,0.011064082384109497,0.014209670014679432,-0.09134206920862198,-0.2708755433559418,-0.20944665372371674,-0.15542514622211456,-0.016267064958810806,0.33848637342453003,1.3138884241925552e-05,-0.3145374655723572,-0.37487006187438965,0.08202043920755386,-0.06734184175729752,-0.26890313625335693,-0.2482864111661911,-0.37177231907844543,-0.342668354511261,0.05140113830566406,-0.009848969988524914,-0.26512715220451355,-0.21851959824562073,-0.20532433688640594,-0.34504765272140503,-0.12112276256084442,-0.11948814243078232,-0.36536410450935364,-0.43950170278549194,-0.3522815406322479,-0.16667716205120087,-0.33151018619537354,-0.4151010513305664,-0.21129287779331207,0.18422266840934753,-0.13335680961608887,0.17345526814460754,0.07078302651643753,-0.06395117193460464,-0.33869442343711853,-0.20178943872451782,-0.20231139659881592,-0.02338528260588646,0.17116601765155792,-0.3580891191959381,-0.31286415457725525,-0.32009008526802063,-0.1331133246421814,0.18067573010921478,-0.2697615623474121,0.08013986051082611,-0.31005680561065674,0.028209263458848,0.10702250152826309,0.2667584717273712,-0.3471834659576416,-0.3380453586578369,-0.07149399816989899,-0.4023083448410034,-0.10830654948949814,-0.17893050611019135,-0.2754707932472229,0.22065694630146027,-0.1637326180934906,-0.12212429195642471,-0.17535138130187988,-0.2760526239871979,-0.01606772653758526,0.11322040110826492,0.021302759647369385,1.5433651208877563,1.4605567455291748,1.4761929512023926,1.4625173807144165,1.4584766626358032,1.4409211874008179,1.4749916791915894,1.4284682273864746,1.4748163223266602,1.4806615114212036,1.4631659984588623,1.411305546760559,1.4848947525024414,1.0450985431671143,1.4943875074386597,1.3517125844955444,1.4878875017166138,1.4422603845596313,1.5346200466156006,1.5406434535980225,1.499199628829956,1.4141327142715454,1.4414336681365967,1.3886102437973022,1.4608150720596313,1.4566928148269653,1.5145379304885864,1.6541327238082886,1.5264090299606323,1.531654953956604,1.4845945835113525,1.4692519903182983,1.5288569927215576,1.5096898078918457,1.3595173358917236,1.587659478187561,1.4848111867904663,1.480909824371338,1.49443781375885,1.6546357870101929,1.5368572473526,1.2316579818725586,1.3251349925994873,1.5530869960784912,1.5514334440231323,1.5379899740219116,1.494699239730835,1.525313377380371,1.4642879962921143,1.526352882385254,1.288293480873108,1.483452320098877,1.4212583303451538,1.5053179264068604,1.3147481679916382,1.3840261697769165,1.5364909172058105,1.516196370124817,1.4429274797439575,1.4955360889434814,1.5462852716445923,1.454986810684204,1.4761298894882202,1.4729735851287842,1.450022578239441,1.443420648574829,1.4701296091079712,4.530323028564453,4.6652750968933105,4.259049415588379,4.17231559753418,4.669703960418701,4.087711334228516,4.67042350769043,4.63741397857666,4.61146879196167,4.00691556930542,4.465812683105469,4.730312824249268,3.997835874557495,4.689051151275635,4.215987682342529,4.713289260864258,3.9740796089172363,4.662024021148682,4.6290106773376465,4.683803558349609,4.155202865600586,4.592298984527588,4.655975341796875,4.570188999176025,4.67529296875,4.149698734283447,4.111392498016357,4.625186920166016,4.129754066467285,4.608168125152588,4.609062194824219,4.684344291687012,4.598238468170166,4.515726089477539,4.097564697265625,4.126251697540283,4.632607936859131,3.846534013748169,4.030094146728516,4.677065372467041,4.595035552978516,4.631924152374268,4.152017593383789,4.6212286949157715,4.092752933502197,4.058029651641846,4.524569034576416,4.68303918838501,4.761435031890869,4.133310794830322,4.037563800811768,4.604251384735107,4.569831848144531,4.141808032989502,4.4533562660217285,4.653535842895508,4.054136753082275,4.630998134613037,3.962613344192505,1.5877009630203247,1.2727618217468262,1.4802215099334717,1.6104549169540405,1.4569437503814697,1.539625644683838,1.472213864326477,2.0593249797821045,1.3333063125610352,1.6195672750473022,0.646668553352356,1.6028765439987183,1.468073844909668,1.4336971044540405,1.469533920288086,1.5507453680038452,1.6590402126312256,1.5533472299575806,1.5326228141784668,1.4909952878952026,1.5933552980422974,0.6876698732376099,1.5868359804153442,1.573804259300232,1.5753282308578491,1.2330971956253052,1.6163599491119385,0.7645369172096252,1.5960463285446167,1.578489899635315,0.5172238349914551,1.5027610063552856,1.38727867603302,1.5584850311279297,1.571655035018921,1.5743860006332397,1.5758932828903198,1.8300682306289673,1.7321003675460815,1.5929148197174072,1.4410037994384766,1.594342589378357,1.557225227355957,1.3510727882385254,1.2829564809799194,1.1802170276641846,1.5979278087615967,1.6627514362335205,1.6593539714813232,1.5607953071594238,1.3650426864624023,0.5767338275909424,1.5249618291854858,1.5589072704315186,1.6015998125076294,1.5737286806106567,1.52560555934906,1.5944088697433472,2.3929059505462646,2.3886959552764893,2.2481915950775146,2.366041660308838,2.3146793842315674,2.3162174224853516,2.369158983230591,2.5022075176239014,2.4162755012512207,2.523205280303955,2.5030629634857178,2.322309732437134,2.426605224609375,2.189079761505127,2.5072808265686035,2.1927618980407715,2.2802486419677734,2.404411792755127,2.5210015773773193,2.5109353065490723,2.188095808029175,2.2342336177825928,1.3085989952087402,2.270885467529297,2.520770311355591,2.4987809658050537,2.2820372581481934,2.2575674057006836,2.305651903152466,2.480987310409546,2.6763017177581787,2.3891241550445557,2.503016233444214,2.507106065750122,2.5494508743286133,2.4986634254455566,2.2242064476013184,2.3021676540374756,2.150664806365967,2.744899034500122,2.3470451831817627,2.496939182281494,2.4149718284606934,2.483858346939087,2.3855793476104736,2.2803268432617188,2.255380868911743,2.259631872177124,2.3189663887023926,2.3722052574157715,2.399129867553711,2.2720420360565186,2.2237091064453125,2.365219831466675,2.2338740825653076,2.3938751220703125,3.968130350112915,4.086702346801758,3.7755074501037598,3.991488218307495,3.975116014480591,4.029293060302734,4.046248435974121,4.013284683227539,4.176393032073975,4.156956672668457,4.055082321166992,3.9321210384368896,3.937089204788208,3.9232289791107178,3.8601222038269043,3.866779327392578,3.9942166805267334,3.850517511367798,3.827030897140503,4.08809232711792,3.6295218467712402,4.113508701324463,4.187417507171631,3.6063523292541504,3.9586288928985596,4.24702787399292,3.9166204929351807,4.1035475730896,3.9102394580841064,4.182351112365723,3.869297981262207,3.981309175491333,3.9789490699768066,3.9490838050842285,3.944530487060547,3.942408323287964,4.038426399230957,4.008914947509766,4.139644622802734,3.7226343154907227,4.209354877471924,4.045825958251953,3.9882144927978516,4.28642463684082,4.0150346755981445,3.615358352661133,4.036133289337158,4.028831958770752,3.9747371673583984,3.9180045127868652,3.7974116802215576,4.060174942016602,4.20427942276001,4.09943962097168,-0.5178394317626953,-0.650516152381897,-0.479851096868515,-0.4310290515422821,-0.5378267168998718,-0.6259913444519043,-0.7380387187004089,-0.21319614350795746,-0.7479232549667358,-0.610841691493988,-0.6754353642463684,-0.5430178642272949,-0.6768491268157959,-0.45748335123062134,-0.3581988215446472,-0.8913076519966125,-0.698038637638092,-0.7323350310325623,-0.4265297055244446,-0.6116015911102295,-0.9226518869400024,-0.7730879783630371,-0.7615750432014465,-0.7331415414810181,-0.8823621869087219,-0.8506748676300049,-0.7885114550590515,-0.723310649394989,-0.7920389175415039,-0.6305829882621765,-0.698114275932312,-0.7590868473052979,-0.9453780651092529,-0.47471097111701965,-0.7153941988945007,-0.7514808177947998,-0.32000821828842163,-0.5421023368835449,-0.5268405079841614,-0.7256784439086914,-0.6647657752037048,-0.5601599812507629,-0.8130180239677429,-0.5797785520553589,-0.76743084192276,-0.740344226360321,-0.6408556699752808,-0.4104425609111786,-0.3353818655014038,-0.6775698661804199,-0.7086683511734009,-0.9370354413986206,-0.7324972152709961,3.193037986755371,3.0153839588165283,3.2098007202148438,3.411353826522827,3.419900894165039,2.993455171585083,3.2046985626220703,3.197061538696289,3.5652174949645996,3.1496386528015137,3.4359893798828125,3.163926362991333,3.4493963718414307,3.234320640563965,3.2002310752868652,3.272998094558716,3.3818790912628174,3.3397650718688965,3.0758256912231445,3.1623947620391846,3.384615182876587,3.2408924102783203,3.358213186264038,3.416884660720825,3.167921781539917,3.4248197078704834,3.2836413383483887,3.2514102458953857,3.342782497406006,3.3918700218200684,3.194164752960205,3.047151803970337,3.55344820022583,3.2842931747436523,3.3557095527648926,3.1627235412597656,3.2219274044036865,3.0630640983581543,3.3806440830230713,3.417919874191284,3.0919387340545654,3.379578113555908,3.3994202613830566,3.3199284076690674,-2.003943920135498,-2.018934726715088,-2.0210397243499756,-1.9936052560806274,-1.9008338451385498,-1.989972710609436,-2.0100209712982178,-1.9968105554580688,-2.0147523880004883,-2.0181238651275635,-2.0108845233917236,-1.9405567646026611,-1.9686578512191772,-1.9617516994476318,-2.0073821544647217,-1.730658769607544,-1.9098670482635498,-1.9983524084091187,-2.0032131671905518,-2.021005392074585,-1.78288996219635,-1.9908539056777954,-1.9123069047927856,-1.9989206790924072,-1.8872182369232178,-2.0079305171966553,-1.9907784461975098,-1.9225029945373535,-2.0194551944732666,1.153646469116211],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"#CFD8DC\",\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"other\",\"showlegend\":false,\"x\":[12.260786056518555,10.01115894317627,11.924004554748535,10.16279411315918,12.00646686553955,10.273680686950684,6.795833110809326,10.738099098205566,12.415397644042969,12.344215393066406,8.005266189575195,11.672365188598633,9.678071022033691,11.613967895507812,10.691057205200195,10.729043006896973,12.570730209350586,12.728907585144043,11.094691276550293,12.467524528503418,10.7769136428833,11.365942001342773,11.85193157196045,10.574968338012695,13.300118446350098,8.84424877166748,9.322251319885254,11.329916000366211,9.31579303741455,10.838811874389648,11.459781646728516,11.970154762268066,12.745402336120605,10.581709861755371,9.499332427978516,10.223129272460938,9.936136245727539,11.424376487731934,7.634429931640625,12.565068244934082,12.609332084655762,11.734045028686523,12.76197338104248,8.449498176574707,10.840675354003906,11.946686744689941,8.837322235107422,9.319323539733887,12.509225845336914,10.419879913330078,6.7955498695373535,9.324790954589844,12.908655166625977,11.152015686035156,11.986949920654297,12.218483924865723,11.286534309387207,11.993667602539062,9.96347427368164,11.670063018798828,9.893813133239746,10.930827140808105,9.840456008911133,9.265555381774902,8.651935577392578,9.824747085571289,9.816277503967285,12.087455749511719,11.567899703979492,6.796971321105957,12.668866157531738,12.218778610229492,10.99422836303711,11.2903470993042,13.343911170959473,12.024370193481445,12.494938850402832,12.23508071899414,10.537565231323242,9.071687698364258,9.39167308807373,13.654808044433594,11.47091293334961,11.624275207519531,12.246249198913574,12.476778984069824,11.365124702453613,12.479470252990723,12.073331832885742,10.643670082092285,10.698538780212402,11.801766395568848,11.808372497558594,9.954280853271484,12.55838394165039,8.872209548950195,11.538613319396973,8.866655349731445,10.459000587463379,11.395777702331543,9.855978012084961,11.591615676879883,10.30197811126709,11.654211044311523,11.18188190460205,10.506263732910156,13.050704002380371,11.941532135009766,12.512845993041992,11.652451515197754,13.646256446838379,11.867741584777832,12.657735824584961,12.094124794006348,10.286686897277832,12.581634521484375,11.336400985717773,9.937158584594727,11.021777153015137,9.948188781738281,12.35839557647705,12.886040687561035,9.261913299560547,9.89763355255127,9.462775230407715,9.968618392944336,12.362512588500977,10.358255386352539,8.327774047851562,11.927535057067871,10.00265884399414,12.349808692932129,10.623787879943848,10.678670883178711,11.847443580627441,9.692587852478027,9.262121200561523,10.958932876586914,12.889772415161133,10.948387145996094,11.634035110473633,12.080227851867676,10.378564834594727,10.096247673034668,11.42621898651123,8.7933931350708,11.231094360351562,8.999340057373047,11.527088165283203,12.19135570526123,7.8714375495910645,11.001914978027344,12.61523151397705,9.75306224822998,11.896245002746582,12.523083686828613,9.378335952758789,10.35689926147461,11.08590316772461,13.147154808044434,9.818265914916992,11.11213493347168,11.444499969482422,11.590463638305664,10.737903594970703,9.770731925964355,10.78557014465332,11.933521270751953,10.511313438415527,9.82026481628418,8.356404304504395,10.73415470123291,10.45881462097168,10.396477699279785,12.194050788879395,12.752180099487305,11.78748607635498,10.255226135253906,12.165645599365234,12.292634963989258,12.844697952270508,10.628177642822266,8.569395065307617,11.763611793518066,12.15890884399414,12.740022659301758,12.746966361999512,9.62110424041748,11.45886516571045,10.926743507385254,12.772931098937988,12.837591171264648,12.200713157653809,9.013678550720215,10.967081069946289,9.76983642578125,12.884422302246094,11.670434951782227,13.589189529418945,12.576118469238281,11.409375190734863,9.541470527648926,8.66622257232666,11.438347816467285,11.34601879119873,10.590363502502441,11.35364818572998,11.930695533752441,6.797292709350586,10.794591903686523,10.309884071350098,10.247868537902832,9.914916038513184,13.303763389587402,12.540189743041992,11.354313850402832,8.383038520812988,12.193739891052246,9.227989196777344,9.241358757019043,9.536521911621094,10.984012603759766,12.313669204711914,9.644474029541016,9.546388626098633,9.345878601074219,11.460395812988281,10.699294090270996,9.629450798034668,8.48170280456543,11.0349760055542,10.570517539978027,10.367918014526367,11.483748435974121,10.360127449035645,10.180638313293457,12.789478302001953,10.665085792541504,11.708563804626465,12.97873592376709,11.337641716003418,10.709122657775879,13.167486190795898,12.409531593322754,8.32143497467041,9.063648223876953,10.371134757995605,11.557292938232422,10.586905479431152,9.22173023223877,11.61303424835205,8.834911346435547,13.149587631225586,11.683677673339844,9.396690368652344,11.555591583251953,11.163105964660645,10.546306610107422,13.552511215209961,12.85168170928955,11.200281143188477,9.939804077148438,10.729233741760254,12.406366348266602,11.353873252868652,11.169713973999023,9.460506439208984,10.654193878173828,11.77177906036377,11.467199325561523,12.088428497314453,13.498458862304688,11.279095649719238,12.567978858947754,10.763456344604492,10.033524513244629,12.673567771911621,10.10020637512207,8.970285415649414,9.142569541931152,11.128690719604492,12.259600639343262,8.503023147583008,11.229401588439941,9.610413551330566,10.481382369995117,12.30782699584961,11.052600860595703,13.069833755493164,9.056635856628418,9.835722923278809,11.540289878845215,10.57509994506836,10.630483627319336,12.53976058959961,12.878406524658203,12.195188522338867,12.416590690612793,8.807579040527344,12.123668670654297,10.993809700012207,11.357285499572754,11.609417915344238,8.505378723144531,9.010087966918945,9.596749305725098,12.824377059936523,10.293586730957031,11.473138809204102,12.474746704101562,10.027557373046875,12.210655212402344,10.91020393371582,13.3267240524292,13.175515174865723,11.291525840759277,11.628263473510742,11.242234230041504,9.939929008483887,12.68603515625,9.105117797851562,12.55679702758789,10.582772254943848,11.557234764099121,12.54797649383545,11.068909645080566,12.6818208694458,11.577909469604492,11.18771743774414,13.261770248413086,11.224922180175781,11.866551399230957,9.587552070617676,10.551297187805176,11.903278350830078,12.07540225982666,10.768095970153809,12.016518592834473,8.86601448059082,12.697779655456543,12.205658912658691,10.315733909606934,11.997912406921387,12.761860847473145,11.485032081604004,12.877477645874023,10.77319622039795,9.71633529663086,12.789429664611816,10.579119682312012,12.169694900512695,11.369839668273926,12.047872543334961,11.594593048095703,11.944458961486816,11.05200481414795,9.081618309020996,12.835399627685547,12.314804077148438,9.093341827392578,11.577603340148926,9.215981483459473,11.374435424804688,12.266894340515137,11.160942077636719,8.430356979370117,12.54307746887207,11.239245414733887,10.539368629455566,11.465906143188477,7.657243728637695,12.263589859008789,9.044021606445312,11.270294189453125,9.69029426574707,12.90316104888916,11.74852466583252,8.697430610656738,10.244534492492676,13.194223403930664,8.26854419708252,12.263144493103027,12.15650463104248,10.97741985321045,11.557283401489258,12.207938194274902,9.880388259887695,13.715070724487305,8.669084548950195,11.517677307128906,12.063190460205078,10.891190528869629,8.157964706420898,13.416830062866211,12.172284126281738,9.77486515045166,12.416096687316895,8.317665100097656,12.529857635498047,10.84004020690918,8.873795509338379,12.759370803833008,10.572245597839355,11.383156776428223,11.408194541931152,12.709611892700195,13.176079750061035,7.645970344543457,10.45201587677002,11.888107299804688,8.155491828918457,10.934144973754883,12.047857284545898,11.55852222442627,10.887688636779785,10.773085594177246,10.705704689025879,9.601393699645996,10.455825805664062,9.596738815307617,10.703749656677246,11.308526992797852,9.27015209197998,10.01099681854248,12.847784996032715,12.372241020202637,10.8928804397583,11.546853065490723,9.608847618103027,10.049355506896973,10.234009742736816,12.77387809753418,7.639250755310059,9.369508743286133,11.111124992370605,10.788805961608887,9.64129638671875,10.971961975097656,13.150918960571289,10.595973014831543,9.54521369934082,12.856410026550293,10.756369590759277,11.570320129394531,10.633820533752441,10.61214542388916,11.196186065673828,11.974361419677734,11.977964401245117,9.437406539916992,11.594237327575684,11.464212417602539,10.141727447509766,11.141068458557129,9.565552711486816,12.504912376403809,10.439518928527832,8.916755676269531,12.542708396911621,8.100579261779785,13.614336013793945,9.942835807800293,12.88668441772461,12.289319038391113,10.191901206970215,11.653940200805664,12.273872375488281,9.7015380859375,11.526108741760254,11.894497871398926,9.942673683166504,12.299449920654297,11.311140060424805,12.881195068359375,8.470643043518066,12.338445663452148,11.502923011779785,10.742378234863281,12.947842597961426,11.659990310668945,12.496929168701172,8.366921424865723,9.1043119430542,12.329166412353516,11.74715518951416,11.585480690002441,10.612318992614746,11.377769470214844,11.649738311767578,11.907842636108398,13.692667007446289,11.34955883026123,12.52751636505127,8.80586051940918,12.406229019165039,12.7645902633667,12.445311546325684,11.75776195526123,11.090094566345215,11.42638111114502,10.89482593536377,10.761418342590332,11.150059700012207,10.034558296203613,10.338700294494629,11.178735733032227,12.835885047912598,11.53531551361084,12.381514549255371,11.54314136505127,11.90941047668457,9.358724594116211,10.740930557250977,11.939253807067871,11.149989128112793,11.659534454345703,10.353327751159668,11.331498146057129,10.64175796508789,12.81151008605957,8.662858009338379,10.857192993164062,10.787764549255371,8.953280448913574,12.585387229919434,7.643540382385254,10.447787284851074,10.80845832824707,12.5751371383667,9.113858222961426,9.70373821258545,12.737845420837402,11.416006088256836,12.489952087402344,8.133481979370117,10.278565406799316,11.977149963378906,9.470913887023926,10.66490364074707,10.00718879699707,9.176525115966797,11.815064430236816,8.752781867980957,11.483132362365723,11.89669418334961,12.553421020507812,13.118978500366211,9.782742500305176,10.74406623840332,12.168991088867188,11.756896018981934,12.940293312072754,11.56460189819336,8.64371109008789,11.702585220336914,11.278419494628906,10.558026313781738,11.820279121398926,10.459695816040039,11.502100944519043,11.530735969543457,10.50390911102295,12.36990737915039,12.139140129089355,11.131754875183105,10.48403263092041,13.041622161865234,11.032854080200195,11.734776496887207,10.479324340820312,9.945302963256836,11.825505256652832,11.676788330078125,9.411858558654785,12.318131446838379,11.462925910949707,12.87320613861084,13.661906242370605,12.474398612976074,10.583564758300781,11.804007530212402,11.14046573638916,11.841476440429688,11.781903266906738,9.183506965637207,11.8534574508667,11.49117660522461,11.63294506072998,11.224637985229492,12.764777183532715,12.330574035644531,11.128299713134766,12.93774127960205,10.32089614868164,12.478330612182617,11.827890396118164,11.804841041564941,11.44624137878418,11.820039749145508,10.238183975219727,7.784416675567627,12.978934288024902,12.451269149780273,9.322071075439453,11.821974754333496,10.541068077087402,11.961499214172363,8.238553047180176,12.170052528381348,12.736010551452637,12.764181137084961,11.65296745300293,12.675060272216797,11.974373817443848,10.117522239685059,9.6543607711792,12.757356643676758,12.056167602539062,9.412654876708984,11.711962699890137,9.174324989318848,10.823249816894531,9.90158462524414,10.913339614868164,8.113393783569336,9.993379592895508,12.194329261779785,13.33237361907959,11.182921409606934,12.732467651367188,13.046422004699707,9.75806713104248,8.685029029846191,11.561057090759277,10.053182601928711,11.998501777648926,13.20248031616211,12.104700088500977,10.906420707702637,11.117636680603027,10.446064949035645,9.715156555175781,12.530933380126953,12.754565238952637,11.461576461791992,11.491990089416504,9.396021842956543,8.587562561035156,12.263270378112793,11.42301082611084,13.491022109985352,10.364217758178711,11.991475105285645,12.44273853302002,11.70559310913086,10.35807991027832,9.996321678161621,9.326005935668945,12.807846069335938,13.570623397827148,11.12037467956543,10.716169357299805,10.50700569152832,10.22071361541748,12.695380210876465,11.297102928161621,9.085335731506348,12.861201286315918,11.170536994934082,10.041293144226074,10.805912017822266,11.3994722366333,9.783839225769043,11.411025047302246,10.608845710754395,9.42233657836914,11.715644836425781,11.849699020385742,12.349161148071289,8.975476264953613,10.749741554260254,9.420133590698242,10.120325088500977,12.375459671020508,10.686391830444336,11.969694137573242,11.85922622680664,12.443221092224121,9.37954330444336,13.088716506958008,10.281312942504883,12.127240180969238,11.985560417175293,12.442952156066895,12.119172096252441,9.390953063964844,10.271842956542969,10.183282852172852,12.758428573608398,9.88254451751709,11.704732894897461,11.48423957824707,9.189234733581543,11.172453880310059,11.48064136505127,10.315546035766602,12.061239242553711,11.559906005859375,11.86010456085205,11.664885520935059,12.843629837036133,9.624467849731445,12.458562850952148,12.246566772460938,11.838500022888184,10.999716758728027,9.12990951538086,11.840607643127441,11.96013069152832,11.915284156799316,12.08629035949707,10.903203964233398,8.849540710449219,11.167780876159668,10.531004905700684,12.467206954956055,10.145415306091309,9.549249649047852,6.795407295227051,12.180326461791992,12.817878723144531,10.757889747619629,11.568016052246094,11.672552108764648,7.97341775894165,10.022518157958984,9.927380561828613,10.70942211151123,11.941184043884277,12.794625282287598,12.879822731018066,12.178878784179688,11.878340721130371,10.52446460723877,10.071585655212402,10.56197738647461,11.002744674682617,12.245721817016602,10.67922592163086,12.140796661376953,12.277287483215332,11.456153869628906,12.081787109375,11.62030029296875,10.880305290222168,10.69717025756836,9.24069881439209,12.846972465515137,10.142430305480957,12.146927833557129,9.958215713500977,9.50244140625,10.51760196685791,10.551128387451172,11.390852928161621,11.688502311706543,11.491974830627441,12.741220474243164,12.295635223388672,12.218851089477539,8.685344696044922,13.047622680664062,11.958925247192383,9.383042335510254,11.218199729919434,11.51757526397705,9.624102592468262,13.13804817199707,12.326669692993164,12.624595642089844,12.161945343017578,9.43790054321289,11.324953079223633,8.503750801086426,11.257233619689941,10.58021354675293,9.62783145904541,9.703179359436035,11.924786567687988,8.906972885131836,9.254066467285156,12.899577140808105,11.277064323425293,11.152223587036133,12.76430606842041,10.148778915405273,12.378602027893066,11.560495376586914,12.818056106567383,12.91047191619873,8.343107223510742,10.998586654663086,11.666386604309082,10.141661643981934,11.702606201171875,10.432024955749512,10.108580589294434,9.184564590454102,13.428140640258789,11.512848854064941,12.044175148010254,11.576578140258789,12.28610610961914,11.385955810546875,12.952836036682129,12.021284103393555,11.287923812866211,12.099640846252441,12.082901000976562,12.028682708740234,12.383368492126465,8.773681640625,12.879551887512207,11.319714546203613,8.952622413635254,11.267468452453613,12.477086067199707,12.503398895263672,9.42236614227295,11.20785140991211,11.332497596740723,8.951685905456543,10.856745719909668,10.309080123901367,11.73598861694336,8.723488807678223,12.038217544555664,9.878068923950195,12.995759963989258,10.974395751953125,9.64431381225586,10.158196449279785,13.631218910217285,8.542502403259277,11.165329933166504,10.789525032043457,13.625770568847656,11.960083961486816,12.425254821777344,11.940552711486816,13.470690727233887,13.038926124572754,7.834033012390137,9.87307071685791,11.216033935546875,12.414644241333008,12.093451499938965,11.877202987670898,12.003032684326172,10.026086807250977,12.491192817687988,13.477025032043457,12.77425479888916,10.46001148223877,8.880654335021973,9.948962211608887,10.48256778717041,12.090274810791016,11.583839416503906,11.40567684173584,6.7874345779418945,9.088058471679688,11.758285522460938,11.31846809387207,11.73054313659668,12.907270431518555,12.770581245422363,11.114065170288086,11.227639198303223,10.648099899291992,12.066269874572754,13.3859224319458,12.3936767578125,11.536118507385254,12.09136962890625,7.971031665802002,10.897802352905273,9.173691749572754,11.609256744384766,12.269068717956543,9.517958641052246,11.856656074523926,11.710532188415527,12.882142066955566,12.293147087097168,9.226627349853516,11.812376022338867,9.573324203491211,11.98694896697998,11.025315284729004,12.620606422424316,12.66535758972168,11.626893997192383,11.87524127960205,9.740761756896973,11.66956615447998,9.677865982055664,13.708874702453613,13.700621604919434,10.8056058883667,12.719548225402832,11.30461597442627,9.15710163116455,11.5343599319458,9.219347953796387,11.886124610900879,11.679952621459961,12.069474220275879,12.361296653747559,10.542925834655762,10.790267944335938,12.395968437194824,10.26074504852295,12.145330429077148,11.74125862121582,12.40096664428711,10.930389404296875,8.466429710388184,11.920580863952637,12.118764877319336,13.445467948913574,10.998306274414062,11.986994743347168,12.903426170349121,11.159873962402344,12.18824291229248,9.372452735900879,12.712847709655762,13.078712463378906,10.631550788879395,9.169002532958984,8.790238380432129,11.811575889587402,12.28490924835205,9.769627571105957,12.567846298217773,10.349342346191406,11.191412925720215,11.537104606628418,10.127388000488281,10.513239860534668,8.915346145629883,11.823579788208008,11.909771919250488,11.520251274108887,13.776591300964355,12.594861030578613,6.803589344024658,10.791962623596191,11.956903457641602,7.651430606842041,12.864334106445312,11.632024765014648,10.91291332244873,11.907623291015625,10.763083457946777,11.546504974365234,13.262833595275879,12.147619247436523,11.872856140136719,12.41122055053711,12.41154670715332,13.054023742675781,10.615890502929688,9.164365768432617],\"y\":[1.976219654083252,1.265775442123413,1.0926563739776611,-0.5182633996009827,2.4570963382720947,0.48775091767311096,-0.910475492477417,-0.5370233654975891,1.2164555788040161,1.3136889934539795,-0.12520691752433777,1.813036561012268,1.1669845581054688,1.774245023727417,-0.3897693157196045,1.3097110986709595,3.413337469100952,1.671027660369873,0.8366751074790955,3.2766458988189697,0.3775566816329956,0.44715550541877747,2.3352620601654053,3.230710506439209,2.2592904567718506,0.6792904138565063,0.25117701292037964,0.6122933626174927,1.182349681854248,0.9189866781234741,3.0709445476531982,0.9211485385894775,3.500269889831543,3.7989087104797363,1.823943853378296,1.6083993911743164,-0.23820368945598602,2.4324207305908203,0.19927099347114563,3.7869391441345215,2.054600715637207,0.9205240607261658,3.078073263168335,0.10503587871789932,0.8371968865394592,0.3198699951171875,-0.16870951652526855,2.7976632118225098,2.6678123474121094,-0.570141077041626,-0.9106079339981079,3.7905776500701904,2.7144033908843994,0.9463844299316406,1.3737200498580933,0.33007562160491943,-0.9850686192512512,1.1608271598815918,1.0532095432281494,2.060023546218872,1.5419471263885498,1.308414101600647,-1.1935114860534668,-0.17752167582511902,-0.4857664108276367,3.832371234893799,1.3047598600387573,3.552154541015625,1.942522644996643,-0.909134566783905,2.3067994117736816,0.9871791005134583,2.9792444705963135,2.830186128616333,0.7007690072059631,3.5236191749572754,3.808678388595581,0.5856729745864868,0.5755625367164612,-0.488295316696167,0.6351603269577026,0.5080202221870422,1.0401160717010498,2.3617639541625977,1.8510946035385132,0.9317828416824341,3.2759201526641846,2.4336321353912354,-1.299113392829895,3.8617777824401855,3.202488422393799,1.037284016609192,-0.5317288637161255,-0.5715892314910889,1.2100167274475098,1.357236623764038,3.533149480819702,3.0465500354766846,0.1627165526151657,0.7375492453575134,0.9511770009994507,0.07118048518896103,-0.3919447958469391,1.378462314605713,-0.3244394361972809,0.7082329988479614,2.143453359603882,1.1407673358917236,0.9031526446342468,2.8212311267852783,1.1145505905151367,1.4429399967193604,2.296320676803589,-0.001279126270674169,-0.10712704062461853,3.0526630878448486,0.7859264612197876,-0.6041418313980103,1.40556800365448,-0.5446928143501282,1.2216320037841797,3.4056971073150635,3.86862850189209,1.1494468450546265,0.1541697382926941,-0.2646180987358093,2.934096097946167,1.085005283355713,0.1298372447490692,1.4595853090286255,0.5229455232620239,1.7139097452163696,3.179950475692749,0.8122704029083252,2.313836097717285,0.9151896834373474,0.0694771260023117,2.251661777496338,2.7393033504486084,1.8044404983520508,2.4212090969085693,1.369358777999878,1.4411309957504272,1.778269648551941,1.9663785696029663,-0.2085247039794922,1.8884222507476807,0.01605393923819065,0.9991214275360107,2.1229145526885986,-0.17486093938350677,1.2444441318511963,1.5270284414291382,1.034622311592102,0.5874497890472412,1.0864499807357788,0.7937063574790955,0.2653231918811798,0.6648417711257935,2.1787331104278564,-1.4033302068710327,0.993794858455658,3.9591336250305176,2.2620389461517334,2.2779126167297363,1.026957631111145,1.6402356624603271,1.106472134590149,3.609027624130249,4.001310348510742,2.4320061206817627,1.3439748287200928,1.444767951965332,0.8551923036575317,0.15037234127521515,2.425750494003296,2.3774797916412354,2.089541435241699,0.14875546097755432,1.2632057666778564,1.3453816175460815,3.9026010036468506,0.47671282291412354,1.526449203491211,1.530004620552063,0.44732722640037537,1.5337772369384766,-0.5403087735176086,-1.0967426300048828,0.833954393863678,0.7645677924156189,2.70750093460083,1.2675151824951172,0.1613716185092926,3.286445379257202,0.9860133528709412,3.4668221473693848,1.4706974029541016,-0.06515169888734818,3.346012830734253,3.19698166847229,1.0387732982635498,-0.47503626346588135,1.1350661516189575,-0.12550556659698486,0.48462581634521484,3.611987352371216,0.44945693016052246,-0.9105373620986938,0.9302946925163269,2.0090830326080322,0.1268618106842041,-0.5888694524765015,1.5191290378570557,2.7231905460357666,3.63657283782959,0.20960384607315063,0.04279949516057968,2.0899455547332764,2.395681142807007,-0.3550689220428467,1.1873329877853394,1.6772342920303345,-0.551196813583374,1.0045689344406128,3.5376405715942383,-0.2728564739227295,1.6083232164382935,3.451906681060791,0.4097013771533966,0.28619876503944397,2.9995458126068115,1.430558204650879,2.9486751556396484,1.4180095195770264,3.394707679748535,-1.0172475576400757,1.6186503171920776,2.2325010299682617,3.3960022926330566,2.3339428901672363,2.508234739303589,1.917661428451538,2.007206678390503,0.7356185913085938,1.3998340368270874,1.426846981048584,-0.10650590807199478,3.196619987487793,1.5687898397445679,3.3810524940490723,-0.20458725094795227,1.947638988494873,-0.17957328259944916,0.6638663411140442,1.4990065097808838,1.0839821100234985,-0.13390646874904633,1.798355221748352,1.460438847541809,2.6365201473236084,2.2331058979034424,0.5038972496986389,1.672692894935608,3.618595838546753,2.4914488792419434,-0.7637554407119751,2.602285861968994,1.6152315139770508,2.3165502548217773,4.036905288696289,1.7626341581344604,-0.30745241045951843,1.3061516284942627,3.0544943809509277,2.2432503700256348,0.5260573625564575,0.26869526505470276,0.03409036993980408,-0.5223769545555115,0.39339277148246765,2.0766706466674805,-0.38802358508110046,3.801676034927368,2.3770365715026855,3.5236270427703857,3.0124449729919434,3.137105703353882,3.03592848777771,1.0055789947509766,2.3000667095184326,3.485448122024536,2.8649325370788574,3.2783849239349365,2.067582130432129,1.7887386083602905,1.117841124534607,1.9235390424728394,-0.5475263595581055,2.5567400455474854,-0.42588913440704346,1.6459540128707886,3.0523805618286133,3.548125743865967,0.9675278663635254,1.0511583089828491,1.8744837045669556,-0.2662609815597534,2.020230531692505,2.395968198776245,1.5245238542556763,2.208904504776001,-0.23743867874145508,2.1941235065460205,3.066039800643921,0.8780515789985657,-0.3361736238002777,0.12655974924564362,0.5816671252250671,2.5916388034820557,1.8386311531066895,0.9971460103988647,-0.09463711082935333,1.4201574325561523,3.414994478225708,-0.5812013745307922,1.0859824419021606,3.207399845123291,1.0073139667510986,1.059942603111267,0.9441660642623901,2.421081066131592,-0.9699038863182068,-0.7918293476104736,3.2947211265563965,3.453516721725464,2.6633572578430176,1.12492835521698,0.9515612125396729,1.8676871061325073,2.2264864444732666,0.4588735103607178,-0.7277568578720093,-1.0359772443771362,2.857710599899292,2.6983368396759033,1.5439820289611816,0.8810650110244751,3.6186933517456055,3.788325071334839,2.0987229347229004,3.1248533725738525,-0.6990334391593933,3.2795536518096924,0.4755188822746277,2.3451409339904785,1.9918140172958374,2.239830255508423,2.980741500854492,3.4494681358337402,2.7023935317993164,0.8716931343078613,1.143310546875,0.12861472368240356,1.2377668619155884,-0.3922814130783081,3.3432180881500244,1.7890307903289795,0.9444081783294678,2.8529300689697266,0.18657220900058746,2.6263394355773926,0.48836591839790344,0.770980954170227,0.9084759950637817,2.9118480682373047,0.899668276309967,0.3940073549747467,0.8528211116790771,1.6868844032287598,0.24871914088726044,2.4808976650238037,2.190305709838867,1.4015684127807617,-0.11070117354393005,1.4967073202133179,0.9283192753791809,0.4603424668312073,1.1601427793502808,1.9638222455978394,-1.3094513416290283,1.8755643367767334,-0.034235879778862,2.5533201694488525,2.5379700660705566,3.77651309967041,2.733030319213867,0.3953371047973633,2.7038984298706055,1.0902845859527588,3.95662522315979,3.4667394161224365,1.4680428504943848,-0.2102063000202179,1.9835692644119263,0.25430288910865784,2.2354414463043213,0.19437259435653687,1.457194447517395,1.478986144065857,0.024254370480775833,0.9681276082992554,-1.2682996988296509,1.4637945890426636,0.9524343013763428,3.2517035007476807,1.0663663148880005,1.047045350074768,2.5798704624176025,1.9295272827148438,-0.4900413751602173,1.2294294834136963,3.744098663330078,1.1958093643188477,3.4060301780700684,1.4086087942123413,3.195308208465576,-0.21053600311279297,1.9594486951828003,1.3294121026992798,0.7901973128318787,-1.0279371738433838,0.18727345764636993,2.7890584468841553,1.1563249826431274,-0.1735643744468689,2.2202253341674805,0.9077820777893066,2.668337821960449,2.603579521179199,0.24776341021060944,1.158795714378357,1.6285470724105835,1.96036958694458,3.842803716659546,-0.05351594090461731,1.0603022575378418,3.5160486698150635,3.203173875808716,-0.22633570432662964,1.7337943315505981,1.1724951267242432,0.17315110564231873,0.7105343341827393,2.4431610107421875,3.8104262351989746,0.10712434351444244,0.713238537311554,3.1322901248931885,3.2645468711853027,0.534716784954071,0.5656581521034241,3.5241639614105225,2.5362460613250732,-0.7327529191970825,-0.8334300518035889,2.0500335693359375,0.9878270626068115,2.0549156665802,3.280917167663574,0.543793261051178,0.40602004528045654,1.2652292251586914,1.6847025156021118,0.6613606810569763,0.48743921518325806,1.5848220586776733,0.15714702010154724,3.097687244415283,3.4039037227630615,3.8253872394561768,-0.3216516077518463,0.36484676599502563,3.5613582134246826,0.849107563495636,2.804208278656006,3.8222780227661133,1.8004229068756104,1.4634736776351929,3.1937408447265625,0.8675617575645447,3.476331949234009,0.3727441728115082,3.1206748485565186,1.5812299251556396,-0.6251404285430908,3.3036246299743652,3.7486493587493896,1.7302109003067017,2.3743913173675537,0.9696566462516785,0.4081820845603943,0.6557876467704773,2.6788806915283203,3.448852300643921,1.163936972618103,3.5292248725891113,2.8257150650024414,3.515986680984497,1.9399545192718506,0.4077995717525482,0.766963541507721,-0.4135100543498993,0.42341694235801697,2.8088972568511963,-0.6076659560203552,0.1522863209247589,3.2291979789733887,1.588698387145996,1.8325613737106323,0.9248793125152588,1.3020033836364746,3.420180559158325,1.643860101699829,1.1540601253509521,0.21492479741573334,0.1739095002412796,1.5416576862335205,3.289651393890381,-0.5035665035247803,-0.46198034286499023,3.0262765884399414,3.655320405960083,1.8784151077270508,0.3909582197666168,0.8705004453659058,1.1364428997039795,1.208755612373352,0.26025456190109253,0.44635772705078125,0.12183551490306854,-0.15554407238960266,-0.25469839572906494,-0.45932525396347046,0.3801633417606354,2.1852316856384277,3.0103812217712402,0.16040968894958496,0.17387855052947998,2.1033482551574707,3.5950064659118652,1.6760215759277344,3.2061989307403564,0.3962271809577942,2.2076709270477295,-0.8994259834289551,2.514460802078247,0.979424774646759,3.5055930614471436,0.26043587923049927,0.08888838440179825,0.9722239375114441,1.9090890884399414,1.4592763185501099,0.7288630604743958,0.5196076035499573,2.52851939201355,0.3626737594604492,0.9660953879356384,1.4570801258087158,1.5067415237426758,1.0723236799240112,1.9673024415969849,0.5451234579086304,2.6414546966552734,0.7439343333244324,3.4168541431427,1.108274221420288,0.9759180545806885,3.0499231815338135,0.3342808485031128,1.1762615442276,1.2191663980484009,1.9902498722076416,2.7178072929382324,2.1931447982788086,1.4139740467071533,3.4444408416748047,-0.2844769358634949,2.468306541442871,2.982534408569336,1.1627116203308105,2.345379114151001,-0.26387879252433777,2.15932035446167,-0.09391316771507263,2.315988540649414,3.9019370079040527,1.4993696212768555,0.8209245204925537,0.1551581770181656,2.0490360260009766,1.3861424922943115,1.0227948427200317,-0.12519274652004242,3.113816976547241,1.2089561223983765,-0.26088747382164,1.8862637281417847,3.144066333770752,-1.0393073558807373,2.0746145248413086,1.443308711051941,1.180871844291687,-0.6809756755828857,1.017892599105835,1.8907074928283691,-1.2943123579025269,0.1569855809211731,0.6024736166000366,0.8391635417938232,2.21470046043396,-1.1607123613357544,1.8066606521606445,2.297105312347412,2.370729446411133,1.7525073289871216,0.9918195605278015,0.6040194034576416,2.8629562854766846,3.4195144176483154,1.8813725709915161,3.7637126445770264,0.20570334792137146,2.110116958618164,1.3198965787887573,1.0308681726455688,1.5510575771331787,1.0269742012023926,3.243104934692383,0.5696408748626709,-1.0560033321380615,3.80950927734375,-0.9885093569755554,2.1285574436187744,0.27239152789115906,-0.9925388693809509,2.664095401763916,0.5445898771286011,3.4431591033935547,2.9532716274261475,1.4311859607696533,2.9324100017547607,3.054269313812256,0.8955436944961548,0.025119181722402573,3.7716782093048096,0.6967297196388245,2.6491522789001465,2.385422945022583,3.098644495010376,3.803661823272705,1.4175082445144653,2.0697543621063232,3.0386083126068115,3.8905580043792725,1.805167555809021,3.370250940322876,0.6452329754829407,1.198987364768982,4.435891628265381,3.9789042472839355,0.2224683314561844,0.784505307674408,3.900918483734131,-0.08793401718139648,-0.20969590544700623,0.9383320212364197,2.9824304580688477,0.4146740138530731,3.451068878173828,0.58146733045578,2.629016876220703,2.5896897315979004,0.8220140933990479,2.19604229927063,2.3683815002441406,1.0513523817062378,-0.5379165410995483,2.0229125022888184,2.577913522720337,1.3024059534072876,2.953939914703369,1.1526473760604858,-1.245646595954895,0.6586165428161621,1.9431781768798828,0.795759916305542,0.4541209042072296,1.3323132991790771,0.9036451578140259,1.0493559837341309,2.936338424682617,1.658136010169983,2.7779719829559326,0.217825248837471,-0.08116167783737183,0.8917245864868164,1.92644202709198,1.5383085012435913,3.03080153465271,-0.5870935916900635,1.1749986410140991,1.7433806657791138,3.2300631999969482,1.3536816835403442,1.4494167566299438,2.132977247238159,1.142106533050537,1.0964027643203735,-1.2843916416168213,2.0309720039367676,0.38181358575820923,0.4180848002433777,1.397927165031433,1.0205312967300415,-1.0413587093353271,-0.3866738975048065,-0.9114582538604736,2.2314321994781494,-0.9689415693283081,0.8953730463981628,-0.7223781943321228,3.3937878608703613,-0.1200912594795227,1.4780992269515991,-1.0540677309036255,1.243822693824768,1.1921714544296265,-0.9881787300109863,2.7142724990844727,1.035949468612671,1.7600966691970825,3.096761703491211,1.0765949487686157,-0.3055479824542999,0.8458605408668518,1.8884222507476807,0.7532628178596497,3.692111015319824,2.047429084777832,1.5398664474487305,2.6263246536254883,3.3768866062164307,1.3354458808898926,2.8829421997070312,3.073280096054077,2.618222951889038,2.2130420207977295,1.131805658340454,1.4583265781402588,-0.6291894912719727,3.3120579719543457,3.235522985458374,2.4912800788879395,0.8031163215637207,2.785531520843506,1.2172937393188477,2.5934207439422607,0.44533485174179077,3.7607927322387695,0.8943837881088257,2.2763075828552246,0.15780554711818695,2.040842056274414,0.27494242787361145,3.4471356868743896,2.545928955078125,3.3168694972991943,3.057359218597412,2.0106849670410156,3.7272539138793945,-0.2229020744562149,-0.3912656009197235,2.5650482177734375,0.5276594758033752,1.9719535112380981,3.524745225906372,1.5316689014434814,0.3282936215400696,2.1106743812561035,2.4451394081115723,1.7900829315185547,2.8499183654785156,2.917431354522705,-0.6730669736862183,2.2880237102508545,2.3319878578186035,0.7816553711891174,3.079967975616455,0.3783358633518219,3.771744728088379,3.4605367183685303,3.4098503589630127,2.232668399810791,2.7577970027923584,0.8026202917098999,0.8486886024475098,1.336401104927063,1.9623465538024902,2.0950241088867188,2.3696439266204834,2.694972276687622,0.7788678407669067,1.5454350709915161,0.642559289932251,1.9104548692703247,2.817577838897705,1.2944279909133911,2.2089552879333496,2.9942898750305176,0.6504705548286438,2.6843504905700684,1.2824846506118774,-0.555378794670105,1.9917773008346558,2.4853670597076416,2.2720730304718018,0.05783434212207794,1.741772174835205,3.9805495738983154,1.6283323764801025,3.9042468070983887,-0.08662469685077667,1.0114973783493042,-0.2651676535606384,2.276035785675049,-0.6100764274597168,2.0067296028137207,1.3208794593811035,1.0322285890579224,1.9896197319030762,2.3109211921691895,2.2940375804901123,0.7046573758125305,2.1666882038116455,0.8269322514533997,1.038419246673584,3.1033222675323486,2.011228084564209,0.4995725154876709,3.0119717121124268,1.7737351655960083,1.1829763650894165,3.768042802810669,2.453611135482788,-1.2905018329620361,0.4045698344707489,-0.7276555299758911,1.108733057975769,0.7043386101722717,1.7317860126495361,3.1328790187835693,1.5019123554229736,0.00757896201685071,1.7896612882614136,4.704597473144531,-1.2845878601074219,1.471731185913086,3.7090561389923096,-0.9188922643661499,0.25417712330818176,0.40908655524253845,-0.6070359349250793,3.279542922973633,2.3822507858276367,-1.0290439128875732,-1.0308316946029663,1.6611809730529785,0.5246748328208923,1.5900543928146362,1.5552963018417358,1.9574549198150635,0.3487606346607208,1.906638503074646,-0.15992236137390137,-0.1188686341047287,1.874509334564209,2.240257501602173,3.598365068435669,2.4335744380950928,3.23817777633667,1.554663896560669,2.292228937149048,1.6409223079681396,1.495506763458252,3.2188000679016113,3.7398290634155273,2.8911819458007812,3.769780397415161,2.451735019683838,2.5036814212799072,3.150447130203247,1.2291960716247559,0.9691900610923767,3.689431667327881,-0.07314413040876389,0.73487389087677,0.839801013469696,1.4524890184402466,2.1931369304656982,1.3259851932525635,3.0612916946411133,0.2924751043319702,0.6916547417640686,2.3885679244995117,0.7535249590873718,1.8109625577926636,3.2876040935516357,2.6223843097686768,2.415797472000122,2.450838565826416,1.9013489484786987,2.8400003910064697,3.637033224105835,1.2191064357757568,0.49888309836387634,0.527450680732727,3.3894202709198,2.8021371364593506,1.5861856937408447,-0.5647964477539062,1.773905634880066,2.8972055912017822,0.4014616310596466,3.552330493927002,3.5676872730255127,0.44267264008522034,2.05513334274292,2.6361255645751953,2.9887442588806152,0.289135605096817,3.268169641494751,3.1244053840637207,0.8573437333106995,-0.19672121107578278,-0.7705172300338745,1.6629849672317505,3.544947862625122,-0.6638178825378418,3.600919008255005,-0.5163041353225708,-0.025210803374648094,1.4350130558013916,1.8289581537246704,0.6889894008636475,1.6060850620269775,-0.9072462916374207,2.4202475547790527,1.693398356437683,0.15449632704257965,3.578507661819458,2.314469575881958,4.283806324005127,2.3266613483428955,0.48093342781066895,3.2087161540985107,-0.08493365347385406,1.7356622219085693,1.580642580986023,1.29006028175354,0.9935421347618103,2.0631296634674072,2.585738182067871,1.8831286430358887],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"14_speech_asr_s2st\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"14_speech_asr_s2st\"],\"x\":[13.670157432556152,13.798901557922363,13.369604110717773,13.828919410705566,13.80864143371582,13.781868934631348,13.87149715423584,13.169575691223145,13.689194679260254,13.725122451782227,13.867948532104492,13.486695289611816,13.795697212219238,13.88539981842041,13.794876098632812,13.703969955444336,13.475425720214844,13.485220909118652,13.642592430114746,13.45029067993164,13.17920970916748,13.825347900390625,13.710762977600098,13.833584785461426,13.823281288146973,13.777862548828125,13.709671974182129,13.589035034179688,13.624615669250488,13.818294525146484,13.520774841308594,13.499052047729492,13.662908554077148],\"y\":[1.930212140083313,1.9589751958847046,1.9856090545654297,2.118469476699829,1.8433870077133179,2.083873748779297,1.9659463167190552,1.6238290071487427,1.9088102579116821,1.984571933746338,1.8491621017456055,1.8348524570465088,1.9095227718353271,2.1351234912872314,2.0434975624084473,1.9872238636016846,1.9833941459655762,1.972872018814087,1.8849225044250488,1.9644986391067505,1.6182628870010376,2.087661027908325,1.9592905044555664,1.9400484561920166,1.8442121744155884,2.052821636199951,2.1948325634002686,1.9715791940689087,1.9485830068588257,1.9467763900756836,2.033796548843384,2.05224347114563,1.9568393230438232],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"18_prompts_prompt_continuous\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"18_prompts_prompt_continuous\"],\"x\":[9.238378524780273,9.30147647857666,9.232583999633789,9.182598114013672,9.238131523132324,9.218341827392578,9.234349250793457,9.302995681762695,9.232303619384766,9.124175071716309,9.416687965393066,9.293940544128418,9.349409103393555,9.29613208770752,9.22963809967041,9.208431243896484,9.365690231323242,9.037209510803223,9.345200538635254,9.246965408325195,9.236005783081055,9.268945693969727,9.25452709197998],\"y\":[0.11233166605234146,0.2616926431655884,0.13499541580677032,0.13999022543430328,0.13154223561286926,0.03455710411071777,0.1343674212694168,0.09137420356273651,0.0880853608250618,-0.03193724900484085,0.282012939453125,0.19786401093006134,0.27887430787086487,0.23954655230045319,0.1252308189868927,0.2048257291316986,0.21515387296676636,0.2964838743209839,0.28831443190574646,0.10827220976352692,0.010295077227056026,0.17448420822620392,0.15992532670497894],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"20_chatgpt_chatgpts_rewriting\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"20_chatgpt_chatgpts_rewriting\"],\"x\":[8.408228874206543,8.558862686157227,8.468222618103027,8.394969940185547,8.488740921020508,8.462080955505371,8.458389282226562,8.648653030395508,8.45824146270752,8.496983528137207,8.497143745422363,8.60595989227295,8.652069091796875,8.46937370300293,8.726569175720215,8.704874992370605,8.474954605102539,8.499213218688965,8.526307106018066],\"y\":[0.8488311767578125,0.8990485668182373,0.930065929889679,0.9668167233467102,0.8830121159553528,0.8673952221870422,0.9546473622322083,0.9203405976295471,0.9518395662307739,0.8990188241004944,0.8826806545257568,1.001754879951477,0.8333197236061096,0.94631427526474,0.7836666703224182,0.7688788771629333,0.916922390460968,0.7554416656494141,0.88944411277771],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"21_tables_table_tableqa\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"21_tables_table_tableqa\"],\"x\":[10.386235237121582,10.229713439941406,10.389017105102539,10.301673889160156,10.361222267150879,10.3976469039917,10.311823844909668,10.32546329498291,10.373322486877441,10.308487892150879,10.342009544372559,10.355380058288574,10.305009841918945,10.051471710205078,10.35975456237793,10.3198823928833],\"y\":[-0.7522066235542297,-0.7360655665397644,-0.7566789388656616,-0.7287562489509583,-0.7565518617630005,-0.6721810698509216,-0.7266584038734436,-0.7462540864944458,-0.6999889016151428,-0.7211228609085083,-0.7179433107376099,-0.7139708399772644,-0.7397772073745728,-0.6260511875152588,-0.7241697311401367,-0.7212250828742981],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"22_oosf_instances_proxy\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"22_oosf_instances_proxy\"],\"x\":[11.03643798828125,11.07364559173584,11.248010635375977,11.170310974121094,10.949195861816406,11.285311698913574,10.957159996032715,11.096034049987793,11.2238130569458,11.023275375366211,11.042583465576172,11.16788101196289,11.188385009765625,11.112465858459473],\"y\":[3.106004476547241,3.0234382152557373,3.3541853427886963,3.115736246109009,3.0922281742095947,3.283050775527954,3.0349786281585693,3.2528135776519775,3.086516857147217,3.087618112564087,3.101942300796509,3.0745949745178223,3.133884906768799,3.1343841552734375],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"23_figurative_metaphors_sociocultural\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"23_figurative_metaphors_sociocultural\"],\"x\":[12.890239715576172,12.9169282913208,12.898111343383789,12.95676326751709,12.863883018493652,12.8355073928833,12.85640811920166,13.136683464050293,12.889006614685059,7.990469932556152,12.911820411682129,12.735673904418945,12.899605751037598,12.521623611450195],\"y\":[0.026861172169446945,-0.11374158412218094,-0.0861361101269722,-0.08715442568063736,0.14325851202011108,-0.09389468282461166,-0.06494750827550888,-0.21590127050876617,-0.029119817540049553,0.18973292410373688,-0.05029120296239853,0.2098502218723297,-0.02140832133591175,-0.014837851747870445],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"24_sarcasm_irony_humor\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"24_sarcasm_irony_humor\"],\"x\":[7.443446159362793,7.453991889953613,7.434319496154785,7.457515239715576,7.445913314819336,7.432080268859863,7.492039680480957,7.444361686706543,7.45388650894165,7.470397472381592,7.456826210021973,7.457319259643555,7.452033042907715,7.453394889831543],\"y\":[3.3650591373443604,3.3583810329437256,3.3732411861419678,3.3514108657836914,3.3623621463775635,3.374535083770752,3.3646886348724365,3.3602709770202637,3.329127788543701,3.332568883895874,3.3524117469787598,3.345237970352173,3.361050605773926,3.3561806678771973],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"25_dense_retrieval_retrievers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"25_dense_retrieval_retrievers\"],\"x\":[11.089601516723633,11.02492904663086,10.883186340332031,10.981522560119629,10.981151580810547,10.95821762084961,10.9121675491333,10.982845306396484,11.157200813293457,10.896283149719238,10.92767333984375,10.96117877960205,10.979663848876953],\"y\":[2.4378020763397217,2.470427989959717,2.49906587600708,2.4681739807128906,2.4417030811309814,2.4632935523986816,2.5861077308654785,2.4596455097198486,2.333122968673706,2.6070048809051514,2.5345096588134766,2.460688352584839,2.480128526687622],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"26_argumentative_environmental_firms\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"26_argumentative_environmental_firms\"],\"x\":[9.245034217834473,9.240274429321289,9.255699157714844,9.306416511535645,9.330011367797852,9.5535249710083,9.301241874694824,9.279705047607422,9.539828300476074,9.543222427368164,9.282510757446289,9.352497100830078],\"y\":[2.6040866374969482,2.5882256031036377,2.6099462509155273,2.364428997039795,2.5036041736602783,2.5556437969207764,2.5966074466705322,2.450493335723877,2.5414693355560303,2.4918816089630127,2.483466386795044,2.526350498199463],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"27_proprietary_opensourced_recommendatio\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"27_proprietary_opensourced_recommendatio\"],\"x\":[8.700318336486816,8.662346839904785,8.710755348205566,8.685635566711426,10.063787460327148,8.825141906738281,9.746978759765625,8.632473945617676,8.7011137008667,9.762179374694824,10.958747863769531,9.222679138183594],\"y\":[1.2634673118591309,1.199538230895996,1.2940387725830078,1.2151010036468506,2.198474407196045,1.475010633468628,3.3699069023132324,1.1698120832443237,1.2515336275100708,3.3798534870147705,1.8569083213806152,1.78851318359375],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"28_modification_mp2_multiword\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"28_modification_mp2_multiword\"],\"x\":[12.416332244873047,12.561809539794922,12.500757217407227,12.375421524047852,12.55825138092041,12.481864929199219,12.49730396270752,12.1989107131958,12.471221923828125,12.578041076660156,12.469315528869629,12.464476585388184],\"y\":[1.9628962278366089,2.3608059883117676,2.222209930419922,2.1565310955047607,2.1810107231140137,2.1097028255462646,2.3813178539276123,2.188523292541504,2.159067153930664,2.3556671142578125,1.6200826168060303,2.1543469429016113],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"29_instructional_teaching_students\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"29_instructional_teaching_students\"],\"x\":[8.679919242858887,8.863896369934082,8.659948348999023,8.705171585083008,8.791703224182129,8.700974464416504,8.720808029174805,9.014729499816895,8.698103904724121,8.63776969909668,8.665627479553223,8.739877700805664],\"y\":[0.4001754820346832,0.5290360450744629,0.39178118109703064,0.5974481105804443,0.46730977296829224,0.44593337178230286,0.3917016386985779,0.42155221104621887,0.3866553008556366,0.5923276543617249,0.5228016972541809,0.4678838849067688],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"30_counterfactuals_scone_counterfactual\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"30_counterfactuals_scone_counterfactual\"],\"x\":[10.26634407043457,10.128600120544434,10.363204956054688,10.149825096130371,10.254651069641113,10.238754272460938,10.266803741455078,10.18940258026123,10.27680492401123,10.135074615478516,10.226946830749512],\"y\":[2.246767520904541,2.1721770763397217,2.3008975982666016,2.423623561859131,2.2409117221832275,2.265772581100464,2.3193318843841553,2.154175281524658,2.2092831134796143,2.1171963214874268,2.24501371383667],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"45_reasoning_transformer_parameters_tran\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"45_reasoning_transformer\"],\"x\":[12.020341873168945,12.169936180114746,12.101693153381348,12.094861030578613,11.659080505371094,12.379535675048828,12.050992965698242,12.262616157531738,12.431180000305176,11.988661766052246,12.082118034362793,12.085043907165527,12.180397033691406,12.176328659057617,11.84795093536377,12.084325790405273,12.128952980041504,12.28459644317627,12.194994926452637,12.008187294006348,11.911856651306152,12.048596382141113,12.229466438293457,12.182258605957031,12.1466703414917,12.20826530456543,11.903266906738281,11.976790428161621,11.813048362731934,12.021183967590332,12.020978927612305,12.224409103393555,11.788299560546875,12.209701538085938,11.93454647064209,11.924176216125488,12.019671440124512,12.825284004211426,12.15970516204834,12.297412872314453,12.158596992492676,12.110843658447266,12.26720905303955,12.509387016296387,12.426708221435547,12.31076431274414,12.200416564941406,12.108731269836426,12.027480125427246,12.294480323791504,12.179040908813477,11.868775367736816,12.016388893127441,12.533409118652344,11.871016502380371,12.278585433959961,12.497551918029785,11.953156471252441,12.103267669677734,12.292784690856934,12.235295295715332,11.931829452514648,13.446449279785156,13.293713569641113,13.339015007019043,13.234209060668945,13.390213012695312,13.275945663452148,13.059819221496582,13.199735641479492,13.168375968933105,13.352516174316406,13.391827583312988,13.276883125305176,13.3883638381958,13.391206741333008,13.22536563873291,13.355441093444824,13.415063858032227,13.445826530456543,13.263446807861328,13.253372192382812,13.080596923828125,13.292510986328125,13.387595176696777,13.407983779907227,13.065677642822266,13.157443046569824,13.285446166992188,13.371879577636719,13.443427085876465,13.312077522277832,13.325447082519531,13.201516151428223,13.412653923034668,13.387580871582031,13.376065254211426,13.23760986328125,13.391357421875,13.28543758392334,13.155158996582031,13.134252548217773,13.264272689819336,13.326519012451172,13.441741943359375,13.180994987487793,13.229814529418945,13.385966300964355,13.26877498626709,13.252023696899414,9.99891185760498,9.666812896728516,9.87675666809082,9.759414672851562,9.77737045288086,9.712926864624023,9.860082626342773,10.150372505187988,9.757026672363281,9.756099700927734,9.88352108001709,9.667094230651855,9.834576606750488,9.662117958068848,9.90071964263916,9.823662757873535,9.885151863098145,9.897980690002441,9.599701881408691,9.828813552856445,10.166223526000977,9.664942741394043,9.68136215209961,9.909587860107422,9.613624572753906,9.683416366577148,9.677172660827637,9.895627975463867,9.831110954284668,9.791001319885254,9.888591766357422,10.030932426452637,10.003292083740234,11.668291091918945,11.893047332763672,11.399813652038574,11.399984359741211,11.689655303955078,11.275130271911621,11.221017837524414,12.198081016540527,12.027531623840332,11.27303695678711,11.897500038146973,11.873612403869629,11.806347846984863,11.46186637878418,11.398143768310547,11.580697059631348,11.322710990905762,11.420403480529785,11.564152717590332,11.334028244018555,11.482306480407715,11.22659969329834,11.433297157287598,11.450419425964355,11.489837646484375,11.502764701843262,11.421748161315918,11.442119598388672,12.117900848388672,11.278861999511719,11.956093788146973,12.490596771240234,12.429107666015625,12.604439735412598,12.537409782409668,12.447066307067871,12.514530181884766,12.494388580322266,12.444210052490234,12.318142890930176,12.335335731506348,12.499151229858398,12.503713607788086,12.477373123168945,12.43051815032959,12.483855247497559,12.453995704650879,12.551338195800781,12.408796310424805,12.414511680603027,11.968720436096191],\"y\":[3.4334287643432617,3.9167301654815674,3.949054002761841,3.438246250152588,3.445382595062256,3.36970853805542,3.7780611515045166,2.977111577987671,3.824798583984375,4.014473915100098,3.966614246368408,3.947669744491577,3.546800136566162,3.5740740299224854,3.722076654434204,3.7931604385375977,3.4850759506225586,3.1680963039398193,4.030865669250488,3.494990587234497,3.631016969680786,3.9821364879608154,3.874598264694214,3.628248453140259,3.362018585205078,3.6282668113708496,3.4427807331085205,3.539586305618286,3.9404172897338867,3.503599166870117,3.3990092277526855,3.441103219985962,2.9686286449432373,3.454322576522827,3.507044553756714,3.588791847229004,3.930246591567993,3.598909854888916,3.4998040199279785,3.598375082015991,3.547398805618286,3.815009355545044,3.1761443614959717,3.7004573345184326,2.9148776531219482,3.357351303100586,3.646010398864746,4.046837329864502,3.844369649887085,3.3957107067108154,3.5331528186798096,3.4589388370513916,3.6157045364379883,3.513286590576172,3.4719655513763428,3.176704168319702,3.3323616981506348,3.9445858001708984,3.7024219036102295,3.222160577774048,3.219435214996338,3.6485061645507812,2.849513053894043,2.6924357414245605,2.519382953643799,2.685671091079712,3.4131829738616943,3.3496320247650146,2.9062132835388184,2.5787570476531982,2.8339319229125977,2.6660850048065186,3.363393783569336,3.0692508220672607,3.403278112411499,2.94502854347229,2.862663984298706,2.849548578262329,3.311357021331787,2.857738494873047,2.829335927963257,2.7103164196014404,2.6873369216918945,3.1780202388763428,2.415661573410034,2.984997510910034,2.9960906505584717,3.0859627723693848,3.064985752105713,3.3739285469055176,2.99613094329834,3.3052146434783936,2.759016990661621,2.554168224334717,2.7810420989990234,3.370600700378418,2.8825161457061768,2.5941309928894043,3.402747631072998,3.1956849098205566,2.574057102203369,2.4850573539733887,3.1060783863067627,2.757913827896118,2.8156826496124268,2.6702122688293457,3.1264901161193848,3.3880455493927,2.8700947761535645,2.6230366230010986,-1.2806227207183838,-1.4134907722473145,-1.3984967470169067,-0.8591680526733398,-1.1667323112487793,-0.9986860752105713,-1.226072072982788,-1.2499401569366455,-1.3567789793014526,-1.3018178939819336,-1.279323697090149,-1.3926376104354858,-0.8900179862976074,-1.3667707443237305,-1.3356115818023682,-1.4169893264770508,-1.3518718481063843,-1.17157781124115,-1.1077362298965454,-1.3785203695297241,-1.242591381072998,-1.3728218078613281,-1.36669921875,-1.1680376529693604,-1.3303998708724976,-0.8987433314323425,-1.368369460105896,-1.1938246488571167,-1.3907575607299805,-1.302925705909729,-1.1836531162261963,-1.1196755170822144,-1.154442310333252,1.6213551759719849,1.7356750965118408,1.8189188241958618,2.011199474334717,1.6284370422363281,1.6911828517913818,1.434216856956482,1.9124113321304321,1.708951473236084,1.345273733139038,1.9241002798080444,1.706599235534668,1.5955106019973755,1.6075619459152222,1.8218833208084106,1.9240765571594238,1.5612646341323853,1.4347044229507446,1.8333743810653687,1.4203897714614868,1.508184552192688,1.6106964349746704,1.7237390279769897,1.6263391971588135,1.7565888166427612,1.8806627988815308,1.4971145391464233,1.4235330820083618,2.043459415435791,1.6557258367538452,-0.01690675877034664,-0.3107600808143616,-0.2527309060096741,-0.2602718472480774,-0.33801159262657166,-0.32423314452171326,-0.19620446860790253,-0.286507248878479,-0.2868269979953766,2.4987032413482666,-0.19626036286354065,-0.2882573902606964,2.8162295818328857,-0.27515092492103577,-0.28718024492263794,-0.29625701904296875,-0.3112342059612274,-0.32359012961387634,-0.29335451126098633,-0.24639558792114258,1.9293910264968872],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"46_dialogue_languages_dataset_language_e\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"46_dialogue_languages\"],\"x\":[13.87353229522705,13.208016395568848,14.106237411499023,13.092193603515625,13.336145401000977,13.386882781982422,12.319914817810059,13.021512985229492,13.98583698272705,12.727828025817871,14.158334732055664,12.96531867980957,13.214096069335938,13.292146682739258,13.683382034301758,13.187692642211914,13.279541969299316,12.989252090454102,12.971858978271484,12.976456642150879,13.035022735595703,12.499113082885742,13.1813325881958,12.979851722717285,13.296390533447266,13.039684295654297,13.842495918273926,13.132991790771484,13.9254732131958,12.167425155639648,14.098097801208496,13.920403480529785,12.558815002441406,12.849774360656738,12.970587730407715,14.02845287322998,12.911493301391602,13.765254974365234,13.905959129333496,13.326302528381348,13.942405700683594,12.925593376159668,13.41197395324707,13.25645637512207,12.991842269897461,13.592504501342773,13.934208869934082,13.955780029296875,13.298561096191406,13.263577461242676,12.345218658447266,13.947273254394531,13.009997367858887,13.643670082092285,13.180303573608398,13.74915599822998,12.808438301086426,13.863404273986816,12.991873741149902,13.162498474121094,12.859356880187988,13.075448036193848,13.156818389892578,12.886246681213379,13.997769355773926,13.03771686553955,12.784741401672363,12.894862174987793,12.97797966003418,13.60038948059082,14.051366806030273,12.930017471313477,12.712410926818848,12.782764434814453,12.85952377319336,12.852415084838867,13.387727737426758,13.939677238464355,12.471564292907715,13.912345886230469,12.641358375549316,13.367488861083984,13.8226318359375,13.64598560333252,13.757844924926758,12.249322891235352,13.732461929321289,13.127846717834473,12.968668937683105,13.813699722290039,13.807978630065918,13.008394241333008,13.787175178527832,13.993060111999512,13.531773567199707,13.984949111938477,13.01711654663086,14.002456665039062,13.822026252746582,13.829243659973145,13.329290390014648,12.908177375793457,14.121039390563965,13.951969146728516,12.813560485839844,13.843708038330078,13.839166641235352,14.121064186096191,12.558928489685059,13.817070960998535,13.30069637298584,13.859855651855469,13.09512710571289,12.266813278198242,12.803173065185547,13.087562561035156,13.733831405639648,13.884516716003418,12.79514217376709,13.156198501586914,12.635796546936035,13.271025657653809,12.98554801940918,13.871160507202148,13.031496047973633,14.019685745239258,13.770309448242188,13.913249969482422,13.995428085327148,12.395220756530762,13.254109382629395,12.519502639770508,13.016617774963379,13.239713668823242,13.700984954833984,13.32162857055664,13.05946159362793,12.602531433105469,12.626888275146484,13.298456192016602,13.010909080505371,12.473347663879395,13.30741024017334,12.52789306640625,13.24752140045166,14.047928810119629,12.949474334716797,13.951871871948242,14.054276466369629,12.840779304504395,12.930652618408203,13.249177932739258,14.00323486328125,12.957128524780273,13.71989631652832,12.736191749572754,13.225967407226562,11.134943962097168,11.025127410888672,11.245323181152344,11.604127883911133,11.156891822814941,11.352185249328613,11.527443885803223,11.282083511352539,11.126087188720703,11.73695182800293,11.573534965515137,13.375391006469727,11.404452323913574,11.218412399291992,10.824934959411621,11.162917137145996,11.561422348022461,11.366466522216797,10.656908988952637,11.300393104553223,11.272549629211426,11.278218269348145,11.19459056854248,11.2599458694458,11.612763404846191,10.9222412109375,11.204724311828613,11.096510887145996,11.001474380493164,11.493638038635254,11.407891273498535,11.244793891906738,11.168529510498047,11.017486572265625,11.451024055480957,11.223573684692383,11.187163352966309,11.271790504455566,11.26801872253418,11.24870491027832,11.188407897949219,11.215272903442383,11.429038047790527,11.241147994995117,11.28900146484375,11.684981346130371,11.643854141235352,11.422016143798828,11.644282341003418,11.211750030517578,11.794293403625488,10.666879653930664,11.676538467407227,11.204644203186035,11.4844388961792,11.210556983947754,10.822954177856445,11.602618217468262,11.172928810119629,11.261439323425293,11.18604850769043,11.24404525756836,11.392868995666504,11.273892402648926,11.069701194763184,11.81900691986084,11.298412322998047,11.697004318237305,11.151240348815918,11.614058494567871,11.220532417297363,11.269240379333496,11.204486846923828,11.25660228729248,11.144911766052246,11.228436470031738,11.389638900756836,11.027185440063477,11.341411590576172,11.200804710388184,10.692567825317383,11.169511795043945,11.02439022064209,10.92267894744873,11.365792274475098,11.304564476013184,11.123757362365723,11.317601203918457,11.2233304977417,11.252074241638184,10.953468322753906,11.1937837600708,11.191131591796875,11.507041931152344,13.65318489074707,13.939993858337402,13.685359001159668,13.784351348876953,13.504968643188477,13.94735336303711,13.143325805664062,13.834990501403809,13.44897747039795,13.599161148071289,13.199520111083984,13.714217185974121,13.943765640258789,13.477546691894531,13.838189125061035,13.14396858215332,13.451980590820312,13.139006614685059,13.305334091186523,13.572847366333008,13.578728675842285,13.640463829040527,13.756519317626953,13.478137016296387,13.640913963317871,13.371881484985352,13.30630874633789,13.778650283813477,13.48436164855957,13.48245620727539,13.242944717407227,13.188318252563477,13.671472549438477,13.161858558654785,13.182262420654297,13.67568302154541,13.578418731689453,13.574835777282715,13.530089378356934,12.509833335876465,13.550978660583496,13.269086837768555,13.929557800292969,13.697040557861328,13.825508117675781,13.642824172973633,13.51134204864502,13.453692436218262,13.557821273803711,13.553349494934082,13.648940086364746,13.369372367858887,13.12069034576416,13.566463470458984,13.692540168762207,13.704095840454102,13.338266372680664,13.631470680236816,13.964845657348633,13.344286918640137,13.934170722961426,13.56275463104248,13.731254577636719,13.44790267944336,13.115975379943848,13.600167274475098,8.303739547729492,13.527002334594727,13.690975189208984,13.423891067504883,13.938860893249512,13.628800392150879,13.415291786193848,13.883581161499023,13.465723037719727,13.598390579223633,13.958017349243164,13.456777572631836,13.795967102050781,9.905937194824219,13.677034378051758,13.943849563598633,13.411235809326172,13.518133163452148,13.331656455993652,8.079171180725098,8.06274700164795,7.97455358505249,7.848491191864014,8.032004356384277,7.964422702789307,7.964263916015625,8.134442329406738,8.114529609680176,7.841091632843018,7.8066725730896,8.066001892089844,7.995242118835449,8.329507827758789,8.135189056396484,7.784605979919434,7.775929927825928,8.180281639099121,8.422222137451172,7.919281005859375,7.885015487670898,7.895800590515137,7.771629810333252,8.177929878234863,7.973200798034668,8.28538703918457,7.766735553741455,7.8133392333984375,7.980544567108154,7.834801197052002,7.792785167694092,8.400049209594727,8.152592658996582,7.950541019439697,7.951925754547119,8.21005630493164,7.920266628265381,7.854425430297852,8.000518798828125,7.90179443359375,8.401914596557617,8.250751495361328,7.81003999710083,8.432668685913086,7.853431224822998,8.134767532348633,8.218738555908203,7.8007283210754395,8.225289344787598,7.825068473815918,8.072797775268555,8.098258972167969,8.405604362487793,7.813677787780762,7.821072101593018,7.985950946807861,7.810969352722168,8.3153657913208,8.099970817565918,7.857753276824951,8.26423168182373,7.91890811920166,7.952602386474609,7.912245273590088,7.974617958068848,7.784420490264893,8.138467788696289,8.036944389343262,7.761767864227295,7.890107154846191,8.475704193115234,8.049864768981934,7.910464286804199,7.735909938812256,8.021018028259277,7.9187726974487305,7.908551216125488,7.699703216552734,7.959645748138428,7.826615333557129,8.282672882080078,7.934024333953857,8.200965881347656,7.851012229919434,7.659298419952393,7.964986801147461,7.375672340393066,7.989088535308838,8.16100788116455,7.880589008331299,7.8358564376831055,7.8488359451293945,7.915194511413574,8.008590698242188,7.819448471069336,7.807568073272705,7.392760753631592,7.372259140014648,7.680557727813721,8.323735237121582,7.405981540679932,7.390983581542969,8.47547721862793,7.814692974090576,8.292101860046387,8.115535736083984,7.816876411437988,7.764798164367676,7.4307541847229,8.173230171203613,7.816125869750977,7.800354957580566,7.687350749969482,7.416838645935059,8.04630184173584,7.372389793395996,8.061315536499023,7.425755023956299,7.877260684967041,8.388016700744629,7.779290199279785,8.227629661560059,8.057212829589844,8.02259349822998,7.782587051391602,8.23399543762207,7.90402889251709,7.8760199546813965,7.4668121337890625,8.308961868286133,7.824159622192383,7.712260723114014,7.914191246032715,7.685945987701416,7.940428733825684,10.677026748657227,10.417326927185059,10.19378662109375,10.34382438659668,10.359889030456543,10.168207168579102,10.379969596862793,10.382912635803223,10.3951997756958,10.006451606750488,10.593812942504883,10.523648262023926,10.564718246459961,10.390353202819824,10.409890174865723,10.375975608825684,10.270461082458496,10.380226135253906,10.342330932617188,10.389986038208008,10.570111274719238,10.563359260559082,10.393143653869629,10.445023536682129,10.406803131103516,10.406514167785645,9.976093292236328,10.553617477416992,10.514777183532715,10.404207229614258,10.352548599243164,10.402541160583496,10.373939514160156,10.66649055480957,10.147748947143555,10.177634239196777,10.380538940429688,10.481019020080566,10.569191932678223,10.38936710357666,10.453319549560547,10.417245864868164,10.462665557861328,10.532641410827637,10.139423370361328,10.995478630065918,10.441753387451172,10.408876419067383,10.595641136169434,10.508939743041992,10.599928855895996,10.383362770080566,10.315134048461914,10.520706176757812,10.196410179138184,10.421682357788086,10.112748146057129,10.42435073852539,9.957427978515625,9.407848358154297,9.24934196472168,9.988895416259766,9.616469383239746,9.254940032958984,9.619887351989746,9.763313293457031,9.41704273223877,9.903557777404785,9.640640258789062,9.904218673706055,9.833464622497559,9.501309394836426,10.281272888183594,9.5150146484375,9.301063537597656,9.468867301940918,9.583669662475586,9.652227401733398,9.2976655960083,9.610597610473633,9.950066566467285,9.628329277038574,9.354300498962402,9.685493469238281,9.437472343444824,9.71446418762207,9.970356941223145,9.591911315917969,9.4934720993042,11.245697975158691,9.630011558532715,9.857994079589844,9.644820213317871,9.684257507324219,9.61834716796875,9.774521827697754,9.085481643676758,10.036049842834473,9.738116264343262,9.506299018859863,9.30492877960205,9.699036598205566,9.771791458129883,9.192770957946777,9.514463424682617,9.63692855834961,9.843480110168457,9.38808536529541,9.520946502685547,9.217379570007324,9.838109970092773,9.648056030273438,9.77692699432373,9.458232879638672,9.670414924621582,9.680063247680664,9.057123184204102,9.02020263671875,8.764460563659668,9.149687767028809,8.96601676940918,8.83845329284668,7.854395389556885,8.691113471984863,8.074891090393066,7.991252422332764,8.15771198272705,8.11121940612793,7.889158725738525,8.559086799621582,7.732449054718018,8.129582405090332,7.739405155181885,8.906782150268555,8.779287338256836,8.384539604187012,8.096807479858398,7.73988151550293,7.74143123626709,10.807848930358887,8.938881874084473,8.25653076171875,8.156562805175781,8.910407066345215,8.929011344909668,8.912620544433594,8.349308967590332,9.150344848632812,9.063459396362305,8.188298225402832,8.082099914550781,8.124395370483398,8.095884323120117,7.761401653289795,7.85822057723999,7.735099792480469,9.143465995788574,7.9051079750061035,8.62359619140625,8.771903991699219,8.10483455657959,8.881553649902344,7.842296600341797,7.751319408416748,8.932684898376465,8.793137550354004,8.686760902404785,8.676131248474121,8.958822250366211,7.80643892288208,8.740694046020508,7.7468767166137695,8.771867752075195,9.189112663269043,9.039446830749512,9.115840911865234,9.096116065979004,9.138891220092773,8.879927635192871,8.893594741821289,8.761467933654785,8.84201431274414,8.92170524597168,8.984542846679688,8.870153427124023,9.148591041564941,9.1036376953125,8.759328842163086,9.173047065734863,9.000730514526367,9.311477661132812,9.226938247680664,8.916337966918945,9.266477584838867,8.85345458984375,8.894457817077637,9.150871276855469,8.838128089904785,8.816519737243652,9.046035766601562,8.901216506958008,9.159720420837402,8.813364028930664,9.301010131835938,8.885666847229004,9.035273551940918,9.151689529418945,9.173745155334473,9.153688430786133,9.068913459777832,9.077773094177246,8.93253231048584,9.23687744140625,8.845597267150879,8.906045913696289,9.104510307312012,8.794829368591309,8.974413871765137,8.559188842773438,9.103609085083008,8.886739730834961,8.847504615783691,8.852912902832031,8.7335786819458,8.971410751342773,8.835782051086426,8.914285659790039,9.40961742401123,9.401433944702148,9.173538208007812,9.561808586120605,9.133511543273926,9.540070533752441,9.700275421142578,9.41574478149414,9.289504051208496,9.243343353271484,9.41695785522461,9.496110916137695,9.767770767211914,9.445943832397461,9.416340827941895,9.574084281921387,9.292332649230957,9.226899147033691,9.462366104125977,9.370545387268066,9.44404411315918,9.387925148010254,9.353792190551758,9.420747756958008,9.548852920532227,9.489824295043945,9.414810180664062,9.33342456817627,9.402825355529785,9.672316551208496,9.393604278564453,9.239797592163086,9.441904067993164,9.516083717346191,9.340642929077148,9.363749504089355,9.59388256072998,9.435190200805664,9.716001510620117,9.322286605834961,9.686580657958984,9.433467864990234,9.423392295837402,9.48121166229248,9.288214683532715,9.267616271972656,9.553618431091309,9.478880882263184,9.371923446655273,9.760424613952637,9.519524574279785,9.401512145996094,9.502867698669434,8.319539070129395,8.872060775756836,8.969176292419434,8.707355499267578,8.74543285369873,8.927281379699707,8.319100379943848,8.32375717163086,8.666680335998535,8.335197448730469,8.70714282989502,9.042021751403809,8.712772369384766,8.291357040405273,8.325663566589355,8.38085651397705,8.75160026550293,8.50749397277832,8.352300643920898,8.444624900817871,8.73957347869873,8.366410255432129,8.543909072875977,8.721636772155762,8.295104026794434,8.693644523620605,8.322171211242676,8.898469924926758,8.4146146774292,8.773049354553223,8.323225975036621,9.074934959411621,8.57706356048584,8.7324857711792,8.81857967376709,9.022188186645508,8.334091186523438,8.382010459899902,8.761948585510254,8.723730087280273,9.060052871704102,8.774007797241211,8.736745834350586,8.403592109680176,11.366277694702148,11.415215492248535,11.413336753845215,11.3681058883667,11.386617660522461,11.464384078979492,11.333633422851562,11.434808731079102,11.411873817443848,11.339577674865723,11.400299072265625,11.394913673400879,11.41909122467041,11.410325050354004,11.436091423034668,11.440723419189453,11.421466827392578,11.424187660217285,10.752309799194336,11.30122184753418,11.429636001586914,11.37482738494873,11.405726432800293,11.432455062866211,11.494464874267578,11.39643669128418,11.412282943725586,11.394112586975098,11.40078353881836,9.92600154876709,9.90449047088623,9.936745643615723,9.92866325378418,9.815776824951172,9.909724235534668,9.910490989685059,9.891134262084961,9.903509140014648,9.932821273803711,9.916892051696777,9.828373908996582,9.930082321166992,9.88005256652832,9.91939926147461,10.002853393554688,9.763079643249512,9.934072494506836,9.936637878417969,9.912662506103516,9.736724853515625,9.912676811218262,9.76375961303711,9.90211296081543,9.953424453735352,9.921619415283203,9.907418251037598,9.980151176452637,9.924909591674805,10.54301929473877],\"y\":[1.3817375898361206,1.0612719058990479,1.1385481357574463,1.08525550365448,1.1626895666122437,1.2480696439743042,0.4848286211490631,1.2720167636871338,1.1438474655151367,1.3171401023864746,1.4076839685440063,1.264595627784729,1.0884864330291748,1.1517572402954102,1.355188012123108,1.7229746580123901,0.972616970539093,1.1486741304397583,0.8810750246047974,1.257293701171875,0.9572843313217163,0.8085752129554749,0.9309483766555786,0.7958930730819702,1.0975350141525269,0.6360377073287964,0.9854050874710083,1.798175573348999,1.3100166320800781,0.67192143201828,1.2066819667816162,1.2865713834762573,0.9056100845336914,0.6249911189079285,1.7463622093200684,1.2420947551727295,0.7832163572311401,1.1565204858779907,1.3271960020065308,1.3933436870574951,1.0287328958511353,0.7349646091461182,1.1217514276504517,1.414216160774231,1.152062177658081,1.4249526262283325,1.1767480373382568,1.2981083393096924,1.1263619661331177,1.0864992141723633,0.5083565711975098,1.0866432189941406,1.8492233753204346,1.125476598739624,0.944848895072937,1.1385208368301392,0.678071916103363,1.3165032863616943,0.6076777577400208,1.3482673168182373,1.6210135221481323,0.9811966419219971,1.2457525730133057,1.171410322189331,1.1819112300872803,0.8604665398597717,0.942077100276947,0.516913115978241,1.7408256530761719,1.4816997051239014,1.3886042833328247,1.7745593786239624,0.794830322265625,0.5049740076065063,0.8629714250564575,0.9947225451469421,1.3124425411224365,1.181308388710022,0.65378338098526,0.9567456841468811,0.8112806677818298,1.0578453540802002,1.270293116569519,1.1971663236618042,1.2050573825836182,0.49335476756095886,1.5063146352767944,1.3702335357666016,1.2522984743118286,1.2417391538619995,1.4349349737167358,1.29513418674469,1.3764736652374268,1.1924550533294678,1.3044599294662476,1.0554274320602417,0.9203598499298096,1.1263551712036133,1.4461302757263184,1.4113985300064087,1.1233012676239014,1.292132019996643,1.383273959159851,1.2921757698059082,1.4860016107559204,1.4392153024673462,1.4001719951629639,0.9837578535079956,0.6659143567085266,0.9185031056404114,1.1586668491363525,1.4281121492385864,1.1629738807678223,0.510953962802887,0.5897344946861267,0.8766728639602661,1.4713213443756104,1.1049691438674927,0.5188847184181213,1.4383823871612549,1.3952010869979858,1.3247922658920288,0.9874094128608704,1.13014554977417,0.8806256651878357,1.1321403980255127,1.1713860034942627,1.29535710811615,1.1741254329681396,0.570444643497467,1.2960554361343384,0.7852280735969543,0.9467426538467407,1.7510077953338623,1.3233586549758911,1.1152814626693726,1.0483297109603882,0.647891640663147,0.5596342086791992,1.3211227655410767,0.9677881002426147,0.6502654552459717,1.0543253421783447,0.6772043704986572,1.212782382965088,1.1973955631256104,1.1335924863815308,1.4092952013015747,1.3452917337417603,0.6293814778327942,1.8506160974502563,1.066854476928711,1.2767295837402344,0.7192075848579407,1.712160587310791,0.8005959391593933,1.1127575635910034,-0.2791852056980133,-0.56585294008255,-0.33805036544799805,-0.5237462520599365,0.26701799035072327,-0.765414834022522,-0.7155061364173889,-0.6682496070861816,-0.6063860654830933,-0.3886547386646271,-0.6360691785812378,-0.5970664024353027,-0.6077590584754944,-0.3316837251186371,0.027478214353322983,-1.0895836353302002,-0.5561929941177368,0.06399431079626083,0.6634865999221802,-0.7742803692817688,0.14976142346858978,-1.053679347038269,-0.8100492358207703,-0.6574156284332275,-0.6182628273963928,2.9431962966918945,-0.8820046186447144,-0.8308306336402893,-0.7561229467391968,0.06208992749452591,-1.2723103761672974,0.021186748519539833,-0.9865592122077942,0.42442458868026733,-0.5611394047737122,-0.19294996559619904,-0.6945754289627075,-0.3245929181575775,0.14573052525520325,-0.8709970712661743,-0.352821946144104,-0.20292386412620544,-0.3928586542606354,-0.45419180393218994,-0.6536911725997925,-0.43990081548690796,0.47350263595581055,0.13158506155014038,-0.7095873355865479,-1.041664481163025,-0.30340245366096497,0.33025580644607544,-0.5825144648551941,-0.9634666442871094,0.1646973192691803,-0.9077123999595642,0.321090430021286,-0.4256440997123718,0.016457444056868553,-0.3007483184337616,-0.9926905035972595,-0.40955081582069397,-0.7163980603218079,-0.7613950967788696,-0.5563022494316101,-0.30304890871047974,-0.6749861836433411,-0.5403029322624207,-0.3122972548007965,-0.610802948474884,-0.7764879465103149,-0.3963066339492798,-0.9999945163726807,-0.2833714783191681,-0.40003347396850586,-0.9637208580970764,-0.45351165533065796,-0.5818671584129333,-0.7205086350440979,-0.9992696046829224,0.6563940644264221,-1.185823917388916,-1.0215363502502441,-0.13112245500087738,-0.6537771224975586,-0.7294257283210754,-0.7630423307418823,-0.7293157577514648,-0.9494206309318542,-0.3285471796989441,-0.1588168889284134,-0.36618950963020325,-0.9938457608222961,0.16926676034927368,-0.4724136292934418,0.05542946234345436,-0.2140544056892395,-0.305911123752594,-0.019443433731794357,0.04754577577114105,-0.5232704877853394,-0.11347304284572601,-0.42826223373413086,-0.3631307780742645,-0.34572187066078186,-0.06021670997142792,0.02899486944079399,-0.5741272568702698,-0.17813809216022491,-0.3307811915874481,-0.5624246597290039,-0.33823102712631226,-0.49408844113349915,-0.2875242233276367,-0.5673142671585083,-0.5054513216018677,-0.19457708299160004,0.04647556692361832,0.15025267004966736,-0.4029132127761841,-0.09265140444040298,-0.06226486340165138,-0.49906080961227417,-0.42240944504737854,-0.37416812777519226,-0.4376997649669647,-0.5433786511421204,-0.48809999227523804,-0.3587982952594757,-0.1461024433374405,-0.4376213848590851,-0.5315554738044739,-0.5797322392463684,2.760768413543701,-0.5617931485176086,-0.37373608350753784,1.0235728025436401,-0.2975458800792694,-0.12453179061412811,-0.39394140243530273,0.08051753044128418,-0.5586355328559875,-0.5786116719245911,-0.5049486756324768,0.1441514790058136,-0.7706948518753052,-0.21628828346729279,-0.3426320552825928,-0.16151897609233856,-0.39145922660827637,-0.5925371646881104,-0.2956721782684326,0.027539370581507683,-0.30378881096839905,0.042292892932891846,-0.12347960472106934,-0.06413992494344711,-0.6200217604637146,-0.4954264461994171,-0.34516313672065735,3.190534830093384,-0.2043280303478241,-0.5706292986869812,-0.7155386209487915,0.12354090809822083,-0.219037726521492,-0.293334424495697,0.07163935899734497,-0.5650919675827026,-0.29002729058265686,0.07026058435440063,-0.5817172527313232,-0.10595829039812088,-1.2771755456924438,-0.2213153839111328,0.0876251682639122,-0.7389064431190491,-0.14196763932704926,-0.6306360363960266,0.011064082384109497,0.014209670014679432,-0.09134206920862198,-0.2708755433559418,-0.20944665372371674,-0.15542514622211456,-0.016267064958810806,0.33848637342453003,1.3138884241925552e-05,-0.3145374655723572,-0.37487006187438965,0.08202043920755386,-0.06734184175729752,-0.26890313625335693,-0.2482864111661911,-0.37177231907844543,-0.342668354511261,0.05140113830566406,-0.009848969988524914,-0.26512715220451355,-0.21851959824562073,-0.20532433688640594,-0.34504765272140503,-0.12112276256084442,-0.11948814243078232,-0.36536410450935364,-0.43950170278549194,-0.3522815406322479,-0.16667716205120087,-0.33151018619537354,-0.4151010513305664,-0.21129287779331207,0.18422266840934753,-0.13335680961608887,0.17345526814460754,0.07078302651643753,-0.06395117193460464,-0.33869442343711853,-0.20178943872451782,-0.20231139659881592,-0.02338528260588646,0.17116601765155792,-0.3580891191959381,-0.31286415457725525,-0.32009008526802063,-0.1331133246421814,0.18067573010921478,-0.2697615623474121,0.08013986051082611,-0.31005680561065674,0.028209263458848,0.10702250152826309,0.2667584717273712,-0.3471834659576416,-0.3380453586578369,-0.07149399816989899,-0.4023083448410034,-0.10830654948949814,-0.17893050611019135,-0.2754707932472229,0.22065694630146027,-0.1637326180934906,-0.12212429195642471,-0.17535138130187988,-0.2760526239871979,-0.01606772653758526,0.11322040110826492,0.021302759647369385,1.5433651208877563,1.4605567455291748,1.4761929512023926,1.4625173807144165,1.4584766626358032,1.4409211874008179,1.4749916791915894,1.4284682273864746,1.4748163223266602,1.4806615114212036,1.4631659984588623,1.411305546760559,1.4848947525024414,1.0450985431671143,1.4943875074386597,1.3517125844955444,1.4878875017166138,1.4422603845596313,1.5346200466156006,1.5406434535980225,1.499199628829956,1.4141327142715454,1.4414336681365967,1.3886102437973022,1.4608150720596313,1.4566928148269653,1.5145379304885864,1.6541327238082886,1.5264090299606323,1.531654953956604,1.4845945835113525,1.4692519903182983,1.5288569927215576,1.5096898078918457,1.3595173358917236,1.587659478187561,1.4848111867904663,1.480909824371338,1.49443781375885,1.6546357870101929,1.5368572473526,1.2316579818725586,1.3251349925994873,1.5530869960784912,1.5514334440231323,1.5379899740219116,1.494699239730835,1.525313377380371,1.4642879962921143,1.526352882385254,1.288293480873108,1.483452320098877,1.4212583303451538,1.5053179264068604,1.3147481679916382,1.3840261697769165,1.5364909172058105,1.516196370124817,1.4429274797439575,1.4955360889434814,1.5462852716445923,1.454986810684204,1.4761298894882202,1.4729735851287842,1.450022578239441,1.443420648574829,1.4701296091079712,4.530323028564453,4.6652750968933105,4.259049415588379,4.17231559753418,4.669703960418701,4.087711334228516,4.67042350769043,4.63741397857666,4.61146879196167,4.00691556930542,4.465812683105469,4.730312824249268,3.997835874557495,4.689051151275635,4.215987682342529,4.713289260864258,3.9740796089172363,4.662024021148682,4.6290106773376465,4.683803558349609,4.155202865600586,4.592298984527588,4.655975341796875,4.570188999176025,4.67529296875,4.149698734283447,4.111392498016357,4.625186920166016,4.129754066467285,4.608168125152588,4.609062194824219,4.684344291687012,4.598238468170166,4.515726089477539,4.097564697265625,4.126251697540283,4.632607936859131,3.846534013748169,4.030094146728516,4.677065372467041,4.595035552978516,4.631924152374268,4.152017593383789,4.6212286949157715,4.092752933502197,4.058029651641846,4.524569034576416,4.68303918838501,4.761435031890869,4.133310794830322,4.037563800811768,4.604251384735107,4.569831848144531,4.141808032989502,4.4533562660217285,4.653535842895508,4.054136753082275,4.630998134613037,3.962613344192505,1.5877009630203247,1.2727618217468262,1.4802215099334717,1.6104549169540405,1.4569437503814697,1.539625644683838,1.472213864326477,2.0593249797821045,1.3333063125610352,1.6195672750473022,0.646668553352356,1.6028765439987183,1.468073844909668,1.4336971044540405,1.469533920288086,1.5507453680038452,1.6590402126312256,1.5533472299575806,1.5326228141784668,1.4909952878952026,1.5933552980422974,0.6876698732376099,1.5868359804153442,1.573804259300232,1.5753282308578491,1.2330971956253052,1.6163599491119385,0.7645369172096252,1.5960463285446167,1.578489899635315,0.5172238349914551,1.5027610063552856,1.38727867603302,1.5584850311279297,1.571655035018921,1.5743860006332397,1.5758932828903198,1.8300682306289673,1.7321003675460815,1.5929148197174072,1.4410037994384766,1.594342589378357,1.557225227355957,1.3510727882385254,1.2829564809799194,1.1802170276641846,1.5979278087615967,1.6627514362335205,1.6593539714813232,1.5607953071594238,1.3650426864624023,0.5767338275909424,1.5249618291854858,1.5589072704315186,1.6015998125076294,1.5737286806106567,1.52560555934906,1.5944088697433472,2.3929059505462646,2.3886959552764893,2.2481915950775146,2.366041660308838,2.3146793842315674,2.3162174224853516,2.369158983230591,2.5022075176239014,2.4162755012512207,2.523205280303955,2.5030629634857178,2.322309732437134,2.426605224609375,2.189079761505127,2.5072808265686035,2.1927618980407715,2.2802486419677734,2.404411792755127,2.5210015773773193,2.5109353065490723,2.188095808029175,2.2342336177825928,1.3085989952087402,2.270885467529297,2.520770311355591,2.4987809658050537,2.2820372581481934,2.2575674057006836,2.305651903152466,2.480987310409546,2.6763017177581787,2.3891241550445557,2.503016233444214,2.507106065750122,2.5494508743286133,2.4986634254455566,2.2242064476013184,2.3021676540374756,2.150664806365967,2.744899034500122,2.3470451831817627,2.496939182281494,2.4149718284606934,2.483858346939087,2.3855793476104736,2.2803268432617188,2.255380868911743,2.259631872177124,2.3189663887023926,2.3722052574157715,2.399129867553711,2.2720420360565186,2.2237091064453125,2.365219831466675,2.2338740825653076,2.3938751220703125,3.968130350112915,4.086702346801758,3.7755074501037598,3.991488218307495,3.975116014480591,4.029293060302734,4.046248435974121,4.013284683227539,4.176393032073975,4.156956672668457,4.055082321166992,3.9321210384368896,3.937089204788208,3.9232289791107178,3.8601222038269043,3.866779327392578,3.9942166805267334,3.850517511367798,3.827030897140503,4.08809232711792,3.6295218467712402,4.113508701324463,4.187417507171631,3.6063523292541504,3.9586288928985596,4.24702787399292,3.9166204929351807,4.1035475730896,3.9102394580841064,4.182351112365723,3.869297981262207,3.981309175491333,3.9789490699768066,3.9490838050842285,3.944530487060547,3.942408323287964,4.038426399230957,4.008914947509766,4.139644622802734,3.7226343154907227,4.209354877471924,4.045825958251953,3.9882144927978516,4.28642463684082,4.0150346755981445,3.615358352661133,4.036133289337158,4.028831958770752,3.9747371673583984,3.9180045127868652,3.7974116802215576,4.060174942016602,4.20427942276001,4.09943962097168,-0.5178394317626953,-0.650516152381897,-0.479851096868515,-0.4310290515422821,-0.5378267168998718,-0.6259913444519043,-0.7380387187004089,-0.21319614350795746,-0.7479232549667358,-0.610841691493988,-0.6754353642463684,-0.5430178642272949,-0.6768491268157959,-0.45748335123062134,-0.3581988215446472,-0.8913076519966125,-0.698038637638092,-0.7323350310325623,-0.4265297055244446,-0.6116015911102295,-0.9226518869400024,-0.7730879783630371,-0.7615750432014465,-0.7331415414810181,-0.8823621869087219,-0.8506748676300049,-0.7885114550590515,-0.723310649394989,-0.7920389175415039,-0.6305829882621765,-0.698114275932312,-0.7590868473052979,-0.9453780651092529,-0.47471097111701965,-0.7153941988945007,-0.7514808177947998,-0.32000821828842163,-0.5421023368835449,-0.5268405079841614,-0.7256784439086914,-0.6647657752037048,-0.5601599812507629,-0.8130180239677429,-0.5797785520553589,-0.76743084192276,-0.740344226360321,-0.6408556699752808,-0.4104425609111786,-0.3353818655014038,-0.6775698661804199,-0.7086683511734009,-0.9370354413986206,-0.7324972152709961,3.193037986755371,3.0153839588165283,3.2098007202148438,3.411353826522827,3.419900894165039,2.993455171585083,3.2046985626220703,3.197061538696289,3.5652174949645996,3.1496386528015137,3.4359893798828125,3.163926362991333,3.4493963718414307,3.234320640563965,3.2002310752868652,3.272998094558716,3.3818790912628174,3.3397650718688965,3.0758256912231445,3.1623947620391846,3.384615182876587,3.2408924102783203,3.358213186264038,3.416884660720825,3.167921781539917,3.4248197078704834,3.2836413383483887,3.2514102458953857,3.342782497406006,3.3918700218200684,3.194164752960205,3.047151803970337,3.55344820022583,3.2842931747436523,3.3557095527648926,3.1627235412597656,3.2219274044036865,3.0630640983581543,3.3806440830230713,3.417919874191284,3.0919387340545654,3.379578113555908,3.3994202613830566,3.3199284076690674,-1.7047370672225952,-1.7189042568206787,-1.7727406024932861,-1.7914379835128784,-1.7604166269302368,-1.5030242204666138,-1.8196848630905151,-1.7163331508636475,-1.6494393348693848,-1.6843397617340088,-1.7724716663360596,-1.7692655324935913,-1.7378288507461548,-1.7441824674606323,-1.464735746383667,-1.7443996667861938,-1.7241114377975464,-1.6847602128982544,0.36447063088417053,-1.677884578704834,-1.6699968576431274,-1.7729989290237427,-1.7663508653640747,-1.7174915075302124,-1.603288173675537,-1.7217741012573242,-1.7533800601959229,-1.768631100654602,-1.7753701210021973,-2.003943920135498,-2.018934726715088,-2.0210397243499756,-1.9936052560806274,-1.9008338451385498,-1.989972710609436,-2.0100209712982178,-1.9968105554580688,-2.0147523880004883,-2.0181238651275635,-2.0108845233917236,-1.9405567646026611,-1.9686578512191772,-1.9617516994476318,-2.0073821544647217,-1.730658769607544,-1.9098670482635498,-1.9983524084091187,-2.0032131671905518,-2.021005392074585,-1.78288996219635,-1.9908539056777954,-1.9123069047927856,-1.9989206790924072,-1.8872182369232178,-2.0079305171966553,-1.9907784461975098,-1.9225029945373535,-2.0194551944732666,1.058592438697815],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"#CFD8DC\",\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"other\",\"showlegend\":false,\"x\":[12.260786056518555,10.01115894317627,11.924004554748535,10.16279411315918,12.00646686553955,10.273680686950684,6.795833110809326,10.738099098205566,12.415397644042969,12.344215393066406,8.005266189575195,11.672365188598633,9.678071022033691,11.613967895507812,10.691057205200195,10.729043006896973,12.570730209350586,12.728907585144043,11.094691276550293,12.467524528503418,10.7769136428833,11.365942001342773,11.85193157196045,10.574968338012695,13.300118446350098,8.84424877166748,9.322251319885254,11.329916000366211,9.31579303741455,10.838811874389648,11.459781646728516,11.970154762268066,12.745402336120605,10.581709861755371,9.499332427978516,10.223129272460938,9.936136245727539,11.424376487731934,7.634429931640625,12.565068244934082,12.609332084655762,11.734045028686523,12.76197338104248,8.449498176574707,10.840675354003906,11.946686744689941,8.837322235107422,9.319323539733887,12.509225845336914,10.419879913330078,6.7955498695373535,9.324790954589844,12.908655166625977,11.152015686035156,11.986949920654297,12.218483924865723,11.286534309387207,11.993667602539062,9.96347427368164,11.670063018798828,9.893813133239746,10.930827140808105,9.840456008911133,9.265555381774902,8.651935577392578,9.824747085571289,9.816277503967285,12.087455749511719,11.567899703979492,6.796971321105957,12.668866157531738,12.218778610229492,10.99422836303711,11.2903470993042,13.343911170959473,12.024370193481445,12.494938850402832,12.23508071899414,10.537565231323242,9.071687698364258,9.39167308807373,13.654808044433594,11.47091293334961,11.624275207519531,12.246249198913574,12.476778984069824,11.365124702453613,12.479470252990723,12.073331832885742,10.643670082092285,10.698538780212402,11.801766395568848,11.808372497558594,9.954280853271484,12.55838394165039,8.872209548950195,11.538613319396973,8.866655349731445,10.459000587463379,11.395777702331543,9.855978012084961,11.591615676879883,10.30197811126709,11.654211044311523,11.18188190460205,10.506263732910156,13.050704002380371,11.941532135009766,12.512845993041992,11.652451515197754,13.646256446838379,11.867741584777832,12.657735824584961,12.094124794006348,10.286686897277832,12.581634521484375,11.336400985717773,9.937158584594727,11.021777153015137,9.948188781738281,12.35839557647705,12.886040687561035,9.261913299560547,9.89763355255127,9.462775230407715,9.968618392944336,12.362512588500977,10.358255386352539,8.327774047851562,11.927535057067871,10.00265884399414,12.349808692932129,10.623787879943848,10.678670883178711,11.847443580627441,9.692587852478027,9.262121200561523,10.958932876586914,12.889772415161133,10.948387145996094,11.634035110473633,12.080227851867676,10.378564834594727,10.096247673034668,11.42621898651123,8.7933931350708,11.231094360351562,8.999340057373047,11.527088165283203,12.19135570526123,7.8714375495910645,11.001914978027344,12.61523151397705,9.75306224822998,11.896245002746582,12.523083686828613,9.378335952758789,10.35689926147461,11.08590316772461,13.147154808044434,9.818265914916992,11.11213493347168,11.444499969482422,11.590463638305664,10.737903594970703,9.770731925964355,10.78557014465332,11.933521270751953,10.511313438415527,9.82026481628418,8.356404304504395,10.73415470123291,10.45881462097168,10.396477699279785,12.194050788879395,12.752180099487305,11.78748607635498,10.255226135253906,12.165645599365234,12.292634963989258,12.844697952270508,10.628177642822266,8.569395065307617,11.763611793518066,12.15890884399414,12.740022659301758,12.746966361999512,9.62110424041748,11.45886516571045,10.926743507385254,12.772931098937988,12.837591171264648,12.200713157653809,9.013678550720215,10.967081069946289,9.76983642578125,12.884422302246094,11.670434951782227,13.589189529418945,12.576118469238281,11.409375190734863,9.541470527648926,8.66622257232666,11.438347816467285,11.34601879119873,10.590363502502441,11.35364818572998,11.930695533752441,6.797292709350586,10.794591903686523,10.309884071350098,10.247868537902832,9.914916038513184,13.303763389587402,12.540189743041992,11.354313850402832,8.383038520812988,12.193739891052246,9.227989196777344,9.241358757019043,9.536521911621094,10.984012603759766,12.313669204711914,9.644474029541016,9.546388626098633,9.345878601074219,11.460395812988281,10.699294090270996,9.629450798034668,8.48170280456543,11.0349760055542,10.570517539978027,10.367918014526367,11.483748435974121,10.360127449035645,10.180638313293457,12.789478302001953,10.665085792541504,11.708563804626465,12.97873592376709,11.337641716003418,10.709122657775879,13.167486190795898,12.409531593322754,8.32143497467041,9.063648223876953,10.371134757995605,11.557292938232422,10.586905479431152,9.22173023223877,11.61303424835205,8.834911346435547,13.149587631225586,11.683677673339844,9.396690368652344,11.555591583251953,11.163105964660645,10.546306610107422,13.552511215209961,12.85168170928955,11.200281143188477,9.939804077148438,10.729233741760254,12.406366348266602,11.353873252868652,11.169713973999023,9.460506439208984,10.654193878173828,11.77177906036377,11.467199325561523,12.088428497314453,13.498458862304688,11.279095649719238,12.567978858947754,10.763456344604492,10.033524513244629,12.673567771911621,10.10020637512207,8.970285415649414,9.142569541931152,11.128690719604492,12.259600639343262,8.503023147583008,11.229401588439941,9.610413551330566,10.481382369995117,12.30782699584961,11.052600860595703,13.069833755493164,9.056635856628418,9.835722923278809,11.540289878845215,10.57509994506836,10.630483627319336,12.53976058959961,12.878406524658203,12.195188522338867,12.416590690612793,8.807579040527344,12.123668670654297,10.993809700012207,11.357285499572754,11.609417915344238,8.505378723144531,9.010087966918945,9.596749305725098,12.824377059936523,10.293586730957031,11.473138809204102,12.474746704101562,10.027557373046875,12.210655212402344,10.91020393371582,13.3267240524292,13.175515174865723,11.291525840759277,11.628263473510742,11.242234230041504,9.939929008483887,12.68603515625,9.105117797851562,12.55679702758789,10.582772254943848,11.557234764099121,12.54797649383545,11.068909645080566,12.6818208694458,11.577909469604492,11.18771743774414,13.261770248413086,11.224922180175781,11.866551399230957,9.587552070617676,10.551297187805176,11.903278350830078,12.07540225982666,10.768095970153809,12.016518592834473,8.86601448059082,12.697779655456543,12.205658912658691,10.315733909606934,11.997912406921387,12.761860847473145,11.485032081604004,12.877477645874023,10.77319622039795,9.71633529663086,12.789429664611816,10.579119682312012,12.169694900512695,11.369839668273926,12.047872543334961,11.594593048095703,11.944458961486816,11.05200481414795,9.081618309020996,12.835399627685547,12.314804077148438,9.093341827392578,11.577603340148926,9.215981483459473,11.374435424804688,12.266894340515137,11.160942077636719,8.430356979370117,12.54307746887207,11.239245414733887,10.539368629455566,11.465906143188477,7.657243728637695,12.263589859008789,9.044021606445312,11.270294189453125,9.69029426574707,12.90316104888916,11.74852466583252,8.697430610656738,10.244534492492676,13.194223403930664,8.26854419708252,12.263144493103027,12.15650463104248,10.97741985321045,11.557283401489258,12.207938194274902,9.880388259887695,13.715070724487305,8.669084548950195,11.517677307128906,12.063190460205078,10.891190528869629,8.157964706420898,13.416830062866211,12.172284126281738,9.77486515045166,12.416096687316895,8.317665100097656,12.529857635498047,10.84004020690918,8.873795509338379,12.759370803833008,10.572245597839355,11.383156776428223,11.408194541931152,12.709611892700195,13.176079750061035,7.645970344543457,10.45201587677002,11.888107299804688,8.155491828918457,10.934144973754883,12.047857284545898,11.55852222442627,10.887688636779785,10.773085594177246,10.705704689025879,9.601393699645996,10.455825805664062,9.596738815307617,10.703749656677246,11.308526992797852,9.27015209197998,10.01099681854248,12.847784996032715,12.372241020202637,10.8928804397583,11.546853065490723,9.608847618103027,10.049355506896973,10.234009742736816,12.77387809753418,7.639250755310059,9.369508743286133,11.111124992370605,10.788805961608887,9.64129638671875,10.971961975097656,13.150918960571289,10.595973014831543,9.54521369934082,12.856410026550293,10.756369590759277,11.570320129394531,10.633820533752441,10.61214542388916,11.196186065673828,11.974361419677734,11.977964401245117,9.437406539916992,11.594237327575684,11.464212417602539,10.141727447509766,11.141068458557129,9.565552711486816,12.504912376403809,10.439518928527832,8.916755676269531,12.542708396911621,8.100579261779785,13.614336013793945,9.942835807800293,12.88668441772461,12.289319038391113,10.191901206970215,11.653940200805664,12.273872375488281,9.7015380859375,11.526108741760254,11.894497871398926,9.942673683166504,12.299449920654297,11.311140060424805,12.881195068359375,8.470643043518066,12.338445663452148,11.502923011779785,10.742378234863281,12.947842597961426,11.659990310668945,12.496929168701172,8.366921424865723,9.1043119430542,12.329166412353516,11.74715518951416,11.585480690002441,10.612318992614746,11.377769470214844,11.649738311767578,11.907842636108398,13.692667007446289,11.34955883026123,12.52751636505127,8.80586051940918,12.406229019165039,12.7645902633667,12.445311546325684,11.75776195526123,11.090094566345215,11.42638111114502,10.89482593536377,10.761418342590332,11.150059700012207,10.034558296203613,10.338700294494629,11.178735733032227,12.835885047912598,11.53531551361084,12.381514549255371,11.54314136505127,11.90941047668457,9.358724594116211,10.740930557250977,11.939253807067871,11.149989128112793,11.659534454345703,10.353327751159668,11.331498146057129,10.64175796508789,12.81151008605957,8.662858009338379,10.857192993164062,10.787764549255371,8.953280448913574,12.585387229919434,7.643540382385254,10.447787284851074,10.80845832824707,12.5751371383667,9.113858222961426,9.70373821258545,12.737845420837402,11.416006088256836,12.489952087402344,8.133481979370117,10.278565406799316,11.977149963378906,9.470913887023926,10.66490364074707,10.00718879699707,9.176525115966797,11.815064430236816,8.752781867980957,11.483132362365723,11.89669418334961,12.553421020507812,13.118978500366211,9.782742500305176,10.74406623840332,12.168991088867188,11.756896018981934,12.940293312072754,11.56460189819336,8.64371109008789,11.702585220336914,11.278419494628906,10.558026313781738,11.820279121398926,10.459695816040039,11.502100944519043,11.530735969543457,10.50390911102295,12.36990737915039,12.139140129089355,11.131754875183105,10.48403263092041,13.041622161865234,11.032854080200195,11.734776496887207,10.479324340820312,9.945302963256836,11.825505256652832,11.676788330078125,9.411858558654785,12.318131446838379,11.462925910949707,12.87320613861084,13.661906242370605,12.474398612976074,10.583564758300781,11.804007530212402,11.14046573638916,11.841476440429688,11.781903266906738,9.183506965637207,11.8534574508667,11.49117660522461,11.63294506072998,11.224637985229492,12.764777183532715,12.330574035644531,11.128299713134766,12.93774127960205,10.32089614868164,12.478330612182617,11.827890396118164,11.804841041564941,11.44624137878418,11.820039749145508,10.238183975219727,7.784416675567627,12.978934288024902,12.451269149780273,9.322071075439453,11.821974754333496,10.541068077087402,11.961499214172363,8.238553047180176,12.170052528381348,12.736010551452637,12.764181137084961,11.65296745300293,12.675060272216797,11.974373817443848,10.117522239685059,9.6543607711792,12.757356643676758,12.056167602539062,9.412654876708984,11.711962699890137,9.174324989318848,10.823249816894531,9.90158462524414,10.913339614868164,8.113393783569336,9.993379592895508,12.194329261779785,13.33237361907959,11.182921409606934,12.732467651367188,13.046422004699707,9.75806713104248,8.685029029846191,11.561057090759277,10.053182601928711,11.998501777648926,13.20248031616211,12.104700088500977,10.906420707702637,11.117636680603027,10.446064949035645,9.715156555175781,12.530933380126953,12.754565238952637,11.461576461791992,11.491990089416504,9.396021842956543,8.587562561035156,12.263270378112793,11.42301082611084,13.491022109985352,10.364217758178711,11.991475105285645,12.44273853302002,11.70559310913086,10.35807991027832,9.996321678161621,9.326005935668945,12.807846069335938,13.570623397827148,11.12037467956543,10.716169357299805,10.50700569152832,10.22071361541748,12.695380210876465,11.297102928161621,9.085335731506348,12.861201286315918,11.170536994934082,10.041293144226074,10.805912017822266,11.3994722366333,9.783839225769043,11.411025047302246,10.608845710754395,9.42233657836914,11.715644836425781,11.849699020385742,12.349161148071289,8.975476264953613,10.749741554260254,9.420133590698242,10.120325088500977,12.375459671020508,10.686391830444336,11.969694137573242,11.85922622680664,12.443221092224121,9.37954330444336,13.088716506958008,10.281312942504883,12.127240180969238,11.985560417175293,12.442952156066895,12.119172096252441,9.390953063964844,10.271842956542969,10.183282852172852,12.758428573608398,9.88254451751709,11.704732894897461,11.48423957824707,9.189234733581543,11.172453880310059,11.48064136505127,10.315546035766602,12.061239242553711,11.559906005859375,11.86010456085205,11.664885520935059,12.843629837036133,9.624467849731445,12.458562850952148,12.246566772460938,11.838500022888184,10.999716758728027,9.12990951538086,11.840607643127441,11.96013069152832,11.915284156799316,12.08629035949707,10.903203964233398,8.849540710449219,11.167780876159668,10.531004905700684,12.467206954956055,10.145415306091309,9.549249649047852,6.795407295227051,12.180326461791992,12.817878723144531,10.757889747619629,11.568016052246094,11.672552108764648,7.97341775894165,10.022518157958984,9.927380561828613,10.70942211151123,11.941184043884277,12.794625282287598,12.879822731018066,12.178878784179688,11.878340721130371,10.52446460723877,10.071585655212402,10.56197738647461,11.002744674682617,12.245721817016602,10.67922592163086,12.140796661376953,12.277287483215332,11.456153869628906,12.081787109375,11.62030029296875,10.880305290222168,10.69717025756836,9.24069881439209,12.846972465515137,10.142430305480957,12.146927833557129,9.958215713500977,9.50244140625,10.51760196685791,10.551128387451172,11.390852928161621,11.688502311706543,11.491974830627441,12.741220474243164,12.295635223388672,12.218851089477539,8.685344696044922,13.047622680664062,11.958925247192383,9.383042335510254,11.218199729919434,11.51757526397705,9.624102592468262,13.13804817199707,12.326669692993164,12.624595642089844,12.161945343017578,9.43790054321289,11.324953079223633,8.503750801086426,11.257233619689941,10.58021354675293,9.62783145904541,9.703179359436035,11.924786567687988,8.906972885131836,9.254066467285156,12.899577140808105,11.277064323425293,11.152223587036133,12.76430606842041,10.148778915405273,12.378602027893066,11.560495376586914,12.818056106567383,12.91047191619873,8.343107223510742,10.998586654663086,11.666386604309082,10.141661643981934,11.702606201171875,10.432024955749512,10.108580589294434,9.184564590454102,13.428140640258789,11.512848854064941,12.044175148010254,11.576578140258789,12.28610610961914,11.385955810546875,12.952836036682129,12.021284103393555,11.287923812866211,12.099640846252441,12.082901000976562,12.028682708740234,12.383368492126465,8.773681640625,12.879551887512207,11.319714546203613,8.952622413635254,11.267468452453613,12.477086067199707,12.503398895263672,9.42236614227295,11.20785140991211,11.332497596740723,8.951685905456543,10.856745719909668,10.309080123901367,11.73598861694336,8.723488807678223,12.038217544555664,9.878068923950195,12.995759963989258,10.974395751953125,9.64431381225586,10.158196449279785,13.631218910217285,8.542502403259277,11.165329933166504,10.789525032043457,13.625770568847656,11.960083961486816,12.425254821777344,11.940552711486816,13.470690727233887,13.038926124572754,7.834033012390137,9.87307071685791,11.216033935546875,12.414644241333008,12.093451499938965,11.877202987670898,12.003032684326172,10.026086807250977,12.491192817687988,13.477025032043457,12.77425479888916,10.46001148223877,8.880654335021973,9.948962211608887,10.48256778717041,12.090274810791016,11.583839416503906,11.40567684173584,6.7874345779418945,9.088058471679688,11.758285522460938,11.31846809387207,11.73054313659668,12.907270431518555,12.770581245422363,11.114065170288086,11.227639198303223,10.648099899291992,12.066269874572754,13.3859224319458,12.3936767578125,11.536118507385254,12.09136962890625,7.971031665802002,10.897802352905273,9.173691749572754,11.609256744384766,12.269068717956543,9.517958641052246,11.856656074523926,11.710532188415527,12.882142066955566,12.293147087097168,9.226627349853516,11.812376022338867,9.573324203491211,11.98694896697998,11.025315284729004,12.620606422424316,12.66535758972168,11.626893997192383,11.87524127960205,9.740761756896973,11.66956615447998,9.677865982055664,13.708874702453613,13.700621604919434,10.8056058883667,12.719548225402832,11.30461597442627,9.15710163116455,11.5343599319458,9.219347953796387,11.886124610900879,11.679952621459961,12.069474220275879,12.361296653747559,10.542925834655762,10.790267944335938,12.395968437194824,10.26074504852295,12.145330429077148,11.74125862121582,12.40096664428711,10.930389404296875,8.466429710388184,11.920580863952637,12.118764877319336,13.445467948913574,10.998306274414062,11.986994743347168,12.903426170349121,11.159873962402344,12.18824291229248,9.372452735900879,12.712847709655762,13.078712463378906,10.631550788879395,9.169002532958984,8.790238380432129,11.811575889587402,12.28490924835205,9.769627571105957,12.567846298217773,10.349342346191406,11.191412925720215,11.537104606628418,10.127388000488281,10.513239860534668,8.915346145629883,11.823579788208008,11.909771919250488,11.520251274108887,13.776591300964355,12.594861030578613,6.803589344024658,10.791962623596191,11.956903457641602,7.651430606842041,12.864334106445312,11.632024765014648,10.91291332244873,11.907623291015625,10.763083457946777,11.546504974365234,13.262833595275879,12.147619247436523,11.872856140136719,12.41122055053711,12.41154670715332,13.054023742675781,10.615890502929688,9.164365768432617],\"y\":[1.976219654083252,1.265775442123413,1.0926563739776611,-0.5182633996009827,2.4570963382720947,0.48775091767311096,-0.910475492477417,-0.5370233654975891,1.2164555788040161,1.3136889934539795,-0.12520691752433777,1.813036561012268,1.1669845581054688,1.774245023727417,-0.3897693157196045,1.3097110986709595,3.413337469100952,1.671027660369873,0.8366751074790955,3.2766458988189697,0.3775566816329956,0.44715550541877747,2.3352620601654053,3.230710506439209,2.2592904567718506,0.6792904138565063,0.25117701292037964,0.6122933626174927,1.182349681854248,0.9189866781234741,3.0709445476531982,0.9211485385894775,3.500269889831543,3.7989087104797363,1.823943853378296,1.6083993911743164,-0.23820368945598602,2.4324207305908203,0.19927099347114563,3.7869391441345215,2.054600715637207,0.9205240607261658,3.078073263168335,0.10503587871789932,0.8371968865394592,0.3198699951171875,-0.16870951652526855,2.7976632118225098,2.6678123474121094,-0.570141077041626,-0.9106079339981079,3.7905776500701904,2.7144033908843994,0.9463844299316406,1.3737200498580933,0.33007562160491943,-0.9850686192512512,1.1608271598815918,1.0532095432281494,2.060023546218872,1.5419471263885498,1.308414101600647,-1.1935114860534668,-0.17752167582511902,-0.4857664108276367,3.832371234893799,1.3047598600387573,3.552154541015625,1.942522644996643,-0.909134566783905,2.3067994117736816,0.9871791005134583,2.9792444705963135,2.830186128616333,0.7007690072059631,3.5236191749572754,3.808678388595581,0.5856729745864868,0.5755625367164612,-0.488295316696167,0.6351603269577026,0.5080202221870422,1.0401160717010498,2.3617639541625977,1.8510946035385132,0.9317828416824341,3.2759201526641846,2.4336321353912354,-1.299113392829895,3.8617777824401855,3.202488422393799,1.037284016609192,-0.5317288637161255,-0.5715892314910889,1.2100167274475098,1.357236623764038,3.533149480819702,3.0465500354766846,0.1627165526151657,0.7375492453575134,0.9511770009994507,0.07118048518896103,-0.3919447958469391,1.378462314605713,-0.3244394361972809,0.7082329988479614,2.143453359603882,1.1407673358917236,0.9031526446342468,2.8212311267852783,1.1145505905151367,1.4429399967193604,2.296320676803589,-0.001279126270674169,-0.10712704062461853,3.0526630878448486,0.7859264612197876,-0.6041418313980103,1.40556800365448,-0.5446928143501282,1.2216320037841797,3.4056971073150635,3.86862850189209,1.1494468450546265,0.1541697382926941,-0.2646180987358093,2.934096097946167,1.085005283355713,0.1298372447490692,1.4595853090286255,0.5229455232620239,1.7139097452163696,3.179950475692749,0.8122704029083252,2.313836097717285,0.9151896834373474,0.0694771260023117,2.251661777496338,2.7393033504486084,1.8044404983520508,2.4212090969085693,1.369358777999878,1.4411309957504272,1.778269648551941,1.9663785696029663,-0.2085247039794922,1.8884222507476807,0.01605393923819065,0.9991214275360107,2.1229145526885986,-0.17486093938350677,1.2444441318511963,1.5270284414291382,1.034622311592102,0.5874497890472412,1.0864499807357788,0.7937063574790955,0.2653231918811798,0.6648417711257935,2.1787331104278564,-1.4033302068710327,0.993794858455658,3.9591336250305176,2.2620389461517334,2.2779126167297363,1.026957631111145,1.6402356624603271,1.106472134590149,3.609027624130249,4.001310348510742,2.4320061206817627,1.3439748287200928,1.444767951965332,0.8551923036575317,0.15037234127521515,2.425750494003296,2.3774797916412354,2.089541435241699,0.14875546097755432,1.2632057666778564,1.3453816175460815,3.9026010036468506,0.47671282291412354,1.526449203491211,1.530004620552063,0.44732722640037537,1.5337772369384766,-0.5403087735176086,-1.0967426300048828,0.833954393863678,0.7645677924156189,2.70750093460083,1.2675151824951172,0.1613716185092926,3.286445379257202,0.9860133528709412,3.4668221473693848,1.4706974029541016,-0.06515169888734818,3.346012830734253,3.19698166847229,1.0387732982635498,-0.47503626346588135,1.1350661516189575,-0.12550556659698486,0.48462581634521484,3.611987352371216,0.44945693016052246,-0.9105373620986938,0.9302946925163269,2.0090830326080322,0.1268618106842041,-0.5888694524765015,1.5191290378570557,2.7231905460357666,3.63657283782959,0.20960384607315063,0.04279949516057968,2.0899455547332764,2.395681142807007,-0.3550689220428467,1.1873329877853394,1.6772342920303345,-0.551196813583374,1.0045689344406128,3.5376405715942383,-0.2728564739227295,1.6083232164382935,3.451906681060791,0.4097013771533966,0.28619876503944397,2.9995458126068115,1.430558204650879,2.9486751556396484,1.4180095195770264,3.394707679748535,-1.0172475576400757,1.6186503171920776,2.2325010299682617,3.3960022926330566,2.3339428901672363,2.508234739303589,1.917661428451538,2.007206678390503,0.7356185913085938,1.3998340368270874,1.426846981048584,-0.10650590807199478,3.196619987487793,1.5687898397445679,3.3810524940490723,-0.20458725094795227,1.947638988494873,-0.17957328259944916,0.6638663411140442,1.4990065097808838,1.0839821100234985,-0.13390646874904633,1.798355221748352,1.460438847541809,2.6365201473236084,2.2331058979034424,0.5038972496986389,1.672692894935608,3.618595838546753,2.4914488792419434,-0.7637554407119751,2.602285861968994,1.6152315139770508,2.3165502548217773,4.036905288696289,1.7626341581344604,-0.30745241045951843,1.3061516284942627,3.0544943809509277,2.2432503700256348,0.5260573625564575,0.26869526505470276,0.03409036993980408,-0.5223769545555115,0.39339277148246765,2.0766706466674805,-0.38802358508110046,3.801676034927368,2.3770365715026855,3.5236270427703857,3.0124449729919434,3.137105703353882,3.03592848777771,1.0055789947509766,2.3000667095184326,3.485448122024536,2.8649325370788574,3.2783849239349365,2.067582130432129,1.7887386083602905,1.117841124534607,1.9235390424728394,-0.5475263595581055,2.5567400455474854,-0.42588913440704346,1.6459540128707886,3.0523805618286133,3.548125743865967,0.9675278663635254,1.0511583089828491,1.8744837045669556,-0.2662609815597534,2.020230531692505,2.395968198776245,1.5245238542556763,2.208904504776001,-0.23743867874145508,2.1941235065460205,3.066039800643921,0.8780515789985657,-0.3361736238002777,0.12655974924564362,0.5816671252250671,2.5916388034820557,1.8386311531066895,0.9971460103988647,-0.09463711082935333,1.4201574325561523,3.414994478225708,-0.5812013745307922,1.0859824419021606,3.207399845123291,1.0073139667510986,1.059942603111267,0.9441660642623901,2.421081066131592,-0.9699038863182068,-0.7918293476104736,3.2947211265563965,3.453516721725464,2.6633572578430176,1.12492835521698,0.9515612125396729,1.8676871061325073,2.2264864444732666,0.4588735103607178,-0.7277568578720093,-1.0359772443771362,2.857710599899292,2.6983368396759033,1.5439820289611816,0.8810650110244751,3.6186933517456055,3.788325071334839,2.0987229347229004,3.1248533725738525,-0.6990334391593933,3.2795536518096924,0.4755188822746277,2.3451409339904785,1.9918140172958374,2.239830255508423,2.980741500854492,3.4494681358337402,2.7023935317993164,0.8716931343078613,1.143310546875,0.12861472368240356,1.2377668619155884,-0.3922814130783081,3.3432180881500244,1.7890307903289795,0.9444081783294678,2.8529300689697266,0.18657220900058746,2.6263394355773926,0.48836591839790344,0.770980954170227,0.9084759950637817,2.9118480682373047,0.899668276309967,0.3940073549747467,0.8528211116790771,1.6868844032287598,0.24871914088726044,2.4808976650238037,2.190305709838867,1.4015684127807617,-0.11070117354393005,1.4967073202133179,0.9283192753791809,0.4603424668312073,1.1601427793502808,1.9638222455978394,-1.3094513416290283,1.8755643367767334,-0.034235879778862,2.5533201694488525,2.5379700660705566,3.77651309967041,2.733030319213867,0.3953371047973633,2.7038984298706055,1.0902845859527588,3.95662522315979,3.4667394161224365,1.4680428504943848,-0.2102063000202179,1.9835692644119263,0.25430288910865784,2.2354414463043213,0.19437259435653687,1.457194447517395,1.478986144065857,0.024254370480775833,0.9681276082992554,-1.2682996988296509,1.4637945890426636,0.9524343013763428,3.2517035007476807,1.0663663148880005,1.047045350074768,2.5798704624176025,1.9295272827148438,-0.4900413751602173,1.2294294834136963,3.744098663330078,1.1958093643188477,3.4060301780700684,1.4086087942123413,3.195308208465576,-0.21053600311279297,1.9594486951828003,1.3294121026992798,0.7901973128318787,-1.0279371738433838,0.18727345764636993,2.7890584468841553,1.1563249826431274,-0.1735643744468689,2.2202253341674805,0.9077820777893066,2.668337821960449,2.603579521179199,0.24776341021060944,1.158795714378357,1.6285470724105835,1.96036958694458,3.842803716659546,-0.05351594090461731,1.0603022575378418,3.5160486698150635,3.203173875808716,-0.22633570432662964,1.7337943315505981,1.1724951267242432,0.17315110564231873,0.7105343341827393,2.4431610107421875,3.8104262351989746,0.10712434351444244,0.713238537311554,3.1322901248931885,3.2645468711853027,0.534716784954071,0.5656581521034241,3.5241639614105225,2.5362460613250732,-0.7327529191970825,-0.8334300518035889,2.0500335693359375,0.9878270626068115,2.0549156665802,3.280917167663574,0.543793261051178,0.40602004528045654,1.2652292251586914,1.6847025156021118,0.6613606810569763,0.48743921518325806,1.5848220586776733,0.15714702010154724,3.097687244415283,3.4039037227630615,3.8253872394561768,-0.3216516077518463,0.36484676599502563,3.5613582134246826,0.849107563495636,2.804208278656006,3.8222780227661133,1.8004229068756104,1.4634736776351929,3.1937408447265625,0.8675617575645447,3.476331949234009,0.3727441728115082,3.1206748485565186,1.5812299251556396,-0.6251404285430908,3.3036246299743652,3.7486493587493896,1.7302109003067017,2.3743913173675537,0.9696566462516785,0.4081820845603943,0.6557876467704773,2.6788806915283203,3.448852300643921,1.163936972618103,3.5292248725891113,2.8257150650024414,3.515986680984497,1.9399545192718506,0.4077995717525482,0.766963541507721,-0.4135100543498993,0.42341694235801697,2.8088972568511963,-0.6076659560203552,0.1522863209247589,3.2291979789733887,1.588698387145996,1.8325613737106323,0.9248793125152588,1.3020033836364746,3.420180559158325,1.643860101699829,1.1540601253509521,0.21492479741573334,0.1739095002412796,1.5416576862335205,3.289651393890381,-0.5035665035247803,-0.46198034286499023,3.0262765884399414,3.655320405960083,1.8784151077270508,0.3909582197666168,0.8705004453659058,1.1364428997039795,1.208755612373352,0.26025456190109253,0.44635772705078125,0.12183551490306854,-0.15554407238960266,-0.25469839572906494,-0.45932525396347046,0.3801633417606354,2.1852316856384277,3.0103812217712402,0.16040968894958496,0.17387855052947998,2.1033482551574707,3.5950064659118652,1.6760215759277344,3.2061989307403564,0.3962271809577942,2.2076709270477295,-0.8994259834289551,2.514460802078247,0.979424774646759,3.5055930614471436,0.26043587923049927,0.08888838440179825,0.9722239375114441,1.9090890884399414,1.4592763185501099,0.7288630604743958,0.5196076035499573,2.52851939201355,0.3626737594604492,0.9660953879356384,1.4570801258087158,1.5067415237426758,1.0723236799240112,1.9673024415969849,0.5451234579086304,2.6414546966552734,0.7439343333244324,3.4168541431427,1.108274221420288,0.9759180545806885,3.0499231815338135,0.3342808485031128,1.1762615442276,1.2191663980484009,1.9902498722076416,2.7178072929382324,2.1931447982788086,1.4139740467071533,3.4444408416748047,-0.2844769358634949,2.468306541442871,2.982534408569336,1.1627116203308105,2.345379114151001,-0.26387879252433777,2.15932035446167,-0.09391316771507263,2.315988540649414,3.9019370079040527,1.4993696212768555,0.8209245204925537,0.1551581770181656,2.0490360260009766,1.3861424922943115,1.0227948427200317,-0.12519274652004242,3.113816976547241,1.2089561223983765,-0.26088747382164,1.8862637281417847,3.144066333770752,-1.0393073558807373,2.0746145248413086,1.443308711051941,1.180871844291687,-0.6809756755828857,1.017892599105835,1.8907074928283691,-1.2943123579025269,0.1569855809211731,0.6024736166000366,0.8391635417938232,2.21470046043396,-1.1607123613357544,1.8066606521606445,2.297105312347412,2.370729446411133,1.7525073289871216,0.9918195605278015,0.6040194034576416,2.8629562854766846,3.4195144176483154,1.8813725709915161,3.7637126445770264,0.20570334792137146,2.110116958618164,1.3198965787887573,1.0308681726455688,1.5510575771331787,1.0269742012023926,3.243104934692383,0.5696408748626709,-1.0560033321380615,3.80950927734375,-0.9885093569755554,2.1285574436187744,0.27239152789115906,-0.9925388693809509,2.664095401763916,0.5445898771286011,3.4431591033935547,2.9532716274261475,1.4311859607696533,2.9324100017547607,3.054269313812256,0.8955436944961548,0.025119181722402573,3.7716782093048096,0.6967297196388245,2.6491522789001465,2.385422945022583,3.098644495010376,3.803661823272705,1.4175082445144653,2.0697543621063232,3.0386083126068115,3.8905580043792725,1.805167555809021,3.370250940322876,0.6452329754829407,1.198987364768982,4.435891628265381,3.9789042472839355,0.2224683314561844,0.784505307674408,3.900918483734131,-0.08793401718139648,-0.20969590544700623,0.9383320212364197,2.9824304580688477,0.4146740138530731,3.451068878173828,0.58146733045578,2.629016876220703,2.5896897315979004,0.8220140933990479,2.19604229927063,2.3683815002441406,1.0513523817062378,-0.5379165410995483,2.0229125022888184,2.577913522720337,1.3024059534072876,2.953939914703369,1.1526473760604858,-1.245646595954895,0.6586165428161621,1.9431781768798828,0.795759916305542,0.4541209042072296,1.3323132991790771,0.9036451578140259,1.0493559837341309,2.936338424682617,1.658136010169983,2.7779719829559326,0.217825248837471,-0.08116167783737183,0.8917245864868164,1.92644202709198,1.5383085012435913,3.03080153465271,-0.5870935916900635,1.1749986410140991,1.7433806657791138,3.2300631999969482,1.3536816835403442,1.4494167566299438,2.132977247238159,1.142106533050537,1.0964027643203735,-1.2843916416168213,2.0309720039367676,0.38181358575820923,0.4180848002433777,1.397927165031433,1.0205312967300415,-1.0413587093353271,-0.3866738975048065,-0.9114582538604736,2.2314321994781494,-0.9689415693283081,0.8953730463981628,-0.7223781943321228,3.3937878608703613,-0.1200912594795227,1.4780992269515991,-1.0540677309036255,1.243822693824768,1.1921714544296265,-0.9881787300109863,2.7142724990844727,1.035949468612671,1.7600966691970825,3.096761703491211,1.0765949487686157,-0.3055479824542999,0.8458605408668518,1.8884222507476807,0.7532628178596497,3.692111015319824,2.047429084777832,1.5398664474487305,2.6263246536254883,3.3768866062164307,1.3354458808898926,2.8829421997070312,3.073280096054077,2.618222951889038,2.2130420207977295,1.131805658340454,1.4583265781402588,-0.6291894912719727,3.3120579719543457,3.235522985458374,2.4912800788879395,0.8031163215637207,2.785531520843506,1.2172937393188477,2.5934207439422607,0.44533485174179077,3.7607927322387695,0.8943837881088257,2.2763075828552246,0.15780554711818695,2.040842056274414,0.27494242787361145,3.4471356868743896,2.545928955078125,3.3168694972991943,3.057359218597412,2.0106849670410156,3.7272539138793945,-0.2229020744562149,-0.3912656009197235,2.5650482177734375,0.5276594758033752,1.9719535112380981,3.524745225906372,1.5316689014434814,0.3282936215400696,2.1106743812561035,2.4451394081115723,1.7900829315185547,2.8499183654785156,2.917431354522705,-0.6730669736862183,2.2880237102508545,2.3319878578186035,0.7816553711891174,3.079967975616455,0.3783358633518219,3.771744728088379,3.4605367183685303,3.4098503589630127,2.232668399810791,2.7577970027923584,0.8026202917098999,0.8486886024475098,1.336401104927063,1.9623465538024902,2.0950241088867188,2.3696439266204834,2.694972276687622,0.7788678407669067,1.5454350709915161,0.642559289932251,1.9104548692703247,2.817577838897705,1.2944279909133911,2.2089552879333496,2.9942898750305176,0.6504705548286438,2.6843504905700684,1.2824846506118774,-0.555378794670105,1.9917773008346558,2.4853670597076416,2.2720730304718018,0.05783434212207794,1.741772174835205,3.9805495738983154,1.6283323764801025,3.9042468070983887,-0.08662469685077667,1.0114973783493042,-0.2651676535606384,2.276035785675049,-0.6100764274597168,2.0067296028137207,1.3208794593811035,1.0322285890579224,1.9896197319030762,2.3109211921691895,2.2940375804901123,0.7046573758125305,2.1666882038116455,0.8269322514533997,1.038419246673584,3.1033222675323486,2.011228084564209,0.4995725154876709,3.0119717121124268,1.7737351655960083,1.1829763650894165,3.768042802810669,2.453611135482788,-1.2905018329620361,0.4045698344707489,-0.7276555299758911,1.108733057975769,0.7043386101722717,1.7317860126495361,3.1328790187835693,1.5019123554229736,0.00757896201685071,1.7896612882614136,4.704597473144531,-1.2845878601074219,1.471731185913086,3.7090561389923096,-0.9188922643661499,0.25417712330818176,0.40908655524253845,-0.6070359349250793,3.279542922973633,2.3822507858276367,-1.0290439128875732,-1.0308316946029663,1.6611809730529785,0.5246748328208923,1.5900543928146362,1.5552963018417358,1.9574549198150635,0.3487606346607208,1.906638503074646,-0.15992236137390137,-0.1188686341047287,1.874509334564209,2.240257501602173,3.598365068435669,2.4335744380950928,3.23817777633667,1.554663896560669,2.292228937149048,1.6409223079681396,1.495506763458252,3.2188000679016113,3.7398290634155273,2.8911819458007812,3.769780397415161,2.451735019683838,2.5036814212799072,3.150447130203247,1.2291960716247559,0.9691900610923767,3.689431667327881,-0.07314413040876389,0.73487389087677,0.839801013469696,1.4524890184402466,2.1931369304656982,1.3259851932525635,3.0612916946411133,0.2924751043319702,0.6916547417640686,2.3885679244995117,0.7535249590873718,1.8109625577926636,3.2876040935516357,2.6223843097686768,2.415797472000122,2.450838565826416,1.9013489484786987,2.8400003910064697,3.637033224105835,1.2191064357757568,0.49888309836387634,0.527450680732727,3.3894202709198,2.8021371364593506,1.5861856937408447,-0.5647964477539062,1.773905634880066,2.8972055912017822,0.4014616310596466,3.552330493927002,3.5676872730255127,0.44267264008522034,2.05513334274292,2.6361255645751953,2.9887442588806152,0.289135605096817,3.268169641494751,3.1244053840637207,0.8573437333106995,-0.19672121107578278,-0.7705172300338745,1.6629849672317505,3.544947862625122,-0.6638178825378418,3.600919008255005,-0.5163041353225708,-0.025210803374648094,1.4350130558013916,1.8289581537246704,0.6889894008636475,1.6060850620269775,-0.9072462916374207,2.4202475547790527,1.693398356437683,0.15449632704257965,3.578507661819458,2.314469575881958,4.283806324005127,2.3266613483428955,0.48093342781066895,3.2087161540985107,-0.08493365347385406,1.7356622219085693,1.580642580986023,1.29006028175354,0.9935421347618103,2.0631296634674072,2.585738182067871,1.8831286430358887],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"21_tables_table_tableqa\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"21_tables_table_tableqa\"],\"x\":[10.386235237121582,10.229713439941406,10.389017105102539,10.301673889160156,10.361222267150879,10.3976469039917,10.311823844909668,10.32546329498291,10.373322486877441,10.308487892150879,10.342009544372559,10.355380058288574,10.305009841918945,10.051471710205078,10.35975456237793,10.3198823928833],\"y\":[-0.7522066235542297,-0.7360655665397644,-0.7566789388656616,-0.7287562489509583,-0.7565518617630005,-0.6721810698509216,-0.7266584038734436,-0.7462540864944458,-0.6999889016151428,-0.7211228609085083,-0.7179433107376099,-0.7139708399772644,-0.7397772073745728,-0.6260511875152588,-0.7241697311401367,-0.7212250828742981],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"22_oosf_instances_proxy\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"22_oosf_instances_proxy\"],\"x\":[11.03643798828125,11.07364559173584,11.248010635375977,11.170310974121094,10.949195861816406,11.285311698913574,10.957159996032715,11.096034049987793,11.2238130569458,11.023275375366211,11.042583465576172,11.16788101196289,11.188385009765625,11.112465858459473],\"y\":[3.106004476547241,3.0234382152557373,3.3541853427886963,3.115736246109009,3.0922281742095947,3.283050775527954,3.0349786281585693,3.2528135776519775,3.086516857147217,3.087618112564087,3.101942300796509,3.0745949745178223,3.133884906768799,3.1343841552734375],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"23_figurative_metaphors_sociocultural\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"23_figurative_metaphors_sociocultural\"],\"x\":[12.890239715576172,12.9169282913208,12.898111343383789,12.95676326751709,12.863883018493652,12.8355073928833,12.85640811920166,13.136683464050293,12.889006614685059,7.990469932556152,12.911820411682129,12.735673904418945,12.899605751037598,12.521623611450195],\"y\":[0.026861172169446945,-0.11374158412218094,-0.0861361101269722,-0.08715442568063736,0.14325851202011108,-0.09389468282461166,-0.06494750827550888,-0.21590127050876617,-0.029119817540049553,0.18973292410373688,-0.05029120296239853,0.2098502218723297,-0.02140832133591175,-0.014837851747870445],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"24_sarcasm_irony_humor\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"24_sarcasm_irony_humor\"],\"x\":[7.443446159362793,7.453991889953613,7.434319496154785,7.457515239715576,7.445913314819336,7.432080268859863,7.492039680480957,7.444361686706543,7.45388650894165,7.470397472381592,7.456826210021973,7.457319259643555,7.452033042907715,7.453394889831543],\"y\":[3.3650591373443604,3.3583810329437256,3.3732411861419678,3.3514108657836914,3.3623621463775635,3.374535083770752,3.3646886348724365,3.3602709770202637,3.329127788543701,3.332568883895874,3.3524117469787598,3.345237970352173,3.361050605773926,3.3561806678771973],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"25_dense_retrieval_retrievers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"25_dense_retrieval_retrievers\"],\"x\":[11.089601516723633,11.02492904663086,10.883186340332031,10.981522560119629,10.981151580810547,10.95821762084961,10.9121675491333,10.982845306396484,11.157200813293457,10.896283149719238,10.92767333984375,10.96117877960205,10.979663848876953],\"y\":[2.4378020763397217,2.470427989959717,2.49906587600708,2.4681739807128906,2.4417030811309814,2.4632935523986816,2.5861077308654785,2.4596455097198486,2.333122968673706,2.6070048809051514,2.5345096588134766,2.460688352584839,2.480128526687622],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"26_argumentative_environmental_firms\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"26_argumentative_environmental_firms\"],\"x\":[9.245034217834473,9.240274429321289,9.255699157714844,9.306416511535645,9.330011367797852,9.5535249710083,9.301241874694824,9.279705047607422,9.539828300476074,9.543222427368164,9.282510757446289,9.352497100830078],\"y\":[2.6040866374969482,2.5882256031036377,2.6099462509155273,2.364428997039795,2.5036041736602783,2.5556437969207764,2.5966074466705322,2.450493335723877,2.5414693355560303,2.4918816089630127,2.483466386795044,2.526350498199463],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"27_proprietary_opensourced_recommendatio\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"27_proprietary_opensourced_recommendatio\"],\"x\":[8.700318336486816,8.662346839904785,8.710755348205566,8.685635566711426,10.063787460327148,8.825141906738281,9.746978759765625,8.632473945617676,8.7011137008667,9.762179374694824,10.958747863769531,9.222679138183594],\"y\":[1.2634673118591309,1.199538230895996,1.2940387725830078,1.2151010036468506,2.198474407196045,1.475010633468628,3.3699069023132324,1.1698120832443237,1.2515336275100708,3.3798534870147705,1.8569083213806152,1.78851318359375],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"28_modification_mp2_multiword\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"28_modification_mp2_multiword\"],\"x\":[12.416332244873047,12.561809539794922,12.500757217407227,12.375421524047852,12.55825138092041,12.481864929199219,12.49730396270752,12.1989107131958,12.471221923828125,12.578041076660156,12.469315528869629,12.464476585388184],\"y\":[1.9628962278366089,2.3608059883117676,2.222209930419922,2.1565310955047607,2.1810107231140137,2.1097028255462646,2.3813178539276123,2.188523292541504,2.159067153930664,2.3556671142578125,1.6200826168060303,2.1543469429016113],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"30_counterfactuals_scone_counterfactual\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"30_counterfactuals_scone_counterfactual\"],\"x\":[10.26634407043457,10.128600120544434,10.363204956054688,10.149825096130371,10.254651069641113,10.238754272460938,10.266803741455078,10.18940258026123,10.27680492401123,10.135074615478516,10.226946830749512],\"y\":[2.246767520904541,2.1721770763397217,2.3008975982666016,2.423623561859131,2.2409117221832275,2.265772581100464,2.3193318843841553,2.154175281524658,2.2092831134796143,2.1171963214874268,2.24501371383667],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"47_reasoning_transformer_prompt_paramete\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"47_reasoning_transformer\"],\"x\":[12.020341873168945,12.169936180114746,12.101693153381348,12.094861030578613,11.659080505371094,12.379535675048828,12.050992965698242,12.262616157531738,12.431180000305176,11.988661766052246,12.082118034362793,12.085043907165527,12.180397033691406,12.176328659057617,11.84795093536377,12.084325790405273,12.128952980041504,12.28459644317627,12.194994926452637,12.008187294006348,11.911856651306152,12.048596382141113,12.229466438293457,12.182258605957031,12.1466703414917,12.20826530456543,11.903266906738281,11.976790428161621,11.813048362731934,12.021183967590332,12.020978927612305,12.224409103393555,11.788299560546875,12.209701538085938,11.93454647064209,11.924176216125488,12.019671440124512,12.825284004211426,12.15970516204834,12.297412872314453,12.158596992492676,12.110843658447266,12.26720905303955,12.509387016296387,12.426708221435547,12.31076431274414,12.200416564941406,12.108731269836426,12.027480125427246,12.294480323791504,12.179040908813477,11.868775367736816,12.016388893127441,12.533409118652344,11.871016502380371,12.278585433959961,12.497551918029785,11.953156471252441,12.103267669677734,12.292784690856934,12.235295295715332,11.931829452514648,13.446449279785156,13.293713569641113,13.339015007019043,13.234209060668945,13.390213012695312,13.275945663452148,13.059819221496582,13.199735641479492,13.168375968933105,13.352516174316406,13.391827583312988,13.276883125305176,13.3883638381958,13.391206741333008,13.22536563873291,13.355441093444824,13.415063858032227,13.445826530456543,13.263446807861328,13.253372192382812,13.080596923828125,13.292510986328125,13.387595176696777,13.407983779907227,13.065677642822266,13.157443046569824,13.285446166992188,13.371879577636719,13.443427085876465,13.312077522277832,13.325447082519531,13.201516151428223,13.412653923034668,13.387580871582031,13.376065254211426,13.23760986328125,13.391357421875,13.28543758392334,13.155158996582031,13.134252548217773,13.264272689819336,13.326519012451172,13.441741943359375,13.180994987487793,13.229814529418945,13.385966300964355,13.26877498626709,13.252023696899414,9.99891185760498,9.666812896728516,9.87675666809082,9.759414672851562,9.77737045288086,9.712926864624023,9.860082626342773,10.150372505187988,9.757026672363281,9.756099700927734,9.88352108001709,9.667094230651855,9.834576606750488,9.662117958068848,9.90071964263916,9.823662757873535,9.885151863098145,9.897980690002441,9.599701881408691,9.828813552856445,10.166223526000977,9.664942741394043,9.68136215209961,9.909587860107422,9.613624572753906,9.683416366577148,9.677172660827637,9.895627975463867,9.831110954284668,9.791001319885254,9.888591766357422,10.030932426452637,10.003292083740234,11.668291091918945,11.893047332763672,11.399813652038574,11.399984359741211,11.689655303955078,11.275130271911621,11.221017837524414,12.198081016540527,12.027531623840332,11.27303695678711,11.897500038146973,11.873612403869629,11.806347846984863,11.46186637878418,11.398143768310547,11.580697059631348,11.322710990905762,11.420403480529785,11.564152717590332,11.334028244018555,11.482306480407715,11.22659969329834,11.433297157287598,11.450419425964355,11.489837646484375,11.502764701843262,11.421748161315918,11.442119598388672,12.117900848388672,11.278861999511719,9.238378524780273,9.30147647857666,9.232583999633789,9.182598114013672,9.238131523132324,9.218341827392578,9.234349250793457,9.302995681762695,9.232303619384766,9.124175071716309,9.416687965393066,9.293940544128418,9.349409103393555,9.29613208770752,9.22963809967041,9.208431243896484,9.365690231323242,9.037209510803223,9.345200538635254,9.246965408325195,9.236005783081055,9.268945693969727,11.956093788146973,12.490596771240234,12.429107666015625,12.604439735412598,12.537409782409668,12.447066307067871,12.514530181884766,12.494388580322266,12.444210052490234,12.318142890930176,12.335335731506348,12.499151229858398,12.503713607788086,12.477373123168945,12.43051815032959,12.483855247497559,12.453995704650879,12.551338195800781,12.408796310424805,12.414511680603027,11.69098949432373],\"y\":[3.4334287643432617,3.9167301654815674,3.949054002761841,3.438246250152588,3.445382595062256,3.36970853805542,3.7780611515045166,2.977111577987671,3.824798583984375,4.014473915100098,3.966614246368408,3.947669744491577,3.546800136566162,3.5740740299224854,3.722076654434204,3.7931604385375977,3.4850759506225586,3.1680963039398193,4.030865669250488,3.494990587234497,3.631016969680786,3.9821364879608154,3.874598264694214,3.628248453140259,3.362018585205078,3.6282668113708496,3.4427807331085205,3.539586305618286,3.9404172897338867,3.503599166870117,3.3990092277526855,3.441103219985962,2.9686286449432373,3.454322576522827,3.507044553756714,3.588791847229004,3.930246591567993,3.598909854888916,3.4998040199279785,3.598375082015991,3.547398805618286,3.815009355545044,3.1761443614959717,3.7004573345184326,2.9148776531219482,3.357351303100586,3.646010398864746,4.046837329864502,3.844369649887085,3.3957107067108154,3.5331528186798096,3.4589388370513916,3.6157045364379883,3.513286590576172,3.4719655513763428,3.176704168319702,3.3323616981506348,3.9445858001708984,3.7024219036102295,3.222160577774048,3.219435214996338,3.6485061645507812,2.849513053894043,2.6924357414245605,2.519382953643799,2.685671091079712,3.4131829738616943,3.3496320247650146,2.9062132835388184,2.5787570476531982,2.8339319229125977,2.6660850048065186,3.363393783569336,3.0692508220672607,3.403278112411499,2.94502854347229,2.862663984298706,2.849548578262329,3.311357021331787,2.857738494873047,2.829335927963257,2.7103164196014404,2.6873369216918945,3.1780202388763428,2.415661573410034,2.984997510910034,2.9960906505584717,3.0859627723693848,3.064985752105713,3.3739285469055176,2.99613094329834,3.3052146434783936,2.759016990661621,2.554168224334717,2.7810420989990234,3.370600700378418,2.8825161457061768,2.5941309928894043,3.402747631072998,3.1956849098205566,2.574057102203369,2.4850573539733887,3.1060783863067627,2.757913827896118,2.8156826496124268,2.6702122688293457,3.1264901161193848,3.3880455493927,2.8700947761535645,2.6230366230010986,-1.2806227207183838,-1.4134907722473145,-1.3984967470169067,-0.8591680526733398,-1.1667323112487793,-0.9986860752105713,-1.226072072982788,-1.2499401569366455,-1.3567789793014526,-1.3018178939819336,-1.279323697090149,-1.3926376104354858,-0.8900179862976074,-1.3667707443237305,-1.3356115818023682,-1.4169893264770508,-1.3518718481063843,-1.17157781124115,-1.1077362298965454,-1.3785203695297241,-1.242591381072998,-1.3728218078613281,-1.36669921875,-1.1680376529693604,-1.3303998708724976,-0.8987433314323425,-1.368369460105896,-1.1938246488571167,-1.3907575607299805,-1.302925705909729,-1.1836531162261963,-1.1196755170822144,-1.154442310333252,1.6213551759719849,1.7356750965118408,1.8189188241958618,2.011199474334717,1.6284370422363281,1.6911828517913818,1.434216856956482,1.9124113321304321,1.708951473236084,1.345273733139038,1.9241002798080444,1.706599235534668,1.5955106019973755,1.6075619459152222,1.8218833208084106,1.9240765571594238,1.5612646341323853,1.4347044229507446,1.8333743810653687,1.4203897714614868,1.508184552192688,1.6106964349746704,1.7237390279769897,1.6263391971588135,1.7565888166427612,1.8806627988815308,1.4971145391464233,1.4235330820083618,2.043459415435791,1.6557258367538452,0.11233166605234146,0.2616926431655884,0.13499541580677032,0.13999022543430328,0.13154223561286926,0.03455710411071777,0.1343674212694168,0.09137420356273651,0.0880853608250618,-0.03193724900484085,0.282012939453125,0.19786401093006134,0.27887430787086487,0.23954655230045319,0.1252308189868927,0.2048257291316986,0.21515387296676636,0.2964838743209839,0.28831443190574646,0.10827220976352692,0.010295077227056026,0.17448420822620392,-0.01690675877034664,-0.3107600808143616,-0.2527309060096741,-0.2602718472480774,-0.33801159262657166,-0.32423314452171326,-0.19620446860790253,-0.286507248878479,-0.2868269979953766,2.4987032413482666,-0.19626036286354065,-0.2882573902606964,2.8162295818328857,-0.27515092492103577,-0.28718024492263794,-0.29625701904296875,-0.3112342059612274,-0.32359012961387634,-0.29335451126098633,-0.24639558792114258,1.748329520225525],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"48_chatgpt_responses_chatgpts_instructio\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"48_chatgpt_responses\"],\"x\":[8.408228874206543,8.558862686157227,8.468222618103027,8.394969940185547,8.488740921020508,8.462080955505371,8.458389282226562,8.648653030395508,8.45824146270752,8.496983528137207,8.497143745422363,8.60595989227295,8.652069091796875,8.46937370300293,8.726569175720215,8.704874992370605,8.474954605102539,8.499213218688965,8.679919242858887,8.863896369934082,8.659948348999023,8.705171585083008,8.791703224182129,8.700974464416504,8.720808029174805,9.014729499816895,8.698103904724121,8.63776969909668,8.665627479553223,8.607316970825195],\"y\":[0.8488311767578125,0.8990485668182373,0.930065929889679,0.9668167233467102,0.8830121159553528,0.8673952221870422,0.9546473622322083,0.9203405976295471,0.9518395662307739,0.8990188241004944,0.8826806545257568,1.001754879951477,0.8333197236061096,0.94631427526474,0.7836666703224182,0.7688788771629333,0.916922390460968,0.7554416656494141,0.4001754820346832,0.5290360450744629,0.39178118109703064,0.5974481105804443,0.46730977296829224,0.44593337178230286,0.3917016386985779,0.42155221104621887,0.3866553008556366,0.5923276543617249,0.5228016972541809,0.7295419573783875],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"49_dialogue_languages_dataset_language_d\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"49_dialogue_languages\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"x\":[13.87353229522705,13.208016395568848,14.106237411499023,13.092193603515625,13.336145401000977,13.386882781982422,12.319914817810059,13.021512985229492,13.98583698272705,12.727828025817871,14.158334732055664,12.96531867980957,13.214096069335938,13.292146682739258,13.683382034301758,13.187692642211914,13.279541969299316,12.989252090454102,12.971858978271484,12.976456642150879,13.035022735595703,12.499113082885742,13.1813325881958,12.979851722717285,13.296390533447266,13.039684295654297,13.842495918273926,13.132991790771484,13.9254732131958,12.167425155639648,14.098097801208496,13.920403480529785,12.558815002441406,12.849774360656738,12.970587730407715,14.02845287322998,12.911493301391602,13.765254974365234,13.905959129333496,13.326302528381348,13.942405700683594,12.925593376159668,13.41197395324707,13.25645637512207,12.991842269897461,13.592504501342773,13.934208869934082,13.955780029296875,13.298561096191406,13.263577461242676,12.345218658447266,13.947273254394531,13.009997367858887,13.643670082092285,13.180303573608398,13.74915599822998,12.808438301086426,13.863404273986816,12.991873741149902,13.162498474121094,12.859356880187988,13.075448036193848,13.156818389892578,12.886246681213379,13.997769355773926,13.03771686553955,12.784741401672363,12.894862174987793,12.97797966003418,13.60038948059082,14.051366806030273,12.930017471313477,12.712410926818848,12.782764434814453,12.85952377319336,12.852415084838867,13.387727737426758,13.939677238464355,12.471564292907715,13.912345886230469,12.641358375549316,13.367488861083984,13.8226318359375,13.64598560333252,13.757844924926758,12.249322891235352,13.732461929321289,13.127846717834473,12.968668937683105,13.813699722290039,13.807978630065918,13.008394241333008,13.787175178527832,13.993060111999512,13.531773567199707,13.984949111938477,13.01711654663086,14.002456665039062,13.822026252746582,13.829243659973145,13.329290390014648,12.908177375793457,14.121039390563965,13.951969146728516,12.813560485839844,13.843708038330078,13.839166641235352,14.121064186096191,12.558928489685059,13.817070960998535,13.30069637298584,13.859855651855469,13.09512710571289,12.266813278198242,12.803173065185547,13.087562561035156,13.733831405639648,13.884516716003418,12.79514217376709,13.156198501586914,12.635796546936035,13.271025657653809,12.98554801940918,13.871160507202148,13.031496047973633,14.019685745239258,13.770309448242188,13.913249969482422,13.995428085327148,12.395220756530762,13.254109382629395,12.519502639770508,13.016617774963379,13.239713668823242,13.700984954833984,13.32162857055664,13.05946159362793,12.602531433105469,12.626888275146484,13.298456192016602,13.010909080505371,12.473347663879395,13.30741024017334,12.52789306640625,13.24752140045166,14.047928810119629,12.949474334716797,13.951871871948242,14.054276466369629,12.840779304504395,12.930652618408203,13.249177932739258,14.00323486328125,12.957128524780273,13.71989631652832,12.736191749572754,13.225967407226562,11.134943962097168,11.025127410888672,11.245323181152344,11.604127883911133,11.156891822814941,11.352185249328613,11.527443885803223,11.282083511352539,11.126087188720703,11.73695182800293,11.573534965515137,13.375391006469727,11.404452323913574,11.218412399291992,10.824934959411621,11.162917137145996,11.561422348022461,11.366466522216797,10.656908988952637,11.300393104553223,11.272549629211426,11.278218269348145,11.19459056854248,11.2599458694458,11.612763404846191,10.9222412109375,11.204724311828613,11.096510887145996,11.001474380493164,11.493638038635254,11.407891273498535,11.244793891906738,11.168529510498047,11.017486572265625,11.451024055480957,11.223573684692383,11.187163352966309,11.271790504455566,11.26801872253418,11.24870491027832,11.188407897949219,11.215272903442383,11.429038047790527,11.241147994995117,11.28900146484375,11.684981346130371,11.643854141235352,11.422016143798828,11.644282341003418,11.211750030517578,11.794293403625488,10.666879653930664,11.676538467407227,11.204644203186035,11.4844388961792,11.210556983947754,10.822954177856445,11.602618217468262,11.172928810119629,11.261439323425293,11.18604850769043,11.24404525756836,11.392868995666504,11.273892402648926,11.069701194763184,11.81900691986084,11.298412322998047,11.697004318237305,11.151240348815918,11.614058494567871,11.220532417297363,11.269240379333496,11.204486846923828,11.25660228729248,11.144911766052246,11.228436470031738,11.389638900756836,11.027185440063477,11.341411590576172,11.200804710388184,10.692567825317383,11.169511795043945,11.02439022064209,10.92267894744873,11.365792274475098,11.304564476013184,11.123757362365723,11.317601203918457,11.2233304977417,11.252074241638184,10.953468322753906,11.1937837600708,11.191131591796875,11.507041931152344,13.65318489074707,13.939993858337402,13.685359001159668,13.784351348876953,13.504968643188477,13.94735336303711,13.143325805664062,13.834990501403809,13.44897747039795,13.599161148071289,13.199520111083984,13.714217185974121,13.943765640258789,13.477546691894531,13.838189125061035,13.14396858215332,13.451980590820312,13.139006614685059,13.305334091186523,13.572847366333008,13.578728675842285,13.640463829040527,13.756519317626953,13.478137016296387,13.640913963317871,13.371881484985352,13.30630874633789,13.778650283813477,13.48436164855957,13.48245620727539,13.242944717407227,13.188318252563477,13.671472549438477,13.161858558654785,13.182262420654297,13.67568302154541,13.578418731689453,13.574835777282715,13.530089378356934,12.509833335876465,13.550978660583496,13.269086837768555,13.929557800292969,13.697040557861328,13.825508117675781,13.642824172973633,13.51134204864502,13.453692436218262,13.557821273803711,13.553349494934082,13.648940086364746,13.369372367858887,13.12069034576416,13.566463470458984,13.692540168762207,13.704095840454102,13.338266372680664,13.631470680236816,13.964845657348633,13.344286918640137,13.934170722961426,13.56275463104248,13.731254577636719,13.44790267944336,13.115975379943848,13.600167274475098,8.303739547729492,13.527002334594727,13.690975189208984,13.423891067504883,13.938860893249512,13.628800392150879,13.415291786193848,13.883581161499023,13.465723037719727,13.598390579223633,13.958017349243164,13.456777572631836,13.795967102050781,9.905937194824219,13.677034378051758,13.943849563598633,13.411235809326172,13.518133163452148,13.331656455993652,8.079171180725098,8.06274700164795,7.97455358505249,7.848491191864014,8.032004356384277,7.964422702789307,7.964263916015625,8.134442329406738,8.114529609680176,7.841091632843018,7.8066725730896,8.066001892089844,7.995242118835449,8.329507827758789,8.135189056396484,7.784605979919434,7.775929927825928,8.180281639099121,8.422222137451172,7.919281005859375,7.885015487670898,7.895800590515137,7.771629810333252,8.177929878234863,7.973200798034668,8.28538703918457,7.766735553741455,7.8133392333984375,7.980544567108154,7.834801197052002,7.792785167694092,8.400049209594727,8.152592658996582,7.950541019439697,7.951925754547119,8.21005630493164,7.920266628265381,7.854425430297852,8.000518798828125,7.90179443359375,8.401914596557617,8.250751495361328,7.81003999710083,8.432668685913086,7.853431224822998,8.134767532348633,8.218738555908203,7.8007283210754395,8.225289344787598,7.825068473815918,8.072797775268555,8.098258972167969,8.405604362487793,7.813677787780762,7.821072101593018,7.985950946807861,7.810969352722168,8.3153657913208,8.099970817565918,7.857753276824951,8.26423168182373,7.91890811920166,7.952602386474609,7.912245273590088,7.974617958068848,7.784420490264893,8.138467788696289,8.036944389343262,7.761767864227295,7.890107154846191,8.475704193115234,8.049864768981934,7.910464286804199,7.735909938812256,8.021018028259277,7.9187726974487305,7.908551216125488,7.699703216552734,7.959645748138428,7.826615333557129,8.282672882080078,7.934024333953857,8.200965881347656,7.851012229919434,7.659298419952393,7.964986801147461,7.375672340393066,7.989088535308838,8.16100788116455,7.880589008331299,7.8358564376831055,7.8488359451293945,7.915194511413574,8.008590698242188,7.819448471069336,7.807568073272705,7.392760753631592,7.372259140014648,7.680557727813721,8.323735237121582,7.405981540679932,7.390983581542969,8.47547721862793,7.814692974090576,8.292101860046387,8.115535736083984,7.816876411437988,7.764798164367676,7.4307541847229,8.173230171203613,7.816125869750977,7.800354957580566,7.687350749969482,7.416838645935059,8.04630184173584,7.372389793395996,8.061315536499023,7.425755023956299,7.877260684967041,8.388016700744629,7.779290199279785,8.227629661560059,8.057212829589844,8.02259349822998,7.782587051391602,8.23399543762207,7.90402889251709,7.8760199546813965,7.4668121337890625,8.308961868286133,7.824159622192383,7.712260723114014,7.914191246032715,7.685945987701416,7.940428733825684,10.677026748657227,10.417326927185059,10.19378662109375,10.34382438659668,10.359889030456543,10.168207168579102,10.379969596862793,10.382912635803223,10.3951997756958,10.006451606750488,10.593812942504883,10.523648262023926,10.564718246459961,10.390353202819824,10.409890174865723,10.375975608825684,10.270461082458496,10.380226135253906,10.342330932617188,10.389986038208008,10.570111274719238,10.563359260559082,10.393143653869629,10.445023536682129,10.406803131103516,10.406514167785645,9.976093292236328,10.553617477416992,10.514777183532715,10.404207229614258,10.352548599243164,10.402541160583496,10.373939514160156,10.66649055480957,10.147748947143555,10.177634239196777,10.380538940429688,10.481019020080566,10.569191932678223,10.38936710357666,10.453319549560547,10.417245864868164,10.462665557861328,10.532641410827637,10.139423370361328,10.995478630065918,10.441753387451172,10.408876419067383,10.595641136169434,10.508939743041992,10.599928855895996,10.383362770080566,10.315134048461914,10.520706176757812,10.196410179138184,10.421682357788086,10.112748146057129,10.42435073852539,9.957427978515625,9.407848358154297,9.24934196472168,9.988895416259766,9.616469383239746,9.254940032958984,9.619887351989746,9.763313293457031,9.41704273223877,9.903557777404785,9.640640258789062,9.904218673706055,9.833464622497559,9.501309394836426,10.281272888183594,9.5150146484375,9.301063537597656,9.468867301940918,9.583669662475586,9.652227401733398,9.2976655960083,9.610597610473633,9.950066566467285,9.628329277038574,9.354300498962402,9.685493469238281,9.437472343444824,9.71446418762207,9.970356941223145,9.591911315917969,9.4934720993042,11.245697975158691,9.630011558532715,9.857994079589844,9.644820213317871,9.684257507324219,9.61834716796875,9.774521827697754,9.085481643676758,10.036049842834473,9.738116264343262,9.506299018859863,9.30492877960205,9.699036598205566,9.771791458129883,9.192770957946777,9.514463424682617,9.63692855834961,9.843480110168457,9.38808536529541,9.520946502685547,9.217379570007324,9.838109970092773,9.648056030273438,9.77692699432373,9.458232879638672,9.670414924621582,9.680063247680664,9.057123184204102,9.02020263671875,8.764460563659668,9.149687767028809,8.96601676940918,8.83845329284668,7.854395389556885,8.691113471984863,8.074891090393066,7.991252422332764,8.15771198272705,8.11121940612793,7.889158725738525,8.559086799621582,7.732449054718018,8.129582405090332,7.739405155181885,8.906782150268555,8.779287338256836,8.384539604187012,8.096807479858398,7.73988151550293,7.74143123626709,10.807848930358887,8.938881874084473,8.25653076171875,8.156562805175781,8.910407066345215,8.929011344909668,8.912620544433594,8.349308967590332,9.150344848632812,9.063459396362305,8.188298225402832,8.082099914550781,8.124395370483398,8.095884323120117,7.761401653289795,7.85822057723999,7.735099792480469,9.143465995788574,7.9051079750061035,8.62359619140625,8.771903991699219,8.10483455657959,8.881553649902344,7.842296600341797,7.751319408416748,8.932684898376465,8.793137550354004,8.686760902404785,8.676131248474121,8.958822250366211,7.80643892288208,8.740694046020508,7.7468767166137695,8.771867752075195,9.189112663269043,9.039446830749512,9.115840911865234,9.096116065979004,9.138891220092773,8.879927635192871,8.893594741821289,8.761467933654785,8.84201431274414,8.92170524597168,8.984542846679688,8.870153427124023,9.148591041564941,9.1036376953125,8.759328842163086,9.173047065734863,9.000730514526367,9.311477661132812,9.226938247680664,8.916337966918945,9.266477584838867,8.85345458984375,8.894457817077637,9.150871276855469,8.838128089904785,8.816519737243652,9.046035766601562,8.901216506958008,9.159720420837402,8.813364028930664,9.301010131835938,8.885666847229004,9.035273551940918,9.151689529418945,9.173745155334473,9.153688430786133,9.068913459777832,9.077773094177246,8.93253231048584,9.23687744140625,8.845597267150879,8.906045913696289,9.104510307312012,8.794829368591309,8.974413871765137,8.559188842773438,9.103609085083008,8.886739730834961,8.847504615783691,8.852912902832031,8.7335786819458,8.971410751342773,8.835782051086426,8.914285659790039,9.40961742401123,9.401433944702148,9.173538208007812,9.561808586120605,9.133511543273926,9.540070533752441,9.700275421142578,9.41574478149414,9.289504051208496,9.243343353271484,9.41695785522461,9.496110916137695,9.767770767211914,9.445943832397461,9.416340827941895,9.574084281921387,9.292332649230957,9.226899147033691,9.462366104125977,9.370545387268066,9.44404411315918,9.387925148010254,9.353792190551758,9.420747756958008,9.548852920532227,9.489824295043945,9.414810180664062,9.33342456817627,9.402825355529785,9.672316551208496,9.393604278564453,9.239797592163086,9.441904067993164,9.516083717346191,9.340642929077148,9.363749504089355,9.59388256072998,9.435190200805664,9.716001510620117,9.322286605834961,9.686580657958984,9.433467864990234,9.423392295837402,9.48121166229248,9.288214683532715,9.267616271972656,9.553618431091309,9.478880882263184,9.371923446655273,9.760424613952637,9.519524574279785,9.401512145996094,9.502867698669434,8.319539070129395,8.872060775756836,8.969176292419434,8.707355499267578,8.74543285369873,8.927281379699707,8.319100379943848,8.32375717163086,8.666680335998535,8.335197448730469,8.70714282989502,9.042021751403809,8.712772369384766,8.291357040405273,8.325663566589355,8.38085651397705,8.75160026550293,8.50749397277832,8.352300643920898,8.444624900817871,8.73957347869873,8.366410255432129,8.543909072875977,10.658498764038086,null,8.693644523620605,8.322171211242676,8.898469924926758,8.4146146774292,8.773049354553223,8.323225975036621,9.074934959411621,8.57706356048584,8.7324857711792,8.81857967376709,9.022188186645508,8.334091186523438,8.382010459899902,8.761948585510254,8.723730087280273,9.060052871704102,8.774007797241211,8.736745834350586,8.403592109680176,13.670157432556152,13.798901557922363,13.369604110717773,13.828919410705566,13.80864143371582,13.781868934631348,13.87149715423584,13.169575691223145,13.689194679260254,13.725122451782227,13.867948532104492,13.486695289611816,13.795697212219238,13.88539981842041,13.794876098632812,13.703969955444336,13.475425720214844,13.485220909118652,13.642592430114746,13.45029067993164,13.17920970916748,13.825347900390625,13.710762977600098,13.833584785461426,13.823281288146973,13.777862548828125,13.709671974182129,13.589035034179688,13.624615669250488,13.818294525146484,13.520774841308594,13.499052047729492,11.366277694702148,11.415215492248535,11.413336753845215,11.3681058883667,11.386617660522461,11.464384078979492,11.333633422851562,11.434808731079102,11.411873817443848,11.339577674865723,11.400299072265625,11.394913673400879,11.41909122467041,11.410325050354004,11.436091423034668,11.440723419189453,11.421466827392578,11.424187660217285,10.752309799194336,11.30122184753418,11.429636001586914,11.37482738494873,11.405726432800293,11.432455062866211,11.494464874267578,11.39643669128418,11.412282943725586,11.394112586975098,11.40078353881836,9.92600154876709,9.90449047088623,9.936745643615723,9.92866325378418,9.815776824951172,9.909724235534668,9.910490989685059,9.891134262084961,9.903509140014648,9.932821273803711,9.916892051696777,9.828373908996582,9.930082321166992,9.88005256652832,9.91939926147461,10.002853393554688,9.763079643249512,9.934072494506836,9.936637878417969,9.912662506103516,9.736724853515625,9.912676811218262,9.76375961303711,9.90211296081543,9.953424453735352,9.921619415283203,9.907418251037598,9.980151176452637,9.924909591674805],\"y\":[1.3817375898361206,1.0612719058990479,1.1385481357574463,1.08525550365448,1.1626895666122437,1.2480696439743042,0.4848286211490631,1.2720167636871338,1.1438474655151367,1.3171401023864746,1.4076839685440063,1.264595627784729,1.0884864330291748,1.1517572402954102,1.355188012123108,1.7229746580123901,0.972616970539093,1.1486741304397583,0.8810750246047974,1.257293701171875,0.9572843313217163,0.8085752129554749,0.9309483766555786,0.7958930730819702,1.0975350141525269,0.6360377073287964,0.9854050874710083,1.798175573348999,1.3100166320800781,0.67192143201828,1.2066819667816162,1.2865713834762573,0.9056100845336914,0.6249911189079285,1.7463622093200684,1.2420947551727295,0.7832163572311401,1.1565204858779907,1.3271960020065308,1.3933436870574951,1.0287328958511353,0.7349646091461182,1.1217514276504517,1.414216160774231,1.152062177658081,1.4249526262283325,1.1767480373382568,1.2981083393096924,1.1263619661331177,1.0864992141723633,0.5083565711975098,1.0866432189941406,1.8492233753204346,1.125476598739624,0.944848895072937,1.1385208368301392,0.678071916103363,1.3165032863616943,0.6076777577400208,1.3482673168182373,1.6210135221481323,0.9811966419219971,1.2457525730133057,1.171410322189331,1.1819112300872803,0.8604665398597717,0.942077100276947,0.516913115978241,1.7408256530761719,1.4816997051239014,1.3886042833328247,1.7745593786239624,0.794830322265625,0.5049740076065063,0.8629714250564575,0.9947225451469421,1.3124425411224365,1.181308388710022,0.65378338098526,0.9567456841468811,0.8112806677818298,1.0578453540802002,1.270293116569519,1.1971663236618042,1.2050573825836182,0.49335476756095886,1.5063146352767944,1.3702335357666016,1.2522984743118286,1.2417391538619995,1.4349349737167358,1.29513418674469,1.3764736652374268,1.1924550533294678,1.3044599294662476,1.0554274320602417,0.9203598499298096,1.1263551712036133,1.4461302757263184,1.4113985300064087,1.1233012676239014,1.292132019996643,1.383273959159851,1.2921757698059082,1.4860016107559204,1.4392153024673462,1.4001719951629639,0.9837578535079956,0.6659143567085266,0.9185031056404114,1.1586668491363525,1.4281121492385864,1.1629738807678223,0.510953962802887,0.5897344946861267,0.8766728639602661,1.4713213443756104,1.1049691438674927,0.5188847184181213,1.4383823871612549,1.3952010869979858,1.3247922658920288,0.9874094128608704,1.13014554977417,0.8806256651878357,1.1321403980255127,1.1713860034942627,1.29535710811615,1.1741254329681396,0.570444643497467,1.2960554361343384,0.7852280735969543,0.9467426538467407,1.7510077953338623,1.3233586549758911,1.1152814626693726,1.0483297109603882,0.647891640663147,0.5596342086791992,1.3211227655410767,0.9677881002426147,0.6502654552459717,1.0543253421783447,0.6772043704986572,1.212782382965088,1.1973955631256104,1.1335924863815308,1.4092952013015747,1.3452917337417603,0.6293814778327942,1.8506160974502563,1.066854476928711,1.2767295837402344,0.7192075848579407,1.712160587310791,0.8005959391593933,1.1127575635910034,-0.2791852056980133,-0.56585294008255,-0.33805036544799805,-0.5237462520599365,0.26701799035072327,-0.765414834022522,-0.7155061364173889,-0.6682496070861816,-0.6063860654830933,-0.3886547386646271,-0.6360691785812378,-0.5970664024353027,-0.6077590584754944,-0.3316837251186371,0.027478214353322983,-1.0895836353302002,-0.5561929941177368,0.06399431079626083,0.6634865999221802,-0.7742803692817688,0.14976142346858978,-1.053679347038269,-0.8100492358207703,-0.6574156284332275,-0.6182628273963928,2.9431962966918945,-0.8820046186447144,-0.8308306336402893,-0.7561229467391968,0.06208992749452591,-1.2723103761672974,0.021186748519539833,-0.9865592122077942,0.42442458868026733,-0.5611394047737122,-0.19294996559619904,-0.6945754289627075,-0.3245929181575775,0.14573052525520325,-0.8709970712661743,-0.352821946144104,-0.20292386412620544,-0.3928586542606354,-0.45419180393218994,-0.6536911725997925,-0.43990081548690796,0.47350263595581055,0.13158506155014038,-0.7095873355865479,-1.041664481163025,-0.30340245366096497,0.33025580644607544,-0.5825144648551941,-0.9634666442871094,0.1646973192691803,-0.9077123999595642,0.321090430021286,-0.4256440997123718,0.016457444056868553,-0.3007483184337616,-0.9926905035972595,-0.40955081582069397,-0.7163980603218079,-0.7613950967788696,-0.5563022494316101,-0.30304890871047974,-0.6749861836433411,-0.5403029322624207,-0.3122972548007965,-0.610802948474884,-0.7764879465103149,-0.3963066339492798,-0.9999945163726807,-0.2833714783191681,-0.40003347396850586,-0.9637208580970764,-0.45351165533065796,-0.5818671584129333,-0.7205086350440979,-0.9992696046829224,0.6563940644264221,-1.185823917388916,-1.0215363502502441,-0.13112245500087738,-0.6537771224975586,-0.7294257283210754,-0.7630423307418823,-0.7293157577514648,-0.9494206309318542,-0.3285471796989441,-0.1588168889284134,-0.36618950963020325,-0.9938457608222961,0.16926676034927368,-0.4724136292934418,0.05542946234345436,-0.2140544056892395,-0.305911123752594,-0.019443433731794357,0.04754577577114105,-0.5232704877853394,-0.11347304284572601,-0.42826223373413086,-0.3631307780742645,-0.34572187066078186,-0.06021670997142792,0.02899486944079399,-0.5741272568702698,-0.17813809216022491,-0.3307811915874481,-0.5624246597290039,-0.33823102712631226,-0.49408844113349915,-0.2875242233276367,-0.5673142671585083,-0.5054513216018677,-0.19457708299160004,0.04647556692361832,0.15025267004966736,-0.4029132127761841,-0.09265140444040298,-0.06226486340165138,-0.49906080961227417,-0.42240944504737854,-0.37416812777519226,-0.4376997649669647,-0.5433786511421204,-0.48809999227523804,-0.3587982952594757,-0.1461024433374405,-0.4376213848590851,-0.5315554738044739,-0.5797322392463684,2.760768413543701,-0.5617931485176086,-0.37373608350753784,1.0235728025436401,-0.2975458800792694,-0.12453179061412811,-0.39394140243530273,0.08051753044128418,-0.5586355328559875,-0.5786116719245911,-0.5049486756324768,0.1441514790058136,-0.7706948518753052,-0.21628828346729279,-0.3426320552825928,-0.16151897609233856,-0.39145922660827637,-0.5925371646881104,-0.2956721782684326,0.027539370581507683,-0.30378881096839905,0.042292892932891846,-0.12347960472106934,-0.06413992494344711,-0.6200217604637146,-0.4954264461994171,-0.34516313672065735,3.190534830093384,-0.2043280303478241,-0.5706292986869812,-0.7155386209487915,0.12354090809822083,-0.219037726521492,-0.293334424495697,0.07163935899734497,-0.5650919675827026,-0.29002729058265686,0.07026058435440063,-0.5817172527313232,-0.10595829039812088,-1.2771755456924438,-0.2213153839111328,0.0876251682639122,-0.7389064431190491,-0.14196763932704926,-0.6306360363960266,0.011064082384109497,0.014209670014679432,-0.09134206920862198,-0.2708755433559418,-0.20944665372371674,-0.15542514622211456,-0.016267064958810806,0.33848637342453003,1.3138884241925552e-05,-0.3145374655723572,-0.37487006187438965,0.08202043920755386,-0.06734184175729752,-0.26890313625335693,-0.2482864111661911,-0.37177231907844543,-0.342668354511261,0.05140113830566406,-0.009848969988524914,-0.26512715220451355,-0.21851959824562073,-0.20532433688640594,-0.34504765272140503,-0.12112276256084442,-0.11948814243078232,-0.36536410450935364,-0.43950170278549194,-0.3522815406322479,-0.16667716205120087,-0.33151018619537354,-0.4151010513305664,-0.21129287779331207,0.18422266840934753,-0.13335680961608887,0.17345526814460754,0.07078302651643753,-0.06395117193460464,-0.33869442343711853,-0.20178943872451782,-0.20231139659881592,-0.02338528260588646,0.17116601765155792,-0.3580891191959381,-0.31286415457725525,-0.32009008526802063,-0.1331133246421814,0.18067573010921478,-0.2697615623474121,0.08013986051082611,-0.31005680561065674,0.028209263458848,0.10702250152826309,0.2667584717273712,-0.3471834659576416,-0.3380453586578369,-0.07149399816989899,-0.4023083448410034,-0.10830654948949814,-0.17893050611019135,-0.2754707932472229,0.22065694630146027,-0.1637326180934906,-0.12212429195642471,-0.17535138130187988,-0.2760526239871979,-0.01606772653758526,0.11322040110826492,0.021302759647369385,1.5433651208877563,1.4605567455291748,1.4761929512023926,1.4625173807144165,1.4584766626358032,1.4409211874008179,1.4749916791915894,1.4284682273864746,1.4748163223266602,1.4806615114212036,1.4631659984588623,1.411305546760559,1.4848947525024414,1.0450985431671143,1.4943875074386597,1.3517125844955444,1.4878875017166138,1.4422603845596313,1.5346200466156006,1.5406434535980225,1.499199628829956,1.4141327142715454,1.4414336681365967,1.3886102437973022,1.4608150720596313,1.4566928148269653,1.5145379304885864,1.6541327238082886,1.5264090299606323,1.531654953956604,1.4845945835113525,1.4692519903182983,1.5288569927215576,1.5096898078918457,1.3595173358917236,1.587659478187561,1.4848111867904663,1.480909824371338,1.49443781375885,1.6546357870101929,1.5368572473526,1.2316579818725586,1.3251349925994873,1.5530869960784912,1.5514334440231323,1.5379899740219116,1.494699239730835,1.525313377380371,1.4642879962921143,1.526352882385254,1.288293480873108,1.483452320098877,1.4212583303451538,1.5053179264068604,1.3147481679916382,1.3840261697769165,1.5364909172058105,1.516196370124817,1.4429274797439575,1.4955360889434814,1.5462852716445923,1.454986810684204,1.4761298894882202,1.4729735851287842,1.450022578239441,1.443420648574829,1.4701296091079712,4.530323028564453,4.6652750968933105,4.259049415588379,4.17231559753418,4.669703960418701,4.087711334228516,4.67042350769043,4.63741397857666,4.61146879196167,4.00691556930542,4.465812683105469,4.730312824249268,3.997835874557495,4.689051151275635,4.215987682342529,4.713289260864258,3.9740796089172363,4.662024021148682,4.6290106773376465,4.683803558349609,4.155202865600586,4.592298984527588,4.655975341796875,4.570188999176025,4.67529296875,4.149698734283447,4.111392498016357,4.625186920166016,4.129754066467285,4.608168125152588,4.609062194824219,4.684344291687012,4.598238468170166,4.515726089477539,4.097564697265625,4.126251697540283,4.632607936859131,3.846534013748169,4.030094146728516,4.677065372467041,4.595035552978516,4.631924152374268,4.152017593383789,4.6212286949157715,4.092752933502197,4.058029651641846,4.524569034576416,4.68303918838501,4.761435031890869,4.133310794830322,4.037563800811768,4.604251384735107,4.569831848144531,4.141808032989502,4.4533562660217285,4.653535842895508,4.054136753082275,4.630998134613037,3.962613344192505,1.5877009630203247,1.2727618217468262,1.4802215099334717,1.6104549169540405,1.4569437503814697,1.539625644683838,1.472213864326477,2.0593249797821045,1.3333063125610352,1.6195672750473022,0.646668553352356,1.6028765439987183,1.468073844909668,1.4336971044540405,1.469533920288086,1.5507453680038452,1.6590402126312256,1.5533472299575806,1.5326228141784668,1.4909952878952026,1.5933552980422974,0.6876698732376099,1.5868359804153442,1.573804259300232,1.5753282308578491,1.2330971956253052,1.6163599491119385,0.7645369172096252,1.5960463285446167,1.578489899635315,0.5172238349914551,1.5027610063552856,1.38727867603302,1.5584850311279297,1.571655035018921,1.5743860006332397,1.5758932828903198,1.8300682306289673,1.7321003675460815,1.5929148197174072,1.4410037994384766,1.594342589378357,1.557225227355957,1.3510727882385254,1.2829564809799194,1.1802170276641846,1.5979278087615967,1.6627514362335205,1.6593539714813232,1.5607953071594238,1.3650426864624023,0.5767338275909424,1.5249618291854858,1.5589072704315186,1.6015998125076294,1.5737286806106567,1.52560555934906,1.5944088697433472,2.3929059505462646,2.3886959552764893,2.2481915950775146,2.366041660308838,2.3146793842315674,2.3162174224853516,2.369158983230591,2.5022075176239014,2.4162755012512207,2.523205280303955,2.5030629634857178,2.322309732437134,2.426605224609375,2.189079761505127,2.5072808265686035,2.1927618980407715,2.2802486419677734,2.404411792755127,2.5210015773773193,2.5109353065490723,2.188095808029175,2.2342336177825928,1.3085989952087402,2.270885467529297,2.520770311355591,2.4987809658050537,2.2820372581481934,2.2575674057006836,2.305651903152466,2.480987310409546,2.6763017177581787,2.3891241550445557,2.503016233444214,2.507106065750122,2.5494508743286133,2.4986634254455566,2.2242064476013184,2.3021676540374756,2.150664806365967,2.744899034500122,2.3470451831817627,2.496939182281494,2.4149718284606934,2.483858346939087,2.3855793476104736,2.2803268432617188,2.255380868911743,2.259631872177124,2.3189663887023926,2.3722052574157715,2.399129867553711,2.2720420360565186,2.2237091064453125,2.365219831466675,2.2338740825653076,2.3938751220703125,3.968130350112915,4.086702346801758,3.7755074501037598,3.991488218307495,3.975116014480591,4.029293060302734,4.046248435974121,4.013284683227539,4.176393032073975,4.156956672668457,4.055082321166992,3.9321210384368896,3.937089204788208,3.9232289791107178,3.8601222038269043,3.866779327392578,3.9942166805267334,3.850517511367798,3.827030897140503,4.08809232711792,3.6295218467712402,4.113508701324463,4.187417507171631,3.6063523292541504,3.9586288928985596,4.24702787399292,3.9166204929351807,4.1035475730896,3.9102394580841064,4.182351112365723,3.869297981262207,3.981309175491333,3.9789490699768066,3.9490838050842285,3.944530487060547,3.942408323287964,4.038426399230957,4.008914947509766,4.139644622802734,3.7226343154907227,4.209354877471924,4.045825958251953,3.9882144927978516,4.28642463684082,4.0150346755981445,3.615358352661133,4.036133289337158,4.028831958770752,3.9747371673583984,3.9180045127868652,3.7974116802215576,4.060174942016602,4.20427942276001,4.09943962097168,-0.5178394317626953,-0.650516152381897,-0.479851096868515,-0.4310290515422821,-0.5378267168998718,-0.6259913444519043,-0.7380387187004089,-0.21319614350795746,-0.7479232549667358,-0.610841691493988,-0.6754353642463684,-0.5430178642272949,-0.6768491268157959,-0.45748335123062134,-0.3581988215446472,-0.8913076519966125,-0.698038637638092,-0.7323350310325623,-0.4265297055244446,-0.6116015911102295,-0.9226518869400024,-0.7730879783630371,-0.7615750432014465,-0.7331415414810181,-0.8823621869087219,-0.8506748676300049,-0.7885114550590515,-0.723310649394989,-0.7920389175415039,-0.6305829882621765,-0.698114275932312,-0.7590868473052979,-0.9453780651092529,-0.47471097111701965,-0.7153941988945007,-0.7514808177947998,-0.32000821828842163,-0.5421023368835449,-0.5268405079841614,-0.7256784439086914,-0.6647657752037048,-0.5601599812507629,-0.8130180239677429,-0.5797785520553589,-0.76743084192276,-0.740344226360321,-0.6408556699752808,-0.4104425609111786,-0.3353818655014038,-0.6775698661804199,-0.7086683511734009,-0.9370354413986206,-0.7324972152709961,3.193037986755371,3.0153839588165283,3.2098007202148438,3.411353826522827,3.419900894165039,2.993455171585083,3.2046985626220703,3.197061538696289,3.5652174949645996,3.1496386528015137,3.4359893798828125,3.163926362991333,3.4493963718414307,3.234320640563965,3.2002310752868652,3.272998094558716,3.3818790912628174,3.3397650718688965,3.0758256912231445,3.1623947620391846,3.384615182876587,3.2408924102783203,3.358213186264038,1.0887218713760376,null,3.4248197078704834,3.2836413383483887,3.2514102458953857,3.342782497406006,3.3918700218200684,3.194164752960205,3.047151803970337,3.55344820022583,3.2842931747436523,3.3557095527648926,3.1627235412597656,3.2219274044036865,3.0630640983581543,3.3806440830230713,3.417919874191284,3.0919387340545654,3.379578113555908,3.3994202613830566,3.3199284076690674,1.930212140083313,1.9589751958847046,1.9856090545654297,2.118469476699829,1.8433870077133179,2.083873748779297,1.9659463167190552,1.6238290071487427,1.9088102579116821,1.984571933746338,1.8491621017456055,1.8348524570465088,1.9095227718353271,2.1351234912872314,2.0434975624084473,1.9872238636016846,1.9833941459655762,1.972872018814087,1.8849225044250488,1.9644986391067505,1.6182628870010376,2.087661027908325,1.9592905044555664,1.9400484561920166,1.8442121744155884,2.052821636199951,2.1948325634002686,1.9715791940689087,1.9485830068588257,1.9467763900756836,2.033796548843384,2.05224347114563,-1.7047370672225952,-1.7189042568206787,-1.7727406024932861,-1.7914379835128784,-1.7604166269302368,-1.5030242204666138,-1.8196848630905151,-1.7163331508636475,-1.6494393348693848,-1.6843397617340088,-1.7724716663360596,-1.7692655324935913,-1.7378288507461548,-1.7441824674606323,-1.464735746383667,-1.7443996667861938,-1.7241114377975464,-1.6847602128982544,0.36447063088417053,-1.677884578704834,-1.6699968576431274,-1.7729989290237427,-1.7663508653640747,-1.7174915075302124,-1.603288173675537,-1.7217741012573242,-1.7533800601959229,-1.768631100654602,-1.7753701210021973,-2.003943920135498,-2.018934726715088,-2.0210397243499756,-1.9936052560806274,-1.9008338451385498,-1.989972710609436,-2.0100209712982178,-1.9968105554580688,-2.0147523880004883,-2.0181238651275635,-2.0108845233917236,-1.9405567646026611,-1.9686578512191772,-1.9617516994476318,-2.0073821544647217,-1.730658769607544,-1.9098670482635498,-1.9983524084091187,-2.0032131671905518,-2.021005392074585,-1.78288996219635,-1.9908539056777954,-1.9123069047927856,-1.9989206790924072,-1.8872182369232178,-2.0079305171966553,-1.9907784461975098,-1.9225029945373535,-2.0194551944732666],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"#CFD8DC\",\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"other\",\"showlegend\":false,\"x\":[12.260786056518555,10.01115894317627,11.924004554748535,10.16279411315918,12.00646686553955,10.273680686950684,6.795833110809326,10.738099098205566,12.415397644042969,12.344215393066406,8.005266189575195,11.672365188598633,9.678071022033691,11.613967895507812,10.691057205200195,10.729043006896973,12.570730209350586,12.728907585144043,11.094691276550293,12.467524528503418,10.7769136428833,11.365942001342773,11.85193157196045,10.574968338012695,13.300118446350098,8.84424877166748,9.322251319885254,11.329916000366211,9.31579303741455,10.838811874389648,11.459781646728516,11.970154762268066,12.745402336120605,10.581709861755371,9.499332427978516,10.223129272460938,9.936136245727539,11.424376487731934,7.634429931640625,12.565068244934082,12.609332084655762,11.734045028686523,12.76197338104248,8.449498176574707,10.840675354003906,11.946686744689941,8.837322235107422,9.319323539733887,12.509225845336914,10.419879913330078,6.7955498695373535,9.324790954589844,12.908655166625977,11.152015686035156,11.986949920654297,12.218483924865723,11.286534309387207,11.993667602539062,9.96347427368164,11.670063018798828,9.893813133239746,10.930827140808105,9.840456008911133,9.265555381774902,8.651935577392578,9.824747085571289,9.816277503967285,12.087455749511719,11.567899703979492,6.796971321105957,12.668866157531738,12.218778610229492,10.99422836303711,11.2903470993042,13.343911170959473,12.024370193481445,12.494938850402832,12.23508071899414,10.537565231323242,9.071687698364258,9.39167308807373,13.654808044433594,11.47091293334961,11.624275207519531,12.246249198913574,12.476778984069824,11.365124702453613,12.479470252990723,12.073331832885742,10.643670082092285,10.698538780212402,11.801766395568848,11.808372497558594,9.954280853271484,12.55838394165039,8.872209548950195,11.538613319396973,8.866655349731445,10.459000587463379,11.395777702331543,9.855978012084961,11.591615676879883,10.30197811126709,11.654211044311523,11.18188190460205,10.506263732910156,13.050704002380371,11.941532135009766,12.512845993041992,11.652451515197754,13.646256446838379,11.867741584777832,12.657735824584961,12.094124794006348,10.286686897277832,12.581634521484375,11.336400985717773,9.937158584594727,11.021777153015137,9.948188781738281,12.35839557647705,12.886040687561035,9.261913299560547,9.89763355255127,9.462775230407715,9.968618392944336,12.362512588500977,10.358255386352539,8.327774047851562,11.927535057067871,10.00265884399414,12.349808692932129,10.623787879943848,10.678670883178711,11.847443580627441,9.692587852478027,9.262121200561523,10.958932876586914,12.889772415161133,10.948387145996094,11.634035110473633,12.080227851867676,10.378564834594727,10.096247673034668,11.42621898651123,8.7933931350708,11.231094360351562,8.999340057373047,11.527088165283203,12.19135570526123,7.8714375495910645,11.001914978027344,12.61523151397705,9.75306224822998,11.896245002746582,12.523083686828613,9.378335952758789,10.35689926147461,11.08590316772461,13.147154808044434,9.818265914916992,11.11213493347168,11.444499969482422,11.590463638305664,10.737903594970703,9.770731925964355,10.78557014465332,11.933521270751953,10.511313438415527,9.82026481628418,8.356404304504395,10.73415470123291,10.45881462097168,10.396477699279785,12.194050788879395,12.752180099487305,11.78748607635498,10.255226135253906,12.165645599365234,12.292634963989258,12.844697952270508,10.628177642822266,8.569395065307617,11.763611793518066,12.15890884399414,12.740022659301758,12.746966361999512,9.62110424041748,11.45886516571045,10.926743507385254,12.772931098937988,12.837591171264648,12.200713157653809,9.013678550720215,10.967081069946289,9.76983642578125,12.884422302246094,11.670434951782227,13.589189529418945,12.576118469238281,11.409375190734863,9.541470527648926,8.66622257232666,11.438347816467285,11.34601879119873,10.590363502502441,11.35364818572998,11.930695533752441,6.797292709350586,10.794591903686523,10.309884071350098,10.247868537902832,9.914916038513184,13.303763389587402,12.540189743041992,11.354313850402832,8.383038520812988,12.193739891052246,9.227989196777344,9.241358757019043,9.536521911621094,10.984012603759766,12.313669204711914,9.644474029541016,9.546388626098633,9.345878601074219,11.460395812988281,10.699294090270996,9.629450798034668,8.48170280456543,11.0349760055542,10.570517539978027,10.367918014526367,11.483748435974121,10.360127449035645,10.180638313293457,12.789478302001953,10.665085792541504,11.708563804626465,12.97873592376709,11.337641716003418,10.709122657775879,13.167486190795898,12.409531593322754,8.32143497467041,9.063648223876953,10.371134757995605,11.557292938232422,10.586905479431152,9.22173023223877,11.61303424835205,8.834911346435547,13.149587631225586,11.683677673339844,9.396690368652344,11.555591583251953,11.163105964660645,10.546306610107422,13.552511215209961,12.85168170928955,11.200281143188477,9.939804077148438,10.729233741760254,12.406366348266602,11.353873252868652,11.169713973999023,9.460506439208984,10.654193878173828,11.77177906036377,11.467199325561523,12.088428497314453,13.498458862304688,11.279095649719238,12.567978858947754,10.763456344604492,10.033524513244629,12.673567771911621,10.10020637512207,8.970285415649414,9.142569541931152,11.128690719604492,12.259600639343262,8.503023147583008,11.229401588439941,9.610413551330566,10.481382369995117,12.30782699584961,11.052600860595703,13.069833755493164,9.056635856628418,9.835722923278809,11.540289878845215,10.57509994506836,10.630483627319336,12.53976058959961,12.878406524658203,12.195188522338867,12.416590690612793,8.807579040527344,12.123668670654297,10.993809700012207,11.357285499572754,11.609417915344238,8.505378723144531,9.010087966918945,9.596749305725098,12.824377059936523,10.293586730957031,11.473138809204102,12.474746704101562,10.027557373046875,12.210655212402344,10.91020393371582,13.3267240524292,13.175515174865723,11.291525840759277,11.628263473510742,11.242234230041504,9.939929008483887,12.68603515625,9.105117797851562,12.55679702758789,10.582772254943848,11.557234764099121,12.54797649383545,11.068909645080566,12.6818208694458,11.577909469604492,11.18771743774414,13.261770248413086,11.224922180175781,11.866551399230957,9.587552070617676,10.551297187805176,11.903278350830078,12.07540225982666,10.768095970153809,12.016518592834473,8.86601448059082,12.697779655456543,12.205658912658691,10.315733909606934,11.997912406921387,12.761860847473145,11.485032081604004,12.877477645874023,10.77319622039795,9.71633529663086,12.789429664611816,10.579119682312012,12.169694900512695,11.369839668273926,12.047872543334961,11.594593048095703,11.944458961486816,11.05200481414795,9.081618309020996,12.835399627685547,12.314804077148438,9.093341827392578,11.577603340148926,9.215981483459473,11.374435424804688,12.266894340515137,11.160942077636719,8.430356979370117,12.54307746887207,11.239245414733887,10.539368629455566,11.465906143188477,7.657243728637695,12.263589859008789,9.044021606445312,11.270294189453125,9.69029426574707,12.90316104888916,11.74852466583252,8.697430610656738,10.244534492492676,13.194223403930664,8.26854419708252,12.263144493103027,12.15650463104248,10.97741985321045,11.557283401489258,12.207938194274902,9.880388259887695,13.715070724487305,8.669084548950195,11.517677307128906,12.063190460205078,10.891190528869629,8.157964706420898,13.416830062866211,12.172284126281738,9.77486515045166,12.416096687316895,8.317665100097656,12.529857635498047,10.84004020690918,8.873795509338379,12.759370803833008,10.572245597839355,11.383156776428223,11.408194541931152,12.709611892700195,13.176079750061035,7.645970344543457,10.45201587677002,11.888107299804688,8.155491828918457,10.934144973754883,12.047857284545898,11.55852222442627,10.887688636779785,10.773085594177246,10.705704689025879,9.601393699645996,10.455825805664062,9.596738815307617,10.703749656677246,11.308526992797852,9.27015209197998,10.01099681854248,12.847784996032715,12.372241020202637,10.8928804397583,11.546853065490723,9.608847618103027,10.049355506896973,10.234009742736816,12.77387809753418,7.639250755310059,9.369508743286133,11.111124992370605,10.788805961608887,9.64129638671875,10.971961975097656,13.150918960571289,10.595973014831543,9.54521369934082,12.856410026550293,10.756369590759277,11.570320129394531,10.633820533752441,10.61214542388916,11.196186065673828,11.974361419677734,11.977964401245117,9.437406539916992,11.594237327575684,11.464212417602539,10.141727447509766,11.141068458557129,9.565552711486816,12.504912376403809,10.439518928527832,8.916755676269531,12.542708396911621,8.100579261779785,13.614336013793945,9.942835807800293,12.88668441772461,12.289319038391113,10.191901206970215,11.653940200805664,12.273872375488281,9.7015380859375,11.526108741760254,11.894497871398926,9.942673683166504,12.299449920654297,11.311140060424805,12.881195068359375,8.470643043518066,12.338445663452148,11.502923011779785,10.742378234863281,12.947842597961426,11.659990310668945,12.496929168701172,8.366921424865723,9.1043119430542,12.329166412353516,11.74715518951416,11.585480690002441,10.612318992614746,11.377769470214844,11.649738311767578,11.907842636108398,13.692667007446289,11.34955883026123,12.52751636505127,8.80586051940918,12.406229019165039,12.7645902633667,12.445311546325684,11.75776195526123,11.090094566345215,11.42638111114502,10.89482593536377,10.761418342590332,11.150059700012207,10.034558296203613,10.338700294494629,11.178735733032227,12.835885047912598,11.53531551361084,12.381514549255371,11.54314136505127,11.90941047668457,9.358724594116211,10.740930557250977,11.939253807067871,11.149989128112793,11.659534454345703,10.353327751159668,11.331498146057129,10.64175796508789,12.81151008605957,8.662858009338379,10.857192993164062,10.787764549255371,8.953280448913574,12.585387229919434,7.643540382385254,10.447787284851074,10.80845832824707,12.5751371383667,9.113858222961426,9.70373821258545,12.737845420837402,11.416006088256836,12.489952087402344,8.133481979370117,10.278565406799316,11.977149963378906,9.470913887023926,10.66490364074707,10.00718879699707,9.176525115966797,11.815064430236816,8.752781867980957,11.483132362365723,11.89669418334961,12.553421020507812,13.118978500366211,9.782742500305176,10.74406623840332,12.168991088867188,11.756896018981934,12.940293312072754,11.56460189819336,8.64371109008789,11.702585220336914,11.278419494628906,10.558026313781738,11.820279121398926,10.459695816040039,11.502100944519043,11.530735969543457,10.50390911102295,12.36990737915039,12.139140129089355,11.131754875183105,10.48403263092041,13.041622161865234,11.032854080200195,11.734776496887207,10.479324340820312,9.945302963256836,11.825505256652832,11.676788330078125,9.411858558654785,12.318131446838379,11.462925910949707,12.87320613861084,13.661906242370605,12.474398612976074,10.583564758300781,11.804007530212402,11.14046573638916,11.841476440429688,11.781903266906738,9.183506965637207,11.8534574508667,11.49117660522461,11.63294506072998,11.224637985229492,12.764777183532715,12.330574035644531,11.128299713134766,12.93774127960205,10.32089614868164,12.478330612182617,11.827890396118164,11.804841041564941,11.44624137878418,11.820039749145508,10.238183975219727,7.784416675567627,12.978934288024902,12.451269149780273,9.322071075439453,11.821974754333496,10.541068077087402,11.961499214172363,8.238553047180176,12.170052528381348,12.736010551452637,12.764181137084961,11.65296745300293,12.675060272216797,11.974373817443848,10.117522239685059,9.6543607711792,12.757356643676758,12.056167602539062,9.412654876708984,11.711962699890137,9.174324989318848,10.823249816894531,9.90158462524414,10.913339614868164,8.113393783569336,9.993379592895508,12.194329261779785,13.33237361907959,11.182921409606934,12.732467651367188,13.046422004699707,9.75806713104248,8.685029029846191,11.561057090759277,10.053182601928711,11.998501777648926,13.20248031616211,12.104700088500977,10.906420707702637,11.117636680603027,10.446064949035645,9.715156555175781,12.530933380126953,12.754565238952637,11.461576461791992,11.491990089416504,9.396021842956543,8.587562561035156,12.263270378112793,11.42301082611084,13.491022109985352,10.364217758178711,11.991475105285645,12.44273853302002,11.70559310913086,10.35807991027832,9.996321678161621,9.326005935668945,12.807846069335938,13.570623397827148,11.12037467956543,10.716169357299805,10.50700569152832,10.22071361541748,12.695380210876465,11.297102928161621,9.085335731506348,12.861201286315918,11.170536994934082,10.041293144226074,10.805912017822266,11.3994722366333,9.783839225769043,11.411025047302246,10.608845710754395,9.42233657836914,11.715644836425781,11.849699020385742,12.349161148071289,8.975476264953613,10.749741554260254,9.420133590698242,10.120325088500977,12.375459671020508,10.686391830444336,11.969694137573242,11.85922622680664,12.443221092224121,9.37954330444336,13.088716506958008,10.281312942504883,12.127240180969238,11.985560417175293,12.442952156066895,12.119172096252441,9.390953063964844,10.271842956542969,10.183282852172852,12.758428573608398,9.88254451751709,11.704732894897461,11.48423957824707,9.189234733581543,11.172453880310059,11.48064136505127,10.315546035766602,12.061239242553711,11.559906005859375,11.86010456085205,11.664885520935059,12.843629837036133,9.624467849731445,12.458562850952148,12.246566772460938,11.838500022888184,10.999716758728027,9.12990951538086,11.840607643127441,11.96013069152832,11.915284156799316,12.08629035949707,10.903203964233398,8.849540710449219,11.167780876159668,10.531004905700684,12.467206954956055,10.145415306091309,9.549249649047852,6.795407295227051,12.180326461791992,12.817878723144531,10.757889747619629,11.568016052246094,11.672552108764648,7.97341775894165,10.022518157958984,9.927380561828613,10.70942211151123,11.941184043884277,12.794625282287598,12.879822731018066,12.178878784179688,11.878340721130371,10.52446460723877,10.071585655212402,10.56197738647461,11.002744674682617,12.245721817016602,10.67922592163086,12.140796661376953,12.277287483215332,11.456153869628906,12.081787109375,11.62030029296875,10.880305290222168,10.69717025756836,9.24069881439209,12.846972465515137,10.142430305480957,12.146927833557129,9.958215713500977,9.50244140625,10.51760196685791,10.551128387451172,11.390852928161621,11.688502311706543,11.491974830627441,12.741220474243164,12.295635223388672,12.218851089477539,8.685344696044922,13.047622680664062,11.958925247192383,9.383042335510254,11.218199729919434,11.51757526397705,9.624102592468262,13.13804817199707,12.326669692993164,12.624595642089844,12.161945343017578,9.43790054321289,11.324953079223633,8.503750801086426,11.257233619689941,10.58021354675293,9.62783145904541,9.703179359436035,11.924786567687988,8.906972885131836,9.254066467285156,12.899577140808105,11.277064323425293,11.152223587036133,12.76430606842041,10.148778915405273,12.378602027893066,11.560495376586914,12.818056106567383,12.91047191619873,8.343107223510742,10.998586654663086,11.666386604309082,10.141661643981934,11.702606201171875,10.432024955749512,10.108580589294434,9.184564590454102,13.428140640258789,11.512848854064941,12.044175148010254,11.576578140258789,12.28610610961914,11.385955810546875,12.952836036682129,12.021284103393555,11.287923812866211,12.099640846252441,12.082901000976562,12.028682708740234,12.383368492126465,8.773681640625,12.879551887512207,11.319714546203613,8.952622413635254,11.267468452453613,12.477086067199707,12.503398895263672,9.42236614227295,11.20785140991211,11.332497596740723,8.951685905456543,10.856745719909668,10.309080123901367,11.73598861694336,8.723488807678223,12.038217544555664,9.878068923950195,12.995759963989258,10.974395751953125,9.64431381225586,10.158196449279785,13.631218910217285,8.542502403259277,11.165329933166504,10.789525032043457,13.625770568847656,11.960083961486816,12.425254821777344,11.940552711486816,13.470690727233887,13.038926124572754,7.834033012390137,9.87307071685791,11.216033935546875,12.414644241333008,12.093451499938965,11.877202987670898,12.003032684326172,10.026086807250977,12.491192817687988,13.477025032043457,12.77425479888916,10.46001148223877,8.880654335021973,9.948962211608887,10.48256778717041,12.090274810791016,11.583839416503906,11.40567684173584,6.7874345779418945,9.088058471679688,11.758285522460938,11.31846809387207,11.73054313659668,12.907270431518555,12.770581245422363,11.114065170288086,11.227639198303223,10.648099899291992,12.066269874572754,13.3859224319458,12.3936767578125,11.536118507385254,12.09136962890625,7.971031665802002,10.897802352905273,9.173691749572754,11.609256744384766,12.269068717956543,9.517958641052246,11.856656074523926,11.710532188415527,12.882142066955566,12.293147087097168,9.226627349853516,11.812376022338867,9.573324203491211,11.98694896697998,11.025315284729004,12.620606422424316,12.66535758972168,11.626893997192383,11.87524127960205,9.740761756896973,11.66956615447998,9.677865982055664,13.708874702453613,13.700621604919434,10.8056058883667,12.719548225402832,11.30461597442627,9.15710163116455,11.5343599319458,9.219347953796387,11.886124610900879,11.679952621459961,12.069474220275879,12.361296653747559,10.542925834655762,10.790267944335938,12.395968437194824,10.26074504852295,12.145330429077148,11.74125862121582,12.40096664428711,10.930389404296875,8.466429710388184,11.920580863952637,12.118764877319336,13.445467948913574,10.998306274414062,11.986994743347168,12.903426170349121,11.159873962402344,12.18824291229248,9.372452735900879,12.712847709655762,13.078712463378906,10.631550788879395,9.169002532958984,8.790238380432129,11.811575889587402,12.28490924835205,9.769627571105957,12.567846298217773,10.349342346191406,11.191412925720215,11.537104606628418,10.127388000488281,10.513239860534668,8.915346145629883,11.823579788208008,11.909771919250488,11.520251274108887,13.776591300964355,12.594861030578613,6.803589344024658,10.791962623596191,11.956903457641602,7.651430606842041,12.864334106445312,11.632024765014648,10.91291332244873,11.907623291015625,10.763083457946777,11.546504974365234,13.262833595275879,12.147619247436523,11.872856140136719,12.41122055053711,12.41154670715332,13.054023742675781,10.615890502929688,9.164365768432617],\"y\":[1.976219654083252,1.265775442123413,1.0926563739776611,-0.5182633996009827,2.4570963382720947,0.48775091767311096,-0.910475492477417,-0.5370233654975891,1.2164555788040161,1.3136889934539795,-0.12520691752433777,1.813036561012268,1.1669845581054688,1.774245023727417,-0.3897693157196045,1.3097110986709595,3.413337469100952,1.671027660369873,0.8366751074790955,3.2766458988189697,0.3775566816329956,0.44715550541877747,2.3352620601654053,3.230710506439209,2.2592904567718506,0.6792904138565063,0.25117701292037964,0.6122933626174927,1.182349681854248,0.9189866781234741,3.0709445476531982,0.9211485385894775,3.500269889831543,3.7989087104797363,1.823943853378296,1.6083993911743164,-0.23820368945598602,2.4324207305908203,0.19927099347114563,3.7869391441345215,2.054600715637207,0.9205240607261658,3.078073263168335,0.10503587871789932,0.8371968865394592,0.3198699951171875,-0.16870951652526855,2.7976632118225098,2.6678123474121094,-0.570141077041626,-0.9106079339981079,3.7905776500701904,2.7144033908843994,0.9463844299316406,1.3737200498580933,0.33007562160491943,-0.9850686192512512,1.1608271598815918,1.0532095432281494,2.060023546218872,1.5419471263885498,1.308414101600647,-1.1935114860534668,-0.17752167582511902,-0.4857664108276367,3.832371234893799,1.3047598600387573,3.552154541015625,1.942522644996643,-0.909134566783905,2.3067994117736816,0.9871791005134583,2.9792444705963135,2.830186128616333,0.7007690072059631,3.5236191749572754,3.808678388595581,0.5856729745864868,0.5755625367164612,-0.488295316696167,0.6351603269577026,0.5080202221870422,1.0401160717010498,2.3617639541625977,1.8510946035385132,0.9317828416824341,3.2759201526641846,2.4336321353912354,-1.299113392829895,3.8617777824401855,3.202488422393799,1.037284016609192,-0.5317288637161255,-0.5715892314910889,1.2100167274475098,1.357236623764038,3.533149480819702,3.0465500354766846,0.1627165526151657,0.7375492453575134,0.9511770009994507,0.07118048518896103,-0.3919447958469391,1.378462314605713,-0.3244394361972809,0.7082329988479614,2.143453359603882,1.1407673358917236,0.9031526446342468,2.8212311267852783,1.1145505905151367,1.4429399967193604,2.296320676803589,-0.001279126270674169,-0.10712704062461853,3.0526630878448486,0.7859264612197876,-0.6041418313980103,1.40556800365448,-0.5446928143501282,1.2216320037841797,3.4056971073150635,3.86862850189209,1.1494468450546265,0.1541697382926941,-0.2646180987358093,2.934096097946167,1.085005283355713,0.1298372447490692,1.4595853090286255,0.5229455232620239,1.7139097452163696,3.179950475692749,0.8122704029083252,2.313836097717285,0.9151896834373474,0.0694771260023117,2.251661777496338,2.7393033504486084,1.8044404983520508,2.4212090969085693,1.369358777999878,1.4411309957504272,1.778269648551941,1.9663785696029663,-0.2085247039794922,1.8884222507476807,0.01605393923819065,0.9991214275360107,2.1229145526885986,-0.17486093938350677,1.2444441318511963,1.5270284414291382,1.034622311592102,0.5874497890472412,1.0864499807357788,0.7937063574790955,0.2653231918811798,0.6648417711257935,2.1787331104278564,-1.4033302068710327,0.993794858455658,3.9591336250305176,2.2620389461517334,2.2779126167297363,1.026957631111145,1.6402356624603271,1.106472134590149,3.609027624130249,4.001310348510742,2.4320061206817627,1.3439748287200928,1.444767951965332,0.8551923036575317,0.15037234127521515,2.425750494003296,2.3774797916412354,2.089541435241699,0.14875546097755432,1.2632057666778564,1.3453816175460815,3.9026010036468506,0.47671282291412354,1.526449203491211,1.530004620552063,0.44732722640037537,1.5337772369384766,-0.5403087735176086,-1.0967426300048828,0.833954393863678,0.7645677924156189,2.70750093460083,1.2675151824951172,0.1613716185092926,3.286445379257202,0.9860133528709412,3.4668221473693848,1.4706974029541016,-0.06515169888734818,3.346012830734253,3.19698166847229,1.0387732982635498,-0.47503626346588135,1.1350661516189575,-0.12550556659698486,0.48462581634521484,3.611987352371216,0.44945693016052246,-0.9105373620986938,0.9302946925163269,2.0090830326080322,0.1268618106842041,-0.5888694524765015,1.5191290378570557,2.7231905460357666,3.63657283782959,0.20960384607315063,0.04279949516057968,2.0899455547332764,2.395681142807007,-0.3550689220428467,1.1873329877853394,1.6772342920303345,-0.551196813583374,1.0045689344406128,3.5376405715942383,-0.2728564739227295,1.6083232164382935,3.451906681060791,0.4097013771533966,0.28619876503944397,2.9995458126068115,1.430558204650879,2.9486751556396484,1.4180095195770264,3.394707679748535,-1.0172475576400757,1.6186503171920776,2.2325010299682617,3.3960022926330566,2.3339428901672363,2.508234739303589,1.917661428451538,2.007206678390503,0.7356185913085938,1.3998340368270874,1.426846981048584,-0.10650590807199478,3.196619987487793,1.5687898397445679,3.3810524940490723,-0.20458725094795227,1.947638988494873,-0.17957328259944916,0.6638663411140442,1.4990065097808838,1.0839821100234985,-0.13390646874904633,1.798355221748352,1.460438847541809,2.6365201473236084,2.2331058979034424,0.5038972496986389,1.672692894935608,3.618595838546753,2.4914488792419434,-0.7637554407119751,2.602285861968994,1.6152315139770508,2.3165502548217773,4.036905288696289,1.7626341581344604,-0.30745241045951843,1.3061516284942627,3.0544943809509277,2.2432503700256348,0.5260573625564575,0.26869526505470276,0.03409036993980408,-0.5223769545555115,0.39339277148246765,2.0766706466674805,-0.38802358508110046,3.801676034927368,2.3770365715026855,3.5236270427703857,3.0124449729919434,3.137105703353882,3.03592848777771,1.0055789947509766,2.3000667095184326,3.485448122024536,2.8649325370788574,3.2783849239349365,2.067582130432129,1.7887386083602905,1.117841124534607,1.9235390424728394,-0.5475263595581055,2.5567400455474854,-0.42588913440704346,1.6459540128707886,3.0523805618286133,3.548125743865967,0.9675278663635254,1.0511583089828491,1.8744837045669556,-0.2662609815597534,2.020230531692505,2.395968198776245,1.5245238542556763,2.208904504776001,-0.23743867874145508,2.1941235065460205,3.066039800643921,0.8780515789985657,-0.3361736238002777,0.12655974924564362,0.5816671252250671,2.5916388034820557,1.8386311531066895,0.9971460103988647,-0.09463711082935333,1.4201574325561523,3.414994478225708,-0.5812013745307922,1.0859824419021606,3.207399845123291,1.0073139667510986,1.059942603111267,0.9441660642623901,2.421081066131592,-0.9699038863182068,-0.7918293476104736,3.2947211265563965,3.453516721725464,2.6633572578430176,1.12492835521698,0.9515612125396729,1.8676871061325073,2.2264864444732666,0.4588735103607178,-0.7277568578720093,-1.0359772443771362,2.857710599899292,2.6983368396759033,1.5439820289611816,0.8810650110244751,3.6186933517456055,3.788325071334839,2.0987229347229004,3.1248533725738525,-0.6990334391593933,3.2795536518096924,0.4755188822746277,2.3451409339904785,1.9918140172958374,2.239830255508423,2.980741500854492,3.4494681358337402,2.7023935317993164,0.8716931343078613,1.143310546875,0.12861472368240356,1.2377668619155884,-0.3922814130783081,3.3432180881500244,1.7890307903289795,0.9444081783294678,2.8529300689697266,0.18657220900058746,2.6263394355773926,0.48836591839790344,0.770980954170227,0.9084759950637817,2.9118480682373047,0.899668276309967,0.3940073549747467,0.8528211116790771,1.6868844032287598,0.24871914088726044,2.4808976650238037,2.190305709838867,1.4015684127807617,-0.11070117354393005,1.4967073202133179,0.9283192753791809,0.4603424668312073,1.1601427793502808,1.9638222455978394,-1.3094513416290283,1.8755643367767334,-0.034235879778862,2.5533201694488525,2.5379700660705566,3.77651309967041,2.733030319213867,0.3953371047973633,2.7038984298706055,1.0902845859527588,3.95662522315979,3.4667394161224365,1.4680428504943848,-0.2102063000202179,1.9835692644119263,0.25430288910865784,2.2354414463043213,0.19437259435653687,1.457194447517395,1.478986144065857,0.024254370480775833,0.9681276082992554,-1.2682996988296509,1.4637945890426636,0.9524343013763428,3.2517035007476807,1.0663663148880005,1.047045350074768,2.5798704624176025,1.9295272827148438,-0.4900413751602173,1.2294294834136963,3.744098663330078,1.1958093643188477,3.4060301780700684,1.4086087942123413,3.195308208465576,-0.21053600311279297,1.9594486951828003,1.3294121026992798,0.7901973128318787,-1.0279371738433838,0.18727345764636993,2.7890584468841553,1.1563249826431274,-0.1735643744468689,2.2202253341674805,0.9077820777893066,2.668337821960449,2.603579521179199,0.24776341021060944,1.158795714378357,1.6285470724105835,1.96036958694458,3.842803716659546,-0.05351594090461731,1.0603022575378418,3.5160486698150635,3.203173875808716,-0.22633570432662964,1.7337943315505981,1.1724951267242432,0.17315110564231873,0.7105343341827393,2.4431610107421875,3.8104262351989746,0.10712434351444244,0.713238537311554,3.1322901248931885,3.2645468711853027,0.534716784954071,0.5656581521034241,3.5241639614105225,2.5362460613250732,-0.7327529191970825,-0.8334300518035889,2.0500335693359375,0.9878270626068115,2.0549156665802,3.280917167663574,0.543793261051178,0.40602004528045654,1.2652292251586914,1.6847025156021118,0.6613606810569763,0.48743921518325806,1.5848220586776733,0.15714702010154724,3.097687244415283,3.4039037227630615,3.8253872394561768,-0.3216516077518463,0.36484676599502563,3.5613582134246826,0.849107563495636,2.804208278656006,3.8222780227661133,1.8004229068756104,1.4634736776351929,3.1937408447265625,0.8675617575645447,3.476331949234009,0.3727441728115082,3.1206748485565186,1.5812299251556396,-0.6251404285430908,3.3036246299743652,3.7486493587493896,1.7302109003067017,2.3743913173675537,0.9696566462516785,0.4081820845603943,0.6557876467704773,2.6788806915283203,3.448852300643921,1.163936972618103,3.5292248725891113,2.8257150650024414,3.515986680984497,1.9399545192718506,0.4077995717525482,0.766963541507721,-0.4135100543498993,0.42341694235801697,2.8088972568511963,-0.6076659560203552,0.1522863209247589,3.2291979789733887,1.588698387145996,1.8325613737106323,0.9248793125152588,1.3020033836364746,3.420180559158325,1.643860101699829,1.1540601253509521,0.21492479741573334,0.1739095002412796,1.5416576862335205,3.289651393890381,-0.5035665035247803,-0.46198034286499023,3.0262765884399414,3.655320405960083,1.8784151077270508,0.3909582197666168,0.8705004453659058,1.1364428997039795,1.208755612373352,0.26025456190109253,0.44635772705078125,0.12183551490306854,-0.15554407238960266,-0.25469839572906494,-0.45932525396347046,0.3801633417606354,2.1852316856384277,3.0103812217712402,0.16040968894958496,0.17387855052947998,2.1033482551574707,3.5950064659118652,1.6760215759277344,3.2061989307403564,0.3962271809577942,2.2076709270477295,-0.8994259834289551,2.514460802078247,0.979424774646759,3.5055930614471436,0.26043587923049927,0.08888838440179825,0.9722239375114441,1.9090890884399414,1.4592763185501099,0.7288630604743958,0.5196076035499573,2.52851939201355,0.3626737594604492,0.9660953879356384,1.4570801258087158,1.5067415237426758,1.0723236799240112,1.9673024415969849,0.5451234579086304,2.6414546966552734,0.7439343333244324,3.4168541431427,1.108274221420288,0.9759180545806885,3.0499231815338135,0.3342808485031128,1.1762615442276,1.2191663980484009,1.9902498722076416,2.7178072929382324,2.1931447982788086,1.4139740467071533,3.4444408416748047,-0.2844769358634949,2.468306541442871,2.982534408569336,1.1627116203308105,2.345379114151001,-0.26387879252433777,2.15932035446167,-0.09391316771507263,2.315988540649414,3.9019370079040527,1.4993696212768555,0.8209245204925537,0.1551581770181656,2.0490360260009766,1.3861424922943115,1.0227948427200317,-0.12519274652004242,3.113816976547241,1.2089561223983765,-0.26088747382164,1.8862637281417847,3.144066333770752,-1.0393073558807373,2.0746145248413086,1.443308711051941,1.180871844291687,-0.6809756755828857,1.017892599105835,1.8907074928283691,-1.2943123579025269,0.1569855809211731,0.6024736166000366,0.8391635417938232,2.21470046043396,-1.1607123613357544,1.8066606521606445,2.297105312347412,2.370729446411133,1.7525073289871216,0.9918195605278015,0.6040194034576416,2.8629562854766846,3.4195144176483154,1.8813725709915161,3.7637126445770264,0.20570334792137146,2.110116958618164,1.3198965787887573,1.0308681726455688,1.5510575771331787,1.0269742012023926,3.243104934692383,0.5696408748626709,-1.0560033321380615,3.80950927734375,-0.9885093569755554,2.1285574436187744,0.27239152789115906,-0.9925388693809509,2.664095401763916,0.5445898771286011,3.4431591033935547,2.9532716274261475,1.4311859607696533,2.9324100017547607,3.054269313812256,0.8955436944961548,0.025119181722402573,3.7716782093048096,0.6967297196388245,2.6491522789001465,2.385422945022583,3.098644495010376,3.803661823272705,1.4175082445144653,2.0697543621063232,3.0386083126068115,3.8905580043792725,1.805167555809021,3.370250940322876,0.6452329754829407,1.198987364768982,4.435891628265381,3.9789042472839355,0.2224683314561844,0.784505307674408,3.900918483734131,-0.08793401718139648,-0.20969590544700623,0.9383320212364197,2.9824304580688477,0.4146740138530731,3.451068878173828,0.58146733045578,2.629016876220703,2.5896897315979004,0.8220140933990479,2.19604229927063,2.3683815002441406,1.0513523817062378,-0.5379165410995483,2.0229125022888184,2.577913522720337,1.3024059534072876,2.953939914703369,1.1526473760604858,-1.245646595954895,0.6586165428161621,1.9431781768798828,0.795759916305542,0.4541209042072296,1.3323132991790771,0.9036451578140259,1.0493559837341309,2.936338424682617,1.658136010169983,2.7779719829559326,0.217825248837471,-0.08116167783737183,0.8917245864868164,1.92644202709198,1.5383085012435913,3.03080153465271,-0.5870935916900635,1.1749986410140991,1.7433806657791138,3.2300631999969482,1.3536816835403442,1.4494167566299438,2.132977247238159,1.142106533050537,1.0964027643203735,-1.2843916416168213,2.0309720039367676,0.38181358575820923,0.4180848002433777,1.397927165031433,1.0205312967300415,-1.0413587093353271,-0.3866738975048065,-0.9114582538604736,2.2314321994781494,-0.9689415693283081,0.8953730463981628,-0.7223781943321228,3.3937878608703613,-0.1200912594795227,1.4780992269515991,-1.0540677309036255,1.243822693824768,1.1921714544296265,-0.9881787300109863,2.7142724990844727,1.035949468612671,1.7600966691970825,3.096761703491211,1.0765949487686157,-0.3055479824542999,0.8458605408668518,1.8884222507476807,0.7532628178596497,3.692111015319824,2.047429084777832,1.5398664474487305,2.6263246536254883,3.3768866062164307,1.3354458808898926,2.8829421997070312,3.073280096054077,2.618222951889038,2.2130420207977295,1.131805658340454,1.4583265781402588,-0.6291894912719727,3.3120579719543457,3.235522985458374,2.4912800788879395,0.8031163215637207,2.785531520843506,1.2172937393188477,2.5934207439422607,0.44533485174179077,3.7607927322387695,0.8943837881088257,2.2763075828552246,0.15780554711818695,2.040842056274414,0.27494242787361145,3.4471356868743896,2.545928955078125,3.3168694972991943,3.057359218597412,2.0106849670410156,3.7272539138793945,-0.2229020744562149,-0.3912656009197235,2.5650482177734375,0.5276594758033752,1.9719535112380981,3.524745225906372,1.5316689014434814,0.3282936215400696,2.1106743812561035,2.4451394081115723,1.7900829315185547,2.8499183654785156,2.917431354522705,-0.6730669736862183,2.2880237102508545,2.3319878578186035,0.7816553711891174,3.079967975616455,0.3783358633518219,3.771744728088379,3.4605367183685303,3.4098503589630127,2.232668399810791,2.7577970027923584,0.8026202917098999,0.8486886024475098,1.336401104927063,1.9623465538024902,2.0950241088867188,2.3696439266204834,2.694972276687622,0.7788678407669067,1.5454350709915161,0.642559289932251,1.9104548692703247,2.817577838897705,1.2944279909133911,2.2089552879333496,2.9942898750305176,0.6504705548286438,2.6843504905700684,1.2824846506118774,-0.555378794670105,1.9917773008346558,2.4853670597076416,2.2720730304718018,0.05783434212207794,1.741772174835205,3.9805495738983154,1.6283323764801025,3.9042468070983887,-0.08662469685077667,1.0114973783493042,-0.2651676535606384,2.276035785675049,-0.6100764274597168,2.0067296028137207,1.3208794593811035,1.0322285890579224,1.9896197319030762,2.3109211921691895,2.2940375804901123,0.7046573758125305,2.1666882038116455,0.8269322514533997,1.038419246673584,3.1033222675323486,2.011228084564209,0.4995725154876709,3.0119717121124268,1.7737351655960083,1.1829763650894165,3.768042802810669,2.453611135482788,-1.2905018329620361,0.4045698344707489,-0.7276555299758911,1.108733057975769,0.7043386101722717,1.7317860126495361,3.1328790187835693,1.5019123554229736,0.00757896201685071,1.7896612882614136,4.704597473144531,-1.2845878601074219,1.471731185913086,3.7090561389923096,-0.9188922643661499,0.25417712330818176,0.40908655524253845,-0.6070359349250793,3.279542922973633,2.3822507858276367,-1.0290439128875732,-1.0308316946029663,1.6611809730529785,0.5246748328208923,1.5900543928146362,1.5552963018417358,1.9574549198150635,0.3487606346607208,1.906638503074646,-0.15992236137390137,-0.1188686341047287,1.874509334564209,2.240257501602173,3.598365068435669,2.4335744380950928,3.23817777633667,1.554663896560669,2.292228937149048,1.6409223079681396,1.495506763458252,3.2188000679016113,3.7398290634155273,2.8911819458007812,3.769780397415161,2.451735019683838,2.5036814212799072,3.150447130203247,1.2291960716247559,0.9691900610923767,3.689431667327881,-0.07314413040876389,0.73487389087677,0.839801013469696,1.4524890184402466,2.1931369304656982,1.3259851932525635,3.0612916946411133,0.2924751043319702,0.6916547417640686,2.3885679244995117,0.7535249590873718,1.8109625577926636,3.2876040935516357,2.6223843097686768,2.415797472000122,2.450838565826416,1.9013489484786987,2.8400003910064697,3.637033224105835,1.2191064357757568,0.49888309836387634,0.527450680732727,3.3894202709198,2.8021371364593506,1.5861856937408447,-0.5647964477539062,1.773905634880066,2.8972055912017822,0.4014616310596466,3.552330493927002,3.5676872730255127,0.44267264008522034,2.05513334274292,2.6361255645751953,2.9887442588806152,0.289135605096817,3.268169641494751,3.1244053840637207,0.8573437333106995,-0.19672121107578278,-0.7705172300338745,1.6629849672317505,3.544947862625122,-0.6638178825378418,3.600919008255005,-0.5163041353225708,-0.025210803374648094,1.4350130558013916,1.8289581537246704,0.6889894008636475,1.6060850620269775,-0.9072462916374207,2.4202475547790527,1.693398356437683,0.15449632704257965,3.578507661819458,2.314469575881958,4.283806324005127,2.3266613483428955,0.48093342781066895,3.2087161540985107,-0.08493365347385406,1.7356622219085693,1.580642580986023,1.29006028175354,0.9935421347618103,2.0631296634674072,2.585738182067871,1.8831286430358887],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"24_sarcasm_irony_humor\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"24_sarcasm_irony_humor\"],\"x\":[7.443446159362793,7.453991889953613,7.434319496154785,7.457515239715576,7.445913314819336,7.432080268859863,7.492039680480957,7.444361686706543,7.45388650894165,7.470397472381592,7.456826210021973,7.457319259643555,7.452033042907715,7.453394889831543],\"y\":[3.3650591373443604,3.3583810329437256,3.3732411861419678,3.3514108657836914,3.3623621463775635,3.374535083770752,3.3646886348724365,3.3602709770202637,3.329127788543701,3.332568883895874,3.3524117469787598,3.345237970352173,3.361050605773926,3.3561806678771973],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"25_dense_retrieval_retrievers\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"25_dense_retrieval_retrievers\"],\"x\":[11.089601516723633,11.02492904663086,10.883186340332031,10.981522560119629,10.981151580810547,10.95821762084961,10.9121675491333,10.982845306396484,11.157200813293457,10.896283149719238,10.92767333984375,10.96117877960205,10.979663848876953],\"y\":[2.4378020763397217,2.470427989959717,2.49906587600708,2.4681739807128906,2.4417030811309814,2.4632935523986816,2.5861077308654785,2.4596455097198486,2.333122968673706,2.6070048809051514,2.5345096588134766,2.460688352584839,2.480128526687622],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"26_argumentative_environmental_firms\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"26_argumentative_environmental_firms\"],\"x\":[9.245034217834473,9.240274429321289,9.255699157714844,9.306416511535645,9.330011367797852,9.5535249710083,9.301241874694824,9.279705047607422,9.539828300476074,9.543222427368164,9.282510757446289,9.352497100830078],\"y\":[2.6040866374969482,2.5882256031036377,2.6099462509155273,2.364428997039795,2.5036041736602783,2.5556437969207764,2.5966074466705322,2.450493335723877,2.5414693355560303,2.4918816089630127,2.483466386795044,2.526350498199463],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"28_modification_mp2_multiword\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"28_modification_mp2_multiword\"],\"x\":[12.416332244873047,12.561809539794922,12.500757217407227,12.375421524047852,12.55825138092041,12.481864929199219,12.49730396270752,12.1989107131958,12.471221923828125,12.578041076660156,12.469315528869629,12.464476585388184],\"y\":[1.9628962278366089,2.3608059883117676,2.222209930419922,2.1565310955047607,2.1810107231140137,2.1097028255462646,2.3813178539276123,2.188523292541504,2.159067153930664,2.3556671142578125,1.6200826168060303,2.1543469429016113],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"30_counterfactuals_scone_counterfactual\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"30_counterfactuals_scone_counterfactual\"],\"x\":[10.26634407043457,10.128600120544434,10.363204956054688,10.149825096130371,10.254651069641113,10.238754272460938,10.266803741455078,10.18940258026123,10.27680492401123,10.135074615478516,10.226946830749512],\"y\":[2.246767520904541,2.1721770763397217,2.3008975982666016,2.423623561859131,2.2409117221832275,2.265772581100464,2.3193318843841553,2.154175281524658,2.2092831134796143,2.1171963214874268,2.24501371383667],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"48_chatgpt_responses_chatgpts_instructio\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"48_chatgpt_responses\"],\"x\":[8.408228874206543,8.558862686157227,8.468222618103027,8.394969940185547,8.488740921020508,8.462080955505371,8.458389282226562,8.648653030395508,8.45824146270752,8.496983528137207,8.497143745422363,8.60595989227295,8.652069091796875,8.46937370300293,8.726569175720215,8.704874992370605,8.474954605102539,8.499213218688965,8.679919242858887,8.863896369934082,8.659948348999023,8.705171585083008,8.791703224182129,8.700974464416504,8.720808029174805,9.014729499816895,8.698103904724121,8.63776969909668,8.665627479553223,8.607316970825195],\"y\":[0.8488311767578125,0.8990485668182373,0.930065929889679,0.9668167233467102,0.8830121159553528,0.8673952221870422,0.9546473622322083,0.9203405976295471,0.9518395662307739,0.8990188241004944,0.8826806545257568,1.001754879951477,0.8333197236061096,0.94631427526474,0.7836666703224182,0.7688788771629333,0.916922390460968,0.7554416656494141,0.4001754820346832,0.5290360450744629,0.39178118109703064,0.5974481105804443,0.46730977296829224,0.44593337178230286,0.3917016386985779,0.42155221104621887,0.3866553008556366,0.5923276543617249,0.5228016972541809,0.7295419573783875],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"50_proprietary_opensourced_oosf_utility_\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"50_proprietary_opensourced\"],\"x\":[11.03643798828125,11.07364559173584,11.248010635375977,11.170310974121094,10.949195861816406,11.285311698913574,10.957159996032715,11.096034049987793,11.2238130569458,11.023275375366211,11.042583465576172,11.16788101196289,11.188385009765625,8.700318336486816,8.662346839904785,8.710755348205566,8.685635566711426,10.063787460327148,8.825141906738281,9.746978759765625,8.632473945617676,8.7011137008667,9.762179374694824,10.958747863769531,10.246313095092773],\"y\":[3.106004476547241,3.0234382152557373,3.3541853427886963,3.115736246109009,3.0922281742095947,3.283050775527954,3.0349786281585693,3.2528135776519775,3.086516857147217,3.087618112564087,3.101942300796509,3.0745949745178223,3.133884906768799,1.2634673118591309,1.199538230895996,1.2940387725830078,1.2151010036468506,2.198474407196045,1.475010633468628,3.3699069023132324,1.1698120832443237,1.2515336275100708,3.3798534870147705,1.8569083213806152,2.517526388168335],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"51_transformer_reasoning_parameters_prom\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"51_transformer_reasoning\"],\"x\":[12.020341873168945,12.169936180114746,12.101693153381348,12.094861030578613,11.659080505371094,12.379535675048828,12.050992965698242,12.262616157531738,12.431180000305176,11.988661766052246,12.082118034362793,12.085043907165527,12.180397033691406,12.176328659057617,11.84795093536377,12.084325790405273,12.128952980041504,12.28459644317627,12.194994926452637,12.008187294006348,11.911856651306152,12.048596382141113,12.229466438293457,12.182258605957031,12.1466703414917,12.20826530456543,11.903266906738281,11.976790428161621,11.813048362731934,12.021183967590332,12.020978927612305,12.224409103393555,11.788299560546875,12.209701538085938,11.93454647064209,11.924176216125488,12.019671440124512,12.825284004211426,12.15970516204834,12.297412872314453,12.158596992492676,12.110843658447266,12.26720905303955,12.509387016296387,12.426708221435547,12.31076431274414,12.200416564941406,12.108731269836426,12.027480125427246,12.294480323791504,12.179040908813477,11.868775367736816,12.016388893127441,12.533409118652344,11.871016502380371,12.278585433959961,12.497551918029785,11.953156471252441,12.103267669677734,12.292784690856934,12.235295295715332,11.931829452514648,13.446449279785156,13.293713569641113,13.339015007019043,13.234209060668945,13.390213012695312,13.275945663452148,13.059819221496582,13.199735641479492,13.168375968933105,13.352516174316406,13.391827583312988,13.276883125305176,13.3883638381958,13.391206741333008,13.22536563873291,13.355441093444824,13.415063858032227,13.445826530456543,13.263446807861328,13.253372192382812,13.080596923828125,13.292510986328125,13.387595176696777,13.407983779907227,13.065677642822266,13.157443046569824,13.285446166992188,13.371879577636719,13.443427085876465,13.312077522277832,13.325447082519531,13.201516151428223,13.412653923034668,13.387580871582031,13.376065254211426,13.23760986328125,13.391357421875,13.28543758392334,13.155158996582031,13.134252548217773,13.264272689819336,13.326519012451172,13.441741943359375,13.180994987487793,13.229814529418945,13.385966300964355,13.26877498626709,13.252023696899414,9.99891185760498,9.666812896728516,9.87675666809082,9.759414672851562,9.77737045288086,9.712926864624023,9.860082626342773,10.150372505187988,9.757026672363281,9.756099700927734,9.88352108001709,9.667094230651855,9.834576606750488,9.662117958068848,9.90071964263916,9.823662757873535,9.885151863098145,9.897980690002441,9.599701881408691,9.828813552856445,10.166223526000977,9.664942741394043,9.68136215209961,9.909587860107422,9.613624572753906,9.683416366577148,9.677172660827637,9.895627975463867,9.831110954284668,9.791001319885254,9.888591766357422,10.030932426452637,10.003292083740234,11.668291091918945,11.893047332763672,11.399813652038574,11.399984359741211,11.689655303955078,11.275130271911621,11.221017837524414,12.198081016540527,12.027531623840332,11.27303695678711,11.897500038146973,11.873612403869629,11.806347846984863,11.46186637878418,11.398143768310547,11.580697059631348,11.322710990905762,11.420403480529785,11.564152717590332,11.334028244018555,11.482306480407715,11.22659969329834,11.433297157287598,11.450419425964355,11.489837646484375,11.502764701843262,11.421748161315918,11.442119598388672,12.117900848388672,11.278861999511719,9.238378524780273,9.30147647857666,9.232583999633789,9.182598114013672,9.238131523132324,9.218341827392578,9.234349250793457,9.302995681762695,9.232303619384766,9.124175071716309,9.416687965393066,9.293940544128418,9.349409103393555,9.29613208770752,9.22963809967041,9.208431243896484,9.365690231323242,9.037209510803223,9.345200538635254,9.246965408325195,9.236005783081055,9.268945693969727,11.956093788146973,12.490596771240234,12.429107666015625,12.604439735412598,12.537409782409668,12.447066307067871,12.514530181884766,12.494388580322266,12.444210052490234,12.318142890930176,12.335335731506348,12.499151229858398,12.503713607788086,12.477373123168945,12.43051815032959,12.483855247497559,12.453995704650879,12.551338195800781,12.408796310424805,12.414511680603027,10.386235237121582,10.229713439941406,10.389017105102539,10.301673889160156,10.361222267150879,10.3976469039917,10.311823844909668,10.32546329498291,10.373322486877441,10.308487892150879,10.342009544372559,10.355380058288574,10.305009841918945,10.051471710205078,10.35975456237793,11.601570129394531],\"y\":[3.4334287643432617,3.9167301654815674,3.949054002761841,3.438246250152588,3.445382595062256,3.36970853805542,3.7780611515045166,2.977111577987671,3.824798583984375,4.014473915100098,3.966614246368408,3.947669744491577,3.546800136566162,3.5740740299224854,3.722076654434204,3.7931604385375977,3.4850759506225586,3.1680963039398193,4.030865669250488,3.494990587234497,3.631016969680786,3.9821364879608154,3.874598264694214,3.628248453140259,3.362018585205078,3.6282668113708496,3.4427807331085205,3.539586305618286,3.9404172897338867,3.503599166870117,3.3990092277526855,3.441103219985962,2.9686286449432373,3.454322576522827,3.507044553756714,3.588791847229004,3.930246591567993,3.598909854888916,3.4998040199279785,3.598375082015991,3.547398805618286,3.815009355545044,3.1761443614959717,3.7004573345184326,2.9148776531219482,3.357351303100586,3.646010398864746,4.046837329864502,3.844369649887085,3.3957107067108154,3.5331528186798096,3.4589388370513916,3.6157045364379883,3.513286590576172,3.4719655513763428,3.176704168319702,3.3323616981506348,3.9445858001708984,3.7024219036102295,3.222160577774048,3.219435214996338,3.6485061645507812,2.849513053894043,2.6924357414245605,2.519382953643799,2.685671091079712,3.4131829738616943,3.3496320247650146,2.9062132835388184,2.5787570476531982,2.8339319229125977,2.6660850048065186,3.363393783569336,3.0692508220672607,3.403278112411499,2.94502854347229,2.862663984298706,2.849548578262329,3.311357021331787,2.857738494873047,2.829335927963257,2.7103164196014404,2.6873369216918945,3.1780202388763428,2.415661573410034,2.984997510910034,2.9960906505584717,3.0859627723693848,3.064985752105713,3.3739285469055176,2.99613094329834,3.3052146434783936,2.759016990661621,2.554168224334717,2.7810420989990234,3.370600700378418,2.8825161457061768,2.5941309928894043,3.402747631072998,3.1956849098205566,2.574057102203369,2.4850573539733887,3.1060783863067627,2.757913827896118,2.8156826496124268,2.6702122688293457,3.1264901161193848,3.3880455493927,2.8700947761535645,2.6230366230010986,-1.2806227207183838,-1.4134907722473145,-1.3984967470169067,-0.8591680526733398,-1.1667323112487793,-0.9986860752105713,-1.226072072982788,-1.2499401569366455,-1.3567789793014526,-1.3018178939819336,-1.279323697090149,-1.3926376104354858,-0.8900179862976074,-1.3667707443237305,-1.3356115818023682,-1.4169893264770508,-1.3518718481063843,-1.17157781124115,-1.1077362298965454,-1.3785203695297241,-1.242591381072998,-1.3728218078613281,-1.36669921875,-1.1680376529693604,-1.3303998708724976,-0.8987433314323425,-1.368369460105896,-1.1938246488571167,-1.3907575607299805,-1.302925705909729,-1.1836531162261963,-1.1196755170822144,-1.154442310333252,1.6213551759719849,1.7356750965118408,1.8189188241958618,2.011199474334717,1.6284370422363281,1.6911828517913818,1.434216856956482,1.9124113321304321,1.708951473236084,1.345273733139038,1.9241002798080444,1.706599235534668,1.5955106019973755,1.6075619459152222,1.8218833208084106,1.9240765571594238,1.5612646341323853,1.4347044229507446,1.8333743810653687,1.4203897714614868,1.508184552192688,1.6106964349746704,1.7237390279769897,1.6263391971588135,1.7565888166427612,1.8806627988815308,1.4971145391464233,1.4235330820083618,2.043459415435791,1.6557258367538452,0.11233166605234146,0.2616926431655884,0.13499541580677032,0.13999022543430328,0.13154223561286926,0.03455710411071777,0.1343674212694168,0.09137420356273651,0.0880853608250618,-0.03193724900484085,0.282012939453125,0.19786401093006134,0.27887430787086487,0.23954655230045319,0.1252308189868927,0.2048257291316986,0.21515387296676636,0.2964838743209839,0.28831443190574646,0.10827220976352692,0.010295077227056026,0.17448420822620392,-0.01690675877034664,-0.3107600808143616,-0.2527309060096741,-0.2602718472480774,-0.33801159262657166,-0.32423314452171326,-0.19620446860790253,-0.286507248878479,-0.2868269979953766,2.4987032413482666,-0.19626036286354065,-0.2882573902606964,2.8162295818328857,-0.27515092492103577,-0.28718024492263794,-0.29625701904296875,-0.3112342059612274,-0.32359012961387634,-0.29335451126098633,-0.24639558792114258,-0.7522066235542297,-0.7360655665397644,-0.7566789388656616,-0.7287562489509583,-0.7565518617630005,-0.6721810698509216,-0.7266584038734436,-0.7462540864944458,-0.6999889016151428,-0.7211228609085083,-0.7179433107376099,-0.7139708399772644,-0.7397772073745728,-0.6260511875152588,-0.7241697311401367,1.5872716903686523],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"52_dialogue_languages_dataset_language_e\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"52_dialogue_languages\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"x\":[13.87353229522705,13.208016395568848,14.106237411499023,13.092193603515625,13.336145401000977,13.386882781982422,12.319914817810059,13.021512985229492,13.98583698272705,12.727828025817871,14.158334732055664,12.96531867980957,13.214096069335938,13.292146682739258,13.683382034301758,13.187692642211914,13.279541969299316,12.989252090454102,12.971858978271484,12.976456642150879,13.035022735595703,12.499113082885742,13.1813325881958,12.979851722717285,13.296390533447266,13.039684295654297,13.842495918273926,13.132991790771484,13.9254732131958,12.167425155639648,14.098097801208496,13.920403480529785,12.558815002441406,12.849774360656738,12.970587730407715,14.02845287322998,12.911493301391602,13.765254974365234,13.905959129333496,13.326302528381348,13.942405700683594,12.925593376159668,13.41197395324707,13.25645637512207,12.991842269897461,13.592504501342773,13.934208869934082,13.955780029296875,13.298561096191406,13.263577461242676,12.345218658447266,13.947273254394531,13.009997367858887,13.643670082092285,13.180303573608398,13.74915599822998,12.808438301086426,13.863404273986816,12.991873741149902,13.162498474121094,12.859356880187988,13.075448036193848,13.156818389892578,12.886246681213379,13.997769355773926,13.03771686553955,12.784741401672363,12.894862174987793,12.97797966003418,13.60038948059082,14.051366806030273,12.930017471313477,12.712410926818848,12.782764434814453,12.85952377319336,12.852415084838867,13.387727737426758,13.939677238464355,12.471564292907715,13.912345886230469,12.641358375549316,13.367488861083984,13.8226318359375,13.64598560333252,13.757844924926758,12.249322891235352,13.732461929321289,13.127846717834473,12.968668937683105,13.813699722290039,13.807978630065918,13.008394241333008,13.787175178527832,13.993060111999512,13.531773567199707,13.984949111938477,13.01711654663086,14.002456665039062,13.822026252746582,13.829243659973145,13.329290390014648,12.908177375793457,14.121039390563965,13.951969146728516,12.813560485839844,13.843708038330078,13.839166641235352,14.121064186096191,12.558928489685059,13.817070960998535,13.30069637298584,13.859855651855469,13.09512710571289,12.266813278198242,12.803173065185547,13.087562561035156,13.733831405639648,13.884516716003418,12.79514217376709,13.156198501586914,12.635796546936035,13.271025657653809,12.98554801940918,13.871160507202148,13.031496047973633,14.019685745239258,13.770309448242188,13.913249969482422,13.995428085327148,12.395220756530762,13.254109382629395,12.519502639770508,13.016617774963379,13.239713668823242,13.700984954833984,13.32162857055664,13.05946159362793,12.602531433105469,12.626888275146484,13.298456192016602,13.010909080505371,12.473347663879395,13.30741024017334,12.52789306640625,13.24752140045166,14.047928810119629,12.949474334716797,13.951871871948242,14.054276466369629,12.840779304504395,12.930652618408203,13.249177932739258,14.00323486328125,12.957128524780273,13.71989631652832,12.736191749572754,13.225967407226562,11.134943962097168,11.025127410888672,11.245323181152344,11.604127883911133,11.156891822814941,11.352185249328613,11.527443885803223,11.282083511352539,11.126087188720703,11.73695182800293,11.573534965515137,13.375391006469727,11.404452323913574,11.218412399291992,10.824934959411621,11.162917137145996,11.561422348022461,11.366466522216797,10.656908988952637,11.300393104553223,11.272549629211426,11.278218269348145,11.19459056854248,11.2599458694458,11.612763404846191,10.9222412109375,11.204724311828613,11.096510887145996,11.001474380493164,11.493638038635254,11.407891273498535,11.244793891906738,11.168529510498047,11.017486572265625,11.451024055480957,11.223573684692383,11.187163352966309,11.271790504455566,11.26801872253418,11.24870491027832,11.188407897949219,11.215272903442383,11.429038047790527,11.241147994995117,11.28900146484375,11.684981346130371,11.643854141235352,11.422016143798828,11.644282341003418,11.211750030517578,11.794293403625488,10.666879653930664,11.676538467407227,11.204644203186035,11.4844388961792,11.210556983947754,10.822954177856445,11.602618217468262,11.172928810119629,11.261439323425293,11.18604850769043,11.24404525756836,11.392868995666504,11.273892402648926,11.069701194763184,11.81900691986084,11.298412322998047,11.697004318237305,11.151240348815918,11.614058494567871,11.220532417297363,11.269240379333496,11.204486846923828,11.25660228729248,11.144911766052246,11.228436470031738,11.389638900756836,11.027185440063477,11.341411590576172,11.200804710388184,10.692567825317383,11.169511795043945,11.02439022064209,10.92267894744873,11.365792274475098,11.304564476013184,11.123757362365723,11.317601203918457,11.2233304977417,11.252074241638184,10.953468322753906,11.1937837600708,11.191131591796875,11.507041931152344,13.65318489074707,13.939993858337402,13.685359001159668,13.784351348876953,13.504968643188477,13.94735336303711,13.143325805664062,13.834990501403809,13.44897747039795,13.599161148071289,13.199520111083984,13.714217185974121,13.943765640258789,13.477546691894531,13.838189125061035,13.14396858215332,13.451980590820312,13.139006614685059,13.305334091186523,13.572847366333008,13.578728675842285,13.640463829040527,13.756519317626953,13.478137016296387,13.640913963317871,13.371881484985352,13.30630874633789,13.778650283813477,13.48436164855957,13.48245620727539,13.242944717407227,13.188318252563477,13.671472549438477,13.161858558654785,13.182262420654297,13.67568302154541,13.578418731689453,13.574835777282715,13.530089378356934,12.509833335876465,13.550978660583496,13.269086837768555,13.929557800292969,13.697040557861328,13.825508117675781,13.642824172973633,13.51134204864502,13.453692436218262,13.557821273803711,13.553349494934082,13.648940086364746,13.369372367858887,13.12069034576416,13.566463470458984,13.692540168762207,13.704095840454102,13.338266372680664,13.631470680236816,13.964845657348633,13.344286918640137,13.934170722961426,13.56275463104248,13.731254577636719,13.44790267944336,13.115975379943848,13.600167274475098,8.303739547729492,13.527002334594727,13.690975189208984,13.423891067504883,13.938860893249512,13.628800392150879,13.415291786193848,13.883581161499023,13.465723037719727,13.598390579223633,13.958017349243164,13.456777572631836,13.795967102050781,9.905937194824219,13.677034378051758,13.943849563598633,13.411235809326172,13.518133163452148,13.331656455993652,8.079171180725098,8.06274700164795,7.97455358505249,7.848491191864014,8.032004356384277,7.964422702789307,7.964263916015625,8.134442329406738,8.114529609680176,7.841091632843018,7.8066725730896,8.066001892089844,7.995242118835449,8.329507827758789,8.135189056396484,7.784605979919434,7.775929927825928,8.180281639099121,8.422222137451172,7.919281005859375,7.885015487670898,7.895800590515137,7.771629810333252,8.177929878234863,7.973200798034668,8.28538703918457,7.766735553741455,7.8133392333984375,7.980544567108154,7.834801197052002,7.792785167694092,8.400049209594727,8.152592658996582,7.950541019439697,7.951925754547119,8.21005630493164,7.920266628265381,7.854425430297852,8.000518798828125,7.90179443359375,8.401914596557617,8.250751495361328,7.81003999710083,8.432668685913086,7.853431224822998,8.134767532348633,8.218738555908203,7.8007283210754395,8.225289344787598,7.825068473815918,8.072797775268555,8.098258972167969,8.405604362487793,7.813677787780762,7.821072101593018,7.985950946807861,7.810969352722168,8.3153657913208,8.099970817565918,7.857753276824951,8.26423168182373,7.91890811920166,7.952602386474609,7.912245273590088,7.974617958068848,7.784420490264893,8.138467788696289,8.036944389343262,7.761767864227295,7.890107154846191,8.475704193115234,8.049864768981934,7.910464286804199,7.735909938812256,8.021018028259277,7.9187726974487305,7.908551216125488,7.699703216552734,7.959645748138428,7.826615333557129,8.282672882080078,7.934024333953857,8.200965881347656,7.851012229919434,7.659298419952393,7.964986801147461,7.375672340393066,7.989088535308838,8.16100788116455,7.880589008331299,7.8358564376831055,7.8488359451293945,7.915194511413574,8.008590698242188,7.819448471069336,7.807568073272705,7.392760753631592,7.372259140014648,7.680557727813721,8.323735237121582,7.405981540679932,7.390983581542969,8.47547721862793,7.814692974090576,8.292101860046387,8.115535736083984,7.816876411437988,7.764798164367676,7.4307541847229,8.173230171203613,7.816125869750977,7.800354957580566,7.687350749969482,7.416838645935059,8.04630184173584,7.372389793395996,8.061315536499023,7.425755023956299,7.877260684967041,8.388016700744629,7.779290199279785,8.227629661560059,8.057212829589844,8.02259349822998,7.782587051391602,8.23399543762207,7.90402889251709,7.8760199546813965,7.4668121337890625,8.308961868286133,7.824159622192383,7.712260723114014,7.914191246032715,7.685945987701416,7.940428733825684,10.677026748657227,10.417326927185059,10.19378662109375,10.34382438659668,10.359889030456543,10.168207168579102,10.379969596862793,10.382912635803223,10.3951997756958,10.006451606750488,10.593812942504883,10.523648262023926,10.564718246459961,10.390353202819824,10.409890174865723,10.375975608825684,10.270461082458496,10.380226135253906,10.342330932617188,10.389986038208008,10.570111274719238,10.563359260559082,10.393143653869629,10.445023536682129,10.406803131103516,10.406514167785645,9.976093292236328,10.553617477416992,10.514777183532715,10.404207229614258,10.352548599243164,10.402541160583496,10.373939514160156,10.66649055480957,10.147748947143555,10.177634239196777,10.380538940429688,10.481019020080566,10.569191932678223,10.38936710357666,10.453319549560547,10.417245864868164,10.462665557861328,10.532641410827637,10.139423370361328,10.995478630065918,10.441753387451172,10.408876419067383,10.595641136169434,10.508939743041992,10.599928855895996,10.383362770080566,10.315134048461914,10.520706176757812,10.196410179138184,10.421682357788086,10.112748146057129,10.42435073852539,9.957427978515625,9.407848358154297,9.24934196472168,9.988895416259766,9.616469383239746,9.254940032958984,9.619887351989746,9.763313293457031,9.41704273223877,9.903557777404785,9.640640258789062,9.904218673706055,9.833464622497559,9.501309394836426,10.281272888183594,9.5150146484375,9.301063537597656,9.468867301940918,9.583669662475586,9.652227401733398,9.2976655960083,9.610597610473633,9.950066566467285,9.628329277038574,9.354300498962402,9.685493469238281,9.437472343444824,9.71446418762207,9.970356941223145,9.591911315917969,9.4934720993042,11.245697975158691,9.630011558532715,9.857994079589844,9.644820213317871,9.684257507324219,9.61834716796875,9.774521827697754,9.085481643676758,10.036049842834473,9.738116264343262,9.506299018859863,9.30492877960205,9.699036598205566,9.771791458129883,9.192770957946777,9.514463424682617,9.63692855834961,9.843480110168457,9.38808536529541,9.520946502685547,9.217379570007324,9.838109970092773,9.648056030273438,9.77692699432373,9.458232879638672,9.670414924621582,9.680063247680664,9.057123184204102,9.02020263671875,8.764460563659668,9.149687767028809,8.96601676940918,8.83845329284668,7.854395389556885,8.691113471984863,8.074891090393066,7.991252422332764,8.15771198272705,8.11121940612793,7.889158725738525,8.559086799621582,7.732449054718018,8.129582405090332,7.739405155181885,8.906782150268555,8.779287338256836,8.384539604187012,8.096807479858398,7.73988151550293,7.74143123626709,10.807848930358887,8.938881874084473,8.25653076171875,8.156562805175781,8.910407066345215,8.929011344909668,8.912620544433594,8.349308967590332,9.150344848632812,9.063459396362305,8.188298225402832,8.082099914550781,8.124395370483398,8.095884323120117,7.761401653289795,7.85822057723999,7.735099792480469,9.143465995788574,7.9051079750061035,8.62359619140625,8.771903991699219,8.10483455657959,8.881553649902344,7.842296600341797,7.751319408416748,8.932684898376465,8.793137550354004,8.686760902404785,8.676131248474121,8.958822250366211,7.80643892288208,8.740694046020508,7.7468767166137695,8.771867752075195,9.189112663269043,9.039446830749512,9.115840911865234,9.096116065979004,9.138891220092773,8.879927635192871,8.893594741821289,8.761467933654785,8.84201431274414,8.92170524597168,8.984542846679688,8.870153427124023,9.148591041564941,9.1036376953125,8.759328842163086,9.173047065734863,9.000730514526367,9.311477661132812,9.226938247680664,8.916337966918945,9.266477584838867,8.85345458984375,8.894457817077637,9.150871276855469,8.838128089904785,8.816519737243652,9.046035766601562,8.901216506958008,9.159720420837402,8.813364028930664,9.301010131835938,8.885666847229004,9.035273551940918,9.151689529418945,9.173745155334473,9.153688430786133,9.068913459777832,9.077773094177246,8.93253231048584,9.23687744140625,8.845597267150879,8.906045913696289,9.104510307312012,8.794829368591309,8.974413871765137,8.559188842773438,9.103609085083008,8.886739730834961,8.847504615783691,8.852912902832031,8.7335786819458,8.971410751342773,8.835782051086426,8.914285659790039,9.40961742401123,9.401433944702148,9.173538208007812,9.561808586120605,9.133511543273926,9.540070533752441,9.700275421142578,9.41574478149414,9.289504051208496,9.243343353271484,9.41695785522461,9.496110916137695,9.767770767211914,9.445943832397461,9.416340827941895,9.574084281921387,9.292332649230957,9.226899147033691,9.462366104125977,9.370545387268066,9.44404411315918,9.387925148010254,9.353792190551758,9.420747756958008,9.548852920532227,9.489824295043945,9.414810180664062,9.33342456817627,9.402825355529785,9.672316551208496,9.393604278564453,9.239797592163086,9.441904067993164,9.516083717346191,9.340642929077148,9.363749504089355,9.59388256072998,9.435190200805664,9.716001510620117,9.322286605834961,9.686580657958984,9.433467864990234,9.423392295837402,9.48121166229248,9.288214683532715,9.267616271972656,9.553618431091309,9.478880882263184,9.371923446655273,9.760424613952637,9.519524574279785,9.401512145996094,9.502867698669434,8.319539070129395,8.872060775756836,8.969176292419434,8.707355499267578,8.74543285369873,8.927281379699707,8.319100379943848,8.32375717163086,8.666680335998535,8.335197448730469,8.70714282989502,9.042021751403809,8.712772369384766,8.291357040405273,8.325663566589355,8.38085651397705,8.75160026550293,8.50749397277832,8.352300643920898,8.444624900817871,8.73957347869873,8.366410255432129,8.543909072875977,8.721636772155762,8.295104026794434,8.693644523620605,8.322171211242676,8.898469924926758,8.4146146774292,8.773049354553223,8.323225975036621,9.074934959411621,8.57706356048584,8.7324857711792,8.81857967376709,9.022188186645508,10.685402870178223,null,8.761948585510254,8.723730087280273,9.060052871704102,8.774007797241211,8.736745834350586,8.403592109680176,13.670157432556152,13.798901557922363,13.369604110717773,13.828919410705566,13.80864143371582,13.781868934631348,13.87149715423584,13.169575691223145,13.689194679260254,13.725122451782227,13.867948532104492,13.486695289611816,13.795697212219238,13.88539981842041,13.794876098632812,13.703969955444336,13.475425720214844,13.485220909118652,13.642592430114746,13.45029067993164,13.17920970916748,13.825347900390625,13.710762977600098,13.833584785461426,13.823281288146973,13.777862548828125,13.709671974182129,13.589035034179688,13.624615669250488,13.818294525146484,13.520774841308594,13.499052047729492,11.366277694702148,11.415215492248535,11.413336753845215,11.3681058883667,11.386617660522461,11.464384078979492,11.333633422851562,11.434808731079102,11.411873817443848,11.339577674865723,11.400299072265625,11.394913673400879,11.41909122467041,11.410325050354004,11.436091423034668,11.440723419189453,11.421466827392578,11.424187660217285,10.752309799194336,11.30122184753418,11.429636001586914,11.37482738494873,11.405726432800293,11.432455062866211,11.494464874267578,11.39643669128418,11.412282943725586,11.394112586975098,11.40078353881836,9.92600154876709,9.90449047088623,9.936745643615723,9.92866325378418,9.815776824951172,9.909724235534668,9.910490989685059,9.891134262084961,9.903509140014648,9.932821273803711,9.916892051696777,9.828373908996582,9.930082321166992,9.88005256652832,9.91939926147461,10.002853393554688,9.763079643249512,9.934072494506836,9.936637878417969,9.912662506103516,9.736724853515625,9.912676811218262,9.76375961303711,9.90211296081543,9.953424453735352,9.921619415283203,9.907418251037598,9.980151176452637,9.924909591674805,12.890239715576172,12.9169282913208,12.898111343383789,12.95676326751709,12.863883018493652,12.8355073928833,12.85640811920166,13.136683464050293,12.889006614685059,7.990469932556152,12.911820411682129,12.735673904418945,12.899605751037598],\"y\":[1.3817375898361206,1.0612719058990479,1.1385481357574463,1.08525550365448,1.1626895666122437,1.2480696439743042,0.4848286211490631,1.2720167636871338,1.1438474655151367,1.3171401023864746,1.4076839685440063,1.264595627784729,1.0884864330291748,1.1517572402954102,1.355188012123108,1.7229746580123901,0.972616970539093,1.1486741304397583,0.8810750246047974,1.257293701171875,0.9572843313217163,0.8085752129554749,0.9309483766555786,0.7958930730819702,1.0975350141525269,0.6360377073287964,0.9854050874710083,1.798175573348999,1.3100166320800781,0.67192143201828,1.2066819667816162,1.2865713834762573,0.9056100845336914,0.6249911189079285,1.7463622093200684,1.2420947551727295,0.7832163572311401,1.1565204858779907,1.3271960020065308,1.3933436870574951,1.0287328958511353,0.7349646091461182,1.1217514276504517,1.414216160774231,1.152062177658081,1.4249526262283325,1.1767480373382568,1.2981083393096924,1.1263619661331177,1.0864992141723633,0.5083565711975098,1.0866432189941406,1.8492233753204346,1.125476598739624,0.944848895072937,1.1385208368301392,0.678071916103363,1.3165032863616943,0.6076777577400208,1.3482673168182373,1.6210135221481323,0.9811966419219971,1.2457525730133057,1.171410322189331,1.1819112300872803,0.8604665398597717,0.942077100276947,0.516913115978241,1.7408256530761719,1.4816997051239014,1.3886042833328247,1.7745593786239624,0.794830322265625,0.5049740076065063,0.8629714250564575,0.9947225451469421,1.3124425411224365,1.181308388710022,0.65378338098526,0.9567456841468811,0.8112806677818298,1.0578453540802002,1.270293116569519,1.1971663236618042,1.2050573825836182,0.49335476756095886,1.5063146352767944,1.3702335357666016,1.2522984743118286,1.2417391538619995,1.4349349737167358,1.29513418674469,1.3764736652374268,1.1924550533294678,1.3044599294662476,1.0554274320602417,0.9203598499298096,1.1263551712036133,1.4461302757263184,1.4113985300064087,1.1233012676239014,1.292132019996643,1.383273959159851,1.2921757698059082,1.4860016107559204,1.4392153024673462,1.4001719951629639,0.9837578535079956,0.6659143567085266,0.9185031056404114,1.1586668491363525,1.4281121492385864,1.1629738807678223,0.510953962802887,0.5897344946861267,0.8766728639602661,1.4713213443756104,1.1049691438674927,0.5188847184181213,1.4383823871612549,1.3952010869979858,1.3247922658920288,0.9874094128608704,1.13014554977417,0.8806256651878357,1.1321403980255127,1.1713860034942627,1.29535710811615,1.1741254329681396,0.570444643497467,1.2960554361343384,0.7852280735969543,0.9467426538467407,1.7510077953338623,1.3233586549758911,1.1152814626693726,1.0483297109603882,0.647891640663147,0.5596342086791992,1.3211227655410767,0.9677881002426147,0.6502654552459717,1.0543253421783447,0.6772043704986572,1.212782382965088,1.1973955631256104,1.1335924863815308,1.4092952013015747,1.3452917337417603,0.6293814778327942,1.8506160974502563,1.066854476928711,1.2767295837402344,0.7192075848579407,1.712160587310791,0.8005959391593933,1.1127575635910034,-0.2791852056980133,-0.56585294008255,-0.33805036544799805,-0.5237462520599365,0.26701799035072327,-0.765414834022522,-0.7155061364173889,-0.6682496070861816,-0.6063860654830933,-0.3886547386646271,-0.6360691785812378,-0.5970664024353027,-0.6077590584754944,-0.3316837251186371,0.027478214353322983,-1.0895836353302002,-0.5561929941177368,0.06399431079626083,0.6634865999221802,-0.7742803692817688,0.14976142346858978,-1.053679347038269,-0.8100492358207703,-0.6574156284332275,-0.6182628273963928,2.9431962966918945,-0.8820046186447144,-0.8308306336402893,-0.7561229467391968,0.06208992749452591,-1.2723103761672974,0.021186748519539833,-0.9865592122077942,0.42442458868026733,-0.5611394047737122,-0.19294996559619904,-0.6945754289627075,-0.3245929181575775,0.14573052525520325,-0.8709970712661743,-0.352821946144104,-0.20292386412620544,-0.3928586542606354,-0.45419180393218994,-0.6536911725997925,-0.43990081548690796,0.47350263595581055,0.13158506155014038,-0.7095873355865479,-1.041664481163025,-0.30340245366096497,0.33025580644607544,-0.5825144648551941,-0.9634666442871094,0.1646973192691803,-0.9077123999595642,0.321090430021286,-0.4256440997123718,0.016457444056868553,-0.3007483184337616,-0.9926905035972595,-0.40955081582069397,-0.7163980603218079,-0.7613950967788696,-0.5563022494316101,-0.30304890871047974,-0.6749861836433411,-0.5403029322624207,-0.3122972548007965,-0.610802948474884,-0.7764879465103149,-0.3963066339492798,-0.9999945163726807,-0.2833714783191681,-0.40003347396850586,-0.9637208580970764,-0.45351165533065796,-0.5818671584129333,-0.7205086350440979,-0.9992696046829224,0.6563940644264221,-1.185823917388916,-1.0215363502502441,-0.13112245500087738,-0.6537771224975586,-0.7294257283210754,-0.7630423307418823,-0.7293157577514648,-0.9494206309318542,-0.3285471796989441,-0.1588168889284134,-0.36618950963020325,-0.9938457608222961,0.16926676034927368,-0.4724136292934418,0.05542946234345436,-0.2140544056892395,-0.305911123752594,-0.019443433731794357,0.04754577577114105,-0.5232704877853394,-0.11347304284572601,-0.42826223373413086,-0.3631307780742645,-0.34572187066078186,-0.06021670997142792,0.02899486944079399,-0.5741272568702698,-0.17813809216022491,-0.3307811915874481,-0.5624246597290039,-0.33823102712631226,-0.49408844113349915,-0.2875242233276367,-0.5673142671585083,-0.5054513216018677,-0.19457708299160004,0.04647556692361832,0.15025267004966736,-0.4029132127761841,-0.09265140444040298,-0.06226486340165138,-0.49906080961227417,-0.42240944504737854,-0.37416812777519226,-0.4376997649669647,-0.5433786511421204,-0.48809999227523804,-0.3587982952594757,-0.1461024433374405,-0.4376213848590851,-0.5315554738044739,-0.5797322392463684,2.760768413543701,-0.5617931485176086,-0.37373608350753784,1.0235728025436401,-0.2975458800792694,-0.12453179061412811,-0.39394140243530273,0.08051753044128418,-0.5586355328559875,-0.5786116719245911,-0.5049486756324768,0.1441514790058136,-0.7706948518753052,-0.21628828346729279,-0.3426320552825928,-0.16151897609233856,-0.39145922660827637,-0.5925371646881104,-0.2956721782684326,0.027539370581507683,-0.30378881096839905,0.042292892932891846,-0.12347960472106934,-0.06413992494344711,-0.6200217604637146,-0.4954264461994171,-0.34516313672065735,3.190534830093384,-0.2043280303478241,-0.5706292986869812,-0.7155386209487915,0.12354090809822083,-0.219037726521492,-0.293334424495697,0.07163935899734497,-0.5650919675827026,-0.29002729058265686,0.07026058435440063,-0.5817172527313232,-0.10595829039812088,-1.2771755456924438,-0.2213153839111328,0.0876251682639122,-0.7389064431190491,-0.14196763932704926,-0.6306360363960266,0.011064082384109497,0.014209670014679432,-0.09134206920862198,-0.2708755433559418,-0.20944665372371674,-0.15542514622211456,-0.016267064958810806,0.33848637342453003,1.3138884241925552e-05,-0.3145374655723572,-0.37487006187438965,0.08202043920755386,-0.06734184175729752,-0.26890313625335693,-0.2482864111661911,-0.37177231907844543,-0.342668354511261,0.05140113830566406,-0.009848969988524914,-0.26512715220451355,-0.21851959824562073,-0.20532433688640594,-0.34504765272140503,-0.12112276256084442,-0.11948814243078232,-0.36536410450935364,-0.43950170278549194,-0.3522815406322479,-0.16667716205120087,-0.33151018619537354,-0.4151010513305664,-0.21129287779331207,0.18422266840934753,-0.13335680961608887,0.17345526814460754,0.07078302651643753,-0.06395117193460464,-0.33869442343711853,-0.20178943872451782,-0.20231139659881592,-0.02338528260588646,0.17116601765155792,-0.3580891191959381,-0.31286415457725525,-0.32009008526802063,-0.1331133246421814,0.18067573010921478,-0.2697615623474121,0.08013986051082611,-0.31005680561065674,0.028209263458848,0.10702250152826309,0.2667584717273712,-0.3471834659576416,-0.3380453586578369,-0.07149399816989899,-0.4023083448410034,-0.10830654948949814,-0.17893050611019135,-0.2754707932472229,0.22065694630146027,-0.1637326180934906,-0.12212429195642471,-0.17535138130187988,-0.2760526239871979,-0.01606772653758526,0.11322040110826492,0.021302759647369385,1.5433651208877563,1.4605567455291748,1.4761929512023926,1.4625173807144165,1.4584766626358032,1.4409211874008179,1.4749916791915894,1.4284682273864746,1.4748163223266602,1.4806615114212036,1.4631659984588623,1.411305546760559,1.4848947525024414,1.0450985431671143,1.4943875074386597,1.3517125844955444,1.4878875017166138,1.4422603845596313,1.5346200466156006,1.5406434535980225,1.499199628829956,1.4141327142715454,1.4414336681365967,1.3886102437973022,1.4608150720596313,1.4566928148269653,1.5145379304885864,1.6541327238082886,1.5264090299606323,1.531654953956604,1.4845945835113525,1.4692519903182983,1.5288569927215576,1.5096898078918457,1.3595173358917236,1.587659478187561,1.4848111867904663,1.480909824371338,1.49443781375885,1.6546357870101929,1.5368572473526,1.2316579818725586,1.3251349925994873,1.5530869960784912,1.5514334440231323,1.5379899740219116,1.494699239730835,1.525313377380371,1.4642879962921143,1.526352882385254,1.288293480873108,1.483452320098877,1.4212583303451538,1.5053179264068604,1.3147481679916382,1.3840261697769165,1.5364909172058105,1.516196370124817,1.4429274797439575,1.4955360889434814,1.5462852716445923,1.454986810684204,1.4761298894882202,1.4729735851287842,1.450022578239441,1.443420648574829,1.4701296091079712,4.530323028564453,4.6652750968933105,4.259049415588379,4.17231559753418,4.669703960418701,4.087711334228516,4.67042350769043,4.63741397857666,4.61146879196167,4.00691556930542,4.465812683105469,4.730312824249268,3.997835874557495,4.689051151275635,4.215987682342529,4.713289260864258,3.9740796089172363,4.662024021148682,4.6290106773376465,4.683803558349609,4.155202865600586,4.592298984527588,4.655975341796875,4.570188999176025,4.67529296875,4.149698734283447,4.111392498016357,4.625186920166016,4.129754066467285,4.608168125152588,4.609062194824219,4.684344291687012,4.598238468170166,4.515726089477539,4.097564697265625,4.126251697540283,4.632607936859131,3.846534013748169,4.030094146728516,4.677065372467041,4.595035552978516,4.631924152374268,4.152017593383789,4.6212286949157715,4.092752933502197,4.058029651641846,4.524569034576416,4.68303918838501,4.761435031890869,4.133310794830322,4.037563800811768,4.604251384735107,4.569831848144531,4.141808032989502,4.4533562660217285,4.653535842895508,4.054136753082275,4.630998134613037,3.962613344192505,1.5877009630203247,1.2727618217468262,1.4802215099334717,1.6104549169540405,1.4569437503814697,1.539625644683838,1.472213864326477,2.0593249797821045,1.3333063125610352,1.6195672750473022,0.646668553352356,1.6028765439987183,1.468073844909668,1.4336971044540405,1.469533920288086,1.5507453680038452,1.6590402126312256,1.5533472299575806,1.5326228141784668,1.4909952878952026,1.5933552980422974,0.6876698732376099,1.5868359804153442,1.573804259300232,1.5753282308578491,1.2330971956253052,1.6163599491119385,0.7645369172096252,1.5960463285446167,1.578489899635315,0.5172238349914551,1.5027610063552856,1.38727867603302,1.5584850311279297,1.571655035018921,1.5743860006332397,1.5758932828903198,1.8300682306289673,1.7321003675460815,1.5929148197174072,1.4410037994384766,1.594342589378357,1.557225227355957,1.3510727882385254,1.2829564809799194,1.1802170276641846,1.5979278087615967,1.6627514362335205,1.6593539714813232,1.5607953071594238,1.3650426864624023,0.5767338275909424,1.5249618291854858,1.5589072704315186,1.6015998125076294,1.5737286806106567,1.52560555934906,1.5944088697433472,2.3929059505462646,2.3886959552764893,2.2481915950775146,2.366041660308838,2.3146793842315674,2.3162174224853516,2.369158983230591,2.5022075176239014,2.4162755012512207,2.523205280303955,2.5030629634857178,2.322309732437134,2.426605224609375,2.189079761505127,2.5072808265686035,2.1927618980407715,2.2802486419677734,2.404411792755127,2.5210015773773193,2.5109353065490723,2.188095808029175,2.2342336177825928,1.3085989952087402,2.270885467529297,2.520770311355591,2.4987809658050537,2.2820372581481934,2.2575674057006836,2.305651903152466,2.480987310409546,2.6763017177581787,2.3891241550445557,2.503016233444214,2.507106065750122,2.5494508743286133,2.4986634254455566,2.2242064476013184,2.3021676540374756,2.150664806365967,2.744899034500122,2.3470451831817627,2.496939182281494,2.4149718284606934,2.483858346939087,2.3855793476104736,2.2803268432617188,2.255380868911743,2.259631872177124,2.3189663887023926,2.3722052574157715,2.399129867553711,2.2720420360565186,2.2237091064453125,2.365219831466675,2.2338740825653076,2.3938751220703125,3.968130350112915,4.086702346801758,3.7755074501037598,3.991488218307495,3.975116014480591,4.029293060302734,4.046248435974121,4.013284683227539,4.176393032073975,4.156956672668457,4.055082321166992,3.9321210384368896,3.937089204788208,3.9232289791107178,3.8601222038269043,3.866779327392578,3.9942166805267334,3.850517511367798,3.827030897140503,4.08809232711792,3.6295218467712402,4.113508701324463,4.187417507171631,3.6063523292541504,3.9586288928985596,4.24702787399292,3.9166204929351807,4.1035475730896,3.9102394580841064,4.182351112365723,3.869297981262207,3.981309175491333,3.9789490699768066,3.9490838050842285,3.944530487060547,3.942408323287964,4.038426399230957,4.008914947509766,4.139644622802734,3.7226343154907227,4.209354877471924,4.045825958251953,3.9882144927978516,4.28642463684082,4.0150346755981445,3.615358352661133,4.036133289337158,4.028831958770752,3.9747371673583984,3.9180045127868652,3.7974116802215576,4.060174942016602,4.20427942276001,4.09943962097168,-0.5178394317626953,-0.650516152381897,-0.479851096868515,-0.4310290515422821,-0.5378267168998718,-0.6259913444519043,-0.7380387187004089,-0.21319614350795746,-0.7479232549667358,-0.610841691493988,-0.6754353642463684,-0.5430178642272949,-0.6768491268157959,-0.45748335123062134,-0.3581988215446472,-0.8913076519966125,-0.698038637638092,-0.7323350310325623,-0.4265297055244446,-0.6116015911102295,-0.9226518869400024,-0.7730879783630371,-0.7615750432014465,-0.7331415414810181,-0.8823621869087219,-0.8506748676300049,-0.7885114550590515,-0.723310649394989,-0.7920389175415039,-0.6305829882621765,-0.698114275932312,-0.7590868473052979,-0.9453780651092529,-0.47471097111701965,-0.7153941988945007,-0.7514808177947998,-0.32000821828842163,-0.5421023368835449,-0.5268405079841614,-0.7256784439086914,-0.6647657752037048,-0.5601599812507629,-0.8130180239677429,-0.5797785520553589,-0.76743084192276,-0.740344226360321,-0.6408556699752808,-0.4104425609111786,-0.3353818655014038,-0.6775698661804199,-0.7086683511734009,-0.9370354413986206,-0.7324972152709961,3.193037986755371,3.0153839588165283,3.2098007202148438,3.411353826522827,3.419900894165039,2.993455171585083,3.2046985626220703,3.197061538696289,3.5652174949645996,3.1496386528015137,3.4359893798828125,3.163926362991333,3.4493963718414307,3.234320640563965,3.2002310752868652,3.272998094558716,3.3818790912628174,3.3397650718688965,3.0758256912231445,3.1623947620391846,3.384615182876587,3.2408924102783203,3.358213186264038,3.416884660720825,3.167921781539917,3.4248197078704834,3.2836413383483887,3.2514102458953857,3.342782497406006,3.3918700218200684,3.194164752960205,3.047151803970337,3.55344820022583,3.2842931747436523,3.3557095527648926,3.1627235412597656,1.072845220565796,null,3.3806440830230713,3.417919874191284,3.0919387340545654,3.379578113555908,3.3994202613830566,3.3199284076690674,1.930212140083313,1.9589751958847046,1.9856090545654297,2.118469476699829,1.8433870077133179,2.083873748779297,1.9659463167190552,1.6238290071487427,1.9088102579116821,1.984571933746338,1.8491621017456055,1.8348524570465088,1.9095227718353271,2.1351234912872314,2.0434975624084473,1.9872238636016846,1.9833941459655762,1.972872018814087,1.8849225044250488,1.9644986391067505,1.6182628870010376,2.087661027908325,1.9592905044555664,1.9400484561920166,1.8442121744155884,2.052821636199951,2.1948325634002686,1.9715791940689087,1.9485830068588257,1.9467763900756836,2.033796548843384,2.05224347114563,-1.7047370672225952,-1.7189042568206787,-1.7727406024932861,-1.7914379835128784,-1.7604166269302368,-1.5030242204666138,-1.8196848630905151,-1.7163331508636475,-1.6494393348693848,-1.6843397617340088,-1.7724716663360596,-1.7692655324935913,-1.7378288507461548,-1.7441824674606323,-1.464735746383667,-1.7443996667861938,-1.7241114377975464,-1.6847602128982544,0.36447063088417053,-1.677884578704834,-1.6699968576431274,-1.7729989290237427,-1.7663508653640747,-1.7174915075302124,-1.603288173675537,-1.7217741012573242,-1.7533800601959229,-1.768631100654602,-1.7753701210021973,-2.003943920135498,-2.018934726715088,-2.0210397243499756,-1.9936052560806274,-1.9008338451385498,-1.989972710609436,-2.0100209712982178,-1.9968105554580688,-2.0147523880004883,-2.0181238651275635,-2.0108845233917236,-1.9405567646026611,-1.9686578512191772,-1.9617516994476318,-2.0073821544647217,-1.730658769607544,-1.9098670482635498,-1.9983524084091187,-2.0032131671905518,-2.021005392074585,-1.78288996219635,-1.9908539056777954,-1.9123069047927856,-1.9989206790924072,-1.8872182369232178,-2.0079305171966553,-1.9907784461975098,-1.9225029945373535,-2.0194551944732666,0.026861172169446945,-0.11374158412218094,-0.0861361101269722,-0.08715442568063736,0.14325851202011108,-0.09389468282461166,-0.06494750827550888,-0.21590127050876617,-0.029119817540049553,0.18973292410373688,-0.05029120296239853,0.2098502218723297,-0.02140832133591175],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"#CFD8DC\",\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"other\",\"showlegend\":false,\"x\":[12.260786056518555,10.01115894317627,11.924004554748535,10.16279411315918,12.00646686553955,10.273680686950684,6.795833110809326,10.738099098205566,12.415397644042969,12.344215393066406,8.005266189575195,11.672365188598633,9.678071022033691,11.613967895507812,10.691057205200195,10.729043006896973,12.570730209350586,12.728907585144043,11.094691276550293,12.467524528503418,10.7769136428833,11.365942001342773,11.85193157196045,10.574968338012695,13.300118446350098,8.84424877166748,9.322251319885254,11.329916000366211,9.31579303741455,10.838811874389648,11.459781646728516,11.970154762268066,12.745402336120605,10.581709861755371,9.499332427978516,10.223129272460938,9.936136245727539,11.424376487731934,7.634429931640625,12.565068244934082,12.609332084655762,11.734045028686523,12.76197338104248,8.449498176574707,10.840675354003906,11.946686744689941,8.837322235107422,9.319323539733887,12.509225845336914,10.419879913330078,6.7955498695373535,9.324790954589844,12.908655166625977,11.152015686035156,11.986949920654297,12.218483924865723,11.286534309387207,11.993667602539062,9.96347427368164,11.670063018798828,9.893813133239746,10.930827140808105,9.840456008911133,9.265555381774902,8.651935577392578,9.824747085571289,9.816277503967285,12.087455749511719,11.567899703979492,6.796971321105957,12.668866157531738,12.218778610229492,10.99422836303711,11.2903470993042,13.343911170959473,12.024370193481445,12.494938850402832,12.23508071899414,10.537565231323242,9.071687698364258,9.39167308807373,13.654808044433594,11.47091293334961,11.624275207519531,12.246249198913574,12.476778984069824,11.365124702453613,12.479470252990723,12.073331832885742,10.643670082092285,10.698538780212402,11.801766395568848,11.808372497558594,9.954280853271484,12.55838394165039,8.872209548950195,11.538613319396973,8.866655349731445,10.459000587463379,11.395777702331543,9.855978012084961,11.591615676879883,10.30197811126709,11.654211044311523,11.18188190460205,10.506263732910156,13.050704002380371,11.941532135009766,12.512845993041992,11.652451515197754,13.646256446838379,11.867741584777832,12.657735824584961,12.094124794006348,10.286686897277832,12.581634521484375,11.336400985717773,9.937158584594727,11.021777153015137,9.948188781738281,12.35839557647705,12.886040687561035,9.261913299560547,9.89763355255127,9.462775230407715,9.968618392944336,12.362512588500977,10.358255386352539,8.327774047851562,11.927535057067871,10.00265884399414,12.349808692932129,10.623787879943848,10.678670883178711,11.847443580627441,9.692587852478027,9.262121200561523,10.958932876586914,12.889772415161133,10.948387145996094,11.634035110473633,12.080227851867676,10.378564834594727,10.096247673034668,11.42621898651123,8.7933931350708,11.231094360351562,8.999340057373047,11.527088165283203,12.19135570526123,7.8714375495910645,11.001914978027344,12.61523151397705,9.75306224822998,11.896245002746582,12.523083686828613,9.378335952758789,10.35689926147461,11.08590316772461,13.147154808044434,9.818265914916992,11.11213493347168,11.444499969482422,11.590463638305664,10.737903594970703,9.770731925964355,10.78557014465332,11.933521270751953,10.511313438415527,9.82026481628418,8.356404304504395,10.73415470123291,10.45881462097168,10.396477699279785,12.194050788879395,12.752180099487305,11.78748607635498,10.255226135253906,12.165645599365234,12.292634963989258,12.844697952270508,10.628177642822266,8.569395065307617,11.763611793518066,12.15890884399414,12.740022659301758,12.746966361999512,9.62110424041748,11.45886516571045,10.926743507385254,12.772931098937988,12.837591171264648,12.200713157653809,9.013678550720215,10.967081069946289,9.76983642578125,12.884422302246094,11.670434951782227,13.589189529418945,12.576118469238281,11.409375190734863,9.541470527648926,8.66622257232666,11.438347816467285,11.34601879119873,10.590363502502441,11.35364818572998,11.930695533752441,6.797292709350586,10.794591903686523,10.309884071350098,10.247868537902832,9.914916038513184,13.303763389587402,12.540189743041992,11.354313850402832,8.383038520812988,12.193739891052246,9.227989196777344,9.241358757019043,9.536521911621094,10.984012603759766,12.313669204711914,9.644474029541016,9.546388626098633,9.345878601074219,11.460395812988281,10.699294090270996,9.629450798034668,8.48170280456543,11.0349760055542,10.570517539978027,10.367918014526367,11.483748435974121,10.360127449035645,10.180638313293457,12.789478302001953,10.665085792541504,11.708563804626465,12.97873592376709,11.337641716003418,10.709122657775879,13.167486190795898,12.409531593322754,8.32143497467041,9.063648223876953,10.371134757995605,11.557292938232422,10.586905479431152,9.22173023223877,11.61303424835205,8.834911346435547,13.149587631225586,11.683677673339844,9.396690368652344,11.555591583251953,11.163105964660645,10.546306610107422,13.552511215209961,12.85168170928955,11.200281143188477,9.939804077148438,10.729233741760254,12.406366348266602,11.353873252868652,11.169713973999023,9.460506439208984,10.654193878173828,11.77177906036377,11.467199325561523,12.088428497314453,13.498458862304688,11.279095649719238,12.567978858947754,10.763456344604492,10.033524513244629,12.673567771911621,10.10020637512207,8.970285415649414,9.142569541931152,11.128690719604492,12.259600639343262,8.503023147583008,11.229401588439941,9.610413551330566,10.481382369995117,12.30782699584961,11.052600860595703,13.069833755493164,9.056635856628418,9.835722923278809,11.540289878845215,10.57509994506836,10.630483627319336,12.53976058959961,12.878406524658203,12.195188522338867,12.416590690612793,8.807579040527344,12.123668670654297,10.993809700012207,11.357285499572754,11.609417915344238,8.505378723144531,9.010087966918945,9.596749305725098,12.824377059936523,10.293586730957031,11.473138809204102,12.474746704101562,10.027557373046875,12.210655212402344,10.91020393371582,13.3267240524292,13.175515174865723,11.291525840759277,11.628263473510742,11.242234230041504,9.939929008483887,12.68603515625,9.105117797851562,12.55679702758789,10.582772254943848,11.557234764099121,12.54797649383545,11.068909645080566,12.6818208694458,11.577909469604492,11.18771743774414,13.261770248413086,11.224922180175781,11.866551399230957,9.587552070617676,10.551297187805176,11.903278350830078,12.07540225982666,10.768095970153809,12.016518592834473,8.86601448059082,12.697779655456543,12.205658912658691,10.315733909606934,11.997912406921387,12.761860847473145,11.485032081604004,12.877477645874023,10.77319622039795,9.71633529663086,12.789429664611816,10.579119682312012,12.169694900512695,11.369839668273926,12.047872543334961,11.594593048095703,11.944458961486816,11.05200481414795,9.081618309020996,12.835399627685547,12.314804077148438,9.093341827392578,11.577603340148926,9.215981483459473,11.374435424804688,12.266894340515137,11.160942077636719,8.430356979370117,12.54307746887207,11.239245414733887,10.539368629455566,11.465906143188477,7.657243728637695,12.263589859008789,9.044021606445312,11.270294189453125,9.69029426574707,12.90316104888916,11.74852466583252,8.697430610656738,10.244534492492676,13.194223403930664,8.26854419708252,12.263144493103027,12.15650463104248,10.97741985321045,11.557283401489258,12.207938194274902,9.880388259887695,13.715070724487305,8.669084548950195,11.517677307128906,12.063190460205078,10.891190528869629,8.157964706420898,13.416830062866211,12.172284126281738,9.77486515045166,12.416096687316895,8.317665100097656,12.529857635498047,10.84004020690918,8.873795509338379,12.759370803833008,10.572245597839355,11.383156776428223,11.408194541931152,12.709611892700195,13.176079750061035,7.645970344543457,10.45201587677002,11.888107299804688,8.155491828918457,10.934144973754883,12.047857284545898,11.55852222442627,10.887688636779785,10.773085594177246,10.705704689025879,9.601393699645996,10.455825805664062,9.596738815307617,10.703749656677246,11.308526992797852,9.27015209197998,10.01099681854248,12.847784996032715,12.372241020202637,10.8928804397583,11.546853065490723,9.608847618103027,10.049355506896973,10.234009742736816,12.77387809753418,7.639250755310059,9.369508743286133,11.111124992370605,10.788805961608887,9.64129638671875,10.971961975097656,13.150918960571289,10.595973014831543,9.54521369934082,12.856410026550293,10.756369590759277,11.570320129394531,10.633820533752441,10.61214542388916,11.196186065673828,11.974361419677734,11.977964401245117,9.437406539916992,11.594237327575684,11.464212417602539,10.141727447509766,11.141068458557129,9.565552711486816,12.504912376403809,10.439518928527832,8.916755676269531,12.542708396911621,8.100579261779785,13.614336013793945,9.942835807800293,12.88668441772461,12.289319038391113,10.191901206970215,11.653940200805664,12.273872375488281,9.7015380859375,11.526108741760254,11.894497871398926,9.942673683166504,12.299449920654297,11.311140060424805,12.881195068359375,8.470643043518066,12.338445663452148,11.502923011779785,10.742378234863281,12.947842597961426,11.659990310668945,12.496929168701172,8.366921424865723,9.1043119430542,12.329166412353516,11.74715518951416,11.585480690002441,10.612318992614746,11.377769470214844,11.649738311767578,11.907842636108398,13.692667007446289,11.34955883026123,12.52751636505127,8.80586051940918,12.406229019165039,12.7645902633667,12.445311546325684,11.75776195526123,11.090094566345215,11.42638111114502,10.89482593536377,10.761418342590332,11.150059700012207,10.034558296203613,10.338700294494629,11.178735733032227,12.835885047912598,11.53531551361084,12.381514549255371,11.54314136505127,11.90941047668457,9.358724594116211,10.740930557250977,11.939253807067871,11.149989128112793,11.659534454345703,10.353327751159668,11.331498146057129,10.64175796508789,12.81151008605957,8.662858009338379,10.857192993164062,10.787764549255371,8.953280448913574,12.585387229919434,7.643540382385254,10.447787284851074,10.80845832824707,12.5751371383667,9.113858222961426,9.70373821258545,12.737845420837402,11.416006088256836,12.489952087402344,8.133481979370117,10.278565406799316,11.977149963378906,9.470913887023926,10.66490364074707,10.00718879699707,9.176525115966797,11.815064430236816,8.752781867980957,11.483132362365723,11.89669418334961,12.553421020507812,13.118978500366211,9.782742500305176,10.74406623840332,12.168991088867188,11.756896018981934,12.940293312072754,11.56460189819336,8.64371109008789,11.702585220336914,11.278419494628906,10.558026313781738,11.820279121398926,10.459695816040039,11.502100944519043,11.530735969543457,10.50390911102295,12.36990737915039,12.139140129089355,11.131754875183105,10.48403263092041,13.041622161865234,11.032854080200195,11.734776496887207,10.479324340820312,9.945302963256836,11.825505256652832,11.676788330078125,9.411858558654785,12.318131446838379,11.462925910949707,12.87320613861084,13.661906242370605,12.474398612976074,10.583564758300781,11.804007530212402,11.14046573638916,11.841476440429688,11.781903266906738,9.183506965637207,11.8534574508667,11.49117660522461,11.63294506072998,11.224637985229492,12.764777183532715,12.330574035644531,11.128299713134766,12.93774127960205,10.32089614868164,12.478330612182617,11.827890396118164,11.804841041564941,11.44624137878418,11.820039749145508,10.238183975219727,7.784416675567627,12.978934288024902,12.451269149780273,9.322071075439453,11.821974754333496,10.541068077087402,11.961499214172363,8.238553047180176,12.170052528381348,12.736010551452637,12.764181137084961,11.65296745300293,12.675060272216797,11.974373817443848,10.117522239685059,9.6543607711792,12.757356643676758,12.056167602539062,9.412654876708984,11.711962699890137,9.174324989318848,10.823249816894531,9.90158462524414,10.913339614868164,8.113393783569336,9.993379592895508,12.194329261779785,13.33237361907959,11.182921409606934,12.732467651367188,13.046422004699707,9.75806713104248,8.685029029846191,11.561057090759277,10.053182601928711,11.998501777648926,13.20248031616211,12.104700088500977,10.906420707702637,11.117636680603027,10.446064949035645,9.715156555175781,12.530933380126953,12.754565238952637,11.461576461791992,11.491990089416504,9.396021842956543,8.587562561035156,12.263270378112793,11.42301082611084,13.491022109985352,10.364217758178711,11.991475105285645,12.44273853302002,11.70559310913086,10.35807991027832,9.996321678161621,9.326005935668945,12.807846069335938,13.570623397827148,11.12037467956543,10.716169357299805,10.50700569152832,10.22071361541748,12.695380210876465,11.297102928161621,9.085335731506348,12.861201286315918,11.170536994934082,10.041293144226074,10.805912017822266,11.3994722366333,9.783839225769043,11.411025047302246,10.608845710754395,9.42233657836914,11.715644836425781,11.849699020385742,12.349161148071289,8.975476264953613,10.749741554260254,9.420133590698242,10.120325088500977,12.375459671020508,10.686391830444336,11.969694137573242,11.85922622680664,12.443221092224121,9.37954330444336,13.088716506958008,10.281312942504883,12.127240180969238,11.985560417175293,12.442952156066895,12.119172096252441,9.390953063964844,10.271842956542969,10.183282852172852,12.758428573608398,9.88254451751709,11.704732894897461,11.48423957824707,9.189234733581543,11.172453880310059,11.48064136505127,10.315546035766602,12.061239242553711,11.559906005859375,11.86010456085205,11.664885520935059,12.843629837036133,9.624467849731445,12.458562850952148,12.246566772460938,11.838500022888184,10.999716758728027,9.12990951538086,11.840607643127441,11.96013069152832,11.915284156799316,12.08629035949707,10.903203964233398,8.849540710449219,11.167780876159668,10.531004905700684,12.467206954956055,10.145415306091309,9.549249649047852,6.795407295227051,12.180326461791992,12.817878723144531,10.757889747619629,11.568016052246094,11.672552108764648,7.97341775894165,10.022518157958984,9.927380561828613,10.70942211151123,11.941184043884277,12.794625282287598,12.879822731018066,12.178878784179688,11.878340721130371,10.52446460723877,10.071585655212402,10.56197738647461,11.002744674682617,12.245721817016602,10.67922592163086,12.140796661376953,12.277287483215332,11.456153869628906,12.081787109375,11.62030029296875,10.880305290222168,10.69717025756836,9.24069881439209,12.846972465515137,10.142430305480957,12.146927833557129,9.958215713500977,9.50244140625,10.51760196685791,10.551128387451172,11.390852928161621,11.688502311706543,11.491974830627441,12.741220474243164,12.295635223388672,12.218851089477539,8.685344696044922,13.047622680664062,11.958925247192383,9.383042335510254,11.218199729919434,11.51757526397705,9.624102592468262,13.13804817199707,12.326669692993164,12.624595642089844,12.161945343017578,9.43790054321289,11.324953079223633,8.503750801086426,11.257233619689941,10.58021354675293,9.62783145904541,9.703179359436035,11.924786567687988,8.906972885131836,9.254066467285156,12.899577140808105,11.277064323425293,11.152223587036133,12.76430606842041,10.148778915405273,12.378602027893066,11.560495376586914,12.818056106567383,12.91047191619873,8.343107223510742,10.998586654663086,11.666386604309082,10.141661643981934,11.702606201171875,10.432024955749512,10.108580589294434,9.184564590454102,13.428140640258789,11.512848854064941,12.044175148010254,11.576578140258789,12.28610610961914,11.385955810546875,12.952836036682129,12.021284103393555,11.287923812866211,12.099640846252441,12.082901000976562,12.028682708740234,12.383368492126465,8.773681640625,12.879551887512207,11.319714546203613,8.952622413635254,11.267468452453613,12.477086067199707,12.503398895263672,9.42236614227295,11.20785140991211,11.332497596740723,8.951685905456543,10.856745719909668,10.309080123901367,11.73598861694336,8.723488807678223,12.038217544555664,9.878068923950195,12.995759963989258,10.974395751953125,9.64431381225586,10.158196449279785,13.631218910217285,8.542502403259277,11.165329933166504,10.789525032043457,13.625770568847656,11.960083961486816,12.425254821777344,11.940552711486816,13.470690727233887,13.038926124572754,7.834033012390137,9.87307071685791,11.216033935546875,12.414644241333008,12.093451499938965,11.877202987670898,12.003032684326172,10.026086807250977,12.491192817687988,13.477025032043457,12.77425479888916,10.46001148223877,8.880654335021973,9.948962211608887,10.48256778717041,12.090274810791016,11.583839416503906,11.40567684173584,6.7874345779418945,9.088058471679688,11.758285522460938,11.31846809387207,11.73054313659668,12.907270431518555,12.770581245422363,11.114065170288086,11.227639198303223,10.648099899291992,12.066269874572754,13.3859224319458,12.3936767578125,11.536118507385254,12.09136962890625,7.971031665802002,10.897802352905273,9.173691749572754,11.609256744384766,12.269068717956543,9.517958641052246,11.856656074523926,11.710532188415527,12.882142066955566,12.293147087097168,9.226627349853516,11.812376022338867,9.573324203491211,11.98694896697998,11.025315284729004,12.620606422424316,12.66535758972168,11.626893997192383,11.87524127960205,9.740761756896973,11.66956615447998,9.677865982055664,13.708874702453613,13.700621604919434,10.8056058883667,12.719548225402832,11.30461597442627,9.15710163116455,11.5343599319458,9.219347953796387,11.886124610900879,11.679952621459961,12.069474220275879,12.361296653747559,10.542925834655762,10.790267944335938,12.395968437194824,10.26074504852295,12.145330429077148,11.74125862121582,12.40096664428711,10.930389404296875,8.466429710388184,11.920580863952637,12.118764877319336,13.445467948913574,10.998306274414062,11.986994743347168,12.903426170349121,11.159873962402344,12.18824291229248,9.372452735900879,12.712847709655762,13.078712463378906,10.631550788879395,9.169002532958984,8.790238380432129,11.811575889587402,12.28490924835205,9.769627571105957,12.567846298217773,10.349342346191406,11.191412925720215,11.537104606628418,10.127388000488281,10.513239860534668,8.915346145629883,11.823579788208008,11.909771919250488,11.520251274108887,13.776591300964355,12.594861030578613,6.803589344024658,10.791962623596191,11.956903457641602,7.651430606842041,12.864334106445312,11.632024765014648,10.91291332244873,11.907623291015625,10.763083457946777,11.546504974365234,13.262833595275879,12.147619247436523,11.872856140136719,12.41122055053711,12.41154670715332,13.054023742675781,10.615890502929688,9.164365768432617],\"y\":[1.976219654083252,1.265775442123413,1.0926563739776611,-0.5182633996009827,2.4570963382720947,0.48775091767311096,-0.910475492477417,-0.5370233654975891,1.2164555788040161,1.3136889934539795,-0.12520691752433777,1.813036561012268,1.1669845581054688,1.774245023727417,-0.3897693157196045,1.3097110986709595,3.413337469100952,1.671027660369873,0.8366751074790955,3.2766458988189697,0.3775566816329956,0.44715550541877747,2.3352620601654053,3.230710506439209,2.2592904567718506,0.6792904138565063,0.25117701292037964,0.6122933626174927,1.182349681854248,0.9189866781234741,3.0709445476531982,0.9211485385894775,3.500269889831543,3.7989087104797363,1.823943853378296,1.6083993911743164,-0.23820368945598602,2.4324207305908203,0.19927099347114563,3.7869391441345215,2.054600715637207,0.9205240607261658,3.078073263168335,0.10503587871789932,0.8371968865394592,0.3198699951171875,-0.16870951652526855,2.7976632118225098,2.6678123474121094,-0.570141077041626,-0.9106079339981079,3.7905776500701904,2.7144033908843994,0.9463844299316406,1.3737200498580933,0.33007562160491943,-0.9850686192512512,1.1608271598815918,1.0532095432281494,2.060023546218872,1.5419471263885498,1.308414101600647,-1.1935114860534668,-0.17752167582511902,-0.4857664108276367,3.832371234893799,1.3047598600387573,3.552154541015625,1.942522644996643,-0.909134566783905,2.3067994117736816,0.9871791005134583,2.9792444705963135,2.830186128616333,0.7007690072059631,3.5236191749572754,3.808678388595581,0.5856729745864868,0.5755625367164612,-0.488295316696167,0.6351603269577026,0.5080202221870422,1.0401160717010498,2.3617639541625977,1.8510946035385132,0.9317828416824341,3.2759201526641846,2.4336321353912354,-1.299113392829895,3.8617777824401855,3.202488422393799,1.037284016609192,-0.5317288637161255,-0.5715892314910889,1.2100167274475098,1.357236623764038,3.533149480819702,3.0465500354766846,0.1627165526151657,0.7375492453575134,0.9511770009994507,0.07118048518896103,-0.3919447958469391,1.378462314605713,-0.3244394361972809,0.7082329988479614,2.143453359603882,1.1407673358917236,0.9031526446342468,2.8212311267852783,1.1145505905151367,1.4429399967193604,2.296320676803589,-0.001279126270674169,-0.10712704062461853,3.0526630878448486,0.7859264612197876,-0.6041418313980103,1.40556800365448,-0.5446928143501282,1.2216320037841797,3.4056971073150635,3.86862850189209,1.1494468450546265,0.1541697382926941,-0.2646180987358093,2.934096097946167,1.085005283355713,0.1298372447490692,1.4595853090286255,0.5229455232620239,1.7139097452163696,3.179950475692749,0.8122704029083252,2.313836097717285,0.9151896834373474,0.0694771260023117,2.251661777496338,2.7393033504486084,1.8044404983520508,2.4212090969085693,1.369358777999878,1.4411309957504272,1.778269648551941,1.9663785696029663,-0.2085247039794922,1.8884222507476807,0.01605393923819065,0.9991214275360107,2.1229145526885986,-0.17486093938350677,1.2444441318511963,1.5270284414291382,1.034622311592102,0.5874497890472412,1.0864499807357788,0.7937063574790955,0.2653231918811798,0.6648417711257935,2.1787331104278564,-1.4033302068710327,0.993794858455658,3.9591336250305176,2.2620389461517334,2.2779126167297363,1.026957631111145,1.6402356624603271,1.106472134590149,3.609027624130249,4.001310348510742,2.4320061206817627,1.3439748287200928,1.444767951965332,0.8551923036575317,0.15037234127521515,2.425750494003296,2.3774797916412354,2.089541435241699,0.14875546097755432,1.2632057666778564,1.3453816175460815,3.9026010036468506,0.47671282291412354,1.526449203491211,1.530004620552063,0.44732722640037537,1.5337772369384766,-0.5403087735176086,-1.0967426300048828,0.833954393863678,0.7645677924156189,2.70750093460083,1.2675151824951172,0.1613716185092926,3.286445379257202,0.9860133528709412,3.4668221473693848,1.4706974029541016,-0.06515169888734818,3.346012830734253,3.19698166847229,1.0387732982635498,-0.47503626346588135,1.1350661516189575,-0.12550556659698486,0.48462581634521484,3.611987352371216,0.44945693016052246,-0.9105373620986938,0.9302946925163269,2.0090830326080322,0.1268618106842041,-0.5888694524765015,1.5191290378570557,2.7231905460357666,3.63657283782959,0.20960384607315063,0.04279949516057968,2.0899455547332764,2.395681142807007,-0.3550689220428467,1.1873329877853394,1.6772342920303345,-0.551196813583374,1.0045689344406128,3.5376405715942383,-0.2728564739227295,1.6083232164382935,3.451906681060791,0.4097013771533966,0.28619876503944397,2.9995458126068115,1.430558204650879,2.9486751556396484,1.4180095195770264,3.394707679748535,-1.0172475576400757,1.6186503171920776,2.2325010299682617,3.3960022926330566,2.3339428901672363,2.508234739303589,1.917661428451538,2.007206678390503,0.7356185913085938,1.3998340368270874,1.426846981048584,-0.10650590807199478,3.196619987487793,1.5687898397445679,3.3810524940490723,-0.20458725094795227,1.947638988494873,-0.17957328259944916,0.6638663411140442,1.4990065097808838,1.0839821100234985,-0.13390646874904633,1.798355221748352,1.460438847541809,2.6365201473236084,2.2331058979034424,0.5038972496986389,1.672692894935608,3.618595838546753,2.4914488792419434,-0.7637554407119751,2.602285861968994,1.6152315139770508,2.3165502548217773,4.036905288696289,1.7626341581344604,-0.30745241045951843,1.3061516284942627,3.0544943809509277,2.2432503700256348,0.5260573625564575,0.26869526505470276,0.03409036993980408,-0.5223769545555115,0.39339277148246765,2.0766706466674805,-0.38802358508110046,3.801676034927368,2.3770365715026855,3.5236270427703857,3.0124449729919434,3.137105703353882,3.03592848777771,1.0055789947509766,2.3000667095184326,3.485448122024536,2.8649325370788574,3.2783849239349365,2.067582130432129,1.7887386083602905,1.117841124534607,1.9235390424728394,-0.5475263595581055,2.5567400455474854,-0.42588913440704346,1.6459540128707886,3.0523805618286133,3.548125743865967,0.9675278663635254,1.0511583089828491,1.8744837045669556,-0.2662609815597534,2.020230531692505,2.395968198776245,1.5245238542556763,2.208904504776001,-0.23743867874145508,2.1941235065460205,3.066039800643921,0.8780515789985657,-0.3361736238002777,0.12655974924564362,0.5816671252250671,2.5916388034820557,1.8386311531066895,0.9971460103988647,-0.09463711082935333,1.4201574325561523,3.414994478225708,-0.5812013745307922,1.0859824419021606,3.207399845123291,1.0073139667510986,1.059942603111267,0.9441660642623901,2.421081066131592,-0.9699038863182068,-0.7918293476104736,3.2947211265563965,3.453516721725464,2.6633572578430176,1.12492835521698,0.9515612125396729,1.8676871061325073,2.2264864444732666,0.4588735103607178,-0.7277568578720093,-1.0359772443771362,2.857710599899292,2.6983368396759033,1.5439820289611816,0.8810650110244751,3.6186933517456055,3.788325071334839,2.0987229347229004,3.1248533725738525,-0.6990334391593933,3.2795536518096924,0.4755188822746277,2.3451409339904785,1.9918140172958374,2.239830255508423,2.980741500854492,3.4494681358337402,2.7023935317993164,0.8716931343078613,1.143310546875,0.12861472368240356,1.2377668619155884,-0.3922814130783081,3.3432180881500244,1.7890307903289795,0.9444081783294678,2.8529300689697266,0.18657220900058746,2.6263394355773926,0.48836591839790344,0.770980954170227,0.9084759950637817,2.9118480682373047,0.899668276309967,0.3940073549747467,0.8528211116790771,1.6868844032287598,0.24871914088726044,2.4808976650238037,2.190305709838867,1.4015684127807617,-0.11070117354393005,1.4967073202133179,0.9283192753791809,0.4603424668312073,1.1601427793502808,1.9638222455978394,-1.3094513416290283,1.8755643367767334,-0.034235879778862,2.5533201694488525,2.5379700660705566,3.77651309967041,2.733030319213867,0.3953371047973633,2.7038984298706055,1.0902845859527588,3.95662522315979,3.4667394161224365,1.4680428504943848,-0.2102063000202179,1.9835692644119263,0.25430288910865784,2.2354414463043213,0.19437259435653687,1.457194447517395,1.478986144065857,0.024254370480775833,0.9681276082992554,-1.2682996988296509,1.4637945890426636,0.9524343013763428,3.2517035007476807,1.0663663148880005,1.047045350074768,2.5798704624176025,1.9295272827148438,-0.4900413751602173,1.2294294834136963,3.744098663330078,1.1958093643188477,3.4060301780700684,1.4086087942123413,3.195308208465576,-0.21053600311279297,1.9594486951828003,1.3294121026992798,0.7901973128318787,-1.0279371738433838,0.18727345764636993,2.7890584468841553,1.1563249826431274,-0.1735643744468689,2.2202253341674805,0.9077820777893066,2.668337821960449,2.603579521179199,0.24776341021060944,1.158795714378357,1.6285470724105835,1.96036958694458,3.842803716659546,-0.05351594090461731,1.0603022575378418,3.5160486698150635,3.203173875808716,-0.22633570432662964,1.7337943315505981,1.1724951267242432,0.17315110564231873,0.7105343341827393,2.4431610107421875,3.8104262351989746,0.10712434351444244,0.713238537311554,3.1322901248931885,3.2645468711853027,0.534716784954071,0.5656581521034241,3.5241639614105225,2.5362460613250732,-0.7327529191970825,-0.8334300518035889,2.0500335693359375,0.9878270626068115,2.0549156665802,3.280917167663574,0.543793261051178,0.40602004528045654,1.2652292251586914,1.6847025156021118,0.6613606810569763,0.48743921518325806,1.5848220586776733,0.15714702010154724,3.097687244415283,3.4039037227630615,3.8253872394561768,-0.3216516077518463,0.36484676599502563,3.5613582134246826,0.849107563495636,2.804208278656006,3.8222780227661133,1.8004229068756104,1.4634736776351929,3.1937408447265625,0.8675617575645447,3.476331949234009,0.3727441728115082,3.1206748485565186,1.5812299251556396,-0.6251404285430908,3.3036246299743652,3.7486493587493896,1.7302109003067017,2.3743913173675537,0.9696566462516785,0.4081820845603943,0.6557876467704773,2.6788806915283203,3.448852300643921,1.163936972618103,3.5292248725891113,2.8257150650024414,3.515986680984497,1.9399545192718506,0.4077995717525482,0.766963541507721,-0.4135100543498993,0.42341694235801697,2.8088972568511963,-0.6076659560203552,0.1522863209247589,3.2291979789733887,1.588698387145996,1.8325613737106323,0.9248793125152588,1.3020033836364746,3.420180559158325,1.643860101699829,1.1540601253509521,0.21492479741573334,0.1739095002412796,1.5416576862335205,3.289651393890381,-0.5035665035247803,-0.46198034286499023,3.0262765884399414,3.655320405960083,1.8784151077270508,0.3909582197666168,0.8705004453659058,1.1364428997039795,1.208755612373352,0.26025456190109253,0.44635772705078125,0.12183551490306854,-0.15554407238960266,-0.25469839572906494,-0.45932525396347046,0.3801633417606354,2.1852316856384277,3.0103812217712402,0.16040968894958496,0.17387855052947998,2.1033482551574707,3.5950064659118652,1.6760215759277344,3.2061989307403564,0.3962271809577942,2.2076709270477295,-0.8994259834289551,2.514460802078247,0.979424774646759,3.5055930614471436,0.26043587923049927,0.08888838440179825,0.9722239375114441,1.9090890884399414,1.4592763185501099,0.7288630604743958,0.5196076035499573,2.52851939201355,0.3626737594604492,0.9660953879356384,1.4570801258087158,1.5067415237426758,1.0723236799240112,1.9673024415969849,0.5451234579086304,2.6414546966552734,0.7439343333244324,3.4168541431427,1.108274221420288,0.9759180545806885,3.0499231815338135,0.3342808485031128,1.1762615442276,1.2191663980484009,1.9902498722076416,2.7178072929382324,2.1931447982788086,1.4139740467071533,3.4444408416748047,-0.2844769358634949,2.468306541442871,2.982534408569336,1.1627116203308105,2.345379114151001,-0.26387879252433777,2.15932035446167,-0.09391316771507263,2.315988540649414,3.9019370079040527,1.4993696212768555,0.8209245204925537,0.1551581770181656,2.0490360260009766,1.3861424922943115,1.0227948427200317,-0.12519274652004242,3.113816976547241,1.2089561223983765,-0.26088747382164,1.8862637281417847,3.144066333770752,-1.0393073558807373,2.0746145248413086,1.443308711051941,1.180871844291687,-0.6809756755828857,1.017892599105835,1.8907074928283691,-1.2943123579025269,0.1569855809211731,0.6024736166000366,0.8391635417938232,2.21470046043396,-1.1607123613357544,1.8066606521606445,2.297105312347412,2.370729446411133,1.7525073289871216,0.9918195605278015,0.6040194034576416,2.8629562854766846,3.4195144176483154,1.8813725709915161,3.7637126445770264,0.20570334792137146,2.110116958618164,1.3198965787887573,1.0308681726455688,1.5510575771331787,1.0269742012023926,3.243104934692383,0.5696408748626709,-1.0560033321380615,3.80950927734375,-0.9885093569755554,2.1285574436187744,0.27239152789115906,-0.9925388693809509,2.664095401763916,0.5445898771286011,3.4431591033935547,2.9532716274261475,1.4311859607696533,2.9324100017547607,3.054269313812256,0.8955436944961548,0.025119181722402573,3.7716782093048096,0.6967297196388245,2.6491522789001465,2.385422945022583,3.098644495010376,3.803661823272705,1.4175082445144653,2.0697543621063232,3.0386083126068115,3.8905580043792725,1.805167555809021,3.370250940322876,0.6452329754829407,1.198987364768982,4.435891628265381,3.9789042472839355,0.2224683314561844,0.784505307674408,3.900918483734131,-0.08793401718139648,-0.20969590544700623,0.9383320212364197,2.9824304580688477,0.4146740138530731,3.451068878173828,0.58146733045578,2.629016876220703,2.5896897315979004,0.8220140933990479,2.19604229927063,2.3683815002441406,1.0513523817062378,-0.5379165410995483,2.0229125022888184,2.577913522720337,1.3024059534072876,2.953939914703369,1.1526473760604858,-1.245646595954895,0.6586165428161621,1.9431781768798828,0.795759916305542,0.4541209042072296,1.3323132991790771,0.9036451578140259,1.0493559837341309,2.936338424682617,1.658136010169983,2.7779719829559326,0.217825248837471,-0.08116167783737183,0.8917245864868164,1.92644202709198,1.5383085012435913,3.03080153465271,-0.5870935916900635,1.1749986410140991,1.7433806657791138,3.2300631999969482,1.3536816835403442,1.4494167566299438,2.132977247238159,1.142106533050537,1.0964027643203735,-1.2843916416168213,2.0309720039367676,0.38181358575820923,0.4180848002433777,1.397927165031433,1.0205312967300415,-1.0413587093353271,-0.3866738975048065,-0.9114582538604736,2.2314321994781494,-0.9689415693283081,0.8953730463981628,-0.7223781943321228,3.3937878608703613,-0.1200912594795227,1.4780992269515991,-1.0540677309036255,1.243822693824768,1.1921714544296265,-0.9881787300109863,2.7142724990844727,1.035949468612671,1.7600966691970825,3.096761703491211,1.0765949487686157,-0.3055479824542999,0.8458605408668518,1.8884222507476807,0.7532628178596497,3.692111015319824,2.047429084777832,1.5398664474487305,2.6263246536254883,3.3768866062164307,1.3354458808898926,2.8829421997070312,3.073280096054077,2.618222951889038,2.2130420207977295,1.131805658340454,1.4583265781402588,-0.6291894912719727,3.3120579719543457,3.235522985458374,2.4912800788879395,0.8031163215637207,2.785531520843506,1.2172937393188477,2.5934207439422607,0.44533485174179077,3.7607927322387695,0.8943837881088257,2.2763075828552246,0.15780554711818695,2.040842056274414,0.27494242787361145,3.4471356868743896,2.545928955078125,3.3168694972991943,3.057359218597412,2.0106849670410156,3.7272539138793945,-0.2229020744562149,-0.3912656009197235,2.5650482177734375,0.5276594758033752,1.9719535112380981,3.524745225906372,1.5316689014434814,0.3282936215400696,2.1106743812561035,2.4451394081115723,1.7900829315185547,2.8499183654785156,2.917431354522705,-0.6730669736862183,2.2880237102508545,2.3319878578186035,0.7816553711891174,3.079967975616455,0.3783358633518219,3.771744728088379,3.4605367183685303,3.4098503589630127,2.232668399810791,2.7577970027923584,0.8026202917098999,0.8486886024475098,1.336401104927063,1.9623465538024902,2.0950241088867188,2.3696439266204834,2.694972276687622,0.7788678407669067,1.5454350709915161,0.642559289932251,1.9104548692703247,2.817577838897705,1.2944279909133911,2.2089552879333496,2.9942898750305176,0.6504705548286438,2.6843504905700684,1.2824846506118774,-0.555378794670105,1.9917773008346558,2.4853670597076416,2.2720730304718018,0.05783434212207794,1.741772174835205,3.9805495738983154,1.6283323764801025,3.9042468070983887,-0.08662469685077667,1.0114973783493042,-0.2651676535606384,2.276035785675049,-0.6100764274597168,2.0067296028137207,1.3208794593811035,1.0322285890579224,1.9896197319030762,2.3109211921691895,2.2940375804901123,0.7046573758125305,2.1666882038116455,0.8269322514533997,1.038419246673584,3.1033222675323486,2.011228084564209,0.4995725154876709,3.0119717121124268,1.7737351655960083,1.1829763650894165,3.768042802810669,2.453611135482788,-1.2905018329620361,0.4045698344707489,-0.7276555299758911,1.108733057975769,0.7043386101722717,1.7317860126495361,3.1328790187835693,1.5019123554229736,0.00757896201685071,1.7896612882614136,4.704597473144531,-1.2845878601074219,1.471731185913086,3.7090561389923096,-0.9188922643661499,0.25417712330818176,0.40908655524253845,-0.6070359349250793,3.279542922973633,2.3822507858276367,-1.0290439128875732,-1.0308316946029663,1.6611809730529785,0.5246748328208923,1.5900543928146362,1.5552963018417358,1.9574549198150635,0.3487606346607208,1.906638503074646,-0.15992236137390137,-0.1188686341047287,1.874509334564209,2.240257501602173,3.598365068435669,2.4335744380950928,3.23817777633667,1.554663896560669,2.292228937149048,1.6409223079681396,1.495506763458252,3.2188000679016113,3.7398290634155273,2.8911819458007812,3.769780397415161,2.451735019683838,2.5036814212799072,3.150447130203247,1.2291960716247559,0.9691900610923767,3.689431667327881,-0.07314413040876389,0.73487389087677,0.839801013469696,1.4524890184402466,2.1931369304656982,1.3259851932525635,3.0612916946411133,0.2924751043319702,0.6916547417640686,2.3885679244995117,0.7535249590873718,1.8109625577926636,3.2876040935516357,2.6223843097686768,2.415797472000122,2.450838565826416,1.9013489484786987,2.8400003910064697,3.637033224105835,1.2191064357757568,0.49888309836387634,0.527450680732727,3.3894202709198,2.8021371364593506,1.5861856937408447,-0.5647964477539062,1.773905634880066,2.8972055912017822,0.4014616310596466,3.552330493927002,3.5676872730255127,0.44267264008522034,2.05513334274292,2.6361255645751953,2.9887442588806152,0.289135605096817,3.268169641494751,3.1244053840637207,0.8573437333106995,-0.19672121107578278,-0.7705172300338745,1.6629849672317505,3.544947862625122,-0.6638178825378418,3.600919008255005,-0.5163041353225708,-0.025210803374648094,1.4350130558013916,1.8289581537246704,0.6889894008636475,1.6060850620269775,-0.9072462916374207,2.4202475547790527,1.693398356437683,0.15449632704257965,3.578507661819458,2.314469575881958,4.283806324005127,2.3266613483428955,0.48093342781066895,3.2087161540985107,-0.08493365347385406,1.7356622219085693,1.580642580986023,1.29006028175354,0.9935421347618103,2.0631296634674072,2.585738182067871,1.8831286430358887],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"26_argumentative_environmental_firms\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"26_argumentative_environmental_firms\"],\"x\":[9.245034217834473,9.240274429321289,9.255699157714844,9.306416511535645,9.330011367797852,9.5535249710083,9.301241874694824,9.279705047607422,9.539828300476074,9.543222427368164,9.282510757446289,9.352497100830078],\"y\":[2.6040866374969482,2.5882256031036377,2.6099462509155273,2.364428997039795,2.5036041736602783,2.5556437969207764,2.5966074466705322,2.450493335723877,2.5414693355560303,2.4918816089630127,2.483466386795044,2.526350498199463],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"51_transformer_reasoning_parameters_prom\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"51_transformer_reasoning\"],\"x\":[12.020341873168945,12.169936180114746,12.101693153381348,12.094861030578613,11.659080505371094,12.379535675048828,12.050992965698242,12.262616157531738,12.431180000305176,11.988661766052246,12.082118034362793,12.085043907165527,12.180397033691406,12.176328659057617,11.84795093536377,12.084325790405273,12.128952980041504,12.28459644317627,12.194994926452637,12.008187294006348,11.911856651306152,12.048596382141113,12.229466438293457,12.182258605957031,12.1466703414917,12.20826530456543,11.903266906738281,11.976790428161621,11.813048362731934,12.021183967590332,12.020978927612305,12.224409103393555,11.788299560546875,12.209701538085938,11.93454647064209,11.924176216125488,12.019671440124512,12.825284004211426,12.15970516204834,12.297412872314453,12.158596992492676,12.110843658447266,12.26720905303955,12.509387016296387,12.426708221435547,12.31076431274414,12.200416564941406,12.108731269836426,12.027480125427246,12.294480323791504,12.179040908813477,11.868775367736816,12.016388893127441,12.533409118652344,11.871016502380371,12.278585433959961,12.497551918029785,11.953156471252441,12.103267669677734,12.292784690856934,12.235295295715332,11.931829452514648,13.446449279785156,13.293713569641113,13.339015007019043,13.234209060668945,13.390213012695312,13.275945663452148,13.059819221496582,13.199735641479492,13.168375968933105,13.352516174316406,13.391827583312988,13.276883125305176,13.3883638381958,13.391206741333008,13.22536563873291,13.355441093444824,13.415063858032227,13.445826530456543,13.263446807861328,13.253372192382812,13.080596923828125,13.292510986328125,13.387595176696777,13.407983779907227,13.065677642822266,13.157443046569824,13.285446166992188,13.371879577636719,13.443427085876465,13.312077522277832,13.325447082519531,13.201516151428223,13.412653923034668,13.387580871582031,13.376065254211426,13.23760986328125,13.391357421875,13.28543758392334,13.155158996582031,13.134252548217773,13.264272689819336,13.326519012451172,13.441741943359375,13.180994987487793,13.229814529418945,13.385966300964355,13.26877498626709,13.252023696899414,9.99891185760498,9.666812896728516,9.87675666809082,9.759414672851562,9.77737045288086,9.712926864624023,9.860082626342773,10.150372505187988,9.757026672363281,9.756099700927734,9.88352108001709,9.667094230651855,9.834576606750488,9.662117958068848,9.90071964263916,9.823662757873535,9.885151863098145,9.897980690002441,9.599701881408691,9.828813552856445,10.166223526000977,9.664942741394043,9.68136215209961,9.909587860107422,9.613624572753906,9.683416366577148,9.677172660827637,9.895627975463867,9.831110954284668,9.791001319885254,9.888591766357422,10.030932426452637,10.003292083740234,11.668291091918945,11.893047332763672,11.399813652038574,11.399984359741211,11.689655303955078,11.275130271911621,11.221017837524414,12.198081016540527,12.027531623840332,11.27303695678711,11.897500038146973,11.873612403869629,11.806347846984863,11.46186637878418,11.398143768310547,11.580697059631348,11.322710990905762,11.420403480529785,11.564152717590332,11.334028244018555,11.482306480407715,11.22659969329834,11.433297157287598,11.450419425964355,11.489837646484375,11.502764701843262,11.421748161315918,11.442119598388672,12.117900848388672,11.278861999511719,9.238378524780273,9.30147647857666,9.232583999633789,9.182598114013672,9.238131523132324,9.218341827392578,9.234349250793457,9.302995681762695,9.232303619384766,9.124175071716309,9.416687965393066,9.293940544128418,9.349409103393555,9.29613208770752,9.22963809967041,9.208431243896484,9.365690231323242,9.037209510803223,9.345200538635254,9.246965408325195,9.236005783081055,9.268945693969727,11.956093788146973,12.490596771240234,12.429107666015625,12.604439735412598,12.537409782409668,12.447066307067871,12.514530181884766,12.494388580322266,12.444210052490234,12.318142890930176,12.335335731506348,12.499151229858398,12.503713607788086,12.477373123168945,12.43051815032959,12.483855247497559,12.453995704650879,12.551338195800781,12.408796310424805,12.414511680603027,10.386235237121582,10.229713439941406,10.389017105102539,10.301673889160156,10.361222267150879,10.3976469039917,10.311823844909668,10.32546329498291,10.373322486877441,10.308487892150879,10.342009544372559,10.355380058288574,10.305009841918945,10.051471710205078,10.35975456237793,11.601570129394531],\"y\":[3.4334287643432617,3.9167301654815674,3.949054002761841,3.438246250152588,3.445382595062256,3.36970853805542,3.7780611515045166,2.977111577987671,3.824798583984375,4.014473915100098,3.966614246368408,3.947669744491577,3.546800136566162,3.5740740299224854,3.722076654434204,3.7931604385375977,3.4850759506225586,3.1680963039398193,4.030865669250488,3.494990587234497,3.631016969680786,3.9821364879608154,3.874598264694214,3.628248453140259,3.362018585205078,3.6282668113708496,3.4427807331085205,3.539586305618286,3.9404172897338867,3.503599166870117,3.3990092277526855,3.441103219985962,2.9686286449432373,3.454322576522827,3.507044553756714,3.588791847229004,3.930246591567993,3.598909854888916,3.4998040199279785,3.598375082015991,3.547398805618286,3.815009355545044,3.1761443614959717,3.7004573345184326,2.9148776531219482,3.357351303100586,3.646010398864746,4.046837329864502,3.844369649887085,3.3957107067108154,3.5331528186798096,3.4589388370513916,3.6157045364379883,3.513286590576172,3.4719655513763428,3.176704168319702,3.3323616981506348,3.9445858001708984,3.7024219036102295,3.222160577774048,3.219435214996338,3.6485061645507812,2.849513053894043,2.6924357414245605,2.519382953643799,2.685671091079712,3.4131829738616943,3.3496320247650146,2.9062132835388184,2.5787570476531982,2.8339319229125977,2.6660850048065186,3.363393783569336,3.0692508220672607,3.403278112411499,2.94502854347229,2.862663984298706,2.849548578262329,3.311357021331787,2.857738494873047,2.829335927963257,2.7103164196014404,2.6873369216918945,3.1780202388763428,2.415661573410034,2.984997510910034,2.9960906505584717,3.0859627723693848,3.064985752105713,3.3739285469055176,2.99613094329834,3.3052146434783936,2.759016990661621,2.554168224334717,2.7810420989990234,3.370600700378418,2.8825161457061768,2.5941309928894043,3.402747631072998,3.1956849098205566,2.574057102203369,2.4850573539733887,3.1060783863067627,2.757913827896118,2.8156826496124268,2.6702122688293457,3.1264901161193848,3.3880455493927,2.8700947761535645,2.6230366230010986,-1.2806227207183838,-1.4134907722473145,-1.3984967470169067,-0.8591680526733398,-1.1667323112487793,-0.9986860752105713,-1.226072072982788,-1.2499401569366455,-1.3567789793014526,-1.3018178939819336,-1.279323697090149,-1.3926376104354858,-0.8900179862976074,-1.3667707443237305,-1.3356115818023682,-1.4169893264770508,-1.3518718481063843,-1.17157781124115,-1.1077362298965454,-1.3785203695297241,-1.242591381072998,-1.3728218078613281,-1.36669921875,-1.1680376529693604,-1.3303998708724976,-0.8987433314323425,-1.368369460105896,-1.1938246488571167,-1.3907575607299805,-1.302925705909729,-1.1836531162261963,-1.1196755170822144,-1.154442310333252,1.6213551759719849,1.7356750965118408,1.8189188241958618,2.011199474334717,1.6284370422363281,1.6911828517913818,1.434216856956482,1.9124113321304321,1.708951473236084,1.345273733139038,1.9241002798080444,1.706599235534668,1.5955106019973755,1.6075619459152222,1.8218833208084106,1.9240765571594238,1.5612646341323853,1.4347044229507446,1.8333743810653687,1.4203897714614868,1.508184552192688,1.6106964349746704,1.7237390279769897,1.6263391971588135,1.7565888166427612,1.8806627988815308,1.4971145391464233,1.4235330820083618,2.043459415435791,1.6557258367538452,0.11233166605234146,0.2616926431655884,0.13499541580677032,0.13999022543430328,0.13154223561286926,0.03455710411071777,0.1343674212694168,0.09137420356273651,0.0880853608250618,-0.03193724900484085,0.282012939453125,0.19786401093006134,0.27887430787086487,0.23954655230045319,0.1252308189868927,0.2048257291316986,0.21515387296676636,0.2964838743209839,0.28831443190574646,0.10827220976352692,0.010295077227056026,0.17448420822620392,-0.01690675877034664,-0.3107600808143616,-0.2527309060096741,-0.2602718472480774,-0.33801159262657166,-0.32423314452171326,-0.19620446860790253,-0.286507248878479,-0.2868269979953766,2.4987032413482666,-0.19626036286354065,-0.2882573902606964,2.8162295818328857,-0.27515092492103577,-0.28718024492263794,-0.29625701904296875,-0.3112342059612274,-0.32359012961387634,-0.29335451126098633,-0.24639558792114258,-0.7522066235542297,-0.7360655665397644,-0.7566789388656616,-0.7287562489509583,-0.7565518617630005,-0.6721810698509216,-0.7266584038734436,-0.7462540864944458,-0.6999889016151428,-0.7211228609085083,-0.7179433107376099,-0.7139708399772644,-0.7397772073745728,-0.6260511875152588,-0.7241697311401367,1.5872716903686523],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"52_dialogue_languages_dataset_language_e\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"52_dialogue_languages\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"x\":[13.87353229522705,13.208016395568848,14.106237411499023,13.092193603515625,13.336145401000977,13.386882781982422,12.319914817810059,13.021512985229492,13.98583698272705,12.727828025817871,14.158334732055664,12.96531867980957,13.214096069335938,13.292146682739258,13.683382034301758,13.187692642211914,13.279541969299316,12.989252090454102,12.971858978271484,12.976456642150879,13.035022735595703,12.499113082885742,13.1813325881958,12.979851722717285,13.296390533447266,13.039684295654297,13.842495918273926,13.132991790771484,13.9254732131958,12.167425155639648,14.098097801208496,13.920403480529785,12.558815002441406,12.849774360656738,12.970587730407715,14.02845287322998,12.911493301391602,13.765254974365234,13.905959129333496,13.326302528381348,13.942405700683594,12.925593376159668,13.41197395324707,13.25645637512207,12.991842269897461,13.592504501342773,13.934208869934082,13.955780029296875,13.298561096191406,13.263577461242676,12.345218658447266,13.947273254394531,13.009997367858887,13.643670082092285,13.180303573608398,13.74915599822998,12.808438301086426,13.863404273986816,12.991873741149902,13.162498474121094,12.859356880187988,13.075448036193848,13.156818389892578,12.886246681213379,13.997769355773926,13.03771686553955,12.784741401672363,12.894862174987793,12.97797966003418,13.60038948059082,14.051366806030273,12.930017471313477,12.712410926818848,12.782764434814453,12.85952377319336,12.852415084838867,13.387727737426758,13.939677238464355,12.471564292907715,13.912345886230469,12.641358375549316,13.367488861083984,13.8226318359375,13.64598560333252,13.757844924926758,12.249322891235352,13.732461929321289,13.127846717834473,12.968668937683105,13.813699722290039,13.807978630065918,13.008394241333008,13.787175178527832,13.993060111999512,13.531773567199707,13.984949111938477,13.01711654663086,14.002456665039062,13.822026252746582,13.829243659973145,13.329290390014648,12.908177375793457,14.121039390563965,13.951969146728516,12.813560485839844,13.843708038330078,13.839166641235352,14.121064186096191,12.558928489685059,13.817070960998535,13.30069637298584,13.859855651855469,13.09512710571289,12.266813278198242,12.803173065185547,13.087562561035156,13.733831405639648,13.884516716003418,12.79514217376709,13.156198501586914,12.635796546936035,13.271025657653809,12.98554801940918,13.871160507202148,13.031496047973633,14.019685745239258,13.770309448242188,13.913249969482422,13.995428085327148,12.395220756530762,13.254109382629395,12.519502639770508,13.016617774963379,13.239713668823242,13.700984954833984,13.32162857055664,13.05946159362793,12.602531433105469,12.626888275146484,13.298456192016602,13.010909080505371,12.473347663879395,13.30741024017334,12.52789306640625,13.24752140045166,14.047928810119629,12.949474334716797,13.951871871948242,14.054276466369629,12.840779304504395,12.930652618408203,13.249177932739258,14.00323486328125,12.957128524780273,13.71989631652832,12.736191749572754,13.225967407226562,11.134943962097168,11.025127410888672,11.245323181152344,11.604127883911133,11.156891822814941,11.352185249328613,11.527443885803223,11.282083511352539,11.126087188720703,11.73695182800293,11.573534965515137,13.375391006469727,11.404452323913574,11.218412399291992,10.824934959411621,11.162917137145996,11.561422348022461,11.366466522216797,10.656908988952637,11.300393104553223,11.272549629211426,11.278218269348145,11.19459056854248,11.2599458694458,11.612763404846191,10.9222412109375,11.204724311828613,11.096510887145996,11.001474380493164,11.493638038635254,11.407891273498535,11.244793891906738,11.168529510498047,11.017486572265625,11.451024055480957,11.223573684692383,11.187163352966309,11.271790504455566,11.26801872253418,11.24870491027832,11.188407897949219,11.215272903442383,11.429038047790527,11.241147994995117,11.28900146484375,11.684981346130371,11.643854141235352,11.422016143798828,11.644282341003418,11.211750030517578,11.794293403625488,10.666879653930664,11.676538467407227,11.204644203186035,11.4844388961792,11.210556983947754,10.822954177856445,11.602618217468262,11.172928810119629,11.261439323425293,11.18604850769043,11.24404525756836,11.392868995666504,11.273892402648926,11.069701194763184,11.81900691986084,11.298412322998047,11.697004318237305,11.151240348815918,11.614058494567871,11.220532417297363,11.269240379333496,11.204486846923828,11.25660228729248,11.144911766052246,11.228436470031738,11.389638900756836,11.027185440063477,11.341411590576172,11.200804710388184,10.692567825317383,11.169511795043945,11.02439022064209,10.92267894744873,11.365792274475098,11.304564476013184,11.123757362365723,11.317601203918457,11.2233304977417,11.252074241638184,10.953468322753906,11.1937837600708,11.191131591796875,11.507041931152344,13.65318489074707,13.939993858337402,13.685359001159668,13.784351348876953,13.504968643188477,13.94735336303711,13.143325805664062,13.834990501403809,13.44897747039795,13.599161148071289,13.199520111083984,13.714217185974121,13.943765640258789,13.477546691894531,13.838189125061035,13.14396858215332,13.451980590820312,13.139006614685059,13.305334091186523,13.572847366333008,13.578728675842285,13.640463829040527,13.756519317626953,13.478137016296387,13.640913963317871,13.371881484985352,13.30630874633789,13.778650283813477,13.48436164855957,13.48245620727539,13.242944717407227,13.188318252563477,13.671472549438477,13.161858558654785,13.182262420654297,13.67568302154541,13.578418731689453,13.574835777282715,13.530089378356934,12.509833335876465,13.550978660583496,13.269086837768555,13.929557800292969,13.697040557861328,13.825508117675781,13.642824172973633,13.51134204864502,13.453692436218262,13.557821273803711,13.553349494934082,13.648940086364746,13.369372367858887,13.12069034576416,13.566463470458984,13.692540168762207,13.704095840454102,13.338266372680664,13.631470680236816,13.964845657348633,13.344286918640137,13.934170722961426,13.56275463104248,13.731254577636719,13.44790267944336,13.115975379943848,13.600167274475098,8.303739547729492,13.527002334594727,13.690975189208984,13.423891067504883,13.938860893249512,13.628800392150879,13.415291786193848,13.883581161499023,13.465723037719727,13.598390579223633,13.958017349243164,13.456777572631836,13.795967102050781,9.905937194824219,13.677034378051758,13.943849563598633,13.411235809326172,13.518133163452148,13.331656455993652,8.079171180725098,8.06274700164795,7.97455358505249,7.848491191864014,8.032004356384277,7.964422702789307,7.964263916015625,8.134442329406738,8.114529609680176,7.841091632843018,7.8066725730896,8.066001892089844,7.995242118835449,8.329507827758789,8.135189056396484,7.784605979919434,7.775929927825928,8.180281639099121,8.422222137451172,7.919281005859375,7.885015487670898,7.895800590515137,7.771629810333252,8.177929878234863,7.973200798034668,8.28538703918457,7.766735553741455,7.8133392333984375,7.980544567108154,7.834801197052002,7.792785167694092,8.400049209594727,8.152592658996582,7.950541019439697,7.951925754547119,8.21005630493164,7.920266628265381,7.854425430297852,8.000518798828125,7.90179443359375,8.401914596557617,8.250751495361328,7.81003999710083,8.432668685913086,7.853431224822998,8.134767532348633,8.218738555908203,7.8007283210754395,8.225289344787598,7.825068473815918,8.072797775268555,8.098258972167969,8.405604362487793,7.813677787780762,7.821072101593018,7.985950946807861,7.810969352722168,8.3153657913208,8.099970817565918,7.857753276824951,8.26423168182373,7.91890811920166,7.952602386474609,7.912245273590088,7.974617958068848,7.784420490264893,8.138467788696289,8.036944389343262,7.761767864227295,7.890107154846191,8.475704193115234,8.049864768981934,7.910464286804199,7.735909938812256,8.021018028259277,7.9187726974487305,7.908551216125488,7.699703216552734,7.959645748138428,7.826615333557129,8.282672882080078,7.934024333953857,8.200965881347656,7.851012229919434,7.659298419952393,7.964986801147461,7.375672340393066,7.989088535308838,8.16100788116455,7.880589008331299,7.8358564376831055,7.8488359451293945,7.915194511413574,8.008590698242188,7.819448471069336,7.807568073272705,7.392760753631592,7.372259140014648,7.680557727813721,8.323735237121582,7.405981540679932,7.390983581542969,8.47547721862793,7.814692974090576,8.292101860046387,8.115535736083984,7.816876411437988,7.764798164367676,7.4307541847229,8.173230171203613,7.816125869750977,7.800354957580566,7.687350749969482,7.416838645935059,8.04630184173584,7.372389793395996,8.061315536499023,7.425755023956299,7.877260684967041,8.388016700744629,7.779290199279785,8.227629661560059,8.057212829589844,8.02259349822998,7.782587051391602,8.23399543762207,7.90402889251709,7.8760199546813965,7.4668121337890625,8.308961868286133,7.824159622192383,7.712260723114014,7.914191246032715,7.685945987701416,7.940428733825684,10.677026748657227,10.417326927185059,10.19378662109375,10.34382438659668,10.359889030456543,10.168207168579102,10.379969596862793,10.382912635803223,10.3951997756958,10.006451606750488,10.593812942504883,10.523648262023926,10.564718246459961,10.390353202819824,10.409890174865723,10.375975608825684,10.270461082458496,10.380226135253906,10.342330932617188,10.389986038208008,10.570111274719238,10.563359260559082,10.393143653869629,10.445023536682129,10.406803131103516,10.406514167785645,9.976093292236328,10.553617477416992,10.514777183532715,10.404207229614258,10.352548599243164,10.402541160583496,10.373939514160156,10.66649055480957,10.147748947143555,10.177634239196777,10.380538940429688,10.481019020080566,10.569191932678223,10.38936710357666,10.453319549560547,10.417245864868164,10.462665557861328,10.532641410827637,10.139423370361328,10.995478630065918,10.441753387451172,10.408876419067383,10.595641136169434,10.508939743041992,10.599928855895996,10.383362770080566,10.315134048461914,10.520706176757812,10.196410179138184,10.421682357788086,10.112748146057129,10.42435073852539,9.957427978515625,9.407848358154297,9.24934196472168,9.988895416259766,9.616469383239746,9.254940032958984,9.619887351989746,9.763313293457031,9.41704273223877,9.903557777404785,9.640640258789062,9.904218673706055,9.833464622497559,9.501309394836426,10.281272888183594,9.5150146484375,9.301063537597656,9.468867301940918,9.583669662475586,9.652227401733398,9.2976655960083,9.610597610473633,9.950066566467285,9.628329277038574,9.354300498962402,9.685493469238281,9.437472343444824,9.71446418762207,9.970356941223145,9.591911315917969,9.4934720993042,11.245697975158691,9.630011558532715,9.857994079589844,9.644820213317871,9.684257507324219,9.61834716796875,9.774521827697754,9.085481643676758,10.036049842834473,9.738116264343262,9.506299018859863,9.30492877960205,9.699036598205566,9.771791458129883,9.192770957946777,9.514463424682617,9.63692855834961,9.843480110168457,9.38808536529541,9.520946502685547,9.217379570007324,9.838109970092773,9.648056030273438,9.77692699432373,9.458232879638672,9.670414924621582,9.680063247680664,9.057123184204102,9.02020263671875,8.764460563659668,9.149687767028809,8.96601676940918,8.83845329284668,7.854395389556885,8.691113471984863,8.074891090393066,7.991252422332764,8.15771198272705,8.11121940612793,7.889158725738525,8.559086799621582,7.732449054718018,8.129582405090332,7.739405155181885,8.906782150268555,8.779287338256836,8.384539604187012,8.096807479858398,7.73988151550293,7.74143123626709,10.807848930358887,8.938881874084473,8.25653076171875,8.156562805175781,8.910407066345215,8.929011344909668,8.912620544433594,8.349308967590332,9.150344848632812,9.063459396362305,8.188298225402832,8.082099914550781,8.124395370483398,8.095884323120117,7.761401653289795,7.85822057723999,7.735099792480469,9.143465995788574,7.9051079750061035,8.62359619140625,8.771903991699219,8.10483455657959,8.881553649902344,7.842296600341797,7.751319408416748,8.932684898376465,8.793137550354004,8.686760902404785,8.676131248474121,8.958822250366211,7.80643892288208,8.740694046020508,7.7468767166137695,8.771867752075195,9.189112663269043,9.039446830749512,9.115840911865234,9.096116065979004,9.138891220092773,8.879927635192871,8.893594741821289,8.761467933654785,8.84201431274414,8.92170524597168,8.984542846679688,8.870153427124023,9.148591041564941,9.1036376953125,8.759328842163086,9.173047065734863,9.000730514526367,9.311477661132812,9.226938247680664,8.916337966918945,9.266477584838867,8.85345458984375,8.894457817077637,9.150871276855469,8.838128089904785,8.816519737243652,9.046035766601562,8.901216506958008,9.159720420837402,8.813364028930664,9.301010131835938,8.885666847229004,9.035273551940918,9.151689529418945,9.173745155334473,9.153688430786133,9.068913459777832,9.077773094177246,8.93253231048584,9.23687744140625,8.845597267150879,8.906045913696289,9.104510307312012,8.794829368591309,8.974413871765137,8.559188842773438,9.103609085083008,8.886739730834961,8.847504615783691,8.852912902832031,8.7335786819458,8.971410751342773,8.835782051086426,8.914285659790039,9.40961742401123,9.401433944702148,9.173538208007812,9.561808586120605,9.133511543273926,9.540070533752441,9.700275421142578,9.41574478149414,9.289504051208496,9.243343353271484,9.41695785522461,9.496110916137695,9.767770767211914,9.445943832397461,9.416340827941895,9.574084281921387,9.292332649230957,9.226899147033691,9.462366104125977,9.370545387268066,9.44404411315918,9.387925148010254,9.353792190551758,9.420747756958008,9.548852920532227,9.489824295043945,9.414810180664062,9.33342456817627,9.402825355529785,9.672316551208496,9.393604278564453,9.239797592163086,9.441904067993164,9.516083717346191,9.340642929077148,9.363749504089355,9.59388256072998,9.435190200805664,9.716001510620117,9.322286605834961,9.686580657958984,9.433467864990234,9.423392295837402,9.48121166229248,9.288214683532715,9.267616271972656,9.553618431091309,9.478880882263184,9.371923446655273,9.760424613952637,9.519524574279785,9.401512145996094,9.502867698669434,8.319539070129395,8.872060775756836,8.969176292419434,8.707355499267578,8.74543285369873,8.927281379699707,8.319100379943848,8.32375717163086,8.666680335998535,8.335197448730469,8.70714282989502,9.042021751403809,8.712772369384766,8.291357040405273,8.325663566589355,8.38085651397705,8.75160026550293,8.50749397277832,8.352300643920898,8.444624900817871,8.73957347869873,8.366410255432129,8.543909072875977,8.721636772155762,8.295104026794434,8.693644523620605,8.322171211242676,8.898469924926758,8.4146146774292,8.773049354553223,8.323225975036621,9.074934959411621,8.57706356048584,8.7324857711792,8.81857967376709,9.022188186645508,10.685402870178223,null,8.761948585510254,8.723730087280273,9.060052871704102,8.774007797241211,8.736745834350586,8.403592109680176,13.670157432556152,13.798901557922363,13.369604110717773,13.828919410705566,13.80864143371582,13.781868934631348,13.87149715423584,13.169575691223145,13.689194679260254,13.725122451782227,13.867948532104492,13.486695289611816,13.795697212219238,13.88539981842041,13.794876098632812,13.703969955444336,13.475425720214844,13.485220909118652,13.642592430114746,13.45029067993164,13.17920970916748,13.825347900390625,13.710762977600098,13.833584785461426,13.823281288146973,13.777862548828125,13.709671974182129,13.589035034179688,13.624615669250488,13.818294525146484,13.520774841308594,13.499052047729492,11.366277694702148,11.415215492248535,11.413336753845215,11.3681058883667,11.386617660522461,11.464384078979492,11.333633422851562,11.434808731079102,11.411873817443848,11.339577674865723,11.400299072265625,11.394913673400879,11.41909122467041,11.410325050354004,11.436091423034668,11.440723419189453,11.421466827392578,11.424187660217285,10.752309799194336,11.30122184753418,11.429636001586914,11.37482738494873,11.405726432800293,11.432455062866211,11.494464874267578,11.39643669128418,11.412282943725586,11.394112586975098,11.40078353881836,9.92600154876709,9.90449047088623,9.936745643615723,9.92866325378418,9.815776824951172,9.909724235534668,9.910490989685059,9.891134262084961,9.903509140014648,9.932821273803711,9.916892051696777,9.828373908996582,9.930082321166992,9.88005256652832,9.91939926147461,10.002853393554688,9.763079643249512,9.934072494506836,9.936637878417969,9.912662506103516,9.736724853515625,9.912676811218262,9.76375961303711,9.90211296081543,9.953424453735352,9.921619415283203,9.907418251037598,9.980151176452637,9.924909591674805,12.890239715576172,12.9169282913208,12.898111343383789,12.95676326751709,12.863883018493652,12.8355073928833,12.85640811920166,13.136683464050293,12.889006614685059,7.990469932556152,12.911820411682129,12.735673904418945,12.899605751037598],\"y\":[1.3817375898361206,1.0612719058990479,1.1385481357574463,1.08525550365448,1.1626895666122437,1.2480696439743042,0.4848286211490631,1.2720167636871338,1.1438474655151367,1.3171401023864746,1.4076839685440063,1.264595627784729,1.0884864330291748,1.1517572402954102,1.355188012123108,1.7229746580123901,0.972616970539093,1.1486741304397583,0.8810750246047974,1.257293701171875,0.9572843313217163,0.8085752129554749,0.9309483766555786,0.7958930730819702,1.0975350141525269,0.6360377073287964,0.9854050874710083,1.798175573348999,1.3100166320800781,0.67192143201828,1.2066819667816162,1.2865713834762573,0.9056100845336914,0.6249911189079285,1.7463622093200684,1.2420947551727295,0.7832163572311401,1.1565204858779907,1.3271960020065308,1.3933436870574951,1.0287328958511353,0.7349646091461182,1.1217514276504517,1.414216160774231,1.152062177658081,1.4249526262283325,1.1767480373382568,1.2981083393096924,1.1263619661331177,1.0864992141723633,0.5083565711975098,1.0866432189941406,1.8492233753204346,1.125476598739624,0.944848895072937,1.1385208368301392,0.678071916103363,1.3165032863616943,0.6076777577400208,1.3482673168182373,1.6210135221481323,0.9811966419219971,1.2457525730133057,1.171410322189331,1.1819112300872803,0.8604665398597717,0.942077100276947,0.516913115978241,1.7408256530761719,1.4816997051239014,1.3886042833328247,1.7745593786239624,0.794830322265625,0.5049740076065063,0.8629714250564575,0.9947225451469421,1.3124425411224365,1.181308388710022,0.65378338098526,0.9567456841468811,0.8112806677818298,1.0578453540802002,1.270293116569519,1.1971663236618042,1.2050573825836182,0.49335476756095886,1.5063146352767944,1.3702335357666016,1.2522984743118286,1.2417391538619995,1.4349349737167358,1.29513418674469,1.3764736652374268,1.1924550533294678,1.3044599294662476,1.0554274320602417,0.9203598499298096,1.1263551712036133,1.4461302757263184,1.4113985300064087,1.1233012676239014,1.292132019996643,1.383273959159851,1.2921757698059082,1.4860016107559204,1.4392153024673462,1.4001719951629639,0.9837578535079956,0.6659143567085266,0.9185031056404114,1.1586668491363525,1.4281121492385864,1.1629738807678223,0.510953962802887,0.5897344946861267,0.8766728639602661,1.4713213443756104,1.1049691438674927,0.5188847184181213,1.4383823871612549,1.3952010869979858,1.3247922658920288,0.9874094128608704,1.13014554977417,0.8806256651878357,1.1321403980255127,1.1713860034942627,1.29535710811615,1.1741254329681396,0.570444643497467,1.2960554361343384,0.7852280735969543,0.9467426538467407,1.7510077953338623,1.3233586549758911,1.1152814626693726,1.0483297109603882,0.647891640663147,0.5596342086791992,1.3211227655410767,0.9677881002426147,0.6502654552459717,1.0543253421783447,0.6772043704986572,1.212782382965088,1.1973955631256104,1.1335924863815308,1.4092952013015747,1.3452917337417603,0.6293814778327942,1.8506160974502563,1.066854476928711,1.2767295837402344,0.7192075848579407,1.712160587310791,0.8005959391593933,1.1127575635910034,-0.2791852056980133,-0.56585294008255,-0.33805036544799805,-0.5237462520599365,0.26701799035072327,-0.765414834022522,-0.7155061364173889,-0.6682496070861816,-0.6063860654830933,-0.3886547386646271,-0.6360691785812378,-0.5970664024353027,-0.6077590584754944,-0.3316837251186371,0.027478214353322983,-1.0895836353302002,-0.5561929941177368,0.06399431079626083,0.6634865999221802,-0.7742803692817688,0.14976142346858978,-1.053679347038269,-0.8100492358207703,-0.6574156284332275,-0.6182628273963928,2.9431962966918945,-0.8820046186447144,-0.8308306336402893,-0.7561229467391968,0.06208992749452591,-1.2723103761672974,0.021186748519539833,-0.9865592122077942,0.42442458868026733,-0.5611394047737122,-0.19294996559619904,-0.6945754289627075,-0.3245929181575775,0.14573052525520325,-0.8709970712661743,-0.352821946144104,-0.20292386412620544,-0.3928586542606354,-0.45419180393218994,-0.6536911725997925,-0.43990081548690796,0.47350263595581055,0.13158506155014038,-0.7095873355865479,-1.041664481163025,-0.30340245366096497,0.33025580644607544,-0.5825144648551941,-0.9634666442871094,0.1646973192691803,-0.9077123999595642,0.321090430021286,-0.4256440997123718,0.016457444056868553,-0.3007483184337616,-0.9926905035972595,-0.40955081582069397,-0.7163980603218079,-0.7613950967788696,-0.5563022494316101,-0.30304890871047974,-0.6749861836433411,-0.5403029322624207,-0.3122972548007965,-0.610802948474884,-0.7764879465103149,-0.3963066339492798,-0.9999945163726807,-0.2833714783191681,-0.40003347396850586,-0.9637208580970764,-0.45351165533065796,-0.5818671584129333,-0.7205086350440979,-0.9992696046829224,0.6563940644264221,-1.185823917388916,-1.0215363502502441,-0.13112245500087738,-0.6537771224975586,-0.7294257283210754,-0.7630423307418823,-0.7293157577514648,-0.9494206309318542,-0.3285471796989441,-0.1588168889284134,-0.36618950963020325,-0.9938457608222961,0.16926676034927368,-0.4724136292934418,0.05542946234345436,-0.2140544056892395,-0.305911123752594,-0.019443433731794357,0.04754577577114105,-0.5232704877853394,-0.11347304284572601,-0.42826223373413086,-0.3631307780742645,-0.34572187066078186,-0.06021670997142792,0.02899486944079399,-0.5741272568702698,-0.17813809216022491,-0.3307811915874481,-0.5624246597290039,-0.33823102712631226,-0.49408844113349915,-0.2875242233276367,-0.5673142671585083,-0.5054513216018677,-0.19457708299160004,0.04647556692361832,0.15025267004966736,-0.4029132127761841,-0.09265140444040298,-0.06226486340165138,-0.49906080961227417,-0.42240944504737854,-0.37416812777519226,-0.4376997649669647,-0.5433786511421204,-0.48809999227523804,-0.3587982952594757,-0.1461024433374405,-0.4376213848590851,-0.5315554738044739,-0.5797322392463684,2.760768413543701,-0.5617931485176086,-0.37373608350753784,1.0235728025436401,-0.2975458800792694,-0.12453179061412811,-0.39394140243530273,0.08051753044128418,-0.5586355328559875,-0.5786116719245911,-0.5049486756324768,0.1441514790058136,-0.7706948518753052,-0.21628828346729279,-0.3426320552825928,-0.16151897609233856,-0.39145922660827637,-0.5925371646881104,-0.2956721782684326,0.027539370581507683,-0.30378881096839905,0.042292892932891846,-0.12347960472106934,-0.06413992494344711,-0.6200217604637146,-0.4954264461994171,-0.34516313672065735,3.190534830093384,-0.2043280303478241,-0.5706292986869812,-0.7155386209487915,0.12354090809822083,-0.219037726521492,-0.293334424495697,0.07163935899734497,-0.5650919675827026,-0.29002729058265686,0.07026058435440063,-0.5817172527313232,-0.10595829039812088,-1.2771755456924438,-0.2213153839111328,0.0876251682639122,-0.7389064431190491,-0.14196763932704926,-0.6306360363960266,0.011064082384109497,0.014209670014679432,-0.09134206920862198,-0.2708755433559418,-0.20944665372371674,-0.15542514622211456,-0.016267064958810806,0.33848637342453003,1.3138884241925552e-05,-0.3145374655723572,-0.37487006187438965,0.08202043920755386,-0.06734184175729752,-0.26890313625335693,-0.2482864111661911,-0.37177231907844543,-0.342668354511261,0.05140113830566406,-0.009848969988524914,-0.26512715220451355,-0.21851959824562073,-0.20532433688640594,-0.34504765272140503,-0.12112276256084442,-0.11948814243078232,-0.36536410450935364,-0.43950170278549194,-0.3522815406322479,-0.16667716205120087,-0.33151018619537354,-0.4151010513305664,-0.21129287779331207,0.18422266840934753,-0.13335680961608887,0.17345526814460754,0.07078302651643753,-0.06395117193460464,-0.33869442343711853,-0.20178943872451782,-0.20231139659881592,-0.02338528260588646,0.17116601765155792,-0.3580891191959381,-0.31286415457725525,-0.32009008526802063,-0.1331133246421814,0.18067573010921478,-0.2697615623474121,0.08013986051082611,-0.31005680561065674,0.028209263458848,0.10702250152826309,0.2667584717273712,-0.3471834659576416,-0.3380453586578369,-0.07149399816989899,-0.4023083448410034,-0.10830654948949814,-0.17893050611019135,-0.2754707932472229,0.22065694630146027,-0.1637326180934906,-0.12212429195642471,-0.17535138130187988,-0.2760526239871979,-0.01606772653758526,0.11322040110826492,0.021302759647369385,1.5433651208877563,1.4605567455291748,1.4761929512023926,1.4625173807144165,1.4584766626358032,1.4409211874008179,1.4749916791915894,1.4284682273864746,1.4748163223266602,1.4806615114212036,1.4631659984588623,1.411305546760559,1.4848947525024414,1.0450985431671143,1.4943875074386597,1.3517125844955444,1.4878875017166138,1.4422603845596313,1.5346200466156006,1.5406434535980225,1.499199628829956,1.4141327142715454,1.4414336681365967,1.3886102437973022,1.4608150720596313,1.4566928148269653,1.5145379304885864,1.6541327238082886,1.5264090299606323,1.531654953956604,1.4845945835113525,1.4692519903182983,1.5288569927215576,1.5096898078918457,1.3595173358917236,1.587659478187561,1.4848111867904663,1.480909824371338,1.49443781375885,1.6546357870101929,1.5368572473526,1.2316579818725586,1.3251349925994873,1.5530869960784912,1.5514334440231323,1.5379899740219116,1.494699239730835,1.525313377380371,1.4642879962921143,1.526352882385254,1.288293480873108,1.483452320098877,1.4212583303451538,1.5053179264068604,1.3147481679916382,1.3840261697769165,1.5364909172058105,1.516196370124817,1.4429274797439575,1.4955360889434814,1.5462852716445923,1.454986810684204,1.4761298894882202,1.4729735851287842,1.450022578239441,1.443420648574829,1.4701296091079712,4.530323028564453,4.6652750968933105,4.259049415588379,4.17231559753418,4.669703960418701,4.087711334228516,4.67042350769043,4.63741397857666,4.61146879196167,4.00691556930542,4.465812683105469,4.730312824249268,3.997835874557495,4.689051151275635,4.215987682342529,4.713289260864258,3.9740796089172363,4.662024021148682,4.6290106773376465,4.683803558349609,4.155202865600586,4.592298984527588,4.655975341796875,4.570188999176025,4.67529296875,4.149698734283447,4.111392498016357,4.625186920166016,4.129754066467285,4.608168125152588,4.609062194824219,4.684344291687012,4.598238468170166,4.515726089477539,4.097564697265625,4.126251697540283,4.632607936859131,3.846534013748169,4.030094146728516,4.677065372467041,4.595035552978516,4.631924152374268,4.152017593383789,4.6212286949157715,4.092752933502197,4.058029651641846,4.524569034576416,4.68303918838501,4.761435031890869,4.133310794830322,4.037563800811768,4.604251384735107,4.569831848144531,4.141808032989502,4.4533562660217285,4.653535842895508,4.054136753082275,4.630998134613037,3.962613344192505,1.5877009630203247,1.2727618217468262,1.4802215099334717,1.6104549169540405,1.4569437503814697,1.539625644683838,1.472213864326477,2.0593249797821045,1.3333063125610352,1.6195672750473022,0.646668553352356,1.6028765439987183,1.468073844909668,1.4336971044540405,1.469533920288086,1.5507453680038452,1.6590402126312256,1.5533472299575806,1.5326228141784668,1.4909952878952026,1.5933552980422974,0.6876698732376099,1.5868359804153442,1.573804259300232,1.5753282308578491,1.2330971956253052,1.6163599491119385,0.7645369172096252,1.5960463285446167,1.578489899635315,0.5172238349914551,1.5027610063552856,1.38727867603302,1.5584850311279297,1.571655035018921,1.5743860006332397,1.5758932828903198,1.8300682306289673,1.7321003675460815,1.5929148197174072,1.4410037994384766,1.594342589378357,1.557225227355957,1.3510727882385254,1.2829564809799194,1.1802170276641846,1.5979278087615967,1.6627514362335205,1.6593539714813232,1.5607953071594238,1.3650426864624023,0.5767338275909424,1.5249618291854858,1.5589072704315186,1.6015998125076294,1.5737286806106567,1.52560555934906,1.5944088697433472,2.3929059505462646,2.3886959552764893,2.2481915950775146,2.366041660308838,2.3146793842315674,2.3162174224853516,2.369158983230591,2.5022075176239014,2.4162755012512207,2.523205280303955,2.5030629634857178,2.322309732437134,2.426605224609375,2.189079761505127,2.5072808265686035,2.1927618980407715,2.2802486419677734,2.404411792755127,2.5210015773773193,2.5109353065490723,2.188095808029175,2.2342336177825928,1.3085989952087402,2.270885467529297,2.520770311355591,2.4987809658050537,2.2820372581481934,2.2575674057006836,2.305651903152466,2.480987310409546,2.6763017177581787,2.3891241550445557,2.503016233444214,2.507106065750122,2.5494508743286133,2.4986634254455566,2.2242064476013184,2.3021676540374756,2.150664806365967,2.744899034500122,2.3470451831817627,2.496939182281494,2.4149718284606934,2.483858346939087,2.3855793476104736,2.2803268432617188,2.255380868911743,2.259631872177124,2.3189663887023926,2.3722052574157715,2.399129867553711,2.2720420360565186,2.2237091064453125,2.365219831466675,2.2338740825653076,2.3938751220703125,3.968130350112915,4.086702346801758,3.7755074501037598,3.991488218307495,3.975116014480591,4.029293060302734,4.046248435974121,4.013284683227539,4.176393032073975,4.156956672668457,4.055082321166992,3.9321210384368896,3.937089204788208,3.9232289791107178,3.8601222038269043,3.866779327392578,3.9942166805267334,3.850517511367798,3.827030897140503,4.08809232711792,3.6295218467712402,4.113508701324463,4.187417507171631,3.6063523292541504,3.9586288928985596,4.24702787399292,3.9166204929351807,4.1035475730896,3.9102394580841064,4.182351112365723,3.869297981262207,3.981309175491333,3.9789490699768066,3.9490838050842285,3.944530487060547,3.942408323287964,4.038426399230957,4.008914947509766,4.139644622802734,3.7226343154907227,4.209354877471924,4.045825958251953,3.9882144927978516,4.28642463684082,4.0150346755981445,3.615358352661133,4.036133289337158,4.028831958770752,3.9747371673583984,3.9180045127868652,3.7974116802215576,4.060174942016602,4.20427942276001,4.09943962097168,-0.5178394317626953,-0.650516152381897,-0.479851096868515,-0.4310290515422821,-0.5378267168998718,-0.6259913444519043,-0.7380387187004089,-0.21319614350795746,-0.7479232549667358,-0.610841691493988,-0.6754353642463684,-0.5430178642272949,-0.6768491268157959,-0.45748335123062134,-0.3581988215446472,-0.8913076519966125,-0.698038637638092,-0.7323350310325623,-0.4265297055244446,-0.6116015911102295,-0.9226518869400024,-0.7730879783630371,-0.7615750432014465,-0.7331415414810181,-0.8823621869087219,-0.8506748676300049,-0.7885114550590515,-0.723310649394989,-0.7920389175415039,-0.6305829882621765,-0.698114275932312,-0.7590868473052979,-0.9453780651092529,-0.47471097111701965,-0.7153941988945007,-0.7514808177947998,-0.32000821828842163,-0.5421023368835449,-0.5268405079841614,-0.7256784439086914,-0.6647657752037048,-0.5601599812507629,-0.8130180239677429,-0.5797785520553589,-0.76743084192276,-0.740344226360321,-0.6408556699752808,-0.4104425609111786,-0.3353818655014038,-0.6775698661804199,-0.7086683511734009,-0.9370354413986206,-0.7324972152709961,3.193037986755371,3.0153839588165283,3.2098007202148438,3.411353826522827,3.419900894165039,2.993455171585083,3.2046985626220703,3.197061538696289,3.5652174949645996,3.1496386528015137,3.4359893798828125,3.163926362991333,3.4493963718414307,3.234320640563965,3.2002310752868652,3.272998094558716,3.3818790912628174,3.3397650718688965,3.0758256912231445,3.1623947620391846,3.384615182876587,3.2408924102783203,3.358213186264038,3.416884660720825,3.167921781539917,3.4248197078704834,3.2836413383483887,3.2514102458953857,3.342782497406006,3.3918700218200684,3.194164752960205,3.047151803970337,3.55344820022583,3.2842931747436523,3.3557095527648926,3.1627235412597656,1.072845220565796,null,3.3806440830230713,3.417919874191284,3.0919387340545654,3.379578113555908,3.3994202613830566,3.3199284076690674,1.930212140083313,1.9589751958847046,1.9856090545654297,2.118469476699829,1.8433870077133179,2.083873748779297,1.9659463167190552,1.6238290071487427,1.9088102579116821,1.984571933746338,1.8491621017456055,1.8348524570465088,1.9095227718353271,2.1351234912872314,2.0434975624084473,1.9872238636016846,1.9833941459655762,1.972872018814087,1.8849225044250488,1.9644986391067505,1.6182628870010376,2.087661027908325,1.9592905044555664,1.9400484561920166,1.8442121744155884,2.052821636199951,2.1948325634002686,1.9715791940689087,1.9485830068588257,1.9467763900756836,2.033796548843384,2.05224347114563,-1.7047370672225952,-1.7189042568206787,-1.7727406024932861,-1.7914379835128784,-1.7604166269302368,-1.5030242204666138,-1.8196848630905151,-1.7163331508636475,-1.6494393348693848,-1.6843397617340088,-1.7724716663360596,-1.7692655324935913,-1.7378288507461548,-1.7441824674606323,-1.464735746383667,-1.7443996667861938,-1.7241114377975464,-1.6847602128982544,0.36447063088417053,-1.677884578704834,-1.6699968576431274,-1.7729989290237427,-1.7663508653640747,-1.7174915075302124,-1.603288173675537,-1.7217741012573242,-1.7533800601959229,-1.768631100654602,-1.7753701210021973,-2.003943920135498,-2.018934726715088,-2.0210397243499756,-1.9936052560806274,-1.9008338451385498,-1.989972710609436,-2.0100209712982178,-1.9968105554580688,-2.0147523880004883,-2.0181238651275635,-2.0108845233917236,-1.9405567646026611,-1.9686578512191772,-1.9617516994476318,-2.0073821544647217,-1.730658769607544,-1.9098670482635498,-1.9983524084091187,-2.0032131671905518,-2.021005392074585,-1.78288996219635,-1.9908539056777954,-1.9123069047927856,-1.9989206790924072,-1.8872182369232178,-2.0079305171966553,-1.9907784461975098,-1.9225029945373535,-2.0194551944732666,0.026861172169446945,-0.11374158412218094,-0.0861361101269722,-0.08715442568063736,0.14325851202011108,-0.09389468282461166,-0.06494750827550888,-0.21590127050876617,-0.029119817540049553,0.18973292410373688,-0.05029120296239853,0.2098502218723297,-0.02140832133591175],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"53_proprietary_opensourced_counterfactua\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"53_proprietary_opensourced\"],\"x\":[11.03643798828125,11.07364559173584,11.248010635375977,11.170310974121094,10.949195861816406,11.285311698913574,10.957159996032715,11.096034049987793,11.2238130569458,11.023275375366211,11.042583465576172,11.16788101196289,11.188385009765625,8.700318336486816,8.662346839904785,8.710755348205566,8.685635566711426,10.063787460327148,8.825141906738281,9.746978759765625,8.632473945617676,8.7011137008667,9.762179374694824,10.958747863769531,10.26634407043457,10.128600120544434,10.363204956054688,10.149825096130371,10.254651069641113,10.238754272460938,10.266803741455078,10.18940258026123,10.27680492401123,10.135074615478516,10.240616798400879],\"y\":[3.106004476547241,3.0234382152557373,3.3541853427886963,3.115736246109009,3.0922281742095947,3.283050775527954,3.0349786281585693,3.2528135776519775,3.086516857147217,3.087618112564087,3.101942300796509,3.0745949745178223,3.133884906768799,1.2634673118591309,1.199538230895996,1.2940387725830078,1.2151010036468506,2.198474407196045,1.475010633468628,3.3699069023132324,1.1698120832443237,1.2515336275100708,3.3798534870147705,1.8569083213806152,2.246767520904541,2.1721770763397217,2.3008975982666016,2.423623561859131,2.2409117221832275,2.265772581100464,2.3193318843841553,2.154175281524658,2.2092831134796143,2.1171963214874268,2.437375783920288],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"54_dense_retrieval_retrievers_bm25_modif\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"54_dense_retrieval\"],\"x\":[11.089601516723633,11.02492904663086,10.883186340332031,10.981522560119629,10.981151580810547,10.95821762084961,10.9121675491333,10.982845306396484,11.157200813293457,10.896283149719238,10.92767333984375,10.96117877960205,12.416332244873047,12.561809539794922,12.500757217407227,12.375421524047852,12.55825138092041,12.481864929199219,12.49730396270752,12.1989107131958,12.471221923828125,12.578041076660156,12.469315528869629,11.689789772033691],\"y\":[2.4378020763397217,2.470427989959717,2.49906587600708,2.4681739807128906,2.4417030811309814,2.4632935523986816,2.5861077308654785,2.4596455097198486,2.333122968673706,2.6070048809051514,2.5345096588134766,2.460688352584839,1.9628962278366089,2.3608059883117676,2.222209930419922,2.1565310955047607,2.1810107231140137,2.1097028255462646,2.3813178539276123,2.188523292541504,2.159067153930664,2.3556671142578125,1.6200826168060303,2.324319839477539],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"55_chatgpt_sarcasm_responses_chatgpts_in\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"55_chatgpt_sarcasm\"],\"x\":[8.408228874206543,8.558862686157227,8.468222618103027,8.394969940185547,8.488740921020508,8.462080955505371,8.458389282226562,8.648653030395508,8.45824146270752,8.496983528137207,8.497143745422363,8.60595989227295,8.652069091796875,8.46937370300293,8.726569175720215,8.704874992370605,8.474954605102539,8.499213218688965,7.443446159362793,7.453991889953613,7.434319496154785,7.457515239715576,7.445913314819336,7.432080268859863,7.492039680480957,7.444361686706543,7.45388650894165,7.470397472381592,7.456826210021973,7.457319259643555,7.452033042907715,8.679919242858887,8.863896369934082,8.659948348999023,8.705171585083008,8.791703224182129,8.700974464416504,8.720808029174805,9.014729499816895,8.698103904724121,8.63776969909668,8.665627479553223,8.250149726867676],\"y\":[0.8488311767578125,0.8990485668182373,0.930065929889679,0.9668167233467102,0.8830121159553528,0.8673952221870422,0.9546473622322083,0.9203405976295471,0.9518395662307739,0.8990188241004944,0.8826806545257568,1.001754879951477,0.8333197236061096,0.94631427526474,0.7836666703224182,0.7688788771629333,0.916922390460968,0.7554416656494141,3.3650591373443604,3.3583810329437256,3.3732411861419678,3.3514108657836914,3.3623621463775635,3.374535083770752,3.3646886348724365,3.3602709770202637,3.329127788543701,3.332568883895874,3.3524117469787598,3.345237970352173,3.361050605773926,0.4001754820346832,0.5290360450744629,0.39178118109703064,0.5974481105804443,0.46730977296829224,0.44593337178230286,0.3917016386985779,0.42155221104621887,0.3866553008556366,0.5923276543617249,0.5228016972541809,1.5425493717193604],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"#CFD8DC\",\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"other\",\"showlegend\":false,\"x\":[12.260786056518555,10.01115894317627,11.924004554748535,10.16279411315918,12.00646686553955,10.273680686950684,6.795833110809326,10.738099098205566,12.415397644042969,12.344215393066406,8.005266189575195,11.672365188598633,9.678071022033691,11.613967895507812,10.691057205200195,10.729043006896973,12.570730209350586,12.728907585144043,11.094691276550293,12.467524528503418,10.7769136428833,11.365942001342773,11.85193157196045,10.574968338012695,13.300118446350098,8.84424877166748,9.322251319885254,11.329916000366211,9.31579303741455,10.838811874389648,11.459781646728516,11.970154762268066,12.745402336120605,10.581709861755371,9.499332427978516,10.223129272460938,9.936136245727539,11.424376487731934,7.634429931640625,12.565068244934082,12.609332084655762,11.734045028686523,12.76197338104248,8.449498176574707,10.840675354003906,11.946686744689941,8.837322235107422,9.319323539733887,12.509225845336914,10.419879913330078,6.7955498695373535,9.324790954589844,12.908655166625977,11.152015686035156,11.986949920654297,12.218483924865723,11.286534309387207,11.993667602539062,9.96347427368164,11.670063018798828,9.893813133239746,10.930827140808105,9.840456008911133,9.265555381774902,8.651935577392578,9.824747085571289,9.816277503967285,12.087455749511719,11.567899703979492,6.796971321105957,12.668866157531738,12.218778610229492,10.99422836303711,11.2903470993042,13.343911170959473,12.024370193481445,12.494938850402832,12.23508071899414,10.537565231323242,9.071687698364258,9.39167308807373,13.654808044433594,11.47091293334961,11.624275207519531,12.246249198913574,12.476778984069824,11.365124702453613,12.479470252990723,12.073331832885742,10.643670082092285,10.698538780212402,11.801766395568848,11.808372497558594,9.954280853271484,12.55838394165039,8.872209548950195,11.538613319396973,8.866655349731445,10.459000587463379,11.395777702331543,9.855978012084961,11.591615676879883,10.30197811126709,11.654211044311523,11.18188190460205,10.506263732910156,13.050704002380371,11.941532135009766,12.512845993041992,11.652451515197754,13.646256446838379,11.867741584777832,12.657735824584961,12.094124794006348,10.286686897277832,12.581634521484375,11.336400985717773,9.937158584594727,11.021777153015137,9.948188781738281,12.35839557647705,12.886040687561035,9.261913299560547,9.89763355255127,9.462775230407715,9.968618392944336,12.362512588500977,10.358255386352539,8.327774047851562,11.927535057067871,10.00265884399414,12.349808692932129,10.623787879943848,10.678670883178711,11.847443580627441,9.692587852478027,9.262121200561523,10.958932876586914,12.889772415161133,10.948387145996094,11.634035110473633,12.080227851867676,10.378564834594727,10.096247673034668,11.42621898651123,8.7933931350708,11.231094360351562,8.999340057373047,11.527088165283203,12.19135570526123,7.8714375495910645,11.001914978027344,12.61523151397705,9.75306224822998,11.896245002746582,12.523083686828613,9.378335952758789,10.35689926147461,11.08590316772461,13.147154808044434,9.818265914916992,11.11213493347168,11.444499969482422,11.590463638305664,10.737903594970703,9.770731925964355,10.78557014465332,11.933521270751953,10.511313438415527,9.82026481628418,8.356404304504395,10.73415470123291,10.45881462097168,10.396477699279785,12.194050788879395,12.752180099487305,11.78748607635498,10.255226135253906,12.165645599365234,12.292634963989258,12.844697952270508,10.628177642822266,8.569395065307617,11.763611793518066,12.15890884399414,12.740022659301758,12.746966361999512,9.62110424041748,11.45886516571045,10.926743507385254,12.772931098937988,12.837591171264648,12.200713157653809,9.013678550720215,10.967081069946289,9.76983642578125,12.884422302246094,11.670434951782227,13.589189529418945,12.576118469238281,11.409375190734863,9.541470527648926,8.66622257232666,11.438347816467285,11.34601879119873,10.590363502502441,11.35364818572998,11.930695533752441,6.797292709350586,10.794591903686523,10.309884071350098,10.247868537902832,9.914916038513184,13.303763389587402,12.540189743041992,11.354313850402832,8.383038520812988,12.193739891052246,9.227989196777344,9.241358757019043,9.536521911621094,10.984012603759766,12.313669204711914,9.644474029541016,9.546388626098633,9.345878601074219,11.460395812988281,10.699294090270996,9.629450798034668,8.48170280456543,11.0349760055542,10.570517539978027,10.367918014526367,11.483748435974121,10.360127449035645,10.180638313293457,12.789478302001953,10.665085792541504,11.708563804626465,12.97873592376709,11.337641716003418,10.709122657775879,13.167486190795898,12.409531593322754,8.32143497467041,9.063648223876953,10.371134757995605,11.557292938232422,10.586905479431152,9.22173023223877,11.61303424835205,8.834911346435547,13.149587631225586,11.683677673339844,9.396690368652344,11.555591583251953,11.163105964660645,10.546306610107422,13.552511215209961,12.85168170928955,11.200281143188477,9.939804077148438,10.729233741760254,12.406366348266602,11.353873252868652,11.169713973999023,9.460506439208984,10.654193878173828,11.77177906036377,11.467199325561523,12.088428497314453,13.498458862304688,11.279095649719238,12.567978858947754,10.763456344604492,10.033524513244629,12.673567771911621,10.10020637512207,8.970285415649414,9.142569541931152,11.128690719604492,12.259600639343262,8.503023147583008,11.229401588439941,9.610413551330566,10.481382369995117,12.30782699584961,11.052600860595703,13.069833755493164,9.056635856628418,9.835722923278809,11.540289878845215,10.57509994506836,10.630483627319336,12.53976058959961,12.878406524658203,12.195188522338867,12.416590690612793,8.807579040527344,12.123668670654297,10.993809700012207,11.357285499572754,11.609417915344238,8.505378723144531,9.010087966918945,9.596749305725098,12.824377059936523,10.293586730957031,11.473138809204102,12.474746704101562,10.027557373046875,12.210655212402344,10.91020393371582,13.3267240524292,13.175515174865723,11.291525840759277,11.628263473510742,11.242234230041504,9.939929008483887,12.68603515625,9.105117797851562,12.55679702758789,10.582772254943848,11.557234764099121,12.54797649383545,11.068909645080566,12.6818208694458,11.577909469604492,11.18771743774414,13.261770248413086,11.224922180175781,11.866551399230957,9.587552070617676,10.551297187805176,11.903278350830078,12.07540225982666,10.768095970153809,12.016518592834473,8.86601448059082,12.697779655456543,12.205658912658691,10.315733909606934,11.997912406921387,12.761860847473145,11.485032081604004,12.877477645874023,10.77319622039795,9.71633529663086,12.789429664611816,10.579119682312012,12.169694900512695,11.369839668273926,12.047872543334961,11.594593048095703,11.944458961486816,11.05200481414795,9.081618309020996,12.835399627685547,12.314804077148438,9.093341827392578,11.577603340148926,9.215981483459473,11.374435424804688,12.266894340515137,11.160942077636719,8.430356979370117,12.54307746887207,11.239245414733887,10.539368629455566,11.465906143188477,7.657243728637695,12.263589859008789,9.044021606445312,11.270294189453125,9.69029426574707,12.90316104888916,11.74852466583252,8.697430610656738,10.244534492492676,13.194223403930664,8.26854419708252,12.263144493103027,12.15650463104248,10.97741985321045,11.557283401489258,12.207938194274902,9.880388259887695,13.715070724487305,8.669084548950195,11.517677307128906,12.063190460205078,10.891190528869629,8.157964706420898,13.416830062866211,12.172284126281738,9.77486515045166,12.416096687316895,8.317665100097656,12.529857635498047,10.84004020690918,8.873795509338379,12.759370803833008,10.572245597839355,11.383156776428223,11.408194541931152,12.709611892700195,13.176079750061035,7.645970344543457,10.45201587677002,11.888107299804688,8.155491828918457,10.934144973754883,12.047857284545898,11.55852222442627,10.887688636779785,10.773085594177246,10.705704689025879,9.601393699645996,10.455825805664062,9.596738815307617,10.703749656677246,11.308526992797852,9.27015209197998,10.01099681854248,12.847784996032715,12.372241020202637,10.8928804397583,11.546853065490723,9.608847618103027,10.049355506896973,10.234009742736816,12.77387809753418,7.639250755310059,9.369508743286133,11.111124992370605,10.788805961608887,9.64129638671875,10.971961975097656,13.150918960571289,10.595973014831543,9.54521369934082,12.856410026550293,10.756369590759277,11.570320129394531,10.633820533752441,10.61214542388916,11.196186065673828,11.974361419677734,11.977964401245117,9.437406539916992,11.594237327575684,11.464212417602539,10.141727447509766,11.141068458557129,9.565552711486816,12.504912376403809,10.439518928527832,8.916755676269531,12.542708396911621,8.100579261779785,13.614336013793945,9.942835807800293,12.88668441772461,12.289319038391113,10.191901206970215,11.653940200805664,12.273872375488281,9.7015380859375,11.526108741760254,11.894497871398926,9.942673683166504,12.299449920654297,11.311140060424805,12.881195068359375,8.470643043518066,12.338445663452148,11.502923011779785,10.742378234863281,12.947842597961426,11.659990310668945,12.496929168701172,8.366921424865723,9.1043119430542,12.329166412353516,11.74715518951416,11.585480690002441,10.612318992614746,11.377769470214844,11.649738311767578,11.907842636108398,13.692667007446289,11.34955883026123,12.52751636505127,8.80586051940918,12.406229019165039,12.7645902633667,12.445311546325684,11.75776195526123,11.090094566345215,11.42638111114502,10.89482593536377,10.761418342590332,11.150059700012207,10.034558296203613,10.338700294494629,11.178735733032227,12.835885047912598,11.53531551361084,12.381514549255371,11.54314136505127,11.90941047668457,9.358724594116211,10.740930557250977,11.939253807067871,11.149989128112793,11.659534454345703,10.353327751159668,11.331498146057129,10.64175796508789,12.81151008605957,8.662858009338379,10.857192993164062,10.787764549255371,8.953280448913574,12.585387229919434,7.643540382385254,10.447787284851074,10.80845832824707,12.5751371383667,9.113858222961426,9.70373821258545,12.737845420837402,11.416006088256836,12.489952087402344,8.133481979370117,10.278565406799316,11.977149963378906,9.470913887023926,10.66490364074707,10.00718879699707,9.176525115966797,11.815064430236816,8.752781867980957,11.483132362365723,11.89669418334961,12.553421020507812,13.118978500366211,9.782742500305176,10.74406623840332,12.168991088867188,11.756896018981934,12.940293312072754,11.56460189819336,8.64371109008789,11.702585220336914,11.278419494628906,10.558026313781738,11.820279121398926,10.459695816040039,11.502100944519043,11.530735969543457,10.50390911102295,12.36990737915039,12.139140129089355,11.131754875183105,10.48403263092041,13.041622161865234,11.032854080200195,11.734776496887207,10.479324340820312,9.945302963256836,11.825505256652832,11.676788330078125,9.411858558654785,12.318131446838379,11.462925910949707,12.87320613861084,13.661906242370605,12.474398612976074,10.583564758300781,11.804007530212402,11.14046573638916,11.841476440429688,11.781903266906738,9.183506965637207,11.8534574508667,11.49117660522461,11.63294506072998,11.224637985229492,12.764777183532715,12.330574035644531,11.128299713134766,12.93774127960205,10.32089614868164,12.478330612182617,11.827890396118164,11.804841041564941,11.44624137878418,11.820039749145508,10.238183975219727,7.784416675567627,12.978934288024902,12.451269149780273,9.322071075439453,11.821974754333496,10.541068077087402,11.961499214172363,8.238553047180176,12.170052528381348,12.736010551452637,12.764181137084961,11.65296745300293,12.675060272216797,11.974373817443848,10.117522239685059,9.6543607711792,12.757356643676758,12.056167602539062,9.412654876708984,11.711962699890137,9.174324989318848,10.823249816894531,9.90158462524414,10.913339614868164,8.113393783569336,9.993379592895508,12.194329261779785,13.33237361907959,11.182921409606934,12.732467651367188,13.046422004699707,9.75806713104248,8.685029029846191,11.561057090759277,10.053182601928711,11.998501777648926,13.20248031616211,12.104700088500977,10.906420707702637,11.117636680603027,10.446064949035645,9.715156555175781,12.530933380126953,12.754565238952637,11.461576461791992,11.491990089416504,9.396021842956543,8.587562561035156,12.263270378112793,11.42301082611084,13.491022109985352,10.364217758178711,11.991475105285645,12.44273853302002,11.70559310913086,10.35807991027832,9.996321678161621,9.326005935668945,12.807846069335938,13.570623397827148,11.12037467956543,10.716169357299805,10.50700569152832,10.22071361541748,12.695380210876465,11.297102928161621,9.085335731506348,12.861201286315918,11.170536994934082,10.041293144226074,10.805912017822266,11.3994722366333,9.783839225769043,11.411025047302246,10.608845710754395,9.42233657836914,11.715644836425781,11.849699020385742,12.349161148071289,8.975476264953613,10.749741554260254,9.420133590698242,10.120325088500977,12.375459671020508,10.686391830444336,11.969694137573242,11.85922622680664,12.443221092224121,9.37954330444336,13.088716506958008,10.281312942504883,12.127240180969238,11.985560417175293,12.442952156066895,12.119172096252441,9.390953063964844,10.271842956542969,10.183282852172852,12.758428573608398,9.88254451751709,11.704732894897461,11.48423957824707,9.189234733581543,11.172453880310059,11.48064136505127,10.315546035766602,12.061239242553711,11.559906005859375,11.86010456085205,11.664885520935059,12.843629837036133,9.624467849731445,12.458562850952148,12.246566772460938,11.838500022888184,10.999716758728027,9.12990951538086,11.840607643127441,11.96013069152832,11.915284156799316,12.08629035949707,10.903203964233398,8.849540710449219,11.167780876159668,10.531004905700684,12.467206954956055,10.145415306091309,9.549249649047852,6.795407295227051,12.180326461791992,12.817878723144531,10.757889747619629,11.568016052246094,11.672552108764648,7.97341775894165,10.022518157958984,9.927380561828613,10.70942211151123,11.941184043884277,12.794625282287598,12.879822731018066,12.178878784179688,11.878340721130371,10.52446460723877,10.071585655212402,10.56197738647461,11.002744674682617,12.245721817016602,10.67922592163086,12.140796661376953,12.277287483215332,11.456153869628906,12.081787109375,11.62030029296875,10.880305290222168,10.69717025756836,9.24069881439209,12.846972465515137,10.142430305480957,12.146927833557129,9.958215713500977,9.50244140625,10.51760196685791,10.551128387451172,11.390852928161621,11.688502311706543,11.491974830627441,12.741220474243164,12.295635223388672,12.218851089477539,8.685344696044922,13.047622680664062,11.958925247192383,9.383042335510254,11.218199729919434,11.51757526397705,9.624102592468262,13.13804817199707,12.326669692993164,12.624595642089844,12.161945343017578,9.43790054321289,11.324953079223633,8.503750801086426,11.257233619689941,10.58021354675293,9.62783145904541,9.703179359436035,11.924786567687988,8.906972885131836,9.254066467285156,12.899577140808105,11.277064323425293,11.152223587036133,12.76430606842041,10.148778915405273,12.378602027893066,11.560495376586914,12.818056106567383,12.91047191619873,8.343107223510742,10.998586654663086,11.666386604309082,10.141661643981934,11.702606201171875,10.432024955749512,10.108580589294434,9.184564590454102,13.428140640258789,11.512848854064941,12.044175148010254,11.576578140258789,12.28610610961914,11.385955810546875,12.952836036682129,12.021284103393555,11.287923812866211,12.099640846252441,12.082901000976562,12.028682708740234,12.383368492126465,8.773681640625,12.879551887512207,11.319714546203613,8.952622413635254,11.267468452453613,12.477086067199707,12.503398895263672,9.42236614227295,11.20785140991211,11.332497596740723,8.951685905456543,10.856745719909668,10.309080123901367,11.73598861694336,8.723488807678223,12.038217544555664,9.878068923950195,12.995759963989258,10.974395751953125,9.64431381225586,10.158196449279785,13.631218910217285,8.542502403259277,11.165329933166504,10.789525032043457,13.625770568847656,11.960083961486816,12.425254821777344,11.940552711486816,13.470690727233887,13.038926124572754,7.834033012390137,9.87307071685791,11.216033935546875,12.414644241333008,12.093451499938965,11.877202987670898,12.003032684326172,10.026086807250977,12.491192817687988,13.477025032043457,12.77425479888916,10.46001148223877,8.880654335021973,9.948962211608887,10.48256778717041,12.090274810791016,11.583839416503906,11.40567684173584,6.7874345779418945,9.088058471679688,11.758285522460938,11.31846809387207,11.73054313659668,12.907270431518555,12.770581245422363,11.114065170288086,11.227639198303223,10.648099899291992,12.066269874572754,13.3859224319458,12.3936767578125,11.536118507385254,12.09136962890625,7.971031665802002,10.897802352905273,9.173691749572754,11.609256744384766,12.269068717956543,9.517958641052246,11.856656074523926,11.710532188415527,12.882142066955566,12.293147087097168,9.226627349853516,11.812376022338867,9.573324203491211,11.98694896697998,11.025315284729004,12.620606422424316,12.66535758972168,11.626893997192383,11.87524127960205,9.740761756896973,11.66956615447998,9.677865982055664,13.708874702453613,13.700621604919434,10.8056058883667,12.719548225402832,11.30461597442627,9.15710163116455,11.5343599319458,9.219347953796387,11.886124610900879,11.679952621459961,12.069474220275879,12.361296653747559,10.542925834655762,10.790267944335938,12.395968437194824,10.26074504852295,12.145330429077148,11.74125862121582,12.40096664428711,10.930389404296875,8.466429710388184,11.920580863952637,12.118764877319336,13.445467948913574,10.998306274414062,11.986994743347168,12.903426170349121,11.159873962402344,12.18824291229248,9.372452735900879,12.712847709655762,13.078712463378906,10.631550788879395,9.169002532958984,8.790238380432129,11.811575889587402,12.28490924835205,9.769627571105957,12.567846298217773,10.349342346191406,11.191412925720215,11.537104606628418,10.127388000488281,10.513239860534668,8.915346145629883,11.823579788208008,11.909771919250488,11.520251274108887,13.776591300964355,12.594861030578613,6.803589344024658,10.791962623596191,11.956903457641602,7.651430606842041,12.864334106445312,11.632024765014648,10.91291332244873,11.907623291015625,10.763083457946777,11.546504974365234,13.262833595275879,12.147619247436523,11.872856140136719,12.41122055053711,12.41154670715332,13.054023742675781,10.615890502929688,9.164365768432617],\"y\":[1.976219654083252,1.265775442123413,1.0926563739776611,-0.5182633996009827,2.4570963382720947,0.48775091767311096,-0.910475492477417,-0.5370233654975891,1.2164555788040161,1.3136889934539795,-0.12520691752433777,1.813036561012268,1.1669845581054688,1.774245023727417,-0.3897693157196045,1.3097110986709595,3.413337469100952,1.671027660369873,0.8366751074790955,3.2766458988189697,0.3775566816329956,0.44715550541877747,2.3352620601654053,3.230710506439209,2.2592904567718506,0.6792904138565063,0.25117701292037964,0.6122933626174927,1.182349681854248,0.9189866781234741,3.0709445476531982,0.9211485385894775,3.500269889831543,3.7989087104797363,1.823943853378296,1.6083993911743164,-0.23820368945598602,2.4324207305908203,0.19927099347114563,3.7869391441345215,2.054600715637207,0.9205240607261658,3.078073263168335,0.10503587871789932,0.8371968865394592,0.3198699951171875,-0.16870951652526855,2.7976632118225098,2.6678123474121094,-0.570141077041626,-0.9106079339981079,3.7905776500701904,2.7144033908843994,0.9463844299316406,1.3737200498580933,0.33007562160491943,-0.9850686192512512,1.1608271598815918,1.0532095432281494,2.060023546218872,1.5419471263885498,1.308414101600647,-1.1935114860534668,-0.17752167582511902,-0.4857664108276367,3.832371234893799,1.3047598600387573,3.552154541015625,1.942522644996643,-0.909134566783905,2.3067994117736816,0.9871791005134583,2.9792444705963135,2.830186128616333,0.7007690072059631,3.5236191749572754,3.808678388595581,0.5856729745864868,0.5755625367164612,-0.488295316696167,0.6351603269577026,0.5080202221870422,1.0401160717010498,2.3617639541625977,1.8510946035385132,0.9317828416824341,3.2759201526641846,2.4336321353912354,-1.299113392829895,3.8617777824401855,3.202488422393799,1.037284016609192,-0.5317288637161255,-0.5715892314910889,1.2100167274475098,1.357236623764038,3.533149480819702,3.0465500354766846,0.1627165526151657,0.7375492453575134,0.9511770009994507,0.07118048518896103,-0.3919447958469391,1.378462314605713,-0.3244394361972809,0.7082329988479614,2.143453359603882,1.1407673358917236,0.9031526446342468,2.8212311267852783,1.1145505905151367,1.4429399967193604,2.296320676803589,-0.001279126270674169,-0.10712704062461853,3.0526630878448486,0.7859264612197876,-0.6041418313980103,1.40556800365448,-0.5446928143501282,1.2216320037841797,3.4056971073150635,3.86862850189209,1.1494468450546265,0.1541697382926941,-0.2646180987358093,2.934096097946167,1.085005283355713,0.1298372447490692,1.4595853090286255,0.5229455232620239,1.7139097452163696,3.179950475692749,0.8122704029083252,2.313836097717285,0.9151896834373474,0.0694771260023117,2.251661777496338,2.7393033504486084,1.8044404983520508,2.4212090969085693,1.369358777999878,1.4411309957504272,1.778269648551941,1.9663785696029663,-0.2085247039794922,1.8884222507476807,0.01605393923819065,0.9991214275360107,2.1229145526885986,-0.17486093938350677,1.2444441318511963,1.5270284414291382,1.034622311592102,0.5874497890472412,1.0864499807357788,0.7937063574790955,0.2653231918811798,0.6648417711257935,2.1787331104278564,-1.4033302068710327,0.993794858455658,3.9591336250305176,2.2620389461517334,2.2779126167297363,1.026957631111145,1.6402356624603271,1.106472134590149,3.609027624130249,4.001310348510742,2.4320061206817627,1.3439748287200928,1.444767951965332,0.8551923036575317,0.15037234127521515,2.425750494003296,2.3774797916412354,2.089541435241699,0.14875546097755432,1.2632057666778564,1.3453816175460815,3.9026010036468506,0.47671282291412354,1.526449203491211,1.530004620552063,0.44732722640037537,1.5337772369384766,-0.5403087735176086,-1.0967426300048828,0.833954393863678,0.7645677924156189,2.70750093460083,1.2675151824951172,0.1613716185092926,3.286445379257202,0.9860133528709412,3.4668221473693848,1.4706974029541016,-0.06515169888734818,3.346012830734253,3.19698166847229,1.0387732982635498,-0.47503626346588135,1.1350661516189575,-0.12550556659698486,0.48462581634521484,3.611987352371216,0.44945693016052246,-0.9105373620986938,0.9302946925163269,2.0090830326080322,0.1268618106842041,-0.5888694524765015,1.5191290378570557,2.7231905460357666,3.63657283782959,0.20960384607315063,0.04279949516057968,2.0899455547332764,2.395681142807007,-0.3550689220428467,1.1873329877853394,1.6772342920303345,-0.551196813583374,1.0045689344406128,3.5376405715942383,-0.2728564739227295,1.6083232164382935,3.451906681060791,0.4097013771533966,0.28619876503944397,2.9995458126068115,1.430558204650879,2.9486751556396484,1.4180095195770264,3.394707679748535,-1.0172475576400757,1.6186503171920776,2.2325010299682617,3.3960022926330566,2.3339428901672363,2.508234739303589,1.917661428451538,2.007206678390503,0.7356185913085938,1.3998340368270874,1.426846981048584,-0.10650590807199478,3.196619987487793,1.5687898397445679,3.3810524940490723,-0.20458725094795227,1.947638988494873,-0.17957328259944916,0.6638663411140442,1.4990065097808838,1.0839821100234985,-0.13390646874904633,1.798355221748352,1.460438847541809,2.6365201473236084,2.2331058979034424,0.5038972496986389,1.672692894935608,3.618595838546753,2.4914488792419434,-0.7637554407119751,2.602285861968994,1.6152315139770508,2.3165502548217773,4.036905288696289,1.7626341581344604,-0.30745241045951843,1.3061516284942627,3.0544943809509277,2.2432503700256348,0.5260573625564575,0.26869526505470276,0.03409036993980408,-0.5223769545555115,0.39339277148246765,2.0766706466674805,-0.38802358508110046,3.801676034927368,2.3770365715026855,3.5236270427703857,3.0124449729919434,3.137105703353882,3.03592848777771,1.0055789947509766,2.3000667095184326,3.485448122024536,2.8649325370788574,3.2783849239349365,2.067582130432129,1.7887386083602905,1.117841124534607,1.9235390424728394,-0.5475263595581055,2.5567400455474854,-0.42588913440704346,1.6459540128707886,3.0523805618286133,3.548125743865967,0.9675278663635254,1.0511583089828491,1.8744837045669556,-0.2662609815597534,2.020230531692505,2.395968198776245,1.5245238542556763,2.208904504776001,-0.23743867874145508,2.1941235065460205,3.066039800643921,0.8780515789985657,-0.3361736238002777,0.12655974924564362,0.5816671252250671,2.5916388034820557,1.8386311531066895,0.9971460103988647,-0.09463711082935333,1.4201574325561523,3.414994478225708,-0.5812013745307922,1.0859824419021606,3.207399845123291,1.0073139667510986,1.059942603111267,0.9441660642623901,2.421081066131592,-0.9699038863182068,-0.7918293476104736,3.2947211265563965,3.453516721725464,2.6633572578430176,1.12492835521698,0.9515612125396729,1.8676871061325073,2.2264864444732666,0.4588735103607178,-0.7277568578720093,-1.0359772443771362,2.857710599899292,2.6983368396759033,1.5439820289611816,0.8810650110244751,3.6186933517456055,3.788325071334839,2.0987229347229004,3.1248533725738525,-0.6990334391593933,3.2795536518096924,0.4755188822746277,2.3451409339904785,1.9918140172958374,2.239830255508423,2.980741500854492,3.4494681358337402,2.7023935317993164,0.8716931343078613,1.143310546875,0.12861472368240356,1.2377668619155884,-0.3922814130783081,3.3432180881500244,1.7890307903289795,0.9444081783294678,2.8529300689697266,0.18657220900058746,2.6263394355773926,0.48836591839790344,0.770980954170227,0.9084759950637817,2.9118480682373047,0.899668276309967,0.3940073549747467,0.8528211116790771,1.6868844032287598,0.24871914088726044,2.4808976650238037,2.190305709838867,1.4015684127807617,-0.11070117354393005,1.4967073202133179,0.9283192753791809,0.4603424668312073,1.1601427793502808,1.9638222455978394,-1.3094513416290283,1.8755643367767334,-0.034235879778862,2.5533201694488525,2.5379700660705566,3.77651309967041,2.733030319213867,0.3953371047973633,2.7038984298706055,1.0902845859527588,3.95662522315979,3.4667394161224365,1.4680428504943848,-0.2102063000202179,1.9835692644119263,0.25430288910865784,2.2354414463043213,0.19437259435653687,1.457194447517395,1.478986144065857,0.024254370480775833,0.9681276082992554,-1.2682996988296509,1.4637945890426636,0.9524343013763428,3.2517035007476807,1.0663663148880005,1.047045350074768,2.5798704624176025,1.9295272827148438,-0.4900413751602173,1.2294294834136963,3.744098663330078,1.1958093643188477,3.4060301780700684,1.4086087942123413,3.195308208465576,-0.21053600311279297,1.9594486951828003,1.3294121026992798,0.7901973128318787,-1.0279371738433838,0.18727345764636993,2.7890584468841553,1.1563249826431274,-0.1735643744468689,2.2202253341674805,0.9077820777893066,2.668337821960449,2.603579521179199,0.24776341021060944,1.158795714378357,1.6285470724105835,1.96036958694458,3.842803716659546,-0.05351594090461731,1.0603022575378418,3.5160486698150635,3.203173875808716,-0.22633570432662964,1.7337943315505981,1.1724951267242432,0.17315110564231873,0.7105343341827393,2.4431610107421875,3.8104262351989746,0.10712434351444244,0.713238537311554,3.1322901248931885,3.2645468711853027,0.534716784954071,0.5656581521034241,3.5241639614105225,2.5362460613250732,-0.7327529191970825,-0.8334300518035889,2.0500335693359375,0.9878270626068115,2.0549156665802,3.280917167663574,0.543793261051178,0.40602004528045654,1.2652292251586914,1.6847025156021118,0.6613606810569763,0.48743921518325806,1.5848220586776733,0.15714702010154724,3.097687244415283,3.4039037227630615,3.8253872394561768,-0.3216516077518463,0.36484676599502563,3.5613582134246826,0.849107563495636,2.804208278656006,3.8222780227661133,1.8004229068756104,1.4634736776351929,3.1937408447265625,0.8675617575645447,3.476331949234009,0.3727441728115082,3.1206748485565186,1.5812299251556396,-0.6251404285430908,3.3036246299743652,3.7486493587493896,1.7302109003067017,2.3743913173675537,0.9696566462516785,0.4081820845603943,0.6557876467704773,2.6788806915283203,3.448852300643921,1.163936972618103,3.5292248725891113,2.8257150650024414,3.515986680984497,1.9399545192718506,0.4077995717525482,0.766963541507721,-0.4135100543498993,0.42341694235801697,2.8088972568511963,-0.6076659560203552,0.1522863209247589,3.2291979789733887,1.588698387145996,1.8325613737106323,0.9248793125152588,1.3020033836364746,3.420180559158325,1.643860101699829,1.1540601253509521,0.21492479741573334,0.1739095002412796,1.5416576862335205,3.289651393890381,-0.5035665035247803,-0.46198034286499023,3.0262765884399414,3.655320405960083,1.8784151077270508,0.3909582197666168,0.8705004453659058,1.1364428997039795,1.208755612373352,0.26025456190109253,0.44635772705078125,0.12183551490306854,-0.15554407238960266,-0.25469839572906494,-0.45932525396347046,0.3801633417606354,2.1852316856384277,3.0103812217712402,0.16040968894958496,0.17387855052947998,2.1033482551574707,3.5950064659118652,1.6760215759277344,3.2061989307403564,0.3962271809577942,2.2076709270477295,-0.8994259834289551,2.514460802078247,0.979424774646759,3.5055930614471436,0.26043587923049927,0.08888838440179825,0.9722239375114441,1.9090890884399414,1.4592763185501099,0.7288630604743958,0.5196076035499573,2.52851939201355,0.3626737594604492,0.9660953879356384,1.4570801258087158,1.5067415237426758,1.0723236799240112,1.9673024415969849,0.5451234579086304,2.6414546966552734,0.7439343333244324,3.4168541431427,1.108274221420288,0.9759180545806885,3.0499231815338135,0.3342808485031128,1.1762615442276,1.2191663980484009,1.9902498722076416,2.7178072929382324,2.1931447982788086,1.4139740467071533,3.4444408416748047,-0.2844769358634949,2.468306541442871,2.982534408569336,1.1627116203308105,2.345379114151001,-0.26387879252433777,2.15932035446167,-0.09391316771507263,2.315988540649414,3.9019370079040527,1.4993696212768555,0.8209245204925537,0.1551581770181656,2.0490360260009766,1.3861424922943115,1.0227948427200317,-0.12519274652004242,3.113816976547241,1.2089561223983765,-0.26088747382164,1.8862637281417847,3.144066333770752,-1.0393073558807373,2.0746145248413086,1.443308711051941,1.180871844291687,-0.6809756755828857,1.017892599105835,1.8907074928283691,-1.2943123579025269,0.1569855809211731,0.6024736166000366,0.8391635417938232,2.21470046043396,-1.1607123613357544,1.8066606521606445,2.297105312347412,2.370729446411133,1.7525073289871216,0.9918195605278015,0.6040194034576416,2.8629562854766846,3.4195144176483154,1.8813725709915161,3.7637126445770264,0.20570334792137146,2.110116958618164,1.3198965787887573,1.0308681726455688,1.5510575771331787,1.0269742012023926,3.243104934692383,0.5696408748626709,-1.0560033321380615,3.80950927734375,-0.9885093569755554,2.1285574436187744,0.27239152789115906,-0.9925388693809509,2.664095401763916,0.5445898771286011,3.4431591033935547,2.9532716274261475,1.4311859607696533,2.9324100017547607,3.054269313812256,0.8955436944961548,0.025119181722402573,3.7716782093048096,0.6967297196388245,2.6491522789001465,2.385422945022583,3.098644495010376,3.803661823272705,1.4175082445144653,2.0697543621063232,3.0386083126068115,3.8905580043792725,1.805167555809021,3.370250940322876,0.6452329754829407,1.198987364768982,4.435891628265381,3.9789042472839355,0.2224683314561844,0.784505307674408,3.900918483734131,-0.08793401718139648,-0.20969590544700623,0.9383320212364197,2.9824304580688477,0.4146740138530731,3.451068878173828,0.58146733045578,2.629016876220703,2.5896897315979004,0.8220140933990479,2.19604229927063,2.3683815002441406,1.0513523817062378,-0.5379165410995483,2.0229125022888184,2.577913522720337,1.3024059534072876,2.953939914703369,1.1526473760604858,-1.245646595954895,0.6586165428161621,1.9431781768798828,0.795759916305542,0.4541209042072296,1.3323132991790771,0.9036451578140259,1.0493559837341309,2.936338424682617,1.658136010169983,2.7779719829559326,0.217825248837471,-0.08116167783737183,0.8917245864868164,1.92644202709198,1.5383085012435913,3.03080153465271,-0.5870935916900635,1.1749986410140991,1.7433806657791138,3.2300631999969482,1.3536816835403442,1.4494167566299438,2.132977247238159,1.142106533050537,1.0964027643203735,-1.2843916416168213,2.0309720039367676,0.38181358575820923,0.4180848002433777,1.397927165031433,1.0205312967300415,-1.0413587093353271,-0.3866738975048065,-0.9114582538604736,2.2314321994781494,-0.9689415693283081,0.8953730463981628,-0.7223781943321228,3.3937878608703613,-0.1200912594795227,1.4780992269515991,-1.0540677309036255,1.243822693824768,1.1921714544296265,-0.9881787300109863,2.7142724990844727,1.035949468612671,1.7600966691970825,3.096761703491211,1.0765949487686157,-0.3055479824542999,0.8458605408668518,1.8884222507476807,0.7532628178596497,3.692111015319824,2.047429084777832,1.5398664474487305,2.6263246536254883,3.3768866062164307,1.3354458808898926,2.8829421997070312,3.073280096054077,2.618222951889038,2.2130420207977295,1.131805658340454,1.4583265781402588,-0.6291894912719727,3.3120579719543457,3.235522985458374,2.4912800788879395,0.8031163215637207,2.785531520843506,1.2172937393188477,2.5934207439422607,0.44533485174179077,3.7607927322387695,0.8943837881088257,2.2763075828552246,0.15780554711818695,2.040842056274414,0.27494242787361145,3.4471356868743896,2.545928955078125,3.3168694972991943,3.057359218597412,2.0106849670410156,3.7272539138793945,-0.2229020744562149,-0.3912656009197235,2.5650482177734375,0.5276594758033752,1.9719535112380981,3.524745225906372,1.5316689014434814,0.3282936215400696,2.1106743812561035,2.4451394081115723,1.7900829315185547,2.8499183654785156,2.917431354522705,-0.6730669736862183,2.2880237102508545,2.3319878578186035,0.7816553711891174,3.079967975616455,0.3783358633518219,3.771744728088379,3.4605367183685303,3.4098503589630127,2.232668399810791,2.7577970027923584,0.8026202917098999,0.8486886024475098,1.336401104927063,1.9623465538024902,2.0950241088867188,2.3696439266204834,2.694972276687622,0.7788678407669067,1.5454350709915161,0.642559289932251,1.9104548692703247,2.817577838897705,1.2944279909133911,2.2089552879333496,2.9942898750305176,0.6504705548286438,2.6843504905700684,1.2824846506118774,-0.555378794670105,1.9917773008346558,2.4853670597076416,2.2720730304718018,0.05783434212207794,1.741772174835205,3.9805495738983154,1.6283323764801025,3.9042468070983887,-0.08662469685077667,1.0114973783493042,-0.2651676535606384,2.276035785675049,-0.6100764274597168,2.0067296028137207,1.3208794593811035,1.0322285890579224,1.9896197319030762,2.3109211921691895,2.2940375804901123,0.7046573758125305,2.1666882038116455,0.8269322514533997,1.038419246673584,3.1033222675323486,2.011228084564209,0.4995725154876709,3.0119717121124268,1.7737351655960083,1.1829763650894165,3.768042802810669,2.453611135482788,-1.2905018329620361,0.4045698344707489,-0.7276555299758911,1.108733057975769,0.7043386101722717,1.7317860126495361,3.1328790187835693,1.5019123554229736,0.00757896201685071,1.7896612882614136,4.704597473144531,-1.2845878601074219,1.471731185913086,3.7090561389923096,-0.9188922643661499,0.25417712330818176,0.40908655524253845,-0.6070359349250793,3.279542922973633,2.3822507858276367,-1.0290439128875732,-1.0308316946029663,1.6611809730529785,0.5246748328208923,1.5900543928146362,1.5552963018417358,1.9574549198150635,0.3487606346607208,1.906638503074646,-0.15992236137390137,-0.1188686341047287,1.874509334564209,2.240257501602173,3.598365068435669,2.4335744380950928,3.23817777633667,1.554663896560669,2.292228937149048,1.6409223079681396,1.495506763458252,3.2188000679016113,3.7398290634155273,2.8911819458007812,3.769780397415161,2.451735019683838,2.5036814212799072,3.150447130203247,1.2291960716247559,0.9691900610923767,3.689431667327881,-0.07314413040876389,0.73487389087677,0.839801013469696,1.4524890184402466,2.1931369304656982,1.3259851932525635,3.0612916946411133,0.2924751043319702,0.6916547417640686,2.3885679244995117,0.7535249590873718,1.8109625577926636,3.2876040935516357,2.6223843097686768,2.415797472000122,2.450838565826416,1.9013489484786987,2.8400003910064697,3.637033224105835,1.2191064357757568,0.49888309836387634,0.527450680732727,3.3894202709198,2.8021371364593506,1.5861856937408447,-0.5647964477539062,1.773905634880066,2.8972055912017822,0.4014616310596466,3.552330493927002,3.5676872730255127,0.44267264008522034,2.05513334274292,2.6361255645751953,2.9887442588806152,0.289135605096817,3.268169641494751,3.1244053840637207,0.8573437333106995,-0.19672121107578278,-0.7705172300338745,1.6629849672317505,3.544947862625122,-0.6638178825378418,3.600919008255005,-0.5163041353225708,-0.025210803374648094,1.4350130558013916,1.8289581537246704,0.6889894008636475,1.6060850620269775,-0.9072462916374207,2.4202475547790527,1.693398356437683,0.15449632704257965,3.578507661819458,2.314469575881958,4.283806324005127,2.3266613483428955,0.48093342781066895,3.2087161540985107,-0.08493365347385406,1.7356622219085693,1.580642580986023,1.29006028175354,0.9935421347618103,2.0631296634674072,2.585738182067871,1.8831286430358887],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"54_dense_retrieval_retrievers_bm25_modif\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"54_dense_retrieval\"],\"x\":[11.089601516723633,11.02492904663086,10.883186340332031,10.981522560119629,10.981151580810547,10.95821762084961,10.9121675491333,10.982845306396484,11.157200813293457,10.896283149719238,10.92767333984375,10.96117877960205,12.416332244873047,12.561809539794922,12.500757217407227,12.375421524047852,12.55825138092041,12.481864929199219,12.49730396270752,12.1989107131958,12.471221923828125,12.578041076660156,12.469315528869629,11.689789772033691],\"y\":[2.4378020763397217,2.470427989959717,2.49906587600708,2.4681739807128906,2.4417030811309814,2.4632935523986816,2.5861077308654785,2.4596455097198486,2.333122968673706,2.6070048809051514,2.5345096588134766,2.460688352584839,1.9628962278366089,2.3608059883117676,2.222209930419922,2.1565310955047607,2.1810107231140137,2.1097028255462646,2.3813178539276123,2.188523292541504,2.159067153930664,2.3556671142578125,1.6200826168060303,2.324319839477539],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"57_language_work_languages_dataset_diffe\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"57_language_work\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"x\":[13.87353229522705,13.208016395568848,14.106237411499023,13.092193603515625,13.336145401000977,13.386882781982422,12.319914817810059,13.021512985229492,13.98583698272705,12.727828025817871,14.158334732055664,12.96531867980957,13.214096069335938,13.292146682739258,13.683382034301758,13.187692642211914,13.279541969299316,12.989252090454102,12.971858978271484,12.976456642150879,13.035022735595703,12.499113082885742,13.1813325881958,12.979851722717285,13.296390533447266,13.039684295654297,13.842495918273926,13.132991790771484,13.9254732131958,12.167425155639648,14.098097801208496,13.920403480529785,12.558815002441406,12.849774360656738,12.970587730407715,14.02845287322998,12.911493301391602,13.765254974365234,13.905959129333496,13.326302528381348,13.942405700683594,12.925593376159668,13.41197395324707,13.25645637512207,12.991842269897461,13.592504501342773,13.934208869934082,13.955780029296875,13.298561096191406,13.263577461242676,12.345218658447266,13.947273254394531,13.009997367858887,13.643670082092285,13.180303573608398,13.74915599822998,12.808438301086426,13.863404273986816,12.991873741149902,13.162498474121094,12.859356880187988,13.075448036193848,13.156818389892578,12.886246681213379,13.997769355773926,13.03771686553955,12.784741401672363,12.894862174987793,12.97797966003418,13.60038948059082,14.051366806030273,12.930017471313477,12.712410926818848,12.782764434814453,12.85952377319336,12.852415084838867,13.387727737426758,13.939677238464355,12.471564292907715,13.912345886230469,12.641358375549316,13.367488861083984,13.8226318359375,13.64598560333252,13.757844924926758,12.249322891235352,13.732461929321289,13.127846717834473,12.968668937683105,13.813699722290039,13.807978630065918,13.008394241333008,13.787175178527832,13.993060111999512,13.531773567199707,13.984949111938477,13.01711654663086,14.002456665039062,13.822026252746582,13.829243659973145,13.329290390014648,12.908177375793457,14.121039390563965,13.951969146728516,12.813560485839844,13.843708038330078,13.839166641235352,14.121064186096191,12.558928489685059,13.817070960998535,13.30069637298584,13.859855651855469,13.09512710571289,12.266813278198242,12.803173065185547,13.087562561035156,13.733831405639648,13.884516716003418,12.79514217376709,13.156198501586914,12.635796546936035,13.271025657653809,12.98554801940918,13.871160507202148,13.031496047973633,14.019685745239258,13.770309448242188,13.913249969482422,13.995428085327148,12.395220756530762,13.254109382629395,12.519502639770508,13.016617774963379,13.239713668823242,13.700984954833984,13.32162857055664,13.05946159362793,12.602531433105469,12.626888275146484,13.298456192016602,13.010909080505371,12.473347663879395,13.30741024017334,12.52789306640625,13.24752140045166,14.047928810119629,12.949474334716797,13.951871871948242,14.054276466369629,12.840779304504395,12.930652618408203,13.249177932739258,14.00323486328125,12.957128524780273,13.71989631652832,12.736191749572754,13.225967407226562,11.134943962097168,11.025127410888672,11.245323181152344,11.604127883911133,11.156891822814941,11.352185249328613,11.527443885803223,11.282083511352539,11.126087188720703,11.73695182800293,11.573534965515137,13.375391006469727,11.404452323913574,11.218412399291992,10.824934959411621,11.162917137145996,11.561422348022461,11.366466522216797,10.656908988952637,11.300393104553223,11.272549629211426,11.278218269348145,11.19459056854248,11.2599458694458,11.612763404846191,10.9222412109375,11.204724311828613,11.096510887145996,11.001474380493164,11.493638038635254,11.407891273498535,11.244793891906738,11.168529510498047,11.017486572265625,11.451024055480957,11.223573684692383,11.187163352966309,11.271790504455566,11.26801872253418,11.24870491027832,11.188407897949219,11.215272903442383,11.429038047790527,11.241147994995117,11.28900146484375,11.684981346130371,11.643854141235352,11.422016143798828,11.644282341003418,11.211750030517578,11.794293403625488,10.666879653930664,11.676538467407227,11.204644203186035,11.4844388961792,11.210556983947754,10.822954177856445,11.602618217468262,11.172928810119629,11.261439323425293,11.18604850769043,11.24404525756836,11.392868995666504,11.273892402648926,11.069701194763184,11.81900691986084,11.298412322998047,11.697004318237305,11.151240348815918,11.614058494567871,11.220532417297363,11.269240379333496,11.204486846923828,11.25660228729248,11.144911766052246,11.228436470031738,11.389638900756836,11.027185440063477,11.341411590576172,11.200804710388184,10.692567825317383,11.169511795043945,11.02439022064209,10.92267894744873,11.365792274475098,11.304564476013184,11.123757362365723,11.317601203918457,11.2233304977417,11.252074241638184,10.953468322753906,11.1937837600708,11.191131591796875,11.507041931152344,13.65318489074707,13.939993858337402,13.685359001159668,13.784351348876953,13.504968643188477,13.94735336303711,13.143325805664062,13.834990501403809,13.44897747039795,13.599161148071289,13.199520111083984,13.714217185974121,13.943765640258789,13.477546691894531,13.838189125061035,13.14396858215332,13.451980590820312,13.139006614685059,13.305334091186523,13.572847366333008,13.578728675842285,13.640463829040527,13.756519317626953,13.478137016296387,13.640913963317871,13.371881484985352,13.30630874633789,13.778650283813477,13.48436164855957,13.48245620727539,13.242944717407227,13.188318252563477,13.671472549438477,13.161858558654785,13.182262420654297,13.67568302154541,13.578418731689453,13.574835777282715,13.530089378356934,12.509833335876465,13.550978660583496,13.269086837768555,13.929557800292969,13.697040557861328,13.825508117675781,13.642824172973633,13.51134204864502,13.453692436218262,13.557821273803711,13.553349494934082,13.648940086364746,13.369372367858887,13.12069034576416,13.566463470458984,13.692540168762207,13.704095840454102,13.338266372680664,13.631470680236816,13.964845657348633,13.344286918640137,13.934170722961426,13.56275463104248,13.731254577636719,13.44790267944336,13.115975379943848,13.600167274475098,8.303739547729492,13.527002334594727,13.690975189208984,13.423891067504883,13.938860893249512,13.628800392150879,13.415291786193848,13.883581161499023,13.465723037719727,13.598390579223633,13.958017349243164,13.456777572631836,13.795967102050781,9.905937194824219,13.677034378051758,13.943849563598633,13.411235809326172,13.518133163452148,13.331656455993652,8.079171180725098,8.06274700164795,7.97455358505249,7.848491191864014,8.032004356384277,7.964422702789307,7.964263916015625,8.134442329406738,8.114529609680176,7.841091632843018,7.8066725730896,8.066001892089844,7.995242118835449,8.329507827758789,8.135189056396484,7.784605979919434,7.775929927825928,8.180281639099121,8.422222137451172,7.919281005859375,7.885015487670898,7.895800590515137,7.771629810333252,8.177929878234863,7.973200798034668,8.28538703918457,7.766735553741455,7.8133392333984375,7.980544567108154,7.834801197052002,7.792785167694092,8.400049209594727,8.152592658996582,7.950541019439697,7.951925754547119,8.21005630493164,7.920266628265381,7.854425430297852,8.000518798828125,7.90179443359375,8.401914596557617,8.250751495361328,7.81003999710083,8.432668685913086,7.853431224822998,8.134767532348633,8.218738555908203,7.8007283210754395,8.225289344787598,7.825068473815918,8.072797775268555,8.098258972167969,8.405604362487793,7.813677787780762,7.821072101593018,7.985950946807861,7.810969352722168,8.3153657913208,8.099970817565918,7.857753276824951,8.26423168182373,7.91890811920166,7.952602386474609,7.912245273590088,7.974617958068848,7.784420490264893,8.138467788696289,8.036944389343262,7.761767864227295,7.890107154846191,8.475704193115234,8.049864768981934,7.910464286804199,7.735909938812256,8.021018028259277,7.9187726974487305,7.908551216125488,7.699703216552734,7.959645748138428,7.826615333557129,8.282672882080078,7.934024333953857,8.200965881347656,7.851012229919434,7.659298419952393,7.964986801147461,7.375672340393066,7.989088535308838,8.16100788116455,7.880589008331299,7.8358564376831055,7.8488359451293945,7.915194511413574,8.008590698242188,7.819448471069336,7.807568073272705,7.392760753631592,7.372259140014648,7.680557727813721,8.323735237121582,7.405981540679932,7.390983581542969,8.47547721862793,7.814692974090576,8.292101860046387,8.115535736083984,7.816876411437988,7.764798164367676,7.4307541847229,8.173230171203613,7.816125869750977,7.800354957580566,7.687350749969482,7.416838645935059,8.04630184173584,7.372389793395996,8.061315536499023,7.425755023956299,7.877260684967041,8.388016700744629,7.779290199279785,8.227629661560059,8.057212829589844,8.02259349822998,7.782587051391602,8.23399543762207,7.90402889251709,7.8760199546813965,7.4668121337890625,8.308961868286133,7.824159622192383,7.712260723114014,7.914191246032715,7.685945987701416,7.940428733825684,12.020341873168945,12.169936180114746,12.101693153381348,12.094861030578613,11.659080505371094,12.379535675048828,12.050992965698242,12.262616157531738,12.431180000305176,11.988661766052246,12.082118034362793,12.085043907165527,12.180397033691406,12.176328659057617,11.84795093536377,12.084325790405273,12.128952980041504,12.28459644317627,12.194994926452637,12.008187294006348,11.911856651306152,12.048596382141113,12.229466438293457,12.182258605957031,12.1466703414917,12.20826530456543,11.903266906738281,11.976790428161621,11.813048362731934,12.021183967590332,12.020978927612305,12.224409103393555,11.788299560546875,12.209701538085938,11.93454647064209,11.924176216125488,12.019671440124512,12.825284004211426,12.15970516204834,12.297412872314453,12.158596992492676,12.110843658447266,12.26720905303955,12.509387016296387,12.426708221435547,12.31076431274414,12.200416564941406,12.108731269836426,12.027480125427246,12.294480323791504,12.179040908813477,11.868775367736816,12.016388893127441,12.533409118652344,11.871016502380371,12.278585433959961,12.497551918029785,11.953156471252441,12.103267669677734,12.292784690856934,12.235295295715332,11.931829452514648,10.677026748657227,10.417326927185059,10.19378662109375,10.34382438659668,10.359889030456543,10.168207168579102,10.379969596862793,10.382912635803223,10.3951997756958,10.006451606750488,10.593812942504883,10.523648262023926,10.564718246459961,10.390353202819824,10.409890174865723,10.375975608825684,10.270461082458496,10.380226135253906,10.342330932617188,10.389986038208008,10.570111274719238,10.563359260559082,10.393143653869629,10.445023536682129,10.406803131103516,10.406514167785645,9.976093292236328,10.553617477416992,10.514777183532715,10.404207229614258,10.352548599243164,10.402541160583496,10.373939514160156,10.66649055480957,10.147748947143555,10.177634239196777,10.380538940429688,10.481019020080566,10.569191932678223,10.38936710357666,10.453319549560547,10.417245864868164,10.462665557861328,10.532641410827637,10.139423370361328,10.995478630065918,10.441753387451172,10.408876419067383,10.595641136169434,10.508939743041992,10.599928855895996,10.383362770080566,10.315134048461914,10.520706176757812,10.196410179138184,10.421682357788086,10.112748146057129,10.42435073852539,9.957427978515625,9.407848358154297,9.24934196472168,9.988895416259766,9.616469383239746,9.254940032958984,9.619887351989746,9.763313293457031,9.41704273223877,9.903557777404785,9.640640258789062,9.904218673706055,9.833464622497559,9.501309394836426,10.281272888183594,9.5150146484375,9.301063537597656,9.468867301940918,9.583669662475586,9.652227401733398,9.2976655960083,9.610597610473633,9.950066566467285,9.628329277038574,9.354300498962402,9.685493469238281,9.437472343444824,9.71446418762207,9.970356941223145,9.591911315917969,9.4934720993042,11.245697975158691,9.630011558532715,9.857994079589844,9.644820213317871,9.684257507324219,9.61834716796875,9.774521827697754,9.085481643676758,10.036049842834473,9.738116264343262,9.506299018859863,9.30492877960205,9.699036598205566,9.771791458129883,9.192770957946777,9.514463424682617,9.63692855834961,9.843480110168457,9.38808536529541,9.520946502685547,9.217379570007324,9.838109970092773,9.648056030273438,9.77692699432373,9.458232879638672,9.670414924621582,9.680063247680664,9.057123184204102,9.02020263671875,8.764460563659668,9.149687767028809,8.96601676940918,8.83845329284668,7.854395389556885,8.691113471984863,8.074891090393066,7.991252422332764,8.15771198272705,8.11121940612793,7.889158725738525,8.559086799621582,7.732449054718018,8.129582405090332,7.739405155181885,8.906782150268555,8.779287338256836,8.384539604187012,8.096807479858398,7.73988151550293,7.74143123626709,10.807848930358887,8.938881874084473,8.25653076171875,8.156562805175781,8.910407066345215,8.929011344909668,8.912620544433594,8.349308967590332,9.150344848632812,9.063459396362305,8.188298225402832,8.082099914550781,8.124395370483398,8.095884323120117,7.761401653289795,7.85822057723999,7.735099792480469,9.143465995788574,7.9051079750061035,8.62359619140625,8.771903991699219,8.10483455657959,8.881553649902344,7.842296600341797,7.751319408416748,8.932684898376465,8.793137550354004,8.686760902404785,8.676131248474121,8.958822250366211,7.80643892288208,8.740694046020508,7.7468767166137695,8.771867752075195,9.189112663269043,9.039446830749512,9.115840911865234,9.096116065979004,9.138891220092773,8.879927635192871,8.893594741821289,8.761467933654785,8.84201431274414,8.92170524597168,8.984542846679688,8.870153427124023,9.148591041564941,9.1036376953125,8.759328842163086,9.173047065734863,9.000730514526367,9.311477661132812,9.226938247680664,8.916337966918945,9.266477584838867,8.85345458984375,8.894457817077637,9.150871276855469,8.838128089904785,8.816519737243652,9.046035766601562,8.901216506958008,9.159720420837402,8.813364028930664,9.301010131835938,8.885666847229004,9.035273551940918,9.151689529418945,9.173745155334473,9.153688430786133,9.068913459777832,9.077773094177246,8.93253231048584,9.23687744140625,8.845597267150879,8.906045913696289,9.104510307312012,8.794829368591309,8.974413871765137,8.559188842773438,9.103609085083008,8.886739730834961,8.847504615783691,8.852912902832031,8.7335786819458,8.971410751342773,8.835782051086426,8.914285659790039,9.40961742401123,9.401433944702148,9.173538208007812,9.561808586120605,9.133511543273926,9.540070533752441,9.700275421142578,9.41574478149414,9.289504051208496,9.243343353271484,9.41695785522461,9.496110916137695,9.767770767211914,9.445943832397461,9.416340827941895,9.574084281921387,9.292332649230957,9.226899147033691,9.462366104125977,9.370545387268066,9.44404411315918,9.387925148010254,9.353792190551758,9.420747756958008,9.548852920532227,9.489824295043945,9.414810180664062,9.33342456817627,9.402825355529785,9.672316551208496,9.393604278564453,9.239797592163086,9.441904067993164,9.516083717346191,9.340642929077148,9.363749504089355,9.59388256072998,9.435190200805664,9.716001510620117,9.322286605834961,9.686580657958984,9.433467864990234,9.423392295837402,9.48121166229248,9.288214683532715,9.267616271972656,9.553618431091309,9.478880882263184,9.371923446655273,9.760424613952637,9.519524574279785,9.401512145996094,9.502867698669434,13.446449279785156,13.293713569641113,13.339015007019043,13.234209060668945,13.390213012695312,13.275945663452148,13.059819221496582,13.199735641479492,13.168375968933105,13.352516174316406,13.391827583312988,13.276883125305176,13.3883638381958,13.391206741333008,13.22536563873291,13.355441093444824,13.415063858032227,13.445826530456543,13.263446807861328,13.253372192382812,13.080596923828125,13.292510986328125,13.387595176696777,13.407983779907227,13.065677642822266,13.157443046569824,13.285446166992188,13.371879577636719,13.443427085876465,13.312077522277832,13.325447082519531,13.201516151428223,13.412653923034668,13.387580871582031,13.376065254211426,13.23760986328125,13.391357421875,13.28543758392334,13.155158996582031,13.134252548217773,13.264272689819336,13.326519012451172,13.441741943359375,13.180994987487793,13.229814529418945,13.385966300964355,13.26877498626709,13.252023696899414,8.319539070129395,8.872060775756836,8.969176292419434,8.707355499267578,8.74543285369873,8.927281379699707,8.319100379943848,8.32375717163086,8.666680335998535,8.335197448730469,8.70714282989502,9.042021751403809,8.712772369384766,8.291357040405273,8.325663566589355,8.38085651397705,8.75160026550293,8.50749397277832,8.352300643920898,8.444624900817871,8.73957347869873,8.366410255432129,8.543909072875977,8.721636772155762,8.295104026794434,8.693644523620605,8.322171211242676,8.898469924926758,8.4146146774292,8.773049354553223,8.323225975036621,9.074934959411621,8.57706356048584,8.7324857711792,8.81857967376709,9.022188186645508,8.334091186523438,8.382010459899902,8.761948585510254,8.723730087280273,9.060052871704102,8.774007797241211,8.736745834350586,8.403592109680176,9.99891185760498,9.666812896728516,9.87675666809082,9.759414672851562,9.77737045288086,9.712926864624023,9.860082626342773,10.150372505187988,9.757026672363281,9.756099700927734,9.88352108001709,9.667094230651855,9.834576606750488,9.662117958068848,9.90071964263916,9.823662757873535,9.885151863098145,9.897980690002441,9.599701881408691,9.828813552856445,10.166223526000977,9.664942741394043,9.68136215209961,9.909587860107422,9.613624572753906,9.683416366577148,9.677172660827637,9.895627975463867,9.831110954284668,9.791001319885254,9.888591766357422,10.030932426452637,10.003292083740234,13.670157432556152,13.798901557922363,13.369604110717773,13.828919410705566,13.80864143371582,13.781868934631348,13.87149715423584,13.169575691223145,13.689194679260254,13.725122451782227,13.867948532104492,13.486695289611816,13.795697212219238,13.88539981842041,13.794876098632812,13.703969955444336,13.475425720214844,13.485220909118652,13.642592430114746,13.45029067993164,13.17920970916748,13.825347900390625,13.710762977600098,13.833584785461426,13.823281288146973,13.777862548828125,13.709671974182129,13.589035034179688,13.624615669250488,13.818294525146484,13.520774841308594,13.499052047729492,11.668291091918945,11.893047332763672,11.399813652038574,11.399984359741211,11.689655303955078,11.275130271911621,11.221017837524414,12.198081016540527,12.027531623840332,11.27303695678711,11.897500038146973,11.873612403869629,11.806347846984863,11.46186637878418,11.398143768310547,11.580697059631348,11.322710990905762,11.420403480529785,11.564152717590332,11.334028244018555,11.482306480407715,11.22659969329834,11.433297157287598,11.450419425964355,11.489837646484375,11.502764701843262,11.421748161315918,11.442119598388672,12.117900848388672,11.278861999511719,11.366277694702148,11.415215492248535,11.413336753845215,11.3681058883667,11.386617660522461,11.464384078979492,11.333633422851562,11.434808731079102,11.411873817443848,11.339577674865723,11.400299072265625,11.394913673400879,11.41909122467041,11.410325050354004,11.436091423034668,11.440723419189453,11.421466827392578,11.424187660217285,10.752309799194336,11.30122184753418,11.429636001586914,11.37482738494873,11.405726432800293,11.432455062866211,11.494464874267578,11.39643669128418,11.412282943725586,11.394112586975098,11.40078353881836,9.92600154876709,9.90449047088623,9.936745643615723,9.92866325378418,9.815776824951172,9.909724235534668,9.910490989685059,9.891134262084961,9.903509140014648,9.932821273803711,9.916892051696777,9.828373908996582,9.930082321166992,9.88005256652832,9.91939926147461,10.002853393554688,9.763079643249512,9.934072494506836,9.936637878417969,9.912662506103516,9.736724853515625,9.912676811218262,9.76375961303711,9.90211296081543,9.953424453735352,9.921619415283203,9.907418251037598,9.980151176452637,9.924909591674805,9.238378524780273,9.30147647857666,9.232583999633789,9.182598114013672,9.238131523132324,9.218341827392578,9.234349250793457,9.302995681762695,9.232303619384766,9.124175071716309,9.416687965393066,9.293940544128418,9.349409103393555,9.29613208770752,9.22963809967041,9.208431243896484,9.365690231323242,9.037209510803223,9.345200538635254,9.246965408325195,9.236005783081055,9.268945693969727,11.956093788146973,12.490596771240234,12.429107666015625,12.604439735412598,12.537409782409668,12.447066307067871,12.514530181884766,12.494388580322266,12.444210052490234,12.318142890930176,12.335335731506348,12.499151229858398,12.503713607788086,12.477373123168945,12.43051815032959,12.483855247497559,12.453995704650879,12.551338195800781,12.408796310424805,12.414511680603027,10.386235237121582,10.229713439941406,10.389017105102539,10.301673889160156,10.361222267150879,10.3976469039917,10.311823844909668,10.32546329498291,10.373322486877441,10.870636940002441,null,10.355380058288574,10.305009841918945,10.051471710205078,10.35975456237793,12.890239715576172,12.9169282913208,12.898111343383789,12.95676326751709,12.863883018493652,12.8355073928833,12.85640811920166,13.136683464050293,12.889006614685059,7.990469932556152,12.911820411682129,12.735673904418945,12.899605751037598],\"y\":[1.3817375898361206,1.0612719058990479,1.1385481357574463,1.08525550365448,1.1626895666122437,1.2480696439743042,0.4848286211490631,1.2720167636871338,1.1438474655151367,1.3171401023864746,1.4076839685440063,1.264595627784729,1.0884864330291748,1.1517572402954102,1.355188012123108,1.7229746580123901,0.972616970539093,1.1486741304397583,0.8810750246047974,1.257293701171875,0.9572843313217163,0.8085752129554749,0.9309483766555786,0.7958930730819702,1.0975350141525269,0.6360377073287964,0.9854050874710083,1.798175573348999,1.3100166320800781,0.67192143201828,1.2066819667816162,1.2865713834762573,0.9056100845336914,0.6249911189079285,1.7463622093200684,1.2420947551727295,0.7832163572311401,1.1565204858779907,1.3271960020065308,1.3933436870574951,1.0287328958511353,0.7349646091461182,1.1217514276504517,1.414216160774231,1.152062177658081,1.4249526262283325,1.1767480373382568,1.2981083393096924,1.1263619661331177,1.0864992141723633,0.5083565711975098,1.0866432189941406,1.8492233753204346,1.125476598739624,0.944848895072937,1.1385208368301392,0.678071916103363,1.3165032863616943,0.6076777577400208,1.3482673168182373,1.6210135221481323,0.9811966419219971,1.2457525730133057,1.171410322189331,1.1819112300872803,0.8604665398597717,0.942077100276947,0.516913115978241,1.7408256530761719,1.4816997051239014,1.3886042833328247,1.7745593786239624,0.794830322265625,0.5049740076065063,0.8629714250564575,0.9947225451469421,1.3124425411224365,1.181308388710022,0.65378338098526,0.9567456841468811,0.8112806677818298,1.0578453540802002,1.270293116569519,1.1971663236618042,1.2050573825836182,0.49335476756095886,1.5063146352767944,1.3702335357666016,1.2522984743118286,1.2417391538619995,1.4349349737167358,1.29513418674469,1.3764736652374268,1.1924550533294678,1.3044599294662476,1.0554274320602417,0.9203598499298096,1.1263551712036133,1.4461302757263184,1.4113985300064087,1.1233012676239014,1.292132019996643,1.383273959159851,1.2921757698059082,1.4860016107559204,1.4392153024673462,1.4001719951629639,0.9837578535079956,0.6659143567085266,0.9185031056404114,1.1586668491363525,1.4281121492385864,1.1629738807678223,0.510953962802887,0.5897344946861267,0.8766728639602661,1.4713213443756104,1.1049691438674927,0.5188847184181213,1.4383823871612549,1.3952010869979858,1.3247922658920288,0.9874094128608704,1.13014554977417,0.8806256651878357,1.1321403980255127,1.1713860034942627,1.29535710811615,1.1741254329681396,0.570444643497467,1.2960554361343384,0.7852280735969543,0.9467426538467407,1.7510077953338623,1.3233586549758911,1.1152814626693726,1.0483297109603882,0.647891640663147,0.5596342086791992,1.3211227655410767,0.9677881002426147,0.6502654552459717,1.0543253421783447,0.6772043704986572,1.212782382965088,1.1973955631256104,1.1335924863815308,1.4092952013015747,1.3452917337417603,0.6293814778327942,1.8506160974502563,1.066854476928711,1.2767295837402344,0.7192075848579407,1.712160587310791,0.8005959391593933,1.1127575635910034,-0.2791852056980133,-0.56585294008255,-0.33805036544799805,-0.5237462520599365,0.26701799035072327,-0.765414834022522,-0.7155061364173889,-0.6682496070861816,-0.6063860654830933,-0.3886547386646271,-0.6360691785812378,-0.5970664024353027,-0.6077590584754944,-0.3316837251186371,0.027478214353322983,-1.0895836353302002,-0.5561929941177368,0.06399431079626083,0.6634865999221802,-0.7742803692817688,0.14976142346858978,-1.053679347038269,-0.8100492358207703,-0.6574156284332275,-0.6182628273963928,2.9431962966918945,-0.8820046186447144,-0.8308306336402893,-0.7561229467391968,0.06208992749452591,-1.2723103761672974,0.021186748519539833,-0.9865592122077942,0.42442458868026733,-0.5611394047737122,-0.19294996559619904,-0.6945754289627075,-0.3245929181575775,0.14573052525520325,-0.8709970712661743,-0.352821946144104,-0.20292386412620544,-0.3928586542606354,-0.45419180393218994,-0.6536911725997925,-0.43990081548690796,0.47350263595581055,0.13158506155014038,-0.7095873355865479,-1.041664481163025,-0.30340245366096497,0.33025580644607544,-0.5825144648551941,-0.9634666442871094,0.1646973192691803,-0.9077123999595642,0.321090430021286,-0.4256440997123718,0.016457444056868553,-0.3007483184337616,-0.9926905035972595,-0.40955081582069397,-0.7163980603218079,-0.7613950967788696,-0.5563022494316101,-0.30304890871047974,-0.6749861836433411,-0.5403029322624207,-0.3122972548007965,-0.610802948474884,-0.7764879465103149,-0.3963066339492798,-0.9999945163726807,-0.2833714783191681,-0.40003347396850586,-0.9637208580970764,-0.45351165533065796,-0.5818671584129333,-0.7205086350440979,-0.9992696046829224,0.6563940644264221,-1.185823917388916,-1.0215363502502441,-0.13112245500087738,-0.6537771224975586,-0.7294257283210754,-0.7630423307418823,-0.7293157577514648,-0.9494206309318542,-0.3285471796989441,-0.1588168889284134,-0.36618950963020325,-0.9938457608222961,0.16926676034927368,-0.4724136292934418,0.05542946234345436,-0.2140544056892395,-0.305911123752594,-0.019443433731794357,0.04754577577114105,-0.5232704877853394,-0.11347304284572601,-0.42826223373413086,-0.3631307780742645,-0.34572187066078186,-0.06021670997142792,0.02899486944079399,-0.5741272568702698,-0.17813809216022491,-0.3307811915874481,-0.5624246597290039,-0.33823102712631226,-0.49408844113349915,-0.2875242233276367,-0.5673142671585083,-0.5054513216018677,-0.19457708299160004,0.04647556692361832,0.15025267004966736,-0.4029132127761841,-0.09265140444040298,-0.06226486340165138,-0.49906080961227417,-0.42240944504737854,-0.37416812777519226,-0.4376997649669647,-0.5433786511421204,-0.48809999227523804,-0.3587982952594757,-0.1461024433374405,-0.4376213848590851,-0.5315554738044739,-0.5797322392463684,2.760768413543701,-0.5617931485176086,-0.37373608350753784,1.0235728025436401,-0.2975458800792694,-0.12453179061412811,-0.39394140243530273,0.08051753044128418,-0.5586355328559875,-0.5786116719245911,-0.5049486756324768,0.1441514790058136,-0.7706948518753052,-0.21628828346729279,-0.3426320552825928,-0.16151897609233856,-0.39145922660827637,-0.5925371646881104,-0.2956721782684326,0.027539370581507683,-0.30378881096839905,0.042292892932891846,-0.12347960472106934,-0.06413992494344711,-0.6200217604637146,-0.4954264461994171,-0.34516313672065735,3.190534830093384,-0.2043280303478241,-0.5706292986869812,-0.7155386209487915,0.12354090809822083,-0.219037726521492,-0.293334424495697,0.07163935899734497,-0.5650919675827026,-0.29002729058265686,0.07026058435440063,-0.5817172527313232,-0.10595829039812088,-1.2771755456924438,-0.2213153839111328,0.0876251682639122,-0.7389064431190491,-0.14196763932704926,-0.6306360363960266,0.011064082384109497,0.014209670014679432,-0.09134206920862198,-0.2708755433559418,-0.20944665372371674,-0.15542514622211456,-0.016267064958810806,0.33848637342453003,1.3138884241925552e-05,-0.3145374655723572,-0.37487006187438965,0.08202043920755386,-0.06734184175729752,-0.26890313625335693,-0.2482864111661911,-0.37177231907844543,-0.342668354511261,0.05140113830566406,-0.009848969988524914,-0.26512715220451355,-0.21851959824562073,-0.20532433688640594,-0.34504765272140503,-0.12112276256084442,-0.11948814243078232,-0.36536410450935364,-0.43950170278549194,-0.3522815406322479,-0.16667716205120087,-0.33151018619537354,-0.4151010513305664,-0.21129287779331207,0.18422266840934753,-0.13335680961608887,0.17345526814460754,0.07078302651643753,-0.06395117193460464,-0.33869442343711853,-0.20178943872451782,-0.20231139659881592,-0.02338528260588646,0.17116601765155792,-0.3580891191959381,-0.31286415457725525,-0.32009008526802063,-0.1331133246421814,0.18067573010921478,-0.2697615623474121,0.08013986051082611,-0.31005680561065674,0.028209263458848,0.10702250152826309,0.2667584717273712,-0.3471834659576416,-0.3380453586578369,-0.07149399816989899,-0.4023083448410034,-0.10830654948949814,-0.17893050611019135,-0.2754707932472229,0.22065694630146027,-0.1637326180934906,-0.12212429195642471,-0.17535138130187988,-0.2760526239871979,-0.01606772653758526,0.11322040110826492,0.021302759647369385,1.5433651208877563,1.4605567455291748,1.4761929512023926,1.4625173807144165,1.4584766626358032,1.4409211874008179,1.4749916791915894,1.4284682273864746,1.4748163223266602,1.4806615114212036,1.4631659984588623,1.411305546760559,1.4848947525024414,1.0450985431671143,1.4943875074386597,1.3517125844955444,1.4878875017166138,1.4422603845596313,1.5346200466156006,1.5406434535980225,1.499199628829956,1.4141327142715454,1.4414336681365967,1.3886102437973022,1.4608150720596313,1.4566928148269653,1.5145379304885864,1.6541327238082886,1.5264090299606323,1.531654953956604,1.4845945835113525,1.4692519903182983,1.5288569927215576,1.5096898078918457,1.3595173358917236,1.587659478187561,1.4848111867904663,1.480909824371338,1.49443781375885,1.6546357870101929,1.5368572473526,1.2316579818725586,1.3251349925994873,1.5530869960784912,1.5514334440231323,1.5379899740219116,1.494699239730835,1.525313377380371,1.4642879962921143,1.526352882385254,1.288293480873108,1.483452320098877,1.4212583303451538,1.5053179264068604,1.3147481679916382,1.3840261697769165,1.5364909172058105,1.516196370124817,1.4429274797439575,1.4955360889434814,1.5462852716445923,1.454986810684204,1.4761298894882202,1.4729735851287842,1.450022578239441,1.443420648574829,1.4701296091079712,3.4334287643432617,3.9167301654815674,3.949054002761841,3.438246250152588,3.445382595062256,3.36970853805542,3.7780611515045166,2.977111577987671,3.824798583984375,4.014473915100098,3.966614246368408,3.947669744491577,3.546800136566162,3.5740740299224854,3.722076654434204,3.7931604385375977,3.4850759506225586,3.1680963039398193,4.030865669250488,3.494990587234497,3.631016969680786,3.9821364879608154,3.874598264694214,3.628248453140259,3.362018585205078,3.6282668113708496,3.4427807331085205,3.539586305618286,3.9404172897338867,3.503599166870117,3.3990092277526855,3.441103219985962,2.9686286449432373,3.454322576522827,3.507044553756714,3.588791847229004,3.930246591567993,3.598909854888916,3.4998040199279785,3.598375082015991,3.547398805618286,3.815009355545044,3.1761443614959717,3.7004573345184326,2.9148776531219482,3.357351303100586,3.646010398864746,4.046837329864502,3.844369649887085,3.3957107067108154,3.5331528186798096,3.4589388370513916,3.6157045364379883,3.513286590576172,3.4719655513763428,3.176704168319702,3.3323616981506348,3.9445858001708984,3.7024219036102295,3.222160577774048,3.219435214996338,3.6485061645507812,4.530323028564453,4.6652750968933105,4.259049415588379,4.17231559753418,4.669703960418701,4.087711334228516,4.67042350769043,4.63741397857666,4.61146879196167,4.00691556930542,4.465812683105469,4.730312824249268,3.997835874557495,4.689051151275635,4.215987682342529,4.713289260864258,3.9740796089172363,4.662024021148682,4.6290106773376465,4.683803558349609,4.155202865600586,4.592298984527588,4.655975341796875,4.570188999176025,4.67529296875,4.149698734283447,4.111392498016357,4.625186920166016,4.129754066467285,4.608168125152588,4.609062194824219,4.684344291687012,4.598238468170166,4.515726089477539,4.097564697265625,4.126251697540283,4.632607936859131,3.846534013748169,4.030094146728516,4.677065372467041,4.595035552978516,4.631924152374268,4.152017593383789,4.6212286949157715,4.092752933502197,4.058029651641846,4.524569034576416,4.68303918838501,4.761435031890869,4.133310794830322,4.037563800811768,4.604251384735107,4.569831848144531,4.141808032989502,4.4533562660217285,4.653535842895508,4.054136753082275,4.630998134613037,3.962613344192505,1.5877009630203247,1.2727618217468262,1.4802215099334717,1.6104549169540405,1.4569437503814697,1.539625644683838,1.472213864326477,2.0593249797821045,1.3333063125610352,1.6195672750473022,0.646668553352356,1.6028765439987183,1.468073844909668,1.4336971044540405,1.469533920288086,1.5507453680038452,1.6590402126312256,1.5533472299575806,1.5326228141784668,1.4909952878952026,1.5933552980422974,0.6876698732376099,1.5868359804153442,1.573804259300232,1.5753282308578491,1.2330971956253052,1.6163599491119385,0.7645369172096252,1.5960463285446167,1.578489899635315,0.5172238349914551,1.5027610063552856,1.38727867603302,1.5584850311279297,1.571655035018921,1.5743860006332397,1.5758932828903198,1.8300682306289673,1.7321003675460815,1.5929148197174072,1.4410037994384766,1.594342589378357,1.557225227355957,1.3510727882385254,1.2829564809799194,1.1802170276641846,1.5979278087615967,1.6627514362335205,1.6593539714813232,1.5607953071594238,1.3650426864624023,0.5767338275909424,1.5249618291854858,1.5589072704315186,1.6015998125076294,1.5737286806106567,1.52560555934906,1.5944088697433472,2.3929059505462646,2.3886959552764893,2.2481915950775146,2.366041660308838,2.3146793842315674,2.3162174224853516,2.369158983230591,2.5022075176239014,2.4162755012512207,2.523205280303955,2.5030629634857178,2.322309732437134,2.426605224609375,2.189079761505127,2.5072808265686035,2.1927618980407715,2.2802486419677734,2.404411792755127,2.5210015773773193,2.5109353065490723,2.188095808029175,2.2342336177825928,1.3085989952087402,2.270885467529297,2.520770311355591,2.4987809658050537,2.2820372581481934,2.2575674057006836,2.305651903152466,2.480987310409546,2.6763017177581787,2.3891241550445557,2.503016233444214,2.507106065750122,2.5494508743286133,2.4986634254455566,2.2242064476013184,2.3021676540374756,2.150664806365967,2.744899034500122,2.3470451831817627,2.496939182281494,2.4149718284606934,2.483858346939087,2.3855793476104736,2.2803268432617188,2.255380868911743,2.259631872177124,2.3189663887023926,2.3722052574157715,2.399129867553711,2.2720420360565186,2.2237091064453125,2.365219831466675,2.2338740825653076,2.3938751220703125,3.968130350112915,4.086702346801758,3.7755074501037598,3.991488218307495,3.975116014480591,4.029293060302734,4.046248435974121,4.013284683227539,4.176393032073975,4.156956672668457,4.055082321166992,3.9321210384368896,3.937089204788208,3.9232289791107178,3.8601222038269043,3.866779327392578,3.9942166805267334,3.850517511367798,3.827030897140503,4.08809232711792,3.6295218467712402,4.113508701324463,4.187417507171631,3.6063523292541504,3.9586288928985596,4.24702787399292,3.9166204929351807,4.1035475730896,3.9102394580841064,4.182351112365723,3.869297981262207,3.981309175491333,3.9789490699768066,3.9490838050842285,3.944530487060547,3.942408323287964,4.038426399230957,4.008914947509766,4.139644622802734,3.7226343154907227,4.209354877471924,4.045825958251953,3.9882144927978516,4.28642463684082,4.0150346755981445,3.615358352661133,4.036133289337158,4.028831958770752,3.9747371673583984,3.9180045127868652,3.7974116802215576,4.060174942016602,4.20427942276001,4.09943962097168,-0.5178394317626953,-0.650516152381897,-0.479851096868515,-0.4310290515422821,-0.5378267168998718,-0.6259913444519043,-0.7380387187004089,-0.21319614350795746,-0.7479232549667358,-0.610841691493988,-0.6754353642463684,-0.5430178642272949,-0.6768491268157959,-0.45748335123062134,-0.3581988215446472,-0.8913076519966125,-0.698038637638092,-0.7323350310325623,-0.4265297055244446,-0.6116015911102295,-0.9226518869400024,-0.7730879783630371,-0.7615750432014465,-0.7331415414810181,-0.8823621869087219,-0.8506748676300049,-0.7885114550590515,-0.723310649394989,-0.7920389175415039,-0.6305829882621765,-0.698114275932312,-0.7590868473052979,-0.9453780651092529,-0.47471097111701965,-0.7153941988945007,-0.7514808177947998,-0.32000821828842163,-0.5421023368835449,-0.5268405079841614,-0.7256784439086914,-0.6647657752037048,-0.5601599812507629,-0.8130180239677429,-0.5797785520553589,-0.76743084192276,-0.740344226360321,-0.6408556699752808,-0.4104425609111786,-0.3353818655014038,-0.6775698661804199,-0.7086683511734009,-0.9370354413986206,-0.7324972152709961,2.849513053894043,2.6924357414245605,2.519382953643799,2.685671091079712,3.4131829738616943,3.3496320247650146,2.9062132835388184,2.5787570476531982,2.8339319229125977,2.6660850048065186,3.363393783569336,3.0692508220672607,3.403278112411499,2.94502854347229,2.862663984298706,2.849548578262329,3.311357021331787,2.857738494873047,2.829335927963257,2.7103164196014404,2.6873369216918945,3.1780202388763428,2.415661573410034,2.984997510910034,2.9960906505584717,3.0859627723693848,3.064985752105713,3.3739285469055176,2.99613094329834,3.3052146434783936,2.759016990661621,2.554168224334717,2.7810420989990234,3.370600700378418,2.8825161457061768,2.5941309928894043,3.402747631072998,3.1956849098205566,2.574057102203369,2.4850573539733887,3.1060783863067627,2.757913827896118,2.8156826496124268,2.6702122688293457,3.1264901161193848,3.3880455493927,2.8700947761535645,2.6230366230010986,3.193037986755371,3.0153839588165283,3.2098007202148438,3.411353826522827,3.419900894165039,2.993455171585083,3.2046985626220703,3.197061538696289,3.5652174949645996,3.1496386528015137,3.4359893798828125,3.163926362991333,3.4493963718414307,3.234320640563965,3.2002310752868652,3.272998094558716,3.3818790912628174,3.3397650718688965,3.0758256912231445,3.1623947620391846,3.384615182876587,3.2408924102783203,3.358213186264038,3.416884660720825,3.167921781539917,3.4248197078704834,3.2836413383483887,3.2514102458953857,3.342782497406006,3.3918700218200684,3.194164752960205,3.047151803970337,3.55344820022583,3.2842931747436523,3.3557095527648926,3.1627235412597656,3.2219274044036865,3.0630640983581543,3.3806440830230713,3.417919874191284,3.0919387340545654,3.379578113555908,3.3994202613830566,3.3199284076690674,-1.2806227207183838,-1.4134907722473145,-1.3984967470169067,-0.8591680526733398,-1.1667323112487793,-0.9986860752105713,-1.226072072982788,-1.2499401569366455,-1.3567789793014526,-1.3018178939819336,-1.279323697090149,-1.3926376104354858,-0.8900179862976074,-1.3667707443237305,-1.3356115818023682,-1.4169893264770508,-1.3518718481063843,-1.17157781124115,-1.1077362298965454,-1.3785203695297241,-1.242591381072998,-1.3728218078613281,-1.36669921875,-1.1680376529693604,-1.3303998708724976,-0.8987433314323425,-1.368369460105896,-1.1938246488571167,-1.3907575607299805,-1.302925705909729,-1.1836531162261963,-1.1196755170822144,-1.154442310333252,1.930212140083313,1.9589751958847046,1.9856090545654297,2.118469476699829,1.8433870077133179,2.083873748779297,1.9659463167190552,1.6238290071487427,1.9088102579116821,1.984571933746338,1.8491621017456055,1.8348524570465088,1.9095227718353271,2.1351234912872314,2.0434975624084473,1.9872238636016846,1.9833941459655762,1.972872018814087,1.8849225044250488,1.9644986391067505,1.6182628870010376,2.087661027908325,1.9592905044555664,1.9400484561920166,1.8442121744155884,2.052821636199951,2.1948325634002686,1.9715791940689087,1.9485830068588257,1.9467763900756836,2.033796548843384,2.05224347114563,1.6213551759719849,1.7356750965118408,1.8189188241958618,2.011199474334717,1.6284370422363281,1.6911828517913818,1.434216856956482,1.9124113321304321,1.708951473236084,1.345273733139038,1.9241002798080444,1.706599235534668,1.5955106019973755,1.6075619459152222,1.8218833208084106,1.9240765571594238,1.5612646341323853,1.4347044229507446,1.8333743810653687,1.4203897714614868,1.508184552192688,1.6106964349746704,1.7237390279769897,1.6263391971588135,1.7565888166427612,1.8806627988815308,1.4971145391464233,1.4235330820083618,2.043459415435791,1.6557258367538452,-1.7047370672225952,-1.7189042568206787,-1.7727406024932861,-1.7914379835128784,-1.7604166269302368,-1.5030242204666138,-1.8196848630905151,-1.7163331508636475,-1.6494393348693848,-1.6843397617340088,-1.7724716663360596,-1.7692655324935913,-1.7378288507461548,-1.7441824674606323,-1.464735746383667,-1.7443996667861938,-1.7241114377975464,-1.6847602128982544,0.36447063088417053,-1.677884578704834,-1.6699968576431274,-1.7729989290237427,-1.7663508653640747,-1.7174915075302124,-1.603288173675537,-1.7217741012573242,-1.7533800601959229,-1.768631100654602,-1.7753701210021973,-2.003943920135498,-2.018934726715088,-2.0210397243499756,-1.9936052560806274,-1.9008338451385498,-1.989972710609436,-2.0100209712982178,-1.9968105554580688,-2.0147523880004883,-2.0181238651275635,-2.0108845233917236,-1.9405567646026611,-1.9686578512191772,-1.9617516994476318,-2.0073821544647217,-1.730658769607544,-1.9098670482635498,-1.9983524084091187,-2.0032131671905518,-2.021005392074585,-1.78288996219635,-1.9908539056777954,-1.9123069047927856,-1.9989206790924072,-1.8872182369232178,-2.0079305171966553,-1.9907784461975098,-1.9225029945373535,-2.0194551944732666,0.11233166605234146,0.2616926431655884,0.13499541580677032,0.13999022543430328,0.13154223561286926,0.03455710411071777,0.1343674212694168,0.09137420356273651,0.0880853608250618,-0.03193724900484085,0.282012939453125,0.19786401093006134,0.27887430787086487,0.23954655230045319,0.1252308189868927,0.2048257291316986,0.21515387296676636,0.2964838743209839,0.28831443190574646,0.10827220976352692,0.010295077227056026,0.17448420822620392,-0.01690675877034664,-0.3107600808143616,-0.2527309060096741,-0.2602718472480774,-0.33801159262657166,-0.32423314452171326,-0.19620446860790253,-0.286507248878479,-0.2868269979953766,2.4987032413482666,-0.19626036286354065,-0.2882573902606964,2.8162295818328857,-0.27515092492103577,-0.28718024492263794,-0.29625701904296875,-0.3112342059612274,-0.32359012961387634,-0.29335451126098633,-0.24639558792114258,-0.7522066235542297,-0.7360655665397644,-0.7566789388656616,-0.7287562489509583,-0.7565518617630005,-0.6721810698509216,-0.7266584038734436,-0.7462540864944458,-0.6999889016151428,1.181185007095337,null,-0.7139708399772644,-0.7397772073745728,-0.6260511875152588,-0.7241697311401367,0.026861172169446945,-0.11374158412218094,-0.0861361101269722,-0.08715442568063736,0.14325851202011108,-0.09389468282461166,-0.06494750827550888,-0.21590127050876617,-0.029119817540049553,0.18973292410373688,-0.05029120296239853,0.2098502218723297,-0.02140832133591175],\"type\":\"scattergl\",\"visible\":false},{\"hoverinfo\":\"text\",\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"58_chatgpt_sarcasm_responses_proprietary\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"58_chatgpt_sarcasm\"],\"x\":[8.408228874206543,8.558862686157227,8.468222618103027,8.394969940185547,8.488740921020508,8.462080955505371,8.458389282226562,8.648653030395508,8.45824146270752,8.496983528137207,8.497143745422363,8.60595989227295,8.652069091796875,8.46937370300293,8.726569175720215,8.704874992370605,8.474954605102539,8.499213218688965,11.03643798828125,11.07364559173584,11.248010635375977,11.170310974121094,10.949195861816406,11.285311698913574,10.957159996032715,11.096034049987793,11.2238130569458,11.023275375366211,11.042583465576172,11.16788101196289,11.188385009765625,7.443446159362793,7.453991889953613,7.434319496154785,7.457515239715576,7.445913314819336,7.432080268859863,7.492039680480957,7.444361686706543,7.45388650894165,7.470397472381592,7.456826210021973,7.457319259643555,7.452033042907715,9.245034217834473,9.240274429321289,9.255699157714844,9.306416511535645,9.330011367797852,9.5535249710083,9.301241874694824,9.279705047607422,9.539828300476074,9.543222427368164,9.282510757446289,8.700318336486816,8.662346839904785,8.710755348205566,8.685635566711426,10.063787460327148,8.825141906738281,9.746978759765625,8.632473945617676,8.7011137008667,9.762179374694824,10.958747863769531,8.679919242858887,8.863896369934082,8.659948348999023,8.705171585083008,8.791703224182129,8.700974464416504,8.720808029174805,9.014729499816895,8.698103904724121,8.63776969909668,8.665627479553223,10.26634407043457,10.128600120544434,10.363204956054688,10.149825096130371,10.254651069641113,10.238754272460938,10.266803741455078,10.18940258026123,10.27680492401123,10.135074615478516,9.167410850524902],\"y\":[0.8488311767578125,0.8990485668182373,0.930065929889679,0.9668167233467102,0.8830121159553528,0.8673952221870422,0.9546473622322083,0.9203405976295471,0.9518395662307739,0.8990188241004944,0.8826806545257568,1.001754879951477,0.8333197236061096,0.94631427526474,0.7836666703224182,0.7688788771629333,0.916922390460968,0.7554416656494141,3.106004476547241,3.0234382152557373,3.3541853427886963,3.115736246109009,3.0922281742095947,3.283050775527954,3.0349786281585693,3.2528135776519775,3.086516857147217,3.087618112564087,3.101942300796509,3.0745949745178223,3.133884906768799,3.3650591373443604,3.3583810329437256,3.3732411861419678,3.3514108657836914,3.3623621463775635,3.374535083770752,3.3646886348724365,3.3602709770202637,3.329127788543701,3.332568883895874,3.3524117469787598,3.345237970352173,3.361050605773926,2.6040866374969482,2.5882256031036377,2.6099462509155273,2.364428997039795,2.5036041736602783,2.5556437969207764,2.5966074466705322,2.450493335723877,2.5414693355560303,2.4918816089630127,2.483466386795044,1.2634673118591309,1.199538230895996,1.2940387725830078,1.2151010036468506,2.198474407196045,1.475010633468628,3.3699069023132324,1.1698120832443237,1.2515336275100708,3.3798534870147705,1.8569083213806152,0.4001754820346832,0.5290360450744629,0.39178118109703064,0.5974481105804443,0.46730977296829224,0.44593337178230286,0.3917016386985779,0.42155221104621887,0.3866553008556366,0.5923276543617249,0.5228016972541809,2.246767520904541,2.1721770763397217,2.3008975982666016,2.423623561859131,2.2409117221832275,2.265772581100464,2.3193318843841553,2.154175281524658,2.2092831134796143,2.1171963214874268,2.0166401863098145],\"type\":\"scattergl\",\"visible\":false}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"shapes\":[{\"line\":{\"color\":\"#CFD8DC\",\"width\":2},\"type\":\"line\",\"x0\":11.025702166557313,\"x1\":11.025702166557313,\"y0\":-2.324195683002472,\"y1\":5.475650286674499},{\"line\":{\"color\":\"#9E9E9E\",\"width\":2},\"type\":\"line\",\"x0\":5.76931939125061,\"x1\":16.282084941864014,\"y0\":1.5757273018360136,\"y1\":1.5757273018360136}],\"annotations\":[{\"showarrow\":false,\"text\":\"D1\",\"x\":5.76931939125061,\"y\":1.5757273018360136,\"yshift\":10},{\"showarrow\":false,\"text\":\"D2\",\"x\":11.025702166557313,\"xshift\":10,\"y\":5.475650286674499}],\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eHierarchical Documents and Topics\\u003c\\u002fb\\u003e\",\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"sliders\":[{\"currentvalue\":{\"prefix\":\"Level: \"},\"pad\":{\"t\":20},\"steps\":[{\"args\":[{\"visible\":[true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]}],\"label\":\"0\",\"method\":\"update\"},{\"args\":[{\"visible\":[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]}],\"label\":\"1\",\"method\":\"update\"},{\"args\":[{\"visible\":[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]}],\"label\":\"2\",\"method\":\"update\"},{\"args\":[{\"visible\":[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]}],\"label\":\"3\",\"method\":\"update\"},{\"args\":[{\"visible\":[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]}],\"label\":\"4\",\"method\":\"update\"},{\"args\":[{\"visible\":[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]}],\"label\":\"5\",\"method\":\"update\"},{\"args\":[{\"visible\":[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,true,true,true,true,true,true,true,true,true,true,true,true,true,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]}],\"label\":\"6\",\"method\":\"update\"},{\"args\":[{\"visible\":[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,true,true,true,true,true,true,true,true,true,true,false,false,false,false,false,false,false,false,false,false,false]}],\"label\":\"7\",\"method\":\"update\"},{\"args\":[{\"visible\":[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,true,true,true,true,true,true,true,false,false,false,false]}],\"label\":\"8\",\"method\":\"update\"},{\"args\":[{\"visible\":[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,true,true,true,true]}],\"label\":\"9\",\"method\":\"update\"}]}],\"width\":1200,\"height\":750,\"xaxis\":{\"visible\":false},\"yaxis\":{\"visible\":false}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('75c17da4-f863-4bed-bd58-8ba3c821cc29');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}